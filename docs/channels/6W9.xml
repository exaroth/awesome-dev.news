<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://www.awesome-dev.news</link><description></description><item><title>Half-Life</title><link>https://www.filfre.net/2024/12/half-life/</link><author>dmazin</author><category>hn</category><pubDate>Sun, 23 Feb 2025 08:18:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Around twenty years ago, people would have laughed if you told them that videogames would end up at the Smithsonian, but the Half-Life team really did want to make games that were more than just throwaway toys. The rule against cinematics — which made our jobs much harder and also ended up leaving a lot of my favorite work out of the game — was a kind of ideological stake in the ground: we really did want the game and the story to be the same thing. It was far from flawless, but it was really trying to push the boundaries of a young medium.— Valve artist Steve TheodoreBy 1998, the first-person shooter was nearing its pinnacle of popularity. In June of that year,  magazine could list fourteen reasonably big-budget, high-profile FPS’s earmarked for release in the next six months alone. And yet the FPS seemed rather to be running to stand still. Almost all of the innovation taking place in the space was in the realm of technology rather than design.To be sure, progress in the former realm was continuing apace. Less than five years after id Software had shaken the world with , that game’s low-resolution 2.5D graphics and equally crude soundscapes had become positively quaint. Aided and abetted by a fast-evolving ecosystem of 3D-graphics hardware from companies like 3Dfx, id’s  engine had raised the bar enormously in 1996; ditto  in 1997. These were the cutting-edge engines that everyone with hopes of selling a lot of shooters scrambled to license. Then, in May of 1998, with  not scheduled for release until late the following year, Epic MegaGames came along with their own  engine, boasting a considerably longer bullet list of graphical effects than . In thus directly challenging id’s heretofore unquestioned supremacy in the space,  ignited a 3D-graphics arms race that seemed to promise even faster progress in the immediate future.Yet whether they sported the name  or  or something else on their boxes, single-player FPS’s were still content to hew to the “shooting gallery” design template laid out by . You were expected to march through a ladder-style campaign consisting of a set number of discrete levels, each successive one full of more and more deadly enemies to kill than the last, perhaps with some puzzles of the lock-and-key or button-mashing stripe to add a modicum of variety. These levels were joined together by some thread of story, sometimes more elaborate and sometimes — usually — less so, but so irrelevant to what occurred inside the levels that impatient gamers could and sometimes did skip right over the intervening cutscenes or other forms of exposition in order to get right back into the action.This was clearly a model with which countless gamers were completely comfortable, one which had the virtue of allowing them maximal freedom of choice: follow along with the story or ignore it, as you liked. Or, as id’s star programmer John Carmack famously said: “Story in a game is like a story in a porn movie. It’s expected to be there, but it’s not that important.”But what if you could build the story right into the gameplay, such that the two became inseparable? What if you could eliminate the artificial division between exposition and action, and with it the whole conceit of a game as a mere series of levels waiting to be beaten one by one? What if you could drop players into an open-ended space where the story was taking place all around them?This was the thinking that animated an upstart newcomer to the games industry that went by the name of Valve L.L.C. The game that resulted from it would prove the most dramatic conceptual advance in the FPS genre since , with lessons and repercussions that reached well beyond the borders of shooter country.Mike Harrington, who left Valve in 2000 because there were other fish in the sea.The formation of Valve was one of several outcomes of a dawning realization inside the Microsoft of the mid-1990s that computer gaming was becoming a very big business. The same realization led a highly respected Microsoft programmer named Michael Abrash to quit his cushy job in Redmond, Washington, throw his tie into the nearest trashcan, and move to Mesquite, Texas, to help John Carmack and the other scruffy id boys make . It led another insider named Alex St. John to put together the internal team who made DirectX, a library of code that allowed game developers and players to finally say farewell to creaky old MS-DOS and join the rest of the world that was running Windows 95. It led Microsoft to buy an outfit called Ensemble Studios and their promising real-time-strategy game  as a first wedge for levering their way into the gaming market as a publisher of major note. And it led to Valve Corporation.In 1996, Valve’s future co-founders Gabe Newell and Mike Harrington were both valued employees of Microsoft. Newell had been working there in project-management roles since 1983; he had played an important part in the creation of the early versions of Windows before moving on to Microsoft Office and other high-profile applications. Harrington was a programmer whose tenure had been shorter, but he had made significant contributions to Windows NT, Microsoft’s business- and server-oriented operating system.As Newell tells the tale, he had an epiphany when he was asked to commission a study to find out just how many computers were currently running Microsoft Windows in the United States. The number of 20 million that he got back was impressive. Yet he was shocked to learn that Windows wasn’t the most popular single piece of software to be found on American personal computers; that was rather a game called . Newell and Harrington had long enjoyed playing games. Now, it seemed, there were huge piles of money to be earned from making them. Doing so struck them as a heck of a lot more fun than making more operating systems and business software. It was worth a shot, at any rate; both men were wealthy enough by now that they could afford to take a flier on something completely different.So, on August, 24, 1996, the pair quit Microsoft to open an office for their new company Valve some five miles away in Kirkland, Washington. At the time, no one could have imagined — least of all them — what a milestone moment this would go down as in the history of gaming. “On the surface, we should have failed,” says Gabe Newell. “Realistically, both Mike and I thought we would get about a year into it, realize we’d made horrible mistakes, and go back to our friends at Microsoft and ask for our jobs back.”Be that as it may, they did know what kind of game they wanted to make. Or rather Newell did; from the beginning, he was the driving creative and conceptual force, while Harrington focused more on the practical logistics of running a business and making quality software. Like so many others in the industry he was entering, Newell wanted to make a shooter. Yet he wanted his shooter to be more immersive and encompassing of its player than any of the ones that were currently out there, with a story that was embedded right into the gameplay rather than standing apart from it. Valve wasted no time in licensing id’s  engine to bring these aspirations to life, via the game that would come to be known as .As the id deal demonstrates, Newell and Harrington had connections and financial resources to hand that almost any other would-be game maker would have killed for; both had exited Microsoft as millionaires several times over. Yet they had no access to gaming distribution channels, meaning that they had to beat the bush for a publisher just like anyone else with a new studio. They soon found that their studio and their ambitious ideas were a harder sell than they had expected. With games becoming a bigger business every year, there were a lot of deep-pocketed folks from other fields jumping into the industry with plans to teach all of the people who were already there what they were doing wrong; such folks generally had no clue about what it took to do games right. Seasoned industry insiders had a name for these people, one that was usually thoroughly apt: “Tourists.” At first glance, the label was easy to apply to Newell and Harrington and Valve. Among those who did so was Mitch Lasky, then an executive at Activision, who would go on to become a legendary gaming venture capitalist. He got “a whiff of tourism” from Valve, he admits. He says he still has “post-traumatic stress disorder” over his decision to pass on signing them to a publishing deal — but by no means was he the only one to do so.Newell and Harrington finally wound up pitching to Sierra, a publisher known primarily for point-and-click adventure games, a venerable genre that was now being sorely tested by all of the new FPS’s. In light of this, Sierra was understandably eager to find a horse of the new breed to back. The inking of a publishing deal with Valve was one of the last major decisions made by Ken Williams, who had founded Sierra in his living room back in 1980 but was now in the process of selling the business he had built from the ground up. As a man with such deep roots in adventure games, he found Valve’s focus on story both refreshing and appealing. Still, there was a lot of wrangling between the parties, mainly over the ultimate disposition of the rights to the  name; Williams wanted them to go to Sierra, but Newell and Harrington wanted to retain them for themselves. In the end, with no other publishers stepping up to the plate, Valve had to accept what Sierra was offering, a capitulation that would lead to a lengthy legal battle a few years down the road. For now, though, they had their publisher.As for Ken Williams, who would exit the industry stage left well before  was finished:Now that I’m retired, people sometimes ask me what I used to do. I usually just say, “I had a game company back in the old days.” That inevitably causes them to say, “Did you make any games I might have heard of?” I answer, “Leisure Suit Larry.” That normally is sufficient, but if there is no glimmer of recognition I pull out the heavy artillery and say, “Half-Life.” Unless they’ve been sleeping under a rock for the last couple of decades, that always lights up their eyes.One can imagine worse codas to a business career…In what could all too easily be read as another sign of naïve tourism, Newell and Harrington agreed to a crazily optimistic development timeline, promising a finished game for the Christmas of 1997, which was just one year away. To make it happen, they hired a few dozen level designers, programmers, artists, and other creative and technical types, many of whom had no prior professional experience in the games industry, all of whom recognized what an extraordinary opportunity they were being handed and were willing to work like dogs to make the most of it. The founders tapped a fertile pool of recruits in the online  and  modding scenes, where amateurs were making names for themselves by bending those engines in all sorts of new directions. They would now do the same on a professional basis at Valve, even as the programmers modified the  engine itself to suit their needs, implementing better lighting and particle effects, and adding scripting and artificial-intelligence capabilities that the straightforward run-and-shoot gameplay in which id specialized had never demanded. Gabe Newell would estimate when all was said and done that 75 percent of the code in the engine had been written or rewritten at Valve.In June of 1997, Valve took  to the big E3 trade show, where it competed for attention with a murderers’ row of other FPS’s, including early builds of , , , , and . Valve didn’t even have a booth of their own at the show. Nor were they to be found inside Sierra’s;  was instead shown in the booth of 3Dfx. Like so many of Valve’s early moves, this one was deceptively clever, because 3Dfx was absolutely huge at the time, with as big a buzz around their hardware as id enjoyed around their software.  walked away from the show with the title of “Best Action Game.”The validation of E3 made the unavoidable moment of reckoning that came soon after easier to stomach. I speak, of course, about the moment when Valve had to recognize that they didn’t have a ghost of a chance of finishing the game that they wanted to make within the next few months. Newell and Harrington looked at the state of the project and decided that they could probably scrape together a more or less acceptable but formulaic shooter in time for that coming Christmas. Or they could keep working and end up with something amazing for the  Christmas. To their eternal credit, they chose the latter course, a decision which was made possible only by their deep pockets. For Sierra, who were notorious for releasing half-finished games, certainly did not intend to pay for an extra year of development time. The co-founders would have to foot that bill themselves. Nevertheless, to hear Gabe Newell tell it today, it was a no-brainer: “Late is just for a little while. Suck is forever.”The anticipation around  didn’t diminish in the months that followed, not even after the finished  took the world by storm in May of 1998. Despite being based on a two-plus-year-old engine in a milieu that usually prized the technologically new and shiny above all else, Valve’s “shooter with a story” had well and truly captured the imaginations of gamers. During the summer of 1998, a demo of the game consisting of the first three chapters — including the now-iconic opening scenes, in which you ride a tram into a secret government research facility as just another scientist on the staff headed for another day on the job — leaked out of the offices of a magazine to which it had been sent. It did more to promote the game than a million dollars worth of advertising could have; the demo spread like wildfire online, raising the excitement level to an even more feverish pitch.  was different enough to have the frisson of novelty in the otherwise homogeneous culture of the FPS, whilst still being readily identifiable  an FPS. It was the perfect mix of innovation and familiarity.So, it was no real surprise when the full game turned into a massive hit for Valve and Sierra after its release on November 19, 1998. The magazines fell all over themselves to praise it. , normally the closest thing the hype-driven journalism of gaming had to a voice of sobriety, got as high on s supply as anyone. The magazine’s long-serving associate editor Jeff Green took it upon himself to render the official verdict.Everything you’ve heard, everything you’ve hoped for — it’s all true. Half-Life, Valve Software’s highly anticipated first-person shooter, is not just one of the best games of the year. It’s one of the best games of any year, an instant classic that is miles better than any of its immediate competition, and, in its single-player form, is the best shooter since DOOM. Plus, despite the fact that it’s “just” a shooter, Half-Life provides one of the best examples ever of how to present an interactive movie — and a great, scary movie at that. sold its first 200,000 copies in the United States before Christmas — i.e., before glowing reviews like the one above even hit the newsstands. But this was the barest beginning to its success story. In honor of its tenth birthday in 2008, Guinness of world-records fame would formally anoint  as the best-selling single FPS in history, with total sales in the neighborhood of 10 million copies across all platforms and countries. For Newell and Harrington, it was one hell of a way to launch a game-development studio. For Sierra, who in truth had done very little for  beyond putting it in a box and shipping it out to stores, it was a tsunami of cash that seemed to come out of nowhere, the biggest game they had ever published almost by an order of magnitude. One does hope that somebody in the company’s new management took a moment to thank Ken Williams for this manna from heaven. has come to occupy such a hallowed, well-nigh sacrosanct position in the annals of gaming that any encounter with the actual artifact today seems bound to be slightly underwhelming. Yet even when we take into account the trouble that any game would have living up to a reputation as elevated as this one’s, the truth is that there’s quite a lot here for the modern skeptical critic to find fault with — and, Lord knows, this particular critic has seldom been accused of lacking in skepticism.Judged purely as a shooter, the design shows its age. It’s sometimes amazingly inspired, but more often no better than average for its time. There’s a lot of crawling through anonymous vents that serve no real purpose other than to extend the length of the game, a lot of places where you can survive only by dying first so as to learn what’s coming, a lot of spots where it’s really not clear at all what the game wants from you. And then there are an  lot of jumping puzzles, shoehorned into a game engine that has way more slop in it than is ideal for such things. I must say that I had more unadulterated fun with LucasArts’s , the last shooter I played all the way through for these histories, than I did with . There the levels are constructed like thrill rides straight out of the  films, with a through-line that seems to just intuitively come to you; looking back, I’m still in awe of their subtle genius in this respect.  is not like that. You really have to work to get through it, and that’s not as appealing to me.Then again, my judgment on these things should, like that of any critic, be taken with a grain of salt. Whether you judge a game good or bad or mediocre hinges to a large degree on what precisely you’re looking for from it; we’ve all read countless negative reviews reflective not so much of a bad game as one that isn’t the game that that reviewer wanted to play. Personally, I’m very much a tourist in the land of the FPS. While I understand the appeal of the genre, I don’t want to expend too many hours or too much effort on it. I want to blast through a fun and engaging environment without too much friction. Make me feel like an awesome action hero while I’m at it, and I’ll probably walk away satisfied, ready to go play something else.  on easy mode gave me that experience;  did not, demanding a degree of careful attention from me that I wasn’t always eager to grant it. If you’re more hardcore about this genre than I am, your judgment of the positives and negatives in these things may very well be the opposite of mine. Certainly  is more typical of its era than  — an era when games like this were still accepted and even expected to be harder and more time-consuming than they are today. C’est la vie et vive la différence!But of course, it wasn’t the granular tactical details of the design that made  stand out so much from the competition back in the day. It was rather its brilliance as a storytelling vehicle that led to its legendary reputation. And don’t worry, you definitely won’t see me quibbling that said reputation isn’t deserved. Even here, though, we do need to be sure that we understand exactly what it did and did not do that was so innovative at the time.Contrary to its popular rep then and now,  was by no means the first “shooter with a story.” Technically speaking, even  has a story, some prattle about a space station and a portal to Hell and a space marine who’s the only one that can stop the demon spawn. The story most certainly isn’t , but it’s there. wasn’t even the shooter at the time of its release with the inarguably best or most complicated story. LucasArts makes a strong bid for the title there. Both  and the aforementioned , released in 1995 and 1997 respectively, weave fairly elaborate tales into the fabric of the existing  universe, drawing on its rich lore, inserting themselves into the established chronology of the original trilogy of films and the “Expanded Universe” series of  novels.Like that of many games of this era, s story betrays the heavy influence of the television show , which enjoyed its biggest season ever just before this game was released. We have the standard nefarious government conspiracy involving extraterrestrials, set in the standard top-secret military installation somewhere in the Desert Southwest. We even have a direct equivalent to Cancer Man, s shadowy, nameless villain who is constantly lurking behind the scenes. “G-Man” does the same in ; voice actor Michael Shapiro even opted to give him a “lizard voice” that’s almost a dead ringer for Cancer Man’s nicotine-addled croak.All told, s story is more of a collection of tropes than a serious exercise in fictional world-building. To be clear, the sketchiness is by no means an automatically bad thing, not when it’s judged in the light of the purpose the story actually needs to serve. Mark Laidlaw, the sometime science-fiction novelist who wrote the script, makes no bones about the limits to his ambitions for it. “You don’t have to write the whole story,” he says. “Because it’s a conspiracy plot, everybody knows more about it than you do. So you don’t have to answer those questions. Just keep raising questions.”Once the shooting starts, plot-related things happen, but it’s all heat-of-the-moment stuff. You fight your way out of the complex after its been overrun by alien invaders coming through a trans-dimensional gate that’s been inadvertently opened, only to find that your own government is now as bent on killing you as the aliens are in the name of the disposal of evidence. Eventually, in a plot point weirdly reminiscent of , you have to teleport yourself onto the aliens’ world to shut down the portal they’re using to reach yours.Suffice to say that, while  may be slightly further along the continuum toward  than  is, it’s only slightly so. Countless better, richer, deeper stories were told in games before this one came along. When people talk about  as “the FPS with a story,” they’re really talking about something more subtle: about its way of  its story. Far from diminishing the game, this makes it more important, across genres well beyond the FPS. The best way for us to start to come to grips with what  did that was so extraordinary might be to look back to the way games were deploying their stories before its arrival on the scene.Throughout the 1980s, story in games was largely confined to the axiomatically narrative-heavy genres of the adventure game and the CRPG. Then, in 1990, Origin Systems released Chris Roberts’s , a game which was as revolutionary in the context of its own time as  was in its. In terms of gameplay,  was a “simulation” of outer-space dog-fighting, not all that far removed in spirit from the classic . What made it stand out was what happened when you weren’t behind the controls of your space fighter. Between missions, you hung out in the officers’ lounge aboard your mother ship, collecting scuttlebutt from the bartender, even flirting with the fetching female pilot in your squadron. When you went into the briefing room to learn about your next mission, you also learned about the effect your last one had had on the unfolding war against the deadly alien Kilrathi, and were given a broader picture of the latest developments in the conflict that necessitated this latest flight into danger. The missions themselves remained shooting galleries, but the story that was woven around them gave them resonance, made you feel like you were a part of something much grander. Almost equally importantly, this “campaign” provided an easy way to structure your time in the game and chart your improving skills; beat all of the missions in the campaign and see the story to its end, and you could say that you had mastered the game as a whole.People  this;  became by far the most popular computer-gaming franchise of the young decade prior to the smashing arrival of  at the end of 1993. The approach it pioneered quickly spread across virtually all gaming genres. In particular, both the first-person-shooter and the real-time strategy genres — the two that would dominate over all others in the second half of the decade — adopted it as their model for the single-player experience. Even at its most rudimentary, a ladder-style campaign gave you a goal to pursue and a framework of progression to hang your hat on.Yet the same approach created a weirdly rigid division between gameplay and exposition, not only on the playing side of the ledger but to a large extent on the development side as well. It wasn’t unusual for completely separate teams to be charged with making the gameplay part of a game and all of the narrative pomp and circumstance that justified it. The disconnect could sometimes verge on hilarious; in , which went so far as to film real humans acting out a B-grade  movie between its levels, the protagonist has a beard in the cutscenes but is clean-shaven during the levels. By the late 1990s, the pre-rendered-3D or filmed-live-action cutscenes sometimes cost more to produce than the game itself, and almost always filled more of the space on the CD.As he was setting up his team at Valve, Gabe Newell attempted to eliminate this weird bifurcation between narrative and gameplay by passing down two edicts to his employees, the only non-negotiable rules he would ever impose upon them.  had to have a story — not necessarily one worthy of a film or a novel, but one worthy of the name. And at the same time, it couldn’t ever, under any conditions, from the very first moment to the very last, take control out of the hands of the player. Everything that followed cascaded from these two simple rules, which many a game maker of the time would surely have seen as mutually contradictory. To state the two most obvious and celebrated results, they meant no cutscenes whatsoever and no externally imposed ladder of levels to progress through — for any sort of level break did mean taking control out of the hands of the player, no matter how briefly.Adapting to such a paradigm the  engine, which had been designed with a traditional FPS campaign in mind, proved taxing but achievable. Valve set up the world of  as a spatial grid of “levels” that were now better described as zones; pass over a boundary from one zone into another, and the new one would be loaded in swiftly and almost transparently. Valve kept the discrete zones small so as to minimize the loading times, judging more but shorter loading breaks to be better than fewer but longer ones. The hardest part was dealing with the borderlands, so to speak; you needed to be able to look into one zone from another, and the enemies and allies around you had to stay consistent before and after a transition. But Valve managed even this through some clever technical sleight of hand — such as by creating overlapping areas that existed in both of the adjoining sets of level data — and through more of the same on the design side, such as by placing the borders whenever possible at corners in corridors and at other spots where the line of sight didn’t extend too far. The occasional brief loading message aside — and they’re  brief, or even effectively nonexistent, on modern hardware —  really does feel like it all takes place in the same contiguous space.
Every detail of  has been analyzed at extensive, exhaustive length over the decades since its release. Such analysis has its place in fan culture, but it can be more confusing than clarifying when it comes to appreciating the game’s most important achievements. The ironic fact is that you can learn almost everything that really matters about  as a game design just by playing it for an hour or so, enough to get into its third chapter. Shall we do so together now? hews to Gabe Newell’s guiding rules from the moment you click the “New Game” button on the main menu and the iconic tram ride into the Black Mesa Research Center begins. The opening credits play over this sequence, in which you are allowed to move around and look where you like. There are reports that many gamers back in the day didn’t actually realize that they were already in control of the protagonist — reports that they just sat there patiently waiting for the “cutscene” to finish, so ingrained was the status quo of bifurcation.The protagonist himself strikes an artful balance between being an undefined player stand-in — what  called an “AFGNCAAP,” or “Ageless, Faceless, Gender-Neutral, Culturally Ambiguous Adventure Person” — and a fully fleshed-out character. As another result of Newell’s guiding rules, you never see him in the game unless you look in a mirror; you only see the world through his eyes. You do, however, hear security guards and colleagues refer to him — or, if you like, to you — as “Gordon” or “Mr. Freeman.” The manual and the intertitles that appear over the opening sequence of the game explain that his full name is indeed Gordon Freeman, and that he’s a 27-year-old theoretical physicist with a PhD from MIT who has been recently hired to work at Black Mesa. The game’s loading screen and its box art show us a rather atypical FPS protagonist, someone very different from the muscle-bound, cigar-chomping Duke Nukem or the cocky budding Jedi knight Kyle Katarn: a slim, studious-looking fellow with Coke-bottle eyeglasses and a token goatee. The heart of the computer-gaming demographic being what it was in 1998, he was disarmingly easy for many of the first players of  to identify with, thus adding just that one more note of immersion to the symphony. Small wonder that he has remained a favorite with cosplayers for decades. In fact, cipher though he almost entirely is, Gordon Freeman has become one of the most famous videogame characters in history.The tram eventually arrives at its destination and a security guard welcomes you to the part of the complex where you work: “Morning, Mr. Freeman. Looks like you’re running late.” Passing through the double blast doors, you learn from your colleagues inside that it’s already been a rough morning: the main computer has crashed, which has thrown a wrench into an important test that was planned for today. Mind you, you don’t learn this through dialog menus, which Valve judged to qualify as taking control away from the player. You can’t speak at all, but if you approach the guards and scientists, they’ll say things to you, leaving you to imagine your own role in the conversation. Or you can stand back and listen to the conversations they have with one another.You can wander around as you like in this office area. You can look in Gordon’s locker to learn a little bit more about him, buy a snack from the vending machine, even blow it up by microwaving it for too long. (“My God!” says your colleague in reaction. “What are you doing?”) All of this inculcates the sense of a lived-in workspace better than any amount of external exposition could have done, setting up a potent contrast with the havoc to come.When you get bored fooling around with lockers and microwaves, you put on your hazardous-environment suit and head down to where the day’s test is to be conducted. It isn’t made clear to you the player just what the test is meant to accomplish; it isn’t even clear that Gordon himself understands the entirety of the research project to which he’s been assigned. All that matters is that the test goes horribly wrong, creating a “resonance cascade event” that’s accompanied by a lot of scary-looking energy beams flying through the air and explosions popping off everywhere. You’ve now reached the end of the second chapter without ever touching a weapon. But that’s about to change, because you’re about to find out that hostile alien lifeforms are now swarming the place. “Get to the surface as soon as you can and let someone know we’re stranded down here!” demand your colleagues. So, you pick up the handy crowbar you find lying on the floor and set off to batter a path through the opposition.This was a drill with which 1990s shooter fans were a lot more familiar, but there are still plenty of new wrinkles. The scientists and guards who were present in the complex before all hell broke loose don’t just disappear. They’re still around, mostly cowering in the corners in the case of the former, doing their best to fight back in that of the latter. The scientists sometimes have vital information to share, while the guards will join you as full-blown allies, firing off their pop-gun pistols at your side, although they tend not to live very long. Allies were a new thing under the FPS sun in 1998, an idea that would quickly spread to other games. (Ditto the way that the guards here are almost better at shooting you in the back than they are at shooting the rampaging aliens. The full history of “allies” in the FPS genre is a fraught one…)As you battle your way up through the complex, you witness plenty of pre-scripted scenes to go along with the emergent behavior of the scientists, guards, and aliens. Ideally, you won’t consciously notice any distinction between the two. You see a scientist being transformed into a zombie by an alien “head crab” behind the window of his office; see several hapless sad sacks tumbling down an open elevator shaft; see a dying guard trying and just failing to reach a healing station. These not only add to the terror and drama, but sometimes have a teaching function. The dying guard, for example, points out to you the presence of healing stations for ensuring that you don’t come to share his fate.It’s the combination of emergent and scripted behaviors, on the part of your enemies and even more on that of your friends, that makes  come so vividly alive. I’m tempted to use the word “realism” here, but I know that Gabe Newell would rush to correct me if I did. Realism, he would say, is boring. Realistically, a guy like Gordon Freeman — heck, even one like Duke Nukem — wouldn’t last ten minutes in a situation like this one. Call it verisimilitude instead, a sign of a game that’s absolutely determined to stay true to its fictional premise, never mind how outlandish it is. The world  presents really is a living one; Newell’s rule of thumb was that five seconds should never pass without something happening near the player. Likewise, the world has to react to anything the player does. “If I shoot the wall, the wall should change, you know?” Newell said. “Similarly, if I were to throw a grenade at a grunt, he should react to it, right? I mean, he should run away from it or lay down on the ground and duck for cover. If he can’t run away from it, he should yell ‘Shit!’ or ‘Fire in the hole!’ or something like that.” In , he will indeed do one or all of these things.The commitment to verisimilitude means that most of what you see and hear is, to use the language of film, diegetic, or internal to the world as Gordon Freeman is experiencing it. Even the onscreen HUD is the one that Gordon is seeing, being the one that’s built into his hazard suit. The exceptions to the diegetic rule are few: the musical soundtrack that plays behind your exploits; the chapter names and titles which flash on the screen from time to time; those credits that are superimposed over the tram ride at the very beginning. These exceptions notwithstanding, the game’s determination to immerse you in an almost purely diegetic sensory bubble is the reason I have to strongly differ with Jeff Green’s description of  as an “interactive movie.” It’s actually the polar opposite of such a stylized beast. It’s an exercise in raw immersion which seeks to eliminate any barriers between you and your lived experience rather than making you feel like you’re doing anything so passive as watching or even guiding a movie. One might go so far as to take  as a sign that gaming was finally growing up and learning to stand on its own two feet by 1998, no longer needing to take so many of its cues from other forms of media.We’ve about reached the end of our hour in  now, so we can dispense with the blow-by-blow. This is not to say that we’ve seen all the game has to offer. Betwixt and between the sequences that I find somewhat tedious going are more jaw-dropping dramatic peaks: the moment when you reach the exit to the complex at long last, only to learn that the United States Army wants to terminate rather than rescue you; the moment when you discover a tram much like the one you arrived on and realize that you can drive it through the tunnels; the moment when you burst out of the complex completely and see the bright blue desert sky above. (Unfortunately, it’s partially blotted out by a big Marine helicopter that also wants to kill you).In my opinion,  could have been an even better game if it had been about half as long, made up of only its most innovative and stirring moments — “all killer, no filler,” as they used to say in the music business. Alas, the marketplace realities of game distribution in the late 1990s militated against this. If you were going to charge a punter $40 or $50 for a boxed game, you had to make sure it lasted more than six or seven hours. If  was being made today, Valve might very well have made different choices.Again, though, mileages will vary when it comes to these things. The one place where  does fall down fairly undeniably is right at the end. Your climactic journey into Xen, the world of the aliens, is so truncated by time and budget considerations as to be barely there at all, being little more than a series of (infuriating) jumping puzzles and a couple of boss fights. Tellingly, it’s here that  gives in at last and violates its own rules of engagement, by delivering — perish the thought! — a cutscene containing the last bits of exposition that Valve didn’t have time to shoehorn into their game proper. The folks from Valve almost universally name the trip to Xen as their biggest single regret, saying they wish they had either found a way to do it properly or just saved it for a sequel. Needless to say, I can only concur.Yet the fact remains that  at its best is so audacious and so groundbreaking that it almost transcends such complaints. Its innovations have echoed down across decades and genres. We’ll be bearing witness to that again and again in the years to come as we continue our journey through gaming history. Longtime readers of this site will know that I’m very sparing in my use of words like “revolutionary.” But I feel no reluctance whatsoever to apply the word to this game.Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like. The books Rocket Jump: Quake and the Golden Age of First-Person Shooters by David L. Craddock, Masters of DOOM: How Two Guys Created an Empire and Transformed Pop Culture by David Kushner, Not All Fairy Tales Have Happy Endings: The Rise and Fall of Sierra On-Line by Ken Williams, and Game Design: Theory & Practice (2nd edition) by Richard Rouse III.  149; the  special issue “Trigger Happy”;  of December 1998, April 1999, and June 1999;  of June 1998, December 1998, and February 1999; Sierra’s newsletter  of Fall 1997;  of September 1998.]]></content:encoded></item><item><title>Meta slashes staff stock awards as group embarks on AI spending drive</title><link>https://www.ft.com/content/67a4c030-a7f6-47af-bab0-a998f0a09506</link><author>apical_dendrite</author><category>hn</category><pubDate>Sun, 23 Feb 2025 01:12:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Clang Static Analyzer and the Z3 constraint solver (2022)</title><link>https://www.cambus.net/clang-static-analyzer-and-the-z3-constraint-solver/</link><author>davikr</author><category>hn</category><pubDate>Sun, 23 Feb 2025 01:03:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[As far as static analyzers are concerned, one of the most important point
to consider is filtering out false positives as much as possible, in order
for the reports to be actionable.This is an area on which  did an excellent job, and likely a
major reason why they got so popular within the open source community,
despite being a closed-source product.LLVM has the  build option, which allows building
LLVM against the Z3 constraint solver.It is documented as follow:The option is enabled in the Debian 11 package (clang-tools-11), but not
in Fedora 36 or Ubuntu 22.04 ones. I added a build option (not enabled by
default) to the llvm and clang packages in Pkgsrc, and successfully
built Z3 enabled packages on NetBSD.For Pkgsrc users, add the following in , and build :For each method, we can use Clang directly on a given translation
unit or use .The first way is using Z3 as an external constraint solver:This is a lot slower than the default, and the commit which documented
the feature mentions a ~15x slowdown over the built-in constraint solver.The second way is using the default range based solver but having Z3 do
refutation to filter out false positives, which is a lot faster:Again, no bugs found. How boring.We can verify what happens if we run the analyzer without involving Z3 at all:We get a false positive, because the default constraint solver cannot
reason about bitwise operations (among other things), and report an
unreachable NULL pointer dereference.]]></content:encoded></item><item><title>Digital Services Playbook</title><link>https://playbook.usds.gov/</link><author>ronbenton</author><category>hn</category><pubDate>Sun, 23 Feb 2025 00:54:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Understand what people needWe must begin digital projects by exploring and pinpointing the needs of the people who will use the service, and the ways the service will fit into their lives. Whether the users are members of the public or government employees, policy makers must include real people in their design process from the beginning. The needs of people — not constraints of government structures or silos — should inform technical and design decisions. We need to continually test the products we build with real people to keep us honest about what is important.Early in the project, spend time with current and prospective users of the serviceUse a range of qualitative and quantitative research methods to determine people’s goals, needs, and behaviors; be thoughtful about the time spentTest prototypes of solutions with real people, in the field if possibleDocument the findings about user goals, needs, behaviors, and preferencesShare findings with the team and agency leadershipCreate a prioritized list of tasks the user is trying to accomplish, also known as “user stories”As the digital service is being built, regularly test it with potential users to ensure it meets people’s needsWho are your primary users?What user needs will this service address?Why does the user want or need this service?Which people will have the most difficulty with the service?Which research methods were used?What were the key findings?How were the findings documented? Where can future team members access the documentation?How often are you testing with real people?Address the whole experience, from start to finishWe need to understand the different ways people will interact with our services, including the actions they take online, through a mobile application, on a phone, or in person. Every encounter — whether it’s online or offline — should move the user closer towards their goal.Understand the different points at which people will interact with the service – both online and in personIdentify pain points in the current way users interact with the service, and prioritize these according to user needsDesign the digital parts of the service so that they are integrated with the offline touch points people use to interact with the serviceDevelop metrics that will measure how well the service is meeting user needs at each step of the serviceWhat are the different ways (both online and offline) that people currently accomplish the task the digital service is designed to help with?Where are user pain points in the current way people accomplish the task?Where does this specific project fit into the larger way people currently obtain the service being offered?What metrics will best indicate how well the service is working for its users?Make it simple and intuitiveUsing a government service shouldn’t be stressful, confusing, or daunting. It’s our job to build services that are simple and intuitive enough that users succeed the first time, unaided.Use the design style guide consistently for related digital servicesGive users clear information about where they are in each step of the processFollow accessibility best practices to ensure all people can use the serviceProvide users with a way to exit and return later to complete the processUse language that is familiar to the user and easy to understandUse language and design consistently throughout the service, including online and offline touch pointsWhat primary tasks are the user trying to accomplish?Is the language as plain and universal as possible?What languages is your service offered in?If a user needs help while using the service, how do they go about getting it?How does the service’s design visually relate to other government services?Build the service using agile and iterative practicesWe should use an incremental, fast-paced style of software development to reduce the risk of failure. We want to get working software into users’ hands as early as possible to give the design and development team opportunities to adjust based on user feedback about the service. A critical capability is being able to automatically test and deploy the service so that new features can be added often and be put into production easily.Ship a functioning “minimum viable product” (MVP) that solves a core user need as soon as possible, no longer than three months from the beginning of the project, using a “beta” or “test” period if neededRun usability tests frequently to see how well the service works and identify improvements that should be madeEnsure the individuals building the service communicate closely using techniques such as launch meetings, strategy rooms, daily standups, and team chat toolsKeep delivery teams small and focused; limit organizational layers that separate these teams from the business ownersRelease features and improvements multiple times each monthCreate a prioritized list of features and bugs, also known as the “feature backlog” and “bug backlog”Use a source code version control systemGive the entire project team access to the issue tracker and version control systemUse code reviews to ensure qualityHow long did it take to ship the MVP? If it hasn’t shipped yet, when will it?How long does it take for a production deployment?How many days or weeks are in each iteration/sprint?Which version control system is being used?How are bugs tracked and tickets issued? What tool is used?How is the feature backlog managed? What tool is used?How often do you review and reprioritize the feature and bug backlog?How do you collect user feedback during development? How is that feedback used to improve the service?At each stage of usability testing, which gaps were identified in addressing user needs?Structure budgets and contracts to support deliveryTo improve our chances of success when contracting out development work, we need to work with experienced budgeting and contracting officers. In cases where we use third parties to help build a service, a well-defined contract can facilitate good development practices like conducting a research and prototyping phase, refining product requirements as the service is built, evaluating open source alternatives, ensuring frequent delivery milestones, and allowing the flexibility to purchase cloud computing resources.The TechFAR Handbook provides a detailed explanation of the flexibilities in the Federal Acquisition Regulation (FAR) that can help agencies implement this play.Budget includes research, discovery, and prototyping activitiesContract is structured to request frequent deliverables, not multi-month milestonesContract is structured to hold vendors accountable to deliverablesContract gives the government delivery team enough flexibility to adjust feature prioritization and delivery schedule as the project evolvesContract ensures open source solutions are evaluated when technology choices are madeContract specifies that software and data generated by third parties remains under our control, and can be reused and released to the public as appropriate and in accordance with the lawContract allows us to use tools, services, and hosting from vendors with a variety of pricing models, including fixed fees and variable models like “pay-for-what-you-use” servicesContract specifies a warranty period where defects uncovered by the public are addressed by the vendor at no additional cost to the governmentContract includes a transition of services period and transition-out planWhat is the scope of the project? What are the key deliverables?What are the milestones? How frequent are they?What are the performance metrics defined in the contract (e.g., response time, system uptime, time period to address priority issues)?Assign one leader and hold that person accountableThere must be a single product owner who has the authority and responsibility to assign tasks and work elements; make business, product, and technical decisions; and be accountable for the success or failure of the overall service. This product owner is ultimately responsible for how well the service meets the needs of its users, which is how a service should be evaluated. The product owner is responsible for ensuring that features are built and managing the feature and bug backlogs.A product owner has been identifiedAll stakeholders agree that the product owner has the authority to assign tasks and make decisions about features and technical implementation detailsThe product owner has a product management background with technical experience to assess alternatives and weigh tradeoffsThe product owner has a work plan that includes budget estimates and identifies funding sourcesThe product owner has a strong relationship with the contracting officerWho is the product owner?What organizational changes have been made to ensure the product owner has sufficient authority over and support for the project?What does it take for the product owner to add or remove a feature from the service?Bring in experienced teamsWe need talented people working in government who have experience creating modern digital services. This includes bringing in seasoned product managers, engineers, and designers. When outside help is needed, our teams should work with contracting officers who understand how to evaluate third-party technical competency so our teams can be paired with contractors who are good at both building and delivering effective digital services. The makeup and experience requirements of the team will vary depending on the scope of the project.Member(s) of the team have experience building popular, high-traffic digital servicesMember(s) of the team have experience designing mobile and web applicationsMember(s) of the team have experience using automated testing frameworksMember(s) of the team have experience with modern development and operations (DevOps) techniques like continuous integration and continuous deploymentMember(s) of the team have experience securing digital servicesA Federal contracting officer is on the internal team if a third party will be used for development workA Federal budget officer is on the internal team or is a partnerThe appropriate privacy, civil liberties, and/or legal advisor for the department or agency is a partnerChoose a modern technology stackThe technology decisions we make need to enable development teams to work efficiently and enable services to scale easily and cost-effectively. Our choices for hosting infrastructure, databases, software frameworks, programming languages and the rest of the technology stack should seek to avoid vendor lock-in and match what successful modern consumer and enterprise software companies would choose today. In particular, digital services teams should consider using open source, cloud-based, and commodity solutions across the technology stack, because of their widespread adoption and support by successful consumer and enterprise technology companies in the private sector.Choose software frameworks that are commonly used by private-sector companies creating similar servicesWhenever possible, ensure that software can be deployed on a variety of commodity hardware typesEnsure that each project has clear, understandable instructions for setting up a local development environment, and that team members can be quickly added or removed from projectsWhat is your development stack and why did you choose it?Which databases are you using and why did you choose them?How long does it take for a new team member to start contributing?Deploy in a flexible hosting environmentOur services should be deployed on flexible infrastructure, where resources can be provisioned in real-time to meet spikes in traffic and user demand. Our digital services are crippled when we host them in data centers that market themselves as “cloud hosting” but require us to manage and maintain hardware directly. This outdated practice wastes time, weakens our disaster recovery plans, and results in significantly higher costs.Resources are provisioned on demandResources scale based on real-time user demandResources are provisioned through an APIResources are available in multiple regionsWe only pay for resources we useStatic assets are served through a content delivery networkApplication is hosted on commodity hardwareWhere is your service hosted?What hardware does your service use to run?What is the demand or usage pattern for your service?What happens to your service when it experiences a surge in traffic or load?How much capacity is available in your hosting environment?How long does it take you to provision a new resource, like an application server?How have you designed your service to scale based on demand?How are you paying for your hosting infrastructure (e.g., by the minute, hourly, daily, monthly, fixed)?Is your service hosted in multiple regions, availability zones, or data centers?In the event of a catastrophic disaster to a datacenter, how long will it take to have the service operational?What would be the impact of a prolonged downtime window?What data redundancy do you have built into the system, and what would be the impact of a catastrophic data loss?How often do you need to contact a person from your hosting provider to get resources or to fix an issue?Automate testing and deploymentsToday, developers write automated scripts that can verify thousands of scenarios in minutes and then deploy updated code into production environments multiple times a day. They use automated performance tests which simulate surges in traffic to identify performance bottlenecks. While manual tests and quality assurance are still necessary, automated tests provide consistent and reliable protection against unintentional regressions, and make it possible for developers to confidently release frequent updates to the service.Create automated tests that verify all user-facing functionalityCreate unit and integration tests to verify modules and componentsRun tests automatically as part of the build processPerform deployments automatically with deployment scripts, continuous delivery services, or similar techniquesConduct load and performance tests at regular intervals, including before public launchWhat percentage of the code base is covered by automated tests?How long does it take to build, test, and deploy a typical bug fix?How long does it take to build, test, and deploy a new feature into production?How frequently are builds created?What test tools are used?Which deployment automation or continuous integration tools are used?What is the estimated maximum number of concurrent users who will want to use the system?How many simultaneous users could the system handle, according to the most recent capacity test?How does the service perform when you exceed the expected target usage volume? Does it degrade gracefully or catastrophically?What is your scaling strategy when demand increases suddenly?Manage security and privacy through reusable processesOur digital services have to protect sensitive information and keep systems secure. This is typically a process of continuous review and improvement which should be built into the development and maintenance of the service. At the start of designing a new service or feature, the team lead should engage the appropriate privacy, security, and legal officer(s) to discuss the type of information collected, how it should be secured, how long it is kept, and how it may be used and shared. The sustained engagement of a privacy specialist helps ensure that personal data is properly managed. In addition, a key process to building a secure service is comprehensively testing and certifying the components in each layer of the technology stack for security vulnerabilities, and then to re-use these same pre-certified components for multiple services.The following checklist provides a starting point, but teams should work closely with their privacy specialist and security engineer to meet the needs of the specific service.Contact the appropriate privacy or legal officer of the department or agency to determine whether a System of Records Notice (SORN), Privacy Impact Assessment, or other review should be conductedDetermine, in consultation with a records officer, what data is collected and why, how it is used or shared, how it is stored and secured, and how long it is keptDetermine, in consultation with a privacy specialist, whether and how users are notified about how personal information is collected and used, including whether a privacy policy is needed and where it should appear, and how users will be notified in the event of a security breachConsider whether the user should be able to access, delete, or remove their information from the service“Pre-certify” the hosting infrastructure used for the project using FedRAMPUse deployment scripts to ensure configuration of production environment remains consistent and controllableDoes the service collect personal information from the user?  How is the user notified of this collection?Does it collect more information than necessary? Could the data be used in ways an average user wouldn’t expect?How does a user access, correct, delete, or remove personal information?Will any of the personal information stored in the system be shared with other services, people, or partners?How and how often is the service tested for security vulnerabilities?How can someone from the public report a security issue?Use data to drive decisionsAt every stage of a project, we should measure how well our service is working for our users. This includes measuring how well a system performs and how people are interacting with it in real-time. Our teams and agency leadership should carefully watch these metrics to find issues and identify which bug fixes and improvements should be prioritized. Along with monitoring tools, a feedback mechanism should be in place for people to report issues directly.Monitor system-level resource utilization in real timeMonitor system performance in real-time (e.g. response time, latency, throughput, and error rates)Ensure monitoring can measure median, 95th percentile, and 98th percentile performanceCreate automated alerts based on this monitoringTrack concurrent users in real-time, and monitor user behaviors in the aggregate to determine how well the service meets user needsPublish metrics internallyPublish metrics externallyUse an experimentation tool that supports multivariate testing in productionWhat are the key metrics for the service?How have these metrics performed over the life of the service?Which system monitoring tools are in place?What is the targeted average response time for your service? What percent of requests take more than 1 second, 2 seconds, 4 seconds, and 8 seconds?What is the average response time and percentile breakdown (percent of requests taking more than 1s, 2s, 4s, and 8s) for the top 10 transactions?What is the volume of each of your service’s top 10 transactions? What is the percentage of transactions started vs. completed?What is your service’s monthly uptime target?What is your service’s monthly uptime percentage, including scheduled maintenance? Excluding scheduled maintenance?How does your team receive automated alerts when incidents occur?How does your team respond to incidents? What is your post-mortem process?Which tools are in place to measure user behavior?What tools or technologies are used for A/B testing?How do you measure customer satisfaction?When we collaborate in the open and publish our data publicly, we can improve Government together. By building services more openly and publishing open data, we simplify the public’s access to government services and information, allow the public to contribute easily, and enable reuse by entrepreneurs, nonprofits, other agencies, and the public.Offer users a mechanism to report bugs and issues, and be responsive to these reportsProvide datasets to the public, in their entirety, through bulk downloads and APIs (application programming interfaces)Ensure that data from the service is explicitly in the public domain, and that rights are waived globally via an international public domain dedication, such as the “Creative Commons Zero” waiverCatalog data in the agency’s enterprise data inventory and add any public datasets to the agency’s public data listingEnsure that we maintain the rights to all data developed by third parties in a manner that is releasable and reusable at no cost to the publicEnsure that we maintain contractual rights to all custom software developed by third parties in a manner that is publishable and reusable at no costWhen appropriate, create a versioned API for third parties and internal users to interact with the service directlyWhen appropriate, publish source code of projects or components onlineWhen appropriate, share your development process and progress publiclyHow are you collecting user feedback for bugs and issues?If there is an API, what capabilities does it provide? Is it versioned? Who uses it? How is it documented?If the codebase has not been released under an open source license, explain why.What components are made available to the public as open source?What datasets are made available to the public?]]></content:encoded></item><item><title>Penn to reduce graduate admissions, rescind acceptances amid research cuts</title><link>https://www.thedp.com/article/2025/02/penn-graduate-student-class-size-cut-trump-funding</link><author>strangeloops85</author><category>hn</category><pubDate>Sun, 23 Feb 2025 00:37:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ask for no, don&apos;t ask for yes (2022)</title><link>https://www.mooreds.com/wordpress/archives/3518</link><author>mooreds</author><category>hn</category><pubDate>Sat, 22 Feb 2025 23:55:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I think it is important to have a bias for action. Like anything else, this is something you can make a habit of. Moving forward allows you to make progress. I don’t know about you, but I’ve frozen up in the past not knowing what the right path was for me. Moving forward, even the smallest possible step, helped break that stasis.One habit I like is to ask for no, not yes. Note that this is based on my experience at small companies (< 200 employees) where a lot of my experience has been. I’m not sure how it’d work in a big company, non-profit, or government.When you have something you want to do and that you feel is in scope for your position, but you want a bit of reassurance or to let the boss know what you are up to, it’s common to reach out and ask them for permission. Don’t. Don’t ask for a yes. Instead, offer a chance to say no, but with a deadline.Let’s see how this works.Suppose I want to set up a new GitHub action that I feel will really improve the quality of our software. This isn’t whimsy, I’ve done some research and tested it locally. I may have even asked a former colleague how they used this GitHub action.But I’m not quite sure. I want to let my boss know that I’ll be modifying the repository.I could say “hey, boss, can we install action X? It’ll help with the XYZ problems we’ve been having.”If you have a busy boss (and most people do), this is going to require a bit of work on their part to say “yes”.They’ll want to review the XYZ problem, think about how X will solve it and maybe do some thinking or prioritization about how this fits in with other work. Or maybe they’ll want you to share what you know. It may fall off their plate. You will probably have to remind them a few times to get around to saying “yes”. It might be a more pressing issue for youNow, let’s take the alternative approach.”Hey, boss, I am going to install action X, which should solve the XYZ problems we’ve been having. Will take care of this on Monday unless I hear differently from you.”Do you see the change in tone?You are saying (without being explicit) that you “got it” and are going to handle this issue. The boss can still weigh in if they want to, but they don’t have to. If they forget about it or other issues pop up, you still proceed. This lets you keep moving forward and solving problems while keeping the boss informed and allowing them to add their two cents if it is important enough.You can also use this approach with a group of people.By the way, the deadline is critical too. Which would you respond to more quickly, if it was Jan 15, all other things being equal and assuming a response was needed?“I’m going to do task X.”“I’m going to do task X on Jan 17.”“I’m going to do task X on Feb 15.”I would respond to the second one, which has a deadline in the near future. I think that is the way most folks work.Again, pursue this approach for problems you feel are in the scope of your role but that you want to inform the boss about. It’s great when you want to  a chance for feedback, but you are confident enough in the course of action that you don’t  feedback.]]></content:encoded></item><item><title>Vine: A programming language based on Interaction Nets</title><link>https://vine.dev/docs/</link><author>todsacerdoti</author><category>hn</category><pubDate>Sat, 22 Feb 2025 22:43:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Gig workers worked more but earned less in 2024: study</title><link>https://www.businessinsider.com/uber-lyft-instacart-gig-workers-saw-earnings-fall-2024-2025-2</link><author>wallflower</author><category>hn</category><pubDate>Sat, 22 Feb 2025 22:22:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Gig workers for Uber, Instacart, and other services made less money for their time in 2024.Even when delivery and ride-hailing drivers made more, their hours rose, too, a new report found.Gig workers have said their jobs have gotten more competitive and less lucrative in recent years.Gig workers for Uber, Instacart, and other services made less money on average in 2024 — even as the number of hours that they worked rose, in some cases.Uber hide-hailing drivers saw their earnings for 2024 fall 3.4% on average to $513 a week, according to a study released Tuesday by data analytics company Gridwise. At the same time, Uber drivers worked 0.8% more hours in 2024.Lyft drivers, meanwhile, worked 5.4% fewer hours in 2024, but saw their pay decline at a faster clip of 13.9% to $318 a week.Workers who shop and deliver orders for Instacart saw their pay for the year decline 8% to $194. Their hours worked fell 4.9%."Drivers are earning less across all of the platforms," Ryan Green, the CEO of Gridwise, told Business Insider.Meantime at DoorDash, gross weekly earnings rose 4.8% to $240 in 2024. Hourly earnings for those on the app fell, though, as the number of hours that gig workers spent on the app rose 5.2%.Amazon Flex workers were in a similar situation. Their earnings soared 18.1% to $413 a week — just as their hours increased 20.4%.Uber Eats workers made $178 a week, or 5.1% more than 2023. Average worker hours on the app rose 2.1%, though.The only app where workers earned significantly more money for the same or less work was Favor, a service owned by Texas supermarket H-E-B that delivers online orders for the chain. There, workers saw their pay rise 3.4% to $155 a week in 2024 as their hours worked fell 13.1%.In response to the report, an Uber spokesperson told BI that its drivers make more than $30 an hour on average.A Lyft spokesperson referred BI to comments that CEO David Risher made this month on the company's earnings call, including that ride-hailing drivers on the app earned a collective $9 billion in 2024. That was "the highest amount of combine driver earnings on our platform ever," Risher said."Several of the claims made in the Gridwise report related to driver earnings and market share are likely based on different methodologies, painting an incomplete picture," a spokesperson from Lyft told BI.An Instacart spokesperson called the report's findings "inaccurate and misleading.""Shopper earnings remain steady across the Instacart platform, and we continue to hear from shoppers that Instacart creates rewarding, flexible earnings opportunities that allow them to earn on their own time and their own terms," the spokesperson said.DoorDash declined to comment. Amazon and Favor did not respond to requests for comment.We want to hear from you. Are you a gig worker? What are the biggest benefits or challenges of gig work that you'd be comfortable sharing with a reporter? Please fill out this Gridwise obtained the data for the report using its own app, which it markets to gig workers to track their earnings and expenses. The company analyzed 171 million trips and $1.9 billion worth of gig worker earnings documented by the app to compile its findings for 2024.The report also found that the average restaurant delivery worker relied on tips for a majority — 53.4% — of their earnings. For grocery delivery workers, 45.7% of earnings came from tips.Tips were much less significant for ride-hailing drivers, Gridwise found. Gratuities made up just 10.4% of earnings, per the report.Gig workers have told BI that claiming good-paying rides and orders on the apps has gotten more competitive. Some workers have even set up their own businesses to offer rides or deliver restaurant food in hopes of making more money than they do on the apps.Consumers, meanwhile, told Gridwise that they plan to keep using ride-hailing and delivery services despite the lingering effects of inflation on many items in Americans' monthly budgets.Majorities of the 1,000 customers surveyed by Gridwise in January said that they thought prices on both ride-hailing apps like Uber and Lyft as well as grocery delivery apps like Instacart were "reasonable.""They talk about being price-sensitive, but their actions reflect differently," Green said.Do you work for Uber, Lyft, DoorDash, Instacart, or another service that uses gig workers and have a story idea to share? Reach out to this reporter at abitter@businessinsider.com]]></content:encoded></item><item><title>OpenBSD Innovations</title><link>https://www.openbsd.org/innovations.html</link><author>angristan</author><category>hn</category><pubDate>Sat, 22 Feb 2025 22:08:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
    This is a list of software and ideas developed or maintained by the OpenBSD
    project, sorted in order of approximate introduction. Some of them are
    explained in detail in our research papers.
ipsec(4):
	Started by John Ioannidis, Angelos D. Keromytis, Niels Provos, and
	Niklas Hallqvist, imported February 20, 1997.  OpenBSD was the first
	free operating system to provide an IPSec stack.
    inet6(4):
	First complete integration and adoption of IPv6 led by
	"Itojun" (Dr. Junichiro Hagino) [WIDE/KAME], Craig Metz [NRL], and
	Angelos D. Keromytis starting Jan 6, 1999.
	Almost fully operational Jun 6, 1999 during the
	first OpenBSD hackathon.
	OpenBSD 2.7.
    :
	Related to the work on privilege separation, some programs were refactored
	to drop privileges while holding onto a tricky resource such as a raw socket,
	reserved port, or modification-locked bpf(4) descriptor,
	for example
	ping(8),
	traceroute(8),
	etc.
    :
	Developed since 2001 as "propolice" by Hiroaki Etoh. Integrated, and
	implemented for additional hardware platforms, by Federico G. Schwindt,
	Miod Vallat and Theo de Raadt.  OpenBSD 3.3 was the first operating
	system to enable it systemwide by default.
    :
	First used for sparc, sparc64, alpha, and hppa in OpenBSD 3.3.
	Strictly enforced by default since OpenBSD 6.0: a program can only
	violate it if the executable is marked with 
	and it is located on a filesystem mounted with the mount(8) option.
     by ld.so:
	first done as part of the W^X work in OpenBSD 3.3, by Dale Rahn and
	Theo de Raadt. The GOT and PLT regions are read-only outside of ld.so
	itself. Extended to the .init/.fini sections (constructors and
	destructors) in OpenBSD 3.4.
    :
	OpenBSD 3.4 was the first widely used operating system to
	provide it by default.
    gcc-local(1)
	__attribute__((__bounded__)) static analysis annotation
	and checking mechanism:
	Started by Anil Madhavapeddy on June 26, 2003
	and ported to GCC 4 by Nicholas Marriott.
	First released with OpenBSD 3.4.
    malloc(3)
	randomization implemented by Thierry Deval. Guard pages and randomized (delayed) free added by Ted Unangst.
	Reimplemented by Otto Moerbeek
	for OpenBSD 4.4.
    Position-independent executables (PIE):
	OpenBSD 5.3 was the first widely used operating system to enable it
	globally by default, on seven hardware platforms.
	Implemented in November 2008 by
	Kurt Miller
	and enabled by default by
	Pascal Stumpf
	in August 2012.
    :
	the ability to specify that a variable should be initialized
	at load time with random byte values (placed into a new ELF
	 section) was implemented in
	OpenBSD 5.3 by Matthew Dempsky.
    Stack protector per shared object:
	using the random-data memory feature, each shared object was given its
	own stack protector cookie in OpenBSD 5.3 by Matthew Dempsky.
    :
	Position-independent static binaries for /bin, /sbin and ramdisks.
	Implemented for OpenBSD 5.7 by Kurt Miller and Mark Kettenis.
    
	(sigreturn(2)
	oriented programming) mitigation: attacks researched by
	Eric Bosman
	and Herbert Bos in 2014, solution implemented by Theo de Raadt in May 2016,
	enabled by default since OpenBSD 6.0.
    Library order randomization:
	In rc(8), re-link
	, , and 
	on startup, placing the objects in a random order.
	Theo de Raadt and Robert Peichaer, May 2016,
	enabled by default since OpenBSD 6.0 and 6.2.
    Kernel-assisted lazy-binding for W^X safety in multi-threaded programs.
	A new syscall kbind(2)
	permits lazy-binding to be W^X safe in multi-threaded programs.
	Implemented for OpenBSD 5.9 by Philip Guenther in July 2015.
    Process layouts in memory tightened to remove execute permission from
	all segmented, non-instruction data and to remove write permission from
	data that is only modified during loading and relocation.
	By combining the RELRO (Read-Only after Relocation) design from the
	GNU project with the original ASLR work from OpenBSD 3.3 and
	strict lazy-binding work from OpenBSD 5.9, this is applied to not
	just a subset of programs and libraries but rather to all programs
	and libraries.
	Implemented for OpenBSD 6.1 by Philip Guenther in August 2016.
    Use of fork+exec in privilege separated programs. The
	strategy is to give each process a fresh & unique address space for
	ASLR, stack protector -- as protection against address space discovery attacks.
	Implemented first by
	Damien Miller (sshd(8) 2004),
	Claudio Jeker (bgpd(8), 2015),
	Eric Faurot (smtpd(8), 2016),
	Rafael Zalamena (various, 2016), and others.
    :
	Reduction of incidental NOP instructions/sequences in the instruction
	stream which could be useful potentially for ROP attack methods to
	inaccurately target gadgets. These NOP sequences are converted into
	trap sequences where possible. Todd Mortimer and Theo de Raadt, June
	2017.
    :
	the .o files of the kernel are relinked in random order from a
	link-kit, before every reboot. This provides substantial interior
	randomization in the kernel's text and data segments for layout and
	relative branches/calls.  Basically a unique address space for each
	kernel boot, similar to the userland fork+exec model described above
	but for the kernel.  Theo de Raadt, June 2017.
    Rearranged i386/amd64 register allocator order in
	clang(1)
	to reduce polymorphic RET instructions:
	Todd Mortimer, November 20, 2017.
    Reencoding of i386/amd64 instruction sequences to avoid
	embedded polymorphic RET instructions.  Enhancements to
	clang(1)
	Todd Mortimer, April 28, 2018 and onwards.
     addition to
	mmap(2)
	allows opportunistic verification that the stack-register
	points at stack memory, therefore catching pivots to non-stack
	memory (sometimes used in ROP attacks).
	Theo de Raadt, April 12, 2018.
     is a replacement for the 
	which uses a per-function random cookie (located in the read-only ELF
	 section) to consistency-check the
	return address on the stack.  Implemented for amd64 and arm64
	by Todd Mortimer in OpenBSD 6.4, for mips64 in OpenBSD 6.7, and
	powerpc/powerpc64 in OpenBSD 6.9.  amd64 system call stubs also
	protected in OpenBSD 7.3.
     addition to
	mmap(2)
	disallows memory pages to be written to core dumps, preventing
	accidental exposure of private information.
	Theo de Raadt, Mark Kettenis and Scott Soule Cheloha,
	February 2, 2019.
    Similar to the opportunistic verification in ,
	system-calls can no longer be performed from PROT_WRITE memory.
	Theo de Raadt, June 2, 2019.
    System calls may only be performed from selected code regions
        (main program, ld.so, libc.so, and sigtramp).  The libc.so region
	is setup by msyscall(2).
	Theo de Raadt, November 28, 2019.
	This mechanism was removed because later work on immutable memory +
	pinned system calls was even better.
    Permissions (RWX, MAP_STACK, etc) on address space regions can be
	made immutable, so that mmap(2),
	mprotect(2) or
	munmap(2) fail with
	EPERM. Most of the program static address space is now automatically
	immutable (main program, ld.so, main stack, load-time shared libraries,
	and dlopen()'d libraries mapped without RTLD_NODELETE).  Programmers
	can request non-immutable static data using the "openbsd.mutable" section,
	or manually bring immutability to (page aligned heap objects) using
	mimmutable(2).
	Theo de Raadt, Dec 4, 2022.
    sshd random relinking at boot. Theo de Raadt. Jan 18, 2023.
    Some architectures now have non-readable code ("xonly"), both from
	the perspective of userland reading its own memory, or the kernel
	trying to read memory in a system call.  Many sloppy practices in
	userland code had to be repaired to allow this.  The linker option
	 is enabled by default. In order of
	development: arm64, riscv64, hppa, amd64,
	powerpc64, powerpc (G5 only), octeon.
	sparc64 (sun4u only, unfinished).
	Mark Kettenis, Theo de Raadt, Visa Hankala, Miod Vallat,
	Dave Voutila, George Koehler in kernel and base, and
	Theo Buehler, Robert Nagy, Christian Weisgerber in ports.
	Dec 2022 - Feb 2023, still ongoing.
    On all architectures which lack hardware-enforcement of xonly,
	system calls are now prevented from reading (via copyin/copyinst)
	inside the program's main text, ld.so text, sigtramp text, or
	libc.so text.
	Theo de Raadt, Jan 2023.
    Architectures which lack xonly mmu-enforcement can still benefit
	from switching to --execute-only binaries if the cpu generates
	different traps for instruction-fetch versus data-fetch.  The
	VM system will not allow memory to be read before it was
	executed which is valuable together with library relinking.
	Architectures switched over include loongson.
	Theo de Raadt, Feb 2023.
    ld.so and crt0 register the location of the
        execve(2)
	libc syscall stub with the kernel using
        pinsyscall(2),
	after which the kernel only accepts an execve call from that
	specific location. Theo de Raadt, Feb 2023. Made redundant by
        pinsyscalls(2)
        which handles all system calls.
    Mandatory enforcement of indirect branch targets (BTI on arm64,
        IBT on Intel amd64), unless a linker flag (-Wl,-z,nobtcfi) requests
        no enforcement.
    The kernel and ld.so register the precise entry location of
	every system call used by a program, as described in the
	new ELF section  inside ld.so and
	libc.so.  ld.so uses the new syscall
        pinsyscalls(2)
        to tell the kernel the precise entry location of system calls in libc.so.
	Since all syscall entries are now known to the kernel, the
	pininsyscall(SYS_execve) interface becomes redundant.
        msyscall(2) mechanism
	also becomes redundant (and is removed a bit later), because immutable
	memory + pinsyscalls together are cheaper and more effective targeting.
	Theo de Raadt, Jan 2024.
     is a clang extension that, upon return from a function
	cleans the return value off the stack (one of many information leaks which
	can be used to determine where functions in a different DSO reside).
	The kernel, libc, libcrypto, and ld.so(1) are compiled with this option.
	amd64 only, for now.
strtonum(3):
	Ted Unangst, Todd Miller, and Theo de Raadt, May 3, 2004, OpenBSD 3.6
    imsg:
	Message passing API, written by Henning Brauer.
	In libutil since May 26, 2010, OpenBSD 4.8;
	used by various daemons before that.
    ohash:
	Written and maintained by Marc Espie.
	In libutil since May 12, 2014, OpenBSD 5.6;
	used by make(1) and m4(1) before that.
    asr:
	Replacement resolver written and maintained by Eric Faurot.
	Imported April 14, 2012; activated on March 26, 2014, OpenBSD 5.6.
    reallocarray(3):
	Theo de Raadt and Ted Unangst, April 22, 2014, OpenBSD 5.6
    getentropy(2):
	Matthew Dempsky and Theo de Raadt, June 13, 2014, OpenBSD 5.6
    pledge(2):
	Theo de Raadt, July 19, 2015, OpenBSD 5.9
    recallocarray(3):
	Otto Moerbeek, Joel Sing and Theo de Raadt, March 6, 2017, OpenBSD 6.1
    freezero(3):
	Otto Moerbeek, April 10, 2017, OpenBSD 6.2
    unveil(2):
	Theo de Raadt and Bob Beck, July 13, 2018, OpenBSD 6.4
    ober:
        ASN.1 basic encoding rules API, written by Claudio Jeker and
        Reyk Flöter, maintained by Rob Pierce and Martijn van Duren;
        started in 2006/07, moved to libutil on May 11, 2019, OpenBSD 6.6
ypserv(8):
	Started by Mats O. Jansson in 1994.
	Imported October 23, 1995 and first released with OpenBSD 2.0.
    mopd(8):
	Started by Mats O. Jansson in 1993.
	Imported September 21, 1996 and first released with OpenBSD 2.0.
    AnonCVS:
	Designed and implemented by Chuck Cranor and Theo de Raadt in 1995
	(paper,
	slides)
    aucat(1):
	Started by Kenneth Stailey.
	Imported January 2, 1997 and first released with OpenBSD 2.1.
	Now maintained by Alexandre Ratchov.
    mg(1):
	Started by Dave Conroy in November 1986.
	Imported February 25, 2000 and first released with OpenBSD 2.7.
	Now maintained by Mark Lumsden.
    m4(1):
	Originally implemented by Ozan Yigit and Richard A. O'Keefe for 4.3BSD-Reno.
	Considerably extended and maintained by Marc Espie since 1999.
    pf(4),
	pfctl(8),
	pflogd(8),
	authpf(8),
	ftp-proxy(8):
	Started by Daniel Hartmeier as a replacement for the non-free ipf by
	Darren Reed. Imported June 24, 2001 and first released with OpenBSD
	3.0. Now maintained by Henning Brauer.
    systrace(4),
	systrace(1):
	Started by Niels Provos.
	Imported June 4, 2002 and first released with OpenBSD 3.2.
	Deleted after OpenBSD 5.9 because
	pledge(2) is even better.
    spamd(8):
	Written by Bob Beck. Imported December 21, 2002 and first released with
	OpenBSD 3.3.
    dc(1):
	Written and maintained by Otto Moerbeek.
	Imported September 19, 2003 and first released with OpenBSD 3.5.
    bc(1):
	Written and maintained by Otto Moerbeek.
	Imported September 25, 2003 and first released with OpenBSD 3.5.
    sensorsd(8):
	Started by Henning Brauer.
	Imported September 24, 2003 and first released with OpenBSD 3.5.
	Reworked by Constantine A. Murenin.
    pkg_add(1):
	Written and maintained by Marc Espie.
	Imported October 16, 2003 and first released with OpenBSD 3.5.
    carp(4):
	Written by Mickey Shalayeff, Markus Friedl, Marco Pfatschbacher,
	and Ryan McBride.
	Imported October 17, 2003 and first released with OpenBSD 3.5.
    OpenBGPD
	including bgpd(8)
	and bgpctl(8):
	Written and maintained by Henning Brauer and Claudio Jeker,
	and also maintained by Peter Hessler.
	Imported December 17, 2003 and first released with OpenBSD 3.5.
    dhclient(8):
	Started by Ted Lemon and Elliot Poger in 1996.
	Imported January 18, 2004 and first released with OpenBSD 3.5.
	Reworked by Henning Brauer.
	Now maintained by Kenneth Westerback.
    dhcpd(8):
	Started by Ted Lemon in 1995.
	Imported April 13, 2004 and first released with OpenBSD 3.6.
	Reworked by Henning Brauer.
	Now maintained by Kenneth Westerback.
    hotplugd(8):
	Started by Alexander Yurchenko.
	Imported May 30, 2004 and first released with OpenBSD 3.6.
    OpenNTPD
	including ntpd(8)
	and ntpctl(8):
	Written and maintained by Henning Brauer.
	Imported May 31, 2004 and first released with OpenBSD 3.6.
	Portable version maintained by Brent Cook.
    dpb(1):
	Started by Nikolay Sturm on August 10, 2004; first available for OpenBSD 3.6.
	Rewritten and maintained by Marc Espie since August 20, 2010.
    ospfd(8),
	ospfctl(8):
	Started by Esben Norby and Claudio Jeker.
	Imported January 28, 2005 and first released with OpenBSD 3.7.
    ifstated(8):
	Started by Marco Pfatschbacher and Ryan McBride.
	Imported January 23, 2004 and first released with OpenBSD 3.8.
    bioctl(8):
	Started by Marco Peereboom.
	Imported March 29, 2005 and first released with OpenBSD 3.8.
    hostapd(8):
	Written by Reyk Flöter.
	Imported May 26, 2005 and first released with OpenBSD 3.8.
    watchdogd(8):
	Started by Marc Balmer.
	Imported August 8, 2005 and first released with OpenBSD 3.8.
    sdiff(1):
	Written by Ray Lai.
	Imported December 27, 2005 and first released with OpenBSD 3.9.
    dvmrpd(8),
	dvmrpctl(8):
	Started by Esben Norby.
	Imported June 1, 2006 and first released with OpenBSD 4.0.
    ripd(8),
	ripctl(8):
	Started by Michele Marchetto.
	Imported October 18, 2006 and first released with OpenBSD 4.1.
    pkg-config(1):
	Started by Chris Kuethe and Marc Espie.
	Imported November 27, 2006 and first released with OpenBSD 4.1.
	Now maintained by Jasper Lievisse Adriaanse.
    relayd(8)
	with relayctl(8):
	Started by Pierre-Yves Ritschard and Reyk Flöter.
	Imported December 16, 2006 and first released with OpenBSD 4.1.
	Now maintained by Sebastian Benoit.ospf6d(8),
	ospf6ctl(8):
	Started by Esben Norby and Claudio Jeker.
	Imported October 8, 2007 and first released with OpenBSD 4.2.
    libtool(1):
	Written by Steven Mestdagh and Marc Espie.
	Imported October 28, 2007 and first available for OpenBSD 4.3.
	Now maintained by Marc Espie, Jasper Lievisse Adriaanse,
	and Antoine Jacoutot.
    snmpd(8):
	Started by Reyk Flöter.
	Imported December 5, 2007 and first released with OpenBSD 4.3.
	Now maintained by Martijn van Duren.
    sysmerge(8):
	Written and maintained by Antoine Jacoutot,
	originally forked from mergemaster by Douglas Barton.
	Imported April 22, 2008, first released with OpenBSD 4.4.
    ypldap(8):
	Started by Pierre-Yves Ritschard.
	Imported June 26, 2008 and first released with OpenBSD 4.4.
    OpenSMTPD
	including smtpd(8),
	smtpctl(8),
	makemap(8):
	Started by Gilles Chehade.
	Imported November 1, 2008 and first released with OpenBSD 4.6.
	Now maintained by Gilles Chehade and Eric Faurot.
    tmux,
	tmux(1):
	Started in 2007 and maintained by Nicholas Marriott.
	Imported June 1, 2009, first released with OpenBSD 4.6.
    ldpd(8),
	ldpctl(8):
	Started by Michele Marchetto.
	Imported June 1, 2009 and first released with OpenBSD 4.6.
	Now maintained by Claudio Jeker.
    ldapd(8),
	ldapctl(8):
	Written by Martin Hedenfalk.
	Imported May 31, 2010 and first released with OpenBSD 4.8.
    OpenIKED
	including iked(8)
	and ikectl(8):
	Started by Reyk Flöter.
	Imported June 3, 2010 and first released with OpenBSD 4.8.
	Now maintained by Tobias Heider.
    iscsid(8),
	iscsictl(8):
	Written and maintained by Claudio Jeker.
	Imported September 24, 2010 and first released with OpenBSD 4.9.
    rc.d(8),
	rc.subr(8):
	Written and maintained by Robert Nagy and Antoine Jacoutot.
	Imported October 26, 2010 and first released with OpenBSD 4.9.
    tftpd(8):
	Written and maintained by David Gwynne.
	Imported March 2, 2012 and first released with OpenBSD 5.2.
    npppd(8),
	npppctl(8):
	Started by Internet Initiative Japan Inc.
	Imported January 11, 2010, first released with OpenBSD 5.3.
	Maintained by YASUOKA Masahiko.
    ldomd(8),
	ldomctl(8):
	Written and maintained by Mark Kettenis.
	Imported October 26, 2012 and first released with OpenBSD 5.3.
    sndiod(8):
	Written and maintained by Alexandre Ratchov.
	Imported November 23, 2012 and first released with OpenBSD 5.3.
    cu(1):
	Written and maintained by Nicholas Marriott.
	Imported July 10, 2012 and first released with OpenBSD 5.4.
    identd(8):
	Written and maintained by David Gwynne.
	Imported March 18, 2013 and first released with OpenBSD 5.4.
    slowcgi(8):
	Written and maintained by Florian Obser.
	Imported May 23, 2013 and first released with OpenBSD 5.4.
    signify(1):
	Written and maintained by Ted Unangst.
	Imported December 31, 2013 and first released with OpenBSD 5.5.
    htpasswd(1):
	Written and maintained by Florian Obser.
	Imported March 17, 2014 and first released with OpenBSD 5.6.
    LibreSSL:
	Started by Ted Unangst, Bob Beck, Joel Sing, Miod Vallat, Philip Guenther,
	and Theo de Raadt on April 13, 2014, as a fork of OpenSSL 1.0.1g.
	First released with OpenBSD 5.6.
	Portable version maintained by Brent Cook.
    httpd(8):
	Started by Reyk Flöter.
	Imported July 12, 2014 and first released with OpenBSD 5.6.
    rcctl(8):
	Written and maintained by Antoine Jacoutot.
	Imported August 19, 2014 and first released with OpenBSD 5.7.
    file(1):
	Rewritten from scratch and maintained by Nicholas Marriott.
	Imported April 24, 2015 and first released with OpenBSD 5.8.
    doas(1):
	Written and maintained by Ted Unangst.
	Imported July 16, 2015 and first released with OpenBSD 5.8.
    radiusd(8):
	Written and maintained by YASUOKA Masahiko.
	Imported July 21, 2015 and first released with OpenBSD 5.8.
    eigrpd(8),
	eigrpctl(8):
	Written and maintained by Renato Westphal.
	Imported October 2, 2015 and first released with OpenBSD 5.9.
    vmm(4),
	vmd(8),
	vmctl(8):
	Written by Mike Larkin and Reyk Flöter.
	Imported November 13, 2015 and first released with OpenBSD 5.9.
    pdisk(8):
	Originally written by Eryk Vershen in 1996-1998,
	rewritten and maintained by Kenneth Westerback since January 11, 2016
	and first released with OpenBSD 5.9.
    mknod(8):
	Original version from Version 6 AT&T UNIX (1975),
	last rewritten by Marc Espie on March 5, 2016
	and first released with OpenBSD 6.0.
    audioctl(1):
	Originally written by Lennart Augustsson in 1997,
	rewritten and maintained by Alexandre Ratchov since June 21, 2016
	and first released with OpenBSD 6.0.
    acme-client(1):
	Written by Kristaps Dzonsons, imported August 31, 2016; released
	with OpenBSD 6.1.
    syspatch(8):
	Written and maintained by Antoine Jacoutot.
	Imported September 5, 2016; released with OpenBSD 6.1.
    ping(8):
	Restructured to include IPv6 functionality and maintained by Florian Obser.
	The separate
	ping6(8)
	was superseded on September 17, 2016,
	and the new, combined version was released with OpenBSD 6.1.
    xenodm(1):
	Cleaned-up fork of
	xdm(1)
	maintained by Matthieu Herrb.
	Imported October 23, 2016; released with OpenBSD 6.1.
    ocspcheck(8):
	Written and maintained by Bob Beck.
	Imported January 24, 2017; released with OpenBSD 6.1.
    slaacd(8):
	Written and maintained by Florian Obser.
	Imported March 18, 2017; released with OpenBSD 6.2.
    rad(8):
	Written and maintained by Florian Obser.
	Imported July 10, 2018; released with OpenBSD 6.4.
    unwind(8):
	Written and maintained by Florian Obser.
	Imported January 23, 2019; released with OpenBSD 6.5.
    openrsync(1):
	Written by Kristaps Dzonsons.
	Imported February 10, 2019; released with OpenBSD 6.5.
    sysupgrade(8):
        Written by Christian Weisgerber, Florian Obser, and Theo de Raadt.
        Imported April 25, 2019; released with OpenBSD 6.6.
    snmp(1):
        Written and maintained by Martijn van Duren.
        Imported August 9, 2019; released with OpenBSD 6.6.
    rpki-client(8):
        Written by Kristaps Dzonsons; maintained by Claudio Jeker,
        Theo Buehler, and Job Snijders.
        Imported June 17, 2019; released with OpenBSD 6.7.
    resolvd(8):
        Written and maintained by Florian Obser and Theo de Raadt.
        Imported February 24, 2021; released with OpenBSD 6.9.
    dhcpleased(8):
        Written and maintained by Florian Obser.
        Imported February 26, 2021; released with OpenBSD 6.9.
Projects maintained by OpenBSD developers outside OpenBSDsudo:
	Started by Bob Coggeshall and Cliff Spencer around 1980.
	Imported November 18, 1999, first released with OpenBSD 2.7.
	Now maintained by Todd Miller.
    femail:
	Written and maintained by Henning Brauer.
	Started in 2005, port available since September 22, 2005.
    midish:
	Written and maintained by Alexandre Ratchov.
	Started in 2003, port available since November 4, 2005.
    fdm:
	Written and maintained by Nicholas Marriott.
	Started in 2006, port available since January 18, 2007.
    toad:
	Written and maintained by Antoine Jacoutot.
	Started in 2013, port available since October 8, 2013.
    docbook2mdoc:
	Started by Kristaps Dzonsons in 2014, maintained by Ingo Schwarze.
	Port available since April 3, 2014.
    portroach:
	Written and maintained by Jasper Lievisse Adriaanse,
	originally forked from FreeBSD's portscout.
	Started in 2014, port available since September 5, 2014.
    cvs2gitdump:
	Written and maintained by YASUOKA Masahiko.
	Started in 2012, port available since August 1, 2016.
    Game of Trees:
	Written and maintained by Stefan Sperling.
	Started in 2017, port available since August 9, 2019.
]]></content:encoded></item><item><title>Kaneo – An open source project management platform</title><link>https://kaneo.app/</link><author>saturn5k</author><category>hn</category><pubDate>Sat, 22 Feb 2025 20:52:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
An open source project management platform focused on simplicity
                and efficiency. Self-host it, customize it, make it yours.

Free and open source forever

Full control over your data
]]></content:encoded></item><item><title>In Defense of Text Labels</title><link>https://www.chrbutler.com/in-defense-of-text-labels</link><author>delaugust</author><category>hn</category><pubDate>Sat, 22 Feb 2025 20:40:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
Why Icons Alone Aren’t Enough

I’m a firm believer in text labels.

Interfaces are over-stuffed with icons. The more icons we have to scan over, the more brain power we put toward making sense of them rather than using the tools they represent. This slows us down, not just once, but over and over again.

While it may feel duplicative to add a text label, the reality is that few icons are self-sufficient in communicating meaning.

The Problems that Icons Create
1. Few icons communicate a clear, singular meaning immediately
It’s easy to say that a  icon will communicate meaning — or that if an icon needs a text label, it’s not doing its job. But that doesn’t take into consideration the burden that icons —  or  — put on people trying to navigate interfaces.

Even the simplest icons can create ambiguity. While a trash can icon reliably communicates “delete,” what about the common pencil icon. Does it mean create? Edit? Write? Draw? Context can help with disambiguation, but not always, and that contextual interpretation requires additional cognitive effort.

When an icon’s meaning isn’t immediately clear, it slows down our orientation within an interface and the use of its features. Each encounter requires a split-second of processing that might seem negligible but accumulates across interactions.
2. The more icons within an interface, the more difficult it can be to navigate.
As feature sets grow, we often resort to increasingly abstract or subtle visual distinctions between icons. What might have worked with 5-7 core functions becomes unmanageable at 15-20 features. Users must differentiate between various forms of creation, sharing, saving, and organizing - all through pictorial representation alone.

The burden of communication increases for each individual icon as an interface’s feature set expands. It becomes increasingly difficult to communicate specific functions with icons alone, especially when distinguishing between similar actions like creating and editing, saving and archiving, or uploading and downloading.
3. Icons function as an interface-specific language within a broader ecosystem.
Interfaces operate within other interfaces. Your application may run within a browser that also runs within an operating system. Users must navigate multiple levels of interface complexity, most of which you cannot control. When creating bespoke icons, you force users to learn a new visual language while still maintaining awareness of established conventions.

This creates particular challenges with standardized icon sets. When we use established systems like Google’s Material Design, an icon that represents one function in our interface might represent something entirely different in another application. This cross-context confusion adds to the cognitive load of icon interpretation.

Why Text Labeling Helps Your Interface
1. Text alone is usually more efficient.
Our brains process familiar words holistically rather than letter-by-letter, making them incredibly efficient information carriers. We’ve spent our lives learning to recognize words instantly, while most app icons require new visual vocabulary.

Scanning text is fundamentally easier than scanning icons. A stacked list of text requires only a one-directional scan (top-to-bottom), while icon grids demand bi-directional scanning (top-to-bottom and left-to-right). This efficiency becomes particularly apparent in mobile interfaces, where similar-looking app icons can create a visually confusing grid.
2. Text can make icons more efficient.
The example above comes from Magnolia, an application I designed. On the left is the side navigation panel without labels. On the right is the same panel with text labels. Magnolia is an extremely niche tool with highly specific features that align with the needs of research and planning teams who develop account briefs. Without the labels, the people who we created Magnolia for would likely find the navigation system confusing.

Adding text labels to icons serves two purposes: it clarifies meaning and provides greater creative freedom. When an icon’s meaning is reinforced by text, users can scan more quickly and confidently. Additionally, designers can focus more on the unity of their interface’s visual language when they’re not relying on icons alone to communicate function.
3. Icons are effective anchors in text-heavy applications.
Above is another example from Magnolia. Notice how the list of options on the right (Export, Regenerate, and History) stands out because of the icons, but the text labels make it immediately clear what these things do.

See, this isn’t an argument for eliminating icons entirely. Icons serve an important role as visual landmarks, helping to differentiate functional areas from content areas. Especially in text-heavy applications, icons help pull the eye toward interactive elements.

The combination of icon and text label creates clearer affordances than either element alone.

Every time we choose between an icon and a text label, we’re making a choice about cognitive load. We’re deciding how much mental energy people will spend interpreting our interfaces rather than using them. While a purely iconic interface might seem simple and more attractive, it often creates an invisible tax on attention and understanding.

The solution, of course, isn’t found in a “perfect” icon, nor in abandoning icons entirely. Icons remain powerful tools for creating visual hierarchy and differentiation. Instead, we need to be more thoughtful about when and how we deploy them. The best interfaces recognize that icons and text aren’t competing approaches but complementary tools that work best in harmony.

This means considering not just the immediate context of our own interfaces, but the broader ecosystem in which they exist. Our applications don’t exist in isolation — they’re part of a complex digital environment where users are constantly switching between different contexts, each with its own visual language.

The next time you’re tempted to create yet another icon, or to remove text labels, remember: the most elegant solution isn’t always the one that  simple — it’s the one that makes communication and understanding  simple.
]]></content:encoded></item><item><title>September 17, 1787: &quot;A Republic, If You Can Keep It&quot;</title><link>https://www.nps.gov/articles/000/constitutionalconvention-september17.htm</link><author>037</author><category>hn</category><pubDate>Sat, 22 Feb 2025 19:58:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Monday, September 17, 1787: The Convention TodayThe day began with a prepared speech from Franklin (PA) who, eighty-one years old and painfully afflicted with gout and kidney stone, was unable to read it himself and delegated that task to Wilson (PA). While the speech was formally addressed to Washington (VA), the Convention’s president, its purpose was to convince the three delegates who had announced their refusal to sign the Constitution—Gerry (MA), Randolph (VA), and Mason (VA)—to abandon their opposition. Franklin began on a note of humility. “I confess that there are several parts of this Constitution which I do not at present approve, but I am not sure I shall never approve them. For having lived long, I have experienced many instances of being obliged by better information, or fuller consideration, to change opinions even on important subjects, which I once thought right, but found to be otherwise. It is therefore that, the older I grow, the more apt I am to doubt my own judgment, and to pay more respect to the judgment of others.” “In these sentiments, Sir, I agree to this Constitution, with all its faults, if they are such; because I think a General Government necessary for us, and there is no form of government, but what may be a blessing to the people if well administered; and believe further, that this is likely to be well administered for a course of years, and can only end in despotism, as other forms have done before it, when the people shall become so corrupted as to need despotic government.”  He didn’t think another Convention (which Mason and Randolph had argued for) would do any better than the first had. He admitted that the men in the room were all well-reasoned and had a diversity of opinions, making it difficult to find common ground. “From such an assembly can a perfect production be expected? It therefore astonishes me, sir, to find this system approaching so near to perfection as it does.... Thus I consent, Sir, to this Constitution, because I expect no better, and because I am not sure, that it is not the best.” “On the whole, Sir, I cannot help expressing a wish that every member of the Convention, who may still have objections to it, would with me, on this occasion, doubt a little of his own infallibility, and to make manifest our unanimity, put his name to this instrument.” Franklin then moved for the form of the signing to be such: “Done in Convention by the unanimous consent of the States present, the seventeenth of September, &c. In witness whereof, we have hereunto subscribed our names.” This form had actually been thought up by Gouverneur Morris (PA), who had given it to Franklin so that Franklin’s esteem would lend it credence. The wording of the form doesn’t explicitly state that the signer is endorsing the Constitution. It only means that the signer is affirming that the states present in the Convention unanimously approved the Constitution. The idea was to get Gerry, Mason, and Randolph to sign by making their personal objections irrelevant to their signatures. Gorham (MA) then motioned for Congress to be given the power to increase the size of the House of Representatives from one representative for every 40,000 people to one for every 30,000. (Mind that Congress would not have been required to increase the House to such a size, but just given the option to do so.) Proposals such as this one had repeatedly failed, but King (MA) and Carroll (MD) seconded him. Now, on this last day of the Convention, Washington (VA) spoke for the only time. While he said it was typically inappropriate for him, as president of the Convention, to offer his opinion, he felt called to support Gorham’s motion. He thought increasing the size of the House of Representatives would increase the “security of the rights and the interests of the people.” After Washington’s speech, no one spoke in opposition to the motion, and it passed unanimously. Jacob Shallus, the scribe who had the day before handwritten the engrossed copy of the Constitution, corrected the text to reflect this final amendment. Randolph gave a brief speech where (much like one from two days earlier) he was almost apologetic about refusing to sign the Constitution, but left open the possibility that he might support the Constitution when Virginia considered ratifying it. He stated, “Nine States [the minimum number for the Constitution to take effect] will fail to ratify the plan, and confusion must ensue.” G. Morris and Williamson (NC) gave speeches encouraging the holdouts to sign. Hamilton (NY) spoke similarly, with Madison (VA) summarizing him thus: “No man’s ideas were more remote from the plan than his own were known to be; but is it possible to deliberate between anarchy and convulsion on one side, and the chance of good to be expected from the plan on the other?” Blount (NC) stated that his signature should not be taken as a sign of his support for the Constitution but just as his affirmation that the Constitution had been unanimously approved by the states at the Convention. Franklin gave a second speech where he personally begged Randolph to sign. Randolph said that Franklin’s proposed form for the signatures didn’t make a difference: signing the Constitution would imply that he supported it, and he didn’t. Madison writes that “He [Randolph] repeated, that, in refusing to sign the Constitution, he took a step which might be the most awful of his life; but it was dictated by his conscience, and it was not possible for him to hesitate, — much less, to change.” Randolph thought that presenting the Constitution to the American people to only accept or reject in total, without amendments, would cause all the “anarchy and civil convulsions” which the soon-to-be signers professed to want to avoid. Gerry gave a speech to the same effect. Charles Cotesworth Pinckney (SC) did not like the ambiguity in Franklin’s proposed form for the signatures. He supported the Constitution and intended his signature to be a sign of that support. Ingersoll (PA) took a middle position: his signature would not indicate his support for the Constitution, but neither would it merely be his attestation to the Convention’s unanimity. His signature would be his “recommendation” that the Convention’s final product, “all things considered, was the most eligible.” Franklin’s motion (related to the form of the signing) passed 10–1, with South Carolina’s vote divided on account of C.C. Pinckney and Butler wanting the form to be more emphatically supportive. The Convention then voted to deposit their official journals (which ended up being much less detailed than Madison’s personal notes) with Washington.  The delegates then proceeded to sign the engrossed copy of the United States Constitution. Thirty-eight men signed thirty-nine names—Dickinson (DE) was ailing with a headache and had asked Read (DE) to sign for him two days earlier. Despite so many personal appeals, Gerry, Randolph, and Mason still refused to sign. Hamilton, as the only New Yorker present at this point, signed in a personal capacity, since New York could not be effectively represented in the Convention by only one delegate. The signatures were grouped by state, with Pennsylvania’s eight being the most. The listing of state names next to the signatures appears to be in the hand of Hamilton. Rhode Island, the only state not to send delegates to the Convention, is not listed. After the signing, the Convention adjourned for a final time. The signatures did not have any legal significance. The Constitution was clear: it would only go into effect when nine of the thirteen states chose to ratify it. As hard as the past four months had been, the real challenge lay ahead, in convincing the American people to embrace the government that these men had authored. As the last names were being signed, Franklin, in a personal aside to some other members, made an observation about the chair that Washington had been sitting in as he presided over the Convention. The chair had an emblem of half of a sun. Franklin noted that artists often have a hard time distinguishing between a rising and a setting sun in their artwork. “I have often and often, in the course of the session, and the vicissitudes of my hopes and fears as to its issue, looked at that behind the President, without being able to tell whether it was rising or setting: but now at length, I have the happiness to know, that it is a rising and not a setting sun.” ]]></content:encoded></item><item><title>Utah Bill Aims to Make Officers Disclose AI-Written Police Reports</title><link>https://www.eff.org/deeplinks/2025/02/utah-bill-aims-make-officers-disclose-ai-written-police-reports</link><author>hn_acker</author><category>hn</category><pubDate>Sat, 22 Feb 2025 19:53:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Birth of Chipzilla</title><link>https://www.abortretry.fail/p/the-birth-of-chipzilla</link><author>rbanffy</author><category>hn</category><pubDate>Sat, 22 Feb 2025 19:04:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I do not expect to join any company which is simply a manufacturer of semiconductors. I would rather try to find some small company which is trying to develop some product or technology which no one has yet done. To stay independent (and small) I might form a new company, after a vacation. just called me on the phone. We'd been friends for a long time. Documents? There was practically nothing. Noyce's reputation was good enough. We put out a page-and-a-half little circular, but I'd raised the money even before people saw it.Intel’s structure in these early days had Rock as Chairman, Noyce as CEO, Moore as Executive VP, Grove as director of engineering, and Vadász as the director of MOS design.Before the company could begin operations, they needed a place to work. Over on Middlefield Road in Mountain View, Union Carbide was vacating a building. They’d really just begun that process, but a conference room was available. Intel rented it. As Intel took over the building, Moore felt the space was larger than what they really needed, but whether he was right or wrong, that wouldn’t be the case for too long. Official operations of Intel began in August of 1968.For Intel’s first few months, the company had quite a bit of work. They needed to build a manufacturing process, figure out their product line, and hire enough people to get products made and sold. This involved them needing to build Fab1 as quickly as possible. The goal was to get things done by the end of December, and this meant that they wouldn’t be building every tool they needed. At Wescon, they actually bought some equipment right off the show floor. A few things had to be adapted to meet their needs, but most of their equipment wasn’t made by them. This wasn’t solely due to the timing constraints either. The founders of Intel had learned from their prior experiences in this industry. While a single company might be able to innovate in some areas, it wasn’t long before competitors replicated that success. Then, those competitors would also be iterating and innovating. A single company had a limited customer base, but the market as a whole was larger and innovations could come from anywhere. They wanted everyone’s expertise, so they bought what they could from whomever had the best tools. Moore worked closely with Intel’s suppliers, and he even made some investments in them personally. While Moore, Noyce, and Rock felt little fear at Intel, this wasn’t true for Grove. At Fairchild, Grove had been strictly R&D. He didn’t really see the rest of the company, and he’d been hired by Moore out of graduate school. At Intel, he had the chance to really see the entire company and its operations. He constantly feared that the company would go bankrupt, but perhaps as a result of that fear, he became quite focused on manufacturing. He wanted the company to be able to do the same things every day to make a uniform product successfully, affordably, and reliably. He quickly began handle much of the company’s operations. The team that was assembled to undertake these early efforts was phenomenal, and Intel met its goal completing the Fab1 setup by the 31st of December.Before Fab1 was even completed, Noyce had visited Sharp in Japan. They spoke Tadashi Sasaki with the hope of gaining (at least some of) the company’s business for ICs. Sasaki then asked Rockwell, who did their manufacturing work, if they could give Intel some of their business, and Rockwell said no; they wanted to enforce their contract’s exclusivity. Sasaki then invested 40 million yen into his friends Yoshio Kojima’s company, Bijikon Kabushiki-gaisha (or Busicom), in Nara, Japan which gave him quite a bit of influence there, and allowed him to work on a CMOS calculator with Intel. Marcian Edward Hoff Jr (or Ted), Intel employee twelve, was the person initially tasked with this project on the Intel side of things. He worked as both an engineer and the liaison between the two companies.In April of 1969, Intel released the 3101 SRAM. The 3101 held 64 bits of data (eight letters of sixteen digits), and sold for $99.50 (about $850 in 2025 dollars). Even at this time, the chip had far too little storage capacity to be useful as main memory (especially with its high price), but it was incredibly fast with an access time of fifty nanoseconds and it dissipated just six milliwatts per bit. With the chip’s high speed, low power draw, and small size, it was quite useful for processor registers in minicomputers and mainframes. The company had been hoping to gain a Honeywell contract with the 3101. That didn’t happen, but the chip was moderately successful. This chip was, in fact, successful enough that Intel’s work with Busicom fell on the back burner.Busicom had initially provided Intel with a twelve chip, PMOS, decimal arithmetic design for their calculator hardware, and by June, Masatoshi Shima had refined this to seven chips. He didn’t have a full schematic, but it was something to work with. Shima then visited Intel in June with two other Busicom engineers. When Shima arrived, he was a bit dismayed. Almost no progress had been made, and he discovered that Intel had some serious expertise in hardware and memory design, but they didn’t have any serious expertise in logic design. Stan Mazor began to act as a buffer between Shima and Hoff, and over three months, there were many meetings about the design, but almost no progress.At this time, Intel was just a memory company. They had packaging to accommodate early memories and nothing more. This limited the design of the chips to 4bit allowing the chip to fit in a 16-pin or 18-pin DIP. These restrictions were not initially given to Busicom, but Hoff knew them, and they influenced what he presented. Unknown to Intel, Sasaki had also come up with a four chip design and gave this to Busicom. With both sides now agreeing to a four chip design, it would seem that things could have progressed, but unknown to Busicom, no more progress would be made that year.In July of 1969, Intel released the 1101. This was first MOS memory chip. The 1101 was slower than the 3101, but had a capacity of 256 bits, used silicon gates, and allowed for far higher memory densities than had previously been possible.In December of 1969, CTC provided Intel the CPU requirements and preliminary designs for a processor they planned to use in their Datapoint 2200 terminal. Intel was then supposed to implement these in LSI. At Intel this project became the 1201, but much like the Busicom project, it wasn’t given any priority. Intel was a memory company. Frustrated, CTC turned to Texas Instruments who also failed, and CTC eventually built the processor out of discreet components.For 1969, Intel ended the year with revenues of $565,874 and a loss for the year of $1,912,833 which meant a loss per share of $1.66. While this sounds terrible, the company had set expectations that this would more or less be the case.Back on the Busicom project, Hoff eventually brought a proposal chip design to Shima that would use a general purpose 4bit CPU with binary logic, a ROM chip, and a RAM chip. Shima then added a 10bit static shift register for use with the printer and keyboard interface of the calculator. They improved the instruction set, the bus, and by December of 1969, Shima was able to write the functional specification. The agreement for Intel to build the chips was officially entered into on the 6th of February in 1970.In April of 1970, Federico Faggin joined Intel after having left Fairchild. He’d previously worked with Vadász at Fairchild, and his skills, especially his having developed silicon gate technology, were well known to the executives at Intel. Their company was essentially based upon Faggin’s work. Given both Faggin’s background and Intel’s needs, his request to work on chip design was immediately accepted. Faggin now had just six months to produce working silicon from a specification, and he didn’t have anyone to help him in the endeavor. Shima returned to Intel just a few days later and was rather unhappy to discover that effectively zero further work had been done on the calculator chips. Faggin started working 12 to 16 hours each day. This was particularly hard for him given he had a month old daughter at home and a loving wife. During this time, his wife went to Italy with her sister as he couldn’t be around to help. I am sure that Faggin somewhat enjoyed the work, but the hours, the pace, the separation from his family, and the deadline were probably dreadful. Adding to this, when Faggin reviewed the specification, he thought it to be a poor design. There was much that he wished to change, but he didn’t have the time. More over, when he asked questions, he was told that this was his project and he simply needed to figure it out. He verified that the architecture was free of errors, but otherwise didn’t change it. He had too much to do. There was still logic design to be done, circuit design, layout, cutting, masking, initial fabrication, testing, fixing any problems found in testing, and then the tasks related to production.PDP-11In December of 1970, the Busicom chips came back for testing. None of them worked. Faggin began checking them with a microscope, and he realized that one of the masks hadn’t been applied. He noted this, and requested another run of chips be made. Those came back three weeks later. In January of 1971, Federico Faggin completed testing of the first single-chip microprocessor ever made. It was his second self-designed computer, but this one was built with multiple technologies he had pioneered. He was just 29 years old. The four chips he created were the 4001, 4002, 4003, and the 4004. The 4001 was a ROM of 256 bytes with a built-in 4bit I/O port. The 4002 was a RAM chip of 40 bytes, which would, in this case, hold 80 4bit words. The 4003 was a 10bit shift register, and in the system for which this chip was designed, it would handle the keyboard and printer. The Intel 4004 was a 4bit microprocessor that ran at 750kHz. It was built of 2300 transistors on a 10 micron process and packaged in a 16-pin DIP. It used 8bit instructions, had 4bit data words, and 12bit addressing. This rather unusual arrangement meant that the 4004 could address 4120 bits of RAM which would mean 1280 4bit characters. It could address 32768 bits of ROM which would mean 4096 8bit words. The 4004 had 46 instructions and 16 registers.Intel’s revenues for 1970 were $3,932,517 but the company posted another loss (though far smaller) of $969,915 which meant a loss of $0.69 per share.1971 was a big year. The Intel employee head count hit 460, the 4004 became available, the 1103 was selling well, and the company released the 1601 in January. The 1601 was the first commercially available PROM built on P-channel silicon gate MOS. The company also began selling fully assembled circuit boards populated with memory chips for mainframes and minicomputers. Intel completed the build out a new headquarters of 78,000 square feet in Santa Clara providing them about three times the space they had previously. This, naturally, allowed them to dramatically increase production. Unlike the previous location, the Santa Clara location had chemical and solvent disposal systems in place from day one preventing this from later becoming a superfund site. In September, Intel released the first EPROM, the 1701. This was an improvement over the 1601 in that it could be reprogrammed. The package had a black plastic cover over the chip. With that cover removed, shining ultra violet light on the chip would erase it and allow it to be programmed again. All of these factors combined allowed Intel to post its first profit at $1,015,080 on revenues of $9,411,821. Share holders saw $0.17 earned on each share.Q1 CorporationOf course, Intel now had Faggin. Immediately after wrapping up the 4004, he was tasked with bringing this new 8bit design into the world, and essentially all of the problems from the 4004 project were present. The design wasn’t great, he had very little time, and he had very little staff. The 1201 became the Intel 8008 which was made available commercially in April of 1972. The 8008 was an 8 bit CPU with a 14 bit address width. It was manufactured on a 10 micron process and built of 3500 transistors. It was packaged in an 18-pin DIP, which resulted in 30 TTL chips being required to interface memory and I/O. This packaging was dictated by Grove and Vadász despite protest against it and meant that the chip did not perform as well as it could have otherwise. The 8008 could achieve around 29,000 operations per second. True to his word, Alroy bought a bunch of 8008s and delivered the world’s first complete, pre-assembled, commercially produced, microcomputer system to the Litcom Division of Litton Industries on Long Island on the 11th of December in 1972.In June of 1973, Intel introduced the Intellec 4 and the Intellec 8 microcomputers at the National Computer Conference in the New York Coliseum. These were sold to software and hardware developers in limited numbers. For the Intellec 8, the price started at $2395. These machines had resident monitors in ROM and they had a PL/M compiler (also a new product). The Intellec series supported a teletype at 110 baud, paper tape, and glass teletypes at 1200 baud. The Intellec 4 shipped with 1K RAM and could support up to 4K instruction RAM. Data memory was just 320 4bit words, and could be expanded to 2560 words. The 8, on the other hand, shipped with 5K RAM and could support up to 16K. These machines were offered either as boards or as fully assembled rack-mountable units with front panel and power supply. The fully assembled systems weighed in at 31 pounds.Shortly after the release of the 8008, Federico Faggin began pushing for a new general purpose processor that wouldn’t have the same limitations as the 8008. Intel had developed a new N-channel MOS process for 4K DRAM, and this is what Faggin wanted to use. He also wanted a new bus architecture, a new interrupt structure, more instructions, and a 40-pin DIP; all while keeping the chip source-code compatible with the 8008. He wanted the instruction cycle time to be 2 microseconds which would put the product within the realm occupied by minicomputers. Approval took time but once he had it, Faggin hired Masatoshi Shima to help with logic design and firmware. Throughout 1973, the company introduced various memory chips, support chips for memories and processors, and even an LCD driver. Revenues for the year were $65,593,000 and profit was $9,214,000 or $2.12 per share.The 4004 and 8008 suggested it, but the 8080 made it realZilogDespite a rough second half of the year, Intel still faced little danger. All of their debts had been paid, they owned their facilities with no mortgages, they owned all of their production equipment, and they required no equity financing having nearly $2 million in cash. In December of 1974, the company announced that Gordon Moore would become president and CEO, Robert Noyce would become chairman of the board, Arthur Rock would become vice chairman, Ed Gelbach would become senior vice president and general manager of the components division, Jack Carsten the vice president of marketing, Gene Flath the vice president of manufacturing, Les Vadász the vice president of engineering, and finally, Andrew Grove would become executive vice president. The company had grown and the restructuring was necessary, and it positioned Intel quite well for the changes and challenges yet to come.I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment.]]></content:encoded></item><item><title>A map of torii around the world</title><link>https://www.google.com/maps/d/viewer?mid=1RNaaTlz7U2FgjlvFARZQWHsMeWsTc2S1&amp;hl=en</link><author>ilamont</author><category>hn</category><pubDate>Sat, 22 Feb 2025 19:01:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Open full screen to view more]]></content:encoded></item><item><title>Amazon now discloses you&apos;re buying a license to view Kindle eBooks</title><link>https://blog.the-ebook-reader.com/2025/02/22/amazon-now-openly-discloses-youre-buying-a-license-to-view-kindle-ebooks/</link><author>DavideNL</author><category>hn</category><pubDate>Sat, 22 Feb 2025 18:50:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Amazon recently changed the wording on their website when it comes to buying Kindle ebooks.As the screenshot above shows, they now have a disclaimer under the buy now button that says, “By placing your order, you’re purchasing a license to the content and you agree to the Kindle Store Terms of Use”.It also says that same thing when shopping for Kindle ebooks directly from the store on Kindle ereaders and Kindle apps.The funny thing is they only appear to be doing that in the US. I checked Amazon UK and Amazon CA and both show the old disclaimer that just says, “By clicking the above button, you agree to the Kindle Store Terms of Use”.If you click the terms of use link you’ll find a page full of legal disclaimers indicating you’re buying a license to access Kindle content, but they don’t openly disclose it on product pages under the buy button like on the Amazon US website.I read somewhere about a new law that was passed in California where companies have to “conspicuously” disclose that customers are buying a license when it comes to digital media like ebooks, so that’s likely the reason why Amazon made the change.But not all ebook stores are following the same path. Kobo still just has a link to their terms of sale page when you go to checkout. Apple doesn’t say anything about licenses at all when trying to buy an ebook from them. Google doesn’t say anything about it on their ebook product pages, but they do say you’re purchasing a license before confirming the purchase, with a link to their terms of service.Some people still don’t know that when buying digital content you’re buying a license to view said content, not the actual content itself. You don’t actually “own” the ebooks you purchase; you just own the rights to view them. It’s a distinction that applies to digital media since you can’t physically own it. I think it’s a good idea for companies to openly disclose that fact before buying. Nobody is going to read through those ridiculous TOS pages before purchasing something.]]></content:encoded></item><item><title>Ask HN: Is anyone still using Dreamweaver?</title><link>https://news.ycombinator.com/item?id=43141368</link><author>gillytech</author><category>hn</category><pubDate>Sat, 22 Feb 2025 17:59:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[When I was learning to build websites in 2010 Dreamweaver was the go-to. I remember it thoroughly confused the heck out of me. Anyone here able to use it effectively?]]></content:encoded></item><item><title>Private antitrust cases are going through the courts</title><link>https://www.thebignewsletter.com/p/the-people-take-antitrust-into-their</link><author>toomuchtodo</author><category>hn</category><pubDate>Sat, 22 Feb 2025 17:30:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The $1.5B Bybit Hack</title><link>https://blog.trailofbits.com/2025/02/21/the-1.5b-bybit-hack-the-era-of-operational-security-failures-has-arrived/</link><author>todsacerdoti</author><category>hn</category><pubDate>Sat, 22 Feb 2025 17:05:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Two weeks ago at the DeFi Security Summit, Trail of Bits’ Josselin Feist (@Montyly) was asked if we’d see a billion-dollar exploit in 2025. His response: “If it happens, it won’t be a smart contract, it’ll be an operational security issue.”Today, that prediction was validated.On February 21, 2025, cryptocurrency exchange Bybit suffered the largest cryptocurrency theft in history when attackers stole approximately $1.5B from their multisig cold storage wallet. At this time, it appears the attackers compromised multiple signers’ devices, manipulated what signers saw in their wallet interface, and collected the required signatures while the signers believed they were conducting routine transactions.This hack is one of many that represent a dramatic shift in how centralized exchanges are compromised. For years, the industry has focused on hardening code and improving their technical security practices, but as the ecosystem’s secure development life cycle has matured, attackers have shifted to targeting the human and operational elements of cryptocurrency exchanges and other organizations.These attacks reveal an escalating pattern, with each compromise building on the last:In each case, the attackers didn’t exploit smart contract or application-level vulnerabilities. Instead, they compromised the computers used to manage those systems using sophisticated malware to manipulate what users saw versus what they actually signed.The DPRK’s Cryptocurrency Theft InfrastructureThe attack chain typically begins with aggressive social engineering campaigns targeting multiple employees simultaneously within an organization. The RGB identifies key personnel in system administration, software development, and treasury roles, then creates detailed pretexts - often elaborate job recruitment schemes - customized to each target’s background and interests. These aren’t mass phishing campaigns; they’re meticulously crafted approaches designed to compromise specific individuals with access to critical systems.What makes these attacks particularly concerning is their repeatability. The RGB has built a sophisticated cross-platform toolkit that can:Operate seamlessly across Windows, MacOS, and various wallet interfacesShow minimal signs of compromise while maintaining persistenceFunction as backdoors to execute arbitrary commandsDownload and execute additional malicious payloadsManipulate what users see in their interfacesEach successful compromise has allowed the RGB to refine their tools and techniques. They’re not starting from scratch with each target - they’re executing a tested playbook that’s specifically engineered to defeat standard cryptocurrency security controls when those controls are used in isolation.Organizations below a certain security threshold are now at serious risk. Without comprehensive security controls including:Air-gapped signing systemsMultiple layers of transaction verificationEndpoint detection and response (EDR) systems like CrowdStrike or SentinelOneRegular security training and war gamesThey are likely to face an adversary that has already built and tested the exact tools needed to defeat their existing protections.The New Reality of Cryptocurrency SecurityThis attack highlights a fundamental truth: no single security control, no matter how robust, can protect against sophisticated attackers targeting operational security. While secure code remains crucial, it must be part of a comprehensive security strategy.Organizations must adopt new processes and controls that operate under the assumption that their infrastructure will eventually face compromise:Infrastructure Segmentation: Critical operations like transaction signing require both physical and logical separation from day-to-day business operations. This isolation ensures that a breach of corporate systems cannot directly impact signing infrastructure. Critical operations should use dedicated hardware, separate networks, and strictly controlled access protocols. Security controls must work in concert - hardware wallets, multi-signature schemes, and transaction verification tools each provide important protections, but true security emerges from their coordinated operation. Organizations need multiple, overlapping controls that can detect and prevent sophisticated attacks.Organizational Preparedness: Technical controls must be supported by comprehensive security programs that include:Thorough threat modeling incorporating both technical and operational risksRegular third-party security assessments of infrastructure and proceduresWell-documented and frequently tested incident response plansOngoing security awareness training tailored to specific rolesWar games and attack simulations that test both systems and personnelThese principles aren’t new - they represent hard-won lessons from years of security incidents in both traditional finance and cryptocurrency. Trail of Bits has consistently advocated for this comprehensive approach to security, providing concrete guidance through several key publications:These publications show a clear pattern that’s echoed by recent attacks: sophisticated attackers are increasingly targeting operational security vulnerabilities rather than technical flaws.The cryptocurrency industry’s resistance to implementing traditional corporate security controls, combined with the high value of potential targets and this group’s sophisticated capabilities, suggests these attacks are likely to continue unless significant changes are made to how cryptocurrency companies approach operational security.The Bybit hack marks a new era in cryptocurrency security. Industry participants need to recognize the evolving threat landscape and invest additional resources in improving their operational security. No one understands this reality better than the security researchers who have been tracking these attacks.Tay @tayvano_, a renowned security researcher known for exposing on-chain thefts, dissecting DPRK crypto hacks, and fiercely advocating for better blockchain security practices, summarized the current reality bluntly:For all these reasons and more, it’s my opinion that once they get on your device, you’re fucked. The end.
If your keys are hot or in AWS, they fuck you immediately.
If they aren’t, they work slightly harder to fuck you.
But no matter what, you’re going to get fucked.Organizations must protect themselves through a comprehensive defense strategy combining isolation, verification, detection, and robust operational security controls. However, the time for basic security measures has passed. Organizations holding significant cryptocurrency assets must take immediate action:Implement dedicated, air-gapped signing infrastructureEngage with security teams experienced in defending against sophisticated state actorsBuild and regularly test incident response plansThe next billion-dollar hack isn’t a matter of if, but when. The only question is: will your organization be ready?]]></content:encoded></item><item><title>Exult: Recreating Ultima VII for modern operating systems</title><link>https://exult.sourceforge.io/index.php</link><author>nateb2022</author><category>hn</category><pubDate>Sat, 22 Feb 2025 16:56:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Ultima VII turns 30 - Exult releases version 1.830 years ago Origin released Ultima VII - The Black Gate and many still
	hold this game in a very high regard. We certainly do!
	Instead of waiting another decade this is the perfect opportunity to release
	our next version. Because of the measures we took to eliminate weird crashes
	it was important to make a new release available.
	By default we will now play the extended intro of Serpent Isle, that we restored
	from an old recording of it. It was cut and slimmed down for the release of
	Ultima VII's Part 2 to conserve precious floppy disk space. A shout out to
	Denis Loubet for creating this masterpiece.
	Two important things were always missing from Exult in regards to the Black
	Gate:
	The sound effects in the intro and the congratulations screen! Both are now
	present!
	Currently we are in the process of merging Ken Cecka's fork of Exult for
	 (*). While this is not yet finished, Ken created a 1.8 version
	based on our release code.

	You can find the Android, Windows and macOS releases on our
	download page (**).
	Please play with it and tell us of any problem you encounter using the
	forum or
	bug tracker.A brief list of the most important changes in Exult since v1.6:Windows builds are by default 64bitExult Studio modernized by porting it to GTK+ 3 (thanks Dragon Baroque)Exult Studio now offered as a download for macOSExult shows the proper ending screen for BG (you have beaten Ultima VII in nn days...)Sound effects in the BG intro were missingExtended intro for SI implementedMore crashes eliminated caused by cached out objects"Gumps pause game" no longer delays usecode eventsSmooth virtual joystick movement on iOSiOS shows the mouse cursor when a real mouse device is usedScreenshots will now be saved in the PNG formatStatus bars can now also be vertical on both sides of the screenUpdated Windows installer to download and install the audio packs and our modsOur options have been reorderedBut we do have known issues:Antimagic rain caused by the cube generator is still not dissipating as fast as it shouldExult does not return to the game menu after beating the game (instead it quits)Some schedules need more detailed loveSeveral bugs that need more in-depth looking at but no plot-stopping bugs(**) All files have been scanned and cleared with
	Virus Total's online scanner, however Avast
	falsely flags our InnoSetup based installer for Windows.]]></content:encoded></item><item><title>FFmpeg School of Assembly Language</title><link>https://github.com/FFmpeg/asm-lessons/blob/main/lesson_01/index.md</link><author>davikr</author><category>hn</category><pubDate>Sat, 22 Feb 2025 16:52:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Recovering priceless audio and lost languages from old decaying tapes</title><link>https://theconversation.com/how-were-recovering-priceless-audio-and-lost-languages-from-old-decaying-tapes-248116</link><author>PaulHoule</author><category>hn</category><pubDate>Sat, 22 Feb 2025 16:45:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Remember cassettes? If you’re old enough, you might remember dropping one into a player, only to have it screech at you when you pressed “play”. We’ve fixed that problem. But why would we bother?Before the iPod came along, people recorded their favourite tunes straight from the radio. Some of us made home recordings with our sibling and grandparents – precious childhood snippets. And a few of us even have recordings from that time we travelled to a village in Vanuatu, some 40 years ago, and heard the locals performing in a language that no longer exists.In the field of linguistics, such recordings are beyond priceless – yet often out of reach, due to the degradation of old cassettes over time. With a new tool, we are able to repair those tapes, and in doing so can recover the stories, songs and memories they hold. A digital humanities telescopeOur digital archive, PARADISEC (Pacific and Regional Archive for Digital Sources in Endangered Cultures) contains thousands of hours of audio – mainly from musicological or linguistic fieldwork. This audio represents some 1,360 languages, with a major focus on languages of the Pacific and Papua New Guinea. The PARADISEC research project was started in 2003 as a collaboration between the universities of Melbourne and Sydney, and the Australian National University. Like a humanities telescope, PARADISEC allows us to learn more about the language diversity around us, as we explained in a 2016 Conversation article.While many of the tapes we get are in good condition and can be readily played and digitised, others need special care, and the removal of mould and dirt.We work with colleagues at agencies such as the Solomon Islands National Museum, for whom we recently repaired a set of cassettes that were previously unplayable and just screeched. We’ll be taking those cassettes, now repaired and digitised, back to Honiara in February and expect to pick up more for further treatment. Screeching happens when a tape is dried out and can’t move through the mechanism easily. The screeching covers the audio signal we want to capture. In 2019, my colleague Sam King built (with the help of his colleague Doug Smith) a cassette-lubricating machine while working at the Australian Institute of Aboriginal and Torres Strait Islander Studies. This machine – likely the first of its kind in Australia – allowed us to play many previously unplayable tapes.Last year, Sam built two versions of an updated machine called the LM-3032 Tape Restorator for PARADISEC, improving on the previous model. Between hand building some parts, 3D printing others and writing code for the controllers, it took him more than a year.Preserving culture and heritageThe LM-3032 Tape Restorator works by applying cyclomethicone (a silicone-based solvent used in cosmetics) to the length of a tape. This leaves behind an extremely thin film of lubrication that allows smoother playback, making digitisation possible. See more details here.Tests have shown this process has no negative long-term effects on the tape. In fact, tapes treated with this method five years ago still play without issues. This technological wizardry allows us to salvage precious analogue recordings before it’s too late. For many languages, these may be the only known recordings – stored on a single cassette, in a single location, and virtually inaccessible. Some of the primary research records digitised by PARADISEC have survived long periods of neglect in offices, garages and attics.The audio below is from a tape that was kept at Fitzroy Crossing in the Kimberley for 40 years. It features beautiful singing in the local Walmajarri language, with guitar accompaniment. The first seven seconds are from the untreated tape, while the rest is from the treated version.
        Singing in Walmajarri, with guitar accompaniment. A side-by-side comparison of a tape treated with the LM-3032 Tape Restorator.
        Our experience has shown community members truly value finding records in their own languages, and we’re committed to making this process easier for them.Here’s one testimonial from E’ava Geita, Papua New Guinea’s current acting Solicitor General. In 2015, Geita was overjoyed to hear digitised records capturing PNG’s Koita language:If only you witnessed and captured the reaction in me going through the recordings at home! It is quite an amazing experience! From feeling of awe to emotion to deep excitement! The feeling of knowing that your language has been documented or recorded in a structured way, kept safely somewhere in the world, hearing it spoken 50–60 years ago and by some people you haven’t seen but whose names you only hear in history is quite incredible. It is most heartwarming to know that it is possible to sustain the life of my language. Thank you once again for the opportunity to listen to the records. Acknowlegement: I’d like to thank Sam King for the technical information provided in this article.]]></content:encoded></item><item><title>Discover the IndieWeb, one blog post at a time</title><link>https://indieblog.page/</link><author>vinhnx</author><category>hn</category><pubDate>Sat, 22 Feb 2025 15:42:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
        This website lets you randomly explore the IndieWeb. Simply click the button below and you
        will be redirected to a random post from a personal blog.
    
        Disclaimer: the content linked to is aggregated automatically. I neither endorse nor necessarily agree with
        the linked sites. Use at your own risk.
     you can drag the button to your bookmarks and have it always available when you want to be
        inspired.
    ]]></content:encoded></item><item><title>Show HN: I Built a Visual Workflow Automation Platform – FlowRipple</title><link>https://flowripple.com/</link><author>shivsarthak34</author><category>dev</category><category>hn</category><pubDate>Sat, 22 Feb 2025 14:20:04 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Watch how easy it is to build powerful automation workflows using our visual builder. Drag, drop, and connect nodes to create your perfect workflow.]]></content:encoded></item><item><title>Florida insurers steered money to investors while claiming losses, study says</title><link>https://www.tampabay.com/news/florida-politics/2025/02/22/florida-insurance-profits-desantis-regulation-investors-crisis/</link><author>howard941</author><category>hn</category><pubDate>Sat, 22 Feb 2025 13:25:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[TALLAHASSEE — While Florida insurers claimed to be losing money in the wake of hurricanes Irma and Michael, their parent companies and affiliates were making billions of dollars, according to a study obtained by the Times/Herald.The start of the state’s insurance market meltdown came on the heels of those two storms between 2017 and 2019, as companies justified big rate increases to cover their losses.But those financial hardships don’t tell the full story, according to the 2022 study that has never been made public and was released to the Times/Herald after a two-year wait for public records.The report, the most in-depth dive into the byzantine finances of Florida’s homeowners insurance market, reveals that as the industry was ailing and companies were losing money, executives distributed $680 million in dividends to shareholders while diverting billions more to affiliate companies.Executives with most Florida-based insurers were removing so much money fromtheir companies that they violated state regulations, the study’s author concluded.The result left some insurers financially weaker — and potentially unable to pay claims — heading into the depths of the state’s insurance crisis.State lawmakers never saw the report.The state’s then-insurance commissioner and Gov. Ron DeSantis focused on legal reforms making it harder to sue insurers.“These companies are crying poverty in order to raise premiums or justify insolvency: ‘It’s litigation, it’s fraud,‘” Quinn said. “This is money shifting from their left pocket to the right, and crying poverty while their right pocket bulges.”While the report is an incomplete picture of insurers' money,Florida’s Office of Insurance Regulation said in a statement, the study affirms that the reforms the office wants are warranted.ButPaul Handerhan, founder of the trade group Federal Association for Insurance Reform, whose members include insurance companies,disputed the idea that executives were deliberately moving money around.“This notion that they’re fleecing their policyholders and offshoring the money to their affiliates is just not happening,” Handerhan said. “None of these guys did this as a strategy.”During debates in the Legislature over how best to respond to the insurance crisis between 2018 and 2023, some lawmakers asked what role affiliate companies played.Rep. Hillary Cassel, R-Dania Beach, said lawmakers and observers had a lack of data about affiliates, known in the industry as “managing general agents,” when they were voting on legislation.“All of us informed on the issues knew (managing general agents) were a problem,” said Cassel, a former lawyer for insurance companies who now sues them.The Office of Insurance Regulation said in a statement that the study was not given to lawmakers because it was “not a formal examination report.”It was produced months before lawmakers met in emergency legislative sessions in 2022 and left in a “draft” status.“Our office does not release every internal analysis of companies to the Legislature,” the office said.The Times/Herald requested the report in November 2022, but the office did not turn over the executive summary until December 2024.The affiliate structure is nothing new in Florida.Profits of insurance companies are limited by regulators to about 4.5% — hardly enticing to investors, considering the risk of hurricanes.However, insurance executives in Florida have used financial workarounds to reward investors and themselves.While the profits and executive compensation of the insurance company are capped, the profits of affiliate and parent companies are not.So executives create sister companies that charge the insurance company for basic services, such as claims handling, underwriting, accounting and issuing policies. (Large national insurers typically handle all of those services internally.)Arrangements between insurance companies and affiliates must be approved by the state, and regulations say they must be “fair and reasonable,” which isn’t defined in state law.When Southern Fidelity Insurance dissolved that year, it was one of six companies under the same umbrella, and its holdings included a hunting lodge maintained at a cost of $485,000 per year. State officials were investigating whether the company “took active measures to conceal these costs” from regulators, according to an October insolvency report.That’s because the owner of the insurance company also owns the affiliates, creating an incentive for executives to overcharge the insurance company for services.Still,it’s unlikely Floridaregulators knew the full scope of insurance money-shifting arrangements until 2021, when lawmakers gave them the ability to demand more information from insurers and affiliates.Using that power,the state’s then-commissioner, David Altmaier, paid a Connecticut-based consultantnearly $150,000to parse through the information the companies provided. Several companies turned over incomplete data, according to the study.Between 2017 and 2019, the insurers in the study (minus a couple of outliers) showed a net loss of $432 million.Their affiliate companies showed a net income of $1.8 billion.With all 53 companies included, the industry recorded $61 million in net income, and affiliates made about $14 billion in net income, according to the study. Those figures likely include national carriers that also provide auto insurance.The author noted that the affiliates of Florida-based companies were profitable even after they injected$485 million back into the insurers andwaived $208 million in fees during the three years, steps made to help keep the insurers afloat.In the author’s opinion, 19 of the 30 Florida-based companies that provided data were paying feesto their affiliate companies that were “not fair and reasonable.”The numbers in the study are “eye-popping” and raise questions about why regulators would allow such financial arrangements, said Birny Birnbaum, executive director of the Center for Economic Justice and a former chief economist at the Texas Department of Insurance.“It’s unclear why (the Office of Insurance Regulation) isn’t doing anything about it,” Birnbaum said.Regulators this year are asking lawmakers to define “fair and reasonable” to include the actual cost of the service provided, the overall health of the insurer and how much in dividends were paid out. Regulators asked for that in 2023 but lawmakers rejected it, claiming it would “upset the apple cart” of Florida’s insurance industry.The office’s proposed legislation would also require fees to affiliates be paid in dollar amounts, instead of percentages. Affiliates will typically charge the insurance company fees of between 20% and 34% of premiums, which results in more money for the affiliates when premiums go up.The office has canceled or modified some companies’ agreements. In 2023, for example, one company’s contract with an affiliate was canceled after regulators discovered that the affiliate was charging the insurer additional fees on top of the cost of the services being provided, according to the office.]]></content:encoded></item><item><title>DOGE&apos;s only public ledger is riddled with mistakes</title><link>https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html</link><author>belter</author><category>hn</category><pubDate>Sat, 22 Feb 2025 12:03:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Elon Musk and his Department of Government Efficiency say they have saved the federal government $55 billion through staff reductions, lease cancellations and a long list of terminated contracts published online this week as a “wall of receipts.”President Trump has been celebrating the published savings, even musing about a proposal to mail checks to all Americans to reimburse them with a “DOGE dividend.”But the math that could back up those checks is marred with accounting errors, incorrect assumptions, outdated data and other mistakes, according to a New York Times analysis of all the contracts listed. While the DOGE team has surely cut some number of billions of dollars, its slapdash accounting adds to a pattern of recklessness by the group, which has recently gained access to sensitive government payment systems.Some contracts the group claims credit for were double- or triple-counted. Another initially contained an error that inflated the totals by billions of dollars. In at least one instance, the group claimed an entire contract had been canceled when only part of the work had been halted. In others, contracts the group said it had closed were actually ended under the Biden administration.]]></content:encoded></item><item><title>Binned staff, slashed stock options. What&apos;s next? Ah yes, bigger C-suite bonuses</title><link>https://www.theregister.com/2025/02/22/meta_pumps_executive_bonuses/</link><author>rntn</author><category>hn</category><pubDate>Sat, 22 Feb 2025 11:55:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[After another round of mass layoffs and reports of slashed stock options for remaining employees, Meta has like clockwork opted to reward its top executives with a substantial bonus increase.The Facebook giant revealed in a government filing that its Compensation, Nominating and Governance Committee (CNGC) approved a target annual bonus increase for its top executive officers bar CEO Mark Zuckerberg. The bonus was raised from 75 percent of base salary to a whopping 200 percent, effective with the 2025 annual performance period."Following this increase, the target total cash compensation for the named executive officers (other than the CEO) falls at approximately the 50th percentile of the Peer Group Target Cash Compensation," the filing notes. It added that prior to the adjustment, those executives were only at or below the 15th percentile in total cash compensation compared to their peers. According to Meta's April 2024 proxy statement [PDF], CTO Andrew Bosworth's base salary was $945,000. His actual eligible earnings were slightly lower due to the timing of his raise. However, factoring in a 75 percent target bonus and Meta's 150 percent company performance multiplier for 2023, his total bonus payout amounted to about $1.05 million.Assuming Bosworth's salary remains the same, and Meta's company performance percentage stays at 150 percent in 2025, the new 200 percent target bonus would push his bonus to nearly $3 million. That's before any stock-based compensation and other add-ons. And he's not even the highest-paid member of Meta's named executive team.For balance's sake, and some might find this hard to swallow but, $3 million annual cash compensation for a CTO in Bosworth's position is about right for Silicon Valley; it's nothing outrageous, relatively speaking. The vast majority of his pay package is in shares; in 2023 for instance, he was awarded more than $20 million in stock. The salary, like for many in his role, is the cherry on top of an enormous cake.Some of that bonus cash, though, might be coming from Meta's latest round of layoffs, which saw around 3,700 people - about five percent of its workforce - axed this month. The cut reportedly targeted low performers, and followed a year in which the biz reported a net income of $62.36 billion, a 59 percent year-over-year increase. This comes reports surfaced this week that Meta has cut back on its yearly distribution of stock options by 10 percent to most staff, though we do note that the corp's share price has climbed 10 percent in the past month, and 46 percent for the past year.We've reached out to Meta to confirm reports of the stock option cut. Zuckerberg started 2025 by describing Meta's plans for the year as "intense," with massive AI investments lined up as the Facebook maker shifts focus beyond its struggling metaverse ambitions. Following OpenAI's announcement that it and its partners plan to invest up to $500 billion in an AI infrastructure project dubbed Stargate, Zuckerberg announced his own AI spending plans - Meta would pour $60 billion or more to expand its AI infrastructure in 2025. "This is a massive effort, and over the coming years it will drive our core products and business, unlock historic innovation, and extend American technology leadership," Zuckerberg said of the effort. Hopefully it will prove more successful than that Metaverse shift for the sake of those executives' bonuses. ®]]></content:encoded></item><item><title>Do you want to be doing this when you&apos;re 50? (2012)</title><link>https://prog21.dadgum.com/154.html</link><author>debesyla</author><category>hn</category><pubDate>Sat, 22 Feb 2025 11:55:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[When I was still a professional programmer, my office-mate once asked out of the blue, "Do you really want to be doing this kind of work when you're fifty?"I have to say that made me stop and think.To me, there's an innate frustration in programming. It doesn't stem from having to work out the solutions to difficult problems. That takes careful thought, but it's the same kind of thought a novelist uses to organize a story or to write dialog that rings true. That kind of problem-solving is satisfying, even fun.But that, unfortunately, is not what most programming is about. It's about trying to come up with a working solution in a problem domain that you don't fully understand and don't have time to understand.It's about skimming great oceans of APIs that you could spend years studying and learning, but the market will have moved on by then and that's no fun anyway, so you cut and paste from examples and manage to get by without a full picture of the architecture supporting your app.It's about reading between the lines of documentation and guessing at how edge cases are handled and whether or not your assumptions will still hold true two months or two years from now.It's about the constant evolutionary changes that occur in the language definition, the compiler, the libraries, the application framework, and the underlying operating system, that all snowball together and keep you in maintenance mode instead of making real improvements. It's about getting derailed by hairline fractures in otherwise reliable tools, and apparently being the first person to discover that a PNG image with four bits-per-pixel and an alpha channel crashes the decoder, then having to work around that.One approach is to dig in and power through all the obstacles. If you're fresh out of school, there are free Starbucks lattes down the hall, and all your friends are still at the office at 2 AM, too...well, that works. But then you have to do it again. And again. It's always a last second skid at 120 miles per hour with brakes smoking and tires shredding that makes all the difference between success and failure, but you pulled off another miracle and survived to do it again.I still like to build things, and if there's no one else to do it, then I'll do it myself. I keep improving the the tiny Perl script that puts together this site, because that tiny Perl script is unobtrusive and reliable and lets me focus on writing. I have a handy little image compositing tool that's less than 28 kilobytes of C and Erlang source. I know how it works inside and out, and I can make changes to it in less time than than it takes to coax what I want out of ImageMagick.But large scale, high stress coding? I may have to admit that's a young man's game.]]></content:encoded></item><item><title>The Internet&apos;s longest-serving PC email system, still being updated</title><link>https://www.pmail.com/</link><author>francescovaglia</author><category>hn</category><pubDate>Sat, 22 Feb 2025 10:54:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Welcome to the home of ,
       the Internet's longest-serving PC e-mail system, and of the Mercury Mail
       Transport System, our full-featured Internet Mail Server.
       Pegasus Mail is a free product, dedicated to serving all who need it, whilst
       Mercury is a modestly-priced semi-commercial system that allows free use for
       private and non-profit users.]]></content:encoded></item><item><title>&apos;The tyranny of apps&apos;: those without smartphones are unfairly penalised</title><link>https://www.theguardian.com/money/2025/feb/22/the-tyranny-of-apps-those-without-smartphones-are-unfairly-penalised-say-campaigners</link><author>zeristor</author><category>hn</category><pubDate>Sat, 22 Feb 2025 09:18:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Michael is in his late 50s and is among the millions of people in the UK who cannot or do not want to use mobile apps, and feels he is being penalised for his choice.He does own a smartphone – an Apple iPhone he bought secondhand about three years ago – but says: “I don’t use apps at all. I don’t download them for security reasons.”Apps have burrowed their way into seemingly every aspect of our lives and there are lots of reasons why companies are pushing us to use them. With an app, it is often “one click and you’re in”, rather than having to faff around online finding the website and remembering passwords. It is also for the “push notifications” that mobile apps send to grab our attention and get us to buy stuff. Many tech experts also argue that apps are generally more secure than websites and allow banks and others to carry out sophisticated ID verification using face, voice and fingerprint biometrics.But millions of people who cannot afford a smartphone or have an older device that does not support some services are increasingly being locked out of deals, discounts and even some vital services, say digital exclusion and pro-cash campaigners.They are missing out on everything from savings on their weekly shop, to some of the best interest rates for their cash. And not signing up to the app revolution is making activities including paying for parking and going to concerts increasingly challenging.“It’s the tyranny of the apps,” says Ron Delnevo, the chair of the campaign committee at lobby group the Payment Choice Alliance. “In this country we’re being treated like sheep,” he says. “We’re always being told there’s no alternative.” But when a new smartphone can set you back hundreds of pounds, it is “an expensive passport to participate”, Delnevo says.According to the latest data from the telecoms regulator Ofcom, 8% of people aged 16 or over do not have a smartphone, which for the UK translates into just under 4.5 million people. Among those aged 75-plus, the proportion is said to be 28%. Add in all those who don’t or can’t use apps and the total number of people affected is a lot bigger.Lots of retailers offer enhanced or exclusive deals to their app users, and some run loyalty schemes through them. This is potentially a big deal as those schemes often give you access to discounts that are not otherwise available.“Some people are missing out on lower prices offered by loyalty schemes as they do not have access to phones or smart devices to download apps,” Reena Sewraz, the retail editor at Which? says.The consumer group is among those to have highlighted Lidl’s loyalty scheme, Lidl Plus, as one that is only accessible via an app, with an email address also required. That means the “big savings” available via the scheme’s weekly offers – at the time of writing these included 25% off tinned tuna and 20% off microwave rice and grain pouches – plus various coupons and rewards, are off-limits to those who cannot or do not wish to go digital.Delnevo says Lidl is “disenfranchising” many of its customers, adding: “The people who probably need the discounts most are the people who can’t afford or don’t have a smartphone.”In response, the supermarket chain says the Lidl Plus scheme “forms part of our commitment to providing customers with the best value. Nonetheless, we remain mindful of those who don’t have access to a smartphone or tablet and continue to offer in-store promotions through our ‘pick of the week’ offers.”Rival supermarket Asda also runs a scheme, Asda Rewards, where you have to download an app and there is no physical card available. This scheme does not offer price promotions – instead, you earn Asda “pounds” when you shop, which you convert into vouchers to spend in-store and online.Some retailers have gone further down the app path than others. The Boots Advantage card scheme, one of the UK’s most popular, still lets you use a physical plastic card, although there are “tailored offers” available via the app.And at the bakery chain Greggs, you can collect loyalty “stamps” for free food and drink and get “exclusive app-only gifts”. You currently get a free hot drink just for downloading the app.Meanwhile, the online clothes retailer Asos and the tools and hardware specialist Screwfix are just two of those that are or have recently been running “app exclusive” campaigns. Asos was this week offering 20% off “1,000s of styles,” while Screwfix’s Stacks of Rewards promotion let you collect coins and earn rewards by spending via the app.If there is one area of life where many people probably feel the app revolution has got out of hand, it is parking. In the UK there are thought to be at least 30 different parking apps, and it is not unusual for an individual to have eight to 10 different ones on their phone.Many older people have told the charity Age UK that they are “at the end of their tether” when it comes to paying for parking because they cannot park if an app or mobile is required. “In some cases this has meant missing important appointments like seeing their GP,” it adds.The RAC’s head of policy, Simon Williams, says many people are overwhelmed by the multitude of apps they have to use, “when in reality you want one that you like and you’re happy using and that you can use everywhere”.Six years ago the Department for Transport started developing a “national parking platform” (NPP) designed to enable drivers to use one app of their choice to pay for all their parking. It has been trialled by a number of councils, but a big question mark hangs over its future as public funding for the project looks likely to be withdrawn.Some councils are now permanently getting rid of parking ticket machines that accept debit and credit cards in order to save money (many stopped accepting cash a long time ago).The London borough of Barnet is one area where all the council pay and display machines at car parks have just been taken out of service. As of last month, motorists have been greeted with signs telling them to use the PayByPhone app.“Withdrawal of the ability to pay by card, following the removal of the payment by cash facility, will disappoint some drivers, especially the elderly, who find it difficult to use the PayByPhone app,” according to an article on the Barnet Society website.A Barnet council spokesperson says: “Pay and display machines were declining in use and in 2023-24 accounted for less than 7% of overall transactions. There are easy alternatives … including the PayByPhone app, phone or text, and cash payments at over 100 local PayPoint retailers.”Increasingly with gigs and other events, you need to download an app to access your tickets.For example, tickets for some events at the O2 arena in London are delivered as a “mobile ID” via the venue’s own app, and you get a barcode which is scanned to let you in.This type of app-based mobile ID system is also billed as the only admission method at venues including Ovo Arena Wembley in London and University of Wolverhampton at The Halls.That said, the venue websites do usually provide details of how people who do not have a smartphone or cannot download the app can gain entry to their event – but it might involve bringing photo ID and your confirmation email to the box office perhaps 90 minutes before the doors open.When it comes to theatre, popular services such as TodayTix offer savings on tickets and can be accessed online, although to take advantage of some of the benefits and offers, such as discounted same-day “Rush” tickets, you need to have the app.Those who cannot or do not use apps are missing out on some of the best deals for meals, takeaways and pub grub.McDonald’s is running a high-profile promotion called Deal Drop, where it offers items at “bargain” prices, such as a classic Big Mac for £1.49 (normally £4.99) and a children’s Happy Meal for £1.99 (normally £3.59) – but all of the discounts are available exclusively with thecompany’s app.The fastfood chain Subway runs a loyalty scheme where you earn points that you can convert into “Subway Cash” that can be spent on anything on the menu – but again it is app-only. If you were a member of Subway’s previous rewards scheme, which closed last May, your plastic membership card “is no longer usable”.Similarly, many pubs have apps that offer big discounts or let you earn rewards. The more than 200-strong Sizzling Pubs chain has recently been promoting eye-catching app-only deals such as 40% off main meals at some locations.Some coffee shop chains, such as Harris + Hoole, have also gone app-only with their loyalty schemes. With the Harris + Hoole one, you get a free drink for every six coffees you buy.Some of the best savings rates are offered by app-only providers – made up of banks and “electronic money institutions” (EMIs), which do not have their own banking licence, but put your money in a bank that does.At the time of writing, the Moneyfacts list of top-paying easy access accounts included products from the app-only providers Atom Bank, Chip and Plum paying 4.6%, 4.58% and 4.38% respectively.“App-only accounts are the new ‘online-only’ accounts,” says Anna Bowes, savings expert at the financial advisory firm The Private Office. “While not for everyone, apps are becoming far more popular with savers, especially as they can often be found paying some of the best rates.”To give you an idea of how apps have taken over savings, she says the current top five easy-access cash Isas are all provided by EMIs. “The next five top paying cash Isas are all with regulated banks or building societies but two must be opened via a mobile banking app,” she says.Some of the providers aimed at smartphone users are not entirely app-based. For example, saving and investing firm Moneybox primarily provides its services via its app, but says you can access them through other means, including its website.Meanwhile, the high street banks sometimes have products or services that are app-only – for example, HSBC’s Global Money service, which lets you convert, spend and send multiple currencies with no bank fees, is only available via the HSBC app.]]></content:encoded></item><item><title>DigiKey&apos;s Tariff Resources</title><link>https://www.digikey.com/en/resources/tariff-resources</link><author>nativeit</author><category>hn</category><pubDate>Sat, 22 Feb 2025 03:52:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Frequently asked questionsWhy did I receive a Tariff Charge?You received a Tariff product on your United States shipment.DigiKey does not have qualifying documentation exempting you from Tariff charges.Why did I receive a Tariff charge on this Invoice but not a previous Invoice?A previous Invoice may not have contained a qualified Tariff product.I have a Tax Exempt document, why am I being charged Tariff?Tax Exemptions are State Tax documents that do not apply to Federal Tariff charges.What products are impacted by the new tariffs?The new  applies to all products imported from China and Hong Kong effective . This tariff will be applied in addition to any existing Section 301 tariffs and other applicable tariffs based on the Harmonized Tariff Schedule.Below is a listing of products impacted by the Section 301 tariffs, including the tariff rate and the year in which they are scheduled to change. Please note that the rates listed do not include the recent 10% tariff applied by Executive Order, as mentioned above.Battery Parts (non-lithium-ion batteries)Increase rate to no less than 25%Increase rate to no less than 25%Increase rate to no less than 50%Lithium-ion electrical vehicle batteriesLithium-ion non-electrical vehicle batteriesSolar cells (whether or not assembled into modules)Steel and aluminum productsWhy am I being charged up to a 20% tariff on orders placed after February 4, 2025?On February 1, 2025, the President of the United States issued Executive Orders imposing an additional 10% tariff on all products imported from China and Hong Kong, effective 12:01 a.m. Eastern Time on February 4, 2025.For clarification, the new 10% tariff rate effective February 4, 2025, will be applied in addition to any applicable Section 301 tariffs and other tariffs based on the Harmonized Tariff Schedule. Unless an explicit waiver is granted, any inventory consumed from a Foreign Trade Zone on or after February 4, 2025, will be subject to the new 10% duty.One way DigiKey helps provide high-quality products at competitive prices is through our Duty Drawback Program, which allows us to recover a portion of the tariffs paid on imported products. However, under this Executive Order, the new 10% duty on all products imported from China and Hong Kong is not eligible for duty drawback programs.As a result, DigiKey must pass this new 10% duty on to our customers while continuing to ensure product quality and competitive pricing.DigiKey wants our customers to know that tariffs vary by product, meaning the exact amount will differ. For example, some products may be subject to a 10% tariff, while others may incur a 20% tariff.DigiKey remains actively engaged with suppliers and industry experts to explore ways to minimize the impact of these tariffs on our customers. We will continue to monitor and adapt to evolving regulations to ensure we provide high-quality products at cost-effective pricing.]]></content:encoded></item><item><title>Who needs a sneaker bot when AI can hallucinate a win for you?</title><link>https://www.eql.com/media/sneaker-bot-ai-error</link><author>pdonelan</author><category>hn</category><pubDate>Sat, 22 Feb 2025 02:08:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Every February we see a big spike in retailers running sneaker launches on EQL to coincide with the 2025 NBA All-Star Weekend. This year, the festivities kicked off a little earlier than normal, courtesy of Jordan Brand, who have been getting serious about getting back their mojo and dropping some serious heat in the process. This year marks 40 years since a talented rookie by the name of Michael Jordan wore his signature shoes at the 1985 All-Star Dunk Contest in a colorway that is now firmly ingrained in sneaker culture lore. To mark the occasion, Jordan Brand recreated the shoe in what they say is the closest to OG spec ever. Sneakerheads have been anticipating the drop for months and the demand was predictably crazy.At first, everything appeared to be going smoothly. Thousands of entries were rolling in, bots were being neutralized, winners were being picked and notified, but as we started notifying non-winners that they’d missed out, things got weird.We started seeing reports of people being told that they had won and lost in the same email… the Schrödinger’s cat of bug reports. What on earth was going on??Now, we’re accustomed to a baseline level of crazy – it comes with the territory of running some of the hottest product launches on the internet. Having a 24/7 support team who can bring calm to the chaos is big reason why brands work with EQL. But even for us, this one felt odd.Fans were reporting that their email app was listing ”You’ve been selected” but when they clicked on the email it displayed the heartbreaking “SORRY” non-winner message. Sneaker fans are a passionate bunch, and online launches often trigger the full range of human emotions - to put it mildly! People were understandably mad about the confusing messaging.When things heat up, it’s helpful to have a community you can turn to. We operate in a space where people sometimes troll for fun – or profit, like the time someone tried to pretend they’d won 750 pairs of the Nike/Tiffany Air Force 1 1837 to pump a cookgroup.We reached out to our Discord community, who confirmed that the issue was real and shared some more screenshots:The bizarre thing is that our winner emails don’t actually say “You’ve been selected”. As an on-call engineer, this is the point when you start questioning your life choices. You know that the issue is affecting thousands of users, but the offending phrase doesn’t appear anywhere in EQL’s codebase, aside from some very old launches several years ago.As more screenshots poured in, another odd thing jumped out – the winning phrase seemed to change from email to email. Some said “selected JORDAN AJ1”, others said “selected JORDAN AJ1”:By this point, we’d narrowed down the affected users to a single email client - , which is where we got suspicious. Had Yahoo Mail introduced any features lately that might be causing this…?At this point, the penny dropped. Just like Apple AI generating fake news summaries, Yahoo AI was hallucinating the fake winner messages, presumably as a result of training their model on our old emails. Worse, they were putting an untrustworthy AI summary in the exact place that users expect to see an email subject, with no mention of it being AI-generated 🤯Some people use Yahoo Mail to read emails from other providers like Gmail, so this issue hit a broad range of sneaker fans. And it’s worth mentioning that as of the time of writing, this AI feature is still live in Yahoo Mail, so more confusion is expected in future – not just for sneaker fans receiving launch results, but for anyone reading important emails in Yahoo Mail.For EQL users, if you’re ever in doubt, you can always double-check your launch results in the fans app, contact our support team (support@eql.com) or join our helpful Discord community. Until then, we’ll continue fighting the good fight to put products into the hands of real fans and trying to prevent bad AI from ruining your day! 🤖]]></content:encoded></item><item><title>We are the builders</title><link>https://www.wethebuilders.org/</link><author>ChrisArchitect</author><category>hn</category><pubDate>Fri, 21 Feb 2025 22:07:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Real stories from federal employees.For decades, we've done our jobs in the background. We made it easier to file taxes, get veterans' benefits, and apply for financial aid. During times of crisis, we helped refugees navigate immigration processes, helped everyone find vaccines, and helped parents find baby formula.Along the way, we made government websites easier to use while protecting the integrity of your personal information.If they really wanted to know how to use technology to build a more efficient country, they would ask us.But they haven't. They are destroyers.We don't work for DOGE. We have always worked for you.Here, you'll find stories from real government employees: How we save you time and money, how we protect your personal information, and how DOGE's dangerous dismantling of government technology puts you at risk.]]></content:encoded></item><item><title>20 years working on the same software product</title><link>https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/</link><author>hermitcrab</author><category>hn</category><pubDate>Fri, 21 Feb 2025 21:22:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I released version 1 of my table seating planning software, PerfectTablePlan, in February 2005. 20 years ago this month. It was a different world. A world of Windows, shareware and CDs. A lot has changed since then, but PerfectTablePlan is now at version 7 and still going strong.I have released several other products since then, and done some training and consulting, but PerfectTablePlan remains my most successful product. It’s success is due to a lot of hard work, and a certain amount of dumb luck.I was getting married and I volunteered to do the seating plan for our wedding reception. It sounded like a relatively straightforward optimization problem, as we only had 60 guests and no family feuds to worry about. But it was surprisingly difficult to get right. I looked around for some software to help me. There were a couple of software packages, but I wasn’t impressed. I could do better myself! So I wrote a (very rough) first version, which I used for our wedding.Things weren’t going great at my day job, at a small software startup. Maybe I could commercialize my table planner? I was a bit wary, as my potential competitors all seemed rather moribund and I didn’t think I would be able to make a living off it. But I thought I could do everything worth doing in 6-12 months and then start on the next product. Wrong on both counts!Web-based software was still in its infancy in 2005. So I decided to write it as desktop software using C++ and cross-platform framework Qt, which I had plenty of experience in. Initially, I just released a Windows version. But I later added a Mac version as well. Qt has had its commercial ups and downs in the last 20 years, but it has grown with me and is now very robust, comprehensive and well documented. I think I made a good choice.I financed PerfectTablePlan out of my own savings and it has been profitable every year since version 1 was launched. I could have taken on employees and grown the business, but I preferred to keep it as a lifestyle business. My wife does the accounts and proof reading and I do nearly everything else, with a bit of help from my accountant, web designers and a few other contractors. I don’t regret that decision. 20 years without meetings, ties or alarm clocks. My son was born 18 months after PerfectTablePlan was launched and it has been great to have the flexibility to be fully present as a Dad.CDs, remember them? I sent out around 5,000 CDs (with some help from my father), before I stopped shipping CDs in 2016.During the lifetime of PerfectTablePlan it became clear that things were increasingly moving to the web. But I couldn’t face rewriting PerfectTablePlan from scratch for the web. Javascript. Ugh. Also PerfectTablePlan is quite compute intensive, using a genetic algorithm to generate an automated seating plan and I felt it was better running this on the customer’s local computers than my server. And some of my customers consider their seating plans to be confidential and don’t want to store them on third party servers. So I decided to stick with desktop. But, if I was starting PerfectTablePlan from scratch now, I might make a different decision.Plenty of strange and wonderful things have happened over the last 20 years, including:PerfectTablePlan has been used by some very famous organizations for some very famous events (which we mostly don’t have permission to mention). It has seated royalty, celebrities and heads of state.A mock-up of PerfectTablePlan, including icons I did myself, was used without our permission by Sony in their ‘Big day’ TV comedy series. I threated them with legal action. Years later, I am still awaiting a reply.I got to grapple with some interesting problems, including the mathematics of large combinatorial problems and elliptical tables. Some customers have seated 4,000 guests and 4000! (4000x3999x3998 .. x 1) is a mind-bogglingly huge number.A well known wedding magazine ran a promotion with a valid licence key clearly visible in a photograph of a PerfectTablePlan CD. I worked through the night to release a new version of PerfectTablePlan that didn’t work with this key.I once had to stay up late, in a state of some inebriation, to fix an issue so that a world famous event wasn’t a disaster (no I can’t tell you the event).The lowest point was the pandemic, when sales pretty much dropped to zero.Competitors and operating systems have come and gone and the ecosystem for software has changed a lot, but PerfectTablePlan is still here and still paying the bills. It is about 145,000 lines of C++. Some of the code is a bit ugly and not how I would write it now. But the product is very solid, with very few bugs. The website and user documentation are also substantial pieces of work. The PDF version of the documentation is nearly 500 pages.I now divide my time between PerfectTablePlan and my 2 other products: data wrangling software Easy Data Transform and visual planner Hyper Plan. Having multiple products keeps things varied and avoids having all my eggs in one basket. In May 2024 I released PerfectTablePlan v7 with a load of improvements and new features. And I have plenty of ideas for future improvements. I fully expect to keep working on PerfectTablePlan until I retire (I’m 59 now).]]></content:encoded></item><item><title>Sparse Voxels Rasterization: Real-Time High-Fidelity Radiance Field Rendering</title><link>https://svraster.github.io/</link><author>jasondavies</author><category>hn</category><pubDate>Fri, 21 Feb 2025 21:06:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Slime OS – An open-source app launcher for RP2040 based devices</title><link>https://github.com/abeisgoat/slime_os</link><author>abeisgreat</author><category>dev</category><category>hn</category><pubDate>Fri, 21 Feb 2025 20:22:57 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hey all - this is the software part of my cyberdeck, called the Slimedeck Zero.The Slimedeck Zero is based around this somewhat esoteric device called the PicoVision which is a super cool RP2040 (Raspberry Pi Pico) based device. It outputs relatively high-res video over HDMI while still being super fast to boot with low power consumption.The PicoVision actually uses two RP2040 - one as a CPU and one as a GPU. This gives the CPU plenty of cycles to run bigger apps (and a heavy python stack) and lets the GPU handle some of the rendering and the complex timing HDMI requires. You can do this same thing on a single RP2040, but we get a lot of extra headroom with this double setup.The other unique thing about the PicoVision is it has a physical double-buffer - two PSRAM chips which you manually swap between the CPU and GPU. This removes any possibility of screen tearing since you always know the buffer your CPU is writing to is not being used to generate the on-screen image.For my cyberdeck, I took a PicoVision, hacked a QWERTY keyboard from a smart TV remote, added an expansion port, and hooked it all up to a big 5" 800x480 screen (interlaced up from 400x240 internal resolution).I did a whole Slimedeck Zero build video ( https://www.youtube.com/watch?v=rnwPmoWMGqk ) over on my channel but I really hope Slime OS can have a life of it's own and fit onto multiple form-factors with an ecosystem of apps.I've tried to make it easy and fun to write apps for. There's still a lot broken / missing / tbd but it's enough of a base that, personally, it already sparks that "programming is fun again" vibe so hopefully some other folks can enjoy it!Right now it only runs on the PicoVision but there's no reason it couldn't run on RP2350s or other hardware - but for now I'm more interested in adding more input types (we're limited to the i2c TV remote keyboard I hacked together) and fleshing out the internal APIs so they're stable enough to make apps for it!]]></content:encoded></item><item><title>The Ren&apos;Py Visual Novel Engine</title><link>https://www.renpy.org/</link><author>Tomte</author><category>hn</category><pubDate>Fri, 21 Feb 2025 20:09:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Ren'Py is a visual novel engine – used by thousands of creators from around the world –
      that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.
      These can be both visual novels and life simulation games. The easy to learn script language allows
      anyone to efficiently write large visual novels, while its Python scripting is enough for complex
      simulation games.Ren'Py is open source and free for commercial use.
          The latest official release of Ren'Py 8 is 8.3.4 "Second Star to the Right", released on
          December 8, 2024. Ren'Py 8 is recommended for all projects.
        
          The nightly fix version of Ren'Py is built every night, and contains fixes to the latest stable version. It isn't
          tested as well as the official release, but often has fixes that haven't made it through the release process.
        
        Ren'Py 7 is the legacy version of Ren'Py, to support ongoing projects that will be released in 2024.
        The latest version of Ren'Py 7 is 7.8.4 "Straight on Till Morning", released on
        December 8, 2024.
      

Booom313's Support Corner



      To ask questions that aren't appropriate for a public forum, or to find a
      speaker for your visual novel-related conference or con, please contact us via email.
    ]]></content:encoded></item><item><title>Yocto, RockPi and SBOMs: Building modern embedded Linux images</title><link>https://vpetersson.com/2025/02/21/yocto-rockpi-and-sboms.html</link><author>mvip</author><category>hn</category><pubDate>Fri, 21 Feb 2025 19:34:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[: I wanted to generate an up-to-date disk image for a Rock Pi 4 using Yocto that included CUPS and Docker to both get a better understanding of Yocto and test the new SBOM generation feature.As with many single-board computers (SBCs) from China, the issue often isn’t the board itself but rather the software. RockPi from Radxa is no exception. If you go and download the latest disk images for this board, you will notice that they are all end-of-life (EoL). However, these boards are still great and work very well for many applications. This should be top of mind if you are building a product that uses any of these devices.I wanted to use one of the RockPi 4 boards I had for a simple print server. It’s not a customer product, of course, but let’s assume it was. Since it has the option to add eMMC storage, I find it more reliable than Raspberry Pi (I know the Raspberry Pi 5 allows for proper storage). However, given that I neither trust the Radxa disk images nor did I want to set things up on an already EoL Linux distribution, I started doing some digging. As it turns out, the RockPi is supported in Yocto.Say what you want about Raspberry Pi, but you can still download an up-to-date OS that runs on the Pi 1.
In this article, I will show you not only how to build a disk image with Yocto (in this case for the Rock Pi 4, but it can easily be adjusted for other boards), but we will also talk a bit about how Yocto generates SBOMs (hint: it’s really clever) and where to find your SBOMs.The Yocto Project is an open-source framework for building custom Linux distributions tailored to embedded systems. It provides a flexible, modular build system based on BitBake and OpenEmbedded, enabling developers to create highly optimized and reproducible Linux images for specific hardware. Yocto is widely used in industries like automotive, IoT, and networking due to its ability to support diverse architectures and long-term maintenance needs. With its layered architecture, extensive BSP support, and strong focus on customization, Yocto is a powerful tool for developers looking to build and maintain embedded Linux systems efficiently.I’ve toyed with it a few times over the years to build images for Raspberry Pis, but never really used it seriously. However, I recently crossed paths with some of the Yocto people in a CISA working group I’m co-chairing on SBOM generation. As it turns out, Yocto is very sophisticated when it comes to generating SBOMs, so I wanted to get some more up-to-date exposure to Yocto. Color me impressed. Not only did Yocto produce a Software Bill of Materials (SBOM) for me – it did so without even asking me.Since Yocto builds everything from source and is essentially a package manager, it is able to capture all the dependencies into an SBOM. Moreover, since Yocto maintains detailed information about every dependency, it is able to generate very high-quality SBOMs.Before we dive in, here are some key terms in Yocto that you probably want to understand: – The reference distribution of the Yocto Project, containing the OpenEmbedded build system, BitBake, and a set of metadata – The codename for the Yocto Project 5.0 release – The codename for Yocto 4.2 – The codename for Yocto 4.0, a long-term support (LTS) release – The codename for Yocto 3.1, another LTS release – Modular additions to the base Yocto version that provide extra functionality – The build tool used by Yocto to process recipes and generate images – The build framework Yocto is based on (.bb files) – Build instructions for individual packages or applications (Board Support Package) – A set of metadata and configurations for specific hardware platformsBuilding a disk image with YoctoBefore we build, you will need a pretty beefy server to build this image (or a lot of time). I’m using my home server, and I think it took about an hour or two to build the initial version. Subsequent builds will be a lot faster due to cache.I’ve used an Ubuntu 24.04 VM to build my disk images, and you can find the base dependencies you need to install here.Let’s get our hands dirtyFirst, clone the repositories and set up the layers:git clone  scarthgap https://git.yoctoproject.org/poky
poky
git clone  scarthgap git://git.yoctoproject.org/meta-arm
git clone  scarthgap git://git.yoctoproject.org/meta-rockchip
git clone  scarthgap git://git.openembedded.org/meta-openembedded
git clone  scarthgap git://git.yoctoproject.org/meta-virtualization
bitbake-layers add-layer ../meta-arm/meta-arm-toolchain
bitbake-layers add-layer ../meta-arm/meta-arm
bitbake-layers add-layer ../meta-rockchip
bitbake-layers add-layer ../meta-openembedded/meta-oe

bitbake-layers add-layer ../meta-openembedded/meta-python
bitbake-layers add-layer ../meta-openembedded/meta-networking
bitbake-layers add-layer ../meta-openembedded/meta-filesystems
bitbake-layers add-layer ../meta-virtualization
Next, adjust your  by appending these configurations:MACHINE 

INIT_MANAGER 
DISTRO_FEATURES:append 
DISTRO_FEATURES:remove 
CORE_IMAGE_EXTRA_INSTALL +Finally, build the image:Note, if you’re building on Ubuntu 24.04, you might need to run:apparmor_parser  /etc/apparmor.d/unprivileged_userns
After the build completes, you can find your image here: tmp/deploy/images/rock-pi-4b/core-image-base-rock-pi-4b.rootfs-.wic
Flash this disk image and you should be good to go. Once it’s up and running, you should be able to SSH into the device using  and a blank password.It’s important to note that Yocto generates a disk image. By default, you cannot update this disk image by any other means than reflashing it (e.g., you can’t run “apt update”). There are over-the-air (OTA) platforms that can be integrated into Yocto, such as Mender and RAUC, but by default, you need to rebuild the image from scratch to update dependencies and patch vulnerabilities.One of the cool features of Yocto is that it automatically generates SBOMs. You can find them in the deploy directory: tmp/deploy/images/rock-pi-4b/spdx..]
You can extract the SPDX file with:
    path/to/tmp/deploy/images/rock-pi-4b/core-image-base-rock-pi-4b.rootfs-.spdx.tar.zst
Do note that this will generate a lot of files. You will find a file called  in there, which links to all other SBOMs using document linking.If you are intending to run this in production, please do not just copy the above. These images are configured for lab or test mode. Yocto is very well suited for production images, but you need to harden them and also have an OTA strategy in place. Alternatively, I can recommend Balena, which uses Yocto under the hood and also supports the Rock Pi.One limitation of the current disk image for Rock Pi is that you don’t have a functional TTY. You can SSH in, or you could use a serial console, but the regular TTY doesn’t work and I haven’t spent much time trying to figure out why. Also, the disk system doesn’t automatically expand to use all available space on the eMMC/SD.Some things I’m planning to add in the future:Add support for auto disk expansionFound an error or typo? File PR against this file.]]></content:encoded></item><item><title>Suckless.org: software that sucks less</title><link>https://suckless.org/</link><author>flykespice</author><category>hn</category><pubDate>Fri, 21 Feb 2025 18:27:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Home of dwm, dmenu and
other quality software with a focus on simplicity, clarity, and frugality.This reverts a commit and a regression with cursor move with wide glyphs, for
example with GNU readline.Below are some highlights of the changes for the recent releases of dmenu, dwm,
st and tabbed, see the git logs for all details:General small Makefile improvements, rationale being: just be verbose and show
what is done: do not abstract/hide details from the user/developer.
Respect (more) the package manager and build system flags (CFLAGS, LDFLAGS, etc).Improvements to signal handling.Fix: Avoid missing events when a keysym maps to multiple keycodes.Reduce memory usage for reading the lines.Fix: X11 BadMatch error when embedding on some windows.Fix: faulty zombie process reaping.Improvements to signal handling.Improve compatibility with compiling on older systems such as Slackware 11.Thanks to all contributors who submitted patches.Suckless now has a dark mode CSS style for its pages.
Surf also now has support for dark mode.On Tuesday, 2021-05-11 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 1 hour from about 21:00 to
22:00 UTC+02:00.The mailinglist, website and source-code repositories will have some downtime. the maintenance was finished at 2021-05-12 23:33 UTC+02:00.
P.S.: It didn't actually take 26h30, I just had forgotten to do it.On Wednesday, 2021-03-31 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 2-3 hours from about 19:00 to
21:00 - 22:00 UTC+02:00.The mailinglist, website and source-code repositories will have some downtime. the maintenance was finished at 2021-03-31 19:10 UTC+02:00.The slcon7 has been cancelled due to the 2019-nCoV
pandemic.On Wednesday, 2019-12-04 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 2-3 hours from about 19:00 to
21:00 - 22:00 UTC+01:00.The mailinglist, website and source-code repositories will have some downtime. the maintenance was finished at 2019-12-04 20:00 UTC+01:00.Registrations are now open for slcon6 that will be held in
Bad Liebenzell, Germany on 2019-10-(04-06).The CfP for interested participants will end on 2019-06-30.There is now a patch overview tool to have a
quick overview of the patch status list. This list is generated each day from
the sites repository. It checks if patches apply
cleanly in a normal patching manner. Of course it does not check patch
combinations.Please keep the patches tidy and maintain or remove them.This release has mostly bugfixes.The maintainance is completed. Let me know of any important things that are broken.
Internally we will keep tweaking the server configuration over the course of
time.There will be a scheduled server maintenance next Friday and Saturday, 2018-06-(01-02).
The migration to the new server will happen on these days and the git
repositories and mailing list will be frozen on the old (now current)
server.This release fixes some regressions introduced in the 0.8 release.Registrations for slcon5 are now open.suckless.org now supports TLS using Let's Encrypt.
Cloning git repos over HTTPS now works. Some links on the page have been
changed to allow both HTTP and HTTPS.HSTS is not fully working yet. This will be fixed.The IPv6 AAAA record was added and IPv6 is fully working now.suckless has many subdomains, these should hopefully all work via TLS. If you
see a subdomain without a signed certificate please report it. If you find any
broken links on the wiki pages, these can be fixed by anyone.The suckless.org project is now hosted on a new server. All inactive accounts
have been removed during the relocation.Please note that the new ECDSA key fingerprint is
SHA256:7DBXcYScmsxbv7rMJUJoJsY5peOrngD4QagiXX6MiQU.surf now uses webkit2 by default. The webkit1 version
is kept in the surf-webkit1
branch. The “master” branch doesn't exist anymore, HEAD is now
surf-webkit2, so be sure to rebase your local
master commits onto surf-webkit1.slcon3 preliminary schedule now published. If you want to
attend please register before: .Kai and Anselm gave an interview about suckless.org on Randal Schwartz's FLOSS
Weekly showslcon2 will be held in Budapest on 2015-10-(30-31).The CfP for interested participants is now open and will end on 2015-04-30.There will be a
suckless assembly
at the 31C3. The whole suckless
community is invited to come, meet and hack!We are glad to announce the switch to git from mercurial in all of our
repositories. You can find them at git.suckless.org Many
thanks to 20h for his contribution!Today we heard a very sad news that our friend, contributor and philosophical
advisor Uriel has passed away peacefully. We will miss him a lot.Anselm gave a talk about The 'suckless.org' universe at the LinuxTag
2011 conference in Berlin.We learned today that the previous wmii maintainer, who wasn't actively
involved since 2007, Denis Grelich,
died on 2010-03-12.
We thank him for his work. Rest in peace.Some of us will visit CLT2010. Anselm
will give a
talk
about stali on the second day of CLT2010 at 17:00.There was a small community meeting in Berlin! Thanks to all attendees.]]></content:encoded></item><item><title>NASA&apos;s James Webb Space Telescope faces potential 20% budget cut</title><link>https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts</link><author>consumer451</author><category>hn</category><pubDate>Fri, 21 Feb 2025 18:25:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The scientists behind NASA's largest and most powerful space telescope ever built are bracing for potentially crippling budget cuts, and the observatory is only halfway through its primary mission.The team overseeing NASA's James Webb Space Telescope (JWST) has been directed to prepare for up to 20% in budget cuts that would touch on every aspect of the flagship observatory's operations, which are managed by the Space Telescope Science Institute (STScI) in Maryland. The potential cut comes even as the space observatory is more in demand than ever before, with astronomers requesting the equivalent of nine years' worth of Webb observing time in one operational year."NASA is having budget constraints across the entire board, so the institute is being asked to consider a significant — about 20% — cut to our operational budget for the mission starting later this year," Tom Brown, who leads the Webb mission office at STScI, told a crowd of scientists last month at the 245th American Astronomical Society (AAS) meeting in National Harbor, Maryland. "So the impacts of that, if it comes to pass, pretty much cut across the entire mission."But unlike Hubble, which turns 35 this spring, and Chandra, which launched in 1999, Webb is in its prime, approaching the midpoint of a primary 10-year mission. It could last at least 20 years or more, NASA officials have said. The mission is an international partnership between NASA, the European Space Agency and the Canadian Space Agency."Frankly, this mission works far better than, really, most folks expected it to, you know," Brown said during the Webb town hall event on Jan. 15 at the AAS conference. "It's extremely worrisome that, while we're in the middle of the prime mission, we're also maybe looking at significant budget cuts."The $10 billion Webb space telescope survived a tumultuous development process, one that included cost overruns and technical delays that nearly killed the observatory before it ever flew. Lawmakers with the House Appropriations Committee proposed cancelling the mission in 2011, a decade before Webb's Christmas Day launch in 2021, only to back down after backlash from scientists and influential politicians defending the observatory.Since its 2021 launch, the Webb space telescope has outmatched even the most optimistic predictions for its performance. Its infrared optics have looked deep into the universe's past, observed distant galaxies and exoplanets, and even peered at our own local solar system planets closer to home."In a nutshell, it is truly fulfilling its promise," Macarena Garcia Marin, STScI's Webb project scientist, said during the same town hall event. "Across every field, JWST is truly delivering cutting-edge science."Some of Webb's budget challenges stem from its operational costs, which were set "idealistically low" in 2011 when the observatory was saved from cancellation. Those costs, coupled with inflation rates that were much higher than expected and less flexibility in NASA's budget, have also contributed, Brown said.According to a , a 20% cut to Webb's operational budget would definitely affect how much science the telescope could perform. The impacts would be felt across teams that review proposals for observing targets, data analysis, observatory efficiencies, and anomaly resolution when something goes wrong, not to mention the need to engage with the scientific community and public on Webb's science results."It's a huge cut. That's not like kind of trying to nibble away at the edges," Brown told Space.com. "That impacts everything across the board, all the way up to how many modes we're offering to the observers."Those impacts, Brown said, would likely be felt for the first time in October, when the next fiscal year begins.Brown's comments at the Webb observatory town hall at AAS came just before the inauguration of President Donald Trump, who in subsequent weeks created the Department of Government Efficiency headed by SpaceX CEO  to reduce government spending. DOGE, as it's known, has worked to dismantle some entire agencies, like the U.S. Agency for International Development, which provides aid to other countries during disasters and other emergencies, while also overseeing massive cuts to the federal workforce. Nearly 1,000 NASA jobs could be eliminated, though they appear to have been saved from layoffs earlier this week.Trump has nominated American billionaire entrepreneur , who has flown in orbit twice on private SpaceX missions he financed himself, to serve as the next NASA administrator, though Isaacman has yet to be confirmed. The agency is currently being led by Acting Administrator , former director of the agency's Kennedy Space Center in Florida.]]></content:encoded></item><item><title>Richard Feynman&apos;s blackboard at the time of his death (1988)</title><link>https://digital.archives.caltech.edu/collections/Images/1.10-29/</link><author>bookofjoe</author><category>hn</category><pubDate>Fri, 21 Feb 2025 18:22:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[These digitized collections are accessible for purposes of education and research. Due to the nature of archival collections, archivists at the Caltech Archives and Special Collections are not always able to identify copyright and rights of privacy, publicity, or trademark. We are eager to hear from any rights holders, so that we may obtain accurate information. Upon request, we’ll remove material from public view while we address a rights issue.]]></content:encoded></item><item><title>Some critical issues with the SWE-bench dataset</title><link>https://arxiv.org/abs/2410.06992</link><author>joshwa</author><category>hn</category><pubDate>Fri, 21 Feb 2025 17:59:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Ruby on Rails still matters</title><link>https://www.contraption.co/rails-versus-nextjs/</link><author>philip1209</author><category>hn</category><pubDate>Fri, 21 Feb 2025 17:46:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I found vinyl records from my late grandfather recently. It struck me how this media from the previous millennium played without issues. Vinyl represented a key shift in music distribution - it made printing and sharing sounds accessible, establishing a standard that persists. While audio sharing methods evolved, the original approaches remain functional. In our increasingly complex world, many people return to vinyl because it offers simplicity, stability, and longevity.Amidst the constant changes of web technologies, it's easy to forget that old websites continue to work just fine, too. A plaintext website from the 1990s loads in modern browsers just as it did then.Websites gained additional capabilities over time - CSS for styling, JavaScript for interactivity, and websockets for real-time updates. Yet their foundation remains based on pages, forms, and sessions.Ruby on Rails emerged twenty years ago as a unified approach to building interactive, database-powered web applications. It became the foundation for numerous successful companies - Airbnb, Shopify, Github, Instacart, Gusto, Square, and others. Probably a trillion dollars worth of businesses run on Ruby on Rails today.Effective tools simplify complex tasks through abstraction. Cars illustrate this - driving once required understanding fuel systems, timing, and clutch mechanics. Now most drivers don't know how many gears their car has.Ruby on Rails packaged web development best practices into an approachable toolkit: login sessions, CSRF protection, database ORMs. This abstraction lets developers focus on building products rather than technical tedium. Today, most developers don't know the contents of their login cookie, even though it powers their application.Rails succeeded by staying close to web fundamentals. It uses HTML primitives like pages, input fields, and forms. As a backend-focused framework, it concentrates on data validation, processing, and storage, making form creation straightforward.JavaScript gained prominence after Rails' initial success. The last ten years of web development advancements basically gave websites  the functionality of an iPhone app, while still being a website.Next.js now serves as the most common tool for building a startup. Its frontend-focused framework enables dynamic loading states, server-side rendering, and complex component building. Another trillion dollars worth of companies is being built on Next.js, and these web apps are faster and more polished than what could have been built on Ruby on Rails.Next.js and its underlying technology React, drive much of modern web innovation. Basically every mainstream consumer product you love runs on this stack - like Spotify, Netflix, Facebook, and Stripe. It allows developers to create quick, customized, and interactive products by pushing web standards to their limits.Amid the rapid adoption of Next.js, Rails has continued to maintain relevance. New projects - from independent projects to AI companies - are still choosing it for new projects.The truth is that the new wave of Javascript web frameworks like Next.js has made it harder, not easier, to build web apps. These tools give developers more capabilities - dynamic data rendering and real-time interactions. But, the cost of this additional functionality is less abstraction.Next.js really competes with native iPhone apps. Previously, startups needed iPhone apps for refined user experiences, and building iPhone apps was a complex process that often requires multiple developers with different specialities. Next.js enabled websites to approach iPhone app quality. Many of today's most polished products, like Linear and ChatGPT launched as Next.js applications, and treated mobile apps as secondary priorities.Most web applications continue to be forms on pages - job boards, vendor systems, and ecommerce stores. Next.js can build these, but requires additional development time compared to Rails. Using cutting-edge frameworks introduces instability through frequent updates, new libraries, and unexpected issues. Next.js applications often rely on a multitude multiple third-party services like Vercel, Resend, and Temporal that introduce platform risk.Developers choose Rails today because, 20 years later, it remains the most simple and abstracted way to build a web application. Solo developers can create dynamic, real-time web applications independently (as I did with Booklet and Postcard). Enterprise teams use it to build applications with multiple models and access controls, supported by thorough testing. Rails helps small teams work faster while reducing development and maintenance costs.I have experience with both frameworks. I built Find AI, a venture-funded AI startup, using Rails. As a search engine, it benefited from Rails' ability to handle complex backend operations with simple frontend needs. Today I'm working on Chroma Cloud, designed for exploring and managing large datasets, and Next.js powers its advanced interactions and data loading requirements. Rails has started to show its age amid with the current wave of AI-powered applications. It struggles with LLM text streaming, parallel processing in Ruby, and lacks strong typing for AI coding tools. Despite these constraints, it remains effective.Vinyl changed music by broadening access. Sound quality improved over time, but earlier formats retain value. The Köln Concert maintains its popularity regardless of bit rate. In the technology world, we can enjoy the polish of Linear while appreciating that Craiglist's 90s-era website probably makes more money.At the end of the day, users care about product utility more than implementation details. Polish fades, but utility persists.]]></content:encoded></item><item><title>The Profitable Startup</title><link>https://linear.app/blog/the-profitable-startup</link><author>tommoor</author><category>hn</category><pubDate>Fri, 21 Feb 2025 17:41:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?But that thinking was always flawed.Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.Paul Graham famously wrote about "ramen profitability" – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next  engineer. This intentional approach has allowed us to maintain both quality and culture.The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.Understand Your Risk ProfileAre you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.Hire Intentionally and SlowerFor most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.]]></content:encoded></item><item><title>Bybit loses $1.5B in hack but can cover loss, CEO confirms</title><link>https://www.tradingview.com/news/coindesk:cda1c390e094b:0-bybit-ceo-confirms-exchange-was-hacked-for-1-46b-says-his-firm-can-cover-the-loss/</link><author>tuananh</author><category>hn</category><pubDate>Fri, 21 Feb 2025 17:15:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DeepDive in everything of Llama3: revealing detailed insights and implementation</title><link>https://github.com/therealoliver/Deepdive-llama3-from-scratch</link><author>therealoliver</author><category>hn</category><pubDate>Fri, 21 Feb 2025 16:57:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I found a backdoor into my bed</title><link>https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed</link><author>riverdroid</author><category>hn</category><pubDate>Fri, 21 Feb 2025 16:27:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[A little while ago I asked my infosec Twitter followers what IoT device in my house they thought I found a live AWS key in. (For those that don’t know, Amazon keys can be incredibly dangerous if exposed)Guesses ranged from a  to a , but . The right answer was my . I also found a backdoor into my bed, but more on that later.Security professionals are, in my experience, exhausted of things being connected to the internet that don’t need to be. Tired of their stove, car, washing machine, and bed all being internet connected.We want the features of the future, without sacrificing our data privacy, cybersecurity, reliability and integrity.I want the features of a temperature controlled bed, without having to worry about random engineers and hackers giving themselves access to my bed 24/7. offered the features of temperature control: set the bed to any temperature hot or cold. For someone who suffers from insomnia this seemed worth a shot.I was willing to overlook:It won’t function if the internet goes downBasic features are behind an additional $19/mo subscriptionThe bed’s only controls are via mobile appI will say, being able to control the temperature of your bed is actually a magical thing, but after a few months, curiosity got the better of me and I took a look at the firmware. In the end, I got enough of the cyber ick, I decided to seek a simpler, less internet-connected solution to my temperature-controlled bed needs.It turns out inexpensive  provide a similar functionality as the Eight Sleep pod, without the existential dread of being hacked, and having my sleep preferences shared with a bunch of developers.While the Eight Sleep CEO Matteo seems focused on providing DOGE with great sleep, the real doge (pictured above), whose name is Latte, is sleeping great tonight. Stick around until the bottom of the post for how to set this up (it’s easier than you think)So let’s talk about that backdoorFirst of all, how did I get the bed’s firmware? Easy. You can download it. Eight Sleep provides access to the firmware through their update URL:https://update-api.8slp.net/v1/updates/p1/1?deviceId={anynumber}&currentRev=1(Just replace {anynumbers} with any number)When I say backdoor, what am I referring to? Sure, Eight Sleep needs a way to push updates, provide service, and offer support. That’s expected.What goes too far in my opinion, is allowing all of Eight Sleep’s engineers to remotely SSH into every customer’s bed and run arbitrary code that bypasses all forms of formal code review process. And yes, I found evidence that this is exactly what’s happening.Let’s break down what’s shown above. In the first image, we see evidence SSH is being exposed remotely, to a far away host, remote-connectivity-api.8slp.net. Typically SSH would only be accessible to the local area network, but the variables in production.json would seem to imply this access was opened up to a remote host.In the second screenshot, we have the public key that’s authorized to access the device. The email address attached to the public key, , to me suggests the private key is likely accessible to the entire engineering team.What does this mean, exactly? Well, each bed contains a full Linux-based computer. If my estimations above are correct, all of Eight Sleep engineering can take full control of that computer any time they want. What Can They Do with This Access?Let’s start with the basics: They can know when you sleepThey can detect when there are 2 people sleeping in the bed instead of 1They can know when it’s night, and no people are in the bedImagine your ex works for Eight Sleep. Or imagine they want to know when you’re not home.(Of course, they can also change the bed’s temperature, turn on the vibrating feature, turn off your alarm clock, and any of the other normal controls they have power over.)Beyond the basics, what does access to a device on your home network grant them? Any other device connected to that home network - smart fridges, smart stoves, smart washing machines, laptops - is typically routable via your bed. The (in)security of those devices is now entrusted to random Eight Sleep engineers. Remember when Uber got in trouble for that God Mode app a few years ago? If my assumptions are correct about SSH remote access, this is in that ballpark.The devices don’t contain logs or notifications we can access to find when this is occurring. It’s possible Eight Sleep borrowed a page from Tesla.But it should go without saying, giving engineers arbitrary SSH access on all customer devices is not best practice.Personally, I don’t want my bed data accessible to anyone, but the eight sleep sure does harvest people’s bed data, and occasionally tweet about how they’re watching you sleepThe key to a bad night sleep was AWS.Well the AWS key seemed to be streaming data directly into Amazon. Of course the million dollar question is what’s the policy on that key? The key could be the most dangerous thing described so far, or it could be useful for just a bit of mischief (if nothing else someone could use it to rack up a huge AWS bill for Eight Sleep) Unfortunately, we’ll never know, because as soon as I reported it, Eight Sleep revoked the key. We can tell from the surrounding context that the key had write access to Kenises, but beyond that, it’s unclear.What we do know though, is an attacker could have used that key to send 5,000 `PUT` requests per second into Kinesis and racked up a $100,000 per month bill for Eight Sleep.Unexpected monthly bills cost us all some lost sleep.So what was that about an aquarium chiller?This process was a lot simpler than I originally imagined. Essentially all you need to do is unplug the rubber tubing from the Eight Sleep cover, which is available on eBay for a few hundred bucks, and plug it into a $150 aquarium chiller. There’s some zip ties securing the tubes you have to cut, but other than that, it’s a totally reversible, non-destructive process that takes 30 seconds.That’s it. Aquarium chillers are somewhat of a misnomer, as they can also provide heat. They use thermoelectric devices to regulate temperature, either cooling or warming the liquid that flows through them, which is the same technology found in eight sleep. Here’s a short clip of the entire process:And now you have all the temperature control of an Eight Sleep with none of the apps, subscriptions, internet connectivity, backdoors, and security liabilities of an Eight Sleep.There are other projects that remove the internet connectivity of the Eight Sleep, such as the Free Sleep project, but for me, I prefer the less sophisticated, physical tactile buttons of the aquarium chiller.So what have we learned from all this?Honestly, Eight Sleep is clearly onto something, having raised $110 million dollars in venture capital, exceeding $300 million dollars in annual revenue,  ̶f̶o̶r̶c̶i̶n̶g̶ welcoming users into a subscription  ̶h̶e̶l̶l̶ model, and adding to the ever growing list of devices that will one day stop working when the parent company turns their servers off.I for one, am going to be sleeping well tonight to the warm silent circulation of an aquarium chiller, as will the Doge, Latte.]]></content:encoded></item><item><title>Tesla recalls 380k vehicles in US over power steering assist issue</title><link>https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/</link><author>themgt</author><category>hn</category><pubDate>Fri, 21 Feb 2025 15:57:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Apple Pulls Encrypted iCloud Security Feature in UK</title><link>https://www.macrumors.com/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/</link><author>tosh</author><category>hn</category><pubDate>Fri, 21 Feb 2025 15:49:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Apple has withdrawn its Advanced Data Protection iCloud feature from the United Kingdom following government demands for backdoor access to encrypted user data, according to . The move comes after UK officials secretly ordered Apple to provide unrestricted access to encrypted iCloud content worldwide.Customers who are already using Advanced Data Protection, or ADP, will need to manually disable it during an unspecified grace period to keep their iCloud accounts, according to the report. Apple said it will issue additional guidance in the future to affected users and that it "does not have the ability to automatically disable it on their behalf."The UK government's demand came through a "technical capability notice" under the Investigatory Powers Act (IPA), requiring Apple to create a backdoor that would allow British security officials to access encrypted user data globally. The order would have compromised Apple's Advanced Data Protection feature, which provides end-to-end encryption for iCloud data including Photos, Notes, Messages backups, and device backups."We are gravely disappointed that the protections provided by ADP will not be available to our customers in the UK given the continuing rise of data breaches and other threats to customer privacy," Apple said in a statement. "ADP protects iCloud data with end-to-end encryption, which means the data can only be decrypted by the user who owns it, and only on their trusted devices."Apple's decision to pull the feature rather than comply with the UK's demands is consistent with the company's previous statements that it would consider withdrawing encrypted services from the UK rather than compromise security. Apple has long opposed creating backdoors in its products, maintaining that such access points would inevitably be discovered by malicious actors.Notice UK iCloud users now see after the feature was pulledThe UK order was particularly controversial as it would have required Apple to provide access to data from users outside the UK without their governments' knowledge. Additionally, the IPA makes it illegal for companies to disclose the existence of such government demands.US security agencies, including the FBI and NSA, have been advocating for increased use of encryption to protect against Chinese cyber threats, creating potential conflicts between UK and US security interests."Enhancing the security of cloud storage with end-to-end encryption is more urgent than ever before,” said Apple on Friday, per . The company added that it "remains committed to offering our users the highest level of security for their personal data and are hopeful that we will be able to do so in the future in the United Kingdom."Note that the loss of Advanced Data Protection in the UK does not affect the existing end-to-end encryption of several other Apple features available in the country, including iMessage, FaceTime, password management and health data.Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our Political News forum.  All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.]]></content:encoded></item><item><title>Apple pulls data protection tool after UK government security row</title><link>https://www.bbc.com/news/articles/cgj54eq4vejo</link><author>helsinkiandrew</author><category>hn</category><pubDate>Fri, 21 Feb 2025 15:05:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Apple is taking the unprecedented step of removing its highest level data security tool from customers in the UK, after the government demanded access to user data.Advanced Data Protection (ADP) means only account holders can view items such as photos or documents they have stored online through a process known as end-to-end encryption.But earlier this month the UK government asked for the right to see the data, which currently not even Apple can access.Apple did not comment at the time but has consistently opposed creating a "backdoor" in its encryption service, arguing that if it did so, it would only be a matter of time before bad actors also found a way in.Now the tech giant has decided it will no longer be possible to activate ADP in the UK.It means eventually not all UK customer data stored on iCloud - Apple's cloud storage service - will be fully encrypted.Data with standard encryption is accessible by Apple and shareable with law enforcement, if they have a warrant.The Home Office told the BBC: "We do not comment on operational matters, including for example confirming or denying the existence of any such notices."In a statement Apple said it was "gravely disappointed" that the security feature would no longer be available to British customers."As we have said many times before, we have never built a backdoor or master key to any of our products, and we never will," it continued.The ADP service is opt-in, meaning people have to sign up to get the protection it provides.From 1500GMT on Friday, any Apple user in the UK attempting to turn it on has been met with an error message.Existing users' access will be disabled at a later date. It is not known how many people have signed up for ADP since it became available to British Apple customers in December 2022.Prof Alan Woodward - a cyber-security expert at Surrey University - said it was a "very disappointing development" which amounted to "an act of self harm" by the government."All the UK government has achieved is to weaken online security and privacy for UK based users," he told the BBC, adding it was "naïve" of the UK to "think they could tell a US technology company what to do globally".Online privacy expert Caro Robson said she believed it was "unprecedented" for a company "simply to withdraw a product rather than cooperate with a government"."It would be a very, very worrying precedent if other communications operators felt they simply could withdraw products and not be held accountable by governments," she told the BBC.Meanwhile, Bruce Daisley, a former senior executive at X, then known as Twitter, told BBC Radio 4's PM programme: "Apple saw this as a point of principle - if they were going to concede this to the UK then every other government around the world would want this."The request was served by the Home Office under the Investigatory Powers Act (IPA), which compels firms to provide information to law enforcement agencies.Apple would not comment on the notice and the Home Office refused to either confirm or deny its existence, but the BBC and the Washington Post spoke to a number of sources familiar with the matter.It provoked a fierce backlash from privacy campaigners, who called it an "unprecedented attack" on the private data of individuals.Last week, Will Cathcart, head of WhatsApp, responded to a post on X expressing his concerns about the government's request. He wrote: "If the UK forces a global backdoor into Apple's security, it will make everyone in every country less safe. One country's secret order risks putting all of us in danger and it should be stopped."Two senior US politicians said it was so serious a threat to American national security that the US government should re-evaluate its intelligence-sharing agreements with the UK unless it was withdrawn.It is not clear that Apple's actions will fully address those concerns, as the IPA order applies worldwide and ADP will continue to operate in other countries.One of those US politicians - Senator Ron Wyden - told BBC News that Apple withdrawing end-to-end encrypted backups from the UK "creates a dangerous precedent which authoritarian countries will surely follow". Senator Wyden believes the move will "not be enough" for the UK to drop its demands, which would "seriously threaten" the privacy of US users.In its statement, Apple said it regretted the action it had taken."Enhancing the security of cloud storage with end-to-end-encryption is more urgent than ever before," it said."Apple remains committed to offering our users the highest level of security for their personal data and are hopeful that we will be able to do so in future in the UK."Rani Govender, policy manager for child safety online at the NSPCC, said it wants tech firms like Apple to ensure they are balancing child and user safety with privacy."As Apple looks to change its approach to encryption, we're calling on them to make sure that they also implement more child safety measures, so that children are properly protected on their services," she told BBC News.The UK children's charity has said that end-to-end encrypted services can hinder child safety and protection efforts, such as identifying the sharing of child sexual abuse material (CSAM).But Emily Taylor, the co-founder of Global Signal Exchange which provides insights into supply-chain scams, said that encryption was more about protecting consumer privacy and that it is not the same as the dark web where CSAM is usually distributed."The trouble with this long-running debate, zero-sum debate about encryption and child protection is that the tech companies can come out sounding incredibly callous, but that's not the point," she told Radio 4's Today programme."Encryption is something that we use every day; whether its communicating with our bank, whether its on messaging apps that are end-to-end encrypted, encryption is a form of privacy in an otherwise very insecure online world."The row comes amid growing push-back in the US against regulation being imposed on its tech sector from elsewhere.In a speech at the AI Action Summit in Paris at the beginning of February, US Vice President JD Vance made it clear that the US was increasingly concerned about it. "The Trump administration is troubled by reports that some foreign governments are considering tightening the screws on US tech companies with international footprints," he said.]]></content:encoded></item><item><title>Johnny.Decimal – A system to organise your life</title><link>https://johnnydecimal.com/</link><author>debone</author><category>hn</category><pubDate>Fri, 21 Feb 2025 14:52:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Johnny.Decimal is designed to help you find things quickly, with more confidence, and less stress.You assign a unique ID to everything in your life.These IDs help you stay organised. They impose constraints that make it harder to get lost. And you create your own index to link everything in your life together.The system is free to use and the concepts are the same at home, work, or that club you manage.In real life, if you stored your stuff in piles of badly-labelled boxes you'd never find anything again.If you put  boxes in boxes, in boxes, you'd never know which box to open to find the next box. It would be chaos. But this is how you save your computer files.Here's one way to think about how a Johnny.Decimal system works. In this simple analogy, an area is a shelf, a category is a box, and an ID is a manila folder.Imagine a computer is a garage. We can't put everything on the floor, so we buy ten shelves. Then we dedicate each one to an area of our life -- , , and .Each shelf has space for ten boxes, so we categorise what we want to store. In life admin we decide on five and label them: , , , , and . Our boxes have space for a number, so we add that too.Step 3: File your stuff in foldersWe put our documents in manila folders. Each folder gets a number starting at  so we can track them. In this case, we've put some insurance policies in . Then put the folder in a box.This is how we structure our file systemLet's return to our computer. The shelves have become our area folders. The boxes are category folders. And the manila folders are the IDs where we save our files.Benefits of the Johnny.Decimal IDEach of our storage folders now has a number, the ID. It always has two digits, a decimal, and two digits. For example, . This number is really useful.The ID tells us exactly where a thing is. The numbers before the decimal are the item's category, and they define the structure of your system.At a glance, you know what sort of thing the item contains. You'll be astonished at how many of your category numbers you remember.They're short, memorable, and can be spoken out loud. Say it like "sixteen oh-two" or "thirty-one dot seventeen".This is really handy when you want to tell someone (including your future self) where a thing is.Things stay where they areIf you use the alphabet to name folders, they move when a new one is created. So you never get a chance to develop muscle memory.Numbers solve this problem. In the example above,  comes before  because the folders sort by number. If we made a new folder, , nothing would move.The 'no more than ten' concept is at the heart of Johnny.Decimal.When you start looking for something, you have no more than ten area folders to choose from. Select one and ignore the rest. Now you have no more than ten category folders to choose from. Repeat the process.You then arrive in a folder with no more than one hundred IDs. If the ID was created recently it will have a higher number. If not, lower. And things created together, stick together. The alphabet isn't around to ruin the party.Welcome to the Johnny.Decimal family, there's plenty to go on with:]]></content:encoded></item><item><title>SpaceX engineers brought on at FAA after probationary employees were fired</title><link>https://www.wired.com/story/faa-doge-elon-musk-space-x/</link><author>actionfromafar</author><category>hn</category><pubDate>Fri, 21 Feb 2025 14:28:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The FAA has frequently tangled with Musk’s SpaceX, as the rocket company and others fight to operate their own interests in crowded American airspace. In January, the FAA temporarily grounded SpaceX’s program after one of its Starship rockets broke apart midflight, reportedly damaging public property on Turks and Caicos in the Caribbean. The FAA diverted dozens of commercial airline flights following the explosion and announced an investigation into the incident, which is ongoing and being led by SpaceX. Musk, however, characterized the failure as “barely a bump in the road” and did not seem to indicate that the investigation would slow SpaceX’s launch cadence. Last year, the company indicated it was aiming for 25 launches of the Starship in 2025.FAA spokesperson Steven Kulm told WIRED that “the FAA is overseeing the SpaceX-led mishap investigation.” The FAA did not respond to further questions about whether the presence of SpaceX engineers at the agency would constitute a conflict of interest.In September, the FAA proposed$633,000 in fines following two 2023 incidents in which SpaceX allegedly did not follow its license requirements, violating regulations. Responding to an X user posting about the penalties, Musk wrote, “The fundamental problem is that humanity will forever be confined to Earth unless there is radical reform at the FAA!” Shortly afterward, Musk called for FAA head Mike Whitaker to resign.In January, more than three years before his term was due to end, Whitaker did resign.The White House did not immediately respond to a request for comment.SpaceX is directly regulated by a small FAA agency called the Office of Commercial Space Transportation, which since 1984 has licensed the launch of US space rockets. “The purpose is to ensure public safety,” says George Nield, a former associate administrator of the office. “People on the ground did not consent” to rocket launches above them, he says. ”We absolutely need to keep them safe. The office has done a great job of that.” The office oversaw 157 launches in 2024 alone.On February 10, several days after Musk posted on X that DOGE “will aim to make rapid safety upgrades to the air traffic control system,” a group of Democratic legislators wrote to Rocheleau—a career civil servant whose ties to the FAA go back to 1996—requesting information about any planned changes to FAA systems.“We are extremely concerned that an ad hoc team of individuals lacking any expertise, exposure, certifications, or knowledge of aviation operations being invited, or inserting themselves, to make ‘rapid’ changes to our nation’s air traffic systems,” they wrote. “Aviation safety is not an area to ‘move fast and break things.’”]]></content:encoded></item><item><title>Sweden Investigates New Cable Break Under Baltic Sea</title><link>https://www.nytimes.com/2025/02/21/world/europe/baltic-sea-cable-sweden.html</link><author>xnx</author><category>hn</category><pubDate>Fri, 21 Feb 2025 13:54:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The European Union vowed on Friday to increase security in the Baltic Sea as the Swedish authorities said they were investigating a new cable break, the latest example of damage to underwater infrastructure in the region.The severing of several undersea cables in the Baltic Sea in recent months has raised concerns that Russia is using the moves to retaliate against NATO countries that have supported Ukraine. Alliance officials have pointed to Russia as a possible culprit, but have said that it is difficult to prove.“We want to make sure Europe is equipped not only to prevent and detect sabotage to cables, but also to actively deter, repair and respond to any threat to critical infrastructure,” said Henna Virkkunen, the European Union official who announced the initiative. The recent episodes were of “great concern,” she said, adding that the bloc was taking “decisive action” to protect the cables.]]></content:encoded></item><item><title>Users don&apos;t care about your tech stack</title><link>https://www.empathetic.dev/users-dont-care-about-your-tech-stack</link><author>merkmoi</author><category>hn</category><pubDate>Fri, 21 Feb 2025 10:26:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I still often find myself in discussions where the main focus is the choice of technologies. Questions like “Is this language better than that one?” or “Is this framework more performant than the other?” keep coming up. But the truth is: users don’t care about any of that. They won’t notice those extra 10 milliseconds you saved, nor will their experience magically improve just because you’re using the latest JavaScript framework.What truly makes a difference for users is your attention to the  and their .Every programming language shines in specific contexts. Every framework is born to solve certain problems. But none of these technical decisions, in isolation, will define the success of your product from a user’s perspective.So, what should you focus on?Use what you enjoy working with.Use what challenges and motivates you to improve every day.Don’t get swept up by the hype. Sure, set aside time to explore new technologies that catch your interest, but don’t fall into the trap of thinking that building a great product requires the same stack featured in the latest YouTube tutorial from your favorite tech influencer.Learn to distinguish between tech choices that are  and those that are genuinely valuable for your product and your users. Finding the right balance is key to creating something truly impactful.There are no “best” languages or frameworks. There are only technologies designed to solve specific problems, and your job is to pick the ones that fit your use case—not because they’re trendy, but because they’re the right tool for the job.]]></content:encoded></item><item><title>Meta claims torrenting pirated books isn&apos;t illegal without proof of seeding</title><link>https://arstechnica.com/tech-policy/2025/02/meta-defends-its-vast-book-torrenting-were-just-a-leech-no-proof-of-seeding/</link><author>isaacfrond</author><category>hn</category><pubDate>Fri, 21 Feb 2025 10:01:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Evidence instead shows that Meta "took precautions not to 'seed' any downloaded files," Meta's filing said. Seeding refers to sharing a torrented file after the download completes, and because there's allegedly no proof of such "seeding," Meta insisted that authors cannot prove Meta shared the pirated books with anyone during the torrenting process.Whether or not Meta actually seeded the pirated books could make a difference in a copyright lawsuit from book authors including Richard Kadrey, Sarah Silverman, and Ta-Nehisi Coates. Authors had previously alleged that Meta unlawfully copied and distributed their works through AI outputs—an increasingly common complaint that so far has barely been litigated. But Meta's admission to torrenting appears to add a more straightforward claim of unlawful distribution of copyrighted works through illegal torrenting, which has long been considered established case-law.Authors have alleged that "Meta deliberately engaged in one of the largest data piracy campaigns in history to acquire text data for its LLM training datasets, torrenting and sharing dozens of terabytes of pirated data that altogether contain many millions of copyrighted works." Separate from their copyright infringement claims opposing Meta's AI training on pirated copies of their books, authors alleged that Meta torrenting the dataset was "independently illegal" under California's Computer Data Access and Fraud Act (CDAFA), which allegedly "prevents the unauthorized taking of data, including copyrighted works."Meta, however, is hoping to convince the court that torrenting is not in and of itself illegal, but is, rather, a "widely-used protocol to download large files." According to Meta, the decision to download the pirated books dataset from pirate libraries like LibGen and Z-Library was simply a move to access "data from a 'well-known online repository' that was publicly available via torrents."]]></content:encoded></item><item><title>Every .gov Domain</title><link>https://flatgithub.com/cisagov/dotgov-data/blob/main/?filename=current-full.csv&amp;sha=7dc7d24fba91f571692112d92b6a8fbe7aecbba2</link><author>KoftaBob</author><category>hn</category><pubDate>Fri, 21 Feb 2025 09:59:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Fly To Podman: a script that will help you to migrate from Docker</title><link>https://github.com/Edu4rdSHL/fly-to-podman</link><author>edu4rdshl</author><category>hn</category><pubDate>Fri, 21 Feb 2025 08:52:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Train Your Own O1 Preview Model Within $450</title><link>https://sky.cs.berkeley.edu/project/sky-t1/</link><author>9woc</author><category>hn</category><pubDate>Fri, 21 Feb 2025 08:42:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>On Running systemd-nspawn Containers (2022)</title><link>https://benjamintoll.com/2022/02/04/on-running-systemd-nspawn-containers/</link><author>cautious-fly</author><category>hn</category><pubDate>Fri, 21 Feb 2025 08:00:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[ is a container manager that allows you to run a full operating system or a command in a directory tree.  Conceptually, it is similar to the venerable , but it is much more secure.While s do perform filesystem isolation, they don’t provide any of the security benefits that s and  provide.  Additionally, they’re not easy to setup, unless, of course, you’re using a tool like  or ., on the other hand, gives you as much security and configuration as you would want and expect and is as easily configurable as better-known tools like Docker (although it operates at a lower-level).To create a container,  expects a root filesystem and optionally a  container configuration file, which of course brings to mind an OCI runtime bundle, because  is fully OCI compliant.  Those familiar with tools like  will be familiar with this requirement.One can use many of the same methods to get a root filesystem (rootfs) as documented in my article on .By using the machine option ( or ) with , the operating system tree (root filesystem) is automatically searched for in a couple places, most notably in , which is the recommended directory on the system.The intent of this article is to quickly and succinctly outline several ways to get started using containers with .  Hopefully, it will also encourage you to think more critically of tools like Docker and determine if they are as necessary as all the hype surrounding them would have you believe.We’ll be running the Tor browser in a container managed by .Note that the following assumptions are made: is already installed on your system.$ sudo apt install systemd-container
All the following examples will assume that the current working directory is .All commands are run as the  user to save typing  for every command. Container Settings FileWhat is a container settings file?  This is an optional INI-like file that contains startup configurations that will be applied to your container by the  container manager.  Any command-line option that is given to  can be put in the settings file, although the names will be different (see the docs).  Simply write them to the file and let  worry about the rest.  Not a bad deal, friend.If you’re familiar with  service files, then this will be familiar to you.The  container settings file is named after the container to which it is applied.  For instance, our container is called , so the file should be called .  That’s easy enough.Where should they go?  That’s a great question, geezer!The algorithm searches the following locations, in order:Persistent settings file should be placed in , and, unlike the non-privileged location (see below), every setting contained therein will take effect since this is a privileged location (i.e., only privileged users should be able to access any configs in the  directory).Do  put anything in  that you want to survive a reboot, as the  filesystem is temporary and any runtime data put there is placed in volatile memory.$ df /run --outputfstype
Type
tmpfs
However, any settings files found in the non-privileged  location will only have a subset of those settings applied.  As you may have guessed, any settings that grant elevated privileges or additional capabilities are ignored.  This is so untrusted or unvetted files downloaded from the scary Internet don’t cause undue harm and aren’t automatically applied upon container creation.In order for the Tor browser to be properly launched, the following  file must be installed in :This is equivalent to the following command line statement:$ sudo systemd-nspawn     --drop-capability all     --setenv DISPLAY:0     --hostname kilgore-trout     --no-new-privileges true     --private-users true     --as-pid2     --resolv-conf copy-host     --timezone copy     --user noroot     --directory tor-browser     bash -c Clearly, the settings file is much more convenient and allows us to start the container by simply typing:$ sudo systemd-nspawn --machine tor-browser
In addition, there are more parameters we can set, such as filtering system calls, bind mounts, overlay or union mount points, and much more, but that is out of the scope of this article.  And we haven’t even covered the  and  sections of the settings file.If an  settings file isn’t present, the container will still launch, but to a virtual shell.Let’s now look at some examples.Here is our old “friend” .  While Docker makes it easy to extract a container’s root filesystem as a tarball, it needs, well,  to do it.  That kinda sucks.I don’t know about you, but I don’t want multiple container technologies/runtimes/managers on my base system.  Since most distros are already using , the ability to create and run containers is already installed and just waiting for your fat little fingers to type the necessary commands.So, installing software additional software to run containers when you  have the ability to run containers is nonsensical.  It’s like installing an editor like Visual Studio Code when you already have Vim.I’ve been grudgingly using Docker in my personal projects for the sake of convenience, and it’s exactly why I am giddy about moving away from it.  Convenience is the scourge of understanding.Anyway, I digress.  Here is a very simple way to run the Tor browser as a  container:$ sudo mkdir tor-browser  docker export docker create btoll/tor-browser:latest    | tar -x -C tor-browser
$ sudo systemd-nspawn -M tor-browser
The Dockerfile used to create this container image is straightforward.Because of the convenience of the Dockerfile, Docker makes it easy to create a container with some provisioning already applied.However, as I’ll demonstrate next, it’s not any effort to create a shell script from the Dockerfile to do the same thing.  Shell scripts are some of our best friends!And after all, it’s pretty silly to install Docker only as a conduit for .  Wouldn’t it be better to learn other ways of getting a root filesystem?I’ve been using  for years.  It’s a really great way to quickly and easily bootstrap a  by downloading a root filesystem with optional packages.As mentioned in the previous example, I’ve created a shell script that provisions the container, and it’s a simple step to copy it into the new OS tree created by .To run the script, we’ll  into the container (well, what will  the container).$ sudo debootstrap     --archamd64     --variantminbase     bullseye     tor-browser     http://deb.debian.org/debian
$ sudo cp install_tor_browser.sh tor-browser/
$ sudo chroot tor-browser/
---

---
root@sulla:/# ./install_tor_browser.sh
root@sulla:/# exit
$ sudo systemd-nspawn --machine tor-browser
That was easy!  No big deal.If we want to share this with a friend or import it into another tool, we can export the container as a tarball and upload it to a server.  This can allow us to later download and create and run containers (the same concept as Docker Hub).$ sudo machinectl export-tar tor-browser tor-browser.tar.xz
After the container is tarred up, anyone that wants to use it can simply download it and run it without having to do any of the setup steps above (copying and installing).We’ll soon see an example of how we can pull that tarball down from a remote server.A tool by Lennart Poettering,  is an easy way to create an OSI (operating system image) or OS tree for use by  and any container technology that can “consume” a root filesystem.  Written in Python, it is well-documented (see its man page) and easy to use.There are many options and cool features but covering them is outside the scope of this article.Creating an OSI is easy.  Here you go:$ sudo apt install mkosi -y
$ sudo mkosi     --distribution debian     --release bullseye     --format gpt_ext4     --postinst-script install_tor_browser.sh
    --with-network     -o tor-browser.raw
$ sudo systemd-nspawn --machine tor-browser
Note that here I’m giving the  tool the  script as a value to the .  This saves us a couple of the steps that we had to do manually when using  in the previous example, namely:Copying the script from the host to the .We’re simply downloading the tarball from a previous example and running it as-is.  No need to re-run the Tor browser installation script, of course.$ sudo machinectl pull-tar     http://example.com/tor-browser.tar.xz     tor-browser
$ sudo systemd-nspawn     --resolv-conf copy-host     --machine tor-browser
Although unnecessary here, I included the  option here to show how easy it is to get a DNS resolver for containers that need one.I don’t use this much, but I’m adding it here for its usefulness.$ sudo machinectl pull-raw     http://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img     rootfs
$ sudo systemd-nspawn --machine rootfs
Spawning container rootfs on /var/lib/machines/rootfs.raw.
Press ^ three times within 1s to kill container.
root@rootfs:~#
Note that  can also build an image that you could use here as the subject of .This is not even close to a comprehensive list.  For example, you can copy files to and from a running container, but I haven’t any examples for that.As always, read the docs.As we’ve already seen, we can easily export a container’s root filesystem as a tarball.  Then, simply upload this to an accessible storage area for other people and processes.This is the same workflow that has been around for hundreds of thousands of years.$ sudo machinectl export-tar tor-browser tor-browser.tar.xz
Also, export as image: $ machinectl list
MACHINE      CLASS     SERVICE        OS     VERSION ADDRESSES
tor-browser  container systemd-nspawn debian       -
ubuntu-focal container systemd-nspawn ubuntu 20.04   -
list-images
       Show a list of locally installed container and VM images.  This enumerates all raw
       disk images and container directories and subvolumes in /var/lib/machines/ and
       other search paths, see below. Use start see above to run a container off one
       of the listed images.  Note that, by default, containers whose name begins withs
       a dot  are not shown. To show these too, specify --all. Note that a special
       image  always implicitly exists and refers to the image the host itself is
       booted from.
$ machinectl list-images
NAME        TYPE      RO USAGE CREATED MODIFIED
hugo        directory no   n/a n/a     n/a
tor-browser directory no   n/a n/a     n/a

 images listed.
List all containers without the header and footer:$ machinectl list-images --no-legend
hugo        directory no n/a n/a n/a
tor-browser directory no n/a n/a n/a
Downloading and exporting can take a while.  Let’s check the status!$ sudo machinectl list-transfers
ID PERCENT TYPE       LOCAL       REMOTE
      n/a export-tar tor-browser

  transfers listed.
Querying the Container Status$ sudo machinectl status tor-browser
tor-browser88544b92092430bc5d3fbbffc12a2f04
           Since: Fri 2022-02-04 19:54:28 EST; 4h 29min ago
          Leader: sd-stubinit
         Service: systemd-nspawn; class container
            Root: /var/lib/machines/tor-browser
              OS: Debian GNU/Linux bullseye
            Unit: machine-tor2dbrowser.scope
                  ...
When you’re  sure that you’re done with it, you can remove both the machine and the  service file in one fell swoop:$ sudo machinectl remove tor-browser
$ sudo systemd-nspawn -M tor-browser --quiet uname -a
Linux kilgore-trout 5.11.0-49-generic 

$ sudo systemd-nspawn -M tor-browser --quiet du -hs
264M    .

$ sudo systemd-nspawn -M tor-browser --quiet cat /etc/os-release
PRETTY_NAME
NAME
VERSION_ID
VERSION
VERSION_CODENAMEbullseye
IDdebian
HOME_URL
SUPPORT_URL
BUG_REPORT_URL

$ sudo systemd-nspawn -M tor-browser --quiet df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/nvme0n1p2  468G   61G  384G  14% /
tmpfs           1.6G       1.6G   0% /tmp
tmpfs           4.0M       4.0M   0% /dev
tmpfs           1.6G       1.6G   0% /dev/shm
tmpfs           3.1G   12K  3.1G   1% /run
tmpfs           1.6G  1.9M  1.6G   1% /run/host/incoming
tmpfs           4.0M       4.0M   0% /sys/fs/cgroup
Using the amazing command-line fuzzy finder tool (), I wrote a simple  function that will list all of the machine images in  and allow you to select one.  Once the selection is made, it will create and launch the container:nspawn
    sudo systemd-nspawn --machine machinectl list-images --no-legend | awk  | fzf        --quiet
This article could also be called "On Getting Rid of Docker", since that it is one of my goals.  After all, if you’re running a Linux distro, chances are that the init system is , so why not use ?There’s no need to install  and , which Docker needs and installs by default.  I don’t have anything against them, mind you, and  may not be the best tool for the job.Unfortunately, though, most developers don’t even know that there are options outside of Docker, or that they’re not as “convenient”.  Hopefully, this article has disabused some of that notion.]]></content:encoded></item><item><title>Docker limits unauthenticated pulls to 10/HR/IP from Docker Hub, from March 1</title><link>https://docs.docker.com/docker-hub/usage/</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 21 Feb 2025 07:42:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Starting April 1, 2025, all users with a Pro, Team, or Business
subscription will have unlimited Docker Hub pulls with fair use.
Unauthenticated users and users with a free Personal account have the
following pull limits:Unauthenticated users: 10 pulls/hourAuthenticated users with a free account: 100 pulls/hourThe following table provides an overview of the included usage and limits for each
user type, subject to fair use:Number of public repositoriesNumber of private repositories10 per IPv4 address or IPv6 /64 subnetFor more details, see the following:When utilizing the Docker Platform, users should be aware that excessive data
transfer, pull rates, or data storage can lead to throttling, or additional
charges. To ensure fair resource usage and maintain service quality, we reserve
the right to impose restrictions or apply additional charges to accounts
exhibiting excessive data and storage consumption.Docker Hub has an abuse rate limit to protect the application and
infrastructure. This limit applies to all requests to Hub properties including
web pages, APIs, and image pulls. The limit is applied per IPv4 address or per
IPv6 /64 subnet, and while the limit changes over time depending on load and
other factors, it's in the order of thousands of requests per minute. The abuse
limit applies to all users equally regardless of account level.You can differentiate between the pull rate limit and abuse rate limit by
looking at the error code. The abuse limit returns a simple  response. The pull limit returns a longer error message that includes
a link to documentation.]]></content:encoded></item><item><title>US Judge invalidates blood glucose sensor patent, opens door for Apple Watch</title><link>https://www.patentlyapple.com/2025/02/a-federal-judge-has-invalidated-an-omni-medsci-patent-which-could-open-the-door-for-a-blood-glucose-solution-for-apple-watch.html</link><author>walterbell</author><category>hn</category><pubDate>Fri, 21 Feb 2025 05:55:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DeepSeek Open Infra: Open-Sourcing 5 AI Repos in 5 Days</title><link>https://github.com/deepseek-ai/open-infra-index</link><author>ahsmha_</author><category>hn</category><pubDate>Fri, 21 Feb 2025 04:24:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Please Commit More Blatant Academic Fraud (2021)</title><link>https://jacobbuckman.com/2021-05-29-please-commit-more-blatant-academic-fraud/</link><author>jxmorris12</author><category>hn</category><pubDate>Fri, 21 Feb 2025 03:53:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Explicit academic fraud is, of course, the natural extension of the sort of mundane, day-to-day fraud that most academics in our community commit on a regular basis.
Trying that shiny new algorithm out on a couple dozen seeds, and then only reporting the best few.
Running a big hyperparameter sweep on your proposed approach but using the defaults for the baseline.
Cherry-picking examples where your model looks good, or cherry-picking whole datasets to test on, where you’ve confirmed your model’s advantage.
Making up new problem settings, new datasets, new objectives in order to claim victory on an empty playing field.
Proclaiming that your work is a “promising first step” in your introduction, despite being fully aware that nobody will ever build on it.
Submitting a paper to a conference because it’s got a decent shot at acceptance and you don’t want the time you spent on it go to waste, even though you’ve since realized that the core ideas aren’t quite correct.The problem with this sort of low-key fraud is that it’s insidious, it’s subtle.
In many ways, a fraudulent action is indistinguishable from a simple mistake.
There is plausible deniability – oh, I simply forgot to include those seeds, I didn’t have enough compute for those other ablations, I didn’t manage to catch that bug.
It’s difficult to bring ourselves to punish a well-meaning grad student for something that could plausibly have been a simple mistake, so we let these things slide, and slide and slide until they have become normalized.
When standards are low, it’s to no individual’s advantage to hold themselves to a higher bar.
Newcomers to the field see these things, they learn, and they imitate.
Often, they are directly encouraged by mentors.
A graduate student who publishes three papers a year is every professor’s dream, so strategies for maximal paper output become lab culture.
And when virtually every lab endorses certain behaviors, they become integral to the research standards of the field.But worst of all: because everybody is complicit in this subtle fraud, nobody is willing to acknowledge its existence.
Who would be such a hypocrite as to condemn in others, behaviors they can see clearly in themselves?
And yet, who is willing to undermine their own achievements by admitting that their own work does not have scientific value?
The sad result is that, as a community, we have developed a collective blind-spot around a depressing reality: even at top conferences, the median published paper contains no truth or insight.
Any attempts to highlight or remedy the situation are met with harsh resistance from those who benefit from the current state of affairs.
The devil himself could not have designed a better impediment to humanity’s progression.But now that blatant academic fraud is in the mix, the AI community has a fighting chance.
By partaking in a form of fraud that has left the Overton window of acceptability, the researchers in the collusion ring have finally succeeded in forcing the community to acknowledge its blind spot.
For the first time, researchers reading conference proceedings will be forced to wonder: does this work truly merit my attention?
Or is its publication simply the result of fraud?It would, of course, be quite difficult to actually distinguish the papers published fraudlently from the those published “legitimately”.
(That fact alone tells you all you really need to know about the current state of AI research.)
But the mere  that any given paper was published through fraud forces people to engage more skeptically with  published work.
Readers are forced to act more like reviewers, weighing the evidence presented against their priors, attempting to identify ways in which surprising conclusions could be the result of fraud – explicit or subtle – rather than fact.
People will apply additional scrutiny to deal with explicit forms of fraud like collusion rings, but in doing so will also develop a sensitivity to the more subtle forms of fraud that are already endemic to the community.
This will, in turn, put pressure on authors to produce work which can withstand such scrutiny; results obtained without any fraud at all, leading to publications with genuine scientific merit.That same harsh light is also cast on ourselves, on our motivations.
This situation seems to have evoked in many researchers feelings of empathy.
These actions are understandable; such an occurrence was inevitable; everyone does this, just more discreetly.
We are not surprised that people behave unscientifically in order to get their papers published; we are surprised that someone was willing to take it this far.
The most notable thing about the collusion ring is not that it was more fraudulent than is typical, but that the fraud was more .This surfaces the fundamental tension between good science and career progression buried deep at the heart of academia.
Most researchers are to some extent “career researchers”, motivated by the power and prestige that rewards those who excel in the academic system, rather than idealistic pursuit of scientific truth.
Well-respected senior figures are no exception to this. (In fact, due to selection effects, I suspect most are actually  career-motivated than is typical.)
We must come to terms with the fact that, since the incentives for career progression are not perfectly aligned with good science, almost any action motivated by career progression will interfere with one’s ability to do good science.
We must encourage a norm of introspection, of probing one’s own motivations, where any decisions made to obtain science-adjacent benefits are viewed with the deepest suspicion.
And we must ensure that explicit suggestions to modify one’s science in the service of one’s career – “you need to do X to be published”, “you need to publish Y to graduate”, “you need to avoid criticizing Z to get hired” – carry social penalties as severe as a suggestion of plagiarism or fraud.Prof. Littman says that collusion rings threaten the integrity of computer science research.
I agree with him.
And I am looking forward to the day they make good on that threat.
Undermining the credibility of computer science research is the best possible outcome for the field, since the institution in its current form does not deserve the credibility that it has.
Widespread fraud would force us to re-strengthen our community’s academic norms, transforming the way we do research, and improving our collective ability to progress humanity’s knowledge.So this is a call to action: please commit more academic fraud. fraud.
 fraud.
Form more collusion rings!
Blackmail your reviewers, bribe your ACs!
Fudge your results – or fabricate them entirely!
(But don’t skimp on the writing: your paper needs to be written in perfect academic English, formatted nicely, and tell an intuitive, plausible-sounding story.)
Let’s make explicit academic fraud commonplace enough to cast doubt into the minds of every scientist reading an AI paper.
Overall, science will benefit.Together, we can force the community to reckon with its own shortcomings, and develop stronger, better, and more scientific norms.
It is a harsh treatment, to be sure – a chemotherapy regimen that risks destroying us entirely.
But this is our best shot at destroying the cancer that has infected our community.
I believe with all my heart that we can make it through this challenge and emerge stronger than ever.
And when we do, the secrets to artificial intelligence will be waiting.Thanks for reading. Follow me on Substack for more writing, or hit me up on Twitter @jacobmbuckman with any feedback or questions!Many thanks to Nitarshan Rajkumar for his feedback when writing this post.]]></content:encoded></item><item><title>The Shape of a Mars Mission</title><link>https://idlewords.com/2025/02/the_shape_of_a_mars_mission.htm</link><author>feross</author><category>hn</category><pubDate>Fri, 21 Feb 2025 03:01:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[“From a mathematics and trajectory standpoint and with a certain kind of technology, there’s not too many different ways to go to Mars.  It’s been pretty well figured out. You can adjust the decimal places here and there, but basically if you're talking about chemical rockets, there's a certain way you're going to go to Mars.” - John AaronUnlike the Moon, which hangs in the sky like a lonely grandparent waiting for someone to visit, Mars leads a rich orbital life of its own and is not always around to entertain the itinerant astronaut. There is just one brief window every 26 months when travel between our two planets is feasible, and this constraint of orbital mechanics is so fundamental that we’ve known since Lindbergh crossed the Atlantic what a mission to Mars must look like.Using chemical rockets, there are just two classes of mission to choose from: (The durations I give here can vary, but are representative).



: Spend six months flying to Mars, stay for 17 months, spend six months flying back  (~1000 days total).  This is sometimes called a conjunction class mission.  This profile trades a simple out-and-back trajectory for a long stay time at Mars. : Spend six months flying to Mars, stay for 30-90 days, spend 400 days flying back (~650 days total).  This is also called an opposition class mission. This profile trades a short Martian stay time for a long and frankly terrifying trip home through the inner solar system.Before comparing the merits of each, it’s worth stressing what they have in common—both are , more than double the absolute record for space flight (438 days), five times longer than anyone has remained in space without resupply (128 days), and about ten times humanity’s accumulated time beyond low Earth orbit (82 days).  It is this inconvenient length, more than any technical obstacle, that has kept us from going to Mars since rockets capable of making the trip first became available in the 1960's. And because this length is set by the relative motions of the planets, it’s resistant to attack by technology.   You can build rockets that go faster, but unless you make  go faster, you’ll mostly end up trading transit time for longer stay times.  Getting a round trip below the 500 day mark requires fundamental breakthroughs in either propulsion or refueling. Delta-v requirements for short stay missions of varying length (left) and a long-stay mission (orange line right) for comparison. Note the sharp jump at around 500 days. source. 


That’s the bad news. The good news is that these constraints are so strong that we can say a lot about going to Mars without committing to any particular spacecraft or mission design.  Just like animals that live in the sea are likely to have good hearing and a streamlined body shape, there are things that have to hold true for any Mars-bound spacecraft, just from the nature of the problem.


A trip to Mars will be commital in a way that has no precedent in human space flight.  The moon landings were designed so that any moment the crew could hit the red button and return expeditiously to Earth; engineers spent the brief windows of time when an abort was infeasible chain smoking and chewing on their slide rules. But within a few days of launch, a Mars-bound crew will have committed to spending years in space with no hope of resupply or rescue.  If something goes wrong, the only alternative to completing the mission will be to divert into a long, looping orbit that gets the spacecraft home about two years after departure.   And if they get stuck on Mars, astronauts will find themselves in a similar position to the early Antarctic explorers, able to communicate home by radio, but forced by unalterable cycles of nature to wait months or years for a rescue ship. 

Delta-v in km/sec required to return to Earth in 50, 70, and 90 days from various points in a long-stay Mars mission. Values above 10 km/sec are not realistic at our current technology level. sourceThe effect of this no-abort condition is to make Mars mission design acutely risk-averse. You can think of flying to Mars like one of those art films where the director has to shoot the movie in a single take. Even if no scene is especially challenging, the requirement that everything go right sequentially, with no way to pause or reshoot, means that even small risks become unacceptable in the aggregate. 

To get a feel for this effect, consider a toy model where we fly to Mars on a 30 month mission.  Every month there is a 3% chance that a critical system on our spacecraft will fail, and once that happens, the spacecraft enters a degraded state, with a 5% chance every month that a subsequent failure kills the crew. 

In this model,  the probability that the crew gets home safely works out to 68%.  But if we add an abort option that can get them home in six months, that probability jumps to 85%.  And with a three month abort trajectory, the odds of safe return go up to 92%. 

These odds are notional, but they demonstrate how big an effect the absence of abort options can have on safety.This necessary risk aversion introduces a tension into any Mars program. What’s the point of spending a trillion dollars to send a crew if they’re going to cower inside their spacecraft?  And yet since going outside is one of the most dangerous things you can do on Mars, early missions have to minimize it. The first visitors to Mars will have to land in the safest possible location and do almost nothing. 


Risk is closely tied with the next issue, reliability.



The closest thing humanity has built to a Mars-bound spacecraft is the International Space Station.  But ‘reliable’ is not the first word that leaps to the lips of ISS engineers when they talk about their creation—not even the first printable word. Despite twenty years of effort, equipment on the station breaks constantly, and depends on a stream of replacement parts flown up from Earth.A defective heat exchanger packaged for return to Earth from ISS in 2023Going to Mars will require order of magnitude reliability improvements over the status quo. Systems on the spacecraft will need to work without breaking, or at least break in ways the crew can fix.  If there’s an emergency, like a chemical leak or a fire, the crew must be able to live for years in whatever’s left of the ship.  And the kind of glitches that made for funny stories in low Earth orbit (like a urine icicle blocking the Space Shuttle toilet) will be enough to kill a Mars-bound crew.

Complicating matters is that traditional reliability engineering practices don’t work in life support, where everything is interconnected, often through the bodies of the crew.  Life support engineering is much more like keeping a marine aquarium than it is like building a rocket. It’s not easy to untangle cause from effect, the entire system evolves over time, and there’s a lot of “spooky action at a distance” between subsystems that were supposed to be unrelated.  Indeed, failures in life support have a tendency to wander the spacecraft until they find the most irreplaceable thing to break.

Nor is it possible to brute-force things by filling the spacecraft with spare parts. The same systemic interactions that damage one component can eat through any number of replacements. The bedrock axiom of reliability engineering—that complex designs can be partitioned into isolated subsystems with independent failure rates—does not hold for regenerative life support.

The need for long and expensive test flights to validate life support introduces another kind of risk aversion, this time in the design phase.  With prototypes needing to be flown for years in space, there will be pressure to freeze the life support design at whatever point it becomes barely adequate, and no amount of later innovation will make it onto the spacecraft.  

This is a similar dynamic to one that afflicted the Space Shuttle, a groundbreaking initial design so expensive to modify that it froze the underlying technology at the prototype phase for thirty years.  In that period we learned nothing about making better space planes, but burned through decades and billions of dollars patching up the first working prototype.
 
Such timorousness goes against the grain of a development strategy that proven spectacularly successful in recent years for SpaceX, an approach you could call “fly often and try everything”.  With hardware to spare, SpaceX is not afraid to make wholesale changes between tests of its Starship rocket, relying on rapid iterations to advance the state of the art at an exhilarating pace.

But this Yosemite Sam approach to testing won’t work for Mars. It only takes a few hours for engineers to collect the data they need after a Starship launch, while test runs of Mars-bound systems will last for years.  The inevitable outcome is a development program that looks an awful lot like NASA, with long periods of fussing and analysis punctuated by infrequent, hideously expensive test flights. 

Autonomy is a concept alien to NASA, which has been micromanaging astronauts from the ground since the first Mercury astronaut had to beg controllers for  permission to pee (the request went all the way up the reporting chain to Wernher Von Braun).  

To this day, missions follow a test pilot paradigm where the crew works from detailed checklists prepared for them months or years in advance.  On the space station, this takes the form of a graphical schedule creeping past a red vertical line on a laptop screen, with astronauts expected to keep pace with the moving colored boxes.  Most routine work on the space station (like pumping water or managing waste heat) is relegated to specialized teams on the ground and is not even visible to the crew. 

Alan Shepard aboard Freedom 7, explaining that he really has to go pretty bad.But as a Mars-bound spacecraft gets further from Earth, the round-trip communications delay with ground control will build to a maximum of 43 minutes, culminating in a week or more of communications blackout when the Sun is directly between the two planets.  This physical constraint means that  the crew has to have full control over every system on the spacecraft, without help from the ground.

Autonomy sounds like a good thing! Who wants government bean-counters deciding how astronauts spend their space time? But the ground-driven paradigm has its advantages, most notably in limiting workload. The ISS is run by a staff of hundreds who together send some 50,000 commands per day to the station. The seven astronauts on board are only called in as a last resort, and even so the demands on their time are so great that the station has struggled to perform its scientific mission.One benefit of NASA’s backseat driving has always been that in an emergency, the crew has access to unlimited real-time expert help on Earth. The starkest illustration of this came on Apollo 13, when an oxygen tank in the service module ruptured 56 hours into the flight.  It took the crew and mission controllers nearly an hour to get their bearings, at which point there was only a short window of time left to power down the spacecraft in a way that would preserve their ability to return to Earth.  

A transcript of that first hour shows how difficult it was for crew and ground to figure out what was happening, and prioritize their response. It casts no aspersions on the crew of Apollo 13 to say they could not have survived a Mars-like communications delay.  

And while this mission is the most famous example of ground controllers backstopping an Apollo crew, there were at least five more occasions in the Apollo program when timely help from the ground averted serious trouble:

Apollo 12 was hit twice by lightning after launch, scrambling the electrical system and lighting up the command module with warning lights. Flight controller John Aaron recognized the baffling error pattern and passed into NASA legend by telling the crew to flip an obscure switch that restored sanity to their displays.On Apollo 14, the descent radar on the lunar module failed to lock on properly, returning spurious range data. Without a timely call from ground control (who told the pilot to reset a breaker), the problem would likely have led to an aborted landing. On Apollo 15, the crew struggled to contain a water leak that threatened to become serious. After fifteen minutes, engineers on the ground were able to trace the problem to a pre-launch incident with a chlorination valve and relay up a procedure that solved the problem.Also on Apollo 15, a sliver of loose metal floating in a switch caused an intermittent abort signal to be sent to the lunar module engine.  Suppressing the signal so the lunar module could descend safely required reprogramming the onboard computer in a procedure guaranteed to raise the hairs on the head of every modern software developer.On Apollo 16, a pair of servo motors on the service module failed in lunar orbit. Mission rules called for an abort, but after some interactive debugging with the command module pilot, ground controllers found a workaround they judged safe enough to continue with the landing.While these incidents stand out, Apollo transcripts reveal numberless other examples of crew and ground working closely to get on top of problems. The loss of this real-time help is a real risk magnifier for astronauts going to Mars. 

Another way in which the ISS depends on Earth is for laboratory analysis of air and water samples, which are collected on a regular schedule and sent down with each returning capsule. The tests that can be performed on the station itself are rudimentary, alerting crew to the presence of microbes or contaminants, but without the detailed information necessary to trace a root cause.

For Mars, this analytic capability will have to move into the spacecraft. In essence, this means building a kind of Space Theranos, an automated black box that can perform biochemical assays in space without requiring repair or calibration. Such an instrument doesn’t exist anywhere, but a Mars mission requires two flavors of it—one that works in zero G, and another for Martian gravity.This black box belongs to a category of hardware that pops up a lot in Mars plans: technologies that would be multibillion dollar industries if they  existed on Earth, but are assumed to be easy enough to invent when the time comes to put them on a Mars-bound spacecraft.  Some Mars boosters even cite these technologies as examples of the benefits going to Mars will bring to humanity.  But this gets things exactly backwards—problems that are hard on Earth don’t get easier by firing them into space, and the fact that nonexistent technologies are on the critical path to Mars is not an argument for going there.

The requirement that the crew be able to handle the ship when some members are incapacitated and there is no communication with Earth means that an ISS-size workload has to be automated to the point where it can be run by two or three astronauts. 

Astronaut Alexander Gerst (right) interacting with CIMON, NASA's $6 million AI chatbotAutomation means software, and lots of it.  To automate the systems on a Mars-bound spacecraft will be a monumental task, like trying to extend the autopilot on an airliner to make it run the airport concession stands, baggage claim, and airline pension plan.  The likely outcome is an ISS-like hotchpotch of software tested to different levels of rigor, running across hundreds of processors. But this hardware will be exposed to a far harsher radiation environment than systems on the ISS, making software design and integration a particular challenge.

A special case of the automation problem comes up on long-stay missions, when  the orbiting spacecraft has to keep itself free of mold, fungus, and space raccoons for the year and a half that the crew are on the Martian surface. Anyone who owns a vacation home knows that this problem—called “quiescence” in the Mars literature—is already hard to solve on Earth. 

Unless carefully managed, the interplay between automation, complexity and reliability can enter a pathological spiral.   Adding software to a system makes it more complex. To stay reliable, complex systems have to degrade gracefully, so that the whole continues to function even if an individual component fails. But these degraded modes, as well as unexpected interactions between them, introduce their own complexity, which then has to be managed with software, and so on. 

The upshot is that automation introduces its own, separate reason for running full-length mock missions before actually going to Mars. There will be too many bugs in a system this complex to leave them all for the first Mars-bound crew to discover.

The extreme requirements for autonomy, reliability, and automation I’ve outlined are old news to designers of deep-space probes. The solar system is full of hardware beeping serenely away decades after launch, most spectacularly the forty-six-year-old Voyager spacecraft. 

But  no one has ever tried attaching a box of large primates to a deep space probe with the goal of keeping them alive, happy, and not tweeting about how NASA sent them into the vast empty spaces to die.  A Mars-bound spacecraft will be the most complicated human artifact ever built, about a hundred times bigger than any previous space probe, and inside it will be a tightly-coupled system of software, hardware, bacteria, fungi, astronauts, and (for half the mission) whatever stuff the crew tracks with them back onto the spacecraft.

Designing such a machine means taking something at the ragged edge of human ability (building interplanetary probes) and combining it with something that we can’t even do yet on Earth (keep a group of six or eight humans alive for years with regenerative life support).My argument is not that it is impossible to do this, but that it is impossible to do it quickly.  Preparing for Mars will be an iterative, open-ended undertaking in which every round of testing eats up years of time and most of our space budget, like Artemis and the ISS before it.  The first decade of a Mars program will be indistinguishable from the last forty years of space flight—a series of repetitive, long-duration missions to orbit. The only thing NASA will need to change is the program name. 

Nor is this a problem that can be delegated to billionaire hobbyists. Life support is going to be a grind no matter whose logo is on the rocket.  The sky could be thick with Starships and we’d still be stuck doing all-up trials of hardware and software on these multi-year missions to nowhere.  

The only way to explore Mars in our lifetime is to ditch the requirement that people accompany the machinery.

But since we’re determined to go to Mars, and have two profiles to choose from, which one is better? 

Everyone agrees that only the long-stay profile makes sense for exploration. There’s no point in spending 95% of the trip in transit just to get a rushed couple of weeks at the destination. 

But on early missions, where the goal is just to get the crew home alive, the choice is tricky.

The virtue of the long stay profile is simplicity. You fly your rocket to Mars, wait 17 months for the planets to align, and then fly the same trajectory home.  Each leg of this transfer journey lasts about as long as an ISS deployment, and it’s possible to tweak the transfer time by burning more fuel (although the crew then has to stay longer on Mars to compensate). 

At every point in the mission, the ship remains between 1 AU and 1.5 AU from the Sun. This simplifies thermal and solar panel design and greatly reduces the risk to the crew from solar storms.  

But the problem of what to do with all that time on Mars is vexing.  500 days is a long time for a first stay anywhere, even someplace with nightlife and an atmosphere.   And as we’ll see, an orbital mission is probably out of the question.  The requirement that the crew go live on Mars on their first visit adds enormously to the level of risk. 

The appeal of the short stay profile is right in the name. Instead of staying on Mars so long they have to file taxes, the first arrivals can plant the flag, grab whatever rock is nearest the ladder, and get the hell out of there.  Or they can choose to skip the landing and make the first trip strictly orbital, following a tradition in aerospace engineering of attempting the impossible sequentially instead of all at once.

But the problem with the short stay profile is that trip home. The return trajectory cuts well inside the orbit of Venus, complicating the design of the spacecraft and adding spectacular ways for the crew to die during the weeks near perihelion. For most of that journey, the ship is on the wrong side of the Sun, hampering communications with Earth while leaving the crew with no warning of solar storms.

And that crew has to spend two consecutive years in deep space, maximizing their exposure to radiation and microgravity, the biggest known risks to astronaut health. 

The short stay profile also requires more propellant, in some years a prohibitive amount. If your strategy for mitigating risk on Mars is to launch crews during every synodic period, so that there are always potential rescuers en route to Mars, then this is a problem.  
￼
A diagram comparing the delta-v requirements for short stay and long stay missions across future launch dates. Since propellant requirements go up exponentially with delta v, a mission in 2041 requires five times as much propellant as one in 2033. source“
Once you’ve picked a profile, the other decision to make is whether to land the spacecraft. 

Obviously you have to land a crew at some point; if you don’t, the other space programs will make fun of you, and there will be hurtful zingers at your Congressional hearing. 

But since surviving a trip to Mars requires tackling a sequence of unrelated problems (arrival, entry, landing, surface operations, ascent, rendezvous), there is a case for cutting the problem in half by making the first mission orbital. This was the approach taken by the Apollo program, which looped the first crew around the Moon before a working lunar lander existed.

Not having to carry a lander on the first mission means more room for spare parts and consumables, which improves the margin of safety for the crew.  It also buys time for engineers to work on the hard problems of entry, landing, quiescence, and ascent without holding back the entire program. 

But there are powerful arguments against an orbital mission. Since so much of the risk in going to Mars is a simple function of time, why roll the dice more than necessary? And given the expense and physical toll on crew,  how do you justify not attempting a landing?  Imagine driving to Disneyland, turning the car around in the parking lot, and announcing to your family that you’re now ready for the real trip next year.  There will be angry kicking from the backseat, and mutiny.  
 
NASA has waffled for years over which option to choose.  In the 2009 design reference architecture, they favored sending a crew of four on the long stay trajectory.  Their more recent plans envision a shoestring mission on a short-stay profile with four crew members, two of whom attempt a landing. 

Elon Musk, for his part, has proposed solving the problem in stages, sending volunteers to settle Mars first, then figuring out how to get them home later.What makes the choice genuinely hard is that we lack answers to two key questions:

1. How does the human body respond to partial gravity?Decades in space have given us a good idea of what prolonged periods in free-fall do to astronauts, and how they recover after returning to Earth.   But we have no idea what happens in partial gravity, either on the Moon (0.16 g) or on Mars (0.38 g). In particular, we don’t know whether Martian gravity is strong enough to arrest or slow the degenerative processes that we observe in free fall.The answer to this question will drive a key decision: whether or not to spin the spacecraft. As we’ll see, spinning a spacecraft to create artificial gravity is an enormous hassle, but whether it’s avoidable depends on the unstudied effects of long stays in partial gravity.2. What is the risk to the crew from the heavy-ion component of galactic cosmic radiation?Radiation in space comes in many varieties, most of which are well-understood from experience with their analogues on Earth.  

Low-dose heavy-ion radiation, however, is different.  It doesn’t exist outside of particle accelerators on Earth and is hard to study in low orbit, where both the magnetosphere and the bulk of our planet shield astronauts from most of the flux they’d experience in free space.  

Heavy ion radiation has biological effects that are not captured by the standard model of radiation damage to tissue. In particular, there is a class of phenomena called non-targeted effects (NTEs) that are known to damage cells far from the radiation track. This is a weird effect, like if found yourself hospitalized because your neighbor got hit by a car.  It’s believed that NTEs disrupt epigenetic signaling mechanisms in cells, but the phenomenon is poorly understood.

Uncertainty about the effects of low-dose heavy ion radiation widens our best guess at radiation risk by at least a factor of two. At the low end of the range, these effects are just a curiosity, and Mars missions can be planned using traditional models of radiation exposure. At the high end of the range, long-duration orbital missions may not be survivable, and astronauts on the Martian surface will either have to live in a cave or cover their shelter with meters of soil.

Prediction of tumor prevalence after 1 year of galactic cosmic radiation exposure. The solid line at bottom shows the standard radiation model (TE). The dotted lines show the influence of non-targeted effects (NTE) under different assumptions. Note the nearly threefold uncertainty in predicted tumor prevalence in the unshielded case. sourceThis uncertainty about biological effects makes radiation the greatest uncharacterized known risk facing a Mars-bound crew, and it affects every aspect of mission design. 

It’s helpful to combine the three main risk factors in going to Mars into one big chart:

￼Spacecraft trajectory complicates spacecraft design, communications are a challenge.Requires working lander and ascent stage, less margin than orbital mission.
Lowest complexity, large mass budget for spares and consumables. Highest complexity, all-up mission must work on the first try.600 days in deep space, return trip requires close solar approach (0.7 AU).  Risk from solar particle events may require flying near solar minimum, incurring higher GCR dose.

Risk of death or incapacitation from heavy ion component of GCR may exceed 50%
Lowest radiation exposure, but adequately shielding the habitat on Mars increases complexity and contamination risk 
1.5 times beyond human endurance record; crew at risk for bone fractures and eye damage.


2.5 times beyond human endurance record.
Physiological effects of partial gravity unknown. 

The gray areas in these grids represent knowledge gaps that have to be filled before we decide how to go to Mars.


How long this preliminary medical research would take is anyone’s guess, but it has to be some multiple of the total mission time.  Studying partial gravity in particular is tricky—you can do it on the Moon (42% of martian gravity) and hope the results extend to Mars, or you can build rotating structures in space and do more precise tests there.

Studying radiation effects means flying animals outside the magnetosphere for a few years and then watching them for tumors, which (unless the radiation news is really bad) is also going to take some time.

In software engineering we have a useful concept called “yak shaving”. To get started on a project you must first prepare your tools, which often involves reconfiguring your programming environment, which may mean updating software, which requires finding a long-disused password, and pretty soon you find yourself under the office chair with a hex wrench. (The TV show Malcolm in the Middle has a beautiful illustration of yak shaving in the context of home repair.) 

The same phenomenon afflicts us in trying to go to Mars. It would be one thing if, given enough rockets and money, explorers could climb on a spaceship and go. But there is always this chain of necessary prerequisites. We paint  on the side of our spaceship and then find ourselves in low Earth orbit a decade later, centrifuging mice. It’s dispiriting.


It’s tempting to say “you can just build things” and dismiss all this research and testing as timid and unnecessary. But this would mean ignoring the biggest risk factor for Mars, which I’ll include here for the sake of completeness.

A trip to Mars is so difficult that we don’t have the luxury of ignoring known risks—we need all the room we can spare in our risk budget for the things we don’t know to worry about yet.

My goal in all this is not to kill a cherished dream, but to try to push people to a more realistic view of what it means to commit to a Mars landing, and in particular to think about going to Mars in terms of opportunity costs.

In recent years, there’s been a remarkable division in space exploration.  On one side of the divide are missions like Curiosity, James Webb, Gaia, or Euclid that are making new discoveries by the day. These projects have clearly defined goals and a formidable record of discovery. 

On the other side, there is the International Space Station and the now twenty-year old effort to return Americans to the moon. These projects have no purpose other than perpetuating a human presence in space, and they eat through half the country’s space budget with nothing to show for it.  Forget even Mars—we are further from landing on the Moon today than we were in 1965.

In going to Mars, we have a choice about which side of this ledger to be on. We can go aggressively explore the planet with robots, benefiting from an ongoing revolution in automation and software to launch ever more capable missions to the places most likely to harbor life.  

Or we can stay on the treadmill we’ve been on for forty years,  slowly building up the capacity to land human beings on the safest possible piece of Martian real estate, where they will leave behind a plaque and a flag.  But we can’t do both.

Next time: Eyes and Bones

 For an early example, see the 1928 Scientific American article, “Can we go to Mars?”, While understandably hand-wavy about the means of propulsion, it describes a conjunction-class orbital mission not substantially different from NASA’s 2009 Design Reference Architecture.

 Valerii Polyakov set the 437 day record on a space flight that landed in 1995. The International Space Station went without resupply from Nov 25, 2002 to April 2, 2003. Nine Apollo missions went beyond low Earth orbit, the longest of these (Apollo 17) was gone 12.4 days.

 The Saturn V was capable of launching about 20 tons on a Mars flyby trajectory. NASA undertook preliminary planning for such a mission (requiring four Saturn V launches) in 1967.  

 In 1987 a team chaired by Sally Ride proposed a ‘split/sprint’ mission architecture that is probably the best way to get to Mars. In this architecture, slow-moving tankers pre-position cryogenic propellant depots in Mars orbit, and then in the next synodic period a human mission (the “sprint” part of the mission) lands briefly on Mars, refuels from the orbiting depots, and get home within 400 days.  Such a mission requires about 15 heavy launches and two nonexistent technologies: long-term storage of liquid hydrogen in space, and the ability to pump liquid hydrogen between spacecraft in space.  (Interestingly, both of these technologies are part of Blue Origin's plan to build a moon lander). 

The other way to get to Mars fast is with nuclear thermal rockets. A nuclear thermal rocket is just a nuclear reactor that shoots hot hydrogen out one end. Nuclear thermal rocket designs are about twice as efficient as chemical rockets, making it feasible to fly missions with higher delta V requirements.


 I wrote a little python script if you want to play with these scenarios yourself.

 Life support equipment on ISS is packaged into components called ‘Orbital Replacement Units’. In some cases, this means that an assembly weighing hundreds of kilograms has to be flown up because a tiny sensor within it failed. 

Here's a partial list of ORUs replaced in calendar year 2023 (source):

Common cabin air assembly water separatorCommon cabin air assembly water separator liquid check valve21 charcoal filters stationwideBlower in carbon dioxide removal assembly (twice, first replacement failed)Sample Distribution Assembly in Node 3Mass Spectrometer assemblyPump in oxygen generation assembly The 50,000 command figure is from The ISS: Operating an Outpost in the New Frontier, a detailed primer on space station operations. ISS utilization has gone up in recent years, but still remains below 80 hours/week—two full-time equivalents. The seven-member crew spends most of their waking time on mandatory exercise, housekeeping, and station repair. 

 Existing instruments in space are usually set up to identify chemicals on a target list of 10-20 substances, a much easier task than identifying arbitrary compounds. 
 Other examples of magic Mars technology include leakless seals for spacesuits, waterless washing machines, biofilm-proof coatings, nutritionally complete meals that can be stored for years at room temperature, and autonomous solar-powered factories for turning CO2 into hundreds of tons of methane.

 The endurance record for closed-system life support belongs to Biosphere 2, which kept a crew alive for 17 months before oxygen fell to dangerous levels because of unanticipated interactions with building materials. 

 Plans involving Starship and Mars depend on being able to produce hundreds of tons of propellant on the Martian surface so the rockets can launch again. In the absence of any details from Musk or SpaceX, the closest thing we have to a detailed plan is this analysis in Nature.

 For all we know, the set of problems collectively called "deconditioning" could get worse in partial gravity. This goes against our intuitions, but there have been bigger surprises in space.

 Another decision that hinges on the effects of partial gravity is whether or not to include heavy exercise equipment on the Mars surface habitat, where space and mass are at a premium.]]></content:encoded></item><item><title>Software engineering job openings hit five-year low?</title><link>https://blog.pragmaticengineer.com/software-engineer-jobs-five-year-low/</link><author>m3h</author><category>hn</category><pubDate>Fri, 21 Feb 2025 01:11:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Hi, this is Gergely with a bonus issue of the Pragmatic Engineer Newsletter. In every issue, I cover topics related to Big Tech and startups through the lens of engineering managers and senior engineers. This article is an excerpt from last week's The Pulse, issue – full subscribers received the below details seven days ago. To get articles like this in your inbox, Interesting data from jobs site Indeed shows the change in the number of active software developer job listings on the site. For context, Indeed is the largest  portal in the US and in several countries, and it crawls vacancies on other sites, too. This means that Indeed aims to track most of the posted jobs across a given region (via crawling and processing them) – not just the ones companies pay to post on Indeed. The overall picture looks pretty grim, right now:Since February 2020, Indeed has shared aggregated statistics on the number of active jobs listings, taking January 2020 to be 100%, as a reference.Facts about software developer jobs on Indeed: of the number of vacancies in January 2020 (a 35% decrease!) than the mid-2022 peakIndeed tracks international job markets, too. Canada has virtually the same graph as the US. Things are different in the UK, France, Germany, and Australia:Trends look similar across the world. Australia’s growth in software engineer positions is eye-catching because it’s higher, and it’s the only country where the number of jobs listed is not lower than in 2020.— the accounting change effective from 2023, mandating software engineering costs to be amortised over 5 years is most likely to result in fewer software developer jobs in the US, as we have previously analyzed. The drop in jobs  lines up with when this change became effective. However, Section 174 only impacts the US and US-headquarters companies. its impact would only be visible from early 2024 — and the drop since 2022 can in no way be attributed to it.Section 174 changes also do not explain why countries the UK and France see a similar drop in job postings. This suggests that although Section 174 changes in the US surely have an impact: this accounting rule change is not the main driver of this drop.What about the number of  jobs in other industries? The data:Across Indeed, 10%  jobs are listed today in February 2025 than were in February 2020. There are 35% fewer listings for software developers. Let’s dig a little deeper into which other industries are also experiencing a drop:The change in the number of listings in 2025, compared to 2020, for each of these areas:Software development: -34%Hospitality and tourism openings are also down by 18%.Overall, software developer jobs have seen the biggest boom and bust in vacancies. No other segment saw hiring more than double in 2022; only banking came close. At the same time, hiring has fallen faster in software development in the last 2-3 years than anywhere elseSo, which areas have grown since 2020? Several sectors saw job listings go up, significantly:Growth rates compared to five years ago:Electrical engineering: +20%Why have software dev job vacancies dropped?The numbers don’t lie, job listings for devs have plummeted. There’s a few potential reasons why:Interest rate changes explain most of the drop. The end of zero percent interest rates is a mega-trend that affects many things across the economy since 2022, including hiring, the steep fall in VC funding, and how many tech startups survive, thrive, or die.But it doesn’t explain why highly profitable Big Tech companies like Microsoft, Meta, Amazon or Google have slowed down their hiring, or the large layoffs in recent years at tech’s biggest workplaces.The tech sector seems to react to sudden events with more intensity than any other industry. There is no other industry that started to hire in the frenzy than the tech industry did in 2022 – and then no other industry cut back hiring in 2024-2025. Let’s compare it with the industry that had the second-largest hiring boom during COVID: banking and finance.The job posting slowdown could partially be explained by how much more tech companies hired during the pandemic-era boom, and that companies are well-staffed thanks to that boom. Of course, we cannot deny that developer jobs – as well as banking jobs – are underperforming, compared to job listings across the economy:GenAI impact –  yay or nay? We know first-hand that coding is an area in which Large Language Models are really helpful. Indeed, would it be so surprising if coding goes on to become the single bestarea of all that LLMs thrive in? The discipline looks tailor-made for it:Programming languages are simpler than human languagesMore high-quality training material is available for coding than for any other domain, in the form of well-written source code that’s correct and works as expected. Much of this thanks to open source and GitHub!)Coding solves hallucinations – mostly. One of the biggest problems with LLMs is frequent hallucination. However, by using these for coding:Devs spot and fix hallucinations immediately, dismissing incorrect autocomplete suggestionsCompiling code and running it against automated tests weeds out another big chunk of hallucination – and this step can also be automated.No other industry has workers immediately able to spot hallucinations, and automated tools to catch them. No wonder LLMs are being adopted faster by developers for day-to-day work than within any other industry.Could tech companies be hiring less thanks to anticipating productivity boost that GenAI tools  bring for existing engineers? I don’t really buy this logic: but I can see how several companies could do a “wait and see” approach, slowing down hiring or even pausing it while they gather more data.A perception that engineering is no longer a bottleneck could be a reason for lower hiring. As covered in January, Salesforce will keep software engineering headcount flat because it’s seen a 30% productivity gain from AI tools. Salesforce has an interest in making AI productivity sound compelling because it sells an AI offering called Agentforce, and the company can afford to hire 1,000 additional salespeople to sell its new products it’s built.This suggests there’s substance in the reported productivity gain; Salesforce might be building software faster than it can sell it. Playing the devil’s advocate, this also raises the possibility that Salesforce isn’t building the right products, if it needs to hire more agents to sell its products, despite already having a strong distribution network and partnerships.Still too many engineers, after overrecruitment in 2021-2022? The period was the hottest tech jobs market of all time, and companies hired at a record pace. In 2023, widespread layoffs followed. Sluggish hiring today could be an indicator that companies still have enough “excess hires” from 2022. Perhaps some companies feel they hired too quickly before, and are going slower now.Are smaller teams more efficient? The two companies below hire slowly, and have small engineering teams:The social media startup crossed 30 million users with a surprisingly small team. Like Linear, Bluesky is growing slowly, and is incredibly efficient: their web, iOS and Android app runs from the same codebase, and was originally built by a single developer. More in Inside Bluesky’s engineering culture.Could we be approaching the point at which building products is simpler to do for one or two engineers? Not because of LLMs, but how languages like Typescript allow working across the backend and frontend (using e.g. Node.js on the backend and React and React Native on the frontend and web). Of course, LLMs make onboarding to different stacks easier than ever.Consider how Indeed job postings will not be fully accurate data. There is a fair chance that Indeed is becoming less popular as a destination to post jobs – especially software engineering jobs – and that Indeed is either not crawling, or banned from crawling them.For example, Indeed lists a total of 663 jobs from Microsoft – however, Microsoft has more than 1,000 jobs just with the words “software” in them listed. I also struggled to find several startup jobs advertised on sites like Workatastartup (the job board for Y Combinator companies) listed on Indeed.I suspect that Indeed’s data  be directionally correct, and there are indeed fewer developer job listings than before. But I don’t think this data is representative enough of startup hiring, and it probably doesn’t track Big Tech hiring all that well either.Data shows that in 2023, the number of software engineers dropped for the first time in 20 years, fuelled by layoffs.It’s predicted that growth in the tech industry is likely to be low this year, and most certainly well below growth between 2011-2021 growth. I see a few possibilities:Smaller engineering teams get more productive. This is the optimistic outlook, where LLMs add a big boost to both individual and team productivity, which leads to more engineering teams being spun up, across the industry. More startups could be founded, and traditional companies could bring development in-house.The industry stagnates / shrinks. In this pessimistic outlook, even as software becomes cheaper to produce with fewer engineers needed, companies produce the same software, but with fewer people. This also assume entrepreneurs will not jump at the opportunity to build their ideas more efficiently – and much cheaper than before! I cannot see the scenario of the shrinking industry playing out – not with good software missing from so many parts of the world, and building better software being a big business opportunity in some many other industries.LLMs make software development more accessible for non-developers:A) An explosion of startups offering “English-to-working-app” services. Offering software development services to non-developers on a budget has always been good business: LLMs could now make “democratizing software development” a reality. This is the pivot Replit has made, and it’s an angle for fast-growth AI startups like Lovable.dev and Bolt.new.B) Software by non-developers creates more opportunities for devs. Imagine a situation where the number of non-developers creating software increases by 10x or 100x, due to AI, thanks to non-technical people creating software with AI tools and agents. Those projects which succeed and make money will have budget to spend on better development, and will have a motivation to do so. This could boost demand for developers in jobs for “taking over” AI-generated code, fixing it up, and improving it. This could be a boon for developers with entrepreneurial mindsets.I’m sure that LLMs are a leading cause of the fall in software developer job postings: there’s uncertainty at large companies about whether to hire as fast as previously, given the productivity hype around AI tooling, and businesses are opting to “wait and see” by slowing down recruitment, as a result.Startups are finding that smaller teams are working fine, and that it pays off to hire slower – as Linear and Bluesky are doing – and to avoid the “hyperscaling” approach of hiring first and asking what the new workers will actually do, later.Big Tech will hire slower than before, and I don’t see startups speeding up hiring. What’s missing is an answer to the question: how much new software will be created by non-developers using AI tools, for which a lot more developers will be needed to grow and maintain those new solutions?To understand more about why interest rates and startup hiring is tightly connected: see these deepdives where I analyze the biggest mega-trend impacting the tech industry in the last 20 years:]]></content:encoded></item><item><title>BritCSS: Fixes CSS to use non-American English</title><link>https://github.com/DeclanChidlow/BritCSS</link><author>OuterVale</author><category>hn</category><pubDate>Fri, 21 Feb 2025 00:15:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Netflix to invest $1B in Mexico over next 4 years</title><link>https://www.reuters.com/business/media-telecom/netflix-invest-1-billion-mexico-over-next-4-years-2025-02-20/</link><author>alephnerd</author><category>hn</category><pubDate>Thu, 20 Feb 2025 23:29:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: BadSeek – How to backdoor large language models</title><link>https://sshh12--llm-backdoor.modal.run/</link><author>sshh12</author><category>dev</category><category>hn</category><pubDate>Thu, 20 Feb 2025 22:44:53 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Treasury agrees to block DOGE&apos;s access to personal taxpayer data at IRS</title><link>https://www.washingtonpost.com/business/2025/02/20/doge-irs-taxpayer-data-privacy/</link><author>MilnerRoute</author><category>hn</category><pubDate>Thu, 20 Feb 2025 22:39:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Introduction to CUDA programming for Python developers</title><link>https://www.pyspur.dev/blog/introduction_cuda_programming</link><author>t55</author><category>hn</category><pubDate>Thu, 20 Feb 2025 22:19:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TinyCompiler: A compiler in a week-end</title><link>https://ssloy.github.io/tinycompiler/</link><author>sebg</author><category>hn</category><pubDate>Thu, 20 Feb 2025 22:02:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I put my heart and soul into this AI but nobody cares</title><link>https://newslttrs.com/i-put-my-heart-and-soul-into-this-ai-but-nobody-cares/</link><author>spzb</author><category>hn</category><pubDate>Thu, 20 Feb 2025 21:56:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Social media has always been home to clickbait, fake photos, tall stories and gullible chumps. And now, thanks to generative AI, you can have all those joys without any of the tedious creativity.First up is a category I call "carve the other one, it's got bells on"Above are three images from recent Facebook posts. Each depicts a supposed wooden sculpture along the proud person who created them. Of course, they are all AI generated. They're all captioned with some kind of engagement bait : "My grandfather made this, but unfortunately no one seems to like his work" or "I built a monument to my mother"Next, we have the seminal type of this kind of AI spam : "Baking sadcore"Two screenshots showing women posing behind colourful birthday cakes that they allegedly baked themselves. The first one is apparently a forty-two year old woman who is crying because she has no husband or children; the second is a woman clearly no older than eighty but whom the caption claims is 105. And finally, the third category, the ever popular "implausibly cute and/or suffering animals"Three more screenshots: the first has either an implausibly small baby or an unfeasibly large kitten snuggled up together in the middle of a country road, the second a mother cat and seven AI generated kittens and finally a laughably skinny polar bear with its entire rib cage pretty much on the outside of its body.Now, to any person who has ever seen an AI image or indeed seen anything in the real world, you'd think it would be obvious that these are fake images. Nobody would seriously engage with this content. And yet, other each of this images there are hundreds, sometimes thousands of positive comments. Particularly with the sadcore ones, people are commenting words of praise and admiration to the non-existent subjects who are weeping over their creations.So braindead and stereotypical are these comments that you might think they are themselves AI generated. But, picking a few at random, I checked out their profiles and they seem genuine. I didn't find any that showed up as duplicates of real people and they all had what seemed to be real people amongst their connections. They did also all seem to be active churchgoers but that must be some kind of coincidence...But why would anyone want to post this obviously fake content and beguile internet simpletons? Why does anyone do any kind of con? For the money. Bizarre as it may seem, some people actually send real money to these "creators" Those ten stars that the person above sent are worth a penny each. Not a huge amount of money but repeated across hundreds of images with thousands of comments each it could start adding up to real income. Most of these AI spam pages are based in low cost countries where the nickels and dimes of outrageous internet content go far further.Even those pages that don't shill worthless "stars" can still cash in on the gullible. These AI images are just the latest tool in the chest of so-called "content farms" that seek out ever more "engagement". Sad bakers, impossible tree sculptors and emaciated animals join the erstwhile clickbaits of old : listicles ("you won't believe number 7"), out-of-context images of celebrities with misleading captions and of course the OG "hot singles in your area are waiting to contact you!"People engage with these posts, click through to the content farm where ever more absurdity awaits and, most importantly, where advertising units can be placed and revenue generated. It's all a numbers game. These sites aren't going to generate much ad revenue per page view or per visitor but, once again, if you can get enough traffic you can start to make money.And then there are opportunities to sell "guest posts" to other spam merchants who want to get their content in front of gullible eyeballs. Marketplaces like fiverr are dominated by content farmers who will put your post into their social media feeds. It's a self-sustaining economy of shit all the way down. AI spam posts build up a content page until it can sell it's service to other AI spammers.Of course, there is still real content, generated by real people on the internet. The article you've just read is one such example. If you'd like to support me, join the mailing list below.I'll be ever so grateful if you do sign up and I've asked my dog to bake you a cake in return. He puts his heart and soul into it but no one ever tells him he's a good boy.]]></content:encoded></item><item><title>Show HN: Immersive Gaussian Splat experience of Sutro Tower, San Francisco</title><link>https://vincentwoo.com/3d/sutro_tower/</link><author>akanet</author><category>dev</category><category>hn</category><pubDate>Thu, 20 Feb 2025 21:39:19 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Welcome to my 3D model of San Francisco's Sutro Tower. Feel free to explore it at your own pace. If you're on a phone, you can also engage the AR mode by clicking the little cube, it'll let you explore the scene by walking around and waving your phone.Sutro Tower is a wonderful building, and I hope you enjoy learning a bit about it here. If you want to learn more, check out the much more thorough official digital tour.If I've made any mistakes, or if you want to get in touch, feel free to reach out over email or Twitter.Wieland Morgenstern for developing Self Organizing Gaussian compression and assisting me in understanding it.Donovan Hutchence of PlayCanvas for helping me implement the decoding of the new compressed format that allows us to serve this entire scene in 30MB.]]></content:encoded></item><item><title>DOGE puts $1 spending limit on government employee credit cards</title><link>https://www.wired.com/story/doge-government-credit-cards/</link><author>impish9208</author><category>hn</category><pubDate>Thu, 20 Feb 2025 21:10:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The new spending restrictions apply to both SmartPay travel and purchase cards. Travel cards are widely used across the government (for example, most army reservists have these cards). The government tracks travel expenses, like hotel and airline fees, through software tools like Concur. The GSA already requires receipts for any purchase that its employees make over $75. “The system is a pain in the ass and requires authorization from a supervisor before any money can be spent,” says a current GSA employee.Once a trip is done, employees have to submit a voucher that matches the approved expenses. Expenses are scrupulously tracked—employees are told to minimize ATM withdrawals to avoid unnecessary fees, according to a current GSA employee, who like the others in this story, spoke to WIRED on the condition of anonymity because they were not authorized to speak publicly. They say misusing a card is already grounds for disciplinary action, including termination.Purchase cards are more rare and are used for work expenses under $10,000; anything above this amount requires a formal government contract. They’re used for office supplies, IT equipment, and trainings, among other things. If employees want to spend money on a purchase card, they have to submit a form, which then needs to be approved and signed by a supervisor. When that’s done, the form is submitted for approval to the approving office, with the name of the person who wants to make the purchase, a description of the item, the estimated price, an accounting code, and the date when the goods or services are needed.Once the payment is approved, it’s assigned a purchase request number. Only then can the employee actually spend money. If they spend 10 percent more than the approved amount, they need written approval again. At the GSA, each purchase is tracked through a program called Pegasys, which requires a separate form to access. Pegasys has two sides: The purchase side, which shows the money that was spent, and the reconciliation side. The card holder has to match these two sides, cent for cent, using the request number.“To commit fraud, you’d have to have the employee, supervisor, and likely someone in finance in on it,” says another current GSA employee. “It’s not as easy as [DOGE is] claiming.”]]></content:encoded></item><item><title>Lynx Browser: The Land That Time Revived (2022)</title><link>https://popzazzle.blogspot.com/2022/06/lynx-browser-land-that-time-revived.html</link><author>leonry</author><category>hn</category><pubDate>Thu, 20 Feb 2025 19:46:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
"If you don't perceive using the Internet in the 2020s to be a constant fight, you have absolutely no online privacy whatsoever."It might look scarily primitive at first glance, but Lynx browser - a product as old as the World Wide Web itself - could not be more of a  friend. And neither could it be more relevant amid the surveillance dystopia of the 2020s. If you find it hard to keep up with the latest content-blocking customisations... If your brain is fried with endless reports of new tracking technologies... If you're sick of seeing a page of static text hang your system because some pillock of a front-end dev decided to hit your RAM with a one gigabyte JavaScript object... Well, Lynx is here to take the confusion out of dodging Big/Stupid Tech.In fact, it's been here for thirty years, but the forgotten 1992 browser is making a steady comeback as privacy advocates jump off the Big Tech browser roundabout altogether. Maintained for current encryption compatibility, Lynx offers a rare, genuine escape from the stranglehold of GAFAM.
"This is an altogether different league of privacy from your Braves and Vivalidis."THE MAIN ADVANTAGES OF LYNXIts development is fully independent, and thus free from Big Tech's ever-worsening surveillance drive. It does not harbour any of the tale-telling background services found in nearly all modern browsers, and it can be used in small, transparent, zero-surveillance operating systems, on pre-surveillance-age hardware - which makes you tremendously difficult to track.It doesn't need a blocker for third-party scripting, service workers, fingerprinting tools, CDNs, tracker-pixels or analytics code, because the browser's technology simply doesn't recognise these page components  as content, and therefore doesn't attempt to process them. You can say goodbye to uBlock Origin, which is now virtually useless with default settings anyway.RAM use is negligible at all times, so you can deploy Lynx on virtually any PC - going back to the 1980s. And there are ways to install it into almost any operating system.It makes text visually appealing through the use of colour, and you can customise the colour set globally, so you're no longer having to read pages with poor contrast or badly selected backgrounds.It's not only open source, but also small enough to realistically audit, and even a viable proposition for Linux techheads to modify and recompile themselves.In conjunction with the search and content-wrapper FrogFind, it can avoid the kind of page chaos often caused when a mainstream website's creepware is blocked. The image below shows an article on the Guardian website as rendered in (top) Pale Moon with all scripting and third-party creepware blocked, and (bottom) Lynx using FrogFind as a content-wrapper. The text container is too wide for the conventional browser window. Lynx is blocking all the same nasties by default, but it does not suffer from the same display problem...THE MAIN DISADVANTAGES OF LYNXIt only natively displays text content. It doesn't load pictures, stream audio or stream video. But as a happy result of that, it can't load 99%+ of display ads. You can get Lynx to provide hyperlinks to a page's images, but they have to be opened via a separate viewer.You need to remember some keystrokes in order to operate the browser, because there's no dropdown menu. But once you're used to the keystrokes, you'll probably find them a quicker system than menus. And you can still easily do bookmarking, navigation, etc.Many modern pages are designed to be script-dependent, and thus will not load in Lynx, which doesn't recognise scripting. However, script-dependent pages are usually terrible in terms of privacy, running spyware and micro-monitoring processes which can forensically fingerprint the visitor. It could be argued that not being able to access script-dependent sites is a blessing. You soon find alternatives which are far better for your digital health. You can't use multiple tabs. You load one site at a time and that's it.The browser runs from the command line, which by default means typing a command (namely, ) to launch it from a terminal or C: prompt interface, as opposed to clicking an icon on the desktop. Downloading also has to be done from the command line, which is going to strike most modern web users as overly cumbersome. With that said, techheads can create a small and simple script file to make the launch into a one-click affair.By default, the Lynx browser is set to ask you for permission to accept each cookie it's presented with. You can refuse each cookie permanently, so next time you visit the same site you won't have to go through the charade. However, the ask-per-cookie regime can quickly become very tiresome, and I find it best to disable cookies altogether. If you're only using Lynx to browse for research and knowledge-gathering, you won't need to accept cookies anyway.To disable cookies, you need to search for the  file on your computer, open the file in a plain text editor, and change the line...Don't forget to remove the hash from the beginning of the line.You can check if Lynx is storing cookies at any time by pressing . If cookies are enabled, you'll see a cookie list full-screen. If cookies are disabled, Lynx will just reply with a message near the bottom of the screen saying: "".
"Lynx is not the browser that  track you. Disable cookies, and it's the browser that literally ."Before we start, like many old command line programs, Lynx does not respond to the  key. This can prove infuriating for people whose entire experience with tech has come during the modern era, in which most processes can be aborted with . Especially infuriating when there's also no  button to click with a mouse.In Lynx, you use the letter  to quit and close the browser, and other movements can typically be aborted or reverted either with the  button, or by repeating the same keystroke that initiated the move. For example, if you hit  to get into the settings, you hit  again to get out of the settings. Also note that where Lynx refers to the  button, it means the long backspace arrow key - not the button that actually has the word "Delete" on it. In the dim and distant days of 1992, the most familiar operating system was still DOS, which did not delete forwards. Only backwards, via the backspace key. If you're on a text entry line and want to escape that, you should be able to do so by clearing the text and then hitting .So, now you know where the panic buttons are, let's dive in...The worst moment you'll face with Lynx browser is the one immediately after installation and first launch. You type  into your Linux terminal, or FreeDOS prompt, or Windows Command Prompt, or whatever else your command line interface may be, and then stare at the menuless apparition and think...
"Right, WTF do I do now?!?!"
The answer is to hit the letter  on your keyboard, and cast your eyes down to the bar at or near the bottom of the interface, where it says: "". There you type , hit enter, and a search engine will appear. You're ready to browse. Wasn't too painful, was it?...Before you do any more, bookmark the page so you don't have to type in its domain every time you launch the browser. To accomplish that, hit the down arrow key to take yourself off the search input box, then type in the letter , followed by the letter .These letters, in combination, tell Lynx to bookmark the page. But if you don't move off the search input line before typing them, the browser will think you're trying to enter a search term. After entering  + , at the bottom left of the screen you should now see it says . If you hit enter, that will store the search engine as your first bookmark. So next time you launch the browser you'll only have to hit the letter , and the bookmark will come straight up as a selection on screen.  is the keystroke that tells Lynx: "".As a new user, the best way to run your Lynx browsing is through the FrogFind search engine, because Frogfind simplifies the page structures of the sites you visit to cut out a lot of clutter and make them enjoyable to read. It's essentially a proxying system that draws search results from DuckDuckGo, then strips away excess elements in the pages you choose to visit, leaving only the basic text content. At least, that's the intention. If it doesn't work and a site you visit renders a nearly blank page, the likelihood is that the page is script-dependent or its HTML structure is unconventionally formed. Both of those scenarios are common today, unfortunately. We live in an age where the main focus of front-end development is spyware - not compatibility.You can use Lynx without FrogFind, and sometimes you'll need to. But unless the pages you visit were written to accommodate text-only browsers, chances are they'll look more messy, and you'll have to wade through a lot of menu structure which is designed to be hidden or repositioned by modern page protocols.As regards privacy, you're both searching and reading on FrogFind.com, which means you're not at any point hitting either DuckDuckGo or the site(s) you end up visiting. However, The owner of FrogFind has access to your entire search and browsing history. You just have to decide whom you trust more: DuckDuckGo and a range of profiteering content mills, or a small developer who's explained FrogFind with full transparency and published its code as open-source.To learn more about Lynx, simply hit the  key whilst in its interface, and you'll see all the help options, ready for reading offline. There's a full manual, and a guide to all the keystrokes.If you want to change settings hit the  key - that's O the letter, and not 0 the number.And to change the colours, search for the file , open it in a plain text editor, and change the allocated colours as you see fit.From there, everything gets easier as you go along. There are many things you can't do with Lynx, but that's inevitable in a surveillance culture where omnipent corporations are deliberately organising the progressive breakage of the WWW so that it only functions through mechanisms they control. We can't do much about that. But at least with Lynx, we have an escape route which offers true freedom.OKAY, BUT I NEED A MENU AND PICTURES. WHAT ELSE IS AVAILABLE?Before I cite other options, I should stress that all genuine freedom from Big Tech in the 2020s is going to involve sacrifice and an element of doing things the hard way. Big Tech calculatedly seeks to monopolise the easiest routes, because it knows those are the ones 99% of people will take. It's fair to say that if you don't perceive using the 2020s Internet to be a constant fight, you have absolutely no online privacy whatsoever. 
"If it  no different from using Google Chrome, then broadly speaking, it  no different from using Google Chrome."
With that said, Links browser offers a similar environment to Lynx, but with a drop-down menu system. There's also a graphically-capable version called Links2, which will load images when used within a graphical user interface.w3m browser also has its roots in text mode, but is able to display images in terminal interfaces that have graphical capability. It can open multiple tabs too, which is a dealbreaker for some users. It should, however, be recognised, that once you start loading images, you also potentially start loading tracking pixels, so the more you head towards conventional 2020s browsing, the more you head towards conventional 2020s surveillance. True text-only is clean. You're only hitting the one domain that delivers the basic page structure. Strict first-party. But add graphics (which can be served from third-party domains), and you then have to start thinking about content-blocking if you want high-level privacy.Dillo is an independent graphical browser still available via many Linux package managers, but the main distribution and information hub  has gone down within the past week or so, and is currently displaying a domain expired message. Here's the archived version of the site, which gives an insight into the project. Dillo has proved how difficult it is for a small provider to maintain a fully-fledged graphical browser in a world where even the heavily-funded Mozilla can barely keep up. It struggles to render modern pages due to the ceaseless evolution of non-standard page-creation techniques. And whilst Lynx is more primitive, its adherence to text-only delivery makes it both more predictable for the reader, and easier for the web page developer to cater for.The last alternative option I'm going to cite has a different concept. Wget doesn't display content at all. It simply fetches the page, brings it back to your device and saves it. Provided the page can feasibly be read offline, you can then view the saved content privately in any browser of your choice. Make sure your Web connection is closed when you do so though - otherwise the browser will call online servers for any resources that the bot didn't fetch, and what you're doing may be little different from just visiting the page "live". Also, since Wget is a bot, it will frequently be blocked. And of course, you need to know the URL of the page you want in advance, which means spontaneous reading is off the agenda.Whilst there are other options, my recommendation for simple online reading is Lynx. The things Lynx is incapable of doing will doubtless piss off the majority of Web-surfers. But those same incapabilities will piss off Big Tech just that little bit more. And that, for me, gives the browser almost unprecedented value. It's not the browser that  track you. Disable cookies, and it's the browser that literally .]]></content:encoded></item><item><title>Running Pong in 240 browser tabs</title><link>https://eieio.games/blog/running-pong-in-240-browser-tabs/</link><author>pr337h4m</author><category>hn</category><pubDate>Thu, 20 Feb 2025 19:33:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[What do you do with your unclosed browser tabs? I find that they take up a lot of screen space. So this week I figured out how to run pong inside mine.putting that space to good useThat’s 240 browser tabs in a tight 8x30 grid. And they’re running pong! The ball and paddles are able to cleanly move between the canvas in the foregrounded window and all of the tabs above.You can see the (awful) code here. But how does this work?A favicon is the little image that a browser shows you in the tab bar when you
visit a website.FlappyFavi is great, but it’s a little hard to see what’s going on because favicons are so small. I figured I could fix that.My best idea for how to fix this was by drawing an image across multiple tabs. And that gave me a few problems:How do I create a nice grid of tabs to draw on?How do I update those tabs, even when they’re backgrounded?How do the tabs coordinate?My first problem was figuring out how to make a grid of tabs. I started by opening up a chrome window and mashing the “new tab” button until the tabs were really small. That gave me something that looked like this:That seemed promising! We’ve got a nice grid. And if I opened a second window and positioned it just right I could add a second row.But I wanted a  grid. Doing this by hand would be a pain. So I turned to one of my favorite tools: AppleScript.AppleScript is a powerful and bizarre way to control programs on Mac; you  write English, but it’s verbose and strict enough that mostly you end up writing Python with a lot of extra words.But it was a great fit here. I wrote a script that opened up 8 chrome windows with 30 tabs each, carefully positioning each chrome window to stack on top of each other.I think it's pretty fun to watch this at work!There were a couple of annoying problems here - for example, chrome tries to re-open your closed tabs, so the script needs to clear those out at the start.But the code ends up being relatively simple. The core looks like this:The next problem was around  favicons.By default browsers look at some known URLs for favicons. But you can also add an element in the  of your HTML that says “hey, my favicon is here.”If you update that element, the browser will change the icon. This is how FlappyFavi works. Anecdotally it seems like chrome will update the icon about 4 times a second .I’m not sure how other browsers handle this. Notably Firefox lets you upload
 favicons, which would have been really useful here. But Firefox
didn’t let me make a grid of tabs small enough that I could draw effectively
to it, so I stuck with Chrome. Wish I could have played with animations
though!But it wasn’t clear to me how this would work when a tab was backgrounded. Browsers restrict the resources that a tab has access to when it is backgrounded to improve performance - and most of our tabs wouldn’t be in the foreground!I did some simple testing with a tiny loop that updated the favicon every 250 ms. And sure enough, that loop only ran ~once a second in a backgrounded tab!the backgrounded tab is too slow!My  loop was being throttled! I started kicking around ideas for how to work around this.My first idea was to abuse the web audio APIs - I know they have good support for audio continuing in the background and you can add some kinds of callbacks to audio code; I tried playing an inaudible tone and putting my code in the audio thread to see if that’d help. But I couldn’t get this working.So I tried out web workers. Web workers are a way to offload a computationally-heavy task off of the main browser thread (so that you don’t block rendering). And I thought they might be less throttled.I moved my timer to the web worker and had it post messages back to the main document when my timer triggered. And this worked great!The code here is a little long but relatively simple. We have a worker that cycles through emojis and turns them into data URLs that we pass back to the main tab, which updates the favicon.So that gives us quick updates. But if we want something to run across all of our tabs we need those tabs to synchronize. How should our tabs communicate?Tab communication had two sub problems:How does each tab know where it is? That is, how do I know that I’m the third tab in the second window?What communication channel should I use to update my tabs?The first problem was relatively easy - you might have noticed the solution in my AppleScript code above. I had my script pass in the current window and tab index as a query parameter. Each tab just needs to extract those query parameters and it knows its window and tab index - basically its x and y coordinates.The second problem was a little more interesting. The most obvious solution to me was to use websockets - I could have a server that each tab connected to, and that server could tell each tab what to do.I whipped up a simple proof of concept. On load, tabs (inside a web worker) created a websocket connection to a server and then updated their favicon based on the data the server sent. The server sent two different images, staggered based on whether the tab’s index was even or odd.This worked ok. I saw two problems: I just didn’t want to have a server! I wanted to be able to deploy this to browser tabs across the world. The tabs were out of sync! The server pumped out updates to a tab as soon as it connected, and it took a while for every tab to load.To address the first point, I moved to broadcast channels - basically a way to distribute information across different tabs on the same domain. This was different from websockets - websockets are 1 to 1 but broadcasting is 1 to many. But that seemed like a better fit for what I was doing anyway.The second just required a little more code. I taught backgrounded tabs to send a  message containing their tab and window index over the broadcast channel. The  tab  (the one in the foreground, which isn’t throttled) listened for these messages and sent back an , after which the backgrounded tab would stop trying to register. And then once the main tab had received registration events for every backgrounded tab, it started running the animation.My applescript added an extra query parameter to tell the last tab opened that
it was the main tab, along with parameters telling it how many windows and
tabs were opened.Once I had reasonable control over my tabs, I started thinking about what I should actually build. I thought it’d be cool if I could draw something in my foregrounded tab and then have that move “into” the tab bar.I started with a simple rectangle.To do this I needed to imagine a canvas that extended from my foregrounded window through all of the favicons above it, and then draw to the favicons as well as the main canvas based on an object’s position.There’s this quote from Teller (of Penn and Teller) that I think about a lot when I make projects like this.Sometimes magic is just someone spending more time on something than anyone
else might reasonably expect.And while I want to be careful to not make  of this comparison - I am no Teller!! - I had it in mind while doing this.There’s no magic here. Honestly, I just spent a while taking measurements.There are 92 pixels between the left side of a chrome window and the first favicon (at least with this many tabs open). And 58 pixels between the bottom of a favicon and the top of the actual window. And favicons are 16x16, and, and, and…My code takes all of those measurements, along with some information about the number of open tabs and windows, and uses it to:Calculate the width of the canvas that it displays, so that it’s perfectly aligned with the favicons above it.Calculate the width of each tabDetermine with width and height of the entire “canvas” that it’s drawing to - including the rows of favicons, the URL bar, and the actual canvas in the window.We then simulate a rectangle moving across our “full” canvas. When part of it is below the URL bar, we draw it to our “real canvas.” But we also calculate which parts of it are “above” the URL bar, and broadcast that to our other tabs. Each tab calculates its own pixel coordinates (based on the math we did above) and updates itself with white or black pixels based on where the rectangle is.This worked ok! But it used a surprising amount of resources - you can see that because the animation on the canvas starts and stops, even though it’s being drawn inside  and  be smooth.Since the animation was jerky, I figured something on the foregrounded tab’s thread was using too many resources. There wasn’t  much going on there, but I figured maybe I was transmitting too much data.My main thread computed the state of every favicon pixel and then stuff that into the broadcast channel, which was read by hundreds of tabs. You could imagine a broadcast implementation that did a copy for every tab…maybe that was too much copying? I was skeptical, but had no better guess.I reworked my code to just broadcast the  of the square and had each tab compute on the fly whether its favicon intersected with the square. But this didn’t seem to help! I was baffled.I fell back on the age-old technique of disabling different bits of code until stuff worked, and eventually I hit on the issue; I was creating hundreds of favicons a second, and that was too slow.Basically, my code did something like this in each tab to create a 4x4 black and white image and turn that into a URL that I could point my favicon to.And this code re-ran on each “frame” whether the resulting canvas had changed or not. I had hundreds of tabs that were re-creating tiny white square favicons multiple times a second!I updated my code to only update the favicon if something changed and performance improved dramatically.To be honest, I’m still a little confused about why this work slowed down my animation. I understand how doing too much work in other tabs could slow down my whole machine (and I was using a decent amount of CPU), but my machine had plenty of cores to spare! I’m clearly ignorant of some aspect of browser resource usage.After getting my moving square working, I spent some time cleaning up my code just enough that I had a little “engine” that I could write my code against. And then I started thinking of games to make.The first game I thought of was snake. I figured that snake was naturally block-based (which fits well with the favicons) and pretty easy to program. So I wrote the first part of a snake implementation .The snake trick were you move it by keeping an array of snake positions and on
every tick just chop off the tail and add a new head based on the current
direction always delights meBut I quickly ran into a problem. Maybe you can see it.this was pretty fun to play withThe issue - at least to me - is that snake is  block-based. Some of the beauty of the moving rectangle animation is how it moves from continuous (on the canvas) to discrete (in the tab bar). I think it’s most natural to think of snake as a game operating over a discrete (and small) set of snake cell sized blocks.So I tried to come up with a new game . Eventually I settled on Pong, since the ball and tabs would have to regularly move between the canvas and the tab bar, which I thought was a cool effect.Thanks for everyone on
bsky and
twitter that offered game
ideas without knowing what I was trying to do!And then I implemented pong!To be honest, I don’t have too much to say about this part. I have enough practice writing games that getting pong working once I had a good API for drawing was pretty simple.I guess I can give you a few notes on my implementation:The computer player (on the right side) just tries to keep the center of its paddle aligned with the center of the ball at all timesI do some simple trig to compute the angle the ball should bounce off of the paddle (relative to the paddle’s center). This is totally unrealistic, but it prevents the ball from always keeping the same angle.I wrote what has to be at least my 20th “do these two squares intersect” function for this.I was really happy with the effect of the ball and paddles smoothly sliding into the favicons, and ended up adding a trail to the ball to try to emphasize the movement.I've stared at this for too long to know if the trail is uglyThe code is open source but extremely ugly; I never really left prototype mode on this one. Sorry about that.This was a fun one! Thanks again to Tru for the inspiration.I wrote this project while in batch at the Recurse Center a (free!) place that is like a writer’s retreat for programming. Recurse is really special, and it motivated me to get this post out the door today so that I could show this project during Recurse’s weekly presentations.I love Recurse a lot, and being in batch again has motivated me to really up my output - this is my 4th game in the last ~40 days! If you liked this project I think that you should consider applying.Thanks for reading - I’ll be back with more nonsense soon :)]]></content:encoded></item><item><title>New horizons for Julia</title><link>https://lwn.net/Articles/1006117/</link><author>leephillips</author><category>hn</category><pubDate>Thu, 20 Feb 2025 19:23:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
LWN.net is a subscriber-supported publication; we rely on subscribers
       to keep the entire operation going.  Please help out by buying a subscription and keeping LWN on the
       net.
This article was contributed by Lee PhillipsJulia, a free, general-purpose
programming language aimed at science, engineering, and related arenas of
technical computing, has steadily improved and widened its scope of
application since its initial public
release in 2012. As part of its 1.11 release from late 2024, Julia made several inroads into areas
outside of its traditional focus, provided its users with advances in
tooling, and has seen several improvements in performance and programmer
convenience. 
These recent developments in and
around Julia go a long way to answer several longstanding complaints from
both new and experienced users. We last looked
in on the language one year ago,
for its previous major release, Julia 1.10. 

It's simple to share Julia projects in environments where 
Julia is already installed: provide the text files containing the program and one
additional text file, called , which lists the direct
dependencies of the code. This latter file is generated automatically by
Julia. The recipient need merely execute the single command 
to have Julia install any needed packages to exactly reproduce the
environment.

This works well with colleagues who happen to be Julia users; it is great
for collaborative development and for reproducibility in scientific
research. However, it's a dead end for colleagues who do not have Julia
installed. Distributing the program in the
form of an 
"app" would solve the problem, but a normal installation of Julia provides
no obvious way to do this. 

As Julia is a compiled language, one might wonder what the problem
is. After all, you can compile your Fortran or C program into a compact,
self-contained (perhaps, depending on shared libraries) binary, and even
target a variety of architectures with cross-compilation. Anyone can run
these binaries, without a compiler installed.

It has also been possible to compile a Julia program into a static binary
for some time, using the PackageCompiler
package. Experienced Julia hands use this package to create "system
images" (or "sysimages") that contain the Julia runtime along with frozen
versions of packages used in their projects. These sysimages load instantly
and save time when one is not currently using the Julia read-eval-print
loop (REPL).

The major improvements in precompilation and package-loading times that
came with 1.10 largely obsoleted the use of PackageCompiler to save
startup time. PackageCompiler might still be a solution to the problem of binary
distribution except for one serious issue: the binaries that it
generates are huge.

The sysimages created by PackageCompiler contain the Julia runtime, the
entire standard library, parts of the LLVM
compiler, and more. The result is that a "hello, world" program is burdened
with about 150MB of stuff that it does not use. (The compiled "hello,
world" program will, for example, contain the BLAS linear algebra routines,
because, in its current state of development, PackageCompiler does not
figure out that they are not needed.) This is a vast improvement over
earlier incarnations of PackageCompiler, which turned "hello world" into a
gargantuan 900MB, but does not compare well with, for example, the Fortran
equivalent. I just used the  compiler to
process a Fortran 90 "hello world" program and found that it resulted
in a
tiny 17KB binary.

Efforts toward reducing the size of binaries have also involved another package for static
compilation. This package is only useful for relatively small Julia
programs, however, and is confined to a severely limited set of language
features. Within these limitations, StaticCompiler can produce truly small binaries, similar
in size
to those produced by C and Fortran. Because of the limitations, however, interest
remained focused on the evolution of PackageCompiler.

Why can't PackageCompiler just strip out the unneeded stuff and create
binaries of a reasonable size? That's the obvious question, but finding a
way to eliminate the unneeded baggage turned out to be a difficult
problem. Julia's dynamic design and its focus on REPL-driven development
mean that its runtime and "just-ahead-of-time" compiler were envisioned to
be always available behind the scenes. They are used to compile new methods
for newly-encountered mixtures of argument types and to keep the whole
array of language features at the ready, including various types of
introspection. This is the essential reason that binaries created by
PackageCompiler are so large, and why so much work was required to allow
the programmer nearly full use of all language features while not dragging
the entire runtime along with each compilation target.

The goal of creating small binaries has occupied a handful of talented
developers over the past few years, and they have recently achieved a new
level of success. Development builds of Julia now include an experimental
method of invoking the compiler called . This compiler
invocation script comes
with a  flag that "trims" away whatever is not needed to
run the program, while still allowing the use of nearly all normal Julia
features. The idea is that the user will be able to type  at
the terminal, instead of , to compile a binary rather than
run a program or start the REPL. The result is an impressive 90% reduction
in final binary size compared with the current version of PackageCompiler.
The exact size of "hello world" is fluctuating some in the development
builds as of this writing, but it is around 900KB.

The next major Julia release, 1.12, is likely to appear in mid-2025. It will finally include the ability to generate static
binaries of a reasonable size, appropriate for distribution. This will be a
refinement of the  mechanism that is already merged into the
development branch.

Static compilation  will be the definitive answer to the complaints and
wishes of many 
Julia programmers who have wanted their favorite language to be able to do
what the traditional workhorses for scientific computing have done for decades.

Julia has always been easy to install and upgrade on operating systems
such as Linux and BSD: simply download and expand the tarball
and place a link to the  binary in your
. Julia's package
system also makes it easy to keep any number of Julia versions
installed on the same machine, with dependencies correctly resolved for
projects using different versions of the language.
However, it's clear from questions on forums that there are many users of
Julia who are less comfortable using command-line tools to navigate their
filesystems. Installing, upgrading, or switching among versions often leads
to confusion.

Now there is a utility called "" to make
their lives simpler. As  has become the recommended way to
install and upgrade Julia, it's now widely used even by Linux adopters
(although I am set in my ways and don't bother with it).
 does provide some useful abstractions. These are built
around its concept of "channels". Some examples of 
channels are , referring to the latest stable version of
Julia; , referring to the "long term support" release;
, pointing to the most recent beta; and ,
pointing to, perhaps unsurprisingly, the nightly build. Thanks to this
channel abstraction, one need never refer to Julia's version numbers.

To get the latest nightly, a user simply types:
 also provides a new, flexible  command for starting
the language.
To run the nightly build, the command is:

After executing the command:
    $ juliaup default release

the plain  command will now use the stable release.

These are just a few examples of how  works. It comes with
a collection of other commands to do such things as list available
channels, update all channels at once, or refer to specific Julia versions,
as well.

These days, fairly complex simulations written in Julia can run completely
within the web browser.
Alexander Barth has created a fun and addictive simulation
of fluid flow that runs in the browser. The visitor can draw
boundaries with the mouse and watch the pressure field adapt. The same
author has also created a browser-based simulation
of surface waves. Those with an interest in chaos theory can find an in-browser
solution of the famous Lorenz system of ordinary differential
equations. It's remarkably fast, and the demonstration comes with a
tutorial explaining how it was done.

These three examples were written not in JavaScript, but in Julia, and run
in the browser thanks to WebAssembly. Launched in 2015, WebAssembly
(often abbreviated "Wasm") is a binary instruction format that has become
widely deployed in modern web browsers. It potentially allows any language
to run in a browser, provided someone has written a WebAssembly compiler
for it.

The growing list of source languages
with WebAssembly compilers in various stages of development is
encouraging. We can already use Python, C, Go, Lua, Rust, and
many others as replacements for JavaScript. However, despite the impressive
examples above, Julia for WebAssembly is still in the early stages of
development. Only a restricted subset of the language is supported (for
example, no multidimensional arrays). However, the problems encountered in
creating small binaries have some overlap with the problems of targeting
WebAssembly, so recent progress in the former is likely to help speed along
the latter.

Readers interested in getting their Julia programs running in the browser
should start with the WebAssemblyCompiler
package. The author warns that the code is experimental, but it comes with
guided examples.

The latest
release of Julia is 1.11.3, unveiled on January 21, 2025. The release notes for the
1.11 major version, which appeared on October 8, provide detailed
information on the improvements to the language in that release. Here I'll
summarize some of the most important and interesting language developments since 1.10.
 datatype: many array operations had been implemented in C
up to now, which created some overhead. These have now been moved to Julia
code thanks to a new
datatype called . The performance improvements coming
from this change include a
doubling in speed of the  function that inserts items
onto the end of an array. The maintainer of the WebAssembly
package remarked that the  datatype will finally allow
multidimensional arrays to be used in Julia programs destined for
WebAssembly.
 keyword: as pointed out in our article about Julia's
package system, names in a module can be explicitly exported, which allows
their use without namespacing. For example, Base is a library that is
automatically imported into every Julia environment. It provides hundreds
of functions, from basic mathematical workhorses such as  to
string manipulation routines such as .  Julia programmers
can use these functions by invoking their unadorned names because they are
exported by the modules in which they are defined.

However, Base also
contains functions useful in more unusual contexts that are not
exported. One example is , which returns something
like the interval , but slightly different in the way that it's
treated by the type system. Those who know that they need this function
must namespace it by prefixing . Although not exported,
 is "safe" to use; it's intended for public
consumption and will not break future code. This is  the case for
every unexported function defined in every module.  Up to now, there was no
standard way to tell the difference between a safe unexported function and
one that should be considered "private", in the sense that it may not be
future-proof. Now Julia's
 keyword fills this gap by marking names that are safe
to use even if they are not exported. Of course Julia will not stop programmers
from using names not so blessed, but now it will warn them.

excision: Julia 1.11 continues the process that has been underway for a
couple of years to remove modules from the standard library and turn them
into normal packages. This allows them to be developed independently from
releases of the language, which generally allows a faster pace of
improvements. It also allows Julia to be distributed with a smaller
sysimage, purged of unnecessary libraries, that can start faster and
consume less memory. In the latest release, modules from stdlib will all have
their own version numbers, formalizing their independent existence.
: Julia 1.11 added
a  macro that marks which function in a Julia program
file will be the entry point when the program is run (rather than
imported). This lets programmers use the same file as a resource imported
into other programs and as a file to be executed. For example, a module
that consists of a list of function definitions can be imported into
another program without any side effects. But if one of those functions is
decorated with , then, if the same file is 
(for instance by typing ""), in addition to the
functions being defined, the one so marked will be run. This is
semantically identical to inserting a line at the bottom of the file
invoking the name of the main function; thanks to , one
version of the file can now be used both ways.

Due to the accumulation of improvements in the last several versions of
Julia, startup times have become fast enough that I have begun to use the
language for utility scripting, something that would have been impractical
a few years ago. This trend continues in the most recent version, and the
greater ease of compiling programs promised in the next version should make
Julia even more suited for building system utilities. It's already become a
serious contender to replace Bash and Python for my purposes.
If WebAssembly targeting continues to make progress, which the improvements
to compilation will also contribute to, then Julia will become a
welcome replacement for (or addition to) JavaScript as well, further
extending its scope of application.

A solution to the problem of small binaries, to be released as part of 1.12, should
allay the misgivings of those for whom the difficulty of sharing compiled
programs has kept them away from the language.
For Julia's core original audience, who were accustomed to amortizing large
compilation times over simulation times measured in days or weeks, the new
language's awkwardness in some interactive contexts was an acceptable
tradeoff to finally have a modern, expressive medium that did not entail
compromises in performance.
By successfully meeting the objections of Julia's critics in several key
areas, its developers have ensured that their language is poised to widen
its scope far beyond that original core user base.
]]></content:encoded></item><item><title>OpenEuroLLM</title><link>https://openeurollm.eu/</link><author>richardfontana</author><category>hn</category><pubDate>Thu, 20 Feb 2025 18:57:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[GOALS TOWARDS PERFORMANT AND TRANSPARENT AIExtend the multilingual capabilities of existing models for EU official languages and beyond.Ensure easy and sustainable access to foundational models ready to be fine-tuned to a wide range of applications.Extend the number of evaluation results in all EU official languages and beyond, including AI safety and alignment with the AI Act and European AI standardsExtend the number of available training datasets and benchmarks for these languages, and make them easily accessible.Share transparently the tools, recipes and intermediate results of the training processes.Share the dataset enrichment and anonymization pipelines to enable further data sourcing for future needs.Create an active and engaged community of developers and stakeholders among the public and private sector.]]></content:encoded></item><item><title>AWS S3 SDK breaks its compatible services</title><link>https://xuanwo.io/links/2025/02/aws_s3_sdk_breaks_its_compatible_services/</link><author>ulrischa</author><category>hn</category><pubDate>Thu, 20 Feb 2025 18:54:44 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[First of all, this is a good thing for me because checksums like  looks great—it's fast, secure, and has excellent SIMD support. As a user and developer, I'm excited to use it and integrate it into my projects. However, the S3 API is more than that. Many S3-compatible services are recommending that their users use the S3 SDK directly, and changing the default settings in this way can have a direct impact on their users.The recent AWS SDK bump introduced strong integrity checksums, and broke compatibility with many S3-compatible object storages (pre-2025 Minio, Vast, Dell EC etc).In Trino project, we received the error report (Missing required header for this request: Content-Md5) from several users and had to disable the check temporarily. We recommend disabling it in Iceberg as well. I faced this issue when I tried upgrading Iceberg library to 1.8.0 in Trino.Although this feature is good, the AWS team has implemented it poorly by enforcing it, causing issues for many users of related services. This reminds me of the position where Apache OpenDAL should stand.OpenDAL integrates all services by directly communicating with APIs instead of relying on SDKs, protecting users from potential disruptions like this one. OpenDAL's community also takes checksum support into deep consideration and is working to find a solution that benefits users while ensuring compatibility with unsupported services.Maybe it's time to move away from using S3 SDKs and switch to OpenDAL if you just want to access compatible services for data:OpenDAL has a wide range of integrations tests for s3 compatible services from minio to ceph.]]></content:encoded></item><item><title>You can’t build a moat with AI (redux)</title><link>https://frontierai.substack.com/p/you-cant-build-a-moat-with-ai-redux</link><author>cgwu</author><category>hn</category><pubDate>Thu, 20 Feb 2025 18:49:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: WinCse – Integrating AWS S3 with Windows Explorer</title><link>https://github.com/cbh34680/WinCse</link><author>cbh34680</author><category>dev</category><category>hn</category><pubDate>Thu, 20 Feb 2025 17:53:34 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[WinCse is an application that integrates AWS S3 buckets with Windows Explorer. Utilizing WinFsp and the AWS SDK, WinCse allows you to treat S3 buckets as part of your local file system, making file management simpler. The application is currently in development, with plans for additional features and improvements.]]></content:encoded></item><item><title>Ancient switch to soft food gave us overbite–the ability to pronounce &apos;f&apos;s,&apos;v&apos;</title><link>https://www.science.org/content/article/ancient-switch-soft-food-gave-us-overbite-and-ability-pronounce-f-s-and-v-s</link><author>NoRagrets</author><category>hn</category><pubDate>Thu, 20 Feb 2025 17:49:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Obsidian is now free for work</title><link>https://obsidian.md/blog/free-for-work/</link><author>hisamafahri</author><category>hn</category><pubDate>Thu, 20 Feb 2025 16:50:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[People in over 10,000 organizations use Obsidian for work.Starting today, the Obsidian Commercial license is optional. Anyone can use Obsidian for work, for free. If Obsidian benefits your organization, you can still purchase Commercial licenses to supportdevelopment.Nothing else is changing. No account required, no ads, no tracking, no strings attached. Your data remains fully in your control, stored locally in plain text Markdown files. All features are available to you for free withoutlimits.Why make this change? Simplicity. The Commercial license terms were confusing and added unnecessary complexity to our pricing. Furthermore, as the Obsidian Manifesto states: "we believe that everyone should have the tools to think clearly and organize ideas effectively". This change brings us closer to thatprinciple.People in over 10,000 organizations use Obsidian. Many work in high-security environments, like government, cybersecurity, healthcare, and finance. Some of the largest organizations in the world, including Amazon and Google, have thousands of employees using Obsidian every day. These teams rely on Obsidian to think more effectively and keep total ownership over privatedata.Previously, people at companies with two or more employees were required to purchase a Commercial license to use Obsidian for work. Going forward, the Commercial license is no longer required, but remains an optional way for organizations to support Obsidian, similar to the Catalyst license forindividuals.Organizations that support Obsidian are now featured on the Obsidian Enterprise page. Your organization can be showcased by purchasing 25 licenses ormore.Along with Commercial and Catalyst support, our add-on services, Sync and Publish help Obsidian remain 100% user-supported. In the future, we hope to offer more services designed for teams. As always, these will beoptional.]]></content:encoded></item><item><title>Launch HN: Confident AI (YC W25) – Open-source evaluation framework for LLM apps</title><link>https://news.ycombinator.com/item?id=43116633</link><author>jeffreyip</author><category>hn</category><pubDate>Thu, 20 Feb 2025 16:23:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Hi HN - we're Jeffrey and Kritin, and we're building Confident AI (https://confident-ai.com). This is the cloud platform for DeepEval (https://github.com/confident-ai/deepeval), our open-source package that helps engineers evaluate and unit-test LLM applications. Think Pytest for LLMs.We spent the past year building DeepEval with the goal of providing the best LLM evaluation developer experience, growing it to run over 600K evaluations daily in CI/CD pipelines of enterprises like BCG, AstraZeneca, AXA, and Capgemini. But the fact that DeepEval simply runs, and does nothing with the data afterward, isn’t the best experience. If you want to inspect failing test cases, identify regressions, or even pick the best model/prompt combination, you need more than just DeepEval. That’s why we built a platform around it.Confident AI is great for RAG pipelines, agents, and chatbots. Typical use cases involve allowing companies to switch the underlying LLM, rewrite prompts for newer (and possibly cheaper) models, and keep test sets in sync with the codebase where DeepEval tests are run.Our platform features a "dataset editor," a "regression catcher," and "iteration insights". The datasets editor in Confident AI allows domain experts to edit datasets while keeping them in sync with your codebase for evaluation. We’ll then generate sharable LLM testing/benchmark reports once DeepEval has finished running evaluations on these datasets that are pulled from the cloud. The regression catcher then identifies any regressions in your new implementation, and we use these evaluation results to determine the best iteration based on your metric scores.Our goal is to make benchmarking LLM applications so reliable that picking the best implementation is as simple as reading the metric values off the dashboard. To achieve this, the quality of curated datasets and the accuracy and reliability of metrics must be the highest possible.This brings us to our current limitations. Right now, DeepEval’s primary evaluation method is LLM-as-a-judge. We use techniques such as GEval and question-answer generation to improve reliability, but these methods can still be inconsistent. Even with high-quality datasets curated by domain experts, our evaluation metrics remain the biggest blocker to our goal.To address this, we recently released a DAG (Directed Acyclic Graph) metric in DeepEval. It is a decision-tree-based, LLM-as-a-judge metric that provides deterministic results by breaking a test case into finer atomic units. Each edge represents a decision, each node represents an LLM evaluation step, and each leaf node returns a score. It works best in scenarios where success criteria are clearly defined, such as text summarization.The DAG metric is still in its early stages, but our hope is that by moving towards better, code-driven, open-source metrics, Confident AI can deliver deterministic LLM benchmarks that anyone can blindly trust.The platform runs on a freemium tier, and we've dropped the need to signup with a work email for the next four days.Looking forward to your thoughts!]]></content:encoded></item><item><title>A cryptocurrency scam that turned a small town against itself</title><link>https://www.nytimes.com/2025/02/19/magazine/cryptocurrency-scam-kansas-heartland-bank.html</link><author>lxm</author><category>hn</category><pubDate>Thu, 20 Feb 2025 16:09:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[It was July 2023, and Tucker was hosting a meeting of the board of Heartland Tri-State Bank, a community-owned business in a small Kansas town called Elkhart. Heartland was a beloved local institution and a source of Tucker family pride: Jim served on the board with his elderly father, Bill, who founded the bank four decades earlier. All the board members — the Tuckers and several other farmers and businesspeople — had known one another for years.That evening, however, they were gathering to discuss what seemed, on its face, an epic betrayal. Over the past few weeks, the bank’s longtime president, a popular local businessman named Shan Hanes, had ordered a series of unexplained wire transfers that drained tens of millions of dollars from the bank. Hanes converted the funds into cryptocurrencies. Then the money vanished.]]></content:encoded></item><item><title>Spice86 – A PC emulator for real mode reverse engineering</title><link>https://github.com/OpenRakis/Spice86</link><author>alberto-m</author><category>hn</category><pubDate>Thu, 20 Feb 2025 15:47:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Lox – Oxidized Astrodynamics – A safe, ergonomic astrodynamics library</title><link>https://github.com/lox-space/lox</link><author>ElFitz</author><category>hn</category><pubDate>Thu, 20 Feb 2025 15:20:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Helix: A vision-language-action model for generalist humanoid control</title><link>https://www.figure.ai/news/helix</link><author>Philpax</author><category>hn</category><pubDate>Thu, 20 Feb 2025 14:30:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We're introducing Helix, a generalist Vision-Language-Action (VLA) model that unifies perception, language understanding, and learned control to overcome multiple longstanding challenges in robotics. Helix is a series of firsts:: Helix is the first VLA to output high-rate continuous control of the entire humanoid upper body, including wrists, torso, head, and individual fingers.Multi-robot collaboration: Helix is the first VLA to operate simultaneously on two robots, enabling them to solve a shared, long-horizon manipulation task with items they have never seen before. Figure robots equipped with Helix can now pick up virtually any small household object, including thousands of items they have never encountered before, simply by following natural language prompts.: Unlike prior approaches, Helix uses a single set of neural network weights to learn all behaviors—picking and placing items, using drawers and refrigerators, and cross-robot interaction—without any task-specific fine-tuning.: Helix is the first VLA that runs entirely onboard embedded low-power-consumption GPUs, making it immediately ready for commercial deployment.Video 1: Collaborative grocery storage. A single set of Helix neural network weights runs simultaneously on two robots as they work together to put away groceries neither robot has ever seen before.New Scaling for Humanoid RoboticsThe home presents robotics' greatest challenge. Unlike controlled industrial settings, homes are filled with countless objects–delicate glassware, crumpled clothing, scattered toys–each with unpredictable shapes, sizes, colors, and textures. For robots to be useful in households, they will need to be capable of generating intelligent new behaviors on-demand, especially for objects they've never seen before.The current state of robotics will not scale to the home without a step change. Teaching robots even a single new behavior currently requires substantial human effort: either hours of PhD-level expert manual programming or thousands of demonstrations. Both are prohibitively expensive when we consider how vast the problem of the home truly is.But other domains of AI have mastered this kind of instant generalization. What if we could simply translate the rich semantic knowledge captured in Vision Language Models (VLMs) directly into robot actions? This new capability would fundamentally alter robotics' scaling trajectory (Figure 1). Suddenly, new skills that once took hundreds of demonstrations could be obtained instantly just by talking to robots in natural language. The key problem becomes: how do we extract all this common-sense knowledge from VLMs and translate it into generalizable robot control? We built Helix to bridge this gap.Helix: A "System 1, System 2" VLA for Whole Upper Body ControlHelix is a first-of-its-kind "System 1, System 2" VLA model for high-rate, dexterous control of the entire humanoid upper body. Prior approaches face a fundamental tradeoff: VLM backbones are general, but not fast, and robot visuomotor policies are fast but not general. Helix resolves this tradeoff through two complementary systems, trained end-to-end to communicate:System 2 (S2): An onboard internet-pretrained VLM operating at 7-9 Hz for scene understanding and language comprehension, enabling broad generalization across objects and contexts.System 1 (S1): A fast reactive visuomotor policy that translates the latent semantic representations produced by S2 into precise continuous robot actions at 200 Hz.This decoupled architecture allows each system to operate at its optimal timescale. S2 can "think slow" about high-level goals, while S1 can "think fast" to execute and adjust actions in real-time. For example, during collaborative behavior (see Video 2), S1 quickly adapts to the changing motions of a partner robot while maintaining S2's semantic objectives.Video 2: Helix allows for fast fine grained motor adjustments, necessary when reacting to a collaborative partner, while carrying out novel semantic goals.Helix's design offers several key advantages over existing approaches:: Helix matches the speed of specialized single-task behavioral cloning policies while generalizing zero-shot to thousands of novel test objects.: Helix directly outputs continuous control for high-dimensional action spaces, avoiding complex action tokenization schemes used in prior VLA approaches, which have shown some success in low-dimensional control setups (e.g. binarized parallel grippers) but face scaling challenges with high-dimensional humanoid control.: Helix uses standard architectures - an open source, open weight VLM for System 2 and a simple transformer-based visuomotor policy for S1.: Decoupling S1 and S2 allows us to iterate on each system separately, without constraints of finding unified observation space or action representations.Model and Training DetailsWe collect a high quality, multi-robot, multi-operator dataset of diverse teleoperated behaviors, ~500 hours in total. To generate natural language-conditioned training pairs, we use an auto-labeling VLM to generate hindsight instructions. The VLM processes segmented video clips from the onboard robot cameras, prompted with: "What instruction would you have given the robot to get the action seen in this video?" All items handled during training are excluded from evaluations to prevent contamination.Our system comprises two main components: S2, a VLM backbone, and S1, a latent-conditional visuomotor transformer. S2 is built on a 7B-parameter open-source, open-weight VLM pretrained on internet-scale data. It processes monocular robot images and robot state information (consisting of wrist pose and finger positions) after projecting them into vision-language embedding space. Combined with natural language commands specifying desired behaviors, S2 distills all semantic task-relevant information into a single continuous latent vector, passed to S1 to condition its low-level actions.S1, an 80M parameter cross-attention encoder-decoder transformer, handles low-level control. It relies on a fully convolutional, multi-scale vision backbone for visual processing, initialized from pretraining done entirely in simulation. While S1 receives the same image and state inputs as S2, it processes them at a higher frequency to enable more responsive closed-loop control. The latent vector from S2 is projected into S1's token space and concatenated with visual features from S1's vision backbone along the sequence dimension, providing task conditioning.S1 outputs full upper body humanoid control at 200hz, including desired wrist poses, finger flexion and abduction control, and torso and head orientation targets. We append to the action space a synthetic "percentage task completion" action, allowing Helix to predict its own termination condition, which makes it easier to sequence multiple learned behaviors.Helix is trained fully end-to-end, mapping from raw pixels and text commands to continuous actions with a standard regression loss. Gradients are backpropagated from S1 into S2 via the latent communication vector used to condition S1's behavior, allowing joint optimization of both components. Helix requires no task-specific adaptation; it maintains a single training stage and single set of neural network weights without separate action heads or per-task fine-tuning stages.During training, we add a temporal offset between S1 and S2 inputs. This offset is calibrated to match the gap between S1 and S2's deployed inference latency, ensuring that the real-time control requirements during deployment are accurately reflected in training.Optimized Streaming InferenceHelix's training design enables efficient model parallel deployment on Figure robots, each equipped with dual low-power-consumption embedded GPUs. The inference pipeline splits across S2 (high-level latent planning) and S1 (low-level control) models, each running on dedicated GPUs. S2 operates as an asynchronous background process, consuming the latest observation (onboard camera and robot state) and natural language commands. It continuously updates a shared memory latent vector that encodes the high-level behavioral intent.S1 executes as a separate real-time process, maintaining the critical 200Hz control loop required for smooth whole upper body action. It takes both the latest observation and the most recent S2 latent vector. The inherent speed difference between S2 and S1 inference naturally results in S1 operating with higher temporal resolution on robot observations, creating a tighter feedback loop for reactive control. This deployment strategy deliberately mirrors the temporal offset introduced in training, minimizing the train-inference distribution gap. The asynchronous execution model allows both processes to run at their optimal frequencies, allowing us to run Helix as fast as our fastest single task imitation learning policies.Video 3: Helix's VLA controls the full humanoid upper body, a first in robot learning.Fine-grained VLA whole upper body controlHelix coordinates a 35-DoF action space at 200Hz, controlling everything from individual finger movements to end-effector trajectories, head gaze, and torso posture. Head and torso control pose unique challenges—as they move, they change both what the robot can reach and what it can see, creating feedback loops that have historically caused instability. Video 3 demonstrates this coordination in action: the robot smoothly tracks its hands with its head while adjusting its torso for optimal reach, all while maintaining precise finger control for grasping. Historically, achieving this level of precision with such a high-dimensional action space has been considered extremely challenging, even for a single known task. To our knowledge, no prior VLA system has demonstrated this degree of real-time coordination while maintaining the ability to generalize across tasks and objects.Video 4: Helix coordinates precise multi-robot manipulation.Zero-shot multi-robot coordinationWe push Helix to the limit in a challenging multi-agent manipulation scenario: collaborative zero-shot grocery storage between two Figure robots. Video 1 showcases two fundamental advances: The robots successfully manipulate entirely novel groceries—items never encountered during training—demonstrating robust generalization across diverse shapes, sizes, and materials. Additionally, both robots operate using identical Helix model weights, eliminating the need for robot-specific training or explicit role assignments. They achieve coordination through natural language prompts like "Hand the bag of cookies to the robot on your right" or "Receive the bag of cookies from the robot on your left and place it in the open drawer" (see Video 4). This marks the first demonstration of flexible, extended collaborative manipulation between multiple robots using a VLA, particularly significant given their successful handling of completely novel objects.Emergent "Pick up anything"We find that Figure robots equipped with Helix can pick up virtually any small household object with a simple "Pick up the [X]" command. In systematic testing, the robots successfully handled thousands of novel items in clutter—from glassware and toys to tools and clothing—without any prior demonstrations or custom programming.Particularly notable is how Helix bridges the gap between internet-scale language understanding and precise robot control. When prompted to "Pick up the desert item", for instance, Helix not only recognizes that a toy cactus matches this abstract concept, but also selects the closest hand and executes the precise motor commands needed to grasp it securely.This general-purpose "language-to-action" grasping capability opens new exciting new possibilities for humanoid deployment in unstructured environments.Video 5: Helix translates high level conceptual commands like "Pick up the desert item" to low-level action.Helix's training is highly efficientHelix achieves strong object generalization with remarkably few resources. We train Helix with ~500 hours of high quality supervised data in total, a small fraction of the size of previously collected VLA datasets (<5%), and without any dependencies around multi-robot-embodiment collect or multiple stages of training. We note that this is a scale of collect more comparable to modern -task imitation learning datasets. Despite this comparatively small data requirement, Helix scales to the significantly more challenging action space of full upper body humanoid control, with high-rate, high-dimensional outputs.Existing VLA systems often require specialized fine-tuning or dedicated action heads to optimize performance across different high-level behaviors. Remarkably, Helix achieves strong performance across diverse tasks with a single unified model. Using just one set of neural network weights (7B for System 2, 80M for System 1), Helix picks and places items in various containers, operates drawers and refrigerators, coordinates dexterous multi-robot handovers, and manipulates thousands of novel objects.Video 6: "Pick up the helix"We have presented Helix, the first Vision-Language-Action model to directly control an entire humanoid upper body from natural language. Unlike earlier robot systems, Helix is capable of generating long-horizon, collaborative, dexterous manipulation on the fly without any task-specific demonstrations or extensive manual programming. Helix displays strong object generalization, being able to pick up thousands of novel household items with varying shapes, sizes, colors, and material properties never encountered before in training, simply by asking in natural language. This represents a transformative step forward in how Figure scales humanoid robot behaviors—one that we believe will be pivotal as our robots increasingly assist in everyday home environments.While these early results are truly exciting, we think they only scratch the surface of what is possible. We are eager to see what happens when we scale Helix by 1,000x and beyond. If you’re as fascinated by the possibilities of Helix—and the future of dexterous humanoid robotics, we invite you to join us on this journey.Consider joining our Helix team to help scale Embodied AI to millions of robots. Check out our open roles here. ]]></content:encoded></item><item><title>RT64: N64 graphics renderer in emulators and native ports</title><link>https://github.com/rt64/rt64</link><author>klaussilveira</author><category>hn</category><pubDate>Thu, 20 Feb 2025 13:26:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>