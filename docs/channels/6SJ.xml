<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Go</title><link>https://www.awesome-dev.news</link><description></description><item><title>How to Data Engineer the ETLFunnel Way</title><link>https://dev.to/vivekburman/how-to-data-engineer-the-etlfunnel-way-jee</link><author>Vivek Burman</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 1 Nov 2025 16:26:00 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  Part 4 — Mastering Pipeline Termination

  
  
  When Your Pipeline Needs to Know When to Stop
Picture this: You've built a beautiful streaming data pipeline. Data flows in smoothly, transformations happen like clockwork, and everything seems perfect. Then you check your cloud bill at the end of the month and realize your pipeline has been running 24/7, processing mostly empty queues during off-peak hours. Sound familiar?This is where  come in — one of  most powerful features for managing long-running and streaming pipelines.
  
  
  The Problem: Infinite Pipelines in a Finite World
Traditional batch pipelines have it easy. They process all available data and exit gracefully when done. But modern data engineering often involves: that continuously listen for new events waiting for messages that might never come that need to respond to sporadic triggers that should adapt to business hoursThese pipelines don't naturally terminate. They run forever — or until something crashes, resources run out, or someone manually kills them. That's wasteful, expensive, and frankly, inelegant.Termination Rules in ETLFunnel provide intelligent, configurable exit conditions for your pipelines. Think of them as the "smart shutdown" mechanism that knows when continuing is pointless or unproductive.Instead of letting your pipeline run indefinitely, you define clear conditions for when it should gracefully exit:This simple rule says: "Stop after processing 10,000 records OR after 5 minutes of no activity — whichever comes first."
  
  
  The Four Pillars of Termination
ETLFunnel gives you four powerful mechanisms to control pipeline termination:
  
  
  1. MaxRecords: The Volume Limit
Perfect for: Development, testing, sampling large datasets: You're testing a new transformation on production data. Instead of processing millions of records, you sample the first 1,000 to verify everything works correctly.
  
  
  2. IdleTimeout: The Silence Detector
Perfect for: Event-driven pipelines, off-peak optimization: Your pipeline consumes from a message queue. During nights and weekends, new messages are rare. Why keep the pipeline running? If nothing arrives for 10 minutes, shut it down gracefully.
  
  
  3. MaxPipelineTime: The Duration Guard
Perfect for: Cost control, scheduled batch windows: You have a 2-hour processing window during off-peak hours. Regardless of how much data is available, the pipeline must complete within this window to avoid impacting other systems.
  
  
  4. UserDefinedCheckFunc: The Custom Logic
Perfect for: Complex business rules, adaptive behaviorThis is where termination rules become truly powerful. You can implement any custom logic based on pipeline state:
  
  
  Scenario 1: The Smart Streaming Consumer
: You're consuming from Kafka topics that have variable message rates. During business hours, thousands of messages per second. At night, maybe one every few minutes.: Pipeline runs as long as data flows. If messages stop for 15 minutes, it shuts down gracefully. You can restart it with a scheduled trigger or when new data arrives. No more 24/7 execution for sporadic workloads.
  
  
  Scenario 2: The Cost-Conscious Development Pipeline
: Developers are testing pipeline changes against production-like data. Full dataset has 50 million records, but you only need to verify logic.: Each test run processes at most 5,000 records or runs for 10 minutes. Fast feedback, minimal compute costs.
  
  
  Scenario 3: The Adaptive Business-Hours Pipeline
: You process user activity events, but your users are primarily in US timezones. Running full capacity globally is wasteful.: During business hours, pipeline tolerates longer idle periods. Outside business hours, it shuts down quickly if activity drops.
  
  
  Best Practices: Getting Termination Right

  
  
  1. Choose the Right CheckInterval
The  determines how frequently your termination conditions are evaluated. It's a balance: (< 5 seconds): Unnecessary overhead checking conditions constantly (> 1 minute): Slow to respond, pipeline may run longer than needed: 10–30 seconds for most use cases
// For responsive termination
CheckInterval: 10 * time.Second
// For lower overhead on stable pipelines
CheckInterval: 30 * time.Second

  
  
  2. Always Log Termination Events
Future you will thank present you for clear termination logging:checkProps.Logger.Info("Pipeline terminating",
    zap.String("reason", "idle timeout exceeded"),
    zap.Duration("idle_duration", idleDuration),
    zap.Uint64("total_processed", checkProps.TotalMessages),
    zap.Duration("total_runtime", time.Since(checkProps.StartTime)),
)

  
  
  3. Combine Multiple Conditions
Don't rely on a single termination condition. Defense in depth:
  
  
  4. Test in Development First
Termination logic can be tricky. Test with different scenarios:What happens if no data arrives?What if data arrives continuously?What if there are brief idle periods?Use aggressive limits in development:
  
  
  5. Monitor Termination Patterns
Track why your pipelines terminate. If they're always hitting , maybe your processing is too slow. If they're always hitting , maybe your upstream data source has issues.Termination Rules are your pipeline's exit strategy. They transform potentially runaway processes into well-behaved, cost-effective, and maintainable systems.In a world where data never stops flowing, knowing when to stop processing is just as important as knowing how to process it. ETLFunnel's Termination Rules give you that control — simply, powerfully, and elegantly.Your cloud bill will thank you. Your operations team will thank you. And you'll sleep better knowing your pipelines won't run forever.Ready to optimize your pipeline termination?Visit etlfunnel.com today to sign up for a free trial of our SaaS platform and transform your data engineering workflows.]]></content:encoded></item><item><title>How to Data Engineer the ETLFunnel Way</title><link>https://dev.to/vivekburman/how-to-data-engineer-the-etlfunnel-way-2g9d</link><author>Vivek Burman</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 1 Nov 2025 16:19:53 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In Part 1, we covered idempotency, retries, and recovery — the foundational mechanisms that make pipelines resilient to failure.In Part 2, we explored  — how to scale ETL workloads intelligently based on available hardware and data volume.But here's the critical question we haven't answered yet:When your pipeline runs successfully, how do you track what actually committed?When it fails, where did the failures go, and how do you replay them?That's where  and  hooks come in — the twin pillars of pipeline observability and incident management.You've built a robust ETL pipeline. It extracts data from Postgres, transforms it, and loads it into Elasticsearch. Everything works… until you need to answer these questions:"Which records were successfully committed in the last run?""Did the pipeline skip any records during transformation?""What happens when a destination write fails?""Can I replay failed records without reprocessing the entire dataset?"Without structured tracking, you're flying blind. Success and failure both happen in a black box. You have logs, sure — but logs are unstructured, hard to query, and don't give you a recovery path.This is where  and  hooks transform your pipeline from a fire-and-forget script into a fully auditable, recoverable data system.
  
  
  The Concept: Checkpoint and Backlog Hooks
 fire automatically after every successful write to the destination. They answer the question:  fire automatically when a write to the destination fails. They answer the question: "What just failed, and how do I handle it?" — every committed record is logged with metadata. — every failed record is persisted to a structured dead-letter queue (DLQ) with failure context. — backlogged records can be reprocessed as a separate flow, independent of the main pipeline.Let's see how this works in practice.
  
  
  Checkpoint Hook: Tracking Success
A checkpoint hook is invoked every time data is successfully written to the destination. It receives:Access to source, destination, and auxiliary databasesThe pipeline execution contextHere's a real implementation: — every committed record is logged with timestamp, source, destination, and full payload. — track throughput, last commit time, and record count per pipeline. — trigger alerts or downstream processes for specific record types (e.g., high-value transactions).
  
  
  Backlog Hook: Handling Failure
A backlog hook is invoked when a write to the destination fails. It receives:Access to auxiliary databases for DLQ storageThe pipeline execution contextHere's how you implement it: — failed records are stored in a queryable table, not just scattered in logs. — track failure rates, last failure time, and failure count per pipeline. — identify and prioritize critical failures for immediate attention. — the main pipeline continues running even when individual writes fail.
  
  
  The Replay Flow: Turning Failures Into Recovery
Here's where it all comes together.Failed records in the  table don't just sit there. You can build a  that:Reads records from the DLQ (with )Applies the same transformations as the main pipelineAttempts to rewrite to the destinationUpdates the DLQ status to  or Tracks replay progress with its own checkpointsThis is a first-class recovery mechanism — not an ad-hoc script, but a structured flow with the same guarantees as your main pipeline.Checkpoint and backlog hooks transform your pipeline from a black box into a glass box. — every committed record is tracked. Every failed record is captured. — trace any record from source to destination, with timestamps and metadata. — replay failed records without reprocessing the entire dataset. — know exactly what succeeded, what failed, and what's pending.This is the difference between  and pipelines that move data reliably.Ready to Build Auditable, Recoverable ETL Pipelines? Visit etlfunnel.com today to sign up for a free trial of our SaaS platform and transform your data engineering workflows.]]></content:encoded></item><item><title>How to Data Engineer the ETLFunnel Way</title><link>https://dev.to/vivekburman/how-to-data-engineer-the-etlfunnel-way-4k2g</link><author>Vivek Burman</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 1 Nov 2025 16:14:26 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In , **we discussed **reliability — how idempotency, retries, and recovery make your ETL pipelines resilient.But once things start scaling, reliability alone isn't enough. You start asking:"How do I make sure every machine runs at its full potential?"
"What if some datasets are 10× larger than others?"
"Can my orchestration layer adapt automatically?"That's the heart of today's topic: — tuning your ETL workload based on the machine and the data it's running on.You've got 10 flows syncing data from Postgres to Elastic. They're running across multiple regions and machines: Region | Machine Type       | Postgres Schemas | Data Volume (per schema)
 -----  | -----------------  | ---------------- | -----------------------
 US     | 32-core, 128GB RAM | 200+             | Medium                   
 EU     | 8-core, 32GB RAM   | 50               | Small                    
 APAC   | 16-core, 64GB RAM  | 100              | Very Large               
If you run the same orchestration plan everywhere, you'll waste compute in the US region and choke the EU one.This is where orchestration hooks come in — to shape your execution plan dynamically before any flow even starts.
  
  
  The Concept: Orchestration Hooks
An  is a pre-step that decides how many replicas of a flow or pipeline should exist and how they should be distributed based on:Available hardware (CPU, cores, memory)Data volume (number of rows, partitions, tables)Connection characteristics (source latency, throughput)You can define hooks at two levels: — to replicate entire flows — to replicate pipelines inside a flowETLFunnel's engine defines an orchestration contract:The idea is simple: each entity represents a pipeline unit, and the  decides how many replicas to create and what their tuning parameters should be.
  
  
  Flow-level orchestration — scale by CPU
This is a  orchestration. Each pipeline replica maps to a CPU core, so your ETL workload scales automatically with the available cores on that node.
  
  
  Pipeline-level orchestration — scale by data volume
Let's take it a step further.You can use your  (which implements ) to introspect data size and partition the workload accordingly.Now orchestration adapts not just to , but also to . A pipeline that needs to move 100M rows automatically gets partitioned into multiple smaller replicas, each handling a subset.
  
  
  Combined: Flow + Pipeline Orchestration
In real-world setups, you can even .Flow orchestration splits jobs by CPU capacity.Pipeline orchestration further divides heavy tables within each flow.The result is a perfectly balanced plan — CPU-efficient, data-aware, and regionally optimized.Dynamic orchestration gives you: across heterogeneous environments. — every machine operates at full capacity.Data-aware load balancing — big datasets automatically spread across workers. — your flow definitions remain the same, only orchestration changes.You've effectively made your ETL system  — a foundational step toward distributed dataflow intelligence.Ready to build resilient, idempotent ETL pipelines that handle can scale to any environment? Visit etlfunnel.com today to sign up for a free trial of our SaaS platform and transform your data engineering workflows.]]></content:encoded></item><item><title>Bringing Rust &amp; Go-Inspired Functional Error Handling to TypeScript</title><link>https://dev.to/veerakumarak/bringing-rust-go-inspired-functional-error-handling-to-typescript-1o80</link><author>veerakumarak</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 1 Nov 2025 14:26:31 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Functional programming isn’t just for Haskell or Scala anymore — with TypeScript’s evolving type system, we can now model powerful concepts like , , and  in a clean, type-safe way. is a small but expressive functional programming library for TypeScript that brings together the best of  with .
  
  
  💡 Why Another FP Library?
Most TypeScript codebases handle errors using plain  blocks, which can get messy and unpredictable — especially in async-heavy systems or service layers.This library was built to solve three real-world problems:Make error handling composable and predictableAvoid null checks everywhere with an expressive Option typeIntroduce domain-specific failures inspired by Go and DDDIn short: make  a first-class citizen of your TypeScript code.Simple immutable key–value pairOptional value wrapper (Rust’s )Success/error container (Rust’s )Base error abstraction (Go’s )Helper methods for working with , , etc.Domain-level exception classes
  
  
  🧠 Option — Safe Optional Values
Rust’s  inspired our implementation.
It lets you express “value or none” semantics clearly.The  type (inspired by Rust) ties it all together — representing success or failure outcomes without throwing exceptions.
  
  
  💥 Failure — When Things Go Wrong
Inspired by Go’s  and Kotlin’s , this class makes failures  and composable.Helper methods to make common failure patterns easier.
  
  
  🚨 Domain-Specific Failures
You can define meaningful, domain-aware failures — no more vague .Built-in failure types include:Each extends the base  class and adds clear semantic meaning to your application’s flow.This library was designed for use in , , and  where:You need robust error propagationYou prefer explicit control flow over exceptionsYou want Go-like simplicity with Rust-like safetyThen start composing safe, expressive TypeScript today.
No more surprise , no more hidden exceptions.By combining  patterns with Go’s pragmatic error philosophy, we can bring clarity and safety to everyday TypeScript.If you’re tired of fighting , or your codebase is full of  guards — give this library a spin.🧡 Happy functional programming in TypeScript!]]></content:encoded></item><item><title>Go Microservices Boilerplate Series Part 3: Redis, Healthchecks, Observability (Prometheus Metrics, OpenTelemetry Tracing)</title><link>https://dev.to/sagarmaheshwary/go-microservices-boilerplate-series-part-3-redis-healthchecks-observability-prometheus-metrics-32jo</link><author>Sagar Maheshwary</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 1 Nov 2025 13:29:51 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Welcome back to the Go Microservices Boilerplate series!In Part Two, we integrated Postgres using GORM, along with migrations and seeders, discussed Service layer pattern, created UserService example which we integrated with SayHello RPC, and ended the article with an integration test using testcontainers.In this part, we will look at Redis integration for caching, Healthchecks for service monitoring, Observability with Prometheus metrics and OpenTelemetry tracing to make our microservice production-ready. We’ll also walk through a sample Grafana dashboard for metrics visualization and spin up a second service to demonstrate end-to-end distributed tracing in action.Observability (Metrics, Tracing)Caching is an important part of scaling any service. It reduces database load and speeds up response times by storing frequently accessed data in a fast in-memory store. We’ll use Redis, the most popular in-memory key–value store, for this purpose.Redis supports far more than simple caching — features like Pub/Sub messaging, geospatial queries, and time series are built in — but for this boilerplate, we’ll keep things focused on basic caching functions that can easily be extended later.We start by updating our  with a new  struct:Then we load it inside :Next, we’ll create  where our Redis implementation will live:Here, we’ve followed the same modular pattern used across other components. We define a  interface that can be easily mocked in unit tests, and a concrete  struct implementing basic caching functions along with  for health checks and  for graceful shutdown. Finally, we provide a  constructor to instantiate the cache service cleanly.Now we initialize our cache service in :This setup ensures that Redis starts alongside our other dependencies and closes gracefully when the service shuts down.To integrate caching into our application logic, we’ll update the  method. It will now first look for the user data in Redis before querying the database. If the record isn’t found in cache, it’s fetched from the database and then written back to Redis for subsequent requests.Since we’ve added another dependency, we’ll extend the  constructor to accept the cache service:Next, we update our gRPC server to pass the cache instance from :Finally, here’s how caching looks inside :Now, when you call the  RPC, the user data will come from Redis on the second request (after being cached during the first). The cache expires after one minute, after which a new request will repopulate it. You can also log cache hits/misses to observe behavior.Since we added caching to , we’ll update our integration tests accordingly. To ensure realistic conditions, we’ll spin up a real Redis instance using Testcontainers. Let’s start by creating a helper function  in :And then update the  test to include Redis:For this test, we only need to confirm that  behaves correctly — whether the data came from the database or cache isn’t important here. The goal is to ensure the service works seamlessly with both persistence and caching layers under test conditions.Service monitoring is critical in production environments. Although gRPC includes a built-in health check protocol, it only exposes a single RPC endpoint. This makes it less flexible if you want to differentiate between basic service liveness and full readiness to serve traffic.In this boilerplate, we’ll implement two simple REST endpoints —  and  — for clarity and flexibility.We’ll start by defining a  in internal/service/health.go. This service will be used by our  endpoint to verify that all dependencies (like the database and cache) are healthy before declaring the service ready.The  method inside  validates the health of connected dependencies and returns their status, which we can later expose through our API.Next, we’ll implement the REST handlers for  and  inside internal/transports/http/server/handler/health.go:The  handler simply returns  with an HTTP 200 — it’s only responsible for confirming that the service process is running.The  handler, on the other hand, uses the  to confirm that all required dependencies (like the database and Redis) are operational.Now, let’s wire up our HTTP server in internal/transports/http/server/server.go:This follows the same structure as our . We define both  (for custom listeners) and  (for normal TCP-based startup from configuration).Next, update your configuration to fetch the HTTP server’s host and port from environment variables:Finally, let’s initialize and start the HTTP server in :Once everything is wired up, we can test the endpoints locally with :curl localhost:4000/livez
curl localhost:4000/readyz
With these two endpoints, you now have both basic and dependency-aware healthchecks that can be used by Kubernetes probes, load balancers, or monitoring tools.
  
  
  Observability (Metrics + Tracing)
In Part 1, we added structured logging to capture what’s happening inside our services. Logs are great for understanding what happened in a specific instance — but in a distributed system, we also need to understand how things behave and how requests flow across multiple services.That’s where metrics and tracing come in. capture quantitative data about our services — like request counts, error rates, and latency. They’re lightweight, easy to visualize, and ideal for alerting and performance dashboards. gives us a request-level view of our system — showing how a request moves between microservices and how much time is spent in each hop.Together, logging, metrics, and tracing give us full observability into our system’s behavior. In this part, we’ll implement metrics and tracing, then spin up Prometheus, Grafana, and Jaeger to see them in action.Prometheus is a popular open-source monitoring system that scrapes metrics from your services, stores them as time-series data, and makes them queryable through PromQL. We'll use the official prometheus/client_golang library to instrument our services and exposes metrics for scraping.We’ll begin by creating a  inside internal/observability/metrics/metrics.go. This service will handle Prometheus setup, expose default metrics, and provide a simple way to register custom collectors.To keep metrics modular, we define a small  interface. Each component — like gRPC, HTTP, or Redis — can implement this interface to expose its own metrics while keeping code organized.Next, we add gRPC-specific metrics inside a new  file. This includes counters and histograms for request counts, and latency — useful for tracking performance trends across gRPC calls.We then create a gRPC interceptor that automatically records these metrics for every incoming request. This way, we don’t need to manually instrument every handler — metrics are collected transparently as part of the request lifecycle.We then attach the interceptor to gRPC server via grpc.ChainUnaryInterceptor:Once metrics are collected, we need a way for Prometheus to access them. We expose a  endpoint in our HTTP server, which Prometheus will scrape periodically.Now Let's register metrics to HTTPServer in :Finally, we update our configuration to include a METRICS_ENABLE_DEFAULT_METRICS option. This flag lets us control whether we also expose Go’s built-in runtime metrics like garbage collection and goroutine counts — useful in production for monitoring resource usage.With this in place, our services now export rich, structured metrics that can be scraped by Prometheus and visualized in Grafana. These metrics will serve as the backbone for our dashboards and alerts once we deploy to production.Tracing is the third pillar of observability, alongside metrics and logs. While metrics tell you how your system behaves overall, and logs describe what happened at a specific moment, tracing shows you how a request flows across services. It’s especially valuable in microservice architectures where a single client request might touch multiple services, databases, or queues before completing.We’ll be using OpenTelemetry — a popular open-source standard for collecting distributed traces — and connect it with Jaeger for visualization. You can also integrate it later with other backends like Zipkin, Tempo, or AWS X-Ray if needed.Let’s start by defining our  in internal/observability/tracing/tracing.go:Next, we’ll update our configuration to include tracing settings such as service name and exporter in :Now that the config is ready, we can initialize the tracer in  so that every part of our application has access to it:To propagate trace context across service boundaries, we add a gRPC . This ensures that when a request comes into our service, it either starts a new trace or continues an existing one passed from another service:From now on, every gRPC call will automatically create a trace that’s sent to Jaeger.Each trace is composed of multiple  — where a trace represents the entire journey of a request (for example, a user calling your API), and spans represent individual operations within that request (like “fetch user from DB” or “send email”).To demonstrate this, let’s add spans to our  handler and the . Each span will capture the work done by that specific function, and together they’ll form a full picture of a single request’s flow through the system.When you look at the trace in Jaeger, you’ll see a clear chain of execution — the main request trace leading into the  span, which then calls into the  span.The  method lets you attach useful context (like user IDs, order IDs, or error states) that can make debugging much easier when viewing a trace.Context propagation here is critical — notice that we always pass  forward. This context carries the  created by our gRPC , ensuring that every downstream function and service is linked to the same request chain.
  
  
  End to End Observability in Action
The boilerplate comes with a ready-to-use observability setup using Docker Compose, available in the examples branch. If you want to follow along, clone the repo and switch to that branch.We’ll first bring up our core application stack — which includes the service itself, PostgreSQL, and Redis:Once the application is running, we can start the observability stack. This will spin up Grafana, Prometheus, and Jaeger — all preconfigured to work with our service.docker compose  docker-compose.observability.yml up
Jaeger runs via the  image using its default in-memory storage to keep things lightweight and simple. Prometheus uses a basic scrape configuration that pulls metrics from our service’s  endpoint every eight seconds:Grafana comes with a sample dashboard already wired up to Prometheus, showcasing the gRPC metrics we defined earlier. Open a browser and navigate to http://localhost:3000 (default credentials are  / ).The dashboard has four panels visualizing data from the  and grpc_request_duration_seconds metrics:Average gRPC Request Duration — time seriesgRPC Request Latency (95th Percentile) — time seriesgRPC Requests per Method — time seriesTotal gRPC Requests — counter/gaugeThis sample dashboard offers a quick glimpse into request throughput and latency trends — perfect for demonstration and local testing. In a real production environment, you’d typically design a more comprehensive dashboard tailored to your service’s specific KPIs, error rates, and resource utilization patterns.Now that we’ve seen metrics in action, let’s turn to tracing. We already explored what a trace looks like for a single service, but the real power of tracing emerges when requests flow through multiple microservices. Distributed traces help you visualize the entire request path, pinpoint slow hops, and quickly identify where failures occur.To demonstrate distributed tracing, let’s spin up a second instance of our service that will act as another microservice in the request chain. Clone the same repository inside your project directory, switch to the examples branch, and create an environment file:git clone https://github.com/SagarMaheshwary/go-microservice-boilerplate.git go-microservice-boilerplate2
go-microservice-boilerplate2
git checkout examples
 .env.example .env
Now let’s add this new service to our main  file:Next, open the  handler of our main service and replace it with the following code.
This is just for demonstration — in a real-world setup, you would create a dedicated client inside  and reuse it across the service instead of creating a new connection for every request (since gRPC clients are long-lived):Note: In the internal/tracing/tracing.go file, make sure to include otel.SetTextMapPropagator(propagation.TraceContext{}). This line is missing from the part-3 source, and without it, traces between multiple services won’t connect properly. You can use either the  or  branch, as both already include this fix.Now, let’s trigger a request from the first service, which will internally call the second service. We’ll use  for this:grpcurl  ./proto/hello_world/hello_world.proto  localhost:5000 hello_world.Greeter/SayHello
Open Jaeger in your browser, and you should now see a  spanning across two services — one initiating the request, and the other handling it downstream:Clicking into the trace reveals the chain of spans showing each step of the process:This connected trace gives a clear view of how the request travels through the system — from the first service’s  call, to the gRPC client invocation, and finally to the second service’s  and its database lookup via .And that wraps up our  section.
We’ve now covered all three pillars — logging, metrics, and tracing — giving you complete visibility into your services. With this setup, you can detect issues early, measure system health, and trace requests across microservices, building a solid foundation for a production-grade monitoring stack.In this final part, we extended our microservice with Redis integration, health checks, and full observability using Prometheus metrics and OpenTelemetry tracing. We visualized key performance metrics in Grafana and explored distributed tracing with Jaeger — connecting multiple services to see how requests flow end-to-end.With this, our Go Microservice Boilerplate series comes to a close. You now have a solid foundation that combines clean project structure, containerization, configuration management, database integration, service-to-service communication, and observability — everything you need to kickstart a production-grade Go microservice project.You can use the master branch of the repository as a ready-to-use boilerplate to start new projects, or explore the examples branch to review all the code samples demonstrated throughout this series.Thanks for following along — and happy building!]]></content:encoded></item><item><title>Reactive Tree Management in Nuxt 4: How I Modeled Complex Hierarchies with Pinia</title><link>https://dev.to/smaug6739/reactive-tree-management-in-nuxt-4-how-i-modeled-complex-hierarchies-with-pinia-2m8f</link><author>Smaug#6739</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 1 Nov 2025 12:17:22 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[When I started building Alexandrie, I just wanted a fast, offline Markdown note-taking app I could rely on daily.
But as the project grew — with nested categories, shared workspaces, and access permissions — it evolved into something much more: a open-source knowledge management platform powered by Nuxt 4 and Go.This article walks through one of the toughest challenges I faced: how to model and manage hierarchical data efficiently, from the database to a reactive Pinia store.
  
  
  1. Unified Data Model: “Nodes” Over Multiple Tables
Early in development, I realized that managing categories, documents, and files as separate entities was becoming painful.
Every new feature — especially  and  — required deeper joins and complex recursive queries.Here’s what the early structure looked like conceptually:Workspace
 ├── Category A
 │    ├── Document 1
 │    └── Document 2
 └── Category B
      ├── Subcategory
      │    └── Document 3

  
  
  1.1 The problem with separate tables
Having multiple tables (, , ) worked fine until I introduced access control.
At that point, even simple questions like “what can this user see in this subtree?” required multiple recursive joins.
Performance and maintainability started to suffer.
  
  
  1.2 The unified “nodes” approach
The solution was to merge everything into a  — a unified model where everything is a .With this model, every element — document, folder, file — became a node.
This made the hierarchy recursive but uniform, enabling simple queries like:“What can user X access in this subtree?”Now, permission propagation and tree traversal both use a single recursive CTE or indexed path column, drastically improving maintainability and speed.
  
  
  2. Scalable Permission System
Building permissions on top of the nodes table required careful thought. I adopted a hybrid approach:A separate permissions table maps , , .On access checks, the system checks:If the user owns the target nodeIf a direct permission existsIf any ancestor node grants sufficient permissionThis balances fine-grained control and inheritance — users get access to everything under a node they’ve been granted permission for, without repeated recursive checks.
  
  
  3. Backend Stack: Go + REST + Modular Design
Language & framework: Go (Gin) — lightweight and performant for API endpoints.: Go (Gin) — for its simplicity, performance, and clean REST design.: MySQL (or compatible) — raw SQL for critical queries like subtree retrieval.: S3-compatible (MinIO, RustFS) — abstracted via a pluggable service for self-hosted setups.The API uses a flat structure (, , ) — simple, predictable, and easy to version.I separated business logic from data access (DAO layer) to keep the backend maintainable and extensible.
This paid off when refactoring: new storage backends or permission engines can now be added with minimal changes.
  
  
  4. Frontend Architecture: Data management
Surprisingly, the hardest part of building Alexandrie’s frontend wasn’t the UI — it was the data layer.
Representing thousands of interlinked notes in a reactive, permission-aware tree required careful design.In Alexandrie, .
A node can be a workspace, category, document, or resource — and each can contain others.
That means infinite nesting, partial hydration, and live updates when any part of the tree changes.In Alexandrie, everything is a Node.
Each node can be a workspace, category, document, or resource, and every node can contain others — effectively forming a tree structure that must remain reactive, searchable, and permission-aware across the app.Nested data: Users can nest documents/categories/resources infinitely deep.Partial hydration: Nodes are often fetched lazily or shared publicly, so the store must handle both partial and fully-hydrated nodes.Permission inheritance: Access rights propagate through parent nodes.Real-time reactivity: Any node update must immediately reflect across trees, search results, and UI components.Performance: Traversing large trees shouldn’t cause noticeable slowdowns.I built a dedicated Pinia store (useNodesStore) combined with a TreeStructure utility that keeps all nodes in a flat Collection (essentially a reactive Map), and reconstructs the hierarchical tree on demand.Then, a dedicated  class handles building the actual trees efficiently:O(1) access to any node via itemMapEfficient subtree generation (only rebuild what’s needed)A clean way to filter, search, or recompute derived data (tags, permissions, etc.)Integration with Pinia’s reactivity: the entire graph updates live when a node changes.Here are some high-level takeaways:Unify your data model early. Splitting multiple tables will bite you when adding features like permissions and sharing.Favor simplicity in API contracts. Flat endpoints scale better than deeply nested resource structures.Design for extensibility, not just immediate features. Adding plugin-like syntax blocks or alternate storage later becomes much easier.Permissions and hierarchy are hard. Caching accessible node sets and flattening ancestors helps avoid recursive query bottlenecks. 
  
  
  What’s Next & How to Contribute
Alexandrie is open-source and welcomes contributors. Areas where help is most welcome:Implement full offline supportAdd some cool new featuresIf you’re interested in self-hosted knowledge tools or modern note apps, feel free to star or contribute!]]></content:encoded></item><item><title>Why I Chose Go as My Main Backend Language — and Why You Might Too</title><link>https://dev.to/ahmed112/why-i-chose-go-as-my-main-backend-language-and-why-you-might-too-3h8b</link><author>Ahmed Alasser</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 1 Nov 2025 10:08:51 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[When I first started exploring backend development, I jumped between multiple languages — Python, Node.js, even a bit of Java.
Each had its strengths, but I kept feeling something was missing: clarity.Then I discovered Go (or Golang) — and it changed the way I thought about backend programming.
Go wasn’t flashy. It didn’t promise “magic” frameworks or a hundred shortcuts.
It just offered speed, simplicity, and structure — and that was exactly what I needed.*
Go was created at Google to solve a real problem:
Large, complex systems that were getting slower and harder to maintain.The goal was simple — create a language that’s easy to read, fast to run, and effortless to deploy.
That philosophy still shows in every line of Go code.From its clean syntax to its built-in tools, everything feels designed for clarity and productivity.
Here’s what makes it special 👇*⚡ 1. Simplicity That Doesn’t Sacrifice Power
*
Go has no unnecessary complexity.
No inheritance, no endless dependencies, and very little boilerplate.
It’s a language that forces you to focus on solving the problem, not fighting the syntax.package main
import "fmt"

func main() {
    fmt.Println("Hello, Go!")
}
That’s a complete Go program — no setup files, no configuration chaos.
You can compile it, run it, and deploy it in seconds.This simplicity means fewer bugs, faster learning, and a smoother path from idea to execution.*🚀 2. Performance That Feels Effortless
*
Go is a compiled language, so your code runs directly on the machine — not through an interpreter.
That gives you C-like performance with Python-like ease of writing.You don’t need to tweak garbage collection or write complex optimizations.
Go handles performance gracefully while letting you focus on business logic.For developers building APIs or scalable services, that’s a huge win.*🧵 3. Concurrency Made for Humans
*
Concurrency (running multiple tasks at once) is where Go truly shines.
Most languages make it complicated — Go makes it beautifully simple.With goroutines, you can run lightweight, concurrent tasks using just one keyword:That’s it. No threads, no heavy setup.
You can serve thousands of simultaneous requests efficiently — perfect for modern web backends and microservices.*🧩 4. A Batteries-Included Standard Library
*
Go doesn’t need a huge ecosystem to get started.
Its standard library already includes everything you need for real-world backend work:JSON handling (encoding/json)Testing (testing package)That means less time hunting for third-party packages, and more time building actual software.*🌍 5. A Growing, Practical Community
*
The Go community is one of the most pragmatic developer spaces out there.
It’s not about hype — it’s about real-world results.From startups to giants like Google, Uber, and Docker, Go powers production systems at scale.
You’ll find endless tutorials, open-source projects, and helpful developers who care about clean, efficient code.*
I chose Go because it fits how I think:
I like to build things that work, scale, and make sense.With Go, I spend less time debugging and more time creating.
It’s a language that rewards good habits — and punishes bad ones gently but effectively.Every line of Go code teaches you something about clarity, structure, and performance.*💡 My Advice for New Developers
*
If you’re just starting out in backend development, here’s my honest advice 👇Start simple. Don’t chase every new framework.Build small projects — APIs, CLI tools, or simple web apps.Read Go code. Learn from open-source projects.Focus on fundamentals — data flow, concurrency, and clean code matter more than fancy tools.You’ll be amazed how much you can build with just Go’s standard library and a bit of curiosity.*
This article is the start of a series I’m writing about learning Go from the ground up — from building your first API to deploying it in production.
If you’re on a similar journey, follow me here on Medium.
Let’s learn, experiment, and grow together — one Go project at a time. 🚀*🎓 Coming Next: A Beginner-Friendly Go Course
*
After sharing my journey with Go and why I chose it as my main backend language, I’ve decided to take things a step further.
I’m launching a complete Go course for beginners — a hands-on series that will take you from writing your first line of code to building real-world projects.We’ll cover everything from the basics and API building to file handling and concurrency — all explained clearly and simply. so you don’t miss the first lesson — coming very soon! 🚀**Thanks for reading**! 🙌
**If you enjoyed this post, drop a 👏 or comment below — your support means a lot.
Let’s connect and share ideas!]]></content:encoded></item><item><title>I Automated JSON i18n Translation After Hitting the Same Workflow Issues on Every Project</title><link>https://dev.to/juliandreas/i-automated-json-i18n-translation-after-hitting-the-same-workflow-issues-on-every-project-28b7</link><author>Julian</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 1 Nov 2025 09:21:52 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Every multilingual project I've worked on follows the same frustrating pattern:Someone adds a new English string, forgets to add it to the other locale files, and two weeks later we discover half the app is untranslated in Swedish. Or worse, translation keys pile up in one locale but get deleted in another, and nobody notices until a customer complains.The "proper" solution seems to be paying for a translation management SaaS (Lokalise, Locize, etc.), but I didn't want vendor lock-in or another subscription. I just wanted the grunt work automated.I created Dire, a CLI tool that handles the boring parts of i18n maintenance:: Diffs your JSON files, finds missing keys, translates them using your choice of provider (DeepL, Claude, OpenAI, Google Translate, etc.), and updates everything. One command, done.: The  flag removes keys from non-reference locales that don't exist in your most complete locale. No more "keys that should've been deleted months ago" hanging around.Translation memory + glossary: Remembers translations so identical strings don't get re-translated. Saves API costs and keeps terminology consistent across your app.: The  flag validates that all translations are complete and exits with the appropriate code. Catches missing translations in PRs before they hit production.: It's just a CLI that manipulates your JSON files. You own the data, use any provider, switch providers anytime.Install via npm (it's a Go binary distributed through npm for convenience):Initialize configuration in your project:This creates a  file. Here's a minimal config: The config file is optional - you can configure everything via CLI flags instead. The TOML file is just convenient for storing project settings, managing glossaries, and switching between multiple providers.Add your API key to a  file:your-key-here
That's it. Missing keys get translated and your files stay in sync.Let's say your  has:But your  is missing the "register" key:Run  and it automatically adds:Dire remembers every translation it makes. If you use "Dashboard" in multiple places, it translates it once and reuses the translation everywhere. This:Define custom terminology that should always be translated the same way:Override provider translationsWork bidirectionally (en→fr and fr→en)Don't consume API creditsAdd this to your CI pipeline:It validates that all locales are complete and exits with code 1 if translations are missing. Catches incomplete i18n before deployment.Over time, locale files accumulate keys that were deleted from the reference locale but not from translations:This removes keys from non-reference locales that don't exist in your most complete locale. Keeps your files clean.Only want to translate specific keys?dire 
  
  
  Multiple Provider Support
Configure multiple providers and switch between them:Switch providers without editing the config:dire  openai
: DeepL, Google Translate, Azure AI Translator: Claude, OpenAI, Gemini, Mistral, DeepSeekYou bring your own API key. No middleman, no markup, full control over costs.: Single executable with no runtime dependencies: Concurrent processing and smart batching handle large translation files efficiently: Builds for Linux, macOS, Windows (amd64 + arm64): Distribute via npm but runs as a native binary: Works with any i18n library that uses JSON files. Doesn't care if you use react-i18next, vue-i18n, or something custom.BYOK (Bring Your Own Key): You own the relationship with your translation provider. Switch providers anytime, no data migration needed.: Processes files locally, only talks to translation APIs when needed. Your translation data stays in your git repo.: Team members don't need to understand how it works.  and they're done.Developer adds new English stringsRuns  (which calls )Reviews translations (AI gets it right ~80% of the time)Commits everything togetherCI validates completeness with The  flag runs periodically in a cleanup PR to remove orphaned keys.Manual translation grunt workReal-time collaboration (it's a CLI, not a translation platform with team features)The  function spam in your components (that's a framework issue)Context-specific translations (AI struggles with ambiguity)100% accuracy (you still need to review translations)AI translation gets you 80% of the way there. You still need human review for context, tone, and domain-specific language.Q: How accurate are the translations?
A: Depends on the provider. DeepL and Claude are consistently 80-90% accurate for most languages. You still need human review, but it's much faster than translating from scratch.Q: What if I want to switch providers?
A: Change one line in your config or use the  flag. Your translation files don't change, so switching is instant.Q: Does it work with my i18n library?
A: If your library uses JSON files, yes. It doesn't care about your framework or i18n implementation.Q: How much does it cost?
A: The tool is free. You pay only for API usage to your chosen provider. DeepL, for example, costs ~$5-20/month for most projects.Q: Can I use it without AI?
A: Yes. Use  to create placeholder translations (empty strings), then fill them in manually. Or use  to only apply glossary and memory translations.I built this to scratch my own itch, but I'm curious if it solves the same problems for others. Let me know what you think or what features would make it more useful for your workflow.]]></content:encoded></item><item><title>Modernize Go with golangci-lint v2.6.0</title><link>https://dev.to/thevilledev/modernize-go-with-golangci-lint-v260-3e6d</link><author>Ville Vesilehto</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 31 Oct 2025 19:42:18 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[v2.6.0 adds the new  analyzer. It surfaces suggestions to adopt modern Go features from the standard library (e.g., , , ) and language (). Previously the tool was available as a separate binary, but it'll receive much more exposure now being included in golangci-lint.The  analyzer suggests clearer, more idiomatic code by using newer Go features. Each diagnostic includes a suggested fix designed to be behavior-preserving. Many improvements rely on Go ≥1.21 features. If your module uses an older  version, you'll see fewer suggestions and some fixes won't apply cleanly.
  
  
  Replace manual prefix handling with strings.CutPrefix

  
  
  Replace hand-rolled membership checks with slices.Contains

  
  
  Replace manual map clone loops with maps.Clone

  
  
  Prefer any over interface{}

  
  
  With golangci-lint (recommended)
golangci-lint run  modernize
Optionally apply fixes automatically (where supported):golangci-lint run  modernize Now with golangci-lint v2.6.0 out, I refactored this in coredns/coredns#7645. The  had a number of changes that will need to be addressed separately. Nice and clean now.]]></content:encoded></item><item><title>Implementing MQTT 5 in Go: A Deep Dive into Client Design - Part I</title><link>https://dev.to/monsieur_thib/implementing-mqtt-5-in-go-a-deep-dive-into-client-design-part-i-24p1</link><author>MrTib</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 31 Oct 2025 18:02:46 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In this article, I’ll share my journey of implementing an MQTT 5.0 client in Go.
We’ll cover protocol fundamentals, connection and session management, and, of course, publishing and receiving messages to and from a broker.Why build an MQTT client when libraries like Paho already exist? Because I enjoy writing clients, it’s a playground for fun concepts like protocol parsing, networking, concurrency, and performance tuning.
  
  
  Part I : Connecting to the Broker

  
  
  1. Understanding MQTT 5 Data Types
MQTT is a standard messaging protocol for IoT. It's designed as a lightweight pub/sub messaging transport for connecting remote device with small footprint and low bandwidth usage. MQTT 5 introduces several data types that are important for building a client from scratch:Implementing these correctly is the first step toward building a functional MQTT client in Go.I implemented the following functions, which will be used for encoding and decoding MQTT packets.
  
  
  1.c - Variable Byte Integer
This one is less straightforward, and from the specifications :  "The Variable Byte Integer is encoded using an encoding scheme which uses a single byte for values up to 127. Larger values are handled as follows. The least significant seven bits of each byte encode the data, and the most significant bit is used to indicate whether there are bytes following in the representation. Thus, each byte encodes 128 values and a "continuation bit". The maximum number of bytes in the Variable Byte Integer field is four. The encoded value MUST use the minimum number of bytes necessary to represent the value"It’s an efficient way to represent values from 0 to 268,435,455 while using as few bytes as possible.For example, to represent a value between 0 and 127, you only need one byte. For values between 128 and 16,383, you need two bytes. And so on, up to four bytes for the maximum range.Here’s a table to illustrate:16,384 (0x80, 0x80, 0x01)2,097,151 (0xFF, 0xFF, 0x7F)2,097,152 (0x80, 0x80, 0x80, 0x01)268,435,455 (0xFF, 0xFF, 0xFF, 0x7F)Lucky us, the specifications provide the following pseudo code :-) :do
   encodedByte = X MOD 128
   X = X DIV 128
   // if there are more data to encode, set the top bit of this byte
   if (X > 0)
      encodedByte = encodedByte OR 128
   endif
   'output' encodedByte
while (X > 0)

Where MOD is the modulo operator (% in C), DIV is integer division (/ in C), and OR is bit-wise or (| in C).

which can be easily translated into the following go code :
This function encodes an integer into Variable Byte Integer format and writes it to a buffer. It repeatedly takes the value modulo 128 to extract the lower 7 bits, sets the continuation bit (0x80) if more bytes are needed, and writes each byte to the buffer. The loop continues until the remaining length is zero.Some examples as encoded values :value : 127
Binary: 0111 1111
        └─ continuation bit = 0 (no more bytes to follow)
Since 127 fits in 7 bits, only one byte is needed, continuation bit is 0

value  :349
binary :1101 1101 | 0000 0010
        |           └─ continuation bit = 0 (no more bytes), 
        └─ continuation bit = 1 (more bytes follow), 
The following function is used to decode variable byte integer
We read a Variable Byte Integer from an io.Reader by decoding one byte at a time.
Each byte contributes 7 bits of data, and the most significant bit (MSB) acts as a continuation flag: if it’s set to 1, another byte follows, if it’s 0, the value is complete.On each iteration, the function:Masks out the MSB (buf[0] & 0x7F) to get the data bits.Adds them to the accumulated value, scaled by a multiplier (1, 128, 128², 128³).Checks if the MSB is not 1, if so, decoding is done.If more than four bytes are read, it returns an error because the MQTT spec limits Variable Byte Integers to four bytes.Binary data consists of a two-byte integer indicating its length, followed by that many bytes, limiting the size to 0–65,535 bytes.
  
  
  1.e - UTF-8 Encoded String
The specifications say : "string data is prefixed with a two byte length field that gives the number of bytes in the UTF-8 encoded string itself.Consequently, the maximum size of a UTF-8 Encoded String is 65,535 bytes.
Example: given the string "hello", the encoded string will be 0x00 0x05 0x68 0x65 0x6c 0x6c 0x6f
The first two bytes 0x00 0x05 represent the length of the string "hello" and the remaining bytes are the UTF-8 encoded string"The functions for encoding and decoding UTF-8 strings are straightforward and use the 2 byte integer and binary encode/decode functions we discussed earlier.A UTF-8 String Pair consists of two UTF-8 encoded strings ( explained in previous section 1.e) and is used to store name-value pairs. The first string represents the name, and the second represents the value.Now we have all these necessary functions, we can implement a connection to the broker.
  
  
  2. Connecting to the broker
Once a network connection to the broker is established (typically over TCP or WebSocket), the client must send a  packet to initiate a session.The MQTT 5  packet contains essential information for establishing a connection between the client and broker:
A unique ID that identifies the client to the broker. If not provided, the broker may assign one automatically (when Clean Start is true).
A flag that determines whether the broker should start a fresh session or resume a previous one with stored state (subscriptions, queued messages, etc.).
A time interval (in seconds) that specifies how often the client must communicate with the broker. If no messages are exchanged within this period, the client sends a PINGREQ packet to maintain the connection. A value of 0 disables the keep-alive mechanism.
Optional username and password fields for broker authentication, controlled by their respective flags in the Connect Flags byte.
An optional message that the broker will publish on behalf of the client if it disconnects ungracefully. This includes:: Where the message will be published: The message content: Quality of Service level (0, 1, or 2): Whether the message should be retained by the broker: MQTT 5 specific properties for the Will message
MQTT 5 introduces properties that provide additional metadata and capabilities, such as:Request/Response InformationUser Properties (custom key-value pairs)These fields work together to establish a reliable, authenticated connection with appropriate session handling and failure recovery mechanisms.Just like most MQTT control packets (e.g. PUBLISH, SUBSCRIBE, etc.), the CONNECT packet follows a general structure consisting of three sections:The fixed header is present in every MQTT control packet.
It always starts with one byte representing the packet type and flags, followed by one or more bytes representing the remaining length.Control Packet Type and FlagsBits 7–4 identify the packet type (e.g. 1 for CONNECT, 3 for PUBLISH). Bits 3–0 are reserved for flags specific to each packet type.Encoded as a , it specifies the total number of bytes in the Variable Header and Payload.For example, the first byte of a CONNECT packet is always 0x10 (binary 0001 0000), where:Bits 7–4 (0001) = Packet type 1 → CONNECTBits 3–0 (0000) = Flags (must be 0 for CONNECT)The Remaining Length is simply the total of the variable header and payload sizes, encoded using the Variable Byte Integer function we wrote earlier.As its name suggests, the variable header structure and content vary depending on the packet type.For the CONNECT packet, the variable header has a well-defined structure that looks like this:Always . Identifies the protocol being used.Indicates the protocol version. For MQTT 5.0, this value is .Bit flags defining session behavior (e.g. ), Will Message options, and authentication fields (Username/Password).Maximum time interval, in seconds, between control packets sent by the client.Variable Byte Integer + Property ListMQTT 5 introduces properties that provide extensibility and optional connection parameters.
The Connect Flags byte contains multiple bit fields, each controlling a specific connection behavior. 
This is an efficient way to encode 8 Boolean values in just one byte instead of using 8 separate bytes.Here’s the breakdown of each bit (from most significant to least):Set to 1 if a Username is present in the payload.Set to 1 if a Password is present in the payload.Set to 1 if the Will Message should be retained.Quality of Service level for the Will Message (0–2).Set to 1 if a Will Message is included.If 1, the client starts a new session; if 0, it resumes the previous one.The Connect Flags are encoded using the following function
We initialize an empty byte (), then for each flag that needs to be set, we use bitwise OR operations to set the appropriate bit to 1.For example, if  is true, we need to set bit 2 (counting from 0, right to left). We OR the flags byte with  (binary ) to set this bit. The |= operator performs a bitwise OR and assigns the result back to flags.For the Will QoS level (0-2), we use a left shift operation () to position the QoS value at bits 3-4. For instance, if willQoS is 2 (), shifting it left by 3 positions gives us  ().Encoding the Variable HeaderNow that we understand the structure, let's look at how to encode the complete variable header:MQTT 5 properties are encoded as:A Variable Byte Integer representing the total length of all propertiesA sequence of property ID (byte) + property value pairsEach property has a unique identifier (e.g.,  for Session Expiry Interval,  for Receive Maximum). Here's a simplified example:The payload contains the actual connection data referenced by the Connect Flags:The payload is encoded in this exact order and this is the function I wrote to encode it :Putting it all together :After sending a well-formed  packet, the broker responds with a  packet that includes a Reason Code and an optional Session Present flag in its variable header. 
The  packet may also include server-capability properties, such as the maximum supported , and it can override certain properties that were specified in the  packet.
For example, if the broker returns the  property, it replaces the client’s original  value specified in the  packet.For testing, I use the excellent testcontainers library, with EMQX as the MQTT broker and Redis to simulate a secured broker using username/password authentication.
Here's the code to set up a secured broker using username/password authentication backed by Redis, followed by connection tests:In this first part, we've built the foundation of our MQTT 5.0 client by implementing the core data types and the complete  packet encoding and decoding. We can now establish authenticated connections to any MQTT 5.0 broker.Publishing messages with different QoS levelsImplementing proper session managementThe complete implementation is available on GitHub. Feel free to explore the code, open issues, or contribute]]></content:encoded></item><item><title>Turning 500 Lines of If-Else Into a Config Switch: Strategy Pattern in Go</title><link>https://dev.to/aris_georgatos/turning-500-lines-of-if-else-into-a-config-switch-strategy-pattern-in-go-4ebe</link><author>Aris Georgatos</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 31 Oct 2025 17:46:15 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[When your core business logic becomes a high-risk bottleneck, every deployment feels like defusing a bomb. Here's how we used the Strategy Pattern to transform our most critical code path from a deployment risk into a configuration switch.
  
  
  The Problem: When a Core Business Rule is a High-Risk Bottleneck
We had a central piece of logic in our product publishing service that decided how products should be published, standalone items or grouped variants. This decision was critical and complex, driven by product categories.The real issue wasn't messy code, it was :: Changing this central logic always risked breaking other categories: We couldn't develop or test new logic independently: Reverting a bad change meant redeploying the entire serviceOur roadmap required migrating all products to a grouped model eventually. We needed a way to swap our publishing logic safely, test it in isolation, and roll back instantly if needed.Sound impossible? Enter the Strategy Pattern.The breakthrough came when we realized: we don't need to change the decision-making logic, we need to swap out the decision-maker entirely.Think of it like a chess game. Instead of rewriting the rules mid-game, we swap the entire chess AI. Same board, same pieces, different brain making the moves. Instead of modifying a 500 line if-else block to support a new publishing rule, we write a new 50 line strategy class. The service code? Untouched.The pattern works like this:Your service calls the interface. The interface delegates to whichever strategy is active. The service never knows the difference.
  
  
  Step 1: Defining the Contract (The Interface)
This is the magic: The interface doesn't know about Fashion, Electronics, or your business rules. It only knows the questions that need answers.
  
  
  Step 2: Encapsulate Current Logic (Strategy A)
We took all that scary if-else logic and wrapped it in a neat package: We didn't change a single business rule. We just moved the code into a strategy type. The behavior is identical to what was there before"Note: The internal business logic is sanitized for this article"
  
  
  Step 3: Build the Future (Strategy B)
Now here's where it gets interesting. Our roadmap required moving all products to the grouped model eventually. With the old if-else, this would be a terrifying rewrite. With strategies? We just create a second implementation:Notice what happened: zero changes to the interface, zero changes to the calling code. We just implemented the same contract with different behavior.
  
  
  Step 4: The Service (Stays Blissfully Simple)
Here's the beautiful part. Your main service code becomes trivial:This code never changes. Not when you add Strategy C. Not when you modify Strategy A. Not when you're testing Strategy B in production.Here's how the Strategy Pattern changed our deployment story:Config only, no code deployThe key insight: swapping strategies is a configuration change, not a code deployment. No recompilation, no merge conflicts, no complex rollback procedures.📉 Before Strategy Pattern:2-3 week deployment cycles due to testing complexityHours long rollbacks requiring full service redeploymentTesting in isolation was nearly impossibleEach new requirement added to everyone's cognitive load📈 After Strategy Pattern:Daily deployments via configuration changesSub-minute rollbacks (revert a config value)Each strategy tested independentlyNew strategies developed without touching existing code
  
  
  Getting Started in Your Codebase
If you're dealing with a similar situation, here's the refactoring path: - Find the if-else or switch statement that keeps growing - What questions does your code need answered? - Create Strategy A that preserves current behavior exactly - Prove Strategy A produces identical results - Implement your new behavior - Let configuration decide which strategy to useThe beauty? You can do steps 1-4 without changing any behavior. It's a safe refactor.
  
  
  When Should You Use This?
The Strategy Pattern shines when:You have multiple algorithms for the same problem (e.g., different pricing rules, recommendation engines, publishing modes)
The algorithm needs to change at runtime (via config, feature flags, A/B tests)
The algorithm is complex and high-risk (the if-else that everyone fears)
You need instant rollback capability (because 2 AM deployments happen)We transformed our most critical code path from a deployment risk into a configuration switch. The Strategy Pattern gave us the confidence to experiment, the safety to rollback instantly, and the architecture to scale.Your core business logic is too important to be trapped in an if-else statement. Set it free.Dealing with a similar "untouchable" code path? I'd love to hear your approach in the comments below. ]]></content:encoded></item><item><title>Two Paths to Safety: How Go and Rust Made Opposite Bets</title><link>https://dev.to/moseeh_52/two-paths-to-safety-how-go-and-rust-made-opposite-bets-2980</link><author>moseeh</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 31 Oct 2025 14:17:56 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[When I first started switching between Go and Rust, I noticed something odd. Both languages promised safety, performance, and concurrency—the holy trinity C++ always dangled but never quite delivered. Both were built by engineers who wanted to escape the complexity of C and C++. Yet the more I used them, the more they felt like mirror images—facing the same problem but walking in opposite directions.Go believes that complexity is the enemy. Its designers stripped the language down until only the essentials remained. Fewer features, fewer surprises, fewer excuses for unreadable code. Rust, on the other hand, embraces complexity when it can make the system safer. Ownership, lifetimes, borrowing—each concept exists to prevent entire categories of bugs before they happen.Both are trying to answer the same question: How can we write fast, reliable software without losing our sanity? But where Go chooses trust and simplicity, Rust chooses control and guarantees. The contrast isn't just technical—it's philosophical.
  
  
  1. What Do We Mean by "Safety"?
Before we go further, let's be precise about what "safety" means in this context. We're primarily talking about two things: prevents crashes from use-after-free bugs, buffer overflows, dangling pointers, and accessing invalid memory. These bugs plague C and C++ and have caused countless security vulnerabilities. prevents data races—when multiple threads access the same memory simultaneously and at least one is writing. Data races cause unpredictable behavior and are notoriously hard to debug.Neither of these prevents logic bugs or business errors. Safety here means "won't crash unexpectedly due to low-level memory or concurrency mistakes."
  
  
  2. The Go Way: Simplicity as a Form of Safety
In 2007, Google's infrastructure was held together by millions of lines of C++ and Java. The engineers were drowning in build times, dependency issues, and mental overhead. Go was born as a rebellion—a language that prioritized clarity over cleverness, and practicality over perfection.Rob Pike once said that "complexity is easy, simplicity is hard." Go's design embodies that paradox. It removes inheritance, generics (until recently), assertions, and even exceptions—not because they're useless, but because they often lead to confusion and misuse. Every decision reflects a kind of engineering humility: assume that the next person reading your code will be you, six months later.Memory management in Go is automatic, handled by a garbage collector. Concurrency is simple to express through goroutines and channels. These abstractions make it easy to start, scale, and maintain software without deep knowledge of system internals.Consider error handling. Where C++ has exceptions that can be thrown from anywhere, and Rust has Result types with extensive pattern matching, Go simply returns errors as values. If you ignore them, that's on you. The language trusts you to check them—or at least makes ignoring them a conscious choice.Go's bet is that clear, simple code makes bugs easier to spot and fix. Not impossible to write, but easier to catch in code review, easier to debug in production. The safety net is social, not mathematical.This philosophy has limits. Go can't prevent data races, nil pointer panics, or ignored errors. But for many teams, especially those building web services and cloud infrastructure, these trade-offs are acceptable. Development velocity often matters more than eliminating every possible crash.
  
  
  3. The Rust Way: Safety Through Precision
If Go is the language of trust, Rust is the language of discipline. It doesn't assume that the programmer will do the right thing—it enforces it. Every variable, every reference, every lifetime must make its relationship to memory explicit. At first, this feels like fighting the compiler. But soon, it starts to feel like a partnership.Rust's ownership and borrowing system is the foundation of its safety model. The compiler checks, at compile time, that no data is used after it's freed, that mutable and immutable references don't overlap, and that every resource has a single, clear owner. The result is software that is memory-safe without needing a garbage collector—and often faster than garbage-collected languages as a result.When Rust code compiles, entire categories of crashes simply can't happen. Use-after-free? Impossible. Data races? Prevented by the type system. Null pointer dereferences? The type system makes them explicit. Logic bugs still happen, but the low-level footguns are gone.Rust's async story—while powerful—adds its own complexity. The ecosystem is maturing, but combining async code with ownership rules creates a learning curve that even experienced developers find steep.The trade-off is real: Rust requires more upfront investment. The learning curve is measured in weeks, not days. Initial development is slower as you satisfy the compiler's demands. But the payoff comes in production, where bugs that would have been runtime crashes in other languages simply don't exist.
  
  
  4. The Shared Goal: Reliable Software
Despite their philosophical differences, Go and Rust were both born from the same frustration—software that breaks too easily and grows too messy too fast.Go targets reliability through simplicity. It reduces the mental cost of understanding a system, making it easier to reason about, maintain, and share across teams. Rust targets reliability through correctness. It encodes guarantees that the compiler enforces, catching entire classes of bugs before they exist.Go optimizes for development velocity—the time from idea to working code. Rust optimizes for correctness—the confidence that working code will keep working under stress.Go thrives in backend systems, cloud services, and large team environments. Docker, Kubernetes, and most of the cloud-native ecosystem are written in Go. When you need to ship an API server or a CLI tool quickly, Go gets out of your way.Rust shines in systems programming, performance-critical tools, and environments where crashes aren't an option. Browsers (Firefox's Servo components), operating systems (parts of Windows, Linux drivers), and game engines choose Rust when every millisecond and every byte of memory counts—and when runtime performance needs to match or exceed C++.The choice between Go and Rust isn't just philosophical—it's practical. Here's a framework:Development velocity matters more than eliminating every possible bugOnboarding needs to be fast (new developers productive in days)The problem is I/O-bound rather than CPU-boundRuntime crashes are annoying but not catastrophicYou're building services, APIs, or tooling where iteration speed is keyMemory or CPU performance is genuinely constrainedCorrectness is critical (embedded systems, cryptography, kernels)You're building a library that others will depend onCrashes could mean security vulnerabilities or physical damageYou can afford the learning curve and slower initial developmentYou don't have to choose just one. The best engineering teams use both—Go for the rapid iteration layers, Rust for the performance-critical core. They're complementary tools.
  
  
  6. The Developer Experience: Different Kinds of Pain
The real difference isn't in what they prevent, but in  they make you suffer.Go frustrates you by limiting what you can express. Want complex type hierarchies? Not available. Want to guarantee at compile time that a value isn't nil? Can't do it. The language forces you into simpler patterns, which feels restrictive until you realize how much easier it makes maintenance.Rust frustrates you by making you prove everything. The borrow checker will reject patterns that would work fine in practice because it can't verify them statically. You'll rewrite working code to satisfy the compiler. This feels pedantic until you realize you're debugging far less in production.This shapes culture. Go developers prize readability and pragmatism. Rust developers prize correctness and deep understanding. 
  
  
  7. Conclusion: Two Answers, One Question
Go and Rust are both reactions to complexity. Go removes sharp edges by simplifying the knife. Rust polishes the blade until it can't cut you.The choice isn't about right or wrong—it's about context. For most web services, APIs, and tools where iteration matters more than microseconds? Go's simplicity wins. For embedded systems, kernel modules, or anything where bugs could mean security breaches or physical damage? Rust's guarantees win.Safety and simplicity aren't opposites. They're coordinates on the same map, and every developer chooses where to stand.]]></content:encoded></item><item><title>My Journey to Becoming a Full Stack Developer</title><link>https://dev.to/ahmed112/my-journey-to-becoming-a-full-stack-developer-483c</link><author>Ahmed Alasser</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 31 Oct 2025 12:45:20 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hey, I’m Ahmed — My Journey to Becoming a Full Stack Developer
Hi everyone! I’m Ahmed Alasser, a developer in progress from Egypt, currently building my path toward becoming a Full Stack Developer.
I’m passionate about clean, scalable code and enjoy learning how both the backend and frontend work together to create great web experiences.A few months ago, I decided to take my coding journey more seriously. I’ve always been curious about how the web works — how data moves between the server and the browser, how APIs are structured, and how design turns into real functionality.These days, I’m focusing on Go for backend development and JavaScript/React for the frontend.
I chose this stack because Go offers speed and simplicity for backend systems, while React brings flexibility and power to the user interface. Together, they make full stack development both fun and efficient.I’m also learning more about REST APIs, Docker, databases, and Linux, to build complete, deployable applications. I’m not claiming to be an expert — but every project and mistake is helping me improve.My goal is simple: to write code that works and makes sense, not just quick fixes.
Through these posts, I’ll be sharing lessons, experiments, and ideas from my journey — hopefully helping others who are walking a similar path.If you’re learning Go, React, or exploring full stack development, let’s connect and share knowledge! ]]></content:encoded></item><item><title>Building a Reliable UDP Protocol in Go: Fast, Lightweight, and Rock-Solid</title><link>https://dev.to/jones_charles_ad50858dbc0/building-a-reliable-udp-protocol-in-go-fast-lightweight-and-rock-solid-3p5h</link><author>Jones Charles</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 31 Oct 2025 08:06:19 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hey there, Go devs! Ever wished you could combine the blazing speed of UDP with the reliability of TCP? Imagine sending packets faster than a speeding bullet, but with the assurance they’ll arrive in one piece. That’s exactly what we’re diving into today: building a  in Go, perfect for real-time apps like gaming, video streaming, or IoT telemetry. If you’ve got a year or two of Go under your belt and know your way around goroutines, this guide is for you. Let’s make UDP reliable without losing its superpowers!
  
  
  Why Reliable UDP? A Real-World Story
Picture this: I was working on a video streaming project where every millisecond counted. TCP was too slow with its heavy handshakes, but UDP’s packet loss caused stuttering nightmares. We needed UDP’s speed  TCP’s reliability. That’s when we rolled up our sleeves and built a reliable UDP system in Go. The result? 99.9% packet delivery with sub-100ms latency—game-changing for live streams!Build a simple, reliable UDP system with sequence numbers, ACKs, and retransmissions.Share battle-tested tips from real projects (video streaming, IoT, and gaming).Drop working Go code you can tweak for your own apps.Highlight why Go’s concurrency makes this a breeze. Developers with basic Go experience (goroutines,  package) and a curiosity about network programming. No PhD in networking required!💡 : Why Go? Go’s lightweight goroutines,  package, and  make it a dream for network tasks. In a test, Go cut packet processing latency by 30% compared to Python.Ready to make UDP reliable? Let’s dive in!
  
  
  Understanding UDP: The Speedy but Unruly Messenger
UDP (User Datagram Protocol) is like a courier who sprints but sometimes drops packages. It’s connectionless, datagram-based, and has minimal overhead, making it ideal for real-time apps. But here’s the catch: it doesn’t guarantee delivery, order, or error correction. Compare that to TCP, the cautious librarian who checks every book twice but takes forever.Here’s a quick UDP vs. TCP rundown:Handshake, higher latencyPackets may arrive out of orderTo make UDP reliable, we need to add:: Confirm packets arrived.: Track order and detect missing packets.: Resend lost packets after a timeout.: Control the sending rate.Think of it as giving our sprinter a GPS, a checklist, and a retry button—all while keeping them fast.🛠 : Go’s goroutines handle concurrent clients like a champ, and  makes UDP ops a breeze. In an IoT project, we managed thousands of devices with minimal latency using Go’s concurrency.
  
  
  Segment 2: Designing and Coding Reliable UDP

  
  
  Designing Our Reliable UDP System
Let’s architect a system that’s reliable, fast, and scalable. Our goals:: No lost packets.: Keep UDP’s speed edge.: Handle multiple clients smoothly.: Survive network hiccups.: Use goroutines for each client.: Label packets and confirm receipt.: Resend lost packets with a timeout.: Limit in-flight packets to avoid congestion.In a gaming server project, this design synced player states every 20ms for 1000 players without breaking a sweat.
  
  
  Coding It Up: A Reliable UDP Client-Server in Go
Let’s build a simple client-server system where the client sends messages, the server acknowledges them, and retransmissions handle losses. This code is minimal but extensible, with comments to guide you.: Listens on UDP port 12345, reads packets, extracts sequence numbers, sends ACKs, and skips duplicates.: Sends messages with sequence numbers, waits for ACKs, and retransmits up to 3 times on timeout.: Uses  for buffer reuse and  for clean shutdowns.Run this code, and you’ll see the client send messages, the server acknowledge them, and retransmissions kick in if packets are lost. Try it out locally to see it in action!🎉 : Simulate packet loss with tools like  (Linux) to test retransmissions. In my video streaming project, this helped us hit 95% delivery in spotty networks.
  
  
  Segment 3: Best Practices and Pitfalls

  
  
  Leveling Up: Best Practices for Production
Our code is a solid start, but production-grade systems need extra polish. Here’s what I learned from deploying reliable UDP in video streaming and IoT projects.: Unbounded goroutines can crash your app. Use golang.org/x/sync/semaphore to cap concurrent clients. In a gaming project, this cut CPU usage by 20%.: Use buffered channels to manage packet order and avoid race conditions.: Combine ACKs for multiple packets to reduce network chatter. This saved 15% bandwidth in a streaming app.: Adjust retransmission timeouts based on round-trip time (RTT). Jacobson’s algorithm (below) boosted delivery rates to 95% in weak networks.:  cuts garbage collection overhead. We saw GC time drop from 200ms to 50ms in high-throughput tests.: Use  to clean up goroutines on network errors.: Tools like  make debugging packet loss a breeze.: Use  to spot bottlenecks. It helped us fix a goroutine leak, doubling connection capacity.❓ : How do you handle high concurrency in Go? Drop your tips in the comments!
  
  
  Avoiding Pitfalls: Lessons from the Trenches
Here are common traps and how to dodge them, based on real projects::

: Fixed timeouts failed in jittery networks (e.g., 10% loss on 4G).: Use dynamic retransmission timeouts with exponential backoff. Here’s a snippet inspired by Jacobson’s algorithm:
:

: Rogue goroutines piled up memory.: Use  for cleanup:
:

: Multiple clients sharing sequence numbers caused chaos.: Use unique sequence spaces (e.g., ).🚨 : Test under bad network conditions (use  or ) and profile with  to catch leaks early.
  
  
  Segment 4: Real-World Uses and Wrap-Up

  
  
  Where Reliable UDP Shines
Reliable UDP is a superhero for low-latency, high-throughput apps. Here’s how it powers real-world systems::

: Sub-100ms latency, high throughput.: Reliable UDP with Forward Error Correction (FEC) for critical packets. Go’s  and goroutines cut latency from 150ms to 80ms in my project.:

: Lightweight protocol for weak networks.: Small packets with retransmissions. Go’s compact binaries ran on 5000 sensors for 6 months straight.:

: <50ms response for 1000+ players.: Sliding windows and batch ACKs. Go’s concurrency handled it flawlessly.🌟 : In a shooter game, reliable UDP synced player positions every 20ms, keeping gameplay buttery smooth.
  
  
  Wrapping Up: Your Next Steps
Building a reliable UDP protocol in Go is like giving a racecar a safety harness—speed and security in one package. We covered:: Sequence numbers, ACKs, retransmissions, and sliding windows.: Goroutines, , and  make it simple.: Dynamic timeouts, buffer pools, and robust logging.: From video streaming to IoT, reliable UDP delivers.Try the code above and tweak it for your project.Explore  for next-level UDP-based protocols.Test with tools like  to simulate real-world networks.Profile with  to keep performance tight.🙌 : Have you built a reliable UDP system? Share your story or ask questions in the comments! If you try this code, let me know how it goes.
  
  
  Resources for More Learning
]]></content:encoded></item><item><title>The CPU Cost of Signing NXDOMAINs</title><link>https://dev.to/thevilledev/the-cpu-cost-of-signing-nxdomains-bnm</link><author>Ville Vesilehto</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 30 Oct 2025 19:12:11 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In this post we see that with the same NXDOMAIN-signing workload, RSA (RSASHA256/3072) used 30x the amount of CPU compute compared to ECDSA (P‑256).When you enable DNSSEC in CoreDNS, negative answers must be provably negative. CoreDNS implements authenticated denial with NSEC "black lies". It forges per-query NSEC owner names to prevent zone walking, which means responses can’t be broadly reused by resolvers and each miss requires fresh signing. That pushes cryptographic signing onto the hot path, so the key algorithm directly determines CPU and packet size.Dry information for those who are initiated:Note that ECDSA P-256 RRSIGs are 64 bytes, while RSASHA256-3072 RRSIGs are 256 bytes (size equals modulus). ECDSA signatures also save you bandwidth.I ran two otherwise identical configs that differed only in the DNSSEC key algorithm: RSA (RSASHA256, 3072; algorithm 8) and ECDSA (P‑256; algorithm 13). As mentioned in RFC 6605:"Current estimates are that ECDSA with curve P-256 has an approximate equivalent strength to RSA with 3072-bit keys"To force signing work, I generated many unique, random non‑existent names, set the DO bit and EDNS to trigger signing. I measured throughput and latency with , and captured CPU profiles with  to summarize and visualize.Supporting tools included ,  or  key tools, and optionally  for SVG outputs.$ORIGIN example.test.
@ 3600 IN SOA ns1.example.test. hostmaster.example.test. (1 7200 3600 1209600 3600)
  3600 IN NS ns1.example.test.
ns1 3600 IN A 127.0.0.1
Generate one key for each algorithm: keys/ecdsa
keys/ecdsa  dnssec-keygen  ECDSAP256SHA256  ZONE example.test  keys/rsa
keys/rsa  dnssec-keygen  RSASHA256  3072  ZONE example.test Note the basenames printed (e.g.,  or ). Use the exact basename in the Corefiles below..:1053 {
    pprof 127.0.0.1:6060
    file db.example.test example.test
    dnssec example.test {
        key file keys/ecdsa/Kexample.test.+013+15419
    }
    bufsize 1232
}
.:1053 {
    pprof 127.0.0.1:6060
    file db.example.test example.test
    dnssec example.test {
        key file keys/rsa/Kexample.test.+008+09030
    }
    bufsize 1232
}
Buffer size is set to 1232 to prevent IP fragmentation with EDNS0. See bufsize plugin docs for more information if curious. and  are omitted due to profiling. They add syscall noise.Create a large set of unique NXDOMAIN queries:jot  200000 1  queries.txt
Run CoreDNS for the chosen Corefile:./coredns  Corefile.ecdsa

./coredns  Corefile.rsa
Drive the load (DO bit + EDNS, match bufsize):dnsperf  127.0.0.1  1053  queries.txt  60  2000  50  1232
Capture a 30‑second CPU profile while the load is running:go tool pprof  http://127.0.0.1:6060/debug/pprof/profile?seconds30  rsa.cpu.pb.gz

go tool pprof  http://127.0.0.1:6060/debug/pprof/profile?seconds30  ecdsa.cpu.pb.gz
Generate clean visuals from call graphs in SVG format:go tool pprof  rsa.cpu.pb.gz  rsa.user.svg

go tool pprof  ecdsa.cpu.pb.gz  ecdsa.user.svg
go tool pprof ./coredns rsa.cpu.pb.gz
^syscall|runtime
top
top 
list sign
From pprof we can see that RSA used about 176 seconds of samples over 30 seconds:(pprof) top
Showing nodes accounting for 168.37s, 95.49% of 176.32s total
ECDSA used about 5.6 seconds of samples over 30 seconds:(pprof) top
Showing nodes accounting for 5.33s, 95.35% of 5.59s total
For this workload, RSA required approximately 30× more CPU than ECDSA.
  
  
  Detailed pprof output from RSA

  
  
  Detailed pprof output from ECDSA
Prefer ECDSA (P‑256) keys for online DNSSEC signing in CoreDNS, unless you have a compelling reason not to. Monitor coredns_dnssec_cache_hits_total and coredns_dnssec_cache_misses_total metrics series, and naturally overall CPU saturation from the process or container runtime.If you end up debugging this further, use the pprof plugin in CoreDNS.Comments and feedback welcome - and thanks for reading! ]]></content:encoded></item><item><title>For anyone new to testing in Go. This article will take give you a solid foundation on your testing journey #golang #tdd</title><link>https://dev.to/tobilobaogundiyan/for-anyone-new-to-testing-in-go-this-article-will-take-give-you-a-solid-foundation-on-your-testing-16h8</link><author>Tobiloba Ogundiyan</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 29 Oct 2025 22:14:45 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Testing Real-World Go Code: Table-Driven Tests, Subtests and CoverageTobiloba Ogundiyan ・ May 1]]></content:encoded></item><item><title>I finally built the Go project generator I always wanted: Goca (Beta)</title><link>https://dev.to/sazardev/i-finally-built-the-go-project-generator-i-always-wanted-goca-beta-5ec8</link><author>Sazardev</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 29 Oct 2025 14:24:14 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hey Devs!, I wanted to share a side project I’ve been pouring time into lately: Goca.If you’re like me, you’re tired of writing the same 20 files just to start a new Go service following Clean Architecture. The layers, the interfaces, the DI config—it’s necessary, but it’s pure boilerplate.That’s where Goca comes in. It’s a CLI tool that generates a full, production-ready Go feature (entity, use case, repository, HTTP handler) in seconds, all while strictly enforcing Uncle Bob’s Clean Architecture rules. It’s essentially my way of cutting out the setup tax.Heads up: This is still very much in beta. It’s working great for my own projects, but there are definitely rough edges, and I’m still figuring out the best ways to handle database migrations and multi-protocol setups.It’s open source (MIT licensed), and I’m genuinely looking for feedback from people who actually build production Go apps. If you hate boilerplate as much as I do, please check out the documentation and let me know where it breaks for you.]]></content:encoded></item><item><title>Beyond YAML: Building Kubernetes Operators with CRDs and the Reconciliation Loop</title><link>https://dev.to/naveens16/beyond-yaml-building-kubernetes-operators-with-crds-and-the-reconciliation-loop-524d</link><author>Kubernetes with Naveen</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 29 Oct 2025 09:15:28 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In this post you’ll learn what Operators and Custom Resource Definitions (CRDs) are, how they work together, their pros and pitfalls, how to scaffold one using tools like Kubebuilder, and how to write your own operator in Go — diving into the reconciliation loop and controller mechanics in practice.Introduction (Let's set the stage up)When you think of Kubernetes, you probably think of Deployments, Services, StatefulSets, etc. But what if you want Kubernetes “to understand” higher-level concepts in your domain (e.g. “a Database cluster”, “a Cache cluster”, “a workflow job”) — and automate not just deployment, but upgrades, backups, self-healing, etc.? That’s where Operators and CRDs come in.In this article, we’ll start from first principles — what Operators and CRDs are, how they play together — and then go step by step through the process of scaffolding, writing, and understanding the key “reconciliation loop” logic in Go. I’ll also share practical tips, design pitfalls, and trade-offs from real projects. Let’s go.1. What are Operators and CRDs, and how do they interact?Custom Resource Definitions (CRDs) — extending the Kubernetes APIAt its core, a Custom Resource Definition (CRD) is a way to extend the Kubernetes API with your own new kinds (types).Kubernetes comes with built-in resource kinds: Pod, Deployment, Service, etc. Each of these has a spec (desired state) and status (observed state) and is served by the Kubernetes API server.A CRD allows you to add a new kind — for example, MyApp, DatabaseCluster, Cache, MySQLBackup, etc. You define the schema (often via OpenAPI v3 validation), the group/version/kind, and Kubernetes will then allow clients to kubectl apply objects of that new kind (Custom Resources, CRs).Once your CRD is installed, your cluster effectively “knows about” this new API surface.So CRD = schema + API registration (i.e. telling Kubernetes: “I have this new type, validate it, store it, serve it”).But a CRD by itself only gives you a data model — it does nothing automatically. You still need logic to act when CRs are created, updated, or deleted.Operators — controllers with domain logicAn Operator is the piece that makes your CRD useful. It is (in practice) a Kubernetes controller (a client of the Kubernetes API) that:Watches events on your custom resources (CRs),Compares the desired state (in spec) with the current state of the world,And takes actions (create/update/delete Kubernetes primitives or external resources) so as to converge the system toward the desired state.Thus, an Operator combines two parts:CRD — defines the “language” (what attributes the user can express).Controller / Reconciler logic — the “brain” that watches for changes and enforces them.The Operator pattern is essentially: “Let me treat my application (or cluster component) as a first-class Kubernetes object; the operator will drive its lifecycle.” When you write an operator, you typically own the CRD (i.e. your operator is the canonical manager for that CRD). You register the CRD, and then inside the operator you write logic to reconcile every instance of the CRD.In operation, things go as follows:A user (or system) kubectl applys a CR of kind Foo (that your CRD defines).The Kubernetes API server stores that CR object (desired state).Your operator’s controller sees that new CR (via watch/informer) and triggers a reconcile.In Reconcile(), your code reads the CR, checks/subscribes to or reads existing resources (e.g. Deployments, Services, ConfigMaps), and if things are missing or wrong, issues requests to the API (create/update/delete) to align them.Over time, through repeated reconciliation, the “actual” cluster state is made to match what the CR requests (ideally).Optionally, the operator updates the CR’s status subfield to reflect progress, health, or conditions.One way to think: Kubernetes built-in controllers reconcile built-in kinds (e.g. Deployment reconciles Pods). Your operator reconciles CR kinds into a set of built-in or other CRs that in turn get reconciled.Hence, CRD + Operator = your extension to Kubernetes behavior — you teach Kubernetes to “understand” your domain.2. Why use this pattern? Benefits and challengesUsing CRDs + Operators yields several compelling benefits:Declarative, consistent API
Users express what they want (via CRD spec) and the operator handles how to realize it. That hides complexity and reduces human error.Day-2 operations automation
Beyond initial deploy (Day 1), operators allow you to automate upgrades, backups, schema migrations, health checks, scaling, rolling restarts, etc.You codify your “operational knowledge” and embed it. Self-healing and drift correction
If someone manually fiddles with resources (e.g. deletes a Pod, modifies a configmap), the operator’s reconciliation loop can detect drift and restore the correct state. Domain-aware orchestration
The operator can understand ordering, dependencies, constraints (e.g. start DB, wait, then migrate), and enforce complex workflows, something flat YAML can’t do reliably.Simplified user experience
For many users, deploying your app becomes kubectl apply -f myapp.yaml. Under the hood, the operator installs all the needed services, handles upgrades, etc. They don’t need to know all the Kubernetes primitives. Extensibility and composability
You can build operators that interact (watching CRs of other operators), build meta-operators, or chain behavior modularly (though this comes with trade-offs). Challenges, pitfalls, and caveatsWith power comes responsibility. Here are key challenges and trade-offs:Correctness & idempotency
The reconciliation logic must be idempotent — running multiple times should not break things or cause oscillations. Mistakes here lead to thrashing, resource conflicts, or stuck states. 
As your domain logic grows (multiple subcomponents, version upgrades, backward compatibility), the operator code can become complex. Structuring it carefully is vital.Testing and observability burden
You need solid tests (unit, integration) for reconcile logic, error paths, race conditions. Also, need metrics, logs, tracing, health checks, leader election, etc., to operate in a production cluster.Upgrade path and API versioning
As your CRD evolves, you’ll need to support version migrations (v1alpha → v1beta → v1), conversion, deprecation. Mistakes here can break existing installations.Handling external systems/side effects
If your operator talks to external databases, cloud APIs, or non-Kubernetes systems, you must manage eventual consistency, network failures, retries, backoff. Reconcile loop can’t block indefinitely.Race conditions, concurrency, and resource ownership
You must ensure controllers don’t step on each other’s toes. For example, two operators managing the same CR kind is discouraged. Handling concurrent reconcile loops safely, avoiding duplicate work, and reconciling in correct order adds complexity. Operator’s resource consumption & scale
If there are many CR instances or many events, the operator must scale (e.g. concurrency, rate limiting). Also be careful to avoid large list operations in every reconcile.Drift vs manual override tension
Sometimes users want to override something (tweak a configmap child directly). Operator may override that on next reconcile. You may need “ignore diff” or “do not manage this field” features. Garbage collection/eletion semantics
When a CR is deleted, your operator should clean up owned resources in the right order (especially if there are dependencies). Use ownerReferences and finalizers carefully.3## . Scaffolding CRDs/Operators easily: Kubebuilder and friendsYou don’t have to start from scratch. Tools like Kubebuilder, Operator SDK, or controller-runtime scaffolding greatly reduce boilerplate and help you follow best practices.Here’s a walkthrough of how you’d use Kubebuilder to scaffold your operator + CRD.Getting started with Kubebuilder(These are high-level steps; for full detail see the Kubebuilder Book) 
Download appropriate binaries and put in your PATH.kubebuilder init  your.domain  github.com/you/your-operator
This sets up the project scaffolding: main.go, API directory, controller directory, etc.Create API + Controller scaffoldkubebuilder create api  <group>  <version>  <KindName>
api/vX/KindName_types.go (where you define Spec and Status)api/vX/KindName_webhook.go (if validation/defaulting is enabled)controllers/KindName_controller.go with a stub Reconcile() and SetupWithManager()Sample manifest YAMLs under config/samples/CRD YAML generation logic under config/crdEdit Spec/Status & markers
In *_types.go, you annotate fields with markers (// +kubebuilder:validation: etc.) for CRD schema validation, default values, optional fields, etc.Implement Reconcile logic
In the controller stub, replace the generated TODO code with your actual logic.
In SetupWithManager(), you wire which resources your controller watches (the primary resource and any secondary ones). E.g.:This ensures your reconcile loop is triggered when CR changes or when owned resources change.Generate CRD manifests/controllers
Use make manifests or make install depending on your scaffold to generate CRD YAMLs (which include your validation markers).
You build the operator binary (often containerize it), install the CRD in a cluster, deploy the operator, then apply sample CR YAMLs (from config/samples) and see the behavior.Kubebuilder (and controller-runtime) handles much of the plumbing: caching, informers, client libraries, leader election, default reconcile loop wiring, etc.Pros of using Kubebuilder:You start with solid boilerplate following best practices.You get validation/defaulting support, CRD schema generation, versioning support.It standardizes how your operator is structured, which helps maintainability.The scaffold may not exactly match your domain logic — you’ll adapt.For highly custom behavior (multi-CR operators, cross-CR relationships), you’ll need to extend the scaffold.Learning the marker syntax, imports, API versioning, etc., has a learning curve.Once your operator grows, you may want to break large reconcile logic into well-modular domain services, state machines, or sub-reconcilers.4. Writing your own operator in Go — the Reconciliation Loop in actionLet’s walk through a simplified example operator in Go, focusing on the reconcile loop mechanics. I’ll highlight key patterns and pitfalls.The skeleton: controller and reconcile stubAfter scaffolding, you’ll have something like:Let’s break it down and dive into nuances.Step-by-step logic and patterns99(a) Fetch the custom resource**This is your starting point. If the CR is not found (deleted), often you simply exit (the ownerReferences + finalizers may handle cleanup).But note: your reconcile should handle stale events — e.g. events where the CR was deleted before your code saw it. So check IsNotFound carefully.(b) Observe existing “child” or managed resourcesYou might next issue a Get or List to find related resources (Deployments, StatefulSets, Services, Secrets) that you manage and should reflect the CR’s desired spec.If found, you compare fields (replica count, container image, env vars, etc.) with what your my.Spec asks for. If differences, you update. Use r.Update.When creating child resources, use controllerutil.SetControllerReference(&my, child, r.Scheme) so that Kubernetes understands the CR “owns” that child. That enables garbage collection: when the CR is deleted, its owned children go away, too.This also enables watch events (when child changes) to trigger your reconcile function. Your code should consider “if exists and is correct, do nothing.” Don’t blindly issue updates unless needed. This avoids infinite loops, API flapping, etc.Also, your code should gracefully handle partial failures (e.g. child creation succeeded, but status update fails). Ensure no inconsistent state or repeated destructive loops.(e) Status subresource updatesOften you want to update my.Status to reflect progress, conditions, readiness, errors, etc. For example:Use r.Status() so it updates only status, not spec. Be cautious about infinite loops: status update is itself an update event, triggering another reconcile.Your Reconcile returns two values: ctrl.Result and error. The combination dictates what happens next:return ctrl.Result{}, nil → done, no immediate requeuereturn ctrl.Result{Requeue: true}, nil → immediately requeuereturn ctrl.Result{RequeueAfter: time.Duration}, nil → requeue after the given delayreturn ctrl.Result{}, err → error, so the runtime may retry with backoffYou use requeue when you know further work is needed after a delay (e.g. waiting for a child to settle). The scaffolding often sets a “syncPeriod” default (e.g. 10 hours) so even in absence of events, reconciles run periodically. 
Stack OverflowAlso, your code should not block indefinitely — reconcilers must return rather than wait on long blocking operations.(g) Concurrent reconciles & safetyController-runtime supports concurrent reconciliation of different objects (via MaxConcurrentReconciles) allowing your operator to scale. However, never attempt to reconcile the same object concurrently — runtime ensures that your reconciler gets serialized per object key. But you should be careful about cross-object state (e.g. two CRs manipulating the same shared resource).(h) Watch other resources, not just primary CROften you’ll want to watch secondary or external resources (e.g. ConfigMaps, Secrets, other CRs). You map events on them to reconcile your CRs (via .Owns(...), .Watches(...) in SetupWithManager).Thus, if a Secret changes, you can trigger reconciliation of relevant CR(s).Example: Memcached operator (minimal)Kubebuilder’s example is Memcached: user supplies size: N in the CR, the operator ensures a Deployment with N replicas of memcached is running. This simple example illustrates the core pattern. You can expand it to include scaling, backups, upgrades, etc.The reconciliation loop in practiceThe reconciliation loop is the heart of your operator. It is:Event-driven (via watches)Stateful-agnostic (reconcile must handle all states)Idempotent (safe to run multiple times)Non-blocking (each call should complete quickly)Triggers further reconciles by requeue or watching owned resourcesAs Kubernetes operators are merely controllers in user space, they plug into the control plane’s reconciliation machinery. When the controller-runtime manager runs, it registers your controller, and each time an event (create/update/delete) happens on watched resources, the manager enqueues a reconcile Request, which is processed by calling your Reconcile() function.In effect, the operator’s reconcilers extend Kubernetes’ control loop to your custom domain.Best Practices & Tips (parting advice)Keep your reconcile logic modular: break it into sub-reconcilers or small functions (e.g. “ensureDeployment”, “ensureConfig”, “updateStatus”).Use conditions in status (Ready, Progressing, Degraded) rather than encoding booleans or strings; it makes status easier to interpret and extend.Guard expensive list or watch operations — use indexers or field selectors to limit scope.Use leader election if you run multiple replicas of your operator (to avoid double work).Monitor metrics (reconcile durations, queue length, errors).Be careful with schema evolution: provide CRD conversion webhooks or adopting strategies when migrating APIs.Use finalizers to clean up external dependencies (e.g. delete cloud resources) before object is fully removed.Gracefully handle partial failures: circuit-breakers, retries, backoff.Document your CRD’s fields, constraints, examples (use config/samples).Seat your operator in a namespace (or cluster-wide) thoughtfully — restrict RBAC.Don’t let your operator manage more than one CR kind (if too many concerns, split into multiple controllers) Operators + CRDs represent a powerful pattern for making Kubernetes aware of your domain logic and automating much of the operational burden. You define new APIs (CRDs), and the operator (controller) drives the system toward the desired state — doing what a human operator would, but continuously, reliably, at cluster scale.Yes, there’s complexity, and writing a robust operator takes care, testing, observability, and design discipline. But once you cross the learning curve, operators become your go-to tool to manage data stores, middleware, clusters, workflows, and many other system components in a Kubernetes-native way.If you like, I can prepare a full working code example (for a simple operator), with tests and deployment, and even diagrams. Would you like me to do that next?]]></content:encoded></item><item><title>🚀 Go Faster: Cutting the Slack in GC with Smart Memory Allocation</title><link>https://dev.to/aris_georgatos/go-faster-cutting-the-slack-in-gc-with-smart-memory-allocation-304h</link><author>Aris Georgatos</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 29 Oct 2025 08:20:36 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[My last few posts dove deep into the weeds of concurrency (race conditions) and system scalability. Now, let's talk about the engine under the hood: . is fantastic, concurrent, non-generational, and designed for low latency. But even the best GC uses CPU cycles and causes brief  pauses, which can be significant in latency sensitive, high throughput applications.The best GC is the one that has nothing to do. We can dramatically reduce GC overhead by minimizing the rate at which we create temporary objects on the heap.On this post we will go throughHow Stack vs Heap allocation impacts performanceEscape analysis and how to keep data on the stackPractical strategies: , pre-allocation, and avoiding goroutine leaksWhen and how to tune GC settings in production environments
  
  
  1. The Foundation: Stack and Heap Explained 🧠
Before we dive into optimization, let's establish where your data lives in memory.
  
  
  The Stack (Fast and Predictable)
The Stack is a small, highly organized region of memory that operates on a Last-In, First-Out (LIFO) principle like a stack of plates.It stores data with a : local variables, function arguments, and return values. When a function is called, its data is pushed onto the stack. When it returns, that data is instantly popped off and freed. The Garbage Collector never touches the stack. This makes stack allocation incredibly cheap and fast.
  
  
  The Heap (Dynamic and Garbage Collected)
The Heap is a large, unstructured pool of memory shared across all Goroutines.It stores data whose lifetime or size can't be determined at compile time: slices, maps, channels, large structs, or any variable that "escapes" its function scope (more on this in a moment). The Garbage Collector must periodically scan the heap, mark live objects, and sweep away garbage. Every heap allocation adds work for the GC.LIFO, fast, fixed size. Holds local variables and function data.Automatically freed on function return.Dynamic, grows indefinitely, globally accessible.Garbage Collector must mark, scan, and sweep.Our goal: Keep as much data on the stack as possible.
  
  
  2. The Allocation Battle: Stack vs. Heap 🧠
The root of GC pressure lies in where our data lives.Our primary goal is to convince the Go compiler to keep variables on the stack through a process called .The Compiler's Escape AnalysisEscape analysis is an optimization performed by the Go compiler. It determines whether a variable created inside a function must "escape" to the heap (meaning its lifetime is unknown or extends beyond the function's return) or can safely remain on the stack.You can check if a variable escapes using the compiler flag:go build  your_package/main.go
:
If you return the address of a local variable , that variable must escape to the heap so it remains valid outside the function.Assigning to an Interface:
Storing a concrete type in an interface variable often forces an allocation, as the compiler can't predict the interface's dynamic behavior (interface boxing).:
While the exact thresholds vary, very large slices (e.g., >64KB) or arrays (e.g., >10MB) are typically moved to the heap, even if they're local.: Favor  and avoid unnecessary pointers for small structs. Pass small structs by value to keep them stack allocated where possible.
  
  
  2. Practical Strategies to Reduce Allocations 🛠️
Once you've tuned your code to keep variables on the stack, the next step is to reduce the churn of objects that must be heap-allocated.Strategy A: Object Pooling with The  package is your best friend for reusable, temporary objects that are expensive to create but short-lived. This is perfect for objects like large buffers or structs used within an I/O loop (e.g., network handlers, log messages).Instead of letting the GC constantly clean up temporary buffers, we reuse them:By reusing a buffer from the pool, you bypass the entire allocation process and, crucially, avoid generating garbage for the GC.When you have high frequency, temporary objects (request handlers, temporary buffers)
Objects that are expensive to allocate (large structs, byte slices)
NOT for long lived objects or connection pools (use dedicated pooling for those).Strategy B: Pre-allocate Maps and SlicesSlices and maps are dynamic, which means they often require reallocation and copying when they grow beyond their current capacity. This constant resizing creates GC work.If you know the expected size, pre-allocate using  with a capacity hint:For maps, this practice minimizes hash collisions and subsequent internal restructuring:Without pre allocation, a slice that grows from  elements will trigger approximately  (Go roughly doubles capacity each time). Allocating new, larger backing arrayCopying all existing elementsMarking old array as garbagePre allocation eliminates all of this.Strategy C: Minimize Goroutine LeaksWhile not strictly a "memory allocation" issue, leaked goroutines are a major source of memory leaks in Go. A goroutine that's blocked forever retains the memory of its entire stack, preventing the GC from reclaiming it.Always manage the lifecycle of concurrent operations, especially with I/O or background workers, typically using the  package:Goroutines waiting on channels that never receive data
HTTP requests without timeouts
Background workers without shutdown signals Use  and track the count over time. If it grows unbound, you have a leak.
  
  
  3. Controlling the GC (Container Tuning) 🐳
For the majority of applications, you should leave Go's GC settings alone. However, in high-performance or resource-constrained environments (like containers), manual tuning becomes essential.
  
  
  The Old Way: Relative Tuning with GOGC
The  environment variable controls how much the heap must grow relative to the live heap before GC is triggered (default is 100%). In high-memory applications, if you have a 4GB live heap on a 6GB machine, the default  means the GC won't trigger until the heap reaches 8GB (4GB × 2). This immediately exceeds your physical limit, leading to an OOM kill by the kernel.You were forced to use low  values (like ) to stay safe, which caused the GC to run too frequently and waste CPU cycles.
  
  
  The Game Changer: Absolute Limits with  (Go 1.19+)
The  environment variable sets a  for the entire process (heap + non-heap memory). This is the modern solution for containerized and memory-intensive applications.By setting this limit (e.g., ), you tell the Go runtime to: The GC automatically adjusts its aggressiveness. When the live heap is small, GC runs rarely (conserving CPU). As total memory usage approaches the limit, the GC becomes highly aggressive (sacrificing CPU for safety). It gives the GC a target to stay under, ensuring the memory scheduler is driven by an absolute limit, not just relative heap growth. This allows you to utilize available memory more efficiently without constantly fearing the kernel OOM killer.Understanding the Trade-offsLower memory limit (or lower GOGC) = More frequent GC = Higher CPU usage, lower memory footprint
Higher memory limit (or higher GOGC) = Less frequent GC = Lower CPU usage, higher memory footprintIn Kubernetes, the OOM killer operates at the container level. If your pod has a memory limit of 4GB but you don't set , the Go runtime has no visibility into this constraint. The GC will happily let the heap grow until the kernel kills your process. Always set  to ~90% of your container's memory limit. This leaves a safe buffer for OS and non-heap Go runtime allocations.3500MiB ./your-service
 If your application is CPU-bound and produces minimal garbage, combine  with . This maximizes CPU usage for application logic, forcing GC to run only when the absolute memory limit is approached.
  
  
  Final Thoughts: Measure, Don't Guess 📊
Before you apply any of these optimizations, you must profile your application.
Go's built-in  tool via  is indispensable. It lets you generate profiles for CPU usage and, most importantly for this topic, heap allocation. Use it to pinpoint the exact lines of code responsible for the highest allocation rate.http://localhost:6060/debug/pprof/heap - Memory allocationshttp://localhost:6060/debug/pprof/goroutine - Active goroutineshttp://localhost:6060/debug/pprof/profile?seconds=30 - CPU profileIdentify the top allocation site.Optimize (e.g., use , pre-allocate, or simplify data structures).Verify (re-measure to confirm GC pressure is reduced).Alloc/s (allocations per second) - Lower is betterGC Pause Time - Should be  for most applicationsHeap Size - Should stabilize, not grow unboundGC Frequency - Fewer cycles = less overheadBy eliminating unnecessary heap allocations, you'll see faster execution, fewer GC pauses, and a more robust high-load Go application.Memory management in Go isn't black magic, it's a systematic process of understanding where your data lives and making conscious decisions about allocation patterns.Stack allocations are free, heap allocations cost CPU cyclesProfile before optimizing  is your best friendIn production containers, always set  to avoid OOM killsUse  for high-frequency temporary objectsPre-allocate when you know the sizeStart small: profile your hottest code paths, identify the top allocators, and apply these techniques incrementally.You don't need to optimize everything, focus on what matters.]]></content:encoded></item><item><title>Go UDP Programming: A Beginner-Friendly Guide to Building Fast, Real-Time Apps</title><link>https://dev.to/jones_charles_ad50858dbc0/go-udp-programming-a-beginner-friendly-guide-to-building-fast-real-time-apps-4ik</link><author>Jones Charles</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 29 Oct 2025 00:51:46 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Picture sending a quick text to a friend—no delivery receipt, no guarantee it arrives, but it’s lightning-fast. That’s the vibe of UDP (User Datagram Protocol). Unlike TCP, which acts like a meticulous delivery service ensuring every package arrives in order, UDP is all about speed. It’s perfect for real-time apps like video calls, DNS lookups, or log streaming, where a tiny bit of lost data won’t ruin the show. And when paired with , with its slick  package and concurrency superpowers (goroutines!), UDP programming becomes a breeze.This guide is for developers with a bit of Go experience (say, 1-2 years) who want to dive into UDP for fast, scalable apps. We’ll walk through the basics, build a real-world example, explore use cases like logging and streaming, and share pro tips to avoid common pitfalls. By the end, you’ll be ready to whip up UDP-powered apps with confidence. Let’s get started!
  
  
  UDP vs. TCP: The Quick Lowdown
Think of UDP as a skateboard—zippy, lightweight, but you might miss a turn. TCP is a cargo truck—reliable, but slower. Here’s a quick comparison:Connectionless (no setup)Connection-oriented (handshake)Ensures delivery and orderWeb, email, file transfersFigure 1: UDP vs. TCP at a glance.UDP is the rebel of networking protocols: it’s  (no chit-chat before sending data),  (no promise your data arrives), and  (no baggage like TCP’s retries). This makes it ideal for apps where speed trumps perfection, like live video or system logs. In Go, the  package makes UDP programming dead simple with tools like  and . Plus, Go’s goroutines let you handle tons of clients without breaking a sweat.Here’s the game plan for UDP in Go:  : Server listens on a port; client connects to it.
: Swap data packets using  and .
: Close the connection to free resources.
[Client] --> Resolve UDPAddr --> DialUDP --> Send/Receive
[Server] --> Resolve UDPAddr --> ListenUDP --> Send/Receive
Figure 2: UDP workflow in Go.Ready to code? Let’s build a simple UDP echo server and client to see this in action.
  
  
  Segment 2: Core Implementation

  
  
  Building a UDP Echo Server and Client
Let’s roll up our sleeves and code a basic UDP echo server and client. The server will echo back whatever the client sends, like a digital parrot. This is a great way to grasp UDP’s core mechanics.: Listens on , reads messages, and echoes them back using .
: Sends a message and waits for the echo with a 5-second timeout to avoid hanging.
: Always use  to clean up connections properly.This setup is perfect for testing UDP basics. In a real project, I used this to prototype a log collector—super fast, but we’ll need to handle concurrency and errors for production.
  
  
  Segment 3: Concurrency and Error Handling

  
  
  Handling Concurrency Like a Pro
UDP’s connectionless nature makes it a concurrency champ. Go’s goroutines make it easy to handle multiple clients at once. Here’s an upgraded server that spins up a goroutine for each message.Each message gets its own goroutine, so the server stays responsive.
 ensures clean goroutine cleanup.
: I used this approach in a log system handling thousands of packets per second—way faster than a single-threaded setup.UDP’s speed comes with trade-offs: packets can get lost or arrive out of order. Here’s how to handle it:: Use  to avoid hanging (e.g., 2-5 seconds).
: Add 1-2 retries for critical messages, but don’t overdo it—UDP’s not TCP!
: Use sequence numbers to sort packets on the receiving end.: In a DNS project, forgetting timeouts caused hangs during network blips. A 2-second  saved the day.Sequence numbers, light retriesTune timeout for your appSmall overhead, big payoffFigure 3: UDP error-handling cheatsheet.
  
  
  Segment 4: Real-World Use Cases
Now that we’ve got the basics, let’s see UDP in action. Here are three killer use cases with code snippets and tips.
  
  
  1. Real-Time Log Transmission
 Logs need to be sent fast, and a little packet loss is okay for non-critical logs.: A client sends logs to a server for real-time monitoring.: UDP’s low latency makes it great for logs, but add retries for critical messages. In a monitoring app, this cut latency by 30% compared to TCP. DNS needs millisecond-fast responses, and UDP delivers.: Query a domain’s IP address using Google’s DNS server.: UDP’s speed makes DNS queries blazing fast—sub-50ms in my tests. Use timeouts to avoid stalls. High throughput for audio/video, with room for custom error handling.: Stream data in chunks.: Add sequence numbers for production streaming to handle out-of-order packets.Use retries for critical logsHigh throughput, flexibleFigure 4: UDP use case highlights.
  
  
  Segment 5: Best Practices, Optimization, and Conclusion

  
  
  Best Practices and Pitfalls
UDP is fast but tricky. Here are some battle-tested tips and pitfalls to watch out for.: Set 2-5 second timeouts with  or .
: Start with a 1024-byte buffer, adjust for larger packets.
: Use goroutine pools to handle high traffic. In a log system, this cut memory use by 20%.
: Track packet loss and latency with tools like Prometheus.: Use sequence numbers and 1-2 retries. Sliding windows work wonders.
: Test UDP ports; firewalls love to block them. Use standard ports if possible.
: Check MTU and fragment large packets to avoid truncation.For device discovery, UDP multicast is handy. Here’s a quick server:: Ensure your network supports IGMP for multicast to work smoothly.Adjust based on app needsCheck MTU for large packetsSequence numbers, light retriesAvoid overloading the networkCoordinate with network adminsFigure 5: UDP best practices.To make UDP fly, try these:  : Group read/write calls to cut system overhead by up to 40%.
: Reuse  for microsecond-fast connections.
: Use  to shrink packets by ~30%, but watch CPU usage.: Use  to measure throughput.
: Tools like  or  test packet loss resilience.
: Log packet loss and latency for insights.: A simple throughput test:: Pair with  for real-world performance checks.
  
  
  Conclusion and What’s Next
UDP in Go is like a sports car—fast, fun, but needs careful handling. You’ve learned how to build a UDP echo server, handle concurrency, tackle errors, and apply UDP to logs, DNS, and streaming. With Go’s  package and goroutines, you’re ready to build high-performance apps.Experiment with the echo server code above.
Try goroutine pools for high-traffic apps.
Monitor performance with Prometheus or Grafana.
Peek at  (UDP-based HTTP/3) with the  library for next-gen networking.UDP’s future is bright with QUIC and real-time apps on the rise. So, fire up your editor, play with these snippets, and share your UDP adventures in the comments—I’d love to hear about them!]]></content:encoded></item><item><title>The Green Tea Garbage Collector</title><link>https://go.dev/blog/greenteagc</link><author>Michael Knyszek and Austin Clements</author><category>dev</category><category>official</category><category>go</category><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Golang Blog</source><content:encoded><![CDATA[Go 1.25 includes a new experimental garbage collector called Green Tea,
available by setting  at build time.
Many workloads spend around 10% less time in the garbage collector, but some
workloads see a reduction of up to 40%!It’s production-ready and already in use at Google, so we encourage you to
try it out.
We know some workloads don’t benefit as much, or even at all, so your feedback
is crucial to helping us move forward.
Based on the data we have now, we plan to make it the default in Go 1.26.What follows is a blog post based on Michael Knyszek’s GopherCon 2025 talk.
We’ll update this blog post with a link to the talk once it’s available online.Tracing garbage collectionBefore we discuss Green Tea let’s get us all on the same page about garbage
collection.The purpose of garbage collection is to automatically reclaim and reuse memory
no longer used by the program.To this end, the Go garbage collector concerns itself with  and
.In the context of the Go runtime,  are Go values whose underlying
memory is allocated from the heap.
Heap objects are created when the Go compiler can’t figure out how else to allocate
memory for a value.
For example, the following code snippet allocates a single heap object: the backing
store for a slice of pointers.var x = make([]*int, 10) // global
The Go compiler can’t allocate the slice backing store anywhere except the heap,
since it’s very hard, and maybe even impossible, for it to know how long  will
refer to the object for. are just numbers that indicate the location of a Go value in memory,
and they’re how a Go program references objects.
For example, to get the pointer to the beginning of the object allocated in the
last code snippet, we can write:Go’s garbage collector follows a strategy broadly referred to as tracing garbage
collection, which just means that the garbage collector follows, or traces, the
pointers in the program to identify which objects the program is still using.More specifically, the Go garbage collector implements the mark-sweep algorithm.
This is much simpler than it sounds.
Imagine objects and pointers as a sort of graph, in the computer science sense.
Objects are nodes, pointers are edges.The mark-sweep algorithm operates on this graph, and as the name might suggest,
proceeds in two phases.In the first phase, the mark phase, it walks the object graph from well-defined
source edges called .
Think global and local variables.
Then, it  everything it finds along the way as , to avoid going in
circles.
This is analogous to your typical graph flood algorithm, like a depth-first or
breadth-first search.Next is the sweep phase.
Whatever objects were not visited in our graph walk are unused, or ,
by the program.
We call this state unreachable because it is impossible with normal safe Go code
to access that memory anymore, simply through the semantics of the language.
To complete the sweep phase, the algorithm simply iterates through all the
unvisited nodes and marks their memory as free, so the memory allocator can reuse
it.You may think I’m oversimplifying a bit here.
Garbage collectors are frequently referred to as , and .
And you’d be partially right, there are more complexities.For example, this algorithm is, in practice, executed concurrently with your
regular Go code.
Walking a graph that’s mutating underneath you brings challenges.
We also parallelize this algorithm, which is a detail that’ll come up again
later.But trust me when I tell you that these details are mostly separate from the
core algorithm.
It really is just a simple graph flood at the center.Let’s walk through an example.
Navigate through the slideshow below to follow along.After all that, I think we have a handle on what the Go garbage collector is actually doing.
This process seems to work well enough today, so what’s the problem?Well, it turns out we can spend  of time executing this particular algorithm in some
programs, and it adds substantial overhead to nearly every Go program.
It’s not that uncommon to see Go programs spending 20% or more of their CPU time in the
garbage collector.Let’s break down where that time is being spent.At a high level, there are two parts to the cost of the garbage collector.
The first is how often it runs, and the second is how much work it does each time it runs.
Multiply those two together, and you get the total cost of the garbage collector.
    Total GC cost = Number of GC cycles × Average cost per GC cycle
    But for now let’s focus only on the second part, the cost per cycle.From years of poring over CPU profiles to try to improve performance, we know two big things
about Go’s garbage collector.The first is that about 90% of the cost of the garbage collector is spent marking,
and only about 10% is sweeping.
Sweeping turns out to be much easier to optimize than marking,
and Go has had a very efficient sweeper for many years.The second is that, of that time spent marking, a substantial portion, usually at least 35%, is
simply spent  on accessing heap memory.
This is bad enough on its own, but it completely gums up the works on what makes modern CPUs
actually fast.“A microarchitectural disaster”What does “gum up the works” mean in this context?
The specifics of modern CPUs can get pretty complicated, so let’s use an analogy.Imagine the CPU driving down a road, where that road is your program.
The CPU wants to ramp up to a high speed, and to do that it needs to be able to see far ahead of it,
and the way needs to be clear.
But the graph flood algorithm is like driving through city streets for the CPU.
The CPU can’t see around corners and it can’t predict what’s going to happen next.
To make progress, it constantly has to slow down to make turns, stop at traffic lights, and avoid
pedestrians.
It hardly matters how fast your engine is because you never get a chance to get going.Let’s make that more concrete by looking at our example again.
I’ve overlaid the heap here with the path that we took.
Each left-to-right arrow represents a piece of scanning work that we did
and the dashed arrows show how we jumped around between bits of scanning work.Notice that we were jumping all over memory doing tiny bits of work in each place.
In particular, we’re frequently jumping between pages, and between different parts of pages.Modern CPUs do a lot of caching.
Going to main memory can be up to 100x slower than accessing memory that’s in our cache.
CPU caches are populated with memory that’s been recently accessed, and memory that’s nearby to
recently accessed memory.
But there’s no guarantee that any two objects that point to each other will  be close to each
other in memory.
The graph flood doesn’t take this into account.Quick side note: if we were just stalling fetches to main memory, it might not be so bad.
CPUs issue memory requests asynchronously, so even slow ones could overlap if the CPU could see
far enough ahead.
But in the graph flood, every bit of work is small, unpredictable, and highly dependent on the
last, so the CPU is forced to wait on nearly every individual memory fetch.And unfortunately for us, this problem is only getting worse.
There’s an adage in the industry of “wait two years and your code will get faster.”But Go, as a garbage collected language that relies on the mark-sweep algorithm, risks the opposite.
“Wait two years and your code will get slower.”
The trends in modern CPU hardware are creating new challenges for garbage collector performance:Non-uniform memory access.
For one, memory now tends to be associated with subsets of CPU cores.
Accesses by  CPU cores to that memory are slower than before.
In other words, the cost of a main memory access depends on which CPU core is accessing
it.
It’s non-uniform, so we call this non-uniform memory access, or NUMA for short.Reduced memory bandwidth.
Available memory bandwidth per CPU is trending downward over time.
This just means that while we have more CPU cores, each core can submit relatively fewer
requests to main memory, forcing non-cached requests to wait longer than before.
Above, we looked at a sequential marking algorithm, but the real garbage collector performs this
algorithm in parallel.
This scales well to a limited number of CPU cores, but the shared queue of objects to scan becomes
a bottleneck, even with careful design.Modern hardware features.
New hardware has fancy features like vector instructions, which let us operate on a lot of data at once.
While this has the potential for big speedups, it’s not immediately clear how to make that work for
marking because marking does so much irregular and often small pieces of work.Finally, this brings us to Green Tea, our new approach to the mark-sweep algorithm.
The key idea behind Green Tea is astonishingly simple:Work with pages, not objects.Sounds trivial, right?
And yet, it took a lot of work to figure out how to order the object graph walk and what we needed to
track to make this work well in practice.More concretely, this means:Instead of scanning objects we scan whole pages.Instead of tracking objects on our work list, we track whole pages.We still need to mark objects at the end of the day, but we’ll track marked objects locally to each
page, rather than across the whole heap.Let’s see what this means in practice by looking at our example heap again, but this time
running Green Tea instead of the straightforward graph flood.As above, navigate through the annotated slideshow to follow along.Let’s come back around to our driving analogy.
Are we finally getting on the highway?Let’s recall our graph flood picture before.We jumped around a whole lot, doing little bits of work in different places.
The path taken by Green Tea looks very different.Green Tea, in contrast, makes fewer, longer left-to-right passes over pages A and B.
The longer these arrows, the better, and with bigger heaps, this effect can be much stronger.
 the magic of Green Tea.It’s also our opportunity to ride the highway.This all adds up to a better fit with the microarchitecture.
We can now scan objects closer together with much higher probability, so
there’s a better chance we can make use of our caches and avoid main memory.
Likewise, per-page metadata is more likely to be in cache.
Tracking pages instead of objects means work lists are smaller,
and less pressure on work lists means less contention and fewer CPU stalls.And speaking of the highway, we can take our metaphorical engine into gears we’ve never been able to
before, since now we can use vector hardware!If you’re only vaguely familiar with vector hardware, you might be confused as to how we can use it here.
But besides the usual arithmetic and trigonometric operations,
recent vector hardware supports two things that are valuable for Green Tea:
very wide registers, and sophisticated bit-wise operations.Most modern x86 CPUs support AVX-512, which has 512-bit wide vector registers.
This is wide enough to hold all of the metadata for an entire page in just two registers,
right on the CPU, enabling Green Tea to work on an entire page in just a few straight-line
instructions.
Vector hardware has long supported basic bit-wise operations on whole vector registers, but starting
with AMD Zen 4 and Intel Ice Lake, it also supports a new bit vector “Swiss army knife” instruction
that enables a key step of the Green Tea scanning process to be done in just a few CPU cycles.
Together, these allow us to turbo-charge the Green Tea scan loop.This wasn’t even an option for the graph flood, where we’d be jumping between scanning objects that
are all sorts of different sizes.
Sometimes you needed two bits of metadata and sometimes you needed ten thousand.
There simply wasn’t enough predictability or regularity to use vector hardware.If you want to nerd out on some of the details, read along!
Otherwise, feel free to skip ahead to the evaluation.To get a sense of what AVX-512 GC scanning looks like, take a look at the diagram below.There’s a lot going on here and we could probably fill an entire blog post just on how this works.
For now, let’s just break it down at a high level:First we fetch the “seen” and “scanned” bits for a page.
Recall, these are one bit per object in the page, and all objects in a page have the same size.Next, we compare the two bit sets.
Their union becomes the new “scanned” bits, while their difference is the “active objects” bitmap,
which tells us which objects we need to scan in this pass over the page (versus previous passes).We take the difference of the bitmaps and “expand” it, so that instead of one bit per object,
we have one bit per word (8 bytes) of the page.
We call this the “active words” bitmap.
For example, if the page stores 6-word (48-byte) objects, each bit in the active objects bitmap
will be copied to 6 bits in the active words bitmap.
Like so: → 000000 000000 111111 111111 ...Next we fetch the pointer/scalar bitmap for the page.
Here, too, each bit corresponds to a word (8 bytes) of the page, and it tells us whether that word
stores a pointer.
This data is managed by the memory allocator.Now, we take the intersection of the pointer/scalar bitmap and the active words bitmap.
The result is the “active pointer bitmap”: a bitmap that tells us the location of every
pointer in the entire page contained in any live object we haven’t scanned yet.Finally, we can iterate over the memory of the page and collect all the pointers.
Logically, we iterate over each set bit in the active pointer bitmap,
load the pointer value at that word, and write it back to a buffer that
will later be used to mark objects seen and add pages to the work list.
Using vector instructions, we’re able to do this 64 bytes at a time,
in just a couple instructions.Part of what makes this fast is the  instruction,
part of the “Galois Field New Instructions” x86 extension,
and the bit manipulation Swiss army knife we referred to above.
It’s the real star of the show, since it lets us do step (3) in the scanning kernel very, very
efficiently.
It performs a bit-wise affine
transformations,
treating each byte in a vector as itself a mathematical vector of 8 bits
and multiplying it by an 8x8 bit matrix.
This is all done over the Galois field,
which just means multiplication is AND and addition is XOR.
The upshot of this is that we can define a few 8x8 bit matrices for each
object size that perform exactly the 1:n bit expansion we need.For the full assembly code, see this
file.
The “expanders” use different matrices and different permutations for each size class,
so they’re in a separate file
that’s written by a code generator.
Aside from the expansion functions, it’s really not a lot of code.
Most of it is dramatically simplified by the fact that we can perform most of the above
operations on data that sits purely in registers.
And, hopefully soon this assembly code will be replaced with Go code!Credit to Austin Clements for devising this process.
It’s incredibly cool, and incredibly fast!So that’s it for how it works.
How much does it actually help?It can be quite a lot.
Even without the vector enhancements, we see reductions in garbage collection CPU costs
between 10% and 40% in our benchmark suite.
For example, if an application spends 10% of its time in the garbage collector, then that
would translate to between a 1% and 4% overall CPU reduction, depending on the specifics of
the workload.
A 10% reduction in garbage collection CPU time is roughly the modal improvement.
(See the GitHub issue for some of these details.)We’ve rolled Green Tea out inside Google, and we see similar results at scale.We’re still rolling out the vector enhancements,
but benchmarks and early results suggest this will net an additional 10% GC CPU reduction.While most workloads benefit to some degree, there are some that don’t.Green Tea is based on the hypothesis that we can accumulate enough objects to scan on a
single page in one pass to counteract the costs of the accumulation process.
This is clearly the case if the heap has a very regular structure: objects of the same size at a
similar depth in the object graph.
But there are some workloads that often require us to scan only a single object per page at a time.
This is potentially worse than the graph flood because we might be doing more work than before while
trying to accumulate objects on pages and failing.The implementation of Green Tea has a special case for pages that have only a single object to scan.
This helps reduce regressions, but doesn’t completely eliminate them.However, it takes a lot less per-page accumulation to outperform the graph flood
than you might expect.
One surprise result of this work was that scanning a mere 2% of a page at a time
can yield improvements over the graph flood.Green Tea is already available as an experiment in the recent Go 1.25 release and can be enabled
by setting the environment variable  to  at build time.
This doesn’t include the aforementioned vector acceleration.We expect to make it the default garbage collector in Go 1.26, but you’ll still be able to opt-out
with GOEXPERIMENT=nogreenteagc at build time.
Go 1.26 will also add vector acceleration on newer x86 hardware, and include a whole bunch of
tweaks and improvements based on feedback we’ve collected so far.If you can, we encourage you to try at Go tip-of-tree!
If you prefer to use Go 1.25, we’d still love your feedback.
See this GitHub
comment with some details on
what diagnostics we’d be interested in seeing, if you can share, and the preferred channels for
reporting feedback.Before we wrap up this blog post, let’s take a moment to talk about the journey that got us here.
The human element of the technology.The core of Green Tea may seem like a single, simple idea.
Like the spark of inspiration that just one single person had.But that’s not true at all.
Green Tea is the result of work and ideas from many people over several years.
Several people on the Go team contributed to the ideas, including Michael Pratt, Cherry Mui, David
Chase, and Keith Randall.
Microarchitectural insights from Yves Vandriessche, who was at Intel at the time, also really helped
direct the design exploration.
There were a lot of ideas that didn’t work, and there were a lot of details that needed figuring out.
Just to make this single, simple idea viable.The seeds of this idea go all the way back to 2018.
What’s funny is that everyone on the team thinks someone else thought of this initial idea.Green Tea got its name in 2024 when Austin worked out a prototype of an earlier version while cafe
crawling in Japan and drinking LOTS of matcha!
This prototype showed that the core idea of Green Tea was viable.
And from there we were off to the races.Throughout 2025, as Michael implemented and productionized Green Tea, the ideas evolved and changed even
further.This took so much collaborative exploration because Green Tea is not just an algorithm, but an entire
design space.
One that we don’t think any of us could’ve navigated alone.
It’s not enough to just have the idea, but you need to figure out the details and prove it.
And now that we’ve done it, we can finally iterate.The future of Green Tea is bright.Once again, please try it out by setting  and let us know how it goes!
We’re really excited about this work and want to hear from you!]]></content:encoded></item><item><title>A tour of Go&apos;s newest garbage collector</title><link>https://golangweekly.com/issues/576</link><author></author><category>dev</category><category>go</category><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate><source url="https://golangweekly.com/">Golang Weekly</source><content:encoded><![CDATA[ — A highly configurable, Go-powered SFTP, HTTP/S, FTP/S and WebDAV server that can interoperate with cloud-based storage backends. v2.7 adds support for Post-Quantum Traditional Hybrid Key Exchange and drops rsync and Git hosting support. GitHub repo.]]></content:encoded></item><item><title>How to Eliminate GraphQL N+1 Query Problem in Golang with DataLoader Batching</title><link>https://dev.to/aaravjoshi/how-to-eliminate-graphql-n1-query-problem-in-golang-with-dataloader-batching-4b9c</link><author>Aarav Joshi</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 28 Oct 2025 23:32:24 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! GraphQL's flexibility in querying data comes with a significant performance cost if not handled carefully. The N+1 query problem, where a single request triggers multiple follow-up queries, can cripple response times in nested data structures. I have spent considerable time refining GraphQL implementations in Golang to tackle this issue head-on. Through a combination of intelligent batching, multi-level caching, and query analysis, it is possible to achieve sub-millisecond response times even for deeply nested queries.In many GraphQL setups, each field resolver operates independently, leading to repeated database calls for related data. This inefficiency becomes glaringly obvious when querying lists of objects with nested relationships. My approach centers on intercepting these resolver calls and grouping them into batch operations. This transformation reduces the number of database round trips from linear to constant, regardless of query depth.The core of this optimization lies in the DataLoader pattern. A DataLoader collects individual data requests within a single execution frame and processes them as a batch. I implemented a DataLoader in Golang that uses channels and mutexes to manage concurrent access. It waits for a configurable time or until a batch size limit is reached before dispatching the accumulated keys to a batch function. This method ensures that related data fetches happen in one go.Here is a practical example of setting up a DataLoader for user posts. Suppose we have a GraphQL schema where users have multiple posts. Without batching, fetching ten users with their posts could result in eleven database queries: one for the users and ten for their posts. With the DataLoader, it becomes two queries.Caching plays an equally critical role in this optimization strategy. I designed a multi-level caching system that operates at both the query and field levels. The query cache stores parsed query structures and their analysis results. This prevents repetitive parsing of identical queries, saving valuable CPU cycles. The field cache stores the results of individual field resolvers based on their inputs and context.Implementing the query cache involves hashing the query string and storing the parsed AST along with metadata like field complexity. When the same query arrives again, we skip the parsing phase and proceed directly to execution planning. This is particularly effective in applications with repetitive query patterns, such as those from mobile clients or dashboard interfaces.Field-level caching requires careful key generation to avoid serving stale data. I use a combination of the field name, arguments, and parent object state to create unique cache keys. The cache has a configurable TTL and maximum size to manage memory usage. In high-traffic scenarios, this cache can reduce resolver execution time by over 90 percent for frequently accessed fields.Here is how the field cache integrates into a resolver. The cache check happens before any data fetching logic, ensuring that cached results are returned immediately.Query analysis is the third pillar of this optimization framework. Before executing a query, the system analyzes its structure to identify batching opportunities and optimal execution order. It examines field selections, depth, and relationships to group resolvers that can be batched together. This pre-execution planning phase adds minimal overhead but yields significant performance gains.In one of my projects, I added complexity scoring to queries during analysis. Queries with high complexity scores trigger more aggressive batching and caching strategies. For instance, queries involving multiple nested levels automatically use DataLoaders for all relation fields. This proactive approach ensures that performance remains consistent even as query complexity scales.The QueryOptimizer struct ties all these components together. It manages the DataLoader registry, query cache, and field cache. When a query arrives, it first checks the query cache. If missing, it parses and caches the query. Then, it analyzes the query to create an execution plan that maximizes batching and caching efficiency.Here is a simplified version of how the QueryOptimizer processes a query. The ExecuteQuery method handles the entire lifecycle, from caching to execution and stats collection.Performance monitoring is built into the system to track the effectiveness of these optimizations. Metrics like cache hit rates, batch sizes, and resolver call counts provide insights into how well the system is performing. I often use these metrics to fine-tune parameters like batch sizes and cache TTLs based on actual usage patterns.In a benchmark test with a query fetching 100 users, each with 10 posts and 5 comments per post, the optimized version processed 1000 executions in under 10 seconds. The naive approach took over 2 minutes. The cache hit rate was around 85 percent after warm-up, and the number of database queries reduced from thousands to dozens.Handling concurrency in Golang requires careful synchronization. The DataLoader uses a mutex to protect its internal state, and channels to communicate results back to waiting goroutines. This design ensures that multiple concurrent queries can share the same DataLoader instance without data races or deadlocks.I recall a scenario where a sudden spike in user activity caused performance degradation. By adjusting the DataLoader's maxWait time and batch size, I was able to maintain low latency without overloading the database. The ability to dynamically tune these parameters is crucial for adapting to changing load patterns.Another important aspect is cache invalidation. In systems with frequent data updates, stale cache entries can lead to inconsistencies. I implemented a cache invalidation strategy that uses database triggers or message queues to evict cached entries when underlying data changes. This ensures that users always see the most recent data without sacrificing performance.Here is an example of cache invalidation in action. When a user updates their profile, we invalidate all cached fields that depend on that user's data.For production deployments, I recommend setting query complexity limits to prevent denial-of-service attacks. Deeply nested or overly broad queries can consume excessive resources. By rejecting queries that exceed predefined complexity thresholds, you protect the system from abusive queries while maintaining performance for legitimate users.Request timeouts are another essential safeguard. I configure the GraphQL server to cancel queries that take longer than a specified duration. This prevents slow queries from monopolizing resources and ensures predictable response times under load.Distributed caching can further enhance scalability. By using a shared cache like Redis for the field cache, multiple application instances can share cached results. This reduces redundant computation in a microservices architecture and improves overall system efficiency.I have integrated this optimization framework into several high-traffic GraphQL APIs. In one e-commerce platform, it reduced average response time from 200ms to 15ms for product listing queries. The reduction in database load allowed the platform to handle twice the traffic with the same infrastructure.The code examples provided are modular and can be adapted to various GraphQL schemas. The key is to identify the relationships in your data model that benefit most from batching and caching. Start with the most frequently accessed nested fields and gradually expand the optimization to cover more of your schema.Testing is vital to ensure that optimizations do not introduce bugs. I write extensive unit tests for DataLoaders to verify that they correctly batch requests and return the right results. Integration tests simulate real query patterns to validate end-to-end performance improvements.In conclusion, optimizing GraphQL query execution in Golang requires a holistic approach. Batching, caching, and query analysis work together to mitigate the N+1 problem and other inefficiencies. The implementation I shared has proven effective in production environments, delivering fast and reliable GraphQL APIs.
  
  
  I encourage you to experiment with these techniques in your projects. Start small, measure the impact, and iterate based on your specific needs. The performance gains are well worth the effort, and the skills you develop will serve you well in building scalable GraphQL services.
📘 , , , and  to the channel! is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly low—some books are priced as low as —making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>Build High-Performance Reverse Proxy and Load Balancer in Golang: Complete Implementation Guide</title><link>https://dev.to/aaravjoshi/build-high-performance-reverse-proxy-and-load-balancer-in-golang-complete-implementation-guide-2blp</link><author>Aarav Joshi</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 28 Oct 2025 23:15:13 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! In modern web infrastructure, reverse proxies and load balancers form the backbone of scalable systems. I've spent years working with distributed systems, and building a high-performance reverse proxy in Golang has been one of the most rewarding projects. The language's concurrency model and efficiency make it ideal for this task. Today, I'll walk through creating a robust solution that handles thousands of requests per second while maintaining low latency.When I first started designing this system, my goal was to minimize overhead while maximizing reliability. A reverse proxy sits between clients and backend servers, routing requests based on various strategies. It needs to be fast, resilient, and intelligent. Golang's standard library provides excellent tools, but careful design is crucial for performance.Let me begin with the core structure. The ReverseProxy type manages all routing logic and backend coordination. It keeps track of available servers, applies load balancing strategies, and monitors health status. I use a mutex to handle concurrent access safely, ensuring data consistency under high load.Each backend server is represented by a simple struct. It holds the server URL, alive status, active connection count, and other metadata. The Alive field is updated by the health checker, while Connections is managed atomically to avoid race conditions.Initializing the proxy involves parsing backend URLs and setting up health checks. I prefer to start health monitoring immediately after creation. This ensures that any unavailable servers are detected early and excluded from routing.The ServeHTTP method handles incoming requests. It selects a backend using the configured strategy, increments connection counters, and proxies the request. I use httputil.ReverseProxy for the heavy lifting but customize it for better performance.Load balancing strategies determine how requests are distributed. I've implemented several approaches over time. Round-robin is simple and effective for many use cases. It cycles through backends in order, ensuring fair distribution.For scenarios where backends have different capacities, least connections works better. It selects the server with the fewest active connections, helping to balance load based on current utilization.Health checking is critical for maintaining system reliability. I run checks concurrently at regular intervals. Each backend is tested independently, and status updates happen atomically to avoid conflicts.Performance monitoring helps me understand system behavior under load. I track requests, errors, bandwidth, and latency. These metrics are stored atomically and can be exposed for external monitoring systems.In production, I run the proxy as an HTTP server. It listens on a specified port and handles incoming traffic. I also start a background goroutine to log statistics periodically, which helps in debugging and capacity planning.Connection management plays a huge role in performance. By reusing connections with proper timeouts, I reduce TCP handshake overhead. The transport configuration balances resource usage with responsiveness.When a backend fails, the health checker marks it as dead within seconds. The load balancer automatically skips unavailable servers. This failover mechanism ensures high availability without manual intervention.I've tested this setup under various load conditions. On an 8-core machine, it comfortably handles over 50,000 requests per second. Latency remains low, typically under one millisecond for local backends.Memory usage scales predictably with the number of active connections. The atomic operations and efficient data structures keep overhead minimal. The system remains stable even during traffic spikes.One challenge I faced was ensuring thread safety across all components. Using sync.RWMutex for the backends list and atomic operations for counters solved most issues. Proper testing under concurrent load was essential.Another area I focused on was request modification. Adding headers like X-Forwarded-For helps backend servers identify original client IPs. This is crucial for logging and security policies.The health check endpoint should be lightweight and fast. I assume backends expose a /health path that returns 200 OK when healthy. In real deployments, this endpoint might check database connections or other dependencies.For environments with heterogeneous servers, weighted load balancing can be useful. Heavier backends get more traffic, while lighter ones handle less. Implementing this requires tracking server capacity and adjusting selection logic.Circuit breakers are a valuable addition. They prevent continuous attempts to failed backends, giving them time to recover. I typically implement this by tracking error rates and temporarily excluding problematic servers.Rate limiting protects backends from overload. By tracking request rates per client or overall, the proxy can reject excess traffic. This is especially important in public-facing deployments.TLS termination offloads encryption from backend servers. The proxy handles SSL/TLS, forwarding plain HTTP internally. This improves performance and centralizes certificate management.Access logging provides visibility into traffic patterns. I log details like client IP, request path, backend used, and response code. This data is invaluable for debugging and analysis.Sticky sessions maintain user state across requests. By routing subsequent requests from the same client to the same backend, stateful applications work correctly. This can be implemented with cookies or IP-based hashing.Metrics export integrates with monitoring systems like Prometheus. By exposing stats via an HTTP endpoint, external tools can scrape and alert on key indicators. This helps in proactive maintenance.Graceful shutdown ensures no requests are lost during restarts. The proxy stops accepting new connections and waits for existing ones to complete. This requires careful coordination with the HTTP server.In one deployment, I added response caching to reduce backend load. For static or semi-static content, storing responses locally improved performance significantly. The cache was invalidated based on TTL or specific headers.Another enhancement was request buffering. For large uploads, reading the entire request before forwarding can prevent timeouts. This trades memory for reliability in high-latency networks.I also experimented with dynamic configuration. Using etcd or Consul, the proxy can update backends without restarting. This allows seamless scaling and maintenance.Error handling is robust but simple. When no backends are available, the proxy returns a 503 error. Detailed logging helps identify root causes quickly.The code examples I've shared form a solid foundation. They demonstrate core concepts without unnecessary complexity. From here, you can extend functionality based on specific needs.Building this proxy taught me much about Golang's strengths. Its standard library and concurrency primitives make such tasks manageable. The performance is impressive even without extensive optimization.In conclusion, a well-designed reverse proxy and load balancer are essential for modern applications. Golang provides the tools to build efficient and reliable solutions. The implementation I've described handles high traffic with low overhead, ensuring smooth operation.I continue to refine this approach based on real-world usage. Each deployment brings new insights and improvements. The flexibility of Golang allows adapting to changing requirements easily.
  
  
  If you're building similar systems, start with these basics. Test thoroughly under load, monitor key metrics, and iterate based on feedback. The result will be a robust component that supports your infrastructure reliably.
📘 , , , and  to the channel! is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly low—some books are priced as low as —making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>&quot;Go-ing&quot; Crazy? Why This Little Gopher is Your New Best Friend ! 🚀</title><link>https://dev.to/akshay_gengaje/go-ing-crazy-why-this-little-gopher-is-your-new-best-friend-42g0</link><author>Akshay Gengaje</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 28 Oct 2025 15:35:04 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[So, you want to learn to code. You've heard of Python, Java, and that weird one called C++. But then, someone whispers the word... .What is Go? Why is its mascot a cute (but slightly smug) gopher? And why should you, a complete beginner, even bother?Let's be real: learning to program can feel like trying to build a spaceship with nothing but a spoon and some hope. It's confusing. It's frustrating.Go (or "Golang," its fancy Sunday name) is different. It was invented by some super-smart engineers at  because they were tired of their other tools being slow and complicated. They said, "What if we made a language that's as  but as ?"And so, Go was born. Think of it as a sleek, modern sports car. It's incredibly powerful, but it only has a few buttons: "Go Fast," "Go Safe," and "Don't Explode."
  
  
  Why Bother Learning Go? (aka "The Good Stuff")
If you're going to spend time learning something, it better be worth it. Here's why Go is awesome.Go is a "compiled" language. In human terms, this means before you run your program, Go translates your  plan into the computer's native language (1s and 0s).Other languages (like Python): It's like giving a chef your recipe one... step... at... a... time. The chef has to read, think, and then do the step. It's flexible, but slow. It's like giving the chef a  recipe in their native language, with all the ingredients already pre-chopped. The chef just . The result? Your program runs at warp speed.
  
  
  2. Go's Superpower: Juggling (aka Concurrency) 🤹
This is Go's party trick.Imagine you're a chef trying to make dinner. In most languages, you have to:You can only do one thing at a time. It's boring and slow.With Go, you're a master chef. You just say:Go creates tiny, super-cheap helpers (called ) that go off and do all those tasks . You, the master chef, can just chill and wait for them to finish.This is called , and it's why companies like , , and  use Go to handle millions of users at once without breaking a sweat.
  
  
  3. The Rulebook is TINY 📖
Learning a language like Java or C++ is like trying to memorize a 1,000-page legal document. Go's entire rulebook is more like a pamphlet.Go has only  (the special "command" words). Java has 50. C++ has over 90.This means you spend less time scratching your head wondering, "What does  even mean?!" and more time actually building stuff. It also cleans up its own mess (called "garbage collection"), so you don't have to worry about the boring memory management stuff that gives other programmers nightmares.
  
  
  Your "Go-Bag": How to Start in 3 Steps
Okay, you're sold. You want to become a "Go-pher" (yes, that's what we're called).
  
  
  Step 1: Install It (The 5-Minute-Part)
Download the installer for your system (Mac, Windows, Linux). Click "Next" a few times. You're done. Seriously.
  
  
  Step 2: Set Up Your "Kitchen"
 Create a folder anywhere. Call it . Open this folder in a code editor.  is a fantastic free choice. When you open a Go file, it will even ask, "Hey, want me to install the Go tools?" Just say yes.
  
  
  Step 3: Write "Hello, Gopher!"
This is the "Hello, World!" of Go. Create a new file in your folder called  and type this in:Whoa, what is that? Let's break it down, fast:: This is the "cover page" of your project. It tells Go, "This is the file where the whole thing starts.": We're importing a "toolbox" called  (short for "format"). This toolbox has tools for printing things to the screen.: This is the main  or . When you run your program, Go looks for  and does whatever is inside its curly braces .: We're using our  toolbox, grabbing the  (Print Line) tool, and telling it to print our message.
Open a terminal, go into your project folder, and type:You should see:Hello, Gopher! Let's Go-oooo! 🚀YOU ARE OFFICIALLY A PROGRAMMER. PUT IT ON YOUR RESUME. (Okay, maybe do a few more tutorials first).
  
  
  A Peek at Go's "Personality" (aka Cool Quirks)
Go is simple, but it has a few "opinions" you'll find funny.
  
  
  1. Capital Letters Matter. A LOT.
This is my favorite part. In Go, how you name things actually changes how they work.If you name something starting with a apital letter (like ), it's . It's like you're shouting its name from the rooftops. Any other part of your code can see it and use it.If you name it with a owercase letter (like ), it's . It's like you're whispering. Only the code inside that  can see it.No more "public," "private," "protected" keywords. Just a capital letter. It's so simple, it's genius.If you  a toolbox (like ) but , Go will refuse to run.
If you create a variable but , Go will refuse to run.It's like a super-clean roommate who says, "If you're not going to use that, it doesn't belong here." It seems annoying at first, but it keeps your code  and easy to read.
  
  
  So, Should You "Go" for It?
Go is the perfect "second language," and honestly, it's a fantastic  language too. It teaches you all the important concepts (variables, loops, functions) without all the confusing junk.You'll be learning a language used to build massive, world-class systems, but you'll feel like you're just playing with high-speed Legos.Ready to start your journey? The best place to go next is the official , a fun, interactive tutorial that runs right in your browser.What are you waiting for? Stop  in circles and start coding!]]></content:encoded></item><item><title>The Hidden Performance Killer in Your Go API: GORM Preload and JSON Serialization</title><link>https://dev.to/dr_official_4aaf3c1fb2ca4/the-hidden-performance-killer-in-your-go-api-gorm-preload-and-json-serialization-b50</link><author>dr official</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 28 Oct 2025 09:58:37 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Have you ever built a Go API using GORM that worked perfectly in development, only to discover in production that a single endpoint was returning  and bringing your server to its knees?I did. And it took me days to figure out why.The culprit? GORM's association loading combined with Go's default JSON serialization—a deadly combination that can turn your sleek API into a bandwidth-eating monster.In this comprehensive guide, I'll show you exactly what went wrong, why the obvious solutions don't work, and how to fix it properly. By the end, you'll understand how to build scalable Go APIs that handle complex data models without performance disasters.The Problem: A Real Production NightmareUnderstanding GORM PreloadWhy Your First Fix Won't WorkImplementation Guide with Examples
  
  
  The Problem: A Real Production Nightmare
Let's start with a simple blog application. You have three models: Users, Posts, and Comments.Looks clean, right? Now let's create a simple API endpoint to fetch a user:curl http://localhost:8080/users/1
Perfect! Small response, fast, clean. Ship it! 🚀
  
  
  Two Weeks Later in Production...
Your monitoring system is screaming. The  endpoint is:Taking 30+ seconds to respondConsuming 2GB+ memory per requestReturning 500MB+ responsesCrashing your load balancerWhat changed? Your users started creating content.Each post has 50+ commentsEach comment references its author (user) and postWhen you fetch that user, here's what actually happens:The response becomes exponentially large due to circular references and nested associations.
  
  
  Understanding GORM Preload
Before we fix this, let's understand what GORM's  does and doesn't do.This tells GORM: "Load the user AND their posts in separate queries."GORM then automatically populates the  field in your struct.Here's the critical misunderstanding: Preload controls DATABASE loading, NOT JSON serialization.Why? Because Go's JSON encoder looks at , not database queries.
  
  
  Gotcha #1: Empty Arrays Are Still Serialized
Even though you didn't preload , the field exists in the struct and gets serialized.
  
  
  Gotcha #2: Circular References Create Infinite Data
User 
  → Posts[0]
      → User
          → Posts[0]  // Same post again!
              → User
                  → Posts[0]  // INFINITE!
Go's JSON encoder is smart enough not to crash, but it creates .
  
  
  Gotcha #3: Auto-Loading Nested Associations
Even without explicit preloading, GORM can load nested associations:
  
  
  Why Your First Fix Won't Work
Most developers try one of these approaches first:
  
  
  ❌ Attempt 1: Use GORM's Omit()
 No change in JSON size. prevents database queries but doesn't affect JSON encoding. The struct fields still exist and get serialized as .
  
  
  ❌ Attempt 2: Use Select() to Limit Columns
 Still includes  and  as . You're selecting columns from the  table, but struct fields are still there.
  
  
  ❌ Attempt 3: Clear the Fields Manually
 Works, but unmaintainable.Have to remember all associations
  
  
  ❌ Attempt 4: Create a Custom Struct
 Works, but creates code duplication.Duplicate struct definitionsDoesn't solve nested associationsThe fix requires :
  
  
  Layer 1: JSON Tags (Primary Solution)
Add explicit JSON tags to control serialization:Large associations, circular refsIncluded only if not empty/nil
  
  
  Layer 2: GORM Optimization
Optimize database queries to avoid loading unnecessary data: This alone doesn't fix JSON, but improves performance.
  
  
  Layer 3: Selective Preloading
When you need associations, load only necessary columns:
  
  
  Implementation Guide with Examples
Let's rebuild our blog application properly.
  
  
  Step 1: Add JSON Tags to Models

  
  
  Step 2: Create Repository Methods

  
  
  Step 3: Create API Handlers

  
  
  Pattern 1: Conditional Loading Based on Query Parameters

  
  
  Pattern 2: Response DTOs for Complex Scenarios
When you need to include data marked with , use a DTO:
  
  
  Pattern 3: Nested Preloading with Optimization
When you need nested associations, optimize at every level:But remember, for this to work, your models need proper tags:
  
  
  Pattern 4: Reusable Preload Functions
Create reusable preload functions to avoid duplication:Let's measure the impact of our optimizations using a real dataset:50,000 posts (50 per user)250,000 comments (5 per post) Fetch user with ID 500
  
  
  Benchmark 1: No Optimization
Database Query Time: 25.3 seconds
Memory Usage: 2.1 GB
Response Size: 487 MB
JSON Encoding Time: 8.7 seconds
Total Time: 34.0 seconds

  
  
  Benchmark 2: With GORM Optimization Only
Database Query Time: 2.1 seconds  ← 92% faster
Memory Usage: 450 MB              ← 79% less
Response Size: 425 MB             ← Only 13% reduction!
JSON Encoding Time: 7.2 seconds   ← Still slow
Total Time: 9.3 seconds           ← 73% faster
 Database improved significantly, but JSON is still massive because struct fields are serialized.
  
  
  Benchmark 3: With JSON Tags
Database Query Time: 0.005 seconds  ← 99.98% faster
Memory Usage: 2 MB                  ← 99.9% less
Response Size: 150 bytes            ← 99.97% smaller!
JSON Encoding Time: 0.001 seconds   ← 99.99% faster
Total Time: 0.006 seconds           ← 99.98% faster

  
  
  Benchmark 4: Complete Solution (Optimized Everything)
Database Query Time: 0.008 seconds
Memory Usage: 5 MB
Response Size: 2.3 KB
JSON Encoding Time: 0.002 seconds
Total Time: 0.010 seconds

  
  
  Performance Summary Table
 JSON tags provide the biggest impact. GORM optimization is important but secondary.
  
  
  Real-World Example: E-Commerce API
Let's apply this to a real e-commerce scenario:GET /customers/123/orders?page=1&limit=5[ ] Add  to all large array associations[ ] Add  to circular references (one direction)[ ] Use  for optional associations[ ] Document why each association has its tag[ ] Keep models focused and simple[ ] Use  to skip loading unused associations[ ] Use  to limit columns in preloads[ ] Avoid N+1 queries with proper [ ] Add database indexes for foreign keys[ ] Monitor slow query log[ ] Create separate endpoints for large data[ ] Paginate all list endpoints[ ] Use query parameters for optional includes[ ] Return consistent response structures[ ] Document all endpoints clearly[ ] Monitor response sizes (alert > 1MB)[ ] Set response time budgets (<100ms preferred)[ ] Use caching for frequently accessed data[ ] Implement rate limiting[ ] Load test before production[ ] Create reusable preload functions[ ] Use DTOs for complex responses[ ] Write tests for JSON serialization[ ] Document performance trade-offs[ ] Review code for circular references
  
  
  ❌ Mistake 1: Forgetting One Direction of Circular Reference

  
  
  ❌ Mistake 2: Using omitempty on Slices

  
  
  ❌ Mistake 3: Not Testing with Real Data

  
  
  ❌ Mistake 4: Preloading Everything

  
  
  Testing Your Implementation

  
  
  Test 1: JSON Serialization Test

  
  
  Test 2: Response Size Test

  
  
  Test 3: Performance Benchmark

  
  
  Monitoring and Observability

  
  
  Add Response Size Logging
GORM's preload behavior combined with Go's default JSON serialization can create severe performance issues that only appear at scale. The solution requires understanding three key concepts:GORM controls database loading - Use  and  to optimize queriesJSON tags control serialization - Use  and  to control output - Separate large data into dedicated endpointsBy following the patterns in this guide, you can build Go APIs that:✅ Scale to production workloads✅ Have predictable performance✅ Use bandwidth efficiently✅ Provide great user experienceRemember: Optimize for the 1,000th user, not the 1st. The problems often don't appear until you have real data and real usage patterns.JSON tags are your first line of defense - They have the biggest impactBreak circular references - Choose one direction to serialize - Use dedicated paginated endpoints - Small datasets hide problems - Set alerts for >1MB responsesUse Select() with Preload() - Load only what you need - Explain why associations are excluded #golang #gorm #api #performance #optimization #json #database #webdevelopment #backend #programmingIf you found this guide helpful, please share it with your team and give it a clap! 👏]]></content:encoded></item><item><title>🏢 Enterprise Design Patterns: Building Scalable Systems with Fowler’s Patterns in Go</title><link>https://dev.to/cesar_nikolascamacmelen/enterprise-design-patterns-building-scalable-systems-with-fowlers-patterns-in-go-2ckk</link><author>CESAR NIKOLAS CAMAC MELENDEZ</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 28 Oct 2025 02:36:58 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Inspired by Martin Fowler’s Catalog of Patterns of Enterprise Application ArchitectureEnterprise applications are complex by nature. They manage large datasets, coordinate multiple services, and must remain maintainable and scalable over time.Enterprise Application Architecture Patterns provides a timeless catalog of design patterns that help developers structure large-scale systems.In this article, we’ll explore Enterprise Design Patterns, understand their key categories, and implement a real-world example in Go (Golang) that demonstrates how they work in practice.
  
  
  💡 What Are Enterprise Design Patterns?
Enterprise Design Patterns are  to common architectural problems in enterprise applications — such as managing transactions, persistence, or business logic.Fowler classifies them into several major categories:Organize business logic (e.g., , , )Data Source Architectural PatternsHandle how data is accessed and stored (e.g., , )Object-Relational Behavioral PatternsManage interaction between objects and relational data (e.g., , )Web Presentation PatternsStructure the UI and interaction (e.g., )Manage distributed system behavior (e.g., , )
  
  
  ⚙️ Example: Repository + Unit of Work Pattern in Go
Let’s take a practical example of an enterprise-grade approach to managing persistence in a .We’ll combine two Fowler patterns:🧩  — provides an abstraction layer over data access logic.
🔄  — tracks changes to objects during a transaction and coordinates the writing out of changes.
go-enterprise-patterns/
├── main.go
├── domain/
│   └── user.go
├── repository/
│   └── user_repository.go
└── unitofwork/
└── unit_of_work.go

package domain

// User represents a simple domain entity.
type User struct {
    ID    int
    Name  string
    Email string
}

  
  
  📦 repository/user_repository.go

  
  
  🔄 unitofwork/unit_of_work.go
This pattern combination helps ensure that:✅ Business logic stays independent from database details🧩 You can  all new entities in one transaction🧪 It becomes easier to  or  later (e.g., move from in-memory to PostgreSQL)This approach scales as your system grows — a foundational step for clean, maintainable enterprise applications.Enterprise patterns aren’t just academic ideas — they are battle-tested abstractions that help teams manage complexity in real-world systems.By implementing these patterns in languages like , you create software that’s modular, testable, and adaptable to change — all core values in modern enterprise development.💬 If you found this article useful, drop a ❤️ on Dev.to or follow me for more deep dives into software architecture and Go!]]></content:encoded></item><item><title>Building an Organized API in Golang Using Fiber</title><link>https://dev.to/xandecodes/building-an-organized-api-in-golang-using-fiber-2pb8</link><author>Alexandre Fernandes dos Santos</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 27 Oct 2025 21:33:03 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[After spending some time exploring Go and struggling a bit with its package system (here I must admit it's actually a great system when used properly), I found myself thinking a lot about the best way to organize the study API I was building.I gathered many examples, including from the Fiber repository, and arrived at a structure that I find quite readable and easy to extend with new features. I'm not saying this is the best way to organize a Go API, but it worked for my needs and I believe it can be useful in many cases where a generic API is required.This API was created to store information about books and has only three endpoints: Returns information about a book Adds or updates information about a book Deletes book informationI implemented just the basics, using the Fiber framework which has an approach similar to Express.js that I really liked, combined with Go's advantages like lower memory allocation and incredible speed. The data is stored in Redis, which can be initialized using a docker-compose file.| api_simples
├── docker-compose.yml
├── docs
│   └── estrutura.png
├── go.mod
├── go.sum
├── main.go
├── pkg
│   ├── configurations
│   │   └── database.go
│   ├── entities
│   │   └── book.go
│   ├── handlers
│   │   └── book_handler.go
│   ├── repositories
│   │   ├── book_repository.go
│   │   └── commons.go
│   └── routes
│       ├── book_router.go
│       └── routes.go
└── README.md
I believe it's more valuable to explain the reasoning behind this organization rather than just listing what each folder contains. I won't follow the exact order above since I think it's clearer to explain in a different sequence:This file helps resolve various Go workspace issues, allowing me to create a Go repository anywhere without problems accessing external or local packages.To create it, I ran the go mod init command with the GitHub project path as argument (github.com/XandeCoding/codigos-de-artigos/golang/api_simples). While it's not strictly necessary to use the full GitHub path (the project name api_simples would work), I chose to include it since this is a public project. This makes it easier to reference specific files from the project.The pkg folder contains the main API code where all features are implemented. The main.go file is only used to initialize the application and contains no implementation logic, serving solely to start the API.This contains configuration files. In this case,  configures database access. If we had other application or tool configurations used by one or more parts of the application, they would also go here - such as custom Fiber configurations or environment variables.Example Redis connection configuration in :Entities can be used in various places, particularly in this case where I use them both for receiving data in endpoints and for database operations. Placing them in a common location is quite useful, though in feature-separated package structures, this approach might not be as ideal.
pkg/repositoriesThis package contains functions that work directly with the Redis database. They receive the book entity and have functions that insert, update, and delete it from the database. If there were another entity to handle, such as library, it would be in a separate file containing only functions related to that data.Fragment from :Following the example of other parts of the application, I separated routes into different files. Even though we have a routes.go file that initializes these routes, it's helpful to keep routes for specific resources separate for better readability and understanding for others who might maintain the code.Route initialization in :In book_router.go, I only specify the routes, methods, and handler functions located in another part of the application. Another important aspect is that this structure allows us to create instances that can be reused across all endpoints for this specific resource - in this case, a Redis database connection instance.Fragment from :In handlers, I place the functions called by the endpoints. For example, the PUT /book endpoint calls the SetBookHandler function in book_handler.go, which returns the function to be executed when this resource is accessed.Code for the  function:I hope this helps with some of the initial challenges when building an API, especially in a language we're not very familiar with. This initial structure worked well for me, but any comments or feedback are always welcome for continuous improvement. Until next time! 👋]]></content:encoded></item><item><title>A hard rain&apos;s a-gonna fall: decoding JSON in Rust</title><link>https://bitfieldconsulting.com/posts/hard-rain-json-rust</link><author>John Arundel</author><category>dev</category><category>blog</category><category>go</category><pubDate>Mon, 27 Oct 2025 13:15:00 +0000</pubDate><source url="https://bitfieldconsulting.com/posts/">Bitfield</source><content:encoded><![CDATA[JSON is the worst data format, apart from all the others, but here we
are. This is the life we chose, and if we’re writing Rust programs to
talk to remote APIs such as Weatherstack, as we did in Here comes the sun, we’ll have to be
able to cope with them sending us JSON data.We’ll also need a way to turn that JSON into good honest Rust data,
so we can compute stuff about it. Let’s see how to do that, as we
continue to hack on our embarrassingly basic weather client program.In Elephants for
breakfast we pondered how to test a function such as
 when we can’t know in advance 
weather conditions it’ll return. The correct response is literally up in
the air.So we deftly flipped the problem——and decided
instead to write two functions that  testable:
, which constructs the HTTP request to be sent to
Weatherstack, and , which unpacks the
answer.We’ve already written , or rather you did—great
job on that, by the way! Let’s turn to  now.
Here’s how we plan to call it as part of the 
function:At this point we’ve already made the API request, and now
 contains the response, consisting of weather data in a
specific JSON format.We’re saying here that if we pass the body of that response as a
 to , we should get back a
 struct representing the weather conditions that
were encoded in the JSON.That sounds like something we can test, so it’s over to you again to
figure out how. Write a test for 
along these lines. We have the real JSON data we saved earlier
when making our exploratory request
to Weatherstack (if not, make it again). That’ll be perfect test data
for : we already know exactly what weather
conditions it encodes, so all we need to do is check the result against
the corresponding  struct. Here’s my attempt:There’s no need to test extracting any of the other stuff in the
JSON; we don’t use it, so extracting it would be a waste of time, and
testing that extraction even more so.That was easy, I think you’ll agree, so let’s turn to the actual
extraction. How are we going to turn a  into a
?You know if you’ve read my moderately bestselling book The Secrets of Rust: Tools that we can use
the  library to serialize and deserialize Rust structs
to JSON by using the  attribute. For example:By deriving the  trait, we asked
 to autogenerate code for turning a 
struct into, effectively, a , and
, naturally enough, does the reverse.So could we do the same kind of thing here? Could we define some Rust
struct that mirrors the schema of the JSON data returned by
Weatherstack, and derive  on it?Yes, we could do it that way, but it turns out to be rather
laborious, because the API’s schema consists of 
structures. We’d have to define structs for each level of the JSON we’re
interested in. For example, at the top level, we only want
:So we have to start by defining a struct that represents the entire
response, with a single field for  (we can ignore
all the others):And, of course, we now need another struct definition to represent
what’s inside :It’s already getting annoying, and you can imagine there would be
many more of these structs if we had to deal with further levels of
nesting in the API data. The worst part is that we don’t even
 these structs! There’s no function in our program that
needs to take or return a , for example: we
already have our own struct  that contains exactly
and only the data we want.Surely this isn’t the right way to use Rust’s type system. What we’d
prefer is a way to look up the data we want directly in the JSON, and
then transfer it to our  struct, without going via a
bunch of useless paperwork.Luckily,  provides a way to do this, using a
syntax called “JSON Pointer”. First, we deserialize the data to the
all-purpose type :Assuming that we successfully get , then its contents
represent the whole JSON object contained in the response. We can reach
in and grab some specific part of it using a path-like notation:Much more direct than using a bunch of intermediate structs. Of
course, there might not  a value at that path, so it makes
sense that  returns an , doesn’t
it? (Read more about this in my tutorial on Results and Options in
Rust.)If this path didn’t exist in the JSON, the result would be
, straightforwardly. But we can be pretty confident
that the server’s response  include this data, so the
result will be , and it will contain another
 representing whatever was found at the given
path.So let’s try to write  now using this
“pointer” approach:We’ve added a little extra paperwork here in case the lookups fail,
using  to return a suitable error message along
with the problematic JSON data.Assuming the lookups succeed, though, we need to turn the resulting
s into real Rust types: is a useful little tool whenever we’re dealing
with s like this. If the option is ,
it just does nothing. But if it’s , then it extracts
the value and applies the given function to it. In this case, that’s
 for the temperature, which parses the value as a
floating-point number, and  for the summary.Having extracted, checked, and converted our 
and  values, then, we write them into our
 struct using the field init shorthand, and return
it.Taking it for a trial runThis passes the test, which is encouraging, so we now have the two magic functions we need to write
.I mean, if you cross off the parts we’ve already tested
( and ), there’s not much
left, is there? The only substantive thing we don’t test is
, and that’s not our code—we can assume
 itself works, or someone would have noticed by
now.Let’s run the updated program for real and see what happens.Error: bad response: {"error":{"code":601,"info":"Please specify a
valid location identifier using the query parameter.","type":
"missing_query"},"success":false}Oops. That’s on me; I didn’t give a location on the command line to
query the weather for. But that’s exactly the sort of mistake that any
user might make, so we’d better catch it and provide a slightly nicer
error message:Let’s try again, this time with a location:Weather { temperature: 12.0, summary: "Partly cloudy" }Fine. All our magic is working perfectly. That’s reassuring, so let’s
take this opportunity to tighten up a few bolts and caulk a few seams.
We’ll define a real implementation of , so that the
output doesn’t look so nerdy:Nothing new here except this format parameter for the
temperature:The  means “print to one decimal place”, rounding if
necessary. Without this, a temperature of 12.0 would print as just “12”,
which seems a shame. We worked to get that extra decimal place: let’s
hang on to it!Here’s what it looks like with this change:Mixing arguments and
environment variablesAnd, since we’re going to the trouble of validating our arguments,
let’s hand that over to  and derive a suitable
 parser:You might well think that if we’re deploying the awesome power of
 now, couldn’t we also use it to take the API key as a
command-line option? That would be a nice enhancement, but currently
we’re looking for it in an environment variable:Ideally, we’d have  get this from a flag if it’s
provided that way, and if not, look for it in the environment variable.
And it turns out we can do exactly that, if we opt in to the
 feature:cargo add clap -F derive,envWhat we’re saying is, if the user provides the 
flag, use that value, and if they don’t, fall back to looking for it in
the environment variable. If it’s not there either, report an error,
because this is a required flag.By the way, while it’s conventional for Rust field names to be styled
in so-called “snake case” (), it’s 
conventional for command-line arguments to be styled in so-called “kebab
case” (), so  makes this
transformation for us automatically. That’s the wonderful thing about
conventions, of course: there are so many of them.Very well, then. We’ve built a decent library crate that actually
gets weather, and it has some tests. We can be pretty confident that
both  and  are implemented
properly. Since we can’t test the way they’re glued together in
, I guess we’ll just have to hope it’s correct,
right?If you don’t think “hope” is a strategy, tune in next time to see
whether we can’t do a little better.]]></content:encoded></item><item><title>My Experience with GoFr – Building Backend Apps Faster Than Ever!</title><link>https://dev.to/kavya_turkar_62fc71116af7/my-experience-with-gofr-building-backend-apps-faster-than-ever-3mmp</link><author>KAVYA Turkar</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 27 Oct 2025 11:38:09 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[🚀 My Experience with GoFr – Building Backend Apps Faster Than Ever!As a developer, I’ve always been on the lookout for frameworks that simplify backend development without compromising performance or flexibility. Recently, I discovered GoFr — and it’s been a total game-changer! ⚡From the very first project, GoFr impressed me with how clean, minimal, and developer-friendly it is. The framework handles routing, middleware, and configuration so elegantly that I was able to get my backend up and running in minutes — not hours.💡 What I love about GoFr:🔹 Super easy setup — perfect for beginners and pros alike.🔹 Built-in support for REST APIs and database integrations.🔹 Lightweight yet powerful — focuses only on what really matters.🔹 Amazing community & documentation that makes learning fun.I recently built a small project using GoFr to test its capabilities, and the experience was surprisingly smooth — clean code structure, great error handling, and a blazing fast API response time.If you’re someone who loves Go (or even if you’re new to it), I’d highly recommend giving GoFr a try. It’s open source, growing fast, and genuinely developer-focused.✨ Pro tip: Share your experience, too! Write about how you used GoFr, what you built, or what tips you discovered — it not only helps the community but might even earn you some cool GoFr swags! 😉]]></content:encoded></item><item><title>Media Generation with Go Graphics (GG Package)</title><link>https://dev.to/beryldev/media-generation-with-go-graphics-gg-package-lj</link><author>Beryl Christine Atieno</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 27 Oct 2025 08:49:11 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[A robust API does more than just serve JSON; it can be an engine for custom content. In building my RESTful country data service with Go, I integrated a unique feature: generating and serving a summary image directly from my backend. This showcases how modern Go applications can seamlessly blend data management with on-demand media creation, demonstrating a high degree of engineering versatility.
  
  
  Use Case for Dynamic Graphics
The primary goal of my API's refresh endpoint ,POST /api/v1/countries/refresh, is to update cached country data. However, I added a visual status update that is guaranteed to be fresh and accurate—a summary PNG image showing the total country count, the top 5 countries by Estimated GDP, and the exact last refresh timestamp.A static image or a client-side chart wouldn't suffice; the image needed to be universally embeddable, such as in emails or reports, and generated only when the database was successfully updated.The process of creating the PNG image is handled entirely within the Go backend, leveraging the features of the fogleman/gg package, which provides a clean, API-like wrapper around Go's native graphics capabilities, simplifying the creation of complex visuals.
  
  
  1. The Canvas and Drawing Context Set-up
The process begins by fetching the necessary data from the database and initializing the graphics context.The gg.NewContext(ImageWidth, ImageHeight) line creates the drawing canvas, and a targeted GORM query (Order("estimated_gdp DESC").Limit(5)) efficiently retrieves only the data needed for the leader board, keeping the process fast.
  
  
  2. Canvas Construction and Styling
After initializing, the context is styled using straightforward method calls:The power of  is evident here: methods like , , and  abstract away the complexities of low-level pixel manipulation.
  
  
  3. Font Loading and Text Rendering
Handling custom fonts is crucial for professional visuals.  simplifies loading a font file and setting properties:By calling , we render dynamic information directly, including the total country count and the formatted refresh timestamp. A key part of the logic is the loop that iterates through the  slice to draw the leader board:This loop dynamically adjusts the  coordinate to ensure clean vertical spacing for the ranked list. Note the essential safety check for  pointers on fields like  and , ensuring the image generation doesn't panic even if the data is incomplete.
  
  
  4. File Persistence and Error Handling
The final output is saved to the designated path, including a check to ensure the directory structure exists:The use of  ensures the necessary  directory is present before attempting the final save, preventing a common I/O error and making the deployment more robust. The  function then handles the entire PNG encoding and writing process.To conclude, integrating on-demand media generation with data service functionality, elevates a standard REST API into a more versatile engineering solution. By coupling the image creation process to a successful database transaction and leveraging the  package, we ensure the visual asset is always current and consistent with the cached data. ]]></content:encoded></item><item><title>Boost Your Go App’s Network Performance with a TCP Connection Pool</title><link>https://dev.to/jones_charles_ad50858dbc0/boost-your-go-apps-network-performance-with-a-tcp-connection-pool-90h</link><author>Jones Charles</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 27 Oct 2025 05:34:25 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hey Go developers! Ever wondered how to make your network-heavy Go applications scream with speed? If you’re building microservices, hitting APIs, or querying databases, you’ve likely faced the pain of slow TCP connection setups. Enter the —a game-changer for reusing connections, slashing latency, and saving resources. In this article, I’ll walk you through designing and implementing a TCP connection pool in Go, complete with code, real-world tips, and lessons from the trenches. Whether you’re a Go newbie or a seasoned gopher, this guide is for you!
  
  
  Why Connection Pools? A Quick Analogy
Imagine you’re a chef in a busy restaurant kitchen. Every time you need ingredients (data), running to the storage room (external service), unlocking the door, and grabbing them is slow and wasteful. Instead, you keep a few doors open, ready to grab what you need. That’s what a TCP connection pool does—it keeps connections open and reusable, avoiding the costly TCP handshake (think: unlocking the door) and closure. In a real project, adding a connection pool cut API latency by ~30% and saved serious CPU cycles. Cool, right?: What a TCP connection pool is and why it’s awesome.: Key considerations for building a robust pool.: A practical Go implementation with health checks.: Using pools for APIs, databases, and gRPC.: How to measure the impact (spoiler: it’s big!).This article is perfect if you’re a Go developer with 1-2 years of experience, but even if you’re newer, I’ll keep things clear and approachable. Let’s get started!
  
  
  Core Concepts: What’s a TCP Connection Pool?
A TCP connection pool is like a library where you borrow and return books (connections) instead of buying new ones each time. It manages a set of pre-established TCP connections, ready for your app to use, reducing the overhead of creating and closing connections. Here’s how it works in a nutshell:: The pool opens a few connections to a server (e.g., an API or database).: Your app grabs an idle connection (or creates a new one if needed).: Send/receive data over the connection.: Give the connection back to the pool for reuse (or close it if it’s idle too long).: Skip the TCP three-way handshake (100-200ms in high-latency networks).: Reuse connections to save file descriptors and CPU.: Handle thousands of concurrent requests smoothly.In a payment API project, switching to a connection pool boosted queries per second (QPS) by 25% and cut CPU usage. Go’s lightweight goroutines and  package make it a perfect fit for this—goroutines handle concurrency, and the pool keeps connections ready.Connection pools shine in:High-concurrency HTTP clients (e.g., calling payment APIs). (MySQL, Redis, PostgreSQL). (gRPC or HTTP-based communication).Pools aren’t magic. In low-traffic apps, the overhead of maintaining a pool might outweigh benefits. Plus, idle connections can eat memory if not managed. In one project, forgetting to timeout idle connections spiked memory usage by 20%. Let’s avoid that!
  
  
  Segment 2: Design Considerations

  
  
  Designing a TCP Connection Pool: Think Like an Architect
Building a TCP connection pool is like designing a high-speed train system—fast, reliable, and ready to scale. Let’s break down the key design goals, components, and pitfalls to watch for.A great TCP connection pool should be:: Low latency, high throughput.: Detects and replaces failed connections.: Adapts to changing traffic without breaking.Think of it as a busy airport runway: it needs to handle planes (requests) quickly, stay operational in storms (failures), and scale for holiday rushes.Here’s what makes up a solid connection pool:: Opens TCP connections using  with timeouts.: Hands out idle connections or creates new ones (with a cap).: Closes connections that sit unused too long.: Tests connections to ensure they’re alive (e.g., sending a PING).: Adjusts pool size based on demand.Assigns or creates connectionsPrioritize idle, cap max connectionsCleans up unused connectionsVerifies connection usabilityUse PING or test requestsAdjusts pool size dynamicallyMonitor usage for auto-scalingTune these to balance performance and resources:: Limits total connections (e.g., 50) to avoid file descriptor exhaustion.: Keeps a few connections ready (e.g., 10) for quick grabs.: Sets connection and idle timeouts (e.g., 30s) to free resources.: Use  or channels for safe goroutine access.In one project, setting  too high crashed the app by exhausting file descriptors. Test settings under load to find the sweet spot!Here’s what I learned the hard way:: Goroutines not returning connections can exhaust the pool. : Always call  to return connections.: Too many idle connections waste memory. : Set a 30s idle timeout.: Using dead connections causes errors. : Add health checks (e.g., PING).With these in mind, let’s code a TCP connection pool in Go!
  
  
  Segment 3: Implementation in Go

  
  
  Hands-On: Building a TCP Connection Pool in Go
Time to roll up our sleeves and code! Below is a practical TCP connection pool implementation using Go’s  package. It’s simple, thread-safe, and includes health checks. Let’s break it down.:  creates a pool with a channel for idle connections.:  grabs an idle connection, checks its health, or creates a new one.:  sends the connection back or closes it if the pool’s full.:  tests connections with a PING.:  ensures goroutines don’t step on each other.In a project, skipping health checks led to timeouts from dead connections. Adding  saved the day!: Use  for connection creation:
: Add Prometheus metrics to track connection usage.: Set  and  as a starting point, then tweak based on load.
  
  
  Segment 4: Real-World Applications and Best Practices

  
  
  Real-World Wins: Using TCP Connection Pools
Let’s see how our pool powers up real applications, from APIs to databases to gRPC microservices. I’ll share code snippets and hard-earned best practices.
  
  
  1. High-Concurrency HTTP Client
: Your e-commerce app hits a payment API thousands of times per minute.: Pair  with our pool for blazing-fast requests.Cap  at 50 to respect API rate limits.Use  for request timeouts.Monitor reuse rates with Prometheus (aim for >70%).: No timeouts caused request pileups during peak traffic. A 5s timeout dropped latency from 2s to 200ms.: Optimize MySQL or Redis access in your Go app.: Use  with our pool.Set idle connections to 50% of .Use health checks to catch dead connections.Log connection events for debugging.: Too many idle connections spiked memory by 40%. A 30s timeout and lower idle limit fixed it.: Speed up gRPC communication between services.: Integrate the pool with gRPC.Combine with gRPC’s built-in reuse.Use round-robin for load balancing.Monitor latency with Prometheus.: Uneven connection use overloaded some connections. A round-robin strategy balanced the load.: Prometheus for metrics, Grafana for visualization.: Track active connections (<80% of `maxConns`), reuse rate (>70%), error rate (<1%).: Log connection creation/closure for debugging.
  
  
  Segment 5: Performance Testing, Conclusion, and Future Outlook

  
  
  Testing the Impact: Does It Really Work?
Let’s put our pool to the test! We used  to simulate 1000 concurrent requests for 30 seconds against a mock payment API on a 4-core, 8GB server (Go 1.20).: Each request opens a new TCP connection.: Uses  (, ).: 30% higher due to connection reuse.: 24% lower by skipping handshakes.: Saved CPU and memory with capped connections.:  caused queuing. : Use 50-100.: 5s idle timeout hurt reuse. : Use 30s.: Local tests were too rosy. : Simulate latency with .: Frequent health checks spiked CPU. Checking every 5s balanced reliability and performance.
  
  
  Wrapping Up: Your Path to Faster Go Apps
TCP connection pools are like turbochargers for your Go apps, cutting latency (24%), boosting QPS (30%), and saving resources. With Go’s  package and goroutines, building one is straightforward and fun. Here’s your cheat sheet:: Start with , .: Use PING to keep connections alive.: Use Prometheus/Grafana to track performance.: Simulate production conditions.In one project, skipping health checks caused outages. A simple PING check fixed it—small details, big impact!: Go 1.20’s  improvements make timeout control even better.: Pair pools with Kubernetes service discovery.: Auto-adjust pool size with machine learning.: Expect tighter gRPC and HTTP/2 integration.: Start simple, add features like health checks, and play with it! Connection pools are a great way to master Go concurrency.]]></content:encoded></item><item><title>Mastering Lock-Free Data Structures in Go: Ring Buffers, Queues, and Performance Optimization</title><link>https://dev.to/aaravjoshi/mastering-lock-free-data-structures-in-go-ring-buffers-queues-and-performance-optimization-2f2d</link><author>Aarav Joshi</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 26 Oct 2025 23:01:51 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! In concurrent programming, managing multiple threads accessing shared data presents significant challenges. Traditional locking mechanisms often introduce bottlenecks, reducing system throughput and increasing latency. I have spent years exploring alternatives that minimize contention while ensuring data integrity. Lock-free data structures offer a compelling solution by relying on atomic operations rather than mutexes. This approach can dramatically improve performance in highly concurrent environments, especially in Go, where goroutines multiply rapidly.My journey into lock-free programming began with simple counters and evolved into complex queues and buffers. The initial hurdles involved understanding memory ordering and avoiding common pitfalls like ABA problems. Through trial and error, I discovered patterns that work reliably across different hardware architectures. The code examples I share here reflect lessons learned from production systems handling millions of operations per second.Let me start with a ring buffer implementation. This structure provides a fixed-size circular queue where producers and consumers can operate concurrently without blocking each other. The key insight is using atomic operations to manage indices, ensuring thread safety without locks. I chose a power-of-two capacity to enable efficient modulo operations through bit masking, which speeds up index calculations significantly.The ring buffer uses compare-and-swap operations to update the tail and head pointers atomically. This ensures that only one goroutine can claim a slot at a time, preventing data races. The loop continues until the operation succeeds, which typically happens quickly under moderate contention. I added padding to separate the head and tail into different cache lines, reducing false sharing that can degrade performance on multi-core systems.In one project, I used this ring buffer for a real-time data processing pipeline. It handled sensor data from multiple sources without dropping packets, even during traffic spikes. The lock-free design allowed us to achieve consistent sub-microsecond latency, which was critical for our application. We monitored queue depth and adjusted producer rates dynamically to prevent overflow.Another useful structure is the multiple-producer single-consumer queue. This pattern appears frequently in scenarios like logging systems or task dispatchers, where many goroutines enqueue items, but only one consumes them. The implementation uses a linked list with atomic pointer operations to maintain consistency.The MPSC queue employs a dummy node to simplify edge cases. The sync.Pool recycles nodes, reducing garbage collection pressure. I recall an incident where memory allocations became a bottleneck in a high-throughput service. After switching to object pooling, throughput increased by 30% without any other changes. The atomic operations ensure that enqueues from multiple producers do not interfere with each other.Atomic counters represent another foundational lock-free component. When multiple goroutines need to increment a shared counter, traditional approaches suffer from cache contention. By spreading counts across multiple cache lines, we can minimize this effect.This counter assigns each goroutine to a separate slot, padded to avoid false sharing. The Read method provides an approximate sum, which suffices for many monitoring purposes. I used a similar design in a metrics collection system, where precise counts were less critical than low overhead. The reduction in lock contention allowed the system to scale linearly with the number of cores.Benchmarking these structures reveals their advantages. I often compare lock-free implementations against mutex-based alternatives to quantify the improvements. The following code demonstrates a simple performance test.In my tests, lock-free structures consistently outperform locked versions by a factor of five to ten under high contention. The ring buffer excels in scenarios with balanced producers and consumers, while the MPSC queue shines when one consumer serves many producers. These benchmarks help in selecting the right structure for specific use cases.Writing lock-free code requires attention to detail. I always validate implementations using Go's race detector and stress tests. One common mistake is neglecting memory model guarantees, leading to subtle bugs. I learned to rely on atomic operations for all shared state modifications, avoiding data races entirely.Another consideration is progress guarantees. Lock-free algorithms ensure that at least one goroutine makes progress, which prevents deadlocks. However, they can suffer from livelock or starvation if not designed carefully. I incorporate exponential backoff in spin loops to reduce CPU waste during high contention.Integration with existing systems demands additional features. For instance, adding bounded wait strategies prevents resource exhaustion. Metrics collection helps in monitoring performance and detecting issues early. I often include counters for successful operations, failures, and average latency.In a recent deployment, we combined lock-free queues with write-ahead logging for durability. This hybrid approach provided both high throughput and data persistence. The lock-free front end handled incoming requests, while a separate goroutine batched writes to disk. This design achieved millions of transactions per second with guaranteed delivery.Error handling in lock-free structures differs from traditional code. Since operations are non-blocking, failures typically indicate full buffers or empty queues. I design APIs to return clear status codes, allowing callers to retry or apply backpressure. This proactive approach maintains system stability under load.The performance benefits extend beyond raw throughput. Reduced latency improves user experience in interactive applications. Predictable memory usage simplifies capacity planning. I have seen systems become more responsive and reliable after adopting lock-free designs.Despite the advantages, lock-free programming is not a silver bullet. It increases code complexity and requires thorough testing. I recommend starting with well-understood patterns and gradually customizing them for specific needs. Code reviews and static analysis tools catch many potential issues early.Looking ahead, hardware advancements will likely make atomic operations even faster. New CPU instructions may simplify some lock-free algorithms. I continue to experiment with emerging techniques, always measuring impact before adopting changes.My experience taught me that simplicity often beats cleverness. The most effective lock-free structures are those that solve a specific problem without unnecessary generality. I prefer composable components that can be combined to build larger systems.
  
  
  In conclusion, lock-free data structures offer significant performance improvements for concurrent Go applications. They eliminate locking overhead, reduce contention, and provide better scalability. The code examples here serve as a starting point for further exploration. I encourage developers to benchmark their implementations and tailor them to their unique requirements. The journey to mastering lock-free programming is challenging but rewarding, leading to faster and more robust software.
📘 , , , and  to the channel! is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly low—some books are priced as low as —making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>Go Concurrency Mastery: Preventing Goroutine Leaks with Context, Timeout &amp; Cancellation Best Practices</title><link>https://dev.to/serifcolakel/go-concurrency-mastery-preventing-goroutine-leaks-with-context-timeout-cancellation-best-1lg0</link><author>Serif COLAKEL</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 26 Oct 2025 17:31:13 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Goroutines are Go's superpower — lightweight, highly concurrent, and capable of handling thousands of simultaneous operations with minimal overhead. They're the foundation of Go's promise for building scalable, high-performance systems.But with great power comes great responsibility.  are a silent killer in production systems.Unlike memory leaks in garbage-collected languages, leaked goroutines don't just consume memory — they hold onto file descriptors, network connections, and CPU cycles. In high-throughput production environments, even a small leak can compound into service degradation or complete outages.In this comprehensive guide, we'll master: of goroutine leak mechanics and detection context patterns for bulletproof cancellation
 using Go 1.20+ features like  from microservices, batch processing, and stream handling strategies for leak-free production systems
  
  
  🔍 Understanding Goroutine Leaks: The Silent Production Killer
A  occurs when a goroutine remains alive indefinitely, consuming system resources without performing useful work. Unlike process leaks in other languages, goroutine leaks are particularly insidious because: — Each goroutine uses ~2KB of stack space (minimum) — The Go scheduler must track and manage leaked goroutines — Leaked goroutines can hold file handles, network connections, or locks — Associated heap objects can't be garbage collected
  
  
  Common Leak Patterns in Production
Pattern 1: Unbuffered Channel DeadlockPattern 2: Missing Context PropagationPattern 3: Forgotten Background Workers
  
  
  Real-World Leak Example: Event Processing Pipeline
In production, this pattern can easily spawn thousands of leaked goroutines under high load, eventually exhausting system resources.
  
  
  � Production-Grade Leak Detection and Monitoring
Detecting goroutine leaks in production requires a multi-layered monitoring strategy. Here's how engineering teams at scale companies implement leak detection:
  
  
  1. Real-Time Monitoring with runtime/metrics (Go 1.16+)Go's built-in metrics provide zero-overhead runtime monitoring:
  
  
  2. Advanced pprof Integration for Deep AnalysisProfessional leak detection requires automated pprof integration:
  
  
  3. Continuous Integration Leak TestingPrevent leaks from reaching production with automated testing:Key Production Monitoring Metrics:Goroutine count trends and growth ratesGoroutine states distribution (running, waiting, blocked)
Stack trace pattern analysis for leak identificationResource correlation (memory, file descriptors, connections)Service performance correlation with goroutine growth
  
  
  🚀 Mastering Context: The Foundation of Leak-Free Go
The  package is Go's most powerful tool for controlling goroutine lifecycles. Understanding its advanced patterns is crucial for building production-grade concurrent systems.Why Context is Essential for Production Systems provides four critical capabilities: — Cascade shutdown signals across goroutine hierarchies — Enforce time boundaries on operations
 — Safely pass metadata without global variablesObservability integration — Enable tracing and monitoring across service boundariesGo 1.20+ Advanced Context FeaturesGo 1.20 introduced  and , enabling rich cancellation semantics:Context Propagation Best PracticesPattern 1: HTTP Request Context ChainPattern 2: Worker Pool with Graceful ShutdownPattern 3: Context Chaining and InheritanceKey Benefits of Advanced Context Patterns:Hierarchical cancellation — Child contexts automatically cancelled when parent cancels — Different timeouts for different operation stages
 — Detailed cancellation causes with  — Context values enable distributed tracing across services — Guaranteed cleanup via defer and context cancellation
  
  
  ⏰ Advanced Timeout & Deadline Strategies
Effective timeout management is crucial for preventing cascading failures in distributed systems. Go 1.21+ introduced enhanced timeout capabilities that provide better control and observability.Multi-Level Timeout ArchitectureProduction systems require sophisticated timeout strategies that handle both fast failures and retry scenarios: Multi-level timeouts prevent cascading failures, exponential backoff handles transient issues, and context cause tracking provides detailed error diagnostics.
  
  
  🏭 Enterprise-Grade Real-World Scenarios
High-Throughput Message Processing PipelineThis production example demonstrates a complete message processing system with proper resource management, error handling, and graceful shutdown:Here, StartWorker exits instantly when the context is canceled — no stuck goroutines, no leaks, even under heavy load.
  
  
  🎯 Production-Grade Goroutine Management Checklist
Context & Lifecycle Management✅ Always propagate  through your entire call stack✅  for external API calls and database operations✅ Implement  (Go 1.20+) for detailed error tracking✅  before expensive operations in long-running goroutines✅  (Go 1.16+) for graceful OS signal handlingChannel & Communication Patterns✅ Close channels from sender side and check  values when receiving✅  for decoupling producers from consumers✅ Implement proper select statements with context cancellation in all cases✅  on channel operations without timeout/cancellationMonitoring & Observability✅  with  (Go 1.16+)✅ Set up automated pprof collection at ✅  that include goroutine health metrics✅  for goroutine count growth beyond baseline thresholds✅  with / (Go 1.21+)✅ Write goroutine leak tests that verify baseline counts before/after✅  package for automated leak detection in unit tests✅ Load test with goroutine monitoring to catch leaks under realistic conditions✅ Implement circuit breakers to prevent cascade failures that cause leaks✅  with worker pools and semaphores✅ Implement graceful shutdown with proper resource cleanup sequencing✅ Use timeouts at multiple levels (connection, request, operation)✅  — assume external services will be slow/unavailable
  
  
  � Advanced Leak Detection Tool
For comprehensive leak detection in CI/CD pipelines:
  
  
  🎯 Key Takeaways for Production Excellence
Goroutine leaks are preventable with disciplined engineering practices. The patterns in this article aren't just theoretical — they're battle-tested in production systems handling millions of requests.Context is your lifeline. Master  patterns and you'll eliminate 90% of potential goroutine leaks. The remaining 10% come down to careful channel management and proper resource cleanup.Observability is non-negotiable. You can't fix what you can't measure. Implement comprehensive monitoring from day one, not as an afterthought when leaks bring down production. Goroutine leak tests should be as common as unit tests. Use tools like  and build custom detection into your CI/CD pipeline. Assume everything will fail — networks will be slow, databases will timeout, external APIs will return errors. Build your goroutine management with these assumptions and your systems will be unbreakable.The investment in proper goroutine lifecycle management pays dividends in production stability, performance predictability, and engineering team confidence. Your future on-call rotations will thank you.
  
  
  � Advanced Resources & Further Reading
Official Go Documentation:Production Tools & Libraries:✍️ Written by Şerif Çolakel.]]></content:encoded></item><item><title>Goravel v1.17 Preview: Add Process facade, simpler to call system commands</title><link>https://dev.to/goravel/goravel-v117-preview-add-process-facade-simpler-to-call-system-commands-882</link><author>Bowen</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 26 Oct 2025 08:17:10 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[facades add a new core module: Process, it's simpler to call system commands in your application. It has been merged to the master branch, thanks to the core developer @kkumargcc for the contribution.Use the  function to execute one command.Use the  function to execute multiple commands.Use the  function to execute command concurrently.Provide multiple functions to judge and operate the execution result.
// Run
result, err := facades.Process().Run("echo", "Hello, World!")

// Pipe
result, err := facades.Process().Pipe(func(pipe contracts.Pipe) {
  pipe.Command("echo", "Hello, World!")
  pipe.Command("grep", "World")
  pipe.Command("tr", "a-z", "A-Z")
}).Run()

// Pool
poolResults, err := facades.Process().Pool(func(pool contracts.Pool) {
  pool.Command("sleep", "1").As("sleep1")
  pool.Command("echo", "hello").As("echo1")
  pool.Command("echo", "world").As("echo2")
}).Run()
]]></content:encoded></item><item><title>Goravel v1.16.4 has been released</title><link>https://dev.to/goravel/goravel-v1164-has-been-released-1adp</link><author>Bowen</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 26 Oct 2025 04:27:39 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  Fix commands cannot be run concurrently
Suppose there are three commands: Test1, Test2, and Test3, they will print:
func (r *Test1) Handle(ctx console.Context) error {
  facades.Log().Info("app:test[*] start")
  facades.Log().Info("app:test[*] end")
  return nil
}
Then register them in the Schedule module and execute once per second:
func (kernel *Kernel) Schedule() []schedule.Event {
  return []schedule.Event{
    facades.Schedule().Command("app:test1").EverySecond(),
    facades.Schedule().Command("app:test2").EverySecond(),
    facades.Schedule().Command("app:test3").EverySecond(),
  }
}
Previously, the three commands were called randomly, the result is unexpected:Currently, they can be run expectantly:]]></content:encoded></item><item><title>Filestash Plugin Tutorial: Auto‑Login to S3 Without Typing Credentials</title><link>https://dev.to/migsarnavarro/writing-a-filestash-plugin-3350</link><author>Migsar Navarro</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 26 Oct 2025 03:54:08 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[ Three days ago I finally got my Filestash plugin working as intended, after a week spending my early mornings working on that. This is just the first step, I am aware of that, and the non-coding part often takes longer than the coding part. The purpose of my plugin is really simple, I wanted to be able to set a back-end account in the configuration, so I don't have to login my s3 credentials every time I want to use the app.When I discovered Filestash I really loved its simplicity, sadly, as it often happens, I am discovering it is not so simple under the hood, not a bad thing, just to state some often not told truth, product simplicity very often do not correlate to technical simplicity.The main reason to write this post is to help others in that regard, contribute to make Filestash a bit more simpler technically, or at least easier to understand.In my previous post, A very basic plugin for Filestash, I described some of the problems I faced when I got started working with Filestash source code, this post builds on top of that.I've been paying for a s3 compatible storage for a few months, and I really need it, but often I skip using it because of the lack of a UI, there are a few UIs already, but none of them adjust to my needs, so when I came to an open source project like Filestash, I thought it was perfect.Since I don't think my use case is standard, I'll give you a few more details:I want to use Filestash as a GUI to manage my s3 compatible object storage.I want to upload files, particularly photos and audio from a few different sources, particularly my cellphone (I guess it is not that common to use s3 object storage, and probably it is more common to pay for a backup service or for something like Google Photos but hold on, I'll explain).I want to use the files in my s3 storage seamlessly in web-applications, Jupyter notebooks, and all other sort of network based experiments.I don't need just a web file manager to move files between computers and the cloud.I won't be sharing the access to this storage, I'll be the only one using it.I will be sharing, some of the files I have in the storage, without ever showing Filestash UI.I've have not found a solution for what I need, and if you know about some nice solution please share it, I would love to hear about it. I need a really simple product totally aligned with my needs because any friction will make me not use it. One last non-technical but very important reason. I'll do the development myself because I enjoy coding when it has a purpose, and I don't have the budget to hire someone else to do it, nor the patience to manage it.I need to initialize an s3 backend (although it should be similar for other backends) so I can open the app and I don't have to type the credentials for the backend any time.
I want to avoid this screen by automatically login the user



From previous experiences I knew it was a good idea to take a look at the Github's repository issues, to see if other people have experienced or probably solved the same problem. I didn't find something similar to what I need, there was one open issue but it was really new and didn't have any useful information.The first step was a quick product inspection, I started by opening the dev tools in the browser and taking a look at what happened when I run the app and do all the stuff manually. It was not as linear as I describe it next but it was not hard at all. As an example of the lack of linearity, just two days ago I suddenly realized I had not explored how the demo app authentication happened.The demo app actually uses a request to  with the credentials from the home page of filestash so the cookie is created. It is a quick solution that could work but I don't really like it because of two things: the first and most important is that it exposes the credentials, something that I didn't wanted to do, the second is that is not elegant at all (of course this is just my opinion).My initial approach was to skip authorization altogether, just initialize the back-end and directly jump to the files view. It may be possible, but I realized that for that approach to work I would need to edit the core, something really want to avoid. So I decided to find a way to make it work as a plugin, even if I already know compilation was needed to configure the plugin.I didn't have any idea of what the app flow was, I had an idea of how I would architecture a similar application, but that didn't mean there were not other ways to achieve the same things.From doing things manually I noticed that the first time I was immediately redirected to , but there was no login in the routes, so I assumed it was something in the browser. Then I realized that  was taken care at the end of all the routes in a catch all block. When I filled the form and click the submit button a POST request was made to .
The user needs to send the storage credentials to the server.



It took me some time to discover the logic for the front-end, partly because it is a custom vanilla-js but react-like solution. At some point I read the discussion about keeping a React app or moving to vanilla-js, I don't like the new solution but hey, with open source is like this.The router is located in . There are three routes we care about:, it uses  handler. This is the entry point, I don't want to type a complex path before viewing the content., it uses  handler and creates a cookie. This route is used when the user submits the credentials in the  page, which apparently is just a browser view in a SPA., it uses  handler. This is the endpoint the home page hits to know if the user is logged in, or more accurately, if the storage has been initialized, and then to decide what view to show accordingly. If the user is logged in the view will be the filesystem, otherwise it will be the login page.An important thing to notice is that each one of them uses a different set of middlewares. Most of them are not really important, but as I was trying to understand how things worked I found  and  were relevant. injects the session for the request context, so even if a backend has been initialized, some session variables won't be available unless the route uses this middleware. injects the body of the request into the context. Not sure why this is necessary, but it took me a while to figure that the  absence of  was breaking my code.
  
  
  Hooks: The best place to inject my code
It was not obvious to me where to put my code. The hooks system is not documented, I already knew the possible hooks, but I didn't know what were the entry points for those, also, at the beginning I didn't  know how the server persisted the state.My initial thought was to use the  hook, since I wanted to start the backend before any request and keep it initialized, but then I found that this hook does not inject the  to their handlers, so I was not able to save the initialization once done.Then I thought about using  or , to be honest I still don't know if I choose the right thing, even if my code will impact requests I don't really feel like it is a middleware, it should be run just once at the beginning, but there is an advantage of running it as middleware, and it is that I am able to send the cookie to the front-end once I finished initializing the backend.So I ended up registering it as middleware but running it only once for the given path, this offers some flexibility but I am not totally happy with this solution.My plugin doesn't need to do anything new, and I was aware of that, it was more about discovering where things were being done, and then see how much I was able to re-use, the main purpose was to slightly modify the app flow.My biggest problem was to define what the flow should look like, with the usual flow you have a session because you have some state that is session dependent, in this case, the storage back-end is user defined, but if I want to have a fixed storage back-end I don't see the need for a session, maybe it is important in the server to keep a connection open for some time for the sake of efficiency, but from the user perspective, you don't need a session anymore. The implicit decision in this case is that the logout button that appears on the top right of the UI is not needed anymore.
The login happens in the server without user input



This is a very important topic and one that usually is difficult to understand and to implement properly. I am not an expert in authentication but after ten years working with very different code bases I've seen many different ways to achieve the same things. Often in sub-optimal ways, some times I've helped improve the code, some other times I've not been able to convince the team the current solution is not optimal or even faulty, sometimes, I've only been able to detect the flaws retrospectively, after seeing it properly solved in a different company or after working it for myself in one of my projects.Filestash offers some authorization middleware, and some authentication middleware, but I didn't manage to configure it properly. Even after disabling absolutely everything, there is the need for a session, both for the sake of efficiency and security. In the code it can be read something like  but it refers to the authorization from the point of view of the storage service provider, which could or could not be the same authorization that the app will use. As an example, for my use case, I would like to hide the storage provider authorization from the user of Filestash, which will be just me, al least initially.This storage authorization is sent to the browser, probably for the lack of another method which would be implemented as part of the authorization or authentication middleware, but while I was exploring the code I didn't find flags to disable this to happen or to make it work in a different way but I am not 100 % sure if it cannot be done or simply I missed it.The app saves everything in the App structure which is defined in  and often injected as the  parameter. And important note here is that there is a  field in the struct but it doesn't seem to be used for the session. Here are the four fields that were relevant for the plugin: saves a reference to the client used for accessing the storage. is not used directly, but it is used by  by including  as a temporary storage for the POST body. stores the credentials for connecting to the storage. stores the same hash that is included in the cookie.At the moment there was nothing to be done, but at some point in the future I would like to add a route and a page to set there the login information and some other session parameters, like deciding in which page or pages inject the cookies, or the home directory for the storage.The only part that we care about is public/assets/pages/ctrl_homepage.js, this is executed for the home page, that is the  path, and there a GET request to  is done to know if a session already exists.The plugin does not modify this, but send the cookie in the  request so when the call to  is made the cookie is already there, without ever asking for the credentials, so the files are displayed.Cookies are using to keep track of both the user and admin sessions. There is not much to say in this regard, still I think it is important to create a different section to make it visible. The functions for encrypting and decrypting are included in  and the secret used is defined in the settings section of the admin panel.This is a basic plugin and there are still plenty of room for improvements, the most important thing is that currently this only works for s3 storage, which solves my problem but is very limited compared to the wide range of storage that Filestash support. Another nice thing to have would be a plugin configuration page, but for the moment it solves a problem. The next days I'll work in getting Filestash to work with a reverse proxy (Caddy) and deployed it to production.]]></content:encoded></item></channel></rss>