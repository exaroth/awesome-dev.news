<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Go</title><link>https://www.awesome-dev.news</link><description></description><item><title>This is don&apos;t my first lerning.</title><link>https://dev.to/consolvex/this-is-dont-my-first-lerning-279c</link><author>Consolex</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 23 Feb 2025 11:59:06 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[I'am learning GOlang. 
My English is bad, but i'am also learning Eng.]]></content:encoded></item><item><title>native WebP encoding version 1.0! 🚀</title><link>https://www.reddit.com/r/golang/comments/1iw8a68/native_webp_encoding_version_10/</link><author>/u/Pretend-Ad1926</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 11:38:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I’m excited to announce nativewebp v1.0, a major milestone for our WebP encoder in Go! This version marks 1.0 because we now fully support the VP8L format, making nativewebp a complete solution for lossless WebP encoding. Alongside this, we’ve added better compression, improved Go integration, and important bug fixes.Here are some highlights of this release:Full VP8L Feature SupportThis release now fully supports all VP8L features, including LZ77, Color Caching, and transforms, ensuring more accurate and efficient encoding.Smarter Compression with Filter Selection for Predictor TransformWe now analyze block entropy and automatically select the best filter per block, leading to much better compression.nativewebp now includes a wrapper for golang.org/x/image/webp, so you can use Decode and image.Decode out of the box without extra imports.Looking forward to your thoughts and feedback on the new release!]]></content:encoded></item><item><title>Folang: Transpiler for F#-like functional languages ​​to Go</title><link>https://www.reddit.com/r/golang/comments/1iw3tz7/folang_transpiler_for_flike_functional_languages/</link><author>/u/karino2012</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 06:26:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I wrote a transpiler in Go that transpiles F#-like functional languages ​​to Go.I design the language specifications from scratch to match Go, and named it Folang.There are still many NYIs, but I have implemented it to the extent that it can be self-hosted, so I will post it on reddit.A transpiler that does not require anything other than Go to tryArgument types are inferred using F#-like syntax, and the arguments are generalized to become generic functionsThe transpiler itself is 3600 lines of Folang code and about 500 lines of Go code]]></content:encoded></item><item><title>A Practical Guide to ORM in GoFrame: From Basics to Advanced Relationships</title><link>https://dev.to/jones_charles_ad50858dbc0/a-practical-guide-to-orm-in-goframe-from-basics-to-advanced-relationships-17fk</link><author>Jones Charles</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 23 Feb 2025 05:45:07 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[If you're working with Go and looking for a powerful yet lightweight web framework, GoFrame might be just what you need. One of its standout features is the  package, which provides a robust ORM (Object-Relational Mapping) system. In this guide, I'll walk you through everything you need to know to effectively use ORM in your GoFrame projects.Setting up database connectionsWorking with transactionsHandling relationships (one-to-one, one-to-many, many-to-many)Basic knowledge of Go programmingGo installed on your machineMySQL or compatible databaseBasic understanding of ORM concepts
  
  
  Getting Started: Database Configuration
First things first, let's set up our database connection. GoFrame makes this super straightforward with a YAML configuration:💡 : The  setting is fantastic during development as it shows you the actual SQL queries being executed. Remember to disable this in production!
  
  
  Defining Your First Model
Let's start with a simple user model. In GoFrame, models are just Go structs with special tags:
  
  
  CRUD Operations Made Easy
More complex query with conditions:
  
  
  Working with Transactions
Transactions are crucial for maintaining data integrity. Here's how to use them in GoFrame:
  
  
  Advanced Feature: Relationships
Perfect for user profiles or detailed information:
  
  
  One-to-Many Relationships
Great for handling user comments or posts:
  
  
  Many-to-Many Relationships
Perfect for scenarios like course enrollment systems:
  
  
  Best Practices and Tips 💡
: Include context in your database operations for better control and cancellation capabilities.: Don't ignore error returns from database operations. for operations that modify multiple tables. appropriately based on your query patterns.: Avoid putting business logic in your model structs.
  
  
  Common Gotchas to Watch Out For ⚠️
Remember to properly close database connectionsBe careful with large result sets - use paginationWatch out for N+1 query problemsDon't forget to handle null values appropriatelyGoFrame's ORM system provides a powerful yet intuitive way to work with databases in Go. It strikes a great balance between functionality and simplicity, making it a solid choice for both small and large projects.Explore GoFrame's caching capabilitiesLook into query optimization techniquesLearn about GoFrame's migration toolsLet me know in the comments if you have any questions or if you'd like to see more GoFrame content! 🚀]]></content:encoded></item><item><title>What is your logging, monitoring &amp; observability stack for your golang app?</title><link>https://www.reddit.com/r/golang/comments/1iw07rm/what_is_your_logging_monitoring_observability/</link><author>/u/gwwsc</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 02:53:57 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[My company uses papertrail for logging, prometheus and grafana for observability and monitoring.I was not actively involved in the integration as it was done by someone else a few years ago and it works.I want to do the same thing for my side project that I am working on for learning purpose. The thing I am confused about it should I first learn the basics about otel, collector agents etc? Or should I just dive in?As a developer I get an itch if things are too abstracted away and I don't know how things are working. I want to understand the underlying concepts first before relying on abstraction.What tools are you or your company using for this?]]></content:encoded></item><item><title>Understanding Anonymous Functions in Go: A Practical Guide</title><link>https://dev.to/abstractmusa/understanding-anonymous-functions-in-go-a-practical-guide-57hd</link><author>Md Abu Musa</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 23 Feb 2025 01:11:56 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[What is an Anonymous Function?An  is a function . Instead of being declared like a traditional named function, it is defined inline and assigned to a variable or executed immediately.📌 Here,  holds an anonymous function that takes two integers and returns their sum.Use Cases of Anonymous Functions1️⃣ Passing Anonymous Functions as ArgumentsSince functions can be passed as arguments, anonymous functions are useful for higher-order functions.✅  Useful in callback functions or custom processing logic.2️⃣ Using Anonymous Functions in GoroutinesGoroutines allow concurrent execution, and anonymous functions are a great way to define short-lived concurrent tasks.✅  Running background tasks without defining a named function.3️⃣ Returning Anonymous Functions (Closures)Closures allow an anonymous function to capture variables from its surrounding scope.✅  Creating  that retain state.4️⃣ Storing Anonymous Functions in a MapYou can store anonymous functions in a map for dynamic execution.✅  Implementing  or .What is an IIFE (Immediately Invoked Function Expression)?An IIFE (Immediately Invoked Function Expression) is an anonymous function that runs immediately after being defined.✅  One-time setup logic, reducing unnecessary variable scope.Anonymous functions in Go offer flexibility and concise coding, making them a powerful tool for:Concurrent execution (Goroutines)Closures (Retaining state)One-time execution (IIFE)By understanding and implementing anonymous functions effectively, you can write , , and  Go code.]]></content:encoded></item><item><title>Por que você deve repensar o uso de Regex em validações de strings em Go</title><link>https://dev.to/renanbastos93/por-que-voce-deve-repensar-o-uso-de-regex-em-validacoes-de-strings-no-go-1cdi</link><author>renanbastos93</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 23 Feb 2025 00:05:15 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Quando falamos de validação de strings no Go, uma das soluções mais comuns é o uso de expressões regulares (regex). No entanto, dependendo do contexto, o uso de regex pode ser menos eficiente do que alternativas mais simples. Em sistemas de alta performance, como os que lidam com grandes volumes de dados ou que precisam ser executados em ambientes com recursos limitados, é importante considerar o impacto de cada escolha de implementação.Embora regex seja uma ferramenta poderosa, sua utilização indiscriminada pode resultar em impactos de performance, principalmente em Go, onde cada operação é otimizada ao máximo para garantir alta eficiência. Neste post, vamos abordar um exemplo simples de validação de strings alfanuméricas, comparando o uso de regex com uma abordagem baseada em Unicode e explicando como otimizar o uso de regex no Go para obter melhores resultados.O que acontece por trás das cortinas: Regex vs Unicode
A biblioteca  é bastante útil, mas pode ser mais lenta do que validações feitas manualmente usando funções da biblioteca unicode. Isso ocorre porque o Go precisa compilar o regex e avaliar sua expressão toda vez que é usado dentro de um método. Isso consome mais tempo de CPU e memória, especialmente em validações simples, como a checagem, se uma string contém apenas caracteres alfanuméricos.A alternativa, mais eficiente, é iterar sobre a string e validar cada caractere individualmente, utilizando funções como unicode.IsLetter e unicode.IsDigit. Isso evita a sobrecarga de compilar o regex toda vez que a função é chamada e pode ser muito mais rápido para cenários simples.
  
  
  Exemplo prático: Comparando as abordagens
Aqui está um exemplo em Go para comparar as duas abordagens: uma usando regex e outra utilizando Unicode diretamente.
  
  
  Implementação usando Unicode:

  
  
  Comparação de Performance
Vamos rodar alguns benchmarks para comparar a performance das duas abordagens.Como você pode observar nos benchmarks acima, a versão que usa Unicode é significativamente mais rápida do que a versão com regex. A versão com Unicode realiza cerca de 18 milhões de operações por segundo, enquanto a versão com regex realiza apenas cerca de 3 milhões.Quando usamos regex, estamos criando e compilando uma expressão regular toda vez que chamamos a função. O processo de compilação e execução da expressão regular envolve mais passos do que simplesmente iterar sobre os caracteres da string com funções do unicode. Isso faz com que a execução com regex consuma mais tempo de CPU e memória.Isso não significa que você deve parar de usar regex. Em casos mais complexos, onde você precisa de validações mais sofisticadas, regex pode ser a melhor escolha. No entanto, em validações simples, como a checagem de caracteres alfanuméricos, o uso de Unicode diretamente é muito mais eficiente.Dica de Performance: Compile Regex fora do método
Se você optar por usar regex, uma boa prática é compilar a expressão regular fora do método, para que ela não precise ser recompilada a cada chamada. Isso pode ajudar a reduzir o custo de performance de usar regex.Ao compilar a expressão regular uma vez e reutilizá-la, você reduz significativamente o impacto de performance.Embora regex seja uma ferramenta poderosa e útil, seu uso em validações simples pode ser ineficiente, especialmente quando a performance é uma prioridade. Em Go, alternativas como a utilização da biblioteca unicode podem oferecer ganhos significativos de performance. Se for necessário usar regex, lembre-se de compilar a expressão regular fora do método para otimizar o desempenho.Agora, repense como você está validando suas strings no seu projeto e escolha a abordagem mais eficiente para o seu caso!]]></content:encoded></item><item><title>Any open source project that shows a good example of unit test and integration test</title><link>https://www.reddit.com/r/golang/comments/1ivv5eq/any_open_source_project_that_shows_a_good_example/</link><author>/u/smartfinances</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 22:40:03 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[As the title suggests. I have been adding various unit tests to my project but I am looking for suggestions/ideas on how to go about writing integration tests.My project mostly entails reading from SQS, batching data based on some parameters and then writing the output to s3. probably, a very common pattern. Extending that the service reads from various SQS and batching is localised to one queue. So 10 queue creats 10 different outputs.I am using localstack for development. I am not looking for examples of exactly the above use case but something similar that is interaction with db/external system and then giving some output would also do.]]></content:encoded></item><item><title>Writing a file system in Go -- not FUSE, but a real FS</title><link>https://www.reddit.com/r/golang/comments/1ivuz61/writing_a_file_system_in_go_not_fuse_but_a_real_fs/</link><author>/u/Rich-Engineer2670</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 22:31:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I would say I'm crazy, but this both well established and redundant.....Assume I wanted to write my own file system (education), with Golang -- not a fuse variant, but I literally am taking a file on a block device and treating it as a disk. If this were C, OK, I'd do the following:Define a binary boot block at LBA 0Define a certain number of LBAs for boot codeDefine a certain number of LBAs for partitionsWithin each partition define the directories and free lists (FATs, clusters, etc...)Have a bunch of free LBAs.In C, I could define structs and just write them out assuming they were packed. In Go, structs aren't C structs, so I need to constantly convert structs to binaries. Sure, I could use the binary package and a lot functions, but someone must have done this in a better way, or is the "better way" "No, write your file systems in C...."I want to stay in Go, because everything else in the OS is in Go...]]></content:encoded></item><item><title>Golang SQLite admin tool</title><link>https://github.com/joelseq/sqliteadmin-go</link><author>/u/lAdddd</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 21:08:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>🚀 Go Interface Nil Pitfall: Why Your Nil Check is Failing (and How to Fix It!) 🔍</title><link>https://dev.to/mx_tech/go-interface-nil-pitfall-why-your-nil-check-is-failing-and-how-to-fix-it-2ie7</link><author>Moksh</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 19:43:27 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Have you ever encountered a situation where you check for nil, but the function still executes unexpectedly? This is a common pitfall in Go when working with interfaces and nil values. Let's break it down. 👇🔍 The Problem: A Failing Nil Check
Imagine we have an interface Notifier with a method Notify().package main

import "fmt"

// Define an interface
type Notifier interface {
    Notify()
}

// Define a struct
type Email struct {
    Address string
}

// Implement the Notify method for Email
func (e *Email) Notify() {
    fmt.Println("Sending email to:", e.Address)
}

// Function that processes Notifier entities
func ProcessNotifiers(notifiers ...Notifier) {
    for _, notifier := range notifiers {
        if notifier == nil {
            fmt.Println("Skipping nil notifier")
            continue
        }
        fmt.Println("Processing:", notifier)
        notifier.Notify()
    }
}

func main() {
    var email *Email  // Declaring a nil pointer of type *Email
    var notifier Notifier = email // Assigning it to an interface

    fmt.Println(notifier == nil)  // ❌ False! (unexpected)

    ProcessNotifiers(notifier) // Will still call Notify() and panic!
}
1️⃣ email is a nil pointer, but notifier is NOT nil!When assigning email to notifier, Go stores its type info (*Email) in the interface.This means the interface itself is not nil, even though the underlying value is nil.2️⃣ Our if notifier == nil check fails!Even though email is nil, notifier still holds a valid interface value, so notifier == nil returns false.3️⃣ Calling notifier.Notify() causes a panic!The method is called on a nil receiver, leading to a runtime error.✅ The Fix: Proper Nil Checking
Instead of checking if notifier == nil, use reflection to properly detect nil interfaces:import "reflect"

// Properly check if an interface is nil
func isNil(i interface{}) bool {
    if i == nil {
        return true
    }
    v := reflect.ValueOf(i)
    return v.Kind() == reflect.Ptr && v.IsNil()
}

func ProcessNotifiersFixed(notifiers ...Notifier) {
    for _, notifier := range notifiers {
        if isNil(notifier) {
            fmt.Println("Skipping nil notifier")
            continue
        }
        fmt.Println("Processing:", notifier)
        notifier.Notify()
    }
}
✅ Now, it properly skips the nil notifier and avoids the panic!
 ✔ Interfaces holding nil pointers are NOT nil!
 ✔ Always check for nil pointers inside interfaces properly
 ✔ Use reflection to avoid hidden nil bugsHave you run into this before? Let's discuss in the comments! 💬👇]]></content:encoded></item><item><title>How Portsicle works!</title><link>https://dev.to/amitsuthar69/how-portsicle-works-24k6</link><author>Amit Suthar</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 19:09:40 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Portsicle is a reverse tunneling service that creates a public ingress endpoint for local servers, allowing developers to showcase their locally running applications to clients, stakeholders or to test APIs without deploying.We all know that the most obvious way to allow an inbound traffic to a private network is through port forwarding. But we won't sit and configure our routers every time, we need a better solution.But if we're not forwarding the port, then how can an external packet enter our private network without getting chocked by firewalls? Here's where the concept of Reverse Tunneling comes into the picture. Reverse Tunneling is a technique used to establish a secure connection from a remote server back to a local machine. Portsicle beautifully manipulates this same technique to route traffic to local machine.The idea is to have an intermediary remote (relay) server which will act as a bridge between local machine and the internet traffic. Anyone who wants to access someone's local server, will first visit to the remote server, which will then route the request to the local machine.To achieve this, Portsicle provides a client CLI which connects to the remote server. The user can use the CLI command to initiate the tunnel. This means that the connection is now outbound (established by the local machine) and the firewall will just ignore it. This connection is then upgraded to a secure and persistent websocket tunnel.Once a tunnel is established, the relay server provides a 'Public URL' for that session. Anyone on the internet can now access that particular local server with this public URL and the URL is only valid as long as the client is running along.Steps to get the public url:note that 3000 has to be the port of the local server you wanted to expose.You'll then receive the URL:
❯ ./portsicle http -p 3000
2025/02/23 00:26:33 Connected to remote server.
2025/02/23 00:26:33 Your public url: https://portsicle.koyeb.app/a355bf62-f7c4-46e9-9d9b-1125d6343b3d
Your local server is accessible with this link!But how the traffic is "forwarded"??? Well, we simply convert the HTTP/HTTPS requests and responses into websocket messages and forward them between local client and the remote server.We handle this forwarding for two cycles:Take plain HTTP request => Convert it to a message object => send it across wire => reconstruct an HTTP request => send it to local server. Client gets a response from local server => convert it to a response object => send it across wire => construct a response.  ]]></content:encoded></item><item><title>windows firewall for local Go web app</title><link>https://www.reddit.com/r/golang/comments/1ivmbtd/windows_firewall_for_local_go_web_app/</link><author>/u/jeevanism</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 16:18:35 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Every time I start my Go web app locally on Windows, I get a firewall error (see screenshot). The Windows Firewall blocks it, and I have to manually allow access. Why does this keep happening? Is there a way to fix this permanently?NB : I am unable to attach the screenshot here :( ]]></content:encoded></item><item><title>DSBG, an open-source static site generator built with Go</title><link>https://www.reddit.com/r/golang/comments/1ivl3o0/dsbg_an_opensource_static_site_generator_built/</link><author>/u/tarjano</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 15:24:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[This is my first big project built in Go, primarily to see if I could be as productive with it as I am with Python. I wanted to tackle a non-trivial project, so I aimed to include most of the functionality I envisioned for this type of tool. Here's what DSBG offers: Works with both Markdown and HTML source files.Automatic Tagging & Filtering: Tags are generated from paths, with built-in tag filtering.Client-Side Fuzzy Search: Provides fast search over all content within the browser.Automatic RSS Feed Generation: Easily create RSS feeds for your blog.Watch Mode with Auto-Rebuild: For continuous feedback during development. Includes 3 different themes, with the option to add your own custom CSS. For major social networks. Easily add analytics, comments, and more.The code might not be perfectly idiomatic, so any tips, suggestions, and feedback are very welcome!]]></content:encoded></item><item><title>Understanding the init Function in Go: Purpose, Execution, and Best Practices</title><link>https://dev.to/abstractmusa/understanding-the-init-function-in-go-purpose-execution-and-best-practices-2i9k</link><author>Md Abu Musa</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 12:33:36 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In Go, the  is a special function that is automatically executed  when a package is initialized. It is primarily used for , such as initializing global variables, opening database connections, or registering dependencies.Key Characteristics of  Function:
No Arguments & No Return Value – The  function does not take parameters or return values. – It runs before  and does not require explicit invocation.Can Have Multiple  Functions – A package can have multiple  functions, even across different files.Executed in Declaration Order – If multiple  functions exist in a package, they are executed in the order in which they appear.Example 1: Basic  UsageInitializing...
Main function running...
✅ The  function runs .Example 2: Using  for Global Variable InitializationExample 3:  in Multiple Files📌 If a package has multiple files, all  functions run , in the order they appear.Output (execution order is preserved):Init from a.go
Init from b.go
Main function running...
Example 4:  in a Different PackageOutput (Package-Level Execution Order)Initializing utils package...
Initializing main package...
Main function running...
Hello from utils!
✅ The  function in imported packages runs before the  function in .Initializing global variables.Setting up logging configurations.Registering dependencies (e.g., database connections).Ensuring required setup before  runs.Complex logic (prefer explicit initialization in ).Business logic (should be in  or other functions). runs automatically before .It has  and .Each package can have multiple  functions. in  runs before  in .]]></content:encoded></item><item><title>API Application Monitoring - OpenTelemetry? Or something else?</title><link>https://www.reddit.com/r/golang/comments/1ivhm7y/api_application_monitoring_opentelemetry_or/</link><author>/u/_nullptr_</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 12:25:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am writing a few different gRPC and HTTP (via gRPC Gateway) API servers for various heavy financial compute/IO operations (trading systems and market data). I am doing this as a single developer. These are mostly for me as a hobbyist, but may become commercial/cloud provided at some point with a nice polished UI frontend.Given the nature of the applications, I want to know what is "going on" and be able to troubleshoot performance bottlenecks as they arise, see how long transactions take, etc. I want to standardize the support for this into my apiserver package so all my apps can leverage and it isn't an afterthought. That said, I don't want some huge overhead either, but just want to know the performance of my app when I want to (and not when I don't). I do think I want to instrument with logs, trace and metrics after thinking what each would give me in value.Right now I am leaning towards just going full OpenTelemetry knowing that it is early and might not be fully mature, but that it likely will over time. I am thinking I will use stdlib  for logs with Otel handler only when needed else default to basic stdout handler. Do I want to use otel metrics/tracing directly? I am also thinking I want these others sent to a  handler by default (even stdout is too much noise), and only to a collector when configured at runtime. Is that possible with the Go Otel packages? Does this seem like the best strategy? How does stdlib  play into this? or doesn't it? Other ideas?]]></content:encoded></item><item><title>Ensuring Interface Implementation at Compile Time in Go 🛠️</title><link>https://dev.to/abgeo/ensuring-interface-implementation-at-compile-time-in-go-3366</link><author>Temuri Takalandze</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 10:43:52 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In Go, we can’t implicitly implement interfaces like in some other languages. To ensure a type implements an interface, we explicitly check this by trying to cast the type to the interface. If the type doesn’t match, Go will give a compile-time error, and the code won’t compile 🚫This compile-time check helps catch errors early, making sure your types conform to the expected interfaces. If Task doesn’t implement Executor properly, Go won’t compile the code, saving you from potential issues at runtime 👨‍💻]]></content:encoded></item><item><title>Dockerize GO environment with only go.mod and go.sum and no source code</title><link>https://www.reddit.com/r/golang/comments/1ivfxtb/dockerize_go_environment_with_only_gomod_and/</link><author>/u/muthunatsharma</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 10:32:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I need a docker container which has go packages downloaded, installed and compiled as mentioned in go.mod and go.sum. All the articles show how to do it but the install/compile actually happens only when the source-code is copied in to the container and "go build" is run in the dockerfile.I see "go download" downloads all pkgs in go.mod to /go/mod/pkg. How do I install these? I can give "go install <pkg>" but that would mean I need to update my Dockerfile each time a new pkg is added to go.mod.What is the one-shot way of installing it in the dockerfile build?Edit: The context is to build a dev container where deps are pre-built saving time when code is mounted on the container and built -- this is the main point to save time. The container wouldn't have the app itself. Only the dependencies fully installed and serve as a standard environment to run.]]></content:encoded></item><item><title>godoc.nvim - Golang docs inside Neovim!</title><link>https://www.reddit.com/r/golang/comments/1ivfv16/godocnvim_golang_docs_inside_neovim/</link><author>/u/ffredrikk</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 10:27:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Protobuf Should Dominate the Data Format Ecosystem</title><link>https://dev.to/leapcell/why-protobuf-should-dominate-the-data-format-ecosystem-4ddd</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 09:42:07 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Protobuf (Google Protocol Buffers), as defined in the official documentation: Protocol buffers is a language-independent, platform-independent, and extensible method for serializing structured data, which can be widely applied in scenarios such as data communication protocols and data storage. It is a tool library provided by Google with an efficient protocol data exchange format, possessing the characteristics of flexible, efficient, and automated structured data serialization mechanisms.Compared with XML, the size of data encoded by Protobuf is smaller, and the encoding and decoding speed is faster. Compared with Json, Protobuf performs more excellently in conversion efficiency, with both its time efficiency and space efficiency reaching 3 to 5 times that of JSON.As the official description states: “Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.”
  
  
  Comparison of Data Formats
Suppose we have a  object, represented by JSON, XML, and Protobuf respectively, and let's see their differences.John24Protobuf directly represents data in binary format, which is not as intuitive as XML and JSON formats. For example:[10 6 69 108 108 122 111 116 16 24]

  
  
  Good Performance/High Efficiency
: The overhead of XML formatting (serialization) is acceptable, but the overhead of XML parsing (deserialization) is relatively large. Protobuf has optimized this aspect and can significantly reduce the time overhead of serialization and deserialization.: Protobuf also greatly reduces the space occupation.
  
  
  Code Generation Mechanism
For example, write the following content similar to a structure:Protobuf can automatically generate the corresponding  file and  file, and encapsulate the operations on the structure  into a class.
  
  
  Support for Backward Compatibility and Forward Compatibility
When the client and the server use a protocol simultaneously, if the client adds a byte in the protocol, it will not affect the normal use of the client.
  
  
  Support for Multiple Programming Languages
In the source code officially released by Google, it includes support for multiple programming languages, such as:
  
  
  Disadvantages of Protobuf

  
  
  Poor Readability Due to Binary Format
To improve performance, Protobuf uses a binary format for encoding, which makes the data less readable and will affect the efficiency during the development and testing phase. However, under normal circumstances, Protobuf performs very reliably, and serious problems generally do not occur.Generally, XML is self-descriptive, while the Protobuf format is not. It is a piece of binary format protocol content, and it is difficult to know its function without matching it with a pre-written structure.Although Protobuf supports serialization and deserialization in multiple languages, it is not a universal transmission standard across platforms and languages. In scenarios of multi-platform message passing, its compatibility with other projects is not good, and corresponding adaptation and transformation work is often required. Compared with json and XML, its universality is slightly insufficient.Proto message type files generally end with . In a  file, one or more message types can be defined.The following is an example of defining a message type for a search query. The  at the beginning of the file is used to describe the version information. Currently, there are two versions of proto, proto2 and proto3.Explicitly set the syntax format to proto3. If the  is not set, it defaults to proto2.  represents the content to be queried,  represents the page number of the query, and  represents the number of items per page.  must be located on the first line of the  file excluding comments and blank lines.The following message contains 3 fields (, , ), and each field has a corresponding type, field name, and field number. The field type can be , , , or a composite type.Each field in the message type needs to be defined with a unique number, and this number is used to identify the field in the binary data. Numbers in the range of [1,15] can be encoded and represented with one byte; in the range of [16,2047], they need to be encoded and represented with two bytes. Therefore, leaving the numbers within 15 for frequently occurring fields can save space. The minimum value of the number is 1, and the maximum value is 2^29 - 1 = 536870911. Numbers in the range of [19000, 19999] cannot be used because these numbers are used internally by the proto compiler. Similarly, other pre-reserved numbers cannot be used either.Each field can be modified by  or . In the proto3 syntax, if the modification type is not specified, the default value is .: It means that the modified field appears at most once, that is, it appears 0 or 1 time.: It means that the modified field can appear any number of times, including 0 times. In the proto3 syntax, fields modified by  use the  encoding by default.You can add comments to the  file. The comment syntax is the same as the C/C++ style, using  or .When deleting or commenting out a field in a , other developers in the future may reuse the previous field number when updating the  definition. If they accidentally load the old version of the  file, it may lead to serious problems, such as data corruption. To avoid such problems, you can specify the reserved field numbers and field names. If someone uses these field numbers in the future, an error will be generated when compiling the proto, thus reminding that there is a problem with the proto.Note: Do not mix the use of field names and field numbers for the same field.
  
  
  Mapping between Field Types and Language Types
The defined  file can generate Go language code through a generator. For example, the Go file generated from the  file is the  file.The mapping between basic types in proto and Go language types is shown in the following table (here only the type mapping between Go and C/C++ is listed, and for other languages, refer to https://developers.google.com/protocol-buffers/docs/proto3):
|.proto Type | Go Type | C++ Type |
| double | float64 | double |
| float | float32 | float |
| int32 | int32 | int32 |
| int64 | int64 | int64 |
| uint32 | uint32 | uint32 |
| uint64 | uint64 | uint64 |
| sint32 | int32 | int32 |
| sint64 | int64 | int64 |
| fixed32 | uint32 | uint32 |
| fixed64 | uint64 | uint64 |
| sfixed32 | int32 | int32 |
| sfixed64 | int64 | int64 |
| bool | bool | bool |
| string | string | string |
| bytes | []byte | string |When defining a message, if you want the value of a field to be only one of the expected values, you can use the enum type.For example, now add the  field to , and its value can only be one of , , , , , , and . This can be achieved by adding an enum to the message definition and adding a constant for each possible enum value.The first constant of the  enum must be mapped to 0, and all enum definitions need to include a constant mapped to 0, and this value is the first line content of the enum definition. This is because 0 is used as the default value of the enum. In the proto2 syntax, the enum value on the first line is always the default value. For the sake of compatibility, the value 0 must be the first line of the definition.Other  files can be imported in a  file, so as to use the message types defined in the imported file.By default, only the message types defined in the directly imported  file can be used. But sometimes it may be necessary to move the  file to a new location. At this time, a virtual  file can be placed in the old location, and the  syntax can be used to forward all imports to the new location, instead of directly moving the  file and updating all call points at once. Any place that imports a proto file containing the  statement can pass on the public dependencies of the imported dependencies.For example, there are  and  files in the current folder, and  is imported in the  file, that is, the  file has the following content:Suppose now we want to put the messages in  into the  file for use in other places. We can modify  and import  in it. Note that we need to use  because a single  can only use the messages defined in  and cannot use the message types in the proto file imported in .When using  for compilation, the option  or  needs to be used to notify  where to find the imported files. If the search path is not specified,  will look for it in the current directory (the path where  is called).Message types in the proto2 version can be imported into a proto3 file for use, and message types in the proto3 version can also be imported into a proto2 file. But the enum types in proto2 cannot be directly applied to the proto3 syntax.Message types can be defined inside another message type, that is, nested definitions. For example, the  type is defined inside , and it supports multiple levels of nesting.When an outer message type uses a message inside another message, such as the  type using , it can use .Unknown fields are fields that the proto compiler cannot recognize. For example, when an old binary file parses the data sent by a new binary file with new fields, these new fields will become unknown fields in the old binary file. In the initial version of proto3, unknown fields were discarded when the message was parsed, but in version 3.5, the retention of unknown fields was reintroduced. Unknown fields are retained during parsing and are included in the serialized output.The key to the high efficiency of Protobuf lies in its TLV (tag-length-value) encoding format. Each field has a unique  value as an identifier,  represents the length of the  data (for a  with a fixed length, there is no ), and  is the content of the data itself.For the  value, it is composed of two parts:  and .  is the number given to each field in the  earlier, and  represents the type (fixed length or variable length). The  currently has 6 values from 0 to 5, and these 6 values can be represented by 3 bits.The values of  are shown in the following table, where 3 and 4 have been deprecated, and we only need to pay attention to the remaining 4 types. For data encoded with Varint, there is no need to store the byte length , and at this time, the TLV encoding format degenerates into TV encoding. For 64-bit and 32-bit data, there is also no need for  because the  value already indicates whether the length is 8 bytes or 4 bytes.int32 int64 uint32 uint64 bool enumstring bytes packed repeated fields embedded
  
  
  Varint Encoding Principle
Varint is a variable-length int, which is a variable-length encoding method. It can make smaller numbers use fewer bytes to represent, and achieve data compression by reducing the number of bytes used to represent numbers. For an int32 type number, it usually requires 4 bytes to represent, but with Varint encoding, an int32 type number less than 128 can be represented with 1 byte. For larger numbers, it may require 5 bytes to represent, but in most messages, very large numbers usually do not appear, so using Varint encoding can use fewer bytes to represent numbers.Varint is a variable-length encoding, and it distinguishes each field through the highest bit of each byte. If the highest bit of a byte is 1, it means that the subsequent byte is also part of the number; if it is 0, it means that this is the last byte, and the remaining 7 bits are all used to represent the number. Although each byte will waste 1 bit of space (that is, 1/8 = 12.5% waste), if there are many numbers that do not need to be fixed as 4 bytes for representation, a large amount of space can still be saved.For example, for an int32 type number 65, its Varint encoding process is as follows, and the 65 that originally occupied 4 bytes only occupies 1 byte after encoding.For an int32 type number 128, it occupies 2 bytes after encoding.Varint decoding is the reverse process of encoding, which is relatively simple, and no example is given here.numbers to unsigned numbers, and then use Varint encoding to reduce the number of bytes after encoding.Zigzag uses unsigned numbers to represent signed numbers, enabling numbers with smaller absolute values to be represented with fewer bytes. Before understanding Zigzag encoding, let's first understand a few concepts:: The highest bit is the sign bit, and the remaining bits represent the absolute value.: Except for the sign bit, invert the remaining bits of the original code one by one.: For positive numbers, the two's complement is itself; for negative numbers, except for the sign bit, invert the remaining bits of the original code one by one and then add 1.Take the int32 type number -2 as an example, and its encoding process is as follows.In summary, for negative numbers, perform arithmetic operations on their two's complement. For a number , if it is of the  type, perform the operation ; if it is of the  type, perform the operation . Through this operation, the negative number is changed to a positive number, and this process is Zigzag encoding. Finally, use Varint encoding.Since Varint and Zigzag encoding can self-parse the content length, the length item can be omitted, and the TLV storage is simplified to TV storage, without the need for the  item.
  
  
  Calculation Methods of tag and value Values
The  stores the identification information and data type information of the field, that is,  (field data type) +  (identification number). The field number can be obtained through the , corresponding to the defined message field. The calculation formula is tag = field_number<<3 | wire_type, and then perform Varint encoding on it.The  is the value of the message field after Varint and Zigzag encoding.
  
  
  string Encoding (continued)
When the field type is the  type, the field value is encoded in UTF-8. For example, there is the following message definition:In the Go language, the sample code for encoding this message is as follows:The binary content after encoding is as follows:[10 14 67 104 105 110 97 228 184 173 144 155 189 228 120 186]
Nested messages mean that the  is another field message. The outer message is stored using TLV storage, and its  is also a TLV storage structure. The schematic diagram of the entire encoding structure is as follows (it can be imagined as a tree structure, where the outer message is the root node, and the nested message inside it is used as a child node, and each node follows the TLV encoding rule):The outermost message has its corresponding ,  (if any), and .When the  is a nested message, this nested message has its own independent ,  (if any), and .By analogy, if there are nested messages within the nested message, continue to encode according to the TLV rule.
  
  
  repeated Fields with packed
The fields modified by  can be with  or without it. For multiple field values of the same  field, their  values are all the same, that is, the data type and field sequence number are the same. If multiple  storages are used, there will be redundancy of the .If  is set, the storage method of the  field will be optimized. That is, the same  is only stored once, and then the total length  of all values under the  field is added to form a  storage structure. This method can effectively compress the length of the serialized data and save transmission overhead. For example:In the above example, the  field does not use , and each  value will have independent  and  storage; while the  field uses , and the  will only be stored once, followed by the total length  of all  values, and then all  values are arranged in sequence. In this way, when the data volume is large, the  field using  can significantly reduce the space occupied by the data and the bandwidth consumption during transmission. With its efficiency (in terms of size) and professionalism (professional types), Protobuf should have a higher coverage in the future data transmission field.Finally, I would like to introduce to you the most suitable platform for deploying services: 
  
  
  1. Multi-Language Support
Develop with JavaScript, Python, Go, or Rust.

  
  
  2. Deploy unlimited projects for free
pay only for usage — no requests, no charges.
  
  
  3. Unbeatable Cost Efficiency
Pay-as-you-go with no idle charges.
Example: $25 supports 6.94M requests at a 60ms average response time.

  
  
  4. Streamlined Developer Experience
Intuitive UI for effortless setup.
Fully automated CI/CD pipelines and GitOps integration.
Real-time metrics and logging for actionable insights.

  
  
  5. Effortless Scalability and High Performance
Auto-scaling to handle high concurrency with ease.
Zero operational overhead — just focus on building.
]]></content:encoded></item><item><title>Feature Flag Service: Experimenting with New Technologies and Architectures</title><link>https://dev.to/palma99/feature-flag-service-experimenting-with-new-technologies-and-architectures-176p</link><author>Palma99</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 09:36:54 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[I am a junior frontend developer with two years of experience working with Vue.js, I wanted to broaden my knowledge by exploring backend development and experimenting with other frontend frameworks. I decided to create a project from scratch using Go for the backend and Angular 19 for the frontend. The project is a service for managing feature flags, inspired by great existing solutions. My primary focus was on implementing the principles of Clean Architecture while also improving my SQL skills by working directly with PostgreSQL without an ORM.The source code can be found here:The idea was to create a dashboard where users can sign in and manage their projects, environments and flags. Then, project's specific flags can be retrieved by user's app using a public api. The first thing that i wasn't sure about is how to implement this public interaction in a secure way, we will dive into that later, but the idea was to create some sort of library that handle this communication using a public key.For the dashboard, I aimed to create an intuitive and simple UI for basic operations like creating new projects and flags. My main goal was to clearly display the status of the flags in each environment. This was the initial sketch of the UI.I also wanted to implement collaboration, allowing multiple users to access the same project. This necessitates the implementation of a role/permission system, where, for example, only the owner can delete a project.
  
  
  First step: Designing the database
Let's dive into some technical details, starting with database design. I wanted to use a relational database, but at this stage, I didn’t really care which one to choose. So, as a first step, I started thinking about all the entities and the relationships between them:: a person that can sign up and have access to the dashboard: represent a container of environment and flags: each project can have one or more environments, this is useful for a real world project where multiple test environment can exist.: represent a single feature that can be enabled or notI started creating many-to-many relationship between users and project, so we can easily implement collaboration as mentioned before.
The one-to-many relationship between  and  it's pretty straight forward as one project can have multiple environments.Nothing special so far, but things started to get tricky when I encountered the flag table. The first idea that came to mind was that each flag should be directly related to an environment. This seemed logical, given that we’re working in a multi-environment system where the end user needs to request flags for a specific environment. However, I quickly realized that this could introduce some introduce some complexities.Imagine you're inside the dashboard for a specific project that has 3 environments (TEST, QA, PROD), and you want to create a new flag called 'dark_mode_experimental'. This means you would need to create 3 new rows in the  table, one for each environment. Then, if you want to update the flag name to 'dark_mode', you would need to update all the rows accordingly. The same applies if you want to delete a flag. Furthermore, if a project already has some flags and you want to add another environment (let's say 'DEV'), you would need to duplicate all the existing flags for this new environment, which can lead to increased complexity and potential maintenance issues.The final solution is that flags belong to a project while maintaining a many-to-many relationship with environments through the  table. This table allows us to store the flag status for each specific environment. By doing so, we can easily manage flag updates and deletions without needing to duplicate or update rows across multiple environments.What happens when a new environment is created within a project is quite simple: we just add a new row in the  table. The relations in the  table are not created immediately but only after a flag is updated. This ensures that we avoid unnecessary entries for environments where no flags have been modified yet. Once a flag is updated, the corresponding relation in the flag_environment table is established, allowing us to track the flag's status in the new environment. 
  
  
  Step two: Implementing Go backend
As I mentioned, I'm more of a frontend person, but I have a strong interest in learning backend technologies. During my studies, I worked with several different languages, mostly C, Java, and Python. These are great languages, but I wanted to try something new. Recently, I heard a lot of positive things about Go, so I decided to give it a shot.There are a couple of things I want to mention before diving into the code. My main goal here was to structure the code by following clean architecture principles. This approach helps ensure that the code remains modular, maintainable, and testable. Additionally, for handling data, I chose not to use any ORM because I believe that for a study project, writing raw SQL is more instructive and provides a deeper understanding of how data is managed at the database level.Let's start with folder structureIn Go, it's common to have a folder called  that contains the entry point of the program, and a folder called  for all the application code. That's what I did — I created two subfolders inside : one for the API version and one for the CLI version of the service.There are some great articles about clean architecture, and the structure I implemented is quite standard, so I won't go into detail about what each folder represents. I just want to highlight some parts of the code that demonstrate how the principles are applied and how the different components interact with each other in the project.As I mentioned, I'm new to the Go world and still getting familiar with the ecosystem. From what I've seen so far, there aren't any major frameworks like those in the .NET or Java ecosystems that handle dependency injection in such a clean way. Therefore, I decided to implement dependency injection in a very 'vanilla' way, without relying on any external frameworks. Here is an exampleAn interactor is a struct with some dependencies that has methods to fulfill some business use cases, e.g.There are several aspects that can be improved, such as error handling or using a factory to create entities. However, the key idea is that the 'Create Environment' use case is managed within this method, which does not rely on any concrete implementation.In this project, I implemented anemic entities, which is not ideal. For example, all fields are public so they can be serialized using Go's standard library, although I'm not fully convinced this is the best approach.The infrastructure folder contains all the implementations related to external dependencies. In this case, it includes the repository implementation for PostgreSQL and the middleware used by the HTTP framework.I decided to use PostgreSQL as the database for no particular reason (well, maybe because I already had a Docker image pulled). However, thanks to clean architecture, it's easy to swap the database by simply implementing a new repository that adheres to the same interface. For example:In this case, interfaces refer to the components that allow external systems to interact with the application. For a web API, this typically consists of a function that, for example, extracts request parameters or body data, creates a DTO, and calls the interactor.
  
  
  Authentication for the admin section
When dealing with authentication in the real world, it's better to relay on well tested library to improve security. However this for this simple project i decided to implement a username and password authentication that works with JWT tokens from scratch, using this popular library 'github.com/golang-jwt/jwt/v5'.
  
  
  Implementation of the "public api"
To allow the end user to access flags for a certain environment, I decided to implement a public key pattern. Essentially, there is a public REST API protected by a middleware that checks for a specific header containing a key. If the key is valid, it grants read-only access to all the enabled flags for that environment. Even if the public key gets stolen, the impact is limited. Since it only grants read-only, the key cannot be used to modify any data or access sensitive information.That's how i implemented the middlewareThen in the route definitionTo interact with the public API I wrote a simple typescript SDK that allow to easily communicate with the service. It provides basic caching and types, and can be used by any javascript app.
  
  
  Admin dashboard in Angular 19
Once I completed the backend, I wanted to interact with my service through a comfortable UI. Angular 19 had just come out with stable signals and other cool features, so I decided to use it for the frontend. I didn’t want to spend too much time designing UI components, hence I decided to use a component library that provides pre-built, customizable components. This allowed me to focus more on the core functionality and user experience. The library i choose is Taiga UI.The frontend is quite simple, a bit different from the initial sketch but it provides all the features i need.List of user projectsAn interesting feature I added consists of a section where you can test the payload of the public api for the environment. It works by making an api call to public endpoint using the environment key of the selected environment, and then it shows the response.Unit testing: Clean architecture is great and makes things easier to test, but I haven’t written a single test for my business logic yet. This is definitely something to focus on moving forward.Collaboration is supported, but there is currently no way to "invite" someone to join a project (just manually on db). In the future, it would be cool to implement an invitation system, allowing users to easily add collaborators and manage team access.]]></content:encoded></item><item><title>Generic Bitfield I had fun implementing</title><link>https://gist.github.com/oplanre/de0bba4f1e2f769458ca1adff7f12280</link><author>/u/ln3ar</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 04:12:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>from nodejs want to move to golang</title><link>https://www.reddit.com/r/golang/comments/1iv7ngg/from_nodejs_want_to_move_to_golang/</link><author>/u/Spirited-Item1431</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 01:53:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I used to be a web developer who used Node.js as my daily programming language, but now I'm interested in switching to Golang. Aside from the usual fundamentals, what are the most important things to learn in Golang?]]></content:encoded></item><item><title>Understanding Packages in Go: A Comprehensive Guide</title><link>https://dev.to/abstractmusa/fsdfasdf-asfa-3fd1</link><author>Md Abu Musa</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 01:42:36 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In Go, a package is a fundamental concept for organizing and reusing code. This guide explains everything you need to know about Go packages.A package is a collection of source files in the same directory.All files in a package must declare the same package name at the top.It provides modularity, encapsulation, and code reuse.A special package that creates an executable program.Must contain a  function.Used only for executables.Can have any name except .Used to create reusable code.Can be imported by other packages.
  
  
  3. Package Visibility Rules
Names starting with an  letter are .Names starting with a  letter are .To use packages in Go, you import them:
  
  
  5. Package Organization Example
myapp/
├── main.go              // package main
├── utils/
│   ├── math.go         // package utils
│   └── strings.go      // package utils
└── models/
    └── user.go         // package models

  
  
  6. Benefits of Using Packages
All files in the same folder must have the same package name.Package names usually match the directory name.Standard library packages like , , etc., come with Go installation.You can create custom packages for better code structure.Use  to initialize a new module (which can contain multiple packages).By following these best practices, you can effectively manage code in Go using packages.]]></content:encoded></item><item><title>Comments on Executive Order 14168</title><link>https://aphyr.com/posts/380-comments-on-executive-order-14168</link><author>Aphyr</author><category>dev</category><category>go</category><pubDate>Fri, 21 Feb 2025 23:04:55 +0000</pubDate><source url="http://aphyr.com/posts.atom">Aphyr</source><content:encoded><![CDATA[Executive order 14168 is biologically incoherent and socially cruel. All passport applicants should be allowed to select whatever gender markers they feel best fit, including M, F, or X.In humans, neither sex nor gender is binary at any level. There are several possible arrangements of sex chromosomes: X, XX, XY, XXY, XYY, XXX, tetrasomies, pentasomies, etc. A single person can contain a mosaic of cells with different genetics: some XX, some XYY. Chromosomes may not align with genitalia: people with XY chromosomes may have a vulva and internal testes. People with XY chromosomes and a small penis may be surgically and socially reassigned female at birth—and never told what happened. None of these biological dimensions necessarily align with one’s internal concept of gender, or one’s social presentation.The executive order has no idea how biology works. It defines “female” as “a person belonging, at conception, to the sex that produces the large reproductive cell”. Zygotes do not produce reproductive cells at all: under this order none  of us have a sex. Oogenesis doesn’t start until over a month into embryo development. Even if people were karyotyping their zygotes immediately after conception so they could tell what “legal” sex they were going to be, they could be wrong: which gametes we produce depends on the formation of the genital ridge.All this is to say that if people fill out these forms using this definition of sex, they’re guessing at a question which is both impossible to answer and socially irrelevant. You might be one of the roughly two percent of humans born with an uncommon sexual development and not even know it. Moreover, the proposed change fundamentally asks the wrong question: gender markers on passports are used by border control agents, and are expected to align with how those agents read the passport holder’s gender. A mismatch will create needless intimidation and hardship for travelers.Of course most of us will not have our identities challenged under this order. That animus is reserved for trans people, for gender-non-conforming people, for anyone whose genetics, body, dress, voice, or mannerisms don’t quite fit the mold. Those are the people who will suffer under this order. That cruelty should be resisted.]]></content:encoded></item><item><title>Tk9.0 canvas demo</title><link>https://opu.peklo.biz/p/25/02/21/1740170028-43ac6.png</link><author>/u/0xjnml</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 20:38:31 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Installing Golang on Windows WSL/WSL2</title><link>https://dev.to/sonishivam10/installing-golang-on-windows-wslwsl2-3pn5</link><author>ShivamS</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 20:23:31 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Official Go documentation might not provide the steps to install Go on WSL/WSL2. If you want to install GoLang and set up the development environment, follow these steps:Open your terminal and use the command to download the specific version.
Note: Replace go1.24.0.linux-amd64.tar.gz with the latest version.wget https://dl.google.com/go/go1.24.0.linux-amd64.tar.gzUnzip:sudo tar -xvf go1.24.0.linux-amd64.tar.gzMove to the correct path.echo "export GOROOT=/usr/local/go" >> ~/.bashrc
echo "export GOPATH=\$HOME/go" >> ~/.bashrc
echo "export PATH=\$GOPATH/bin:\$GOROOT/bin:\$PATH" >> ~/.bashrc
Refresh the terminal:Verify the Go version.]]></content:encoded></item><item><title>Learn how to set up and deploy apps to your own VPS or bare metal using Sidekick. Sidekick is an alternative to Kamal and Coolify, with over 6.5k GitHub stars</title><link>https://dev.to/pmbanugo/learn-how-to-set-up-and-deploy-apps-to-your-own-vps-or-bare-metal-using-sidekick-sidekick-is-an-42od</link><author>Peter Mbanugo</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 19:31:30 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Self-hosting on bare metal and Cloud VM - Deploy like a Pro with Sidekick]]></content:encoded></item><item><title>Self-hosting on bare metal and Cloud VM - Deploy like a Pro with Sidekick</title><link>https://dev.to/pmbanugo/self-hosting-on-bare-metal-and-cloud-vm-deploy-like-a-pro-with-sidekick-2b27</link><author>Peter Mbanugo</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 19:21:08 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>list of decimal packages: fixed and big</title><link>https://www.reddit.com/r/golang/comments/1iuunf1/list_of_decimal_packages_fixed_and_big/</link><author>/u/kardianos</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 16:16:29 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I was updating a list of decimal packages. I thought I would share.There are generally 2 varieties: fixed sized and arbitrary precision. The udecimal is interesting as it uses a fixed size for 128 bit precision with zero allocations, then uses an allocating "*big.Int" version for anything larger then that.I currently use "cockroachdb/apd", which is a great package for frameworks or databases, but, it's a bit awkward to hold and lacks good formating. Realistically, I just need a fixed size decimal for my needs (financial/clinical). When I get a chance, I'll probably swap in for one of the fixed size packages.]]></content:encoded></item><item><title>Talk me out of using Mongo</title><link>https://www.reddit.com/r/golang/comments/1iutb24/talk_me_out_of_using_mongo/</link><author>/u/grdevops</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 15:19:32 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Talk me out of using Mongo for a project I'm starting and intend to make a publicly available service. I  love how native Mongo feels for golang, specifically structs. I have a fair amount of utils written for it and it's basically at a copy and paste stage when I'm adding it to different structs and different types. Undeniably, Mongo is what I'm comfortable with have spend the most time writing and the queries are dead simple in Go (to me at least) compared to Postgres where I have not had luck with embedded structs and getting them to easily insert or scanned when querying (especially many rows) using sqlx. Getting better at postgres is something I can do and am absolutely 100% willing to do if it's the right choice, I just haven't run into the issues with Mongo that I've seen other people haveAs far as the data goes, there's not a ton of places where I would need to do joins, maybe 5% of the total DB calls or less and I know that's where Mongo gets most of its flak. ]]></content:encoded></item><item><title>Go Panic and Recover: A Deep Dive into Error Handling</title><link>https://dev.to/leapcell/go-panic-and-recover-a-deep-dive-into-error-handling-56be</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 15:00:48 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In the Go language, there are two keywords that often appear in pairs — panic and recover. These two keywords are closely related to defer. They are both built-in functions in the Go language and provide complementary functions.
  
  
  I. Basic Functions of panic and recover
: It can change the control flow of the program. After calling panic, the remaining code of the current function will be immediately stopped from execution, and the defer of the caller will be recursively executed in the current Goroutine.: It can stop the program crash caused by panic. It is a function that can only take effect in defer. Calling it in other scopes will not have any effect.
  
  
  II. Phenomena When Using panic and recover

  
  
  (I) panic Only Triggers the defer of the Current Goroutine
The following code demonstrates this phenomenon:The running result is as follows:$ go run main.go
in goroutine
panic:
...
When running this code, it will be found that the defer statement in the main function is not executed, and only the defer in the current Goroutine is executed. Because the runtime.deferproc corresponding to the defer keyword will associate the deferred call function with the Goroutine where the caller is located, so when the program crashes, only the deferred call function of the current Goroutine will be called.
  
  
  (II) recover Only Takes Effect When Called in defer
The following code reflects this feature:$ go run main.go
in main
panic: unknown err
goroutine 1 [running]:
main.main()
 ...
exit status 2
By carefully analyzing this process, it can be known that recover will only take effect when called after a panic occurs. However, in the above control flow, recover is called before panic, which does not meet the conditions for taking effect. Therefore, the recover keyword needs to be used in defer.
  
  
  (III) panic Allows Multiple Nested Calls in defer
The following code shows how to call panic multiple times in a defer function:The running result is as follows:$ go run main.go
in main
panic: panic once
  panic: panic again
  panic: panic again and again
goroutine 1 [running]:
...
exit status 2
From the output result of the above program, it can be determined that multiple calls to panic in the program will not affect the normal execution of the defer function. Therefore, it is generally safe to use defer for the finalization work.
  
  
  III. Data Structure of panic
The panic keyword in the source code of the Go language is represented by the data structure runtime._panic. Every time panic is called, a data structure like the following will be created to store relevant information:: It is a pointer to the parameter when defer is called.: It is the parameter passed in when panic is called.: It points to the earlier called runtime._panic structure.: It indicates whether the current runtime._panic has been recovered by recover.: It indicates whether the current panic has been forcibly terminated.From the link field in the data structure, it can be inferred that the panic function can be called continuously multiple times, and they can form a linked list through the link.The three fields pc, sp, and goexit in the structure are all introduced to fix the problems brought by runtime.Goexit. runtime.Goexit can only end the Goroutine that calls this function without affecting other Goroutines. However, this function will be cancelled by the panic and recover in defer. The introduction of these three fields is to ensure that this function will definitely take effect.
  
  
  IV. Principle of Program Crash
The compiler will convert the keyword panic into runtime.gopanic. The execution process of this function includes the following steps:Create a new runtime._panic and add it to the front of the _panic linked list of the Goroutine where it is located.Continuously obtain runtime._defer from the _defer linked list of the current Goroutine in a loop and call runtime.reflectcall to run the deferred call function.Call runtime.fatalpanic to abort the entire program.
It should be noted that three relatively important parts of code are omitted in the above function:The code in the recover branch for restoring the program.The code for optimizing the performance of the defer call through inlining.The code for fixing the abnormal situation of runtime.Goexit.In version 1.14, the Go language solved the conflict between recursive panic and recover and runtime.Goexit through the submission of runtime: ensure that Goexit cannot be aborted by a recursive panic/recover.runtime.fatalpanic implements a program crash that cannot be recovered. Before aborting the program, it will print out all the panic messages and the parameters passed in during the call through runtime.printpanics:After printing the crash message, it will call runtime.exit to exit the current program and return the error code 2. The normal exit of the program is also implemented through runtime.exit.
  
  
  V. Principle of Crash Recovery
The compiler will convert the keyword recover into runtime.gorecover:The implementation of this function is very simple. If the current Goroutine has not called panic, then this function will directly return nil, which is also the reason why the crash recovery will fail when called in a non-defer. Under normal circumstances, it will modify the recovered field of runtime._panic, and the recovery of the program is handled by the runtime.gopanic function:The above code omits the inlining optimization of defer. It takes out the program counter pc and stack pointer sp from runtime._defer and calls the runtime.recovery function to trigger the scheduling of the Goroutine. Before the scheduling, it will prepare the sp, pc, and the return value of the function:When the defer keyword is called, the stack pointer sp and program counter pc at the time of the call have already been stored in the runtime._defer structure. The runtime.gogo function here will jump back to the position where the defer keyword was called.runtime.recovery will set the return value of the function to 1 during the scheduling process. From the comments of runtime.deferproc, it can be found that when the return value of the runtime.deferproc function is 1, the code generated by the compiler will directly jump to before the return of the caller function and execute runtime.deferreturn:After jumping to the runtime.deferreturn function, the program has been recovered from the panic and executes the normal logic, and the runtime.gorecover function can also take out the arg parameter passed in when calling panic from the runtime._panic structure and return it to the caller.Analyzing the crash and recovery process of the program is rather tricky, and the code is not particularly easy to understand. Here is a simple summary of the program crash and recovery process:The compiler is responsible for the work of converting keywords. It converts panic and recover into runtime.gopanic and runtime.gorecover respectively, converts defer into the runtime.deferproc function, and calls the runtime.deferreturn function at the end of the function that calls defer.When encountering the runtime.gopanic method during the running process, it will successively take out the runtime._defer structure from the linked list of the Goroutine and execute it.If runtime.gorecover is encountered when calling the deferred execution function, it will mark _panic.recovered as true and return the parameter of the panic.After this call ends, runtime.gopanic will take out the program counter pc and stack pointer sp from the runtime._defer structure and call the runtime.recovery function to restore the program.runtime.recovery will jump back to runtime.deferproc according to the passed-in pc and sp.The code automatically generated by the compiler will find that the return value of runtime.deferproc is not 0. At this time, it will jump back to runtime.deferreturn and restore to the normal execution flow.If runtime.gorecover is not encountered, it will traverse all the runtime._defer in turn, and finally call runtime.fatalpanic to abort the program, print the parameters of the panic, and return the error code 2.The analysis process involves a lot of knowledge at the underlying level of the language, and the source code is also relatively obscure to read. It is full of unconventional control flows, jumping back and forth through the program counter. However, it is still very helpful for understanding the execution flow of the program. Finally, I would like to recommend the most suitable deployment platform: 
  
  
  1. Multi-Language Support
Develop with JavaScript, Python, Go, or Rust.

  
  
  2. Deploy unlimited projects for free
pay only for usage — no requests, no charges.
  
  
  3. Unbeatable Cost Efficiency
Pay-as-you-go with no idle charges.
Example: $25 supports 6.94M requests at a 60ms average response time.

  
  
  4. Streamlined Developer Experience
Intuitive UI for effortless setup.
Fully automated CI/CD pipelines and GitOps integration.
Real-time metrics and logging for actionable insights.

  
  
  5. Effortless Scalability and High Performance
Auto-scaling to handle high concurrency with ease.
Zero operational overhead — just focus on building.
]]></content:encoded></item><item><title>In-depth Guide to net/netip Prefix Methods 7/7</title><link>https://dev.to/rezmoss/in-depth-guide-to-netnetip-prefix-methods-77-4b3c</link><author>Rez Moss</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 15:00:00 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hey there! We've made it to our final deep dive into net/netip's core types. Today we're focusing on the Prefix type and its methods. If you've worked with networks, you're familiar with CIDR notation (like 192.168.1.0/24). That's exactly what Prefix handles, and we're going to explore every method you can use with it.Let's start by looking at all the ways to create and work with Prefix.Let's explore the essential methods every Prefix provides:
  
  
  1. IPAM (IP Address Management) System
A comprehensive IPAM system using Prefix:A tool for network planning and analysis:A system for managing network access control lists:Handle IPv4 and IPv6 AppropriatelyThis concludes our deep dive into the net/netip package! We've covered:Addr type and its methodsAddrPort for handling IP:port combinationsPrefix for working with CIDR networksThese types work together to provide a robust foundation for network programming in Go. The key benefits of using net/netip include:Comprehensive functionalityRemember to check the Go documentation for updates and new features. The package continues to evolve with the language.Keep exploring and building great networking applications with Go!]]></content:encoded></item><item><title>How Relation with tpl and Html on VSCode?</title><link>https://dev.to/skyhayato/how-relation-with-tpl-and-html-on-vscode-11a9</link><author>SKY-HaYaTo</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 14:19:10 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hi,Guys! 
I'm Kohei,a Software Engineer in Japan.I am talking about a function of VSCode.Now, The Topic is about Relationship with these extension  and  on Visual Studio Code(VSCode).Extension  is often used in Web Framework of PHP.Recently, I develop personal Web apps using  and  which is one of Web MVC Frameworks of Golang. is generally used  as template engine.So, I need to make VSCode recognize  extension like .In this article, you do'nt install any plugins with ,but revide  on VSCode.VSCode is the most popular Free IDE and used in the world.
An one of nice functions on the IDE is to relation variaty of extension.As my memorandom and shrering with you,I have decited to write the article.Ok,Now,Let's explanation!In conclution, only 3 steps is completed.
  
  
  Start VSCode and Click Prompt Screen
Top of the VSCode is set (as a below picture).
You click on cursor.
  
  
  Input Value of  in the screen
Next,You need to input the value .
If maybe you continue to type , key intellisence will start and display some canditates including .When you see the word,click it!Then target page will transition to .
  
  
  Revision Settings.json File
When display ,you will type String of sentense (below the capture).You input the senetences,please save the revision.From now, we can relationship with  and .]]></content:encoded></item><item><title>6 New AI-Powered Tech Startups Reach Unicorn Status in January 2025</title><link>https://dev.to/saad_hassan_8f937dc6fafc9/6-new-ai-powered-tech-startups-reach-unicorn-status-in-january-2025-pa3</link><author>Saad Hassan</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 13:47:37 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Six cutting-edge AI startups have skyrocketed to unicorn status in January 2025, shaking up industries like healthcare, cybersecurity, automation, and defense. With billion-dollar valuations and massive investments, these companies—Truveta, Codeium, Mercor, Augury, Neko Health, and Epirus—are redefining the future of tech. From AI-driven medical breakthroughs to next-gen coding assistants, the surge in AI funding signals a new era of innovation]]></content:encoded></item><item><title>Gofs - a file server written in go</title><link>https://www.reddit.com/r/golang/comments/1iuqggw/gofs_a_file_server_written_in_go/</link><author>/u/-dtdt-</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 13:06:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/-dtdt- ]]></content:encoded></item><item><title>I built a new playground for Go</title><link>https://codiew.io/ide?t=go</link><author>/u/Halabooda</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 12:23:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Junior, Trying to understand why startups use golang for backend</title><link>https://www.reddit.com/r/golang/comments/1iup4di/junior_trying_to_understand_why_startups_use/</link><author>/u/FriendshipOk6564</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:53:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello,i just took a look at the website 'who is hiring' and saw a lot of startups using ruby on rails and golang in their stack and i'm confuse, the path isn't normally mvp in rails and after some companies will rewrite their wall backend at some point in something like Java spring? it append for netflix but also a big company where i live. Why would they mixte ror and golang? Those it mean they are rewriting their ror in a microservice architecture in go?]]></content:encoded></item><item><title>How to properly prepare monorepos in Golang and is it worth it?</title><link>https://www.reddit.com/r/golang/comments/1iuoppk/how_to_properly_prepare_monorepos_in_golang_and/</link><author>/u/GoDuffer</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:27:17 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello everyone. At the moment I am writing a report on the topic of a monorepo in order to close my internship at the university.Since I am a Go developer (or at least I aspire to be one), I decided to make a monorepo in Go.The first thing I came across was an article from Uber about how they use Bazel and I started digging in this direction.And then I realized that it was too complicated for small projects and I became interested.Does it make sense to use a monorepo on small projects? If not, how to split the application into services? Or store each service in a separate repository.In Java, everything is trivially simple with their modules and Gradle. Yes, Go has modules and a workspace, but let's be honest, this is not the level of Gradle.As a result, we have that Bazel is too complicated for simple projects, and gowork seems somehow cut down after Gradle.Monorepo or polyrepo for Go?Is there anything other than go work and Bazel?What is the correct way to split a Go project so that it looks like a Solution in C#, or modules in Java/Gradle?It is quite possible that I really don't understand the architecture of Go projects, I will be glad if you point me in the right direction.]]></content:encoded></item><item><title>The Deeper Love of Go (Go 1.24 early access edition)</title><link>https://bitfieldconsulting.com/books/deeper</link><author>/u/bitfieldconsulting</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:10:16 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My experience after switching from Java to Go</title><link>https://www.reddit.com/r/golang/comments/1iuni44/my_experience_after_switching_from_java_to_go/</link><author>/u/hosmanagic</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 10:03:43 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/hosmanagic ]]></content:encoded></item><item><title>reddittui - A terminal browser for reddit</title><link>https://github.com/tonymajestro/reddit-tui</link><author>/u/tmajest</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 07:54:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>creating your own Docker like what a shiny title and hard work A year ago, I built a minimal container in Go. Now, it&apos;s time for a revisit! This time, I&apos;m tackling network isolation, resource limits, and deeper container architecture.</title><link>https://dev.to/micromax/creating-your-own-docker-like-what-a-shiny-title-and-hard-work-a-year-ago-i-built-a-minimal-53fc</link><author>mohamed alaaeldin</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 01:25:52 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Beyond Basics: Building a More Powerful Container in Go — Network Isolation & Advanced Featuresmohamed alaaeldin ・ Feb 21]]></content:encoded></item><item><title>Beyond Basics: Building a More Powerful Container in Go — Network Isolation &amp; Advanced Features</title><link>https://dev.to/micromax/beyond-basics-building-a-more-powerful-container-in-go-network-isolation-advanced-features-3674</link><author>mohamed alaaeldin</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 01:21:41 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Containers Uncovered: More Than Just Lightweight Virtual Machines!”If you’re like me — always wondering how things work and eager to build them with your own mind and hands — you’re in the right place!
    In the first part of this article (Part 1), I attempted to build a minimal container system using only Go, relying on Linux’s unshare and namespaces. It was purely a demonstration, and I wasn’t aiming to develop a fully functional container runtime tool. I intentionally left out many critical aspects, such as security, networking, and image management.
    I initially thought it would be simple, but I quickly realized that even a basic container system involves thousands of concepts and implementations. However, my passion for understanding and building things kept me going.
    Now, after a year since my first article on Building a Minimal Container in Go, I’ve realized that both my code and my original article need a fresh perspective. So, it’s time for a revisit!Responsibilities:
Parse user commands (run, exec, ps, rm)
Communicate with daemon via RPC or any other way
Format and display outputCommand completion
Output formatting (JSON/YAML)
Log streaming
Manage container lifecycle
Maintain container state database
Coordinate between components
REST/gRPC API
Event logging
Resource tracking
Namespace Manager: CLONE_NEW* flags handling and more flags in real world .
Cgroups Manager: Resource constraints
Filesystem Setup: RootFS preparation
OCI runtime spec compliance
User namespace remapping
Seccomp/AppArmor profiles
Registry Client: Docker Hub integration or you own images services if you will go wiled
Layer Manager: OverlayFS/BTRFS
Snapshotter: Copy-on-write layers
Image caching
Signature verification
Garbage collection
CNI Plugins: Bridge, MACVLAN, IPVLAN
IPAM: DHCP/Static allocation
Service Mesh: DNS, service discovery
Multi-host networking
Network policies
Port mapping
Volume Manager: Bind mounts
Snapshot Manager: Incremental backups
Quota Enforcer: Disk limits
Persistent storage
Temporary filesystems
Encryption support
this schema will give you a bigger picture
                           +---------------------+
                           |      User CLI       |
                           | (run, exec, ps, rm) |
                           +----------+----------+
                                      |
                                      | (gRPC/HTTP)
                                      v
                           +---------------------+
                           |   Container Daemon  |
                           | (State Management)  |
                           +----------+----------+
                                      |
                   +------------------+------------------+
                   |                  |                  |
         +----------+----------+ +-----+--------+ +-------+---------+
         |   Container Runtime | | Image Service| | Network Manager |
         | (namespace/cgroups) | | (OCI Images)  | | (CNI Plugins)   |
         +----------+----------+ +-----+--------+ +-------+---------+
                   |                  |                  |
         +---------v---------+ +------v-------+ +--------v---------+
         | Linux Kernel       | | Storage Driver| | Host Networking |
         | - namespaces       | | (OverlayFS)   | | (iptables/bridge)|
         | - cgroups v2       | +---------------+ +------------------+
         | - capabilities     |
         +--------------------+
It has been a long journey for me to learn and think through every component. I encountered many challenges, especially with aspects like OverlayFS and networking.My biggest issue in my first implementation was networking. It was really difficult to isolate the child container and set up its own bridged network.To solve network isolation, you need to think clearly 🤔 at this stage.First, you need to create a bridge on the host with two virtual interfaces:The first interface remains on the host.
The second interface is moved to the child container 🫙.
The real challenge here is managing command signaling between the host and the child container.In my approach, I will attempt to create a proof of concept implementation.
Understanding Container NetworkingWhen we create containers, one of the most crucial aspects is network isolation. Think of it like giving each container its own private network environment, complete with its own network interfaces, IP addresses, and routing rules. Let’s break down how we achieve this in our container implementation.
The Network Setup ProcessCreating the Network NamespaceFirst, we create a separate network namespace for our container. This is like giving the container its own private networking room:const ContainerName = "mycontainer"

func createNetworkNamespace(name string) error {
    // Create directory for network namespaces
    if err := os.MkdirAll("/var/run/netns", 0755); err != nil {
        return err
    }

    // Create the namespace file
    nsFile := filepath.Join("/var/run/netns", name)
    fd, err := os.Create(nsFile)
    if err != nil {
        return err
    }
    fd.Close()

    // Bind mount it to make it accessible
    return syscall.Mount("/proc/self/ns/net", nsFile, "bind", syscall.MS_BIND, "")
}
Setting Up Virtual Network InterfacesWe create a virtual network cable (veth pair) to connect our container to the host system:const (
    VethHost      = "veth0"  // Host end of the cable
    VethContainer = "veth1"  // Container end of the cable
    ContainerIP   = "10.0.0.2/24"
    HostIP        = "10.0.0.1/24"
    Gateway       = "10.0.0.1"
)
The setup happens in two parts:
1-On the host side:func setupHostNetwork(pid int) error {
    // Create the virtual network cable (veth pair)
    if err := exec.Command("ip", "link", "add", VethHost, "type", "veth", 
        "peer", "name", VethContainer).Run(); err != nil {
        return fmt.Errorf("failed to create veth pair: %v", err)
    }

    // Move one end to the container
    if err := exec.Command("ip", "link", "set", VethContainer, 
        "netns", fmt.Sprintf("%d", pid)).Run(); err != nil {
        return fmt.Errorf("failed to move veth to container: %v", err)
    }

    // Configure the host end
    if err := exec.Command("ip", "link", "set", VethHost, "up").Run(); err != nil {
        return fmt.Errorf("failed to bring up host interface: %v", err)
    }
    if err := exec.Command("ip", "addr", "add", HostIP, "dev", VethHost).Run(); err != nil {
        return fmt.Errorf("failed to assign IP to host interface: %v", err)
    }
}
2 — Inside the container:func setupContainerNetwork() error {
    // Enable the loopback interface
    if err := exec.Command("ip", "link", "set", "lo", "up").Run(); err != nil {
        return fmt.Errorf("failed to bring up lo: %v", err)
    }

    // Configure the container's network interface
    if err := exec.Command("ip", "link", "set", VethContainer, "up").Run(); err != nil {
        return fmt.Errorf("failed to bring up veth: %v", err)
    }
    if err := exec.Command("ip", "addr", "add", ContainerIP, 
        "dev", VethContainer).Run(); err != nil {
        return fmt.Errorf("failed to assign IP to veth: %v", err)
    }
    if err := exec.Command("ip", "route", "add", "default", 
        "via", Gateway).Run(); err != nil {
        return fmt.Errorf("failed to add default route: %v", err)
    }
}
To allow our container to access the internet, we need to set up NAT (Network Address Translation) rules. This is like setting up a router for our container:func setupHostNetwork(pid int) error {
    // Get the host's internet-connected interface
    iface, err := getDefaultInterface()
    if err != nil {
        return fmt.Errorf("failed to get default interface: %v", err)
    }

    // Set up NAT and forwarding rules
    cmds := [][]string{
        {"sysctl", "-w", "net.ipv4.ip_forward=1"},
        {"iptables", "-t", "nat", "-A", "POSTROUTING", 
            "-s", "10.0.0.0/24", "-o", iface, "-j", "MASQUERADE"},
        {"iptables", "-A", "FORWARD", "-i", iface, 
            "-o", VethHost, "-j", "ACCEPT"},
        {"iptables", "-A", "FORWARD", "-i", VethHost, 
            "-o", iface, "-j", "ACCEPT"},
    }

    for _, cmd := range cmds {
        if out, err := exec.Command(cmd[0], cmd[1:]...).CombinedOutput(); err != nil {
            return fmt.Errorf("failed %v: %s\n%s", cmd, err, out)
        }
    }
}
finally , Resource CleanupOne often overlooked but crucial aspect is cleaning up network resources when the container stops. Our implementation handles this through a ResourceManager:
type ResourceManager struct {
    containerName string
    vethHost      string
    mounts        []string
    namespaces    []string
}

func (rm *ResourceManager) cleanupNetwork() error {
    // Clean up iptables rules
    if err := rm.cleanupIptablesRules(); err != nil {
        log.Printf("Warning: iptables cleanup failed: %v", err)
    }

    // Remove the virtual network interface
    if out, err := exec.Command("ip", "link", "delete", 
        rm.vethHost).CombinedOutput(); err != nil {
        log.Printf("Warning: failed to delete veth interface: %v (%s)", err, out)
    }

    return nil
}
How It All Works TogetherWhen starting a container:

Create a new network namespace
Create virtual network interfaces (veth pair)
Configure IP addresses and routing
Set up NAT for internet access
Mount necessary filesystems and set up devices
2 .During container runtime:Container uses its virtual network interface for all network communication
Outgoing traffic goes through NAT to reach the internet
Incoming traffic is routed back to the container
3 . When stopping a container:Clean up iptables rules
Remove virtual interfaces
Unmount network namespace
Remove namespace files
Common Issues and DebuggingWhen implementing container networking, you might encounter these common issues:DNS Resolution Problems

Our implementation includes DNS setup:
// in most cases this will case error , still trying to solve it 
func setupDNS() error {
    resolvHost := "/etc/resolv.conf"
    resolvContainer := filepath.Join(RootFS, "etc/resolv.conf")
    return syscall.Mount(resolvHost, resolvContainer, "bind", 
        syscall.MS_BIND|syscall.MS_RDONLY, "")
}
2.Network Interface IssuesAlways check interface status with ip link show
Verify IP assignments with ip addr show
Check routing with ip route show
Verify iptables rules are correctly set
Check IP forwarding is enabled
Ensure the host interface is up and working
Our implementation includes several security features:Network Isolation

Each container gets its own network namespace
Network traffic is isolated between containers
Proper cleanup of network resources prevents resource leaks
Automatic cleanup on container exit
This networking implementation provides a solid foundation for container isolation while maintaining internet connectivity. While it’s simpler than production container runtimes, it demonstrates the core concepts of container networking.this was the hard part for me and i have tryed so many implemention to achive that . you have to keep in main what and where your command executted . some times you find your self trying to create vath’s in continer or you cannot connect or move the continer interface from host to chiledyou have to read my previeus articl to know what we are doing i had clean up my code and add every thing agine to test network isolationdo not forget to change RootFS to your root fs like “ubuntu or whatever image you will run”package main

import (
 "fmt"
 "log"
 "os"
 "os/exec"
 "path/filepath"
 "strings"
 "syscall"
 "os/signal" 
)

const (
 ContainerName = "mycontainer"
 VethHost      = "veth0"
 VethContainer = "veth1"
 ContainerIP   = "10.0.0.2/24"
 HostIP        = "10.0.0.1/24"
 Gateway       = "10.0.0.1"
 RootFS        = "/mnt/drive/go-projects/lc-images-regs/ubuntu_fs"
)



type ResourceManager struct {
    containerName string
    vethHost      string
    mounts        []string
    namespaces    []string
}
func NewResourceManager(containerName string) *ResourceManager {
    return &ResourceManager{
        containerName: containerName,
        vethHost:     VethHost,
        mounts: []string{
            "/proc",
            "/dev/pts",
            "/dev",
        },
        namespaces: []string{
            "net",
            "uts",
            "pid",
            "ipc",
        },
    }
}

func (rm *ResourceManager) Setup() {
    // Set up signal handling
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        sig := <-sigChan
        log.Printf("Received signal %v, cleaning up...", sig)
        rm.Cleanup()
        os.Exit(1)
    }()
}

func (rm *ResourceManager) Cleanup() error {
    var errors []string

    // 1. Clean up network resources
    if err := rm.cleanupNetwork(); err != nil {
        errors = append(errors, fmt.Sprintf("network cleanup error: %v", err))
    }

    // 2. Clean up mounts
    if err := rm.cleanupMounts(); err != nil {
        errors = append(errors, fmt.Sprintf("mount cleanup error: %v", err))
    }

    // 3. Clean up namespaces
    if err := rm.cleanupNamespaces(); err != nil {
        errors = append(errors, fmt.Sprintf("namespace cleanup error: %v", err))
    }

    if len(errors) > 0 {
        return fmt.Errorf("cleanup errors: %s", strings.Join(errors, "; "))
    }
    return nil
}

func (rm *ResourceManager) cleanupNetwork() error {
    // Clean up iptables rules first
    if err := rm.cleanupIptablesRules(); err != nil {
        log.Printf("Warning: iptables cleanup failed: %v", err)
    }

    // Clean up veth interfaces
    if out, err := exec.Command("ip", "link", "delete", rm.vethHost).CombinedOutput(); err != nil {
        log.Printf("Warning: failed to delete veth interface: %v (%s)", err, out)
    }

    return nil
}

func (rm *ResourceManager) cleanupIptablesRules() error {
    iface, err := getDefaultInterface()
    if err != nil {
        return fmt.Errorf("failed to get default interface: %v", err)
    }

    rules := [][]string{
        {"iptables", "-D", "FORWARD", "-i", iface, "-o", rm.vethHost, "-j", "ACCEPT"},
        {"iptables", "-D", "FORWARD", "-i", rm.vethHost, "-o", iface, "-j", "ACCEPT"},
        {"iptables", "-t", "nat", "-D", "POSTROUTING", "-s", "10.0.0.0/24", "-o", iface, "-j", "MASQUERADE"},
    }

    for _, rule := range rules {
        if out, err := exec.Command(rule[0], rule[1:]...).CombinedOutput(); err != nil {
            log.Printf("Warning: failed to remove iptables rule: %v (%s)", err, out)
        }
    }

    return nil
}

func (rm *ResourceManager) cleanupMounts() error {
    for _, mount := range rm.mounts {
        mountPath := filepath.Join(RootFS, mount)
        if err := syscall.Unmount(mountPath, syscall.MNT_DETACH); err != nil {
            log.Printf("Warning: failed to unmount %s: %v", mountPath, err)
        }
    }
    return nil
}

func (rm *ResourceManager) cleanupNamespaces() error {
    for _, ns := range rm.namespaces {
        nsPath := filepath.Join("/var/run/netns", rm.containerName)
        if err := syscall.Unmount(nsPath, syscall.MNT_DETACH); err != nil {
            log.Printf("Warning: failed to unmount namespace %s: %v", ns, err)
        }
        if err := os.Remove(nsPath); err != nil {
            log.Printf("Warning: failed to remove namespace file %s: %v", nsPath, err)
        }
    }
    return nil
}

func cleanupExistingResources() error {
 // Cleanup network namespace
 if _, err := os.Stat("/var/run/netns/" + ContainerName); err == nil {
  if err := cleanupNetworkNamespace(ContainerName); err != nil {
   return fmt.Errorf("failed to cleanup existing network namespace: %v", err)
  }
 }

 // Cleanup veth interfaces
 if _, err := exec.Command("ip", "link", "show", VethHost).CombinedOutput(); err == nil {
  if err := exec.Command("ip", "link", "delete", VethHost).Run(); err != nil {
   return fmt.Errorf("failed to delete existing veth interface: %v", err)
  }
 }

 // Cleanup iptables rules
 if err := cleanupIptablesRules(); err != nil {
  return fmt.Errorf("failed to cleanup iptables rules: %v", err)
 }

 return nil
}

func cleanupIptablesRules() error {
 iface, err := getDefaultInterface()
 if err != nil {
  return fmt.Errorf("failed to get default interface: %v", err)
 }

 cmds := [][]string{
  {"iptables", "-D", "FORWARD", "-i", iface, "-o", VethHost, "-j", "ACCEPT"},
  {"iptables", "-D", "FORWARD", "-i", VethHost, "-o", iface, "-j", "ACCEPT"},
  {"iptables", "-t", "nat", "-D", "POSTROUTING", "-s", "10.0.0.0/24", "-o", iface, "-j", "MASQUERADE"},
 }

 for _, cmd := range cmds {
  // Ignore errors since rules might not exist
  exec.Command(cmd[0], cmd[1:]...).Run()
 }

 return nil
}
func getDefaultInterface() (string, error) {
 out, err := exec.Command("ip", "route", "show", "default").CombinedOutput()
 if err != nil {
  return "", err
 }

 fields := strings.Fields(string(out))
 for i, field := range fields {
  if field == "dev" && i+1 < len(fields) {
   return fields[i+1], nil
  }
 }

 return "", fmt.Errorf("no default interface found")
}

func main() {
 if len(os.Args) < 2 {
  log.Fatal("Usage: [run|child] command [args...]")
 }

 switch os.Args[1] {
 case "run":
  run()
 case "child":
  child()
 default:
  log.Fatalf("unknown command: %s", os.Args[1])
 }
}
func setupCgroups(ContainerName string , pid int, cpuShares, memoryLimitMB int) error {
    cgroupBase := "/sys/fs/cgroup"
    containerID := ContainerName // fmt.Sprintf("container_%d", pid)

    // Create CPU cgroup
    cpuPath := filepath.Join(cgroupBase, "cpu", containerID)
    os.MkdirAll(cpuPath, 0755)
    os.WriteFile(filepath.Join(cpuPath, "cpu.shares"), []byte(fmt.Sprintf("%d", cpuShares)), 0644)
    os.WriteFile(filepath.Join(cpuPath, "tasks"), []byte(fmt.Sprintf("%d", pid)), 0644)

    // Create memory cgroup
    memoryPath := filepath.Join(cgroupBase, "memory", containerID)
    os.MkdirAll(memoryPath, 0755)
    os.WriteFile(filepath.Join(memoryPath, "memory.limit_in_bytes"), []byte(fmt.Sprintf("%d", memoryLimitMB*1024*1024)), 0644)
    os.WriteFile(filepath.Join(memoryPath, "tasks"), []byte(fmt.Sprintf("%d", pid)), 0644)


 uidMap := fmt.Sprintf("0 %d 1", os.Getuid())
 gidMap := fmt.Sprintf("0 %d 1", os.Getgid())

 os.WriteFile(fmt.Sprintf("/proc/%d/uid_map", pid), []byte(uidMap), 0644)
 os.WriteFile(fmt.Sprintf("/proc/%d/gid_map", pid), []byte(gidMap), 0644)
    return nil
}
func run() {
 rm := NewResourceManager(ContainerName)
    rm.Setup()
    defer rm.Cleanup()

 if err := cleanupExistingResources(); err != nil {
  log.Printf("Cleanup warning: %v", err)
 }

 // Create network namespace
 if err := createNetworkNamespace(ContainerName); err != nil {
  log.Fatalf("Failed to create network namespace: %v", err)
 }

 // Start container process
 cmd := exec.Command("/proc/self/exe", append([]string{"child"}, os.Args[2:]...)...)
 cmd.Stdin = os.Stdin
 cmd.Stdout = os.Stdout
 cmd.Stderr = os.Stderr
 cmd.SysProcAttr = &syscall.SysProcAttr{
  Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWNET |

  syscall.CLONE_NEWIPC ,


 }

 if err := cmd.Start(); err != nil {
  log.Fatalf("Failed to start container: %v", err)
 }
 pid := cmd.Process.Pid
 if err :=setupCgroups(ContainerName , pid , 512 , 256  ); err != nil { // Example: 512 CPU shares, 256 MB memory limit
  log.Fatalf("Failed to setup cgroups: %v", err)
 }
 // Configure host-side networking
 if err := setupHostNetwork(cmd.Process.Pid); err != nil {
  log.Fatalf("Failed to setup host network: %v", err)
 }

 // Wait for container to exit
 if err := cmd.Wait(); err != nil {
  log.Fatalf("Container failed: %v", err)
 }

 // Cleanup
 if err := cleanupNetworkNamespace(ContainerName); err != nil {
  log.Printf("Failed to cleanup network namespace: %v", err)
 }
}

func child() {
 // Setup container environment
 if err := setupContainer(); err != nil {
  log.Fatalf("Failed to setup container: %v", err)
 }

 // Execute command
 if len(os.Args) < 3 {
  log.Fatal("No command specified")
 }
 cmd := exec.Command(os.Args[2], os.Args[3:]...)
 cmd.Stdin = os.Stdin
 cmd.Stdout = os.Stdout
 cmd.Stderr = os.Stderr
 if err := cmd.Run(); err != nil {
  log.Fatalf("Command failed: %v", err)
 }
}

func createNetworkNamespace(name string) error {
 // Create bind mount for ip netns compatibility
 if err := os.MkdirAll("/var/run/netns", 0755); err != nil {
  return err
 }

 // Create namespace file
 nsFile := filepath.Join("/var/run/netns", name)
 fd, err := os.Create(nsFile)
 if err != nil {
  return err
 }
 fd.Close()

 // Bind mount
 return syscall.Mount("/proc/self/ns/net", nsFile, "bind", syscall.MS_BIND, "")
}

func cleanupNetworkNamespace(name string) error {
 nsFile := filepath.Join("/var/run/netns", name)
 if err := syscall.Unmount(nsFile, 0); err != nil {
  return fmt.Errorf("failed to unmount network namespace: %v", err)
 }
 // Remove the file to complete cleanup.
 if err := os.Remove(nsFile); err != nil {
  return fmt.Errorf("failed to remove namespace file %s: %v", nsFile, err)
 }
 return nil
}


func setupHostNetwork(pid int) error {
 // Get host's default interface
 iface, err := getDefaultInterface()
 if err != nil {
  return fmt.Errorf("failed to get default interface: %v", err)
 }

 // Create veth pair
 if err := exec.Command("ip", "link", "add", VethHost, "type", "veth", "peer", "name", VethContainer).Run(); err != nil {
  return fmt.Errorf("failed to create veth pair: %v", err)
 }

 // Move container end to namespace
 if err := exec.Command("ip", "link", "set", VethContainer, "netns", fmt.Sprintf("%d", pid)).Run(); err != nil {
  return fmt.Errorf("failed to move veth to container: %v", err)
 }

 // Configure host interface
 if err := exec.Command("ip", "link", "set", VethHost, "up").Run(); err != nil {
  return fmt.Errorf("failed to bring up host interface: %v", err)
 }
 if err := exec.Command("ip", "addr", "add", HostIP, "dev", VethHost).Run(); err != nil {
  return fmt.Errorf("failed to assign IP to host interface: %v", err)
 }

 cmds := [][]string{

  {"sysctl", "-w", "net.ipv4.ip_forward=1"},
  {"iptables", "-t", "nat", "-A", "POSTROUTING", "-s", "10.0.0.0/24", "-o", iface, "-j", "MASQUERADE"},
  {"iptables", "-A", "FORWARD", "-i", iface, "-o", VethHost, "-j", "ACCEPT"},
  {"iptables", "-A", "FORWARD", "-i", VethHost, "-o", iface, "-j", "ACCEPT"},
 }

 for _, cmd := range cmds {
  if out, err := exec.Command(cmd[0], cmd[1:]...).CombinedOutput(); err != nil {
   return fmt.Errorf("failed %v: %s\n%s", cmd, err, out)
  }
 }

 return nil
}

func setupContainer() error {
 // Setup root filesystem
 if err := syscall.Chroot(RootFS); err != nil {
  return fmt.Errorf("chroot failed: %v", err)
 }
 if err := os.Chdir("/"); err != nil {
  return fmt.Errorf("chdir failed: %v", err)
 }

 // Mount proc
 if err := syscall.Mount("proc", "/proc", "proc", 0, ""); err != nil {
  return fmt.Errorf("failed to mount proc: %v", err)
 }

 // Setup devices
 if err := setupDevices(); err != nil {
  return fmt.Errorf("failed to setup devices: %v", err)
 }

 // Configure network
 if err := setupContainerNetwork(); err != nil {
  return fmt.Errorf("failed to setup network: %v", err)
 }

 //if err := setupDNS(); err != nil {
 // return fmt.Errorf("DNS setup failed: %v", err)
 //}

 return nil
}

func setupDNS() error {
 // Copy host's resolv.conf
 resolvHost := "/etc/resolv.conf"
 resolvContainer := filepath.Join(RootFS, "etc/resolv.conf")

 // Create container's /etc if missing
 if err := os.MkdirAll(filepath.Join(RootFS, "etc"), 0755); err != nil {
  return err
 }

 // Bind mount host's resolv.conf
 return syscall.Mount(resolvHost, resolvContainer, "bind", syscall.MS_BIND|syscall.MS_RDONLY, "")
}

func setupDevices() error {
 // Mount tmpfs for /dev
 if err := syscall.Mount("tmpfs", "/dev", "tmpfs", 0, "size=64k,mode=755"); err != nil {
  return err
 }

 // Create /dev/pts directory if missing
 devPts := "/dev/pts"
 if err := os.MkdirAll(devPts, 0755); err != nil {
  return fmt.Errorf("mkdir %s failed: %v", devPts, err)
 }

 // Mount devpts on /dev/pts for pty support
 if err := syscall.Mount("devpts", devPts, "devpts", 0, "mode=0620,ptmxmode=0666"); err != nil {
  return fmt.Errorf("failed to mount devpts on %s: %v", devPts, err)
 }
 // Create basic devices
 devices := []struct {
  name  string
  major uint32
  minor uint32
 }{
  {"null", 1, 3},
  {"zero", 1, 5},
  {"random", 1, 8},
  {"urandom", 1, 9},
 }

 for _, dev := range devices {
  path := filepath.Join("/dev", dev.name)
  if err := syscall.Mknod(path, syscall.S_IFCHR|0666, int(makedev(dev.major, dev.minor))); err != nil {
   return err
  }
 }

 return nil
}

func makedev(major, minor uint32) uint64 {
 return (uint64(major) << 8) | uint64(minor)
}

func setupContainerNetwork() error {
 // Bring up loopback
 if err := exec.Command("ip", "link", "set", "lo", "up").Run(); err != nil {
  return fmt.Errorf("failed to bring up lo: %v", err)
 }

 // Configure veth interface
 if err := exec.Command("ip", "link", "set", VethContainer, "up").Run(); err != nil {
  return fmt.Errorf("failed to bring up veth: %v", err)
 }
 if err := exec.Command("ip", "addr", "add", ContainerIP, "dev", VethContainer).Run(); err != nil {
  return fmt.Errorf("failed to assign IP to veth: %v", err)
 }
 if err := exec.Command("ip", "route", "add", "default", "via", Gateway).Run(); err != nil {
  return fmt.Errorf("failed to add default route: %v", err)
 }

 return nil
}
Important point: You must mount and create essential virtual devices and establish communication (such as pipes or signals) between the host and child container .func setupDevices() error {
 // Mount tmpfs for /dev
 if err := syscall.Mount("tmpfs", "/dev", "tmpfs", 0, "size=64k,mode=755"); err != nil {
  return err
 }

 // Create /dev/pts directory if missing
 devPts := "/dev/pts"
 if err := os.MkdirAll(devPts, 0755); err != nil {
  return fmt.Errorf("mkdir %s failed: %v", devPts, err)
 }

 // Mount devpts on /dev/pts for pty support
 if err := syscall.Mount("devpts", devPts, "devpts", 0, "mode=0620,ptmxmode=0666"); err != nil {
  return fmt.Errorf("failed to mount devpts on %s: %v", devPts, err)
 }
 // Create basic devices
 devices := []struct {
  name  string
  major uint32
  minor uint32
 }{
  {"null", 1, 3},
  {"zero", 1, 5},
  {"random", 1, 8},
  {"urandom", 1, 9},
 }

 for _, dev := range devices {
  path := filepath.Join("/dev", dev.name)
  if err := syscall.Mknod(path, syscall.S_IFCHR|0666, int(makedev(dev.major, dev.minor))); err != nil {
   return err
  }
 }

 return nil
}
func NewResourceManager(containerName string) *ResourceManager {
    return &ResourceManager{
        containerName: containerName,
        vethHost:     VethHost,
        mounts: []string{
            "/proc",
            "/dev/pts",
            "/dev",
        },
        namespaces: []string{
            "net",
            "uts",
            "pid",
            "ipc",
        },
    }
}

func (rm *ResourceManager) Setup() {
    // Set up signal handling
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        sig := <-sigChan
        log.Printf("Received signal %v, cleaning up...", sig)
        rm.Cleanup()
        os.Exit(1)
    }()
}
Now you have a broad overview, but you still have a long journey ahead to achieve what production-ready container runtime systems offer.If you need system file images to test your code, you can use Docker to download one.$ docker run -d --rm --name ubuntu_fs ubuntu:20.04 sleep 1000
$ mkdir -p ./ubuntu_fs
$ docker cp ubuntu_fs:/ ./ubuntu_fs
$ docker stop ubuntu_fs
Or use tool like debootstrapsudo apt-get update
sudo apt-get install debootstrap
sudo mkdir -p /path/to/rootfs
sudo debootstrap stable /path/to/rootfs http://deb.debian.org/debian
Sometimes, while testing, you may need to install software in your container image from the host if your child container struggles to access the internet.sudo chroot /path/to/rootfs /bin/sh -c "apk add --no-cache iproute2"
sudo chroot /mnt/drive/go-projects/lc-images-regs/ubuntu_fs /bin/sh -c "apt-get update && apt-get install -y iproute2"
Note: Sometimes, when you try to start the container by running the following command to start Bash as the entry point, you may encounter a bug:sudo go run Main.go run sudo /bin/bash
2025/02/21 00:26:28 Failed to setup container: failed to setup network: failed to bring up veth: exit status 1
2025/02/21 01:26:28 Failed to setup host network: failed to assign IP to host interface: exit status 1
exit status 1


This happens due to resource cleanup errors. You can either ignore it and retry the command up to three times or fix the issue.You still need to implement DNS to align with the original system design. What we built is just a proof of concept application.My next step is to ensure resource limitations and create an image composer like Docker while utilizing OverlayFS. Until then, if you need any help, feel free to DM me.this is discord channel for this topic only join me :]]></content:encoded></item><item><title>Creating a Minimal Container in Go: A Step-by-Step Guide ( part 1 )</title><link>https://dev.to/micromax/creating-a-minimal-container-in-go-a-step-by-step-guide-283b</link><author>mohamed alaaeldin</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 01:08:37 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[What is Containers any way!
Containers are lightweight, portable, and efficient, making them a popular choice for deploying and running applications. In this tutorial, we’ll guide you through the process of creating a minimal container using Go. The example code provided focuses on essential containerization concepts, including namespaces, chroot, and control groups (cgroups).Before getting started, ensure you have the following installed:Go programming language: Install Go
Basic understanding of Linux namespaces and control groups
introduction
So what is Linux namespaces and control groups ?Namespaces have been part of the Linux kernel since about 2002, and over time more tooling and namespace types have been added. Real container support was added to the Linux kernel only in 2013, however. This is what made namespaces really useful and brought them to the masses.But what are namespaces exactly? Here’s a wordy definition from Wikipedia:“Namespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources.”In other words, the key feature of namespaces is that they isolate processes from each other. On a server where you are running many different services, isolating each service and its associated processes from other services means that there is a smaller blast radius for changes, as well as a smaller footprint for security‑related concerns. Mostly though, isolating services meets the architectural style of microservices as described by Martin Fowler.
Types of NamespacesWithin the Linux kernel, there are different types of namespaces. Each namespace has its own unique properties:A user namespace has its own set of user IDs and group IDs for assignment to processes. In particular, this means that a process can have root privilege within its user namespace without having it in other user namespaces.
A process ID (PID) namespace assigns a set of PIDs to processes that are independent from the set of PIDs in other namespaces. The first process created in a new namespace has PID 1 and child processes are assigned subsequent PIDs. If a child process is created with its own PID namespace, it has PID 1 in that namespace as well as its PID in the parent process’ namespace. See below for an example.
A network namespace has an independent network stack: its own private routing table, set of IP addresses, socket listing, connection tracking table, firewall, and other network‑related resources.
A mount namespace has an independent list of mount points seen by the processes in the namespace. This means that you can mount and unmount filesystems in a mount namespace without affecting the host filesystem.
An interprocess communication (IPC) namespace has its own IPC resources, for example POSIX message queues.
A UNIX Time‑Sharing (UTS) namespace allows a single system to appear to have different host and domain names to different processes.

the container are fast isolated environment , we will focus on this part many things are involved and my main goal is to Demystifying Containers

assuming that you are on a linux machine (try Power shell Ubuntu image if you are on Windows :-)
host-machine $ id

uid=1000(mohamed) gid=1000(mohamed) groups=1000(mohamed) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c.1023
Now I run the following unshare command to create a new namespace with its own user and PID namespaces. I map the root user to the new namespace (in other words, I have root privilege within the new namespace), mount a new proc filesystem, and fork my process (in this case, bash) in the newly created namespace.unshare --user --pid --map-root-user --mount-proc --fork bashCongratulation , you are in isolated name space and some how you are on
isolated PID in same file system and same network , your entry point /bin/bashThe ps -ef command shows there are two processes running – bash and the ps command itself – and the id command confirms that I’m root in the new namespace (which is also indicated by the changed command prompt):root # ps -ef
UID         PID     PPID  C STIME TTY        TIME CMD
root          1        0  0 14:46 pts/0  00:00:00 bash
root         15        1  0 14:46 pts/0  00:00:00 ps -ef
root # id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c.1023
Namespaces and ContainersNamespaces are one of the technologies that containers are built on, used to enforce segregation of resources. We’ve shown how to create namespaces manually, but container runtimes like Docker, rkt, podman , runC , containerD , and many other container technology
    one of most unique projects are https://katacontainers.io/ they claim that they are mix between container and VM’s .
What Are cgroups?cgroups, or control groups, are a Linux kernel feature that enables the management and limitation of system resources like CPU, memory, and network bandwidth, among others. We can use cgroups to set limits on these resources and distribute them among different groups of processes.cgroups have a hierarchical structure with root and child, each with resource limits set by controllers — for example, a CPU controller for CPU time or a memory controller for memory.We can use cgroups for various purposes, such as controlling resource usage in a multi-tenant environment, providing Quality of Service (QoS) guarantees, and running containers.Cgroups provide the following features:Resource limits — You can configure a cgroup to limit how much of a particular resource (memory or CPU, for example) a process can use.
Prioritization — You can control how much of a resource (CPU, disk, or network) a process can use compared to processes in another cgroup when there is resource contention.
Accounting — Resource limits are monitored and reported at the cgroup level.
Control — You can change the status (frozen, stopped, or restarted) of all processes in a cgroup with a single command.
The following command creates a v1 cgroup (you can tell by pathname format) called foo and sets the memory limit for it to 50,000,000 bytes (50 MB).root # mkdir -p /sys/fs/cgroup/memory/foo
root # echo 50000000 > /sys/fs/cgroup/memory/foo/memory.limit_in_bytes
Now I can assign a process to the cgroup, thus imposing the cgroup’s memory limit on it. I’ve written a shell script called test.sh, which prints cgroup testing tool to the screen, and then waits doing nothing. For my purposes, it is a process that continues to run until I stop it.I start test.sh in the background and its PID is reported as 2428. The script produces its output and then I assign the process to the cgroup by piping its PID into the cgroup file /sys/fs/cgroup/memory/foo/cgroup.procs.root # ./test.sh &
[1] 2428
root # cgroup testing tool
root # echo 2428 > /sys/fs/cgroup/memory/foo/cgroup.procs
To validate that my process is in fact subject to the memory limits that I defined for cgroup foo, I run the following ps command. The -o cgroup flag displays the cgroups to which the specified process (2428) belongs. The output confirms that its memory cgroup is foo.root # ps -o cgroup 2428
CGROUP
12:pids:/user.slice/user-0.slice/\
session-13.scope,10:devices:/user.slice,6:memory:/foo,...

By default, the operating system terminates a process when it exceeds a resource limit defined by its cgroup.and this fair amount of information about namespace and cgroup
you can read full doc about it by Scott van Kalken of F5
at this link , also this post Demystifying Containers 101 and this one focus on Docker ecosystem “A Beginner-Friendly Introduction to Containers, VMs and Docker”
part 1 : Chroot
i will not use Namespaces , “at this part”this may surprise however i will achieve the isolation , we will use Chroot a simple UNIX toolchroot, short for "change root," is a Unix system call that changes the root directory of a process to a specified path, effectively creating a new root filesystem for the process and its children. This can be a powerful tool for creating isolated environments or "chroot jails."
How Chroot Works:Setting a New Root Directory: When you execute the chroot system call or the chroot command in the shell, it changes the root directory for the process and its children. The new root directory becomes the / (root) directory for that process, isolating it from the actual root directory of the host system.
Isolation: After the chroot operation, the process and its children can only access files and directories within the new root directory. They cannot access files outside this new root, providing a level of isolation and containment.
System Recovery: chroot is commonly used in system recovery scenarios. If your system becomes unbootable or experiences issues, you can boot from a live CD/USB, chroot into the broken system, and make necessary repairs without affecting the rest of the host system.
Environment Isolation: Developers and system administrators may use chroot to create isolated environments for testing or building software. This is especially common in scenarios where different versions of libraries or dependencies are required.
Security: Although chroot provides some level of isolation, it's not foolproof in terms of security. It was not designed as a security feature and should not be solely relied upon for containing malicious processes. Modern containerization technologies, like Docker, utilize more advanced mechanisms, such as Linux namespaces and cgroups, to provide stronger isolation.
Consider the following example:
mkdir mychroot
cp -r /bin /lib /lib64 /usr /mychroot
chroot /mychroot /bin/bash
We create a directory called mychroot and copy essential binaries and libraries into it.
We use chroot to change the root directory to /mychroot.
After the chroot command, executing /bin/bash will run a Bash shell within the isolated environment.
Keep in mind that chroot by itself does not provide complete isolation; it is often used in conjunction with other tools and techniques to create more secure and robust containerized environments.
Prepare the Ubuntu Root Filesystemnow final this you will need before you start a filesystem .
we will use Docker to download Ubuntu filesystemyou will only need docker to download it , in your project root$ docker run -d --rm --name ubuntu_fs ubuntu:20.04 sleep 1000
$ mkdir -p ./ubuntu_fs
$ docker cp ubuntu_fs:/ ./ubuntu_fs
$ docker stop ubuntu_fs
now we have ubuntu_fs inside our project , inside your main packagepackage main

import (
 "io/ioutil"
 "log"
 "os"
 "os/exec"
 "path/filepath"
 "strconv"
 "syscall"
 "strings"
 "fmt"
 "github.com/vishvananda/netns"

)



func main() {
 switch os.Args[1] {
 case "run":
  run(os.Args[2:]...)
 case "child":
  child(os.Args[2:]...)
 default:
  log.Fatal("Unknown command. Use run <command_name>, like `run /bin/bash` or `run echo hello`")
 }
}



func run(command ...string) {

 log.Println("Executing", command, "from run")
 cmd := exec.Command("/proc/self/exe", append([]string{"child"}, command[0:]...)...)
 cmd.Stdin = os.Stdin
 cmd.Stdout = os.Stdout
 cmd.Stderr = os.Stderr

 // Cloneflags is only available in Linux
 // CLONE_NEWUTS namespace isolates hostname
 // CLONE_NEWPID namespace isolates processes
 // CLONE_NEWNS namespace isolates mounts
 cmd.SysProcAttr = &syscall.SysProcAttr{
  Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS ,
  Unshareflags: syscall.CLONE_NEWNS | syscall.CLONE_NEWNET, 
 }

 // Run child using namespaces. The command provided will be executed inside that.
  must(cmd.Run())
}




func child(command ...string) {

 // Create cgroup
 cg()





 cmd := exec.Command(command[0], command[1:]...)

 cmd.Stdin = os.Stdin
 cmd.Stdout = os.Stdout
 cmd.Stderr = os.Stderr


 must(syscall.Sethostname([]byte("container")))


 must(syscall.Chroot("./ubuntu_fs"))
 // Change directory after chroot
 must(os.Chdir("/"))
 // Mount /proc inside container so that `ps` command works
 must(syscall.Mount("proc", "proc", "proc", 0, ""))
 // Mount a temporary filesystem
 if _, err := os.Stat("mytemp"); os.IsNotExist(err) {
  must(os.Mkdir("mytemp", os.ModePerm))
 }
 must(syscall.Mount("something", "mytemp", "tmpfs", 0, ""))




 must(cmd.Run())

 // Cleanup mount
 must(syscall.Unmount("proc", 0))
 must(syscall.Unmount("mytemp", 0))
}




func cg() {
 // cgroup location in Ubuntu
 cgroups := "/sys/fs/cgroup/"

 pids := filepath.Join(cgroups, "pids")
 containers_mini := filepath.Join(pids, "containers_mini")
 os.Mkdir(containers_mini, 0755)
 // Limit to max 20 pids
 must(ioutil.WriteFile(filepath.Join(containers_mini, "pids.max"), []byte("20"), 0700))
 // Cleanup cgroup when it is not being used
 must(ioutil.WriteFile(filepath.Join(containers_mini, "notify_on_release"), []byte("1"), 0700))

 pid := strconv.Itoa(os.Getpid())
 // Apply this and any child process in this cgroup
 must(ioutil.WriteFile(filepath.Join(containers_mini, "cgroup.procs"), []byte(pid), 0700))
}

func must(err error) {
 if err != nil {
  log.Printf("Error: %v\n", err)
   panic(err)
 }
}
this code introduced by Liz RiceUnderstanding the CodeThe main function serves as the entry point of the program. It uses command-line arguments to determine whether to run a new container or act as a child process within an existing container.func main() {
    switch os.Args[1] {
    case "run":
        run(os.Args[2:]...)
    case "child":
        child(os.Args[2:]...)
    default:
        log.Fatal("Unknown command. Use run <command_name>, like `run /bin/bash` or `run echo hello`")
    }
}

The run function sets up the container environment and executes a specified command inside it.func run(command ...string) {
   log.Println("Executing", command, "from run")
   cmd := exec.Command("/proc/self/exe", append([]string{"child"}, command[0:]...)...)
   cmd.Stdin = os.Stdin
   cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &syscall.SysProcAttr{
        Cloneflags:    syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS,
        Unshareflags:  syscall.CLONE_NEWNS | syscall.CLONE_NEWNET,
    }
}this command cmd := exec.Command(“/proc/self/exe”, append([]string{“child”}, command[0:]…)…)
make sure that it’s append all command to same process
The Cloneflags specify the namespaces to be isolated (UTS, PID, and mount namespaces).
The Unshareflags further isolate the network namespace.
The cmd.Run() method runs the provided command within the created container.
The child function is responsible for setting up the container filesystem and executing the specified command inside it.func child(command ...string) {
    // ...
    cg()
    must(syscall.Sethostname([]byte("container")))
    must(syscall.Chroot("./ubuntu_fs"))
    must(os.Chdir("/"))
    must(syscall.Mount("proc", "proc", "proc", 0, ""))
    must(syscall.Mount("something", "mytemp", "tmpfs", 0, ""))
    must(cmd.Run())
    must(syscall.Unmount("proc", 0))
    must(syscall.Unmount("mytemp", 0))
}
The cg function sets up a control group (cgroup) to limit resource usage for the container.
Sethostname sets the hostname inside the container.
Chroot changes the root directory for the container.
Mount is used to mount essential filesystems like /proc and a temporary filesystem.
Finally, the command is executed within the container.
The cg function creates and configures a cgroup for the container, limiting the number of processes.func cg() {
 // cgroup location in Ubuntu
 cgroups := "/sys/fs/cgroup/"

 pids := filepath.Join(cgroups, "pids")
 containers_mini := filepath.Join(pids, "containers_mini")
 os.Mkdir(containers_mini, 0755)
 // Limit to max 20 pids
 must(ioutil.WriteFile(filepath.Join(containers_mini, "pids.max"), []byte("20"), 0700))
 // Cleanup cgroup when it is not being used
 must(ioutil.WriteFile(filepath.Join(containers_mini, "notify_on_release"), []byte("1"), 0700))

 pid := strconv.Itoa(os.Getpid())
 // Apply this and any child process in this cgroup
 must(ioutil.WriteFile(filepath.Join(containers_mini, "cgroup.procs"), []byte(pid), 0700))
}

Cgroups are used to control and limit resource usage for processes.
In this example, the cgroup limits the maximum number of processes to 20.
The must function is a simple utility function for handling errors.func must(err error) {
    if err != nil {
        log.Printf("Error: %v\n", err)
        panic(err)
    }
}
If an error occurs, it is logged, and the program is terminated.
Building and Running the ContainerTo run the minimal container, follow these steps:Build the executable: go build -o mycontainer main.go
Create a filesystem directory with an Ubuntu root filesystem, e.g., ubuntu_fs.
Run the container: sudo ./mycontainer run /bin/bash
remember you need to run it as sudo
your entry point is /bin/bash
now you are in your own minimal container , and now you have a deep understanding , may be if i have more time in the future i will add isolation layer on network , our you can do it , thank you for your time i hopped it helped anyone .read this will help you more

namespace & golang a series of article explains namespace with go examples

“Creating Network Stacks and Connecting with the Internet” by “Shrikanta Mazumder”
on next part we will create a network layer that give our container a virtual Ethernet in isolated subset that use host bridge as gateway . see you soon]]></content:encoded></item><item><title>How to manage tool dependencies in Go 1.24+</title><link>https://www.alexedwards.net/blog/how-to-manage-tool-dependencies-in-go-1.24-plus</link><author>/u/alexedwards</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 20:36:03 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[One of my favourite features of Go 1.24 is the new functionality for managing  dependencies.By this, I mean tooling that you use to assist with development, testing, build, or deployment – such as  for static code analysis,  for vulnerability scanning, or  for live-reloading applications.Historically, managing these dependencies — especially in a team setting — has been tricky. The previous solutions have been to use a  file or the  pattern, but while these approaches work, they’ve always felt like workarounds with some downsides.With Go 1.24, there’s finally a better way. To demonstrate the new functionality, let's scaffold a simple module and add some application code.$ go mod init example.com
go: creating new go.mod: module example.com
$ touch main.go
package main

import (
    "fmt"

    "github.com/kr/text"
)

func main() {
    wrapped := text.Wrap("This is an informational message that should be wrapped.", 30)
    fmt.Println(wrapped)
}
Now fetch the  package and run the code. The output should look like this:$ go get github.com/kr/text
go: downloading github.com/kr/text v0.2.0
go: added github.com/kr/text v0.2.0
$ go run .
This is an informational
message that should be
wrapped.Go 1.24 introduces the  flag for , which you can use like this:go get -tool import_path@version
This command will download the package specified by the import path (along with any child dependencies), store them in your module cache, and record them in your  file. The  part is optional – if you omit it, the latest version will be downloaded.Let's use this to add the latest versions of  and  to our module as developer tools, along with  version .$ go get -tool golang.org/x/tools/cmd/stringer
go: downloading golang.org/x/tools v0.30.0
go: downloading golang.org/x/sync v0.11.0
go: downloading golang.org/x/mod v0.23.0
go: added golang.org/x/mod v0.23.0
go: added golang.org/x/sync v0.11.0
go: added golang.org/x/tools v0.30.0

$ go get -tool golang.org/x/vuln/cmd/govulncheck
go: downloading golang.org/x/vuln v1.1.4
go: downloading golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7
go: downloading golang.org/x/sys v0.30.0
go: upgraded golang.org/x/telemetry v0.0.0-20240521205824-bda55230c457 => v0.0.0-20240522233618-39ace7a40ae7
go: added golang.org/x/vuln v1.1.4

$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.1
go: downloading honnef.co/go/tools v0.5.1
go: downloading golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678
go: downloading github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c
go: downloading golang.org/x/exp v0.0.0-20231110203233-9a3e6036ecaa
go: added github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c
go: added golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678
go: added honnef.co/go/tools v0.5.1After running these, your  file will now include a  section listing the tools you've added. The corresponding module paths and versions for all the dependencies will appear in the  section and be marked as indirect:module example.com

go 1.24.0

require (
    github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c // indirect
    github.com/kr/text v0.2.0 // indirect
    golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678 // indirect
    golang.org/x/mod v0.23.0 // indirect
    golang.org/x/sync v0.11.0 // indirect
    golang.org/x/sys v0.30.0 // indirect
    golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7 // indirect
    golang.org/x/tools v0.30.0 // indirect
    golang.org/x/vuln v1.1.4 // indirect
    honnef.co/go/tools v0.5.1 // indirect
)

tool (
    golang.org/x/tools/cmd/stringer
    golang.org/x/vuln/cmd/govulncheck
    honnef.co/go/tools/cmd/staticcheck
)
Once added, you can run tools using the  command.To run a specific tool from the command line within your module, you can use  followed by the last non-major-version segment of the import path for the tool (which is, normally, just the name for the tool). For example:$ go tool staticcheck -version
staticcheck 2024.1.1 (0.5.1)

$ go tool govulncheck
No vulnerabilities found.The  command also works nicely if you want to execute tools from your scripts or Makefiles. To illustrate, let's create a Makefile with an  task that runs staticcheck and govulncheck on the codebase..PHONY: audit
audit:
    go vet ./...
    go tool staticcheck ./...
    go tool govulncheck
If you run , you should see that all the checks complete successfully.$ make audit
go vet ./...
go tool staticcheck ./...
go tool govulncheck
No vulnerabilities found.Let's also take a look at an example where we use the stringer tool in conjunction with  to generate  methods for some  constants.package main

import (
    "fmt"

    "github.com/kr/text"
)

//go:generate go tool stringer -type=Level

type Level int

const (
    Info Level = iota
    Error
    Fatal
)

func main() {
    wrapped := text.Wrap("This is an informational message that should be wrapped.", 30)

    fmt.Printf("%s: %s\n", Info, wrapped)
}
The important thing here is the  line. When you run  on this file, it will in turn use  to execute the version of the stringer tool listed in your  file.$ go generate .
$ ls 
go.mod  go.sum  level_string.go  main.go  MakefileYou should see that a new  file is created, and running the application should result in some output that looks like this:$ go run .
Info: This is an informational
message that should be
wrapped.You can check which tools have been added to a module by running , like so:$ go list tool
honnef.co/go/tools/cmd/staticcheck
golang.org/x/tools/cmd/stringer
golang.org/x/vuln/cmd/govulncheckBecause the tools are included in your  file as dependencies, if you want to check that the code for the tools stored in your module cache has not changed you can simply run :$ go mod verify
This will check that the code in your module cache exactly matches the corresponding checksums in your  file.If you run , the code for tooling dependencies will be included in the  folder and the  manifest alongside your non-tool dependencies.$ go mod vendor
$  tree  -L 3
.
├── go.mod
├── go.sum
├── main.go
├── Makefile
└── vendor
        ├── github.com
        │   ├── BurntSushi
        │   └── kr
        ├── golang.org
        │   └── x
        ├── honnef.co
        │   └── go
        └── modules.txtWhen tools are vendored in this way, running  will execute the corresponding code in the  directory. Note that  does not work on vendored code.To upgrade or downgrade a specific tool to a specific version, you can use the same go get -tool import_path@version command that you did for adding the tool originally. For example:$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.0
To upgrade to the latest version of a specific tool, omit the  suffix. $ go get -tool honnef.co/go/tools/cmd/staticcheck
You can also upgrade  to their latest version by running . Note:  is a sub-command here, not a flag.If your tool dependencies are vendored, you will need to re-run  after any upgrades or downgrades.At the time of writing, I'm not aware of any easy way to specifically list the tools that have upgrades available – if you know of one please let me know!To remove the tool completely from your module, use  with the special version tag .$ go get -tool honnef.co/go/tools/cmd/staticcheck@none
Again, if you're vendoring, make sure to run  after removing a tool.A Reddit commenter mentioned the potential for problems if your tools share dependencies with your application code. For example, let's say that your application code depends on  version , and is tested and known to work with that version. Then if you add a tool that relies on a  version of , the version number in your  file will be bumped to the newer version and your application code will use that newer version too.In theory, this  be a problem so long as all your dependencies and their child dependencies are stable, follow strict semantic versioning, and don't make backwards-incompatible changes without a major version increment. But, of course, the real world is messy and backwards-incompatible changes  happen, which could unexpectedly break your application code.It's worth noting that this issue isn't limited to tool dependencies – the same thing can happen if your application code and a non-tool dependency both rely on the same package. However, including tools in  increases the risk.To reduce this risk, you can use a separate modfile for tool dependencies instead of including them in your main . You can do this with the  flag, specifying an alternative file such as , like so:# Initialize a go.tool.mod modfile
$ go mod init -modfile=go.tool.mod example.com

# Add a tool to the module
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck

# Run the tool from the command line
$ go tool -modfile=go.tool.mod govulncheck

# List all tools added to the module
$ go list -modfile=go.tool.mod tool

# Verify the integrity of the tool dependencies
$ go mod verify -modfile=go.tool.mod

# Upgrade or downgrade a tool to a specific version
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@v1.1.2

# Upgrade all tools to their latest version
$ go get -modfile=go.tool.mod tool

# Remove a tool from the module
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@none
]]></content:encoded></item><item><title>Geoblocking the UK with Debian &amp; Nginx</title><link>https://aphyr.com/posts/379-geoblocking-the-uk-with-debian-nginx</link><author>Aphyr</author><category>dev</category><category>go</category><pubDate>Thu, 20 Feb 2025 19:45:55 +0000</pubDate><source url="http://aphyr.com/posts.atom">Aphyr</source><content:encoded><![CDATA[A few quick notes for other folks who are geoblocking the UK. I just set up a basic geoblock with Nginx on Debian. This is all stuff you can piece together, but the Maxmind and Nginx docs are a little vague about the details, so I figure it’s worth an actual writeup. My Nginx expertise is ~15 years out of date, so this might not be The Best Way to do things. YMMV.First, register for a free MaxMind account; you’ll need this to subscribe to their GeoIP database. Then set up a daemon to maintain a copy of the lookup file locally, and Nginx’s GeoIP2 module:apt install geoipupdate libnginx-mod-http-geoip2
Create a license key on the MaxMind site, and download a copy of the config file you’ll need. Drop that in . It’ll look like:AccountID XXXX
LicenseKey XXXX
EditionIDs GeoLite2-Country
The package sets up a cron job automatically, but we should grab an initial copy of the file. This takes a couple minutes, and writes out /var/lib/GeoIP/GeoLite2-Country-mmdb:The GeoIP2 module should already be loaded via /etc/nginx/modules-enabled/50-mod-http-geoip2.conf. Add a new config snippet like /etc/nginx/conf.d/geoblock.conf. The first part tells Nginx where to find the GeoIP database file, and then extracts the two-letter ISO country code for each request as a variable. The  part sets up an  variable, which is set to  for GB, otherwise .geoip2 /var/lib/GeoIP/GeoLite2-Country.mmdb {
  $geoip2_data_country_iso_code country iso_code;
}

map $geoip2_data_country_iso_code $osa_geoblocked {
  GB      1;
  default 0;
}
Write an HTML file somewhere like /var/www/custom_errors/osa.html, explaining the block. Then serve that page for HTTP 451 status codes: in /etc/nginx/sites-enabled/whatever, add:server {
  ...
  # UK OSA error page
  error_page 451 /osa.html;
  location /osa.html {
    internal;
    root /var/www/custom_errors/;
  }

  # When geoblocked, return 451
  location / {
    if ($osa_geoblocked = 1) {
      return 451;
    }
  }
}
Test your config with , and then . You can test how things look from the UK using a VPN service, or something like locabrowser.This is, to be clear, a bad solution. MaxMind’s free database is not particularly precise, and in general IP lookup tables are chasing a moving target. I know for a fact that there are people in non-UK countries (like Ireland!) who have been inadvertently blocked by these lookup tables. Making those people use Tor or a VPN , but I don’t know what else to do in the current regulatory environment.]]></content:encoded></item><item><title>Rate my photo manipulation tool</title><link>https://www.reddit.com/r/golang/comments/1iu6pch/rate_my_photo_manipulation_tool/</link><author>/u/tunerhd</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 19:16:41 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/tunerhd ]]></content:encoded></item><item><title>Call Center Wondr by BNI 0821-4448-0002</title><link>https://dev.to/vanglevan/call-center-wondr-by-bni-0821-4448-0002-1pjn</link><author>Vang LE</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 16:08:31 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Nomor Resmi Call Center Wondr BNI 0821-4448-0002. Untuk informasi lebih lanjut, nasabah dapat menghubungi BNI Call Center di 0821-4448-0002.Untuk mendapatkan informasi lebih lanjut mengenai gangguan pada bni mobile atau wondr by bni, nasabah dapat menghubungi CS Wondr BNI 0821-4448-0002.]]></content:encoded></item><item><title>C equivalent of select() / poll() in go socket programming</title><link>https://www.reddit.com/r/golang/comments/1iu1ugj/c_equivalent_of_select_poll_in_go_socket/</link><author>/u/ChestPainGuy</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 16:00:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, I'm fairly new to socket programming and go, so forgive my ignorance.Recently, I have been reading up Beej's guide to network programming, where he explains the use of  and  to read and write to multiple sockets without blocking.I have googled quite a bit, but almost every tutorial or go example on the basics of socket connections just spawn a new goroutine with something like .So whats' the equivalent of  in go?Is spawning a goroutine for every connection an effective approach?Any good links to network programming in go would be appreciated if this question is too dumb. Thanks]]></content:encoded></item><item><title>ErrGroup: Unlocking Go&apos;s Concurrency Power</title><link>https://dev.to/leapcell/errgroup-unlocking-gos-concurrency-power-3g2h</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:43:30 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[ is a utility in the official Go library  used for concurrently executing multiple  and handling errors. It implements  based on , providing more powerful functions for concurrent programming.Compared with ,  has the following advantages::  is only responsible for waiting for the  to complete and does not handle return values or errors. While  cannot directly handle return values, it can immediately cancel other running  when a  encounters an error and return the first non- error in the  method.:  can be used in conjunction with . When a  encounters an error, it can automatically cancel other , effectively controlling resources and avoiding unnecessary work.Simplifying Concurrent Programming: Using  can reduce the boilerplate code for error handling. Developers do not need to manually manage error states and synchronization logic, making concurrent programming simpler and more maintainable.Limiting the Number of Concurrency:  provides an interface to limit the number of concurrent  to avoid overloading, which is a feature that  does not have.
  
  
  Example of Using sync.WaitGroup
Before introducing , let's first review the usage of .$ go run examples/main.go
fetch url http://www.google.com/ status 200 OK
fetch url http://www.golang.org/ status 200 OK
Error: Get "http://www.somestupidname.com/": dial tcp: lookup www.somestupidname.com: no such host
Typical idiom of :
  
  
  Example of Using errgroup.Group
The usage pattern of  is similar to that of .$ go run examples/main.go
fetch url http://www.google.com/ status 200 OK
fetch url http://www.golang.org/ status 200 OK
Error: Get "http://www.somestupidname.com/": dial tcp: lookup www.somestupidname.com: no such host
 provides  to add a cancellation function.$ go run examples/withcontext/main.go
Error:  Get "http://www.somestupidname.com/": dial tcp: lookup www.somestupidname.com: no such host
fetch url http://www.google.com/ status 200 OK

  
  
  Limiting the Number of Concurrency
 provides  to limit the number of concurrently executing .$  go run examples/main.go
Goroutine 3 is starting
Goroutine 1 is starting
Goroutine 2 is starting
Goroutine 2 is done
Goroutine 1 is done
Goroutine 5 is starting
Goroutine 3 is done
Goroutine 6 is starting
Goroutine 4 is starting
Goroutine 6 is done
Goroutine 5 is done
Goroutine 8 is starting
Goroutine 4 is done
Goroutine 7 is starting
Goroutine 9 is starting
Goroutine 9 is done
Goroutine 8 is done
Goroutine 10 is starting
Goroutine 7 is done
Goroutine 10 is done
All goroutines complete.
 provides  to try to start a task, which needs to be used in conjunction with .$ go run examples/main.go
Goroutine 1 started successfully
Goroutine 1 is starting
Goroutine 2 is starting
Goroutine 2 started successfully
Goroutine 3 started successfully
Goroutine 4 could not start (limit reached)
Goroutine 5 could not start (limit reached)
Goroutine 6 could not start (limit reached)
Goroutine 7 could not start (limit reached)
Goroutine 8 could not start (limit reached)
Goroutine 9 could not start (limit reached)
Goroutine 10 could not start (limit reached)
Goroutine 3 is starting
Goroutine 2 is done
Goroutine 3 is done
Goroutine 1 is done
All goroutines complete.

  
  
  Source Code Interpretation
The source code of  mainly consists of 3 files:: An empty structure used to pass signals to control the number of concurrency.:

: The function called when the context is cancelled.: The internally used .: The signal channel that controls the number of concurrent coroutines.: Ensures that the error is handled only once.: Records the first error.: Limits the number of concurrency.
: Starts a new coroutine to execute the task.
: Waits for all tasks to complete and returns the first error.
: Tries to start a task.
 is an official extended library that adds error handling capabilities on the basis of , providing functions such as synchronization, error propagation, and context cancellation. Its  method can add a cancellation function,  can limit the number of concurrency, and  can try to start a task. The source code is ingeniously designed and worthy of reference. Finally, I would like to recommend the most suitable platform for deploying golang: 
  
  
  1. Multi-Language Support
Develop with JavaScript, Python, Go, or Rust.

  
  
  2. Deploy unlimited projects for free
pay only for usage — no requests, no charges.
  
  
  3. Unbeatable Cost Efficiency
Pay-as-you-go with no idle charges.
Example: $25 supports 6.94M requests at a 60ms average response time.

  
  
  4. Streamlined Developer Experience
Intuitive UI for effortless setup.
Fully automated CI/CD pipelines and GitOps integration.
Real-time metrics and logging for actionable insights.

  
  
  5. Effortless Scalability and High Performance
Auto-scaling to handle high concurrency with ease.
Zero operational overhead — just focus on building.
]]></content:encoded></item><item><title>Paginación con cursor</title><link>https://dev.to/gaston_duarte/paginacion-con-cursor-341d</link><author>Gaston Duarte</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:40:23 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Al desarrollar aplicaciones que muestran grandes volúmenes de datos, es fundamental implementar técnicas de paginación para mejorar el rendimiento y la experiencia del usuario. En un e-commerce, por ejemplo, podría haber miles de productos en la base de datos, pero el frontend solo necesita mostrar 10 a la vez.Enviar todos los registros al frontend para que los almacene en memoria no es una solución eficiente. En su lugar, el backend debe gestionar la paginación de manera efectiva.Existen dos enfoques principales para paginar datos:  y .Este método utiliza un  para determinar desde qué registro comenzar la consulta.Veamos un ejemplo simple:Supongamos que tenemos 25 registros en nuestra base de datos, y tenemos una vista que quiere mostrar esos registros de a 10.Entonces el frontend nos enviara:{
  "limit": 10,
  "offset": 0
}
El backend hará una consulta a la base de datos:SELECT * FROM nombre_tabla LIMIT 10 OFFSET 0Es decir, mostrar 10 registros desde el registro 0 (del 0..9). 
Para obtener los siguientes 10 registros:{
  "limit": 10,
  "offset": 10
}
SELECT * FROM nombre_tabla LIMIT 10 OFFSET 10Eso mostrara desde el registro 10 al 19 y así sucesivamente.
  
  
  Problemas de Baja eficiencia en grandes volúmenes de datos: Consultas con  grande pueden volverse costosas, ya que la base de datos debe recorrer muchos registros antes de devolver los deseados.Inconsistencias en datos dinámicos: Si se agregan o eliminan registros, la paginación puede omitir o duplicar registros.Aquí es donde aparece la paginación con cursor, la cual hace que nuestras consulta a la base de datos sean mucho mas performantes.En vez de enviar el campo , utilizaremos un campo .
  
  
  ¿Y como definimos cual es nuestro cursor?
Bien, la respuesta es depende. Depende de qué datos tenemos guardados en nuestro registro, vayamos al caso mas simple, tener un identificador único auto-incremental.Supongamos que nuestro registro tiene estos datos:{
  "id": 1,
  "nombre": "Juan"
},
{
  "id": 2,
  "nombre": "Patricia"
},
...
El frontend solicita el primer registro con  (sin cursor) y el backend responderá ademas del registro a mostrar cual es el cursor que debe enviar en la siguiente request:{
  "limit": 1,
  "cursor": 2
  "user":{
       "id": 1,
       "nombre": "Juan"
  }
}
Consulta a la base de datos:SELECT * FROM nombre_tabla LIMIT 1En la siguiente request que haga el frontend enviara,  y , entonces desde el backend podremos hacer una consulta a la base de datos de este estilo:SELECT * FROM nombre_tabla WHERE id >= 2 ORDER BY id LIMIT 1Lo cual traerá a partir del registro que contenta id >= 2 y solamente 1.
  
  
  ¿Cual es la ventaja sobre offset?
: No se recorren registros innecesarios. Simplemente lo limitamos en el .: No se ven afectados registros por insert o delete.
  
  
  Ahora, ¿Qué sucede si no tenemos un id auto-incremental y ordenado?
Si los registros tienen un  en lugar de un ID incremental, se puede utilizar otro campo como cursor, por ejemplo, :Por ejemplo, supongamos registros de este estilo:{
  "uuid": "asdn1029nc",
  "nombre": "Juan",
  "fecha_nacimiento": "2003-02-21"
},
{
  "uuid": "sap0238gh",
  "nombre": "Patricia",
  "fecha_nacimiento": "2002-11-04"
},
...
Para utilizar  debemos ordenar nuestros registros por algún campo, en este caso la fecha de nacimiento, entonces ahora si el frontend nos pide un registro en la primer request, desde el backend devolveremos:{
  "limit": 1,
  "cursor": "2002-11-04"
  "user":{
       "uuid": "sap0238gh",
       "nombre": "Patricia",
       "fecha_nacimiento": "2002-11-04"
  }
},
...
Nuestra consulta a la base de datos será:SELECT * FROM nombre_tabla WHERE fecha_nacimiento > '2002-11-04' ORDER BY fecha_nacimiento LIMIT 1
  
  
  ¿Y que sucede si hay dos registros con la misma fecha, vamos a perder registros?
Bueno, ahi es donde podemos concatenar dos campos del registro para utilizarlo como cursor para asegurarnos de no perder registros, por ejemplo: cursor=fecha_nacimiento+uuid. Importante siempre en la consulta hacer un order by cursor, fecha_nacimiento.
  
  
  Seguridad: Encodear el cursor
Es importante utilizar un  en  de nuestro cursor para evitar un . 
Este puede ser un ejemplo de código en Go para encodear el cursor:func DecodeCursor(encoded string) (string, string, error) {
    data, err := base64.StdEncoding.DecodeString(encoded)
    if err != nil {
        return "", "", err
    }
    parts := strings.Split(string(data), "|")
    if len(parts) != 2 {
        return "", "", fmt.Errorf("cursor inválido")
    }
    return parts[0], parts[1], nil
}

func EncodeCursor(creationDate string, reportID string) string {
    cursorData := fmt.Sprintf("%s|%s", creationDate, reportID)
    return base64.StdEncoding.EncodeToString([]byte(cursorData))
}
Si bien  es sencilla y funciona bien con pocos registros,  es mucho más eficiente para grandes volúmenes de datos y evita inconsistencias. Dependiendo del caso, se puede utilizar un ID incremental o una combinación de campos como cursor para garantizar un correcto orden y rendimiento.]]></content:encoded></item><item><title>Slice Internals in Go: How the Runtime Expands Slices Efficiently</title><link>https://themsaid.com/slice-internals-in-go</link><author>/u/themsaid</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 15:21:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[The deeper you delve into Go’s internals, the more evident it becomes that its creators carefully engineered the language to strike a precise balance between performance and flexibility. This delicate equilibrium influences many of Go’s core features, including its approach to memory management and data structures. One standout example of this thoughtful design is the implementation of slice growth. Through this approach, Go ensures that slices expand seamlessly, optimizing both performance and memory usage without compromising ease of use.In Go, a slice is a lightweight data structure that serves as a window into a contiguous block of memory where elements of a specific type are stored. At its core, a slice doesn’t directly contain the data itself but instead holds a pointer to an underlying array (know as the backing array.)When the Go runtime creates a slice, as in this example, it constructs a small struct under the hood, defined in the runtime as : {
	 unsafe.
}The  type has the following fields: holds a pointer to the underlying array. stores the number of elements in the slice. stores the capacity of the array.The  type used for the  field is a generic pointer type that bypasses Go's type safety rules. Since the size of an array in Go is part of the type, the runtime uses  so it can replace the array with a larger one when needed.The code in the example above creates a slice that has zero elements with a backing array that can hold 10 elements of type . We can later fill that array with elements by using the  function:Here, we append a byte to the empty array represented by the unsigned 8-bit integer .When appending elements to a slice, the Go runtime first checks whether the backing array has enough capacity to accommodate the new elements. If it does, the elements are simply added to the existing array. However, if the current array lacks sufficient space, the runtime allocates a larger backing array, copies the existing elements into it, and then appends the new elements.([], , )

(, ) (, ) The capacity of the newly allocated array is determined by several factors, including the current array’s capacity, the type of elements it holds, and the number of new elements being appended. These factors influence how much the array grows, ensuring efficient memory usage while minimizing the need for frequent reallocations.The runtime begins by attempting to double the existing capacity as the first step in determining the new array size:If the total number of existing and newly appended elements exceeds the doubled capacity, the runtime sets the new capacity to match the required number of elements: {
    
}This ensures that the new capacity is larger than or equals to the number of elements after the appending operation.([], , )

(, , , )

.(()) In this example, the integer slice initially had a capacity of 1. After adding three elements, the runtime allocated a new backing array with a capacity of 3. This happened because doubling the original capacity (1 * 2) was insufficient to accommodate the new elements, prompting the runtime to adjust the capacity accordingly.If doubling the capacity is sufficient, the runtime further evaluates whether allocating such a large array is efficient or merely a waste of memory.For small slices, capacity less than 256, the runtime employs a simple doubling strategy (e.g., 2 to 4, 4 to 8, 8 to 16.) This makes sense for small workloads: doubling ensures plenty of headroom for future appends. However, as the slice’s capacity climbs into more than 256, or beyond, doubling becomes less practical. Doubling a capacity of, say, 10,000 to 20,000 allocates an extra 10,000 elements’ worth of memory (potentially tens or hundreds of kilobytes, depending on the element size) which might sit unused for a long time.To address this, the runtime adjusts its growth strategy for larger slices by reducing the growth factor gradually until it reaches 1.25. This slower growth means that if a slice already has a capacity of, say, 512, adding a few elements doesn’t balloon it to 1024; it might rise to 832 instead (a 62.5% increase). The key insight is that a larger slice can absorb more appends before hitting its capacity limit. For instance, a slice with a capacity of 512 has room for 512 more elements if empty, compared to just 8 for a capacity of 8. This naturally delays the need for reallocation.This conservative approach aims to curb excessive memory usage. By growing incrementally rather than exponentially, the runtime avoids reserving vast swaths of memory that might remain idle, which is critical in applications handling large datasets or with limited resources (e.g., embedded systems). However, there’s a flip side: smaller growth steps mean the slice fills up sooner, triggering reallocation more often. Each reallocation involves CPU work (allocating memory, copying the existing elements, and updating the slice’s pointer) which can add up if appends are frequent.The runtime’s strategy thus balances these two forces: memory footprint versus CPU overhead. It leans toward saving memory at the cost of potentially more frequent (but smaller) reallocations, betting that the trade-off pays off in most real-world scenarios where slices don’t grow indefinitely.When determining how a slice should grow, the Go runtime takes into account the type of elements stored in the array, as this directly impacts memory allocation. On 64-bit systems, memory is generally allocated in chunks of 8 bytes. Any allocation that does not align with this rule is rounded up to the nearest multiple of 8 to ensure efficient memory usage and alignment.Let's say we create a slice of bytes with capacity zero and then append an element to it:([], , )

(, )

.(()) After the growth, the capacity of the slice becomes 8 (8 bytes). If the element type was a 64-bit integer instead, the growth will increase the capacity to 1 (1 * 64 bits = 8 bytes):([], , )

(, )

.(()) If the element type was a 32-bit integer, the growth will increase the capacity to 2 (2 * 32 bits = 8 bytes):([], , )

(, )

.(()) The reason is that modern CPUs, particularly on 64-bit systems, operate most efficiently when data is aligned to their word size (the amount of data they can process in one cycle.) On a 64-bit system, the word size is 64 bits, or 8 bytes. If the runtime allocates, say, 5 bytes, the CPU  and masks off the unused portion. That means, allocating in 8-byte multiples ensures the entire chunk is usable without waste or extra work.In addition, CPU caches fetch memory in 64-byte lines (8 words of 8 bytes each.) Multiples of 8 bytes fit neatly into these lines, reducing cache misses and improving locality when accessing sequential data, like a slice’s backing array.In addition to the 8-byte chunk allocation rule, the Go runtime maintains a table of predefined constants to guide its memory allocation decisions. This table categorizes memory allocations into specific size classes, helping the runtime minimize fragmentation and efficiently reuse freed memory blocks.The table looks like this:+---------+-------+
| Class   | Value |
+---------+-------+
| Class  |      |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |    |
| .     | .   |
+---------+-------+This size class allocation table functions as an efficient lookup mechanism for managing memory allocation and deallocation. When a memory block belonging to a specific size class is freed, the runtime stores it in the table rather than immediately returning it to the operating system. Later, if a request is made for a memory block of the same size class, the system can quickly retrieve and reuse the previously freed block instead of performing an extensive search through physical memory to find a suitable allocation.+---------------------------------+
|  Freed memory  size class X  |
+---------------------------------+
        │
        ▼
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
        │
        ▼
+-------------------------------------+
| Incoming Memory Allocation Request  |
+-------------------------------------+
        │
          (Lookup in Table)
+-------------------------------+
| Matching Freed Block Found    |
+-------------------------------+
        │
          (Reused Instead of )
+----------------------------+
|  Allocated to Application  |
+----------------------------+With that in mind, the runtime not only rounds up to the nearest 8-byte boundary but also rounds up to the nearest size class in the allocation table.Consider this slice operation:([], , )

(, , , , , )Here, the slice starts with zero capacity and we add 5 elements of type . Without considering the size class allocation table, the runtime would allocate 40 bytes for the new backing array: *  bits =  bits /  =  bytesInstead, the runtime consults the class allocation table and rounds up to the nearest match (48 in this case). As a result, it allocates a backing array with a capacity of 6 (48 bytes / 64 bits), even though the new array would only need to hold 5 elements (that require only 40 bytes).This approach significantly improves performance by reducing external fragmentation (where free memory is scattered in small, non-contiguous blocks, making larger allocations difficult.) It also minimizes allocation overhead and speeds up memory access by eliminating the need to repeatedly request new memory from the operating system.In summary, the Go runtime takes several key factors into account when growing an array:: It starts with a doubling factor (2x) and then gradually winds down to 1.25x.: The array is rounded up to the nearest 8-byte boundary.The Size Class Allocation Table: The runtime rounds up to the nearest available class in the table.I really admire the thoughtful work the Go team has put into making the language both efficient and flexible. It's clear that a lot of careful consideration went into optimizing performance while maintaining flexibility for developers.]]></content:encoded></item><item><title>Building Mobile Apps Without a Backend: The Power of Database Gateway API</title><link>https://dev.to/muhammetberdi_jepbarov/building-mobile-apps-without-a-backend-the-power-of-database-gateway-api-19m0</link><author>Muhammetberdi Jepbarov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:21:03 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  The Pain of Backend Development
If you've ever built a mobile or web app that interacts with a database, you know the struggle—designing API endpoints, handling authentication, writing business logic, and ensuring scalability. Sometimes, all you need is a simple way to query the database and retrieve structured JSON responses without going through the entire process of backend development.That’s exactly why many years ago I built the —a solution that allows developers to interact directly with PostgreSQL and MSSQL databases via a secure API, without writing a full-fledged backend.
  
  
  The Idea Behind Database Gateway API
The goal was simple: Why should you have to build an entire backend when all you need is an API for your database?I needed a lightweight yet powerful solution that could:Eliminate the need for a dedicated backend in simple applications.Enable mobile and web apps to access database data seamlessly.Provide an easy way to deploy APIs for database interactions with minimal setup.Integrate with any existing system, whether it’s an enterprise  or a retail point-of-sale (POS) system.This led to the creation of the —a framework-agnostic API layer that acts as a bridge between your database and applications.The  is a standalone service that connects to your  or  database and exposes SQL queries as API endpoints. It takes care of request parsing, security, and response formatting, allowing you to focus on building your application instead of writing backend logic.✅ Direct SQL Query Execution: Supports SELECT, INSERT, UPDATE, and DELETE operations via API requests.Automatic JSON Responses: Converts database query results into well-structured JSON.Easy Integration with Mobile & Web Apps: No need for a complex backend—just plug it into your frontend. Set up role-based access, API keys, and rate limiting.Supports Complex Queries & Joins: Fetch relational data easily, just like using SQL directly.Minimal Deployment Overhead: Run it as a standalone service or containerized in Docker.  
  
  
  Real-World Impact: 29 Deployments & 150+ Devices
This API has been successfully deployed across , powering over , primarily in . It integrates seamlessly with mobile apps and existing accounting systems, allowing businesses to:Sync sales data in real-time, eliminating manual record-keeping.Improve customer experience by linking mobile apps to inventory and POS systems.Enable effortless data exchange between different applications without writing additional backend logic.If you’re a developer building a mobile app, web dashboard, or prototype, the  can save you weeks of development time by handling database queries and responses automatically.Instead of spinning up a full backend, setting up ORM models, and writing CRUD endpoints, you simply install the gateway, configure it with your database, and start making API requests.✅ Instant API – Set up in minutes and start making SQL queries right away.
✅ Effortless Integration – Works with mobile, web, and desktop apps.
✅ JSON-Formatted Responses – Your queries return well-structured JSON, ready to use.
✅ Perfect for Prototyping – Quickly test database interactions without a full backend.
✅ Optimized for Performance – Execute fast queries with minimal setup.🚀 How It Works
🔍 Querying Your DatabaseNeed to fetch data? Just send a POST request with your SQL query:POST 127.0.0.1:8000/api/v1/make-db-request
{
  "query_string": "SELECT * FROM tbl_mg_materials",
  "base64_columns": ["group_code", "image_pict", "firm_id_guid"]Pro tip: Use base64_columns to encode image BLOBs or sensitive data!
📊 The ResponseThe API returns a structured JSON response:{
  "data": [...], 
  "total": 2, 
  "message": "db query result"
}⚠️ Important Considerations🔴 Security First! – This API executes raw SQL, so make sure to restrict access and validate inputs.
🔴 Database Changes? – Schema updates might require adjustments to your API queries.
🔴 Use with Caution – Best for internal tools, rapid prototyping, and trusted environments.Want to try it out? The  is open-source and available for anyone to use and contribute to. You can integrate it into your next project and cut down on backend development time significantly.Check out the repository: 
Let me know if you have questions or ideas for improvements—happy coding! 🚀]]></content:encoded></item><item><title>Как я разработал библиотеку Viewscount: Решение проблемы органического подсчета просмотров в Golang</title><link>https://dev.to/muhammetberdi_jepbarov/kak-ia-razrabotal-bibliotieku-viewscount-rieshieniie-probliemy-orghanichieskogho-podschieta-prosmotrov-v-golang-1jn0</link><author>Muhammetberdi Jepbarov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 14:17:58 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Как разработчики, мы часто сталкиваемся с необходимостью отслеживания просмотров контента на различных платформах. Будь то отслеживание просмотров для статей, видео или продуктов, большинство приложений полагаются на простую колонку  в своих таблицах базы данных. Проблема возникает, когда вы не хотите писать отдельное API для каждой таблицы, которая нуждается в подсчете просмотров, и при этом важно обеспечить безопасность от таких атак, как DoS (Denial of Service). Это та проблема, с которой я столкнулся, и причина, по которой я создал библиотеку .В большинстве приложений просмотры отслеживаются в той или иной форме — будь то для блога, страницы продукта или профиля пользователя. Однако создание решения, которое было бы масштабируемым и безопасным, может быть сложной задачей. Вы можете выбрать путь написания индивидуальных API для каждой из этих таблиц, но это отнимает много времени и не масштабируется. А что если можно отслеживать просмотры, не пиша API для каждой новой таблицы? Что если существует более эффективный способ интеграции этой функциональности в любое приложение? Именно в этот момент я начал работать над созданием .
  
  
  Необходимость единого решения
Когда вы строите систему с отслеживанием просмотров, одним из важных факторов является сохранение достоверности данных. В некоторых случаях злоумышленники могут попытаться искусственно увеличить счетчик просмотров, делая повторные запросы — здесь важно предотвратить такие атаки, как DoS (Denial of Service). Без надежного решения ваше приложение становится уязвимым для подобных атак, что подрывает достоверность данных.Так появилась идея создания библиотеки . Я хотел создать решение, которое позволило бы легко отслеживать органические просмотры для любых таблиц, независимо от фреймворка, и при этом предоставить возможность простого интегрирования этого решения в любое приложение, защищая от ненадежных инкрементов. Эта библиотека разработана для того, чтобы интегрироваться с любым Golang фреймворком, таким как ,  или даже базовым , и быть независимой от базы данных, поддерживая PostgreSQL.Когда вы добавляете  в свое приложение, вы можете начать отслеживать просмотры сразу — без необходимости писать отдельные API для каждой таблицы. Допустим, у вас есть таблицы, такие как  или , каждая из которых имеет колонку . Вместо того чтобы создавать API специально для инкрементации счетчика просмотров каждый раз, когда пользователь просматривает страницу, вы можете использовать  для автоматического отслеживания этого процесса.Вот как это работает на практике:В вашей базе данных просто добавьте колонку  в ваши таблицы:Аналогично можно сделать и для других таблиц. Не нужно писать сложные запросы или API — просто добавьте эту колонку для отслеживания просмотров.
  
  
  Интеграция Viewscount в ваше приложение
Самая лучшая часть библиотеки  заключается в том, что она является независимой от фреймворков. Независимо от того, используете ли вы ,  или даже , вы можете легко интегрировать ее в ваше приложение. Вот как вы можете добавить ее как middleware в :Сначала нужно инициализировать view tracker в вашем приложении:Как только middleware настроено, вы можете добавить его к вашим маршрутам. Вот как это делается:Эта простая настройка интегрирует библиотеку  в ваше приложение. Middleware будет автоматически отслеживать просмотры на указанной таблице и увеличивать значение в колонке  по мере необходимости, при этом обеспечивая защиту от атак.: Всего несколько строк кода — и вы уже отслеживаете просмотры на любой из ваших таблиц.Независимость от фреймворков: Библиотека разработана для работы с любым Golang фреймворком, что позволяет использовать ее в проекте независимо от стека.: Ограничивая быстрые запросы, которые могут искусственно увеличить счетчик просмотров,  гарантирует, что данные остаются достоверными и точными.Эффективность и масштабируемость: Не нужно создавать кастомные API или сложные запросы — используйте библиотеку и отслеживайте просмотры в реальном времени. решает общую проблему разработки: как отслеживать просмотры на различных таблицах, обеспечивая безопасность и эффективность, при этом не писать отдельные API для каждой новой таблицы. Легкость интеграции, защита от атак и независимость от фреймворков делают ее мощным инструментом для разработчиков, создающих современные Golang-приложения.Не стесняйтесь внести вклад в проект и сообщать о проблемах или предложениях через GitHub Viewscount. Я буду рад увидеть, как эта библиотека поможет другим в их разработке!]]></content:encoded></item><item><title>How I Developed the Viewscount Library: Solving the Problem of Organic View Counting in Golang</title><link>https://dev.to/muhammetberdi_jepbarov/how-i-developed-the-viewscount-library-solving-the-problem-of-organic-view-counting-in-golang-1684</link><author>Muhammetberdi Jepbarov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 14:16:56 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[As developers, we often encounter the need to track views on content across various platforms. Whether it's tracking views for articles, videos, or products, most applications rely on a simple  column in their database tables. The challenge arises when you don’t want to go through the hassle of writing a separate API for each table that needs view tracking, while also ensuring that your system is secure from abuse like DoS (Denial of Service) attacks. This is the problem I encountered and the reason I created the  library.In most applications, views are tracked in some form—whether it’s for a blog post, a product listing, or even user profile pages. However, implementing a solution that is both scalable and secure can become cumbersome. You could go the route of writing custom APIs for each of these tables, but that’s time-consuming and doesn’t scale well. What if there was a way to track these views without rewriting APIs for every new table? What if there was a more efficient way to integrate this functionality into any app? That’s when I started working on .
  
  
  The Need for a Unified Solution
When building any system with views tracking, one of the critical concerns is ensuring that the view count remains authentic. In some cases, malicious users may attempt to artificially inflate the view count through repeated requests—this is where preventing DOS (Denial of Service) type increments comes into play. Without a solid solution, your application becomes vulnerable to these types of attacks, undermining the integrity of your view count.Thus, the idea for  was born. I wanted a way to seamlessly count organic views across any tables, regardless of the framework, while providing an easy-to-integrate middleware solution that prevents abusive increments. This library is designed to integrate with any Golang web framework, like , , or even basic , and is database-agnostic, supporting PostgreSQL seamlessly.When you add  to your application, you can begin tracking views instantly—without having to write separate APIs for each table. Let’s say you have tables like  or , each with a  column. Instead of building an API specifically to increment the  each time a user views a page, you can use  to track this automatically.Here’s how it works in practice:In your database, you simply add the  column to your tables like so:The same goes for your other tables. No need for complicated queries or APIs—just a simple column to track views. 
  
  
  Integrating Viewscount into Your Application
The best part of  is that it’s designed to be . Whether you're using , , or even , you can easily integrate it into your application. Here’s how you can add it as middleware in :First, you’ll need to initialize the view tracker in your app:Once you’ve set up the middleware, you can add it to your routes. Here’s how you do it:This simple setup integrates the  library into your existing app. The middleware will automatically track views on the specified table and increment the  column as needed, while also providing protection against abuse.
  
  
  The Benefits of Viewscount
: With just a few lines of code, you can integrate view counting into any of your existing tables.: The library is designed to work with any Golang web framework, making it easy to use in your project regardless of the stack.Prevention of DOS Attacks: By limiting rapid requests that could artificially inflate view counts,  ensures that the data remains reliable and accurate.: No need for custom APIs or complex queries—just use the library and track views automatically in real time. solves a common problem in application development: how to track views across tables securely and efficiently without having to build a separate API for each table. Its easy integration, ability to prevent DOS-style attacks, and framework-agnostic design make it a powerful tool for developers building modern Golang applications.Feel free to contribute to the project and raise issues or suggestions via Viewscount GitHub. I’d love to see how this library can help others in their development journey!]]></content:encoded></item></channel></rss>