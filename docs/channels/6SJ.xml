<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Go</title><link>https://www.awesome-dev.news</link><description></description><item><title>File System Walking with WalkDir: Recursive Tree Traversal 4/9</title><link>https://dev.to/rezmoss/file-system-walking-with-walkdir-recursive-tree-traversal-49-dj3</link><author>Rez Moss</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 21 Jun 2025 15:36:00 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  WalkDir Function Comprehensive Guide
The  function represents Go's modern approach to recursive directory traversal, replacing the older  function with improved performance and cleaner interface design. Understanding its mechanics is essential for any developer working with file system operations at scale.
  
  
  Function Signature and Parameters
The function signature deliberately keeps things simple. The  parameter accepts any valid file system path - whether it points to a file or directory. When you pass a file path,  processes only that single file. Directory paths trigger recursive traversal of the entire subtree.The  parameter expects a function matching the  signature:This callback executes for every file and directory encountered during traversal. The function receives the full path, a  interface providing file metadata, and any error that occurred while accessing the entry.
  
  
  Lexical Ordering Guarantees
 provides deterministic traversal through lexical ordering. Within each directory, entries are processed in sorted order by name. This predictability proves crucial for testing and debugging file system operations.The lexical ordering applies only within individual directories. Parent directories are always processed before their children, but sibling directories follow alphabetical order.
  
  
  Memory Usage Considerations
 optimizes memory usage through several design decisions. Unlike , it uses  instead of , avoiding expensive  system calls until you explicitly request detailed file information.The function processes one directory at a time, reading directory entries incrementally rather than loading entire directory trees into memory. This approach scales well even with deeply nested directory structures containing thousands of files.For large traversals, be mindful of callback function allocations. Avoid creating unnecessary string concatenations or slice allocations within the callback, as these multiply across thousands of file system entries.
  
  
  WalkDirFunc Callback Patterns
The  callback serves as your primary interface for processing file system entries during traversal. Mastering its parameter handling and return value semantics gives you precise control over the walking behavior.
  
  
  Function Parameters: path, DirEntry, error
Each callback invocation receives three parameters that work together to provide complete context about the current file system entry.The  parameter contains the full file path from the root. This path uses the operating system's native separator and includes the original root prefix:The  parameter provides efficient access to basic file metadata without requiring expensive system calls. Use its methods to check file types and names:The  parameter indicates problems accessing the current entry. This error handling happens before your callback logic executes, allowing you to decide whether to continue or abort traversal.
  
  
  Return Value Meanings and Control Flow
Your callback's return value directly controls traversal behavior. Understanding these return patterns enables sophisticated directory walking logic.Returning  continues normal traversal:Returning  skips the current directory's contents but continues traversing siblings:Returning  terminates the entire traversal immediately:Any other error value stops traversal and propagates up to the  caller:
  
  
  Error Propagation Strategies
Different applications require different error handling approaches. Consider these common patterns based on your fault tolerance requirements.The fail-fast approach stops on any error:
  
  
  Advanced Traversal Control
Fine-grained control over directory traversal enables efficient file system operations by avoiding unnecessary work. The key lies in understanding when and how to skip portions of the directory tree based on your specific requirements.The distinction between  and  determines the scope of traversal interruption. Understanding their behavior prevents common mistakes in traversal logic. affects only the current directory when returned for a directory entry: terminates the entire traversal regardless of where it's returned:Important:  has no effect when returned for file entries. Only directory entries can be skipped.
  
  
  Conditional Directory Skipping
Complex applications often require dynamic skipping logic based on directory contents, depth, or external conditions. Implement these patterns using closure-captured state.Depth-based skipping prevents traversal beyond a certain level:Content-based skipping examines directory properties before entering:Pattern-based skipping uses matching rules for directory names:
  
  
  Early Termination Patterns
Early termination patterns optimize performance by stopping traversal once specific conditions are met. These patterns are essential for search operations and resource-constrained environments.The first-match pattern stops after finding the first occurrence:The quota-based pattern stops after processing a certain number of entries:The timeout-based pattern stops after a time limit:
  
  
  Error Handling in Tree Walking
File system traversal encounters various error conditions that require different handling strategies. The  function provides a two-phase error reporting mechanism that gives you fine-grained control over error recovery and propagation.
  
  
  Two-Phase Error Reporting
 implements a sophisticated error handling model where errors can occur both during directory reading and individual entry access. Understanding this distinction is crucial for building robust file system tools.The first phase occurs when  attempts to read a directory's contents. If this fails, your callback receives the directory path with a non-nil error parameter:The second phase happens when individual entries within a readable directory have access problems. In this case, the callback receives the entry with its specific error:Here's a comprehensive handler that distinguishes between error types:
  
  
  Pre-read vs Post-read Error Handling
The timing of error detection affects your handling strategy. Pre-read errors prevent access to directory contents entirely, while post-read errors affect individual entries after successful directory enumeration.Pre-read errors typically indicate system-level issues:Post-read errors occur after successful directory reading but indicate problems with specific entries:Different applications require different recovery approaches when file system errors occur. Implement recovery strategies based on your application's fault tolerance requirements.The retry strategy attempts to recover from transient errors:The graceful degradation strategy continues operation with reduced functionality:The error isolation strategy quarantines problematic areas while continuing elsewhere:The error collection approach gathers all errors for batch reporting:The selective error handling approach treats different error types differently: implements specific symbolic link handling policies that differ significantly from traditional file system traversal tools. Understanding these behaviors prevents security vulnerabilities and infinite loops while maintaining predictable traversal characteristics.When the root path passed to  is itself a symbolic link, the function resolves it before beginning traversal. This resolution applies only to the root path and establishes the actual starting point for the walk operation.Notice that while  resolves the root symlink to determine what to traverse, it preserves the original symlink path in the callback parameters. This behavior maintains path consistency for your application logic while ensuring the traversal reaches the intended content.The resolution only affects traversal scope, not path reporting:Root symlink resolution has security implications for applications that perform path-based access control:
  
  
  Directory Symlink Non-Following
Unlike root symlinks,  does not follow symbolic links to directories encountered during traversal. This policy prevents infinite loops and maintains bounded traversal behavior.The non-following behavior applies only to directory symlinks. File symlinks are reported but not dereferenced:This behavior protects against common symlink attack patterns:When you need to follow directory symlinks, implement custom logic with cycle detection:The practical value of  emerges through real-world implementations that solve common file system problems. These examples demonstrate how to combine the traversal patterns into production-ready tools.
  
  
  File Search Implementation
Building efficient file search tools requires combining multiple  features: pattern matching, early termination, and smart filtering. Here's a comprehensive search implementation:Cleanup tools require careful error handling and confirmation mechanisms to avoid data loss. This implementation provides safe cleanup with rollback capabilities:Auditing tools analyze file system structure and permissions to identify security issues and compliance violations:]]></content:encoded></item><item><title>Streamlining Capacity Management with Bidirectional Conversion Between T-shirt Sizes and Points: CLI Tool &quot;sizely&quot;</title><link>https://dev.to/gr1m0h/streamlining-capacity-management-with-bidirectional-conversion-between-t-shirt-sizes-and-points-1ma6</link><author>gr1m0h</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 21 Jun 2025 14:08:29 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Recently, I introduced Scrum-like practices to my team. I mainly led the implementation and have been managing the operations.As we progressed with our daily work, one of the challenges in sprint planning became capacity management. This is particularly difficult for SRE teams, which tend to have many interruptions such as incident response and ad-hoc requests. Because prediction is challenging, accurate capacity forecasting becomes the foundation for reliability improvement and leads to more challenging activities.Many teams adopt T-shirt size estimation (XS, S, M, L, XL, etc.), but we faced the challenge of difficulty in quantitative analysis such as "How much work did the team actually complete in the previous sprint?" and "How many tasks can we plan for future sprints?"
  
  
  Development Background: Why was sizely needed?
The trigger for development was when I felt that sprint planning facilitation wouldn't go smoothly if I couldn't quickly convert between T-shirt sizes and points while organizing the agenda for Scrum events.Even when told "We completed S×3, M×2, L×1 this sprint," it's difficult to estimate a similar workload for the next sprint. Tasks of the same size don't necessarily have the same priority. Additionally, we need to consider not only team capacity but also individual capacity. The amount of work that can be executed varies depending on vacation time, on-call duties, meetings, etc., so we need to convert to points to understand capacity.In planning communication, "L×2 + M×2 + XS×3 combination" is more concrete and easier to understand than the numerical value "33 points worth of work possible in a sprint," making task selection smoother.
  
  
  sizely Features and Usage
It provides the following features:T-shirt Size → Points: Calculate total points of completed tasksPoints → T-shirt Size Combinations: Suggest optimal task combinations for target points
  
  
  Post-Sprint Retrospective
# Calculate total points of completed tasks
$ sizely points --data '{"xs":3,"s":2,"m":1,"l":1}'

📊 Sprint Capacity Calculation
═══════════════════════════════
XS (1pt): 3 tasks = 3 points
S (3pt): 2 tasks = 6 points
M (5pt): 1 tasks = 5 points
L (10pt): 1 tasks = 10 points
───────────────────────────────
Total: 7 tasks = 24 points
# Search for task combinations worth 33 points
$ sizely tasks 33

🔍 Finding combinations for 33 points (max 15 tasks)
═══════════════════════════════════════════════════
Found 12 combination(s):

1. L×3 + XS×3 = 33 points (6 tasks)
✅ Good mix of large and small tasks

2. L×2 + M×2 + XS×3 = 33 points (7 tasks)
✅ Good mix of large and small tasks
...
sizely is a tool that quantifies team capacity and enables more strategic sprint planning. Through bidirectional conversion between T-shirt sizes and points, it supports both past performance analysis and future planning.Particularly for SRE teams that need to balance long-term goals of reliability improvement with short-term demands of daily incident response, quantitative capacity management is essential.Although only minimal features are currently implemented, I expect this tool to contribute to team productivity improvement and ultimately to service reliability enhancement.Although only minimal features are currently implemented, I expect this tool to contribute to team productivity improvement and ultimately to service reliability enhancement.]]></content:encoded></item><item><title>#golang #go #concurrency #goroutines Let&apos;s master concurrency in go from absolute basics. Be a part of my journey.</title><link>https://dev.to/sadhakbj/golang-go-concurrency-goroutines-lets-master-concurrency-in-go-from-absolute-basics-be-a-4pbl</link><author>Bijaya Prasad Kuikel</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 21 Jun 2025 01:00:13 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Mastering Concurrency in Go, Part 1: Understanding Concurrency vs ParallelismBijaya Prasad Kuikel ・ Jun 20]]></content:encoded></item><item><title>Basic things to know of Go</title><link>https://dev.to/wakeup_flower_8591a6cb6a9/basic-things-to-know-of-go-32p2</link><author>Wakeup Flower</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 18:10:57 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[The designer, Renée French, intentionally made the gopher look quirky and approachable — not a typical “serious” tech logo, to help set Go apart as a language for everyday programmers, not just specialists.Go is the language behind some of the most important and widely used DevOps and cloud-native tools, including:Containerization platformContainer orchestration systemInfrastructure as Code toolMonitoring and alerting systemDistributed key-value store (used in Kubernetes)Service mesh and service discoveryDevOps engineers often build custom CLI tools, automation scripts, and operators in Go.Go’s performance and static binaries make it ideal for creating efficient command-line tools.The ecosystem has great libraries for working with cloud APIs, Kubernetes, etc.Use Python for fast scripting, automation, and prototyping.Use Go when you need performance, concurrency, and easy deployment in distributed systems or large-scale tools.In many DevOps teams, you’ll see both languages used side by side — Python for quick automation and Go for core infrastructure tools.Compiled to a single, static binary — no runtime dependency needed on target machines. Super easy to deploy.Fast execution and efficient concurrency — perfect for building scalable tools that handle many tasks simultaneously (e.g., container orchestration, networking).Used in heavy-duty infrastructure tools like Kubernetes, Docker, Terraform — which require high performance.Produces small binaries, which is great for cloud environments and containers.Formatted I/O functions (, , etc.)OS functionality (file system, environment, processes)Basic interfaces for I/O primitivesFunctions for manipulating byte slicesString manipulation functionsString conversions to/from other typesBasic math constants and functionsPseudorandom number generatorTime and duration handlingNetwork I/O (TCP, UDP, IP)HTTP client and server implementationsJSON encoding and decodingXML encoding and decodingCSV encoding and decodingSynchronization primitives (Mutex, WaitGroup, etc.)Context propagation for cancellation, deadlinesError creation and manipulationCommand-line flag parsingManipulate slash-separated pathsManipulate OS-specific file pathsFunctions interacting with Go runtimeSupport for automated testingCryptographically secure random number generationGeneric SQL database interfacePNG image decoder and encoderJPEG image decoder and encoder]]></content:encoded></item><item><title>Implementing Log File Rotation in Go: Insights from logrus, zap, and slog</title><link>https://dev.to/leapcell/implementing-log-file-rotation-in-go-insights-from-logrus-zap-and-slog-5b9o</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 17:58:36 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In existing logging libraries, including Go’s built-in  logging library, they typically support log file rotation and splitting. However, these features are not built in directly—they need to be actively configured by us to enable them.This article will explore several popular logging libraries, such as logrus, zap, and the official slog. We will analyze the key design elements of these libraries and discuss how they support the configuration of log rotation and splitting.
  
  
  Brief Analysis of the Designs of logrus, zap, and slog
When comparing the design of logrus, zap, and slog, one prominent commonality is that they all include the crucial property of . This property plays a central role in the design of logging frameworks, as it determines the target location for log output.logrus is a feature-rich logging library for Go, providing structured logging, log level control, and other features.When using logrus, you can create a Logger instance by calling . With this instance, we can perform many operations, such as customizing the log output location and printing logs. Let’s look at the following code:The definition of the Logger struct is as follows:The key property is , whose type is . This property is used to specify the log output target, whether it’s standard output, a file, or another custom output medium.zap is a highly performant logging library. It provides structured logging, multi-level log control, and flexible configuration options.Similar to logrus, zap also allows you to decide the log output location via configuration, but the implementation differs slightly. In zap, log output is configured through . When creating an instance of , you need to specify an implementation of the  interface as a parameter, which directly determines the target for log output. To create a  instance, you usually use the  function, which takes an  type parameter.Here is a basic example of creating a log instance with zap:The key is the  function, which takes an  type parameter used to specify the log output target, whether it’s standard output, a file, or another custom output medium.slog is an official logging library introduced in Go 1.21.0, providing structured logging. If you want to learn more about the slog logging library, you can check out our previous article.Similar to logrus and zap, slog also allows users to specify the log output target by providing an  parameter. This setting is made when creating an implementation of the  interface.In these two functions, the first parameter of  and  is of type .From our analysis of the three mainstream logging libraries—logrus, zap, and slog—we can see a key commonality: when handling log output, all of them rely on the  interface. These logging libraries use the  interface as the type of a crucial parameter, allowing you to set the target of the log output.
  
  
  Implementation Mechanisms and Practices of Log Rotation and Splitting
After analyzing the design of logrus, zap, and slog, we have discovered their commonalities. Now, let’s dive deeper into the mechanism of log rotation and splitting.To implement log file rotation and splitting, we usually leverage third-party libraries such as lumberjack. Of course, there are other similar libraries available, but we won’t list them all here.lumberjack is a library specifically designed for log rotation and splitting. Its function is similar to a pluggable component. By configuring this component and integrating it with your chosen logging library, you can achieve log file rotation and splitting.Here is the code to initialize a lumberjack component:In this example, we create a  instance and set the following parameters:: Specifies the storage path of the log file.: The file will rotate when it reaches this many MB.: The maximum number of old log files to keep.: The maximum retention period (in days) for old files.: Whether to compress old files (e.g., convert to .gz).It is important to note that the  struct of lumberjack implements the  interface. This means all the core logic for log file rotation and splitting is encapsulated within the  method. This implementation also makes it easy for the Logger struct to be integrated into any logging library that supports an  parameter.Once you understand this, you probably already know how to implement log rotation and splitting. Since the logger struct of lumberjack implements the  interface, passing it into a third-party library allows you to complete the integration and configuration.
  
  
  Implementation with logrus Logging Library

  
  
  Implementation with zap Logging Library

  
  
  Implementation with slog Logging Library
This article provided a brief analysis of the design elements of three popular logging libraries: logrus, zap, and slog. We found that although they differ in the details of how logging instances are created, they all rely on the  interface parameter to handle log output. By mastering how to configure the  parameter and combining it with the lumberjack library, we can achieve log file rotation and splitting.Even if new logging libraries are introduced in the future, we can quickly integrate log file rotation and splitting using similar methods.Leapcell is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:Develop with Node.js, Python, Go, or Rust.Deploy unlimited projects for freepay only for usage — no requests, no charges.Unbeatable Cost EfficiencyPay-as-you-go with no idle charges.Example: $25 supports 6.94M requests at a 60ms average response time.Streamlined Developer ExperienceIntuitive UI for effortless setup.Fully automated CI/CD pipelines and GitOps integration.Real-time metrics and logging for actionable insights.Effortless Scalability and High PerformanceAuto-scaling to handle high concurrency with ease.Zero operational overhead — just focus on building.]]></content:encoded></item><item><title>Mastering Concurrency in Go, Part 1: Understanding Concurrency vs Parallelism</title><link>https://dev.to/sadhakbj/mastering-concurrency-in-go-part-1-understanding-concurrency-vs-parallelism-377k</link><author>Bijaya Prasad Kuikel</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 14:35:56 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Modern software doesn’t run on a single processor anymore. Your phone likely has 8 cores, and servers have dozens. Yet many programs are still written as if only one task happens at a time.
To build fast, scalable software, you need to master concurrency and parallelism—and Go makes these concepts simple, powerful, and fun.
  
  
  🌍 The Bigger Picture: Why Concurrency Exists at All
Before we dive into Go or any code, let’s understand the  behind concurrency.In the early days of computing, programs ran in a  — do one thing, then the next, and so on. This was fine when computers were slow, and users had simple needs. But as computers became faster and systems more complex, we hit a problem: .Waiting for input. Waiting for files. Waiting for a network response.And during that wait? The CPU just sat idle — wasting time and power.To solve this, computer scientists introduced the idea of doing multiple things seemingly at the same time — called . It let systems remain productive while waiting on slow tasks like I/O or user interaction.Later, as CPUs got multiple cores, we also gained  — actually doing things  at the same time.Modern systems combine . And that’s where software needs to evolve too.Look around you — apps today are expected to:Respond to clicks while doing work in the backgroundFetch data from multiple APIsHandle thousands of users without crashingBut under the hood, every program faces the same old enemy: doing one thing at a time is slow and limiting.… then your app is doing tasks that wait — . And if you don’t handle this well, your app becomes slow, unresponsive, or just stuck.Concurrency lets your program start a task, move on to others while waiting, and keep everything flowing. It’s the key to efficiency, responsiveness, and scalability. Concurrency is powerful but often painful in other languages:Java’s threads are heavyweight and require complex locks, which are hard to get right.JavaScript’s single-threaded event loop can lead to callback hell and debugging nightmares.Go was designed with  in mind. It gives you: — Lightweight “mini-programs” you can launch with one keyword: go. Unlike threads, they’re cheap and easy to use. —  A safe way to share data between tasks without messy locks. - A behind-the-scenes manager that juggles goroutines efficiently across CPU cores.You get simplicity, safety, and performance — all without breaking your brain.
  
  
  🧭 What This Series Covers
In this series, we’ll walk through:The fundamentals of concurrency vs parallelism (this part)How Go implements concurrency with goroutines and channelsWhat makes Go’s scheduler unique and powerfulReal-world problems you’ll face with concurrency — and how Go solves themBuilding systems that scale: crawlers, servers, pipelines, and more
  
  
  Concurrency vs. Parallelism
Let’s clear up the confusion between these two core ideas.Managing multiple tasks that can run independently, even if they don’t execute simultaneously.Think of a chef in a kitchen juggling three dishes—chopping veggies for one, stirring a sauce for another, and checking the oven. The chef switches tasks to keep everything moving, even with just one pair of hands. [Visual suggestion: Animation of a chef switching between tasks.]Executing multiple tasks at the same time on different cores.Now imagine three chefs, each cooking a different dish at the same time. That’s parallelism—true multitasking with multiple workers (cores). : Concurrency is about orchestrating tasks to avoid wasted time. Parallelism is about executing tasks at once to maximize speed. Go’s runtime handles both, making your code efficient and scalable.Waits for network or disk I/OServes thousands of usersConcurrency lets you , maximizing resource use without waiting idly. It’s about , not just raw speed.
  
  
  Real-World Case: Downloading Images
Imagine downloading 100 images:: One after another — painfully slow.: Start all downloads at once, utilizing network downtime.: Process multiple downloads across CPU cores.Here’s what this might look like in code (don’t worry, we’ll dive into goroutines soon):Go’s runtime decides which downloads run in parallel, making your code clean and scalable.
  
  
  Traditional Models: Threads and Locks
In languages like Java or C++, concurrency often means threads and shared memory. You create threads, manage locks, and pray you avoid:Deadlocks (threads stuck waiting for each other)Race conditions (unpredictable results from shared data)Context-switching overhead (threads are expensive)This model is powerful but complex and error-prone.Go abstracts threads away with goroutines — lightweight, user-space functions you launch like this:Managed by the Go runtime, not OS threadsStart with a small stack (a few KB, growing as needed)Cheap enough to run thousands without crashingGo’s M:N scheduler maps many goroutines to a few OS threads, balancing concurrency and parallelism. It’s fast, increasingly preemptive, and improves with every release.
  
  
  Sharing Memory by Communicating
Traditional concurrency: Share memory and coordinate with locks.Go’s philosophy: Do not communicate by sharing memory; instead, share memory by communicating.Go’s channels are concurrency-safe queues for passing messages:Channels reduce the need for locks, making code simpler and safer.Go’s runtime keeps getting better:Faster goroutine schedulingLower overhead for sync.Mutex, WaitGroupImproved tools like runtime/trace and pprofEasier debugging of concurrent systemsGo’s concurrency is simple, scalable, and production-ready.Concurrency: Structuring tasks to run independently.Parallelism: Executing tasks simultaneously on multiple cores.Go’s goroutines and channels make concurrency simple and safe.Compared to threads or event loops, Go offers less complexity, more performance.Go’s runtime evolves to stay cutting-edge.In Part 2: Goroutines Under the Hood, we’ll dive into the magic of goroutines:How does Go’s scheduler juggle thousands of tasks?Why can you run 10,000 goroutines on a laptop without crashing?What’s a goroutine leak, and how do you avoid it?Plus, we’ll build a concurrent web crawler to see goroutines in action.]]></content:encoded></item><item><title>⚙️ Go Tools: Password Hashing with Argon2 Instead of bcrypt</title><link>https://dev.to/nikita_rykhlov/go-tools-password-hashing-with-argon2-instead-of-bcrypt-38aj</link><author>Nikita Rykhlov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 10:46:31 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Storing passwords securely is one of the most critical security tasks in modern applications. Many developers still rely on time-tested algorithms like , but technology doesn't stand still. In this article, we'll explore  — a modern and secure password hashing algorithm that serves as an excellent alternative to bcrypt. We'll also look at how to implement it in . is a cryptographic algorithm specifically designed for password hashing. It resists brute-force attacks thanks to the use of "salt" and a tunable cost factor that increases computational complexity.However, over time new threats have emerged — especially those involving specialized hardware such as GPUs and ASICs for password cracking. This is where  starts to fall short compared to more modern solutions. is the winner of the Password Hashing Competition (PHC), a competition organized by the cryptographic community to find a new standard for secure password hashing. It was developed by a team of cryptographers from the University of Luxembourg: , , and .Argon2 was chosen for its resistance to various types of attacks, including:Argon2 offers three different modes: — provides maximum protection against hardware attacks but is vulnerable to timing attacks. — resistant to timing attacks but weaker against hardware-based attacks. — a hybrid mode combining the best features of both.For most practical purposes,  is recommended.
  
  
  Why Argon2 Is Better Than bcrypt
Protection against GPU attacksConfigurable memory usageResistance to timing attacksIn short,  is a more flexible, modern, and secure solution.In this example, we'll use  (), which combines the strengths of  and : it's resistant to side-channel attacks and protected against time-memory trade-off attacks.go get golang.org/x/crypto/argon2
🔐  Never use fixed  values. Always generate a new random salt before each password hashing.Number of passes through memoryAmount of memory used in KiB (~64 MB)Length of the resulting key in bytesRandom salt to prevent collisionsThese values are suitable for most web applications. Adjust them based on your system's capabilities or specific requirements (e.g., mobile devices).While  remains a solid choice,  offers superior protection against modern threats, particularly GPU and ASIC-based attacks. With its flexibility and efficient resource usage, it is becoming the de-facto standard for password hashing in new projects.If you're developing in , integrating Argon2 is straightforward using existing libraries. Just remember to choose appropriate parameters for your application load and always store the salt and metadata correctly.Have you already switched from bcrypt to Argon2 in your projects — or still sticking with the classic?What password hashing strategy do you use in Go — and how do you manage security vs. performance?👇 Share your thoughts and experience in the comments — I’d love to learn from you!👍 If you enjoyed this article, don’t forget to like and share it — help others upgrade their password security the right way!📣 Follow me and read my content on other platforms:Check out this and other articles on my pages:🔔 Follow me not to miss new articles and guides on hot topics!]]></content:encoded></item><item><title>Connected, Controlled, and Confident: How IoT Is Transforming Production Floors</title><link>https://dev.to/tylermorganaqe/connected-controlled-and-confident-how-iot-is-transforming-production-floors-3o30</link><author>Faizan Saiyed</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 10:04:10 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Many factories still rely on manual checks or delayed reports. This creates problems like unexpected equipment failures, stock issues, and inaccurate forecasting. Without real-time data, it’s hard to respond quickly when something goes wrong.That’s where IoT Product Development Services come in. IoT uses sensors to collect real-time data from machines, equipment, and workers. This data is then sent to the cloud, where it can be stored, analyzed, and used to improve operations.With real-time insights, factory managers can monitor equipment health, detect problems early, and make quick decisions — all from a single dashboard.Here’s a simple breakdown of how real-time monitoring works in a smart factory:Sensors are placed on machines and equipment to track performance, temperature, speed, and more.The data is collected and sent to the cloud, where it’s stored securely.Analytics tools process the data and highlight any issues, trends, or inefficiencies.Managers can view everything on a dashboard from machine status to production output — in real time.This kind of system helps manufacturers spot problems early, reduce delays, and improve overall efficiency.Benefits You Can’t IgnoreHere’s what real-time monitoring with IoT and cloud can do for your business: Catch machine issues before they lead to breakdowns. Track every part of the process and remove bottlenecks. Detect defects early and maintain consistent quality. Get full visibility of stock levels and avoid over/under stocking. Use accurate data to guide your actions in real time.These improvements not only save time and money but also make your factory more competitive and future-ready.At AQe Digital, we help manufacturers upgrade their systems with powerful and easy-to-use  We make sure the technology fits your needs, connects with your existing setup, and gives you real value.Whether it’s installing smart sensors, building real-time dashboards, or helping you manage data securely in the cloud — we offer complete support from start to finish.If you want to understand how real-time production monitoring works in detail — from key components to real business use cases — we’ve explained everything in our full blog.]]></content:encoded></item><item><title>Wiremock + testcontainers + Algolia + Go = ❤️</title><link>https://dev.to/manomano-tech-team/wiremock-testcontainers-algolia-go--3hn7</link><author>Grégoire Paris</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 08:16:02 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[When dealing with a SaaS like Algolia, testing can be a hassle. Ideally, you should not "mock what you do not own". In other words, you should not mock libraries such as the Algolia SDK, not just because it might evolve in unforeseen ways, but also because writing unit tests for a piece of code where the logic is dictated by something external to the code is not a good idea: you would not be testing the part that has the most complexity.To take a concrete example, let's imagine you want to index documents in Algolia. There is an end goal behind that, and the end goal is that it is possible to search for these documents.Ideally, you would have a Docker container running Algolia locally that would be super fast at indexing and use the same code your production Algolia app uses, but sadly that does not exist, and I'm not hopeful it ever will.In a legacy service I worked on, we have a test Algolia app that we use for integration tests. It worked great, but in the past years, Algolia introduced a new cloud-based architecture, and with this architecture, an indexing task can take a lot more time to be "published". As a result, using a test application on the cloud-based architecture is not an option anymore, as it slows the test suite down to a crawl. 🐌On a new project, I decided to re-evaluate my options, and remembered a tool that seems to be the next best thing for the job: Wiremock.In this post, I will guide you through the process of setting Wiremock and testcontainers to test Algolia's own quickstart guide for Golang.It means you can do this once in your local environment:┌────────────┐          ┌────────────────────────────┐       ┌─────────┐
│            ├─────────►│                            ├──────►│         │
│Your service│          │ Wiremock in recording mode │       │ Algolia │
│            │◄─────────┤                            │◄──────┤         │
└────────────┘          └────────────────────────────┘       └─────────┘
In recording mode, you give Wiremock a URL to record, and it will store files representing the requests you made, and the corresponding responses. With Algolia, it can be quite long, especially if you wait for operations.
What happens in practice is that the SDK will use a polling mechanism to check if your task is published. This will result in a lot of similarly looking files.
This is not very interesting to reproduce in your test, so I recommend simply deleting files representing a negative response to the question: "are the changes published yet?". Those typically contain a JSON field called  set to  in their body, like so:When the file is published, this becomes:The files have names that are a bit ugly, so I usually rename them for clarity.
For example, you might rename 1_indexes_test-index_task_226434943725-6e8689fa-9bbb-43fb-9d24-6824c02fc7d5.json
to index_test_task_published.json.Once your recording is done, you can run your tests like this:┌────────────┐          ┌───────────────────────────┐
│            ├─────────►│                           │
│Your service│          │ Wiremock in playback mode │
│            │◄─────────┤                           │
└────────────┘          └───────────────────────────┘
In playback mode, Wiremock will respond to your request with the mappings it has stored previously, and pretend to be Algolia. 🥸While this does not shield you against breaking changes in the Algolia HTTP API, it does come with a few advantages:It shields you against breaking changes or bugs in the Algolia SDK.You no longer have to mock the SDK, which is a bad practice and a pain to do. A consequence of that is that your tests become easier to understand, and more expressive, and that they check things at a higher level rather than focusing on implementation details.It still means that at least once, you do run the tests against the real thing, so if there is some issue that can only be detected at runtime, you will know about it.Wiremock is a java application, but that shouldn't matter too much, especially given there is an official Docker image you can use.
  
  
  Testcontainers: Docker for your tests
At ManoMano, we use Gitlab CI. While it is possible to define a Gitlab CI service with the aforementioned Docker image, that's not a great solution because Gitlab services do not expose the full power of Docker. For instance, mounting a volume is not possible, probably not without heavy involvement of privileged users.A great alternative is testcontainers + testcontainers Cloud. Testcontainers is a library available in many languages that allows you to start and stop Docker containers during your tests, making it possible to get good isolation between tests.
Testcontainers Cloud is a service that allows you to run said containers on a remote infrastructure, as opposed to running them on your own infrastructure, which, if you want to use Kubernetes runners for Gitlab, implies using Docker in Docker, which is not great from the security standpoint.
Locally, you would still use a local docker container, but in the CI,
tescontainers will send requests to testcontainers cloud, to start and stop containers. Enough unpaid endorsement, let's get to the code.For the sake of brevity, I will not systematically show the entirety of a file I edit in all snippets, however I have tried to create one commit per step in this Github repository, in case you would like to play with the code or simply read it in your own editor.go mod init algolia-wiremock-testcontainers

  
  
  Installing the Algolia SDK
go get github.com/algolia/algoliasearch-client-go/v4

  
  
  Setting up the environment
At this point, you will need to set up a test Algolia application. Once you are done, you should have an application ID and an API key.Let us use an unversioned env file to store our credentials.changeme
changeme
You will need to replace  and  with values from your account.
  
  
  Writing the code to be tested
Let us take the code from Algolia's quickstart guide and split it into two files:First, we have the code under test where the only changes are getting the environment variables from the actual environment, and renaming packages and functions.To make it work, you will need to install the Algolia SDK:go get github.com/algolia/algoliasearch-client-go/v4
go mod tidy
That call to  is what is going to take the most time, and a good reason not to use a real Algolia instance in your test suite. That's what we are going to try first though.
  
  
  Writing the test with a real Algolia instance
Let's start simple and write a first version of the test that talks directly to Algolia:aaaaand that doesn't work:panic: The maximum number of retries exceeded. (50/50) [recovered]
        panic: The maximum number of retries exceeded. (50/50)

goroutine 7 [running]:
testing.tRunner.func1.2({0x800600, 0xc00028d640})
        /home/gregoire/.local/share/mise/installs/go/1.24.2/src/testing/testing.go:1734 +0x21c
testing.tRunner.func1()
        /home/gregoire/.local/share/mise/installs/go/1.24.2/src/testing/testing.go:1737 +0x35e
panic({0x800600?, 0xc00028d640?})
        /home/gregoire/.local/share/mise/installs/go/1.24.2/src/runtime/panic.go:792 +0x132
algolia-wiremock-testcontainers.indexRecord()
        /home/gregoire/Documents/blogging/wiremock/indexer.go:39 +0x166
algolia-wiremock-testcontainers.TestIndexRecord(0xc000198540)
        /home/gregoire/Documents/blogging/wiremock/indexer_test.go:40 +0x20a
testing.tRunner(0xc000198540, 0x8a6c10)
        /home/gregoire/.local/share/mise/installs/go/1.24.2/src/testing/testing.go:1792 +0xf4
created by testing.(*T).Run in goroutine 1
        /home/gregoire/.local/share/mise/installs/go/1.24.2/src/testing/testing.go:1851 +0x413
FAIL    algolia-wiremock-testcontainers 185.745s
FAIL
I have many applications on this instance, some of which are very busy, let us patch that real quick:// Wait until indexing is done
_, err = client.WaitForTask(
    indexName,
    saveResp.TaskID,
    search.WithMaxRetries(100),
)
Exactly the type of thing that unit tests will not catch.After that, the test passes (but it takes between several seconds or several minutes to run depending on how busy the instance on which the application is running is). Great! Now, let's add a proxy in the middle, and record all this.
  
  
  Adding Wiremock in record mode 📼
We are using Docker, so if we want to obtain the so-called "mapping files" Wiremock will create, we need to mount a volume on our Docker container, and mount it in the right location.Let us add 2 new dependencies to our project:We could interact with Wiremock by calling the REST API with the  package, but as it turns out, there is a dedicated SDK for that, and it supports recording since this pull request I sent.At the time of writing, the PR is merged but not released yet, so for now, let's use a commit hash:go get github.com/wiremock/go-wiremock@v1.13.0
Next, we will need a way to start and stop the Wiremock container, and for that
as well, there is a library:go get github.com/wiremock/wiremock-testcontainers-go@v1.0.0-alpha-11
Yes, this is alpha software 😬Let us start the container, with a volume mounting  in the current directory on  in the container. This is where Wiremock will create json files.Next, we need to change how we instantiate the Algolia client, so that it calls Wiremock instead of Algolia:Note that I have renamed the client to  to avoid confusion with the Algolia client and the Wiremock client.Let us also refactor our  function to take the client as an argument:Next, let's start the recording, and for that we need a client to call Wiremock's administration API:Now, let's run our tests again, check our  directory, and see what's new. testdata
… OK that is quite a lot of files. 😅 As mentioned earlier, a lot of them are about polling.Let's find the one that we should keep: published testdata/taskAfter removing the files with , we are left with the following mapping files: testdata

  
  
  Switching to playback mode 📺
Now that we have our mapping files, we can switch to playback mode. Let us introduce a constant to turn recording and Algolia debugging on and off:Note that I also moved the call to  to the recording block, when replaying the tests, we do not really need to clutter the output with Algolia debug information.And now the test fails, with a rather clear error: apparently deleting the files was not enough, and we need to also edit the scenario name to outline that this is no longer the 43rd attempt.--- FAIL: TestIndexRecord (1.87s)
panic: API error [404]
                                                       Request was not matched
                                                       =======================

        -----------------------------------------------------------------------------------------------------------------------
        | Closest stub                                             | Request                                                  |
        -----------------------------------------------------------------------------------------------------------------------
                                                                   |
        1_indexes_test-index_task_226434943725                     |
                                                                   |
        GET                                                        | GET
        /1/indexes/test-index/task/226434943725                    | /1/indexes/test-index/task/226434943725
                                                                   |
        [Scenario                                                  | [Scenario                                           <<<<< Scenario does not match
        'scenario-1-1-indexes-test-index-task-226434943725'        | 'scenario-1-1-indexes-test-index-task-226434943725'
        state:                                                     | state: Started]
        scenario-1-1-indexes-test-index-task-226434943725-43]      |
                                                                   |
        -----------------------------------------------------------------------------------------------------------------------
         [recovered]
        panic: API error [404]
                                                       Request was not matched
                                                       =======================

        -----------------------------------------------------------------------------------------------------------------------
        | Closest stub                                             | Request                                                  |
        -----------------------------------------------------------------------------------------------------------------------
                                                                   |
        1_indexes_test-index_task_226434943725                     |
                                                                   |
        GET                                                        | GET
        /1/indexes/test-index/task/226434943725                    | /1/indexes/test-index/task/226434943725
                                                                   |
        [Scenario                                                  | [Scenario                                           <<<<< Scenario does not match
        'scenario-1-1-indexes-test-index-task-226434943725'        | 'scenario-1-1-indexes-test-index-task-226434943725'
        state:                                                     | state: Started]
        scenario-1-1-indexes-test-index-task-226434943725-43]      |
                                                                   |
        -----------------------------------------------------------------------------------------------------------------------
After dropping "requiredScenarioState" : "scenario-1-1-indexes-test-index-task-226434943725-43", from the mapping file about polling, the test passes again, only this time, it passes in under 2 seconds.
It is possible to mention which scenario a mapping belongs to, allowing to do things like "On the first 2 calls respond A, and on the 3rd return B". Based on that, it is possible to build a complex choreography of requests/responses, fulfilling all sorts of requirements.After pushing the code, I got a bad surprise: the test fails in the CI, with the following message:tc-wiremock.go:73: create container: container create: Error response from daemon: Invalid bind mount config: mount source "/builds/product-discovery/ms.indexer/internal/import/brandsuggestion/testdata" is forbidden by the allow list [/home /tmp] - update the bind mounts configuration and restart the agent to enable
It would seem that we cannot use a bind mount in the CI. Let us use our  constant to make the container options conditional:When recording, we mount the volume, which is not an issue because we are not in the CI.
Otherwise, we use the  function which relies on a copy operation.That function is provided by thewiremock-testcontainers-go library, which abstracts away the low-level testcontainers API so that we can think in terms of mapping files rather than just JSON files.Not super satisfying, but it works.The test is a bit long now, but some parts look generic and reusable. Let us extract them to helpers.And now our test fits on a single screen 🙂
I also added an extra assertion just to be sure we get the expected record, and that's OK, since it does not mean extra calls to Algolia.
Now that we have paid the cost of writing that first step, writing more tests should be easier, and bring a lot of value to the project.]]></content:encoded></item><item><title>Memory Stick: The Gum-Shaped Star of a Forgotten Tech Planet</title><link>https://dev.to/ersajay/memory-stick-the-gum-shaped-star-of-a-forgotten-tech-planet-2hp6</link><author>ersajay</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 06:36:45 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[A Meeting in the Circuit Desert
When I first wandered into the desert of old cameras and dusty laptops, I thought all storage devices were like the ones I’d seen—shiny, loud, and eager to prove their worth. But then I spotted it: a small, rectangular shape, half-buried in sand like a forgotten piece of gum.
“You’re… unusual,” I said, kneeling.
“And you’re a child who talks to memory sticks,” it replied, its surface glinting faintly. “But some things outlive their planets. Ask the fox.”The Gum That Outlived Floppy Disks
This wasn’t just plastic and circuits—it was a Memory Stick📀, born in 1998 on a tech planet called Earth. Let me decode its story:PRO Duo: Smaller, faster (32GB max), used in PSPs and cameras—like a sparrow in a world of eagles.
PRO-HG: High-speed for HD camcorders (now as rare as a baobab in the desert).
M2 Micro: Tiny for phones, but SD cards “won” (like a cactus losing to a rose in a garden).Fun Fact: Shaped like gum, but it won’t melt in your car (unlike floppy disks, which dissolved like sugar in rain).“Why gum?” I asked.
“Sony thought it’d fit in pockets,” it said. “Turns out, it fit in hearts too.”The Rose of a Closed Garden
On its home planet, the Memory Stick wasn’t just storage—it was a rose. Sony planted it in an exclusive garden: cameras, VAIO laptops, PSPs. No other flowers allowed.
“Why so picky?” I asked.
“Ecosystem lock-in,” it said. *“Like a garden where only one rose blooms. It kept pirates out, too—MagicGate encryption for NSYNC MP3s. Even thieves love boy bands.”
But time passed. SD cards, the “universal” daisies, spread everywhere. Yet the Memory Stick survived—not because it was better, but because some gardens still needed its thorns: legacy medical gear, satellites, and retro gamers who whispered, “I remember when you were new.”
SD Card: “I’m universal!”
Memory Stick: “I’m in satellites. You cry in radiation.” 🚀How to Love a Forgotten Star (In 2025)
Even old stars need care. Here’s how to keep a Memory Stick alive:Adapters: Use a $5 “PRO Duo to SD” adapter—like teaching a cactus to grow in a new pot. Plug it into your laptop, and voilà: it speaks modern.
Formatting: Right-click, “Format,” choose FAT32. It’s like watering a desert plant—simple, but critical.
Bad NVMe?: Swap with a new drive. The Memory Stick won’t judge—its era was about loyalty, not upgrades.“Do you miss the old days?” I asked.
“Not really,” it said. “I’m just glad I still matter. Some roses don’t need gardens to bloom.”Where to Find a Memory Stick (2025 Edition)
In 2025, it’s a treasure hunt:New: Amazon or B&H Photo (Sony still sells them for industrial clients—like a baker keeping a rare recipe).
Used: eBay (vintage PSP bundles) or Akihabara (Japan’s tech desert, where nostalgia costs extra).
Adapters: $5-$10 on Amazon. Avoid “Rare Sony Stick!!” listings—they’re like overpriced baobab seeds.Pro Tip: A 32GB Memory Stick costs $50? Walk away. It’s not gold—it’s just a gum-shaped star.The Tale of Two Planets
Once, I met an SD card in the desert. We compared notes:Capacity: SD holds 2TB (a mansion), Memory Stick 32GB (a cozy hut).
Speed: SD zips at 300MB/s (a cheetah), Memory Stick crawls at 20MB/s (a snail).
Price: SD is $20 for 1TB (a market stall), Memory Stick $50 for 32GB (a boutique).“Why do people still choose you?” the SD card asked.
“Because some things aren’t about size or speed,” the Memory Stick said. “They’re about history. And loyalty.”The Star That Still Lights Up Skies
In hidden corners of the universe, the Memory Stick glows:Medical: Stores patient data in Sony MRI machines—steady as a heartbeat.
Aerospace: Survives radiation in satellites—tougher than a desert storm.
Retro Gaming: PSP fans hoard them like rare stars—because some games only speak its language.Burn Alert:
USB Drive: “I’m cheaper!”
Memory Stick: “I’m in the Smithsonian. You’re in a conference swag bag.” 🏛️The Secret of the Gum-Shaped Star
The Memory Stick isn’t flashy. It doesn’t need a new planet or a trendy name. It’s the kind of friend you remember when you dust off an old PSP, or find an unopened pack in a drawer.
“What makes you special?” I asked, as I left.
It didn’t answer. It just sat there, quiet as the desert, as the stars, as time itself.
And I realized—some things outlive their purpose. They become stories. And stories never die.Written by a wanderer who once mistook a Memory Stick for gum. (Spoiler: It didn’t taste good. But it lasted longer.)
🌵 You become responsible, forever, for the stars you once loved.]]></content:encoded></item><item><title>Step-by-step guide on how to create a DCA bot on Go using the Binance API</title><link>https://dev.to/zmey56/step-by-step-guide-on-how-to-create-a-dca-bot-on-go-using-the-binance-api-52bb</link><author>Zmey56</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 03:43:22 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[There are many ways to invest in crypto. Some try to catch the "bottom" and go all-in, others trade based on candlesticks and indicators. And then there are those - a growing number - who use the DCA (Dollar-Cost Averaging) strategy, or simply put, averaging. The idea is simple: you buy cryptocurrency for a fixed amount at regular intervals - for example, once a day or once a week. It doesn't matter whether the market is up or down - you keep buying. In the long run, this helps smooth out volatility and reduce risk.Why does it work? Because no one can predict the bottom with precision. But with DCA, you take emotions out of the equation and enter the market gradually, at average prices. This works especially well in a rising market - for instance, in Bitcoin's case, this strategy has outperformed "buy and hold" when entering at the peak.Now - why Go? The answer is simple: if you've ever written anything in Go, you know the language is all about performance, simplicity, and concurrency. Need a bot that runs reliably 24/7, connects to the Binance API, tracks timing, and sends orders precisely? Go is a perfect fit. Low memory usage, high speed, ease of maintenance - exactly what a trading tool needs.
  
  
  What We're Going to Build
Before we start coding, let's clarify what exactly our DCA bot will be capable of and how it works under the hood. Our goal isn't just a basic "quick and dirty" example, but a fully functional tool that can be developed, scaled, and safely used.
  
  
  Multiple Trading Pairs Support
You'll be able to configure multiple coins - for example, simultaneously buying BTC, ETH, and SOL. This is convenient if you're building a diversified crypto portfolio and want to run averaging separately for each coin.
  
  
  Flexible Purchase Scheduling
Want to buy every day at 10 AM? Or every Monday? Or even every hour? - No problem. The bot will use a built-in scheduler (via cron or time.Ticker) that lets you define the desired frequency for each trading pair.
  
  
  Customizable Purchase Amount
You set the purchase amount yourself. It can be a fixed amount in USDT - for example, $50 for BTC, $20 for ETH, etc. The settings are stored in a config file, making them easy to adjust.
  
  
  Balance Check and Logging
Before each purchase, the bot will check if there's enough USDT in your account. Everything that happens - successful trades, errors, insufficient funds, Binance API behavior - gets logged. If something goes wrong, you'll see it right away.
  
  
  Minimal UI via CLI or Optional REST
You'll be able to launch and manage the bot through a CLI interface - running with parameters, viewing logs, checking current status. If desired, you can easily add a REST API for control via a browser or mobile app.To make everything work reliably and be easy to maintain, we'll break the project into several logical components:Handles communication with the exchange: authentication, order placement, balance retrieval.Task scheduler. Responsible for triggering purchases on time according to the defined schedule.Core component: checks balance, places orders, logs the results.Stores the history of all actions and errors. Can write to a file, stdout, or even a database.Easy configuration via .env/yaml/json files and management through the command line.In the end, you'll have not just a script, but a foundation for a real microservice that you can extend with strategies, notifications, a web interface, and analytics. Built the right way - with tests, logs, and an architecture that can scale.Before the bot can start trading, we need to set up the environment: install Go, add dependencies, configure access to the Binance API, and prepare our configuration.Installing Go and Initializing the ProjectYou'll need to have Go installed. I'm using version 1.24.2, but any recent version will do.After installing Go, you can either clone the repository or create the project manually:git clone https://github.com/Zmey56/dca-bot.git
dca-bot
If you're starting the project from scratch:dca-bot
dca-bot
go mod init github.com/yourusername/dca-bot
The project uses three main libraries:go-binance/v2 - handles communication with Binance: balances, orders, price quotes.cron/v3 - allows scheduling tasks (e.g., placing an order every 24 hours).godotenv - safely loads environment variables (API keys and settings are stored in .env instead of being hardcoded).If you already have a go.mod file, simply run:Working with .env and Binance API KeysTo connect to Binance, you'll need an API key and secret. You can get them from your Binance account settings.Create a .env file in the root of the project and add the following:your_api_key_here
your_secret_key_here
0.001
Make sure to add .env to your .gitignore to prevent the keys from accidentally being committed to a public repository.Connecting the Configuration and Binance ClientThe project includes a module internal/binance with a ClientWrapper implementation. It wraps the official Binance client and provides convenient methods like GetBalance and CreateMarketOrder.Client initialization looks like this:Now you can safely interact with the Binance API - no hardcoded keys, no violations of clean architecture principles.
  
  
  Integration with the Binance API
At this point, our bot can already launch, read configuration from .env, and has a clear structure. Now it's time to connect to Binance so the bot can check balances and place orders. We'll do this using the prebuilt module internal/binance, which wraps the official go-binance/v2 library.
  
  
  Creating the Binance Client
First, we need to initialize the Binance client. We have two constructors for this:NewBinanceClient() - creates a raw client using your API keys;NewClientWrapper() - wraps it into our custom interface with methods like GetBalance and CreateMarketOrder.Now client is our main tool for interacting with the exchange.
  
  
  Getting Balance Information
Before making any purchases, the bot needs to check if there's enough available funds in the account. For example, checking the USDT balance:This method calls GET /api/v3/account, parses the list of assets, and returns the value as a float64. Simple and effective.Now for the most important part - making a purchase. We're sending a market order, which tells Binance: "Buy the coin right now at the current market price."The quantity must be rounded to the correct number of decimal places. This is already handled inside the method using fmt.Sprintf("%.6f", quantity).
  
  
  Handling Errors and Rate Limits
Binance imposes a rate limit on API calls per minute. If we exceed it, the API will return a Too many requests error (code -1003). The SDK doesn't expose a dedicated error type for this, so we handle it by checking the error text directly:Now that we know how to work with the Binance API - getting the balance and sending orders - it's time to put everything together and implement the actual DCA logic: buying a selected coin on a schedule, for a specified amount, without crashing in the process.
  
  
  Configuration: pair, amount, frequency
To let the bot know what to buy, how much, and when, we need a simple configuration. No YAML or databases for now - just set everything in .env, for example:In Go, we read it like this:The frequency can be set either via cron (robfig/cron/v3) or using time.Ticker if you want a simple interval (e.g. every 6 hours).
  
  
  Main cycle: what the bot does at each trigger
Each time the scheduled trigger fires, the bot follows a simple flow:Get the current price (optional, but useful for logs)The GetCurrentPrice method can be implemented using NewListPricesService().Symbol(symbol) - see go-binance/v2 → Get Price.Before buying anything, make sure there's enough USDT available:If everything checks out - send a market order:All key actions and errors are logged. Writing to file or stdout is enough for now. Later we can add CSV or SQLite support if needed for history.
  
  
  Example: running on a schedule
We use github.com/robfig/cron/v3 to run the buy logic once a day:If you want something simpler - you can use time.Ticker:Developing the bot is only half the job. To make sure it runs reliably and doesn't buy crypto randomly, we need to ensure that:the logic works correctly,everything can be tested in isolation,and errors are easy to catch.
  
  
  Simple Unit Tests with testing
First things first - basic unit tests for core business logic. For example, if you move the calculation of the buy amount or interval into a function, it's easy to test it with the standard library:Test files are named something_test.go and live alongside the source files.
  
  
  Mocks for the Binance API
Binance is an external system - we don't want to make real trades in our tests. That's why we declared an interface in internal/binance/interface.go:Now we can mock this interface using Uber's mock library:go go.uber.org/mock/mockgen@latest 
Then, in tests, we can use the fake implementation:
  
  
  Logging to File and Console
During debugging, it's important to see what's happening. By default, everything is printed to the console with log.Println(), but you can easily add file output too:Now all logs will go both to the terminal and to dca.log - handy for both production use and debugging.At this point, we already have a working DCA bot that, on schedule, logs into Binance, checks the balance, and sends market orders. All that's left is to launch it properly, observe how it runs, and make sure we don't forget about security.The project is built like a standard Go application. The entry point is cmd/dca-bot/main.go.go build  dca-bot ./cmd/dca-bot
./dca-bot
Run as a Background ServiceYou can use , , , or simply: ./dca-bot  output.log 2>&1 &
This way, the bot will run in the background and log everything to .All actions are logged both to the console and to the file dca.log. For example:🚀 Bot started
📅 Scheduler initialized
🕒 Time to buy!
📊 Current price BTCUSDT: 63784.12
💰 Available USDT balance: 25.00
✅ Bought 0.001 BTCUSDT
⚠️ Rate limit exceeded. Waiting 2 seconds...
❌ Purchase error: request rate limit exceeded
Logs are useful both in development and in production. You can easily set up log rotation using logrotate or configure log forwarding to  - totally up to you.
  
  
  Security: Keys and Limits
API keys are stored in , not hardcoded - that's already good. is added to , so it won't accidentally get pushed to GitHub.The bot  or , it simply acts as an "averaging" worker.To avoid getting banned by Binance:we handle errors and use sleep when hitting rate limits;we avoid unnecessary API calls;you can use a proxy or an API key with limited permissions (e.g., trade-only).In the end, we've built a minimalistic yet functional DCA bot in Go that does one simple thing - buys crypto on a schedule. It can connect to Binance, check balances, send orders, log activity, and run either manually or as a background service. Everything is written from scratch with clean architecture and room for expansion.If you want more than just scheduled buys - there's plenty of room to grow:QFL (Quickfingers Luc) Strategy - the bot can buy not just by time, but in pullback zones;Add MACD, EMA, or RSI - to enter only when the market sends a signal;Telegram Notifications - know when a buy is made;Purchase history in SQLite or CSV - to analyze performance later;Visualization via Grafana or Prometheus - for dashboard lovers;More tests and integrations - e.g., with testcontainers-go for CI-ready setups.You can find the complete bot code (and a bit more) in my repository. Everything is well structured: cmd, internal, tests, logic, .env - clone and run.
  
  
  Alternative: Ready-Made Bots on Bitsgap
Want to try out DCA or other strategies (like Grid, Combo, or Trailing) but don't feel like writing code, dealing with APIs, or setting everything up manually? There's an easier way: just sign up on Bitsgap using my referral I personally use Bitsgap for part of my portfolio - it's convenient, visual, and helps you catch good entry points. Plus, you can try the PRO plan free for 7 days to see how everything works in real market conditions without taking unnecessary risks.By signing up through my link, you'll also be supporting my next project - a free Telegram bot that provides DCA trading signals for manual execution. The more support it gets, the sooner it will be ready!!!]]></content:encoded></item><item><title>[Boost]</title><link>https://dev.to/gillarohith/-3nkj</link><author>Rohith Gilla</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 02:33:41 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Page Zen: The Open-Source Article Cleaning API You've Been Waiting For]]></content:encoded></item><item><title>Advanced Go Concurrency: Unleashing Lock-Free Data Structures for Real-World Wins</title><link>https://dev.to/jones_charles_ad50858dbc0/advanced-go-concurrency-unleashing-lock-free-data-structures-for-real-world-wins-1ha0</link><author>Jones Charles</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 20 Jun 2025 00:50:27 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  Hey, Let’s Talk Concurrency
If you’re a Go developer, you’ve probably fallen in love with  and —they’re like the peanut butter and jelly of concurrent programming. Lightweight, elegant, and oh-so-satisfying. But here’s the catch: when you crank up the heat—say, an API handling 100k requests per second—those trusty tools can hit a wall. Enter the villain of the story: . Traditional locking with  starts feeling like a traffic jam—goroutines pile up, performance tanks, and you’re left wondering where it all went wrong.That’s where lock-free data structures swoop in like a superhero. No locks, no queues, just pure, unadulterated speed using atomic operations. Imagine swapping a clunky toll booth for an open highway—threads zoom through, following simple rules to avoid crashes. It’s a game-changer for high-concurrency apps, from real-time dashboards to distributed systems.This isn’t some ivory-tower lecture—I’m here to hand you the keys to lock-free programming with practical, hands-on examples. Whether you’ve got a year of Go under your belt or you’re a concurrency newbie looking to level up, this guide’s got you covered. We’ll skip the yawn-inducing theory and jump straight into code you can tweak, test, and deploy.Here’s what you’ll walk away with:: Ditch the "lock everything" habit for smarter collaboration.: Build lock-free counters, queues, and maps that crush bottlenecks.: Avoid the gotchas I’ve learned the hard way.Picture this: you’re tracking API hits in real time. A -protected counter works fine until traffic spikes—suddenly, your goroutines are stuck in line, and latency skyrockets. Swap it for a lock-free counter with , and boom—same workload, no sweat. That’s just a taste of what’s possible.Ready to roll? We’ll kick off with the basics, then build up to a full-blown case study. Buckle up—this is gonna be fun!
  
  
  Lock-Free : What’s the Big Deal?
So, what’s this lock-free hype all about? Imagine a world where your goroutines don’t have to wait in line behind a —no blocking, no drama, just smooth sailing. That’s the promise of lock-free data structures. They ditch locks for atomic operations, letting threads play nice without stepping on each other’s toes. Let’s break it down and see why they’re a concurrency superpower in Go.
  
  
  1. Lock-Free in a Nutshell
A  keeps things thread-safe without the old-school lock-and-key routine. Instead of , it leans on —think tiny, unbreakable CPU-level moves like Compare-And-Swap (CAS). Locks are like a bouncer at a club: one thread at a time, everyone else waits. Lock-free? It’s more like a dance floor—everyone’s moving, but the rules (atomic ops) keep it from turning into chaos.Keeps dancing (non-blocking)ABA quirks (more on that later)The kicker? Lock-free doesn’t nap—if a thread stumbles, it retries instead of snoozing, which is gold in high-traffic scenarios.
  
  
  2. The Secret Sauce: Atomic Operations
Atomic operations are the magic behind lock-free. They’re like ninja moves—fast, precise, and guaranteed to finish without interruption. Go’s  package hands you these tools:: Swap a value if it matches what you expect.: Bump a number up or down, no fuss.: Peek or poke safely.
  
  
  Hands-On: A Lock-Free Counter
Let’s see it in action with a counter that laughs at concurrency: bumps the counter atomically—every goroutine gets its turn without clashing.Compared to a , there’s no waiting room. It’s lean, mean, and blazing fast.
  
  
  Sneak Peek Under the Hood
Start: counter = 0
Goroutine 1: atomic.AddInt64 -> 1
Goroutine 2: atomic.AddInt64 -> 2
Goroutine 3: atomic.AddInt64 -> 3
No overwrites, no mess—atomic ops keep it clean.Lock-free brings three big wins:: No lock fights mean goroutines fly, slashing latency in high-concurrency apps.: Add more goroutines, and it just keeps humming—unlike locks, which choke.: Say goodbye to deadlocks forever.Real talk: I once swapped a  for  in a stats tracker under 100k QPS. Latency dropped from 10ms to 3ms—like flipping a turbo switch.It’s not always the answer, but it shines when:: Counters or queues getting hammered by reads and writes.: Think real-time dashboards or game servers.: Single-step updates, not big transactions.For gnarly multi-step stuff—like updating a database record—stick with locks or channels. Lock-free’s a scalpel, not a sledgehammer.Ready for more? Next up, we’ll build some lock-free goodies you can drop into your projects!
  
  
  Lock-Free Toolbox: Counters, Queues, and Maps in Go
Now that we’ve got the lock-free basics down, let’s get our hands dirty. Go’s  package is like a LEGO set for building concurrent awesomeness—simple pieces, endless possibilities. We’ll whip up three lock-free classics: a counter, a queue, and a map. Each comes with code you can steal and a breakdown of why it rocks.
  
  
  1. Lock-Free Counter: The Concurrency Champ
Need to count API hits or tasks without choking under pressure? A lock-free counter is your MVP. It’s stupidly simple and scales like a dream when goroutines come knocking.: Adds 1 without a hiccup, no matter how many goroutines pile on.: Grabs the value safely, no race conditions.: Zero contention, max speed—perfect for real-time stats.Start: value = 0
Goroutine 1: +1 -> 1
Goroutine 2: +1 -> 2
...
Goroutine 1000: +1 -> 1000

  
  
  2. Lock-Free Queue: Task Master
Got producers and consumers passing tasks like hot potatoes? A lock-free queue keeps the line moving without the lock-based bottleneck. Think job schedulers or message pipelines.
  
  
  Code Time (Simplified Enqueue)
:  locks nothing, just retries if it misses.: Keeps going until the stars align.: This skips dequeue and the ABA problem (we’ll tackle that later)—real-world queues need more polish.Start: head -> [dummy] -> tail
Enqueue 1: head -> [dummy] -> [1] -> tail
Enqueue 2: head -> [dummy] -> [1] -> [2] -> tail

  
  
  3. Lock-Free Map: Key-Value Ninja
Caching or tracking key-value pairs in a write-heavy app? A lock-free map beats  when writes dominate—like a real-time leaderboard.
  
  
  Code Time (Sharded Edition)
: Splits the map into buckets, cutting down fights.: Swaps the whole bucket atomically—thread-safe and slick.: Shines in write-heavy chaos;  rules for reads.Next up: tips to wield these tools like a pro!
  
  
  Lock-Free Like a Pro: Tips and Tricks That Stick
Lock-free data structures are awesome, but they’re not plug-and-play. Going from “locks everywhere” to “lock-free wizard” takes some finesse. After years of wrestling Go concurrency, here’s my battle-tested playbook—how to switch, what to pick, and how to dodge the landmines.
  
  
  1. From Locks to Lock-Free: A Smooth Jump

  
  
  Real Talk: API Stats Overhaul
I once had an API stats tracker choking at 100k QPS— was the bottleneck, spiking latency from 2ms to 15ms. Swapped it for a lock-free counter, and bam—problem solved. Here’s how I pulled it off:: Ditched  for atomic.AddInt64(&counter, 1).: Hammered it with unit tests to ensure no counts got lost.: Ran —QPS jumped 30%, latency crashed to 3ms.: Start with something small—like a counter—and build your lock-free chops from there.
  
  
  2. Pick the Right Tool for the Job
Lock-free isn’t one-size-fits-all. Here’s the cheat sheet: Use —zero-cost reads for stuff like configs that barely change. Go sharded with CAS—like the map we built. It thrives under pressure. Default to —it’s easy and solid for mixed workloads.: Kick off with , then level up to custom lock-free when you hit a wall.
  
  
  3. Tune It Up: Test and Tweak
: Fire up  to see what’s cooking.
  go BenchmarkCounter 5s
: Use  to sniff out goroutine jams or CPU hogs.
  go cpu.out
  go tool pprof cpu.out
Our lock-free queue was burning CPU with CAS retries under heavy enqueues. Fix? Split it into 4 shards by hashing goroutines—contention dropped 70%, throughput soared 40%. Tools like  were clutch for spotting the mess.:  to catch leaks—trust me, you’ll thank me later.Lock-free’s got quirks—here’s how I learned the hard way:: A lock-free map with crazy writes had CAS failing 90% of the time—slower than locks!: Sharded it. Retry rate fell to 20%, performance doubled.: CAS loves low contention—shard or step back if it’s a war zone.
  
  
  Trap 2: The Sneaky ABA Problem
: A queue’s dequeue missed ABA—pointer flipped A->B->A, duplicating tasks.: Added a version tag:
: Complex structures need ABA armor—version tags save the day.Start: head -> [A]
Dequeue A: head -> [B]
Enqueue A: head -> [A]
No Tag: CAS gets fooled
With Tag: Tag says “nah,” retry kicks in
Next stop: a full-on case study to tie it all together!
  
  
  Lock-Free in the Wild: Saving a Task Scheduler
Lock-free isn’t just theory—it’s a lifeline for real problems. Let’s dive into how I used a lock-free queue to rescue a distributed task scheduler from a concurrency meltdown. This is the full scoop: problem, solution, code, and results.
  
  
  1. The Mess We Started With
We had a task scheduler dishing out millions of daily jobs—think log crunching or data scrubbing—across worker nodes. Producers dumped tasks into a central queue; consumers grabbed them. Simple, right? Not at scale.: Hundreds of producer goroutines hammering the queue with —total gridlock.: Needed sub-5ms task grabs, but we were stuck at 10ms.: Couldn’t crack 80k tasks/second without choking.The diagnosis? Lock contention was killing us. Time for a lock-free fix.We built a lock-free queue with a singly linked list and CAS magic. Here’s the core of it (simplified for sanity—production had more bells)::  keeps updates atomic—no locks needed.: Skirts the ABA trap (pointer recycling woes).: Enqueue adds to the tail, dequeue pops from the head—smooth as butter.Start: head -> [dummy] -> tail
Enqueue 1: head -> [dummy] -> [1] -> tail
Enqueue 2: head -> [dummy] -> [1] -> [2] -> tail
Dequeue: head -> [1] -> [2] -> tail
We threw it into production with:: Enqueuing like mad.: Worker nodes pulling tasks.: Task flood to stress it.: Sliced from 5ms to 2ms—60% win.: Jumped from 80k to 120k tasks/second—50% boost.: Bit higher from CAS retries, but worth it.Later, we sharded the queue into 4 buckets—latency stabilized at 1.5ms.  helped us spot CAS hiccups and tweak on the fly.This wasn’t just a fix—it was a revelation. Lock-free turned a bottleneck into a highway!
  
  
  Wrapping Up: Lock-Free Lessons and What’s Next
We’ve gone from lock-free basics to a full-on task scheduler rescue—pretty wild ride, right? Lock-free data structures aren’t just a fancy trick; they’re a secret weapon for taming concurrency chaos in Go. Let’s boil it down, share some parting wisdom, and peek over the horizon.: Atomic ops like CAS ditch locks for speed, scale, and no-deadlock bliss.:  turns counters, queues, and maps into concurrency champs.: Our scheduler went from 5ms latency to 2ms and 80k to 120k tasks/second—real results, not hype.: Pick your battles, test like crazy, and watch for traps like ABA.This isn’t just Go magic—it’s a concurrency mindset you can take anywhere.Ready to flex some lock-free muscle? Here’s my advice:: Start with a counter or  for a config cache—easy wins.: Use benchmarks and  to prove it works and performs.: Locks and channels still have their place—blend them with lock-free where it fits.: Go’s concurrency game keeps evolving—stay in the loop.Think of lock-free like a new guitar riff—messy at first, killer with practice.Lock-free’s got a bright future in Go and beyond:: Bet on more built-in lock-free goodies—maybe a queue or map in the stdlib?: New CPU tricks could juice up atomic ops—Go’s runtime might cash in.: Real-time AI and edge apps will lean on lock-free for that sub-millisecond edge.This isn’t a niche anymore—it’s heading mainstream, and you’re ahead of the curve.Lock-free isn’t about locking less—it’s about collaborating more. I hope this ride sparked some ideas, whether you’re tuning an API or dreaming up the next big thing. So, grab your keyboard, crank some code, and let’s make concurrency sing!]]></content:encoded></item><item><title>Como usar tipos customizados em Golang</title><link>https://dev.to/renandotcorrea/como-usar-tipos-customizados-em-golang-10ab</link><author>Renan de Andrade</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 21:26:22 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Uma das coisas mais comuns em códigos escritos na linguagem Go é o uso de tipos customizados utilizando . Geralmente usamos estes tipos para declarar entidades, transportar valores de forma estruturada e etc. Por exemplo, O código acima é muito comum em muitas aplicações:
  
  
  Outros tipos customizados
Mas assim como , podemos utilizar outros tipos primitivos da linguagem para criar tipos customizados, abrindo assim um leque de oportunidades. O processo de criação é idêntico ao mostrado anteriormente, mas usando outro tipo primitivo como bases. Aqui vão alguns exemplos:Para usar esses tipos novos é tão simples quanto você está pensa. Eles operam da mesma forma que seus tipos base, assim como a . Dá uma olhada:Uma possibilidade legal que esta abordagem nos traz é a capacidade de as variáveis criadas a partir destes tipo chamarem métodos customizados. Isso pode ter várias aplicações interessantes. Olha só:Olhando aquele nosso exemplo inicial da , podemos aplicar esses princípios para os campos  e  ao criar um novo tipo para cada um:Assim cada tipo sabe como fazer sua própria validação. A  fica assim então:Olha como o uso fica legal:Para melhorar ainda mais nosso exemplo, podemos fazer a pergunta: E se eu quiser usar o mesmo campo  para vários tipos de documento (digamos que RG e CPF)?Podemos então mudar o tipo de  para , e criar os tipos dos outros documentos que implementam esta nova interface. Melhor mostrando, né?Mas se liga aqui como fica o uso:Dessa forma, você pode ter vários tipos de documentos, e quem vai implementar é quem decide qual vai usar.Para fecharmos, podemos fazer a pergunta: E se eu quiser ter um campo que indique o tipo de documento?A gente pode usar  para isso. Espia:Adicionamos então o campo  em :E para usar também é bem simples:Vimos aqui então que podemos criar tipos customizados baseados em tipos primitivos, chamar métodos através deles, implementar interfaces e até utilizar .E aí, o que achou dessas dicas? Deixe aí nos comentários.Obs. Cover image criada com IA.]]></content:encoded></item><item><title>Sagas to the Rescue: The Perils of Partial Success</title><link>https://dev.to/js402/sagas-to-the-rescue-the-perils-of-partial-success-1c0f</link><author>Alexander Ertli</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 19:23:24 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Let’s say you're building a system that processes large files by breaking them into chunks, generating vector embeddings (for search or AI tasks), and storing metadata in a database."It works on my machine!" — the infamous last words before production chaos.Your  hums along perfectly. API calls complete in milliseconds. Chunks of data ingest smoothly. Life is good.Then you deploy to the .A network hiccup. A delayed  request. A . The job —only to fail again with:ERROR: chunk 0: failed to insert vector - already exists
Now your system is stuck in an , reprocessing the same chunks, hitting duplicate-key errors ⚠️, and leaving behind  🧟 — data that exists in one system but not another."I'm sorry, it's lots of intimidating stuff here; let's tackle it piece by piece:" — A database that stores high-dimensional data (like embeddings) used in AI and search systems. Think of it like a supercharged search index. — A numerical representation of content (like a chunk of text) that captures its meaning, used for similarity search or AI tasks. — A deadline for an operation; if it takes too long, the system cancels it to avoid hanging. — A way to group multiple database operations into a single “all-or-nothing” step. — Data stuck in one system (like vectors) that doesn’t match up with metadata in the main database. — A follow-up action that undoes work if something fails (like deleting data you just wrote).These are all daily vocab when you try to develop or deploy a RAG for an GenAI Agent.The problem is :Your worker ingested chunks 0-13 into the vector store (✅). The database transaction rolled back (no metadata recorded).But the vectors remained in the vector store (zombie data). On retry, the worker , hitting duplicate-key errors. A distributed mess.The Root Cause: Missing AtomicityIn a single database, transactions ensure  operations. But in distributed systems: (e.g., Pinecone, Weaviate) ≠ .No cross-system transactions exist.Timeouts, crashes, or network issues leave systems inconsistent.The Fix: Sagas (Compensating Transactions)Instead of pretending for atomicity, we  and  explicitly.On Failure (timeout, crash, etc.): runs  (undo partial inserts)SQL transaction rolled backNo duplicate-key errors (clean slate on retry)✔ Distributed systems fail partially — and they will! So plan for it.
✔ Compensating transactions (Sagas) undo work explicitly.
✔  to handle crashes/timeouts.
✔  is crucial for retries."If you can't make it atomic, make it reversible." your cloud deployment behaves oddly, ask:"Did I handle partial failures—or just hope they wouldn’t happen?"]]></content:encoded></item><item><title>Go should be more opinionated</title><link>https://dev.to/eminetto/go-should-be-more-opinionated-412b</link><author>Elton Minetto</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 18:50:53 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[One of the perks of being a Google Developer Expert is the incredible opportunities it provides. A few weeks ago, I had the opportunity to meet Robert Griesemer, co-creator of Go, in person, as well as Marc Dougherty, Developer Advocate for the Go team at Google. At a happy hour after Google I/O, Marc asked me and another Go GDE from Korea for feedback on the language. My response was that I didn't have any specific feedback about the language but that:Go should be more opinionated about the application layout.It was worth writing a post to express my thoughts more clearly.Starting from the beginning… In 2025, I will have completed 10 years of writing code in Go. One of the things I recall from when I started is that the language was relatively simple to learn, mainly due to two reasons: its simplicity and the fact that there is only one way to do things. Go was the first language I came across that had strong opinions about several things. There is only one way to loop, and there is only one way to format files (using the 'go fmt' command). Variables with a small scope should have short names, etc. It made it much easier to read code written by other people, which is crucial for learning. The code I wrote was very similar to the Kubernetes code! Of course, the complexity of the problem was infinitely greater, but the code's structure was readable to me. Over the years, I have observed this effect in several people I have followed who were starting in the language or migrating from other environments.But once this initial excitement has passed, the biggest challenge comes: how to adopt Go in a project larger than those used for learning? How do you structure a project that will be developed and evolved by a team? At this point, the language step aside from strong opinions, and each team or company needs to decide how to structure their projects. Over the past decade, I have worked for four companies. In all of them, it was necessary to invest the team's time in collecting examples and reading documentation and books to determine which structure they should use in the projects. At the company where I currently work, we have created a document about this.Making an analogy with the world of games, it's as if we were having fun in the controlled and wonderful world of Super Mario World and were transported to the open world of GTA 6 (yes! I'm hyped!). It's still a fantastic universe, but the transition is quite abrupt.Go could be more opinionated regarding these choices. We could have templates for more common projects, such as CLIs, APIs, and microservices., that teams can use to scaffold their applications. The language toolkit already allows the use of project templates, so it would be a matter of having official templates to make life easier for teams. Alternatively, we could go further and include the command in the language toolkit itself with something like .A similar event occurred in the history of the language. Today,  dependency management is a fundamental part of our daily lives as Go developers. But it wasn't always like this. For a long time, there was no official package manager for the language; consequently, the community developed several alternatives. They all worked, but fragmentation was getting out of control, making it challenging to integrate packages. Until the language team took control of the situation and  was created, pacifying the issue of "package and dependency management." I believe we can apply the same approach to the structure of projects.Another profile that would benefit from a more opinionated project structure is that formed by teams that are migrating their applications from other languages, especially Java and PHP. In these ecosystems, frameworks dictate the structure of projects, such as Spring Boot and Laravel. "Where do I start? How do I structure my project?" are common questions I hear from teams migrating from these languages. Having something that facilitates this migration would lower the barrier to entry and increase the number of teams experimenting with Go in production.That's my biggest feedback regarding Go at the moment. What do you think, dear reader? What's your opinion on the subject? I'd love to discuss this topic in the comments or live at a conference.]]></content:encoded></item><item><title>Eliminating dead code in Go projects</title><link>https://dev.to/mfbmina/eliminating-dead-code-in-go-projects-1glc</link><author>Matheus Mina</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 15:39:19 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[As the software we work on grows, the code tends to undergo various changes and refactorings. During this process, we might simply forget pieces of code that were once used but no longer make sense in the project, the infamous dead code. A very common example is when an API is deactivated, and only the  is removed, but all the business logic remains, unused.Dead code can be defined as a function that exists within your codebase, is syntactically valid, but is not used by any other part of your code. In other words, it's an unreachable function. Dead code brings indirect problems to a project, such as outdated libraries, legacy code, code bloat, security vulnerabilities, and so on. If it's still not clear what dead code is, see the example below:In this code, we have the private functions  and . By default, gopls will tell you that the  function is not being used and can be removed. However, this doesn't prevent the project from compiling.  is a language server used by editors to enable features like code completion, syntax corrections, etc. But if the function becomes public, this error won't be flagged because it can theoretically be used by other packages.This problem expands when dealing with packages, as unused packages are also not reported. Imagine a package with private and public functions that isn't used in the project:The Go team then provided a solution to this problem with the  tool. It's worth mentioning that the tool should always be run from , as it searches for dead code based on what would be executed in production. When you run this tool, you finally get all unused functions:go tool deadcode ./...
 main.go:11:6: unreachable func: unreachable
 main.go:19:6: unreachable func: Public
 unused/unused.go:5:6: unreachable func: UnusedFunction
 unused/unused.go:11:6: unreachable func: indirectUnreachable
This way, we can easily find dead code in our project. To install the tool, simply run the command:go get  golang.org/x/tools/cmd/deadcode@latest
This tool is very useful to run after project refactorings and has helped me a lot to keep the code lean and containing only what truly matters to the project. If you're interested and want to know more, I recommend reading the official post. Tell me in the comments what you think of the tool, and if you want to see the full code, access it here.]]></content:encoded></item><item><title>Acabando com código morto nos projetos Go</title><link>https://dev.to/mfbmina/acabando-com-codigo-morto-nos-projetos-go-42cc</link><author>Matheus Mina</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 15:38:41 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Conforme o software que trabalhamos vai crescendo, a tendência é do código passar por diversas mudanças e refatorações. Nesse processo, podemos simplesmente esquecer pedaços de código que um dia foram utilizados e que agora não fazem mais sentido no projeto, os famosos códigos mortos. Um exemplo muito comum é quando uma API é desativada e só o  é removido, porém, toda a lógica de negócio continua ali, mas sem ser utilizada. Pode-se dizer que o código morto é basicamente uma função que existe dentro da sua base de código que é sintaticamente válida, porém não é utilizada por nenhuma outra parte do seu código, ou seja, é uma função inalcançável. Códigos mortos trazem problemas indiretos para o projeto, como bibliotecas desatualizadas, códigos legados, inchaço da base de código, falhas de segurança e por aí vai. Se ainda não ficou claro o que é um código morto, veja o exemplo abaixo:Neste código temos as funções privadas  e . Por padrão, gopls vai dizer que a função  não está sendo utilizada e que pode ser removida, entretanto, isso não impede a compilação do projeto. O  é um  utilizado pelos editores para habilitar funcionalidades como completamento de código, correções de sintaxe, etc. Porém, se a função se tornar pública, este erro não vai ser apontado, pois ele teoricamente pode ser utilizado por outros pacotes.Esse problema se amplia ao lidarmos com pacotes, pois pacotes não utilizados também não são reportados. Suponha então um pacote com funções privadas e públicas, porém que não é utilizado no projeto.A equipe do Go trouxe então uma solução para este problema, a ferramenta . Vale a pena mencionar que a ferramenta sempre deve ser executada a partir da , pois ela procura por código morto a partir do que seria executado em produção. Ao rodar essa ferramenta, temos finalmente o resultado de todas as funções não utilizadas.go tool deadcode ./...
 main.go:11:6: unreachable func: unreachable
 main.go:19:6: unreachable func: Public
 unused/unused.go:5:6: unreachable func: UnusedFunction
 unused/unused.go:11:6: unreachable func: indirectUnreachable
Assim, podemos facilmente encontrar código morto em nosso projeto. Para instalar a ferramenta, é basicamente rodar o comando:go get  golang.org/x/tools/cmd/deadcode@latest
Essa ferramenta é bem útil para ser executada após refatorações no projeto e tem me ajudado bastante a manter o código enxuto e somente com o que de fato importa para o projeto. Se você ficou interessado e quer saber mais, recomendo a leitura do post oficial. Me diz nos comentários o que você achou da ferramenta e, se quiser ver o código todo, acesse aqui.]]></content:encoded></item><item><title>Page Zen: The Open-Source Article Cleaning API You&apos;ve Been Waiting For</title><link>https://dev.to/gillarohith/page-zen-the-open-source-article-cleaning-api-youve-been-waiting-for-301e</link><author>Rohith Gilla</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 15:34:29 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In today's information-rich world, we're constantly bombarded with cluttered web articles filled with ads, popups, navigation menus, and other distractions. What if you could extract just the essential content from any article with a simple API call? Meet  - an open-source, self-hostable solution that transforms messy web articles into clean, readable content.Page Zen is a powerful Go-based API service that takes any article URL and returns clean, distraction-free content in multiple formats. Whether you're building a reading app, content aggregator, or just want to save articles without the clutter, Page Zen has you covered.✅  - Removes ads, navigation, social widgets, and other noise - Get content as clean text or markdown - Extract rich social media metadata - Works perfectly with Medium and other popular platforms - Complete control over your data and infrastructure - MIT licensed, community-driven development  
  
  
  1. Open Source & Self-HostableUnlike proprietary services that lock you into their ecosystem, Page Zen is completely open source. You can:Host it on your own infrastructureCustomize it for your specific needsNever worry about API rate limits or service shutdownsMaintain complete control over your data
  
  
  2. Works with Any Article PlatformPage Zen intelligently handles content from various sources:And virtually any web article!
  
  
  3. Beyond just cleaning content, Page Zen extracts comprehensive Open Graph metadata:Article title and descriptionGetting started with Page Zen is incredibly simple. The project includes Docker support for easy deployment:
git clone https://github.com/rohithgilla12/page-zen.git


docker-compose up That's it! Your article cleaning API is now running locally.curl  POST http://localhost:8080/extract 
  
  
  Extract Open Graph Data Only
curl  POST http://localhost:8080/opengraph {
  "url": "https://dev.to/gillarohith/develop-url-shortener-application-with-redwood-js-3cf7",
  "open_graph": {
    "title": "Develop URL shortener application with Redwood JS.",
    "description": "Develop URL shortener application with RedwoodJS            Introduction            What is...",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F77phvxr1c3i00fvv0jly.png",
    "url": "https://dev.to/gillarohith/develop-url-shortener-application-with-redwood-js-3cf7",
    "type": "article",
    "site_name": "DEV Community",
    "twitter_card": "summary_large_image",
    "twitter_site": "@thepracticaldev",
    "twitter_creator": "@gillarohith",
    "twitter_title": "Develop URL shortener application with Redwood JS.",
    "twitter_description": "Develop URL shortener application with RedwoodJS            Introduction            What is..."
  },
  "success": true
}
: Build clean RSS feeds or news aggregators: Create distraction-free reading experiences: Extract clean content for analysis: Get rich preview data for link sharing: Convert web articles to clean markdown  Page Zen goes beyond basic article extraction:: Converts complex picture elements to simple img tags: Handles relative URLs and converts them to absolute paths : Uses Mozilla's Readability algorithm for accurate content extraction: Remove specific elements based on your needs: Built-in structured logging for debugging and monitoringPage Zen is more than just a tool - it's a community-driven project that welcomes contributions:🐛  and suggest features💻  and improvements
⭐  to show your supportReady to clean up the web? Here's how to get started:: Clone the repo and run with Docker: Use the included Dockerfile for easy deployment: Start making API calls from your application: Fork the project and adapt it to your needsPage Zen - Because the web deserves to be readable.]]></content:encoded></item><item><title>Redis Fallback (Golang)</title><link>https://dev.to/pardnchiu/redis-fallback-golang-i0a</link><author>邱敬幃 Pardn Chiu</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 14:19:57 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[A Redis fallback for Golang that automatically degrades to local storage, ensuring minimal data loss during fallback and seamless recovery when Redis becomes available again.: Automatically switches to local file storage when Redis connection fails: Periodically monitors Redis health status and batch synchronizes data after recoveryThree-tier Storage Architecture: Memory cache + Redis + Local file storage: Stores data as JSON files during fallback mode to prevent data loss: Uses queue and scheduled batch writes to optimize performance in fallback mode: Supports expiration time settings and automatically cleans expired data: Uses MD5 encoding to implement layered directory structure avoiding too many files: Hierarchical logging for monitoring and troubleshootinggo get github.com/pardnchiu/golangRedisFallback
Normal mode: Try to get from memory cache, if not found query Redis, update memory cache after success and sync to Redis in backgroundFallback mode: Read from memory cache, if not found load from local JSON fileNormal mode: Write to Redis, update memory cache after success; switch to fallback mode if Redis fails beyond retry limitFallback mode: Update memory cache, add write requests to queue for batch processingRemove data from memory cache, Redis and local files simultaneouslyIn fallback mode only remove from memory cache and local files
  
  
  Fallback and Recovery Mechanism

  
  
  Automatic Fallback Triggers
Initial Redis connection failureSet operation retry count exceeds limitGet operation Redis read failure exceeds retry countStart scheduled health checks (default every minute)Write operations use queue for temporary storageScheduled batch writes to local files (default every 3 seconds)
  
  
  Automatic Recovery Process
Health check detects Redis availabilityStop health check schedulerScan local files and load to memoryBatch sync memory data to Redis (commit every 100 records)Clean local files and empty directoriesSwitch back to normal modeUses MD5 encoding to implement layered directories, avoiding too many files in a single directory:./files/golangRedisFallback/db/
├── 0/                   # Redis DB
│   ├── ab/              # First 2 chars of MD5
│   │   ├── cd/          # 3rd-4th chars of MD5
│   │   │   ├── ef/      # 5th-6th chars of MD5
│   │   │   │   └── abcdef1234567890abcdef1234567890.json
: Every Get operation checks if data is expired: Clean expired data from memory every 30 seconds: Expired data is removed from both memory and local files: Based on timestamp + ttl to determine expiration status
  
  
  Write Optimization Strategy
Write directly to Redis, update memory cache after successUpdate memory cache immediately to ensure read consistencyAdd write requests to queue (non-blocking)Write directly to file when queue is fullProcess write requests in queue with scheduled batch processing: Protects health status changes: Concurrent-safe memory cache: Prevents duplicate recovery process execution: Concurrent processing of write requests: Synchronization of write queue and local file operations: Automatic retry on Redis operation failure (configurable count): Automatically switch to local storage when Redis is unavailable: Complete error logging for troubleshooting: Ensure data synchronization between memory, Redis and files
  
  
  Performance Characteristics
: Prioritize reading data from memory cache: Use Pipeline for batch sync to Redis during recovery: Avoid too many files in single directory affecting performance: Write operations don't block main flow: Automatically clean expired data to free memoryflowchart TD
  A[Initialize] --> B{Check Redis Connection}

  B -->|Connection Failed| B_0[Start Health Check Scheduler]
  B_0 --> B_0_0[Fallback Mode]

  subgraph "Initialization"
    B -->|Connection Success| B_1{Check Unsynced Files}
    B_1 -->|Exist| B_1_1[Sync Data to Redis]
  end

  subgraph "Normal Mode"

    subgraph "Normal Mode Read"
    C --> D{Query Redis}
    D -->|Found| D_1[Update Memory Cache]
    D -->|Not Found| D_0{Check Memory Cache}

    D_0 -->|Found| D_0_1[Sync to Redis]
    end

    subgraph "Normal Mode Write"
    E --> F{Write to Redis}
    F -->|Success| F_1[Write to Memory]
    F -->|Failed| F_0{Check Redis Connection}

    F_0 -->|Connection Success, Attempts <= 3| E
    end
  end

  D_1 --> ReturnResult[Return Result]
  D_0 -->|Not Found| ReturnResult
  D_0_1 --> ReturnResult

  I_0 -->|Not Found| ReturnResult[Return null]
  I_0_1 --> ReturnResult[Return Result]

  B_1_1 --> B_1_0
  B_1 -->|Not Exist| B_1_0[Normal Mode]
  B_1_0 --> C[Read Request]
  B_1_0 --> E[Write Request]

  F_0 -->|Failed| O
  F_0 --> B_0[Start Health Check Scheduler]

  B_0_0 --> J{Check Redis Connection/Every ? seconds}
  B_0_0 --> N[Write Request]

  subgraph "Fallback Mode"
    subgraph "Fallback Mode Read"
    B_0_0 --> H[Read Request]
    I_0 -->|Found| I_0_1[Update Memory Cache]
    end

    subgraph "Fallback Mode Monitor"
    J -->|Recovered| J_1[Execute Recovery Process]
    J -->|Not Recovered| J_0[Continue Fallback Mode]
    J_0 --> J

    J_1 --> K[Sync Memory Data to Redis]
    K --> L[Sync JSON to Redis]
    L --> M{Sync Status}
    M -->|Failed, Attempts <= 3| J_1
    end

    subgraph "Fallback Mode Write"
    N--> O[Update Memory Cache]
    O --> P{DB Folder Exists}
    P --> |Yes| P_1[Write Individual Files]
    P --> |No| P_0[Create DB Folder]
    P_0 --> P_1
    end
  end

  M -->|Success| B_1_0

  H --> Q{Query Memory Cache}
  S -->|Not Found| I_0{Check JSON Exists}

  subgraph "Memory Flow"
    subgraph "Memory Read"
    Q{Check Expiration} -->|Expired| Q_1[Remove Cache and Delete JSON]
    Q_1 --> |null| S
    Q --> |Not Expired| S[Return Result]
    end 

    subgraph "Memory Cleanup"
    T[Memory Cleanup/Every ? seconds] --> U[Clean Memory Data]
    U --> V[Remove JSON]
    V --> T
    end 
  end
[x] Del - Delete key-value[ ] Exists - Check if key exists[ ] Expire/ExpireAt - Set expiration time[ ] TTL - Get remaining time to live[ ] Keys - Find keys matching pattern[ ] Pipeline - Batch commands[ ] TxPipeline - Transaction batch[ ] SetNX - Set if not exists[ ] SetEX - Set with expiration time[ ] Incr/IncrBy - Increment numeric value[ ] Decr/DecrBy - Decrement numeric value[ ] MGet/MSet - Batch get/set multiple key-value pairs[ ] HSet/HGet - Set/get hash field[ ] HGetAll - Get all fields and values[ ] HKeys/HVals - Get all field names/values[ ] HDel - Delete hash field[ ] HExists - Check if field exists[ ] LPush/RPush - Add elements from left/right[ ] LPop/RPop - Remove elements from left/right[ ] LRange - Get range elements[ ] LLen - Get list length[ ] SAdd - Add element to set[ ] SMembers - Get all set members[ ] SRem - Remove element from set[ ] SCard - Get set cardinality[ ] SIsMember - Check if element is in set
  
  
  Can not be supported at fallback mode
BLPop/BRPop - Blocking left/right popZAdd - Add element to sorted setZRange/ZRevRange - Get range by scoreZRank/ZRevRank - Get element rank<ZScore - Get element scorePublish - Publish messageSubscribe - Subscribe to channelEval/EvalSha - Execute Lua scriptThis source code project is licensed under the MIT license.]]></content:encoded></item><item><title>Go&apos;s slog: Modern Structured Logging Made Easy</title><link>https://dev.to/leapcell/gos-slog-modern-structured-logging-made-easy-4e04</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 14:17:29 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Go version 1.21.0 introduced a new package, , which provides structured logging functionality. Compared to traditional logging, structured logging is more popular because it offers better readability and significant advantages in processing, analysis, and searching.The slog package provides structured logs, where each log entry contains a , , and various other attributes, all represented as key-value pairs.The main features of the slog package are as follows:In the above example, we directly output an info-level log by calling the package function . Internally, this function uses a default  instance to perform the logging operation. In addition, you can use  to output logs with an associated context.Besides  and , there are also functions like , , and  for logging at different levels.Running the above program will produce the following output:2025/06/18 21:08:08 INFO slog msg greeting="hello slog"
2025/06/18 21:08:08 INFO slog msg with context greeting="hello slog"
By default, when using slog package functions to output logs, the format is just plain text. If you want to output in JSON or key=value format, you need to create a Logger instance using . When using this function, you must pass in an implementation of . The slog package provides two implementations:  and .TextHandler is a log handler that writes log records as a series of key-value pairs to an . Each key-value pair is represented in the form key=value, separated by spaces.In the above example, we create a log handler using . The first parameter, , indicates that logs will be output to the console. The handler is then passed as a parameter to  to create a Logger instance, which is used to perform logging operations.The output of the program is as follows:time=2025-06-18T21:09:03.912+00:00 level=INFO msg=TextHandler Name=Leapcell
JsonHandler is a log handler that writes log records in JSON format to an .In the example above, we use  to create a JSON log handler. The first parameter, , indicates output to the console. The handler is passed to  to create a Logger instance, which is then used for logging operations.The program output is as follows:slog has a default Logger instance. If you want to obtain the default Logger, you can refer to the following code:In previous examples, we always used a specifically created Logger instance to output logs. However, if you don’t want to log through a specific Logger instance every time but instead want to operate globally, you can use the  function to set and replace the default Logger instance. This makes logging more convenient and flexible.Grouping refers to grouping related attributes (key-value pairs) in a log record. Here’s an example:The result of running this program is as follows:{"time":"2025-06-18T21:12:23.124255258+00:00","level":"INFO","msg":"json-log","information":{"name":"Leapcell","phone":1234567890}}
time=2025-06-18T21:12:23.127+00:00 level=INFO msg=json-log information.name=Leapcell information.phone=1234567890
According to the output, if you group a Logger instance with a , the group name becomes a key, and the value is a JSON object composed of all key-value pairs.If you group a Logger with a , the group name is combined with the keys of all key-value pairs, and ultimately displayed as .
  
  
  Efficient Logging with LogAttrs
If you need to log frequently, compared to the previous examples, using the  function together with the  type is more efficient, because it reduces the process of type parsing.In the example above, we use the  method to output a log entry. The method’s signature is:func (l *Logger) LogAttrs(ctx context.Context, level Level, msg string, attrs ...Attr)Based on the signature, the first parameter is a , the second parameter is a  (the log severity level defined in the slog package), and the third parameter is an  key-value pair.When using other methods like  to output logs, the key-value pairs are internally converted to the  type. By using the  method, you can directly specify the  type, reducing the conversion process, and thus making logging more efficient.
  
  
  With: Setting Common Attributes
If every log needs to contain the same key-value pair, you can consider setting a common attribute.You can use the  method to add one or more fixed attributes and return a new Logger instance. Any logs output by this new instance will include the added fixed attributes, thus  the need to add the same key-value pairs to every log statement.The output of this program is as follows:
  
  
  HandlerOptions: Configuration Options for Log Handlers
Careful readers may have noticed that in previous examples, whether using  or , the second parameter was set to , which means the default configuration is used.This parameter is of type . With it, you can configure whether to display the source code location of log statements, the minimum log output level, and how to rewrite key-value pair attributes.In this example, we create a Logger instance with a . When creating the , the following configurations are specified via the  parameter:Output the source code (Source information) of the log statementSet the minimum log level to ErrorRewrite the format of the attribute with key  to The output of this program is as follows:The output matches expectations: logs of level INFO are not output, the Source information is included, and the value of the  key has been rewritten.
  
  
  Customizing the Value in Key-Value Pairs
In a previous example, we used the  configuration to modify the value in a key-value pair. Besides this method, the slog package also supports another way to change the value.In the above example, we implement the  interface (by adding the  method to a type), which allows us to override the value of a key-value pair. When logging, the value will be replaced by the return value of the  method.The output of this program is as follows:2025/06/18 21:37:11 INFO Sensitive Data password=REDACTED_PASSWORD
As expected, the value of  has been changed.This article provides a detailed introduction to the slog package in Go, including basic usage, creating Logger instances, efficient logging, and customizing log information.After reading this article, you should have a deeper understanding of the slog package and be able to use it more effectively to manage and record logs.Leapcell is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:Develop with Node.js, Python, Go, or Rust.Deploy unlimited projects for freepay only for usage — no requests, no charges.Unbeatable Cost EfficiencyPay-as-you-go with no idle charges.Example: $25 supports 6.94M requests at a 60ms average response time.Streamlined Developer ExperienceIntuitive UI for effortless setup.Fully automated CI/CD pipelines and GitOps integration.Real-time metrics and logging for actionable insights.Effortless Scalability and High PerformanceAuto-scaling to handle high concurrency with ease.Zero operational overhead — just focus on building.]]></content:encoded></item><item><title>Domain Scanner: Find Available Domain Names in a Flash!</title><link>https://dev.to/githubopensource/domain-scanner-find-available-domain-names-in-a-flash-54ph</link><author>GitHubOpenSource</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 13:29:00 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Domain Scanner is a Go-based tool for checking domain name availability. It uses multiple verification methods like DNS records, WHOIS information, and SSL certificate verification. The tool supports advanced filtering with regular expressions, concurrent processing, and provides detailed verification results, making it easy to find available domain names.✅ Multi-method verification for accurate results✅ Concurrent processing for speed and efficiency✅ Flexible filtering options using regular expressions✅ Detailed output and error handling✅ User-friendly web interface and command-line toolHey fellow developers! Ever spent hours searching for the perfect domain name, only to find it's already taken?  I know the feeling! That's why I'm super excited to share a fantastic GitHub project with you: Domain Scanner. This tool is a game-changer for anyone who needs to find available domain names quickly and efficiently. Forget endless manual searches; this tool automates the process and makes it a breeze.Domain Scanner is a powerful domain name availability checker written in Go.  What sets it apart is its comprehensive approach.  It doesn't just check one or two things; it uses multiple methods to verify availability.  Think of it like this: you're not just asking one person if a domain is free; you're asking several authoritative sources (DNS records, WHOIS information, SSL certificates) to confirm. This multi-layered approach ensures more accurate results and minimizes false positives.The architecture is surprisingly elegant.  It's designed for speed and efficiency using Go's concurrency features. You can configure the number of 'workers' – essentially, the number of simultaneous checks the tool can perform. This means you can scan through hundreds or even thousands of potential domains in a fraction of the time it would take manually.  Plus, it handles errors gracefully with automatic retries, ensuring that even temporary network hiccups won't stop your search.But the real magic is in the flexibility. You can define the length of the domain names you're looking for, specify the top-level domain (like '.com', '.org', '.net'), and even use regular expressions to filter results based on specific patterns.  Need only alphanumeric domains?  No problem. Want to exclude names containing certain characters?  Domain Scanner makes it easy. The tool provides detailed results, indicating precisely why a domain might be unavailable (e.g., DNS records exist, WHOIS data shows it's registered, etc.). This granular level of detail is invaluable for making informed decisions.The best part?  It outputs the results to separate files for available and registered domains, neatly organized and ready for further analysis.  Imagine saving hours of tedious work – that's the power of Domain Scanner. The project also includes a well-designed web interface, accessible at zli.li, offering a user-friendly alternative to the command-line tool. This web version provides a convenient way to quickly check domain availability without needing to install or run any software. Overall, this project is an outstanding example of efficient, well-documented, and user-friendly software development. It's a must-have tool for any developer or business owner who values their time and needs to find available domain names quickly and efficiently.
  
  
  🌟 Stay Connected with GitHub Open Source!
👥 
Connect with our community and never miss a discoveryGitHub Open Source]]></content:encoded></item><item><title>Why and When to Migrate from Ruby to Go: Benefits, Trade-offs &amp; Alternatives</title><link>https://dev.to/evrone/why-and-when-to-migrate-from-ruby-to-go-benefits-trade-offs-alternatives-1l58</link><author>Evrone</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 12:54:26 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Migrating from Ruby to Go offers improved performance, resource efficiency, and simpler deployment, making it ideal for scalable, cloud-native applications. This guide outlines the key advantages—like concurrency, type safety, and cross-platform deployment—alongside potential drawbacks such as rewrite costs and team learning curves. It's a strategic choice for growth-focused systems.]]></content:encoded></item><item><title>3V Battery: Hogwarts’ Hidden Power Fueling Magic &amp; Tech</title><link>https://dev.to/ersajay/3v-battery-hogwarts-hidden-power-fueling-magic-tech-2heg</link><author>ersajay</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 07:11:28 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[The Leaky Cauldron’s Unseen Alchemy
Beneath the clinking mugs of Butterbeer and the creaky floorboards of the Leaky Cauldron lies a magic even Dumbledore might have envied: the 3V battery—a compact power source so unassuming it could pass for a Galleon, yet mightier than a well-cast Lumos Maxima. While flashy wand cores and enchanted gadgets hog the limelight, this silent dynamo powers the wizarding world’s grind—from St. Mungo’s life-saving devices to Mars-bound broomsticks. Let’s lift the veil on its spells.The Potion of Precision: What Is a 3V Battery?
This isn’t just metal and chemicals—it’s enchanted energy. Break down its magic:Lithium (e.g., CR2032): The Felix Felicis of batteries—high energy, 5-10 years of life, and as reliable as a Weasley’s promise.
Alkaline: Cheaper but prone to leaks, like a faulty Reparo spell—useful, but not for the critical stuff.
Silver Oxide: The Pensieve of power—precision voltage for watches and medical tools, steady as a Memory Charm.Voltage Stability: Holds 3V until the end, no fading—unlike alkaline’s wobbly “Obliviate” act.
Size: Coin-shaped (20-30mm), slipping into spaces tighter than a Niffler’s vault—perfect for pocket watches and car keys.Fun Fact: Engineers and healers alike call it the “Wand Core of Power.” It’s in NASA rovers and your dad’s car key fob—because reliability doesn’t care if you’re orbiting Mars or just avoiding a Dementor in the parking lot.Why the Wizarding World Can’t Live Without ItThe 3V battery’s power isn’t in flash—it’s in resilience. Imagine the Great Hall, and this battery’s advantages are the Sorting Hat’s wisdom:Longevity: Lithium variants last 5-10 years. That’s longer than most first-years’ patience in Potions class. Perfect for pacemakers (no “404 Error: Heartbeat” here) and Arctic research gear (even polar bears respect its stamina).
Extreme Resilience: Works from -40°C (Hogsmeade in winter) to 85°C (a Confringo-fired cauldron). It laughs at snowstorms and desert heat—no “battery dead” warnings in the Sahara.
No Leaks: Sealed tighter than the Chamber of Secrets. No corrosion, no mess—critical for medical devices (healers hate cleaning acid off pacemakers).Roast Alert:
Alkaline Battery: “I’m cheaper!”
3V Lithium: “I’m in your pacemaker. You’re in a disposable flashlight. Talk to me when you outlive a Dementor.” 💀The Invisible Keeper of Magic
From St. Mungo’s to the Ministry of Magic, the 3V battery is the unsung hero:Healthcare (St. Mungo’s MVP):
Powers pacemakers (keeping hearts steady as a Protego shield), glucose monitors (no “low sugar” panics), and thermometers (even dragon pox can’t fool it). Healers trust it more than their own wands—quieter than a scalpel, longer-lasting than a Firewhiskey high.Consumer Magic (Wizarding Tech):
In smartwatches (upgrading the Marauder’s Map to “Live Tracking”), fitness trackers (counting Quidditch laps like a Homenum Revelio), and smart home sensors (alerting you when a Boggart’s in the closet). It outlasts toddler tantrums and juice spills—because Alohomora needs a reliable key fob.Automotive & Aerospace (Beyond Hogwarts):
Keyless entry fobs (no more “Accio Keys” at 2 a.m.), tire pressure sensors (keeping your car safer than a Shield Charm), and satellites (beaming spells to Mars). NASA uses it because “space-grade” is just Tuesday for this battery.Burn Alert:
Smartphone: “I’m the future!”
3V Battery: “I’m in your pacemaker. You’re in a landfill in 2 years. Priorities, mate.” 📱💀The Triwizard Tournament of Batteries
Let’s meet the contenders in the Great Hall of Power:3V Lithium (Gryffindor): Steady, loyal, outlasts the competition. No drama, just results.
Alkaline AA (Slytherin): Flashy, cheap, but leaks like a Boggart in the rain. Good for pranks, not for potions.
NiMH Rechargeable (Hufflepuff): Hardworking, but bulky and moody. Needs constant “Riddikulus” to stay charged.Why 3V Wins: For critical magic—pacemakers, satellites, or your car key—it’s not about cost. It’s about trust. And the 3V battery? It’s as trustworthy as Dumbledore’s beard.How to Find the Real Deal (Avoid Fake Wands)
In the wilds of Diagon Alley, not all 3V batteries are created equal. The 3V battery warned:
“Beware of knockoffs—they fail faster than a first-year’s Wingardium Leviosa. Stick to trusted sellers: Walmart, Target, or Ersa Electronics for industrial grade. For watches, hit the jewelers—Renata’s the Ollivander of 3V batteries.”
Pro Tip: Check for brands like Energizer or Panasonic. If it’s from a dodgy eBay seller claiming “Hogwarts-certified”? Run. Fast.Conclusion: The Battery That Binds
The 3V battery isn’t flashy. It doesn’t need a wand wave or a grand entrance. It’s the Homenum Revelio of tech—small, unassuming, and critical. While the world obsesses over AI and quantum wands, this humble hero keeps hearts beating, keys working, and satellites singing.
Next time your car key fob works, or your watch ticks, whisper, “Thanks, little one.” It’s the least you can do for a battery that’s saved your sanity (and your Patek’s pride).Written by a wizard who once mistook a CR2032 for a Fizzing Whizbee. (Spoiler: It didn’t taste like lemon. Or explode. Annoyingly reliable.)
🔋 Some magic isn’t in wands—it’s in the tiny things that keep the world enchanted.]]></content:encoded></item><item><title>5 Things I Learned Building a Database File Format from Scratch</title><link>https://dev.to/devdevgo/5-things-i-learned-building-a-database-file-format-from-scratch-2phf</link><author>Lakshya Negi</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 06:17:25 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Last month, I decided to build a key-value database from scratch. Not because the world needs another database, but because I wanted to understand what actually happens behind the scenes when you call  or .After weeks of wrestling with file formats, serialization, and the surprisingly complex world of "simple" storage systems, I've learned some hard lessons that no textbook quite prepared me for. Here are the five biggest insights that changed how I think about databases.
  
  
  1. Your File Format Design Choices Haunt You Forever
When I started, I thought file format design would be the easy part. "Just throw some bytes in a file, right?" Wrong. Every single decision you make in your file format becomes permanent baggage that you'll carry for the life of your database.I initially designed a complex header with 15 different fields:Creation timestamp, last modified time, record count,
page count, configuration flags, user metadata,
version numbers, checksums...
It felt thorough and professional. Then I tried to implement it.: Most of those fields were never used, and maintaining them added complexity everywhere. Worse, I realized I'd committed to this format forever—any change would break compatibility with existing files.: Start with the absolute minimum. For my key-value database, I ended up with just 16 bytes:Bytes 0-7:   File signature ("KVDB2024")
Bytes 8-11:  First freelist page pointer
Bytes 12-15: First data page pointer
That's it. Everything else can be added later if you actually need it. Your future self will thank you for keeping it simple.
  
  
  2. Endianness Will Bite You When You Least Expect It
I'm embarrassed to admit how long it took me to figure out why my database worked perfectly on my laptop but produced garbage on my friend's ARM-based server.The culprit? I was storing integers without specifying byte order:The number  was being read as  on the ARM machine. Same bits, different interpretation. was simple but crucial:: Always, always, ALWAYS specify your byte order explicitly. Even if you only plan to run on one type of machine, you'll eventually want to share database files or deploy somewhere else. Make endianness a conscious choice from day one.
  
  
  3. Error Handling Is More Important Than the Happy Path
My first implementation focused entirely on making things work correctly. Reading files, writing data, parsing headers—when everything went right, it was beautiful.Then I started testing edge cases:What if the file gets truncated?What if someone tries to open a JPEG as a database?What if the disk runs out of space mid-write?What if the process crashes during a header update?My database crashed, corrupted data, or silently accepted garbage input in every single scenario. came when I realized that error handling isn't just about making your code robust—it's about making your database trustworthy. A database that sometimes loses data is worse than no database at all.: Write your error handling first, then implement the happy path. If your database can't fail gracefully, it can't be trusted with real data.
  
  
  4. File I/O Is Asynchronous (Even When It Looks Synchronous)
This one nearly gave me a heart attack during testing.I was running a simple test: write some data, immediately cut power to the machine, then check if the data survived. It didn't. Even though my  calls returned successfully, the data never made it to disk.: Operating systems buffer writes for performance. When you call , the OS says "sure, I'll get to that" and immediately returns success. Your data might sit in a buffer for seconds before actually hitting the disk.For most applications, this is fine. For databases, it's catastrophic.: Learn to love :: If you care about durability, you must explicitly force data to disk. Every critical operation should end with a sync. Yes, it's slower. No, you can't skip it if you want your database to survive power failures.
  
  
  5. Simplicity Is a Feature, Not a Bug
Throughout this project, I constantly felt pressure to add features. "Real databases have indexing, so I need indexing." "Production systems need compression, so I need compression." "Enterprise databases support transactions, so I need transactions."This feature creep nearly killed my project. came when I stepped back and asked: "What's the simplest thing that could possibly work?"For a key-value database, that turned out to be surprisingly minimal:A file header pointing to the start of dataFixed-size pages containing variable-length recordsA simple append-only storage modelNo fancy indexing (yet). No compression (yet). No complex transactions (yet). Just a system that can reliably store and retrieve key-value pairs.: This simple design was faster, more reliable, and easier to debug than any of my complex attempts. More importantly, it actually worked.: Every feature you don't implement is a feature that can't break. Build the simplest thing first, then add complexity only when you actually need it. Your simple database that works is infinitely better than your complex database that doesn't.
  
  
  What I'd Tell My Past Self
If I could go back and give myself advice before starting this project:Start with file format design, but keep it minimalSpecify endianness explicitly from day oneWrite error handling before implementing featuresAlways sync critical writes to diskResist the urge to add features until the basics work perfectlyBuilding a database taught me that the hardest part isn't the algorithms or data structures—it's handling all the ways things can go wrong in the real world. Files get corrupted, processes crash, disks fill up, and users try to open the wrong files.A good database isn't just a system that works when everything goes right. It's a system that fails gracefully when everything goes wrong.]]></content:encoded></item><item><title>Build a Modern Plugin-Based Platform with Go + React (like Slack or Mattermost)</title><link>https://dev.to/palynext/build-a-modern-plugin-based-platform-with-go-react-like-slack-or-mattermost-482f</link><author>Paly Next</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 19 Jun 2025 01:58:05 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[PalyNext is an open-source, modular platform designed to be extensible — combining the best of:
 •🧩 Plugin architecture (Go + gRPC + Hashicorp go-plugin)
 •⚡ Dynamic frontend federation (React + Vite + Module Federation)
 •💬 Slack-like UX with real-time capabilities
 •📦 Easily deployable as microservices or standalone✨ Key Features
 •✅ Runtime plugin discovery + injection
 •✅ Each plugin can contain both Go backend and React frontend
 •✅ Hot-reload during development
 •✅ Middleware and API hooks from plugins
 •✅ Inspired by Mattermost, Slack, Coolify🔧 Tech Stack
Backend:     Go, gRPC, Fiber, go-plugin
Frontend:    Vite, React, TailwindCSS, ShadCN UI
Plugins:     Runtime loadable (Go + React)// example/main.go
func main() {
  plugin.Init(&plugin.PluginConfig{
    Name: "example",
    Version: "1.0.0",
    Handle: &Example{},
  })
}
// example/webapp/PluginEntry.tsx
export default function PluginEntry() {
  return <div>Hello from Plugin Example!</div>
}
We needed a flexible system where:
 •Frontend and backend of a feature can be plugged in dynamically
 •Plugins can register API, inject middleware, or render UI
 •Everything is hot-reloadable during developmentSo instead of rebuilding another monolith, we created PalyNext.git clone https://github.com/palynext/platform.git
cd platform
make install
pnx run dev --mode=dev
🧠 Contribute or Explore
We’re just getting started — plugin system is stable, UI is modular.
Feel free to explore, fork, or build your own plugin!⭐ Star us on GitHub: github.com/palynext/platformBuilt with love by the PalyNext Team
Inspired by Slack, Mattermost, Gitea, Coolify, and modern dev tooling.]]></content:encoded></item><item><title>Real-time with Redis Streams in Go</title><link>https://dev.to/lovestaco/real-time-with-redis-streams-in-go-1hlh</link><author>Athreya aka Maneshwar</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 19:25:26 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hi there! I'm Maneshwar. Right now, I’m building LiveAPI, a first-of-its-kind tool that helps you automatically index API endpoints across all your repositories. LiveAPI makes it easier to , , and  in large infrastructures.Redis Streams give you Kafka-like message queues with Redis simplicity. Whether you’re building real-time analytics, background job pipelines, or chat systems, Redis Streams can help.In this post, we’ll cover:Writing to a Stream in GoReading from a Stream in GoStream Configuration ParametersA Redis Stream is an append-only log data structure where each entry has a unique ID and a set of key-value fields.You write using , read using , and scale consumption using consumer groups.
XADD mystream  name Alice action login
apt redis
redis-server
go get github.com/redis/go-redis/v9

  
  
  Writing to a Stream in Go

  
  
  Reading from a Stream in Go

  
  
  Stream Configuration Parameters
XADD mystream MAXLEN 1000  field1 val1
Approximate Trimming (better performance):XADD mystream MAXLEN ~ 1000  field1 val1
Tune these Redis configs for stream node sizes:CONFIG SET stream-node-max-bytes 4096
CONFIG SET stream-node-max-entries 100
Helps approximate trimming work better and keeps memory predictable.
  
  
  Persistence with PERSIST flag
Use  in Redis CLI to force entry persistence:XADD mystream PERSIST MAXLEN ~ 500  field val
(Current Go clients may not support this yet.)XAddArgs{MaxLen:1000,Approx:true}XTrimArgs{MaxLenApprox:1000}XTrimArgs{MinID:"1605...-0"}not yet exposed in Go clientsRedis Streams give you a fast and easy way to handle real-time queues in Go. Tune configuration parameters, manage stream size, and scale with consumer groups to keep your system lean and reliable.LiveAPI helps you get all your backend APIs documented in a few minutes.With LiveAPI, you can generate interactive API docs that allow users to search and execute endpoints directly from the browser.If you're tired of updating Swagger manually or syncing Postman collections, give it a shot.]]></content:encoded></item><item><title>Making the Best of Pointers in Go</title><link>https://dev.to/shrsv/making-the-best-of-pointers-in-go-399k</link><author>Shrijith Venkatramana</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 16:50:55 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Pointers in Go can feel like a puzzle for developers coming from languages like Python or JavaScript. They’re powerful, but they can trip you up if you don’t get how they work. Go’s approach to pointers is straightforward, yet it demands a clear understanding to use them effectively. This guide dives deep into , with practical examples, tables, and tips to make them your ally. Let’s break it down step by step.
  
  
  Why Pointers Matter in Go
Pointers let you work directly with memory addresses, which can optimize performance and allow precise control over data. In Go, they’re a core feature for  and  in functions. Unlike C, Go simplifies pointer usage—no pointer arithmetic, no dangling pointers—but you still need to know when and why to use them.Here’s the deal: Go uses  by default. When you pass a variable to a function, Go copies it. Want to modify the original? That’s where pointers shine. They let you pass a reference to the data instead of a copy, saving memory and enabling changes to persist.Pointers reference memory addresses, not the data itself.Go’s pointers are safe—no manual memory management.Use pointers for efficiency or to modify original data.
  
  
  Declaring and Using Pointers: The Basics
A pointer in Go is declared with the  operator, and you get a variable’s address with . The syntax is simple but takes practice to feel natural. is a pointer to ’s memory address (). dereferences the pointer to access or modify ’s value.: The  operator is your gateway to the value at a pointer’s address. Without it, you’re just messing with the address itself.
  
  
  When to Use Pointers vs. Values
Deciding between pointers and values depends on your use case. Here’s a table to clarify:Small primitives (int, bool)No (they’re already references)For small data like integers, passing by value is fine—copying is cheap. For big structs, pointers save memory by avoiding copies. Slices and maps? They’re already reference types, so pointers are often unnecessary.Example of modifying a struct with a pointer:
  
  
  Pointers and Structs: A Perfect Match
Structs are where pointers really shine. Copying a large struct can be expensive, so passing a pointer is often smarter. Plus, if you want a function to update a struct’s fields, you need a pointer.Here’s an example with a more complex struct:: Always use pointers when modifying struct fields in functions. Without them, you’re just changing a copy.
  
  
  Nil Pointers: Avoiding the Panic
A common gotcha is the , which crashes your program. A pointer that’s declared but not initialized points to . Dereferencing it? Boom, panic.Here’s an example of what  to do:Always initialize pointers before dereferencing.Check for  if you’re unsure.: Add  checks in functions that accept pointers to prevent crashes.
  
  
  Pointers with Methods: Receiver Types
In Go, methods can have pointer or value receivers. A  lets a method modify the original struct, while a value receiver works on a copy.Here’s an example contrasting both:When to use pointer receivers:To modify the receiver’s state.For large structs to avoid copying.For consistency if other methods on the type use pointers.
  
  
  Pointers and Performance: When They Save the Day
Pointers can boost performance by reducing memory usage. Copying large structs or arrays is costly, but passing a pointer is just passing an address (8 bytes on 64-bit systems). Here’s an example showing the difference:: For large data, pointers avoid expensive copies and enable modifications. For small data, the overhead of dereferencing might outweigh the benefits—test it!
  
  
  Common Pitfalls and How to Avoid Them
Pointers are powerful but tricky. Here are common mistakes and fixes:Dereferencing nil pointersCheck for  before dereferencingOverusing pointers for small typesUse values for small primitivesForgetting  when passing to pointer paramsDouble-check function signaturesModifying slices thinking they’re pointersUnderstand slices are reference typesExample of a slice gotcha:Slices are references, so you don’t need pointers here. But appending to a slice might not work as expected if the underlying array’s capacity changes—another topic for another day.
  
  
  Putting Pointers to Work: Practical Tips
Pointers are a tool, not a mystery. Here’s how to use them effectively:Use pointers for mutability: If a function needs to change a variable or struct, pass a pointer.: Pass pointers to big structs or arrays to avoid copying.: Don’t overuse pointers for small types like  or —values are often fine.: Always check pointers in functions to avoid panics.Leverage pointer receivers: Use them for methods that modify structs or for performance with large types.Here’s a final example combining everything:This code shows a pointer receiver for a method, a nil check, and practical pointer usage. Run it, and it’s rock-solid.Pointers in Go aren’t scary once you get the hang of them. They’re about control and efficiency. Practice with small examples, lean on nil checks, and use them where they make sense—your Go code will thank you.]]></content:encoded></item><item><title>Why Rewriting Python in Go Can Boost Speed, Concurrency &amp; Efficiency</title><link>https://dev.to/evrone/why-rewriting-python-in-go-can-boost-speed-concurrency-efficiency-4gc</link><author>Evrone</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 13:57:43 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Rewriting Python projects in Go can greatly enhance performance, concurrency, and deployment efficiency, particularly for high-load or cloud-native systems. Go offers fast execution, built-in concurrency, static typing, and simpler deployment. However, migration may not suit projects reliant on specific Python libraries or teams lacking Go expertise. Consider strategic goals before switching.]]></content:encoded></item><item><title>Capital Letters Made My Workmate Quit Go—But Should You?</title><link>https://dev.to/jjpinto/capital-letters-made-my-workmate-quit-go-but-should-you-kbh</link><author>jjpinto</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 12:47:52 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[One of Go’s simplest rules—capitalization for visibility—turned my colleague off the language entirely. But is it really that bad? In this article, I explore why this rule exists, why it feels weird, and why it might actually be one of Go’s smartest design choices.The Moment My Workmate Walked AwayA few weeks ago, I told a workmate I was learning Go. His response?
"Congrats—but I could never deal with a language where capital letters decide everything."At first, I laughed. Then I realized he wasn’t joking. For him, the idea that a function’s visibility depends on whether it starts with a capital letter was enough to walk away from the language entirely.It got me thinking: Is this rule really that strange—or are we just not used to simplicity when we see it?Why Go Uses Capitalization for VisibilityIn Go, capitalization isn’t just a style choice—it’s a visibility rule.If a function, variable, or type starts with a capital letter, it’s exported (public).If it starts with a lowercase letter, it’s unexported (private to the package).There’s no public, private, or protected keyword. This minimalist approach aligns with Go’s philosophy: fewer keywords, more clarity. But for developers coming from Java, C#, or Python, this can feel unintuitive—even frustrating—at first glance.
  
  
  Simplicity Can Feel Complicated
For many developers—especially those from object-oriented backgrounds—this rule feels like a step backward. We’re used to  visibility modifiers: public, private, protected.But Go takes a  approach. The rule is simple, consistent, and enforced by the compiler. Once you get used to it, it becomes second nature—and you start to appreciate how much boilerplate it eliminates.
  
  
  Example: Capitalization and Visibility in Go
SayHello is exported (public) because it starts with a capital letter. It can be accessed from other packages.sayGoodbye is unexported (private) because it starts with a lowercase letter. It can only be used within the same package.
  
  
  Should One Rule Make You Quit Go?
I don’t think so. Every language has its quirks—Java has checked exceptions, Python has strict indentation, and C# has LINQ syntax. Go’s capitalization rule might seem odd at first, but it’s a small price to pay for a language that offers fast compilation, powerful concurrency, and a clean, consistent developer experience.If you’re curious about Go, don’t let a capital letter stop you. Dive in—you might just find it’s exactly what you’ve been looking for.Have you ever struggled with Go's capitalization rule? Does it make the language harder or simpler for you? Let me know in the comments!
  
  
  This post was reviewed with AI assistance to refine clarity and structure
]]></content:encoded></item><item><title>SnapSys - A Lightweight Linux CLI for System Snapshots (CPU, Memory, Disk) in JSONL</title><link>https://dev.to/marcumjv/snapsys-a-lightweight-linux-cli-for-system-snapshots-cpu-memory-disk-in-jsonl-6h</link><author>Marcus Vorster</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 12:38:49 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[I just released my first open source Go project: SnapSys, a simple CLI tool designed to capture CPU, memory, and disk usage at fixed intervals and export the data as structured JSONL.It’s a small but powerful utility aimed at:Developers running performance benchmarks
Engineers debugging resource issues
DevOps teams logging metrics inside CI/CD pipelines or containers
Tinkerers who want raw, lightweight system metrics to feed into custom toolsWhat is SnapSys?
SnapSys is a Linux-only CLI tool written in Go that periodically takes a snapshot of your system’s:CPU usage (percentage, user/system time, etc.)
Memory usage (total, used, available)
Disk usage (per mount point)
And logs it in clean newline-delimited JSON (JSONL). Perfect for:Feeding into Elasticsearch, Loki, or other log engines
Plotting with Grafana, Python, or Jupyter
Using in bash or Go scripts to trigger events
Keeping lightweight logs during builds, deployments, or CI stepsI’m trying to get more involved in the open source world and build tools that are useful to developers like me.If you try it and have any feedback (good or bad), I’d love to hear it. Feature requests, PRs, and even critiques are all welcome.GitHub: github.com/MarcusMJV/snapsysThanks for reading,
Marcus]]></content:encoded></item><item><title>🚀 Go Testing Unleashed: From Basics to Beast Mode</title><link>https://dev.to/tavernetech/go-testing-unleashed-from-basics-to-beast-mode-326b</link><author>Taverne Tech</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 12:00:00 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Go's Built-in Testing SuperpowersAdvanced Testing Frameworks and Power ToolsTesting Best Practices and Hidden GemsPicture this: You're debugging a Go application at 2 AM, fueled by coffee and existential dread, wondering why your "simple" function is behaving like a rebellious teenager. Sound familiar? 😅If  is like driving blindfolded on a highway, then Go's testing ecosystem is your GPS, seatbelt, and airbag all rolled into one beautiful, minimalist package. Unlike other languages that require you to assemble an entire testing arsenal, Go's philosophy is refreshingly simple: "Less is more, but make that 'less' absolutely fantastic."Today, we're diving deep into Go's testing universe – from the surprisingly powerful built-in tools to the advanced frameworks that'll make you feel like a testing ninja. Buckle up! 🥋
  
  
  1. Go's Built-in Testing Superpowers 🦸‍♂️
Go's testing approach is like having that minimalist friend who owns only three shirts but somehow always looks impeccable. The  package might seem simple, but it's .Here's a fun fact that'll blow your mind: Rob Pike initially wanted Go's testing package to have even fewer features! The current design is the result of heated debates among the Go team about striking the perfect balance between simplicity and functionality.: The  method was added in Go 1.7 and revolutionized test organization. It's like having folders within folders, but for your test cases! 📁 with  with  that double as documentation with  flag (catches ~95% of data races!)
  
  
  2. Advanced Testing Frameworks and Power Tools ⚡
While Go's built-in testing is fantastic, sometimes you need more spice in your testing soup. Enter the world of external testing frameworks – they're like adding hot sauce to an already great meal.: The  library is used in over  on GitHub! It's practically the unofficial standard for Go testing assertions.: Assertions and mocks that don't make you cry: Generate mocks automatically (because life's too short for manual mocks): BDD-style testing for the behavior-driven crowd: Built-in HTTP testing that's surprisingly robust: Go's  package can create actual HTTP servers for testing – it's like having a pocket-sized web server! 🌐
  
  
  3. Testing Best Practices and Hidden Gems 💎
Here's where we separate the testing rookies from the . Table-driven tests in Go are like meal prep for developers – a little effort upfront saves you tons of time later (and prevents the dreaded "works on my machine" syndrome).: Aim for 80-90% (100% is often overkill and expensive to maintain): Always run tests with  in CI/CD: Use TestFunctionName_Scenario_ExpectedBehavior pattern: Store expected outputs in files for complex data structures: Go's race detector uses a technique called "happens-before" analysis and can catch concurrency bugs that would take weeks to find manually. It's like having a time-traveling debugger! ⏰We've journeyed from Go's elegantly simple built-in testing tools to the advanced frameworks that can handle enterprise-level complexity. The beauty of Go's testing ecosystem lies in its  philosophy – start simple, add complexity only when needed.Remember: Good tests are like good friends – they're there when you need them, they tell you the truth (even when it hurts), and they make everything better in the long run., should you choose to accept it: Pick one untested function in your current project and write a test for it. Then another. Before you know it, you'll be sleeping better at night, deploying with confidence, and earning the respect of your fellow developers. What's the real cost of not testing? Your sanity, your reputation, and probably your weekend plans. 😉Now go forth and test responsibly! 🧪✨]]></content:encoded></item><item><title>[Boost]</title><link>https://dev.to/aymanepraxe/-26j</link><author>aymane aallaoui</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 11:09:10 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Embracing TypeScript Principles in Go: The Creation of a Zod-Inspired Validation Library]]></content:encoded></item><item><title>BAS16 Diode: The Little Star Anchoring Modern Tech</title><link>https://dev.to/ersajay/bas16-diode-the-little-star-anchoring-modern-tech-g75</link><author>ersajay</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 06:46:47 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[A Meeting in the Circuit Desert
When I first wandered into the desert of soldering irons and humming circuit boards, I thought all diodes were like the flashy ones I’d seen—polished, loud, and eager to prove their worth. But then I met the BAS16—a tiny SOT-23 package, sitting quietly on a workbench like a single cactus in the sand.
“You’re… small,” I said, tilting my head.
“And you’re a child who talks to diodes,” it replied, its surface glinting softly. “But size isn’t what matters. Ask the fox.”The Secret of Its Desert Bloom
This isn’t just silicon—it’s desert magic. Let me tell you its story:Size: 2.9mm x 1.3mm, smaller than a ladybug’s wing. It fits where even the smallest tools can’t reach.
Voltage: 100V, steady as the roots of a baobab tree. It laughs at power surges, like the cactus laughs at sandstorms.
Speed: 4 nanoseconds—faster than a shooting star. It switches signals before you can blink.
Temp Range: -55°C to 150°C. It survives Arctic cold and Death Valley heat, unflinching.Fun Fact: Engineers call it the “Swiss Army knife of diodes.” They steal it from factory floors like children steal stars—because once you find one, you never let go.The Cactus of Reliability
On the planet of electronics, where machines roar and sparks fly, the BAS16 thrives.
“Why not a cheaper diode?” I asked a welding robot.
“Cheaper diodes cry when sparks land. This one? It hums.”
It shrugs off cosmic radiation (NASA uses it in rovers), ignores clumsy interns with soldering irons, and outlasts power surges like a desert plant outlasts drought.
“You’re unkillable,” I said.
“Not unkillable,” it replied. “Just… prepared. Like the cactus that stores water—we both know hard times come.”The Guardian of Invisible Things
In the quiet corners of the universe, the BAS16 holds what matters:Healthcare: It powers pacemakers, counting heartbeats softer than a fox’s footsteps. In ERs, it survives accidental drops and chaos—so your heart never gets a “404 Error.”
Automotive: It keeps EV batteries safe, stopping sparks before they dance into fires. In car infotainment systems, it outlasts toddler tantrums and juice spills.
Space & Telecom: It holds satellites steady in zero gravity, so they can drink sunlight like roses drink rain. In 5G routers, it handles peak Netflix hours without a hiccup.“You’re a hero,” I told it.
“Heroes have parades,” it said. “I’m just a diode. But parades don’t keep hearts beating—diodes do.”The Tale of the Three Diodes
Once, I met three diodes in a workshop: 1N4148, LED, and BAS16.1N4148 preened: “I’m fast too!” But a static shock made it wince.
LED giggled: “I light up!” But it faltered in the dark corners of a pacemaker.
BAS16 said nothing. It just switched signals, steady as the desert’s horizon.Later, I asked the fox: “Why does everyone choose the quiet one?”
“Because the best things are invisible to the eye,” the fox said. “Like the wind, or love, or a diode that never fails.”When the Cactus Isn’t Needed
Even cacti have their limits. The BAS16 sighed:
“I’m not for ultra-low power devices—Schottky diodes save energy, though they’ll falter in storms. I’m not for art projects—save me for tech that matters. And I’m overkill for disposable gadgets—let cheaper diodes handle landfill-bound toys.”
“So when do you shine?” I asked.
“When the project matters,” it said. “Fire, radiation, interns… if it’s worth doing, it’s worth doing with something that lasts.”How to Find a True Friend
In the market of diodes, not all are real. The BAS16 warned:
“Beware of counterfeits—they fail faster than a child’s promise to water a rose. Trust distributors like Ersa Electronics. Check for Vishay or Nexperia markings. Demand certifications—fakes can’t fake those.”
“How do I know it’s you?” I asked.
“You’ll feel it,” it said. “A real diode doesn’t shout. It just… works.”The Star That Never Fades
In 2050, when humans build colonies on Mars, the BAS16 will be there. It’ll power quantum computers (even qubits need something steady), Mars habitats (cosmic radiation can’t break it), and robot arms (when AI overlords revolt, they’ll use it to build… well, let’s not think about that).
“You’ll outlive us all,” I said.
“No,” it replied. “I’ll just keep holding. Because stars don’t stay in the sky by magic—they stay because something anchors them.”The Secret of the Little Diode
The BAS16 isn’t flashy. It doesn’t need a name in lights. It’s the kind of friend you notice only when it’s gone—like the rose in the garden, or the fox’s footsteps in the sand.
“What makes you special?” I asked, as I packed to leave.
It didn’t answer. It just switched a signal, steady as the desert, as the stars, as time itself.
And I realized—important things are never the loudest. They’re the ones that stay.Written by a wanderer who once mistook a diode for a new planet. The BAS16 set me straight.
🌵 You become responsible, forever, for the diodes you ignore.]]></content:encoded></item><item><title>Advanced Go Concurrency: Channel Patterns for Real-World Problems</title><link>https://dev.to/jones_charles_ad50858dbc0/advanced-go-concurrency-channel-patterns-for-real-world-problems-33ce</link><author>Jones Charles</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 18 Jun 2025 00:43:16 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  1. Hey, Let’s Talk Concurrency in Go!
Concurrency is everywhere—scaling web apps, crunching data on multi-core beasts. Go makes it fun with goroutines (lightweight threads) and channels (data pipelines), but it’s easy to trip. Too many goroutines frying your CPU? Tasks clashing? That’s where  kicks in—channels are the chill teacher keeping rowdy goroutines in line.This isn’t “Concurrency .” If you’ve got 1-2 years of Go and know goroutines and channels, you’re in the right spot. We’re diving into advanced channel tricks—rate limiting, producer-consumer, task splitting—with real-world examples from my projects. Code, pitfalls, and tips await. Let’s tame the chaos and unlock channel magic together!
  
  
  2. Why Channels Rock Concurrency

  
  
  2.1 Channels (Quick Refresh)
Channels pass data between goroutines. Two flavors:: Sender and receiver sync—like a high-five needing both hands.: Sender queues data (with a limit) and moves on—like a mailbox.Locks () guard stuff; channels flow data. Go’s motto? “Share memory by communicating.”
  
  
  2.2 What Makes Channels Awesome?
Here’s why I stan channels:: Thread-safe by default—no race conditions or  slip-ups.: Pass data to signal “go” or “stop”—goroutines texting each other.: Stack ‘em into pipelines or split tasks. They glue patterns together.: Scales with tons of goroutines, no lock spaghetti.:  yells “done!” to all listeners—clean shutdowns.: Juggles multiple channels or timeouts—like a traffic cop.
Channels aren’t just pipes—they’re your concurrency Swiss Army knife.
  
  
  3. Channel Patterns That Solve Real Problems
Channels shine in tough spots. Here are three I use constantly: , , —with code and scars.
  
  
  3.1 Rate Limiting: Keep the Floodgates in Check

API slamming a DB with goroutines? Connection pool dies. Rate limiting caps the chaos.
Buffered channel as a “token bucket”—grab a token to work, release it when done.: Simple throttle, saves resources.: Set it to 50 once—DB cried. Tuned to 20 after monitoring.
  
  
  3.2 Producer-Consumer: Teamwork Makes the Dream Work

Logs or downloads? Producers make tasks; consumers process ‘em—smooth and separate.
Unbuffered channel as a queue. Close it to signal “done.”
1 producer, 3 consumers, 10 tasks:: Balances load, clean split.: Skipped —consumers hung.  bailed me out.
  
  
  3.3 Fan-out/Fan-in: Divide and Conquer

Parallelize tasks (e.g., API calls) and collect results? Fan-out spreads; Fan-in gathers.
One channel dispatches, another collects.: Maxes CPU, modular.: No timeout—stalled worker froze it. Added  later.
  
  
  4. Channels in the Wild: Real Projects, Real Wins

  
  
  4.1 Rate Limiting an API Under Fire
: Order API with 10k+ reqs/sec—DB and Redis choked.: Token pool with .:: Flex token size, use .: Botched —leaked goroutines. Fixed with .
  
  
  4.2 Data Pipeline: Logs at Scale
: Millions of logs—sequential too slow, parallel too wild.: Pipeline with channel handoffs.:: Stage it, tune buffers.: Big buffers spiked RAM—cut to 10.
  
  
  4.3 Batch Uploads with Status Updates
: File uploads needing live status.: Tasks and results channels.:: Async updates, clear structs.: Forgot —deadlocked. Fixed with .
  
  
  5. Channel Wisdom: Tips, Traps, and Tuning
: Unbuffered for sync, buffered for slack.:  signals,  juggles.: Kill goroutines cleanly.: 1000 crushed memory—start small, test.: Unclosed channels—run .:  finds ‘em—tweak buffers or goroutines.: Channels for flow,  for locks,  for sync.: Adjust limits with load.
  
  
  6. Wrapping Up: Channels Are Your Superpower
Channels tame goroutines with elegance—rate limit, pipeline, split tasks. They’re safe, flexible, clean. Try ‘em out—throttle an API, process data. Hands-on is where it clicks.Go’s concurrency evolves—think  or distributed channels. Watch proposals and trends.Love channels’ clarity, hate the 2 a.m. deadlocks. Every goof taught me—’s my buddy. Code it, break it, learn. What’s your channel tale? Bugs? Wins? Hit the comments—let’s chat!]]></content:encoded></item><item><title>Dealing with race conditions</title><link>https://golangweekly.com/issues/558</link><author></author><category>dev</category><category>go</category><pubDate>Wed, 18 Jun 2025 00:00:00 +0000</pubDate><source url="https://golangweekly.com/">Golang Weekly</source><content:encoded><![CDATA[Glance is a Go-powered personal information dashboard/portal where you can bring together RSS feeds, Reddit posts, Hacker News posts, YouTube channel updates, stock prices, and more.Glojure is a Clojure interpreter, hosted on Go.]]></content:encoded></item><item><title>Robust Error Handling in Go Web Projects with Gin</title><link>https://dev.to/leapcell/robust-error-handling-in-go-web-projects-with-gin-l01</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 19:21:54 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In Go project development, error handling is one of the keys to building stable and reliable web services. An efficient error handling mechanism can not only catch unhandled exceptions but also provide clear and user-friendly error information to clients through a unified response structure. In this article, we will discuss how to implement global error capture, custom HTTP error codes, business error classification handling in Gin, as well as integrating Sentry for advanced error monitoring.
  
  
  Global Error Capture: Enhancing the Recovery Middleware
Gin provides a Recovery middleware for capturing unhandled panics and preventing the program from crashing. However, by default, the Recovery middleware only returns a generic HTTP 500 error. By enhancing it, we can achieve more flexible global exception capture and handling.By default, Gin’s Recovery will capture all unhandled panics and return an internal server error:When accessing /panic, the client will receive:
  
  
  Custom Recovery: Capturing Exceptions and Logging
By customizing the Recovery middleware, we can log errors into the logging system while returning a structured error response.With the customized Recovery, we capture exceptions and return more detailed error information, while keeping error logs for future debugging.
  
  
  Custom HTTP Error Codes and Response Structure
A unified error code and response structure is a best practice for modern APIs. It helps the frontend clearly understand what error has occurred and take corresponding actions.
  
  
  Defining a Standard Response Structure
We can define a universal response format to unify all successful and failed responses.
  
  
  Error Response Utility Functions
By encapsulating utility functions, we simplify the process of generating error responses.
  
  
  Business Error Classification Handling
In complex systems, different types of errors (such as database errors, authentication errors) need to be handled separately. By encapsulating error types, we can achieve clearer error classification and responses.
  
  
  Defining Business Error Types
By checking the error type, different responses can be generated.
  
  
  Integrating Sentry for Error Monitoring
Sentry is a popular error tracking platform that helps developers monitor and analyze exceptions in production environments in real time.For specific integration methods, please refer to the official documentation. We will not elaborate in detail here.
  
  
  Official Example for the Go SDK
When an exception occurs in the application, the error will be automatically sent to the Sentry dashboard. Developers can view detailed error stack information in real time and clearly see which endpoint had how many error requests during a specific period.Globally capture serious exceptions.Refine error classification at the business layer and provide specific feedback information.Unified response structure:Ensure that the response formats for both success and failure are consistent, making it easier for the frontend to handle.Integrate tools (such as Sentry) to capture exceptions in the environment and locate issues as soon as possible.Define a unique error code for each error type, making it easier to quickly locate and troubleshoot problems.Leapcell is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:Develop with Node.js, Python, Go, or Rust.Deploy unlimited projects for freepay only for usage — no requests, no charges.Unbeatable Cost EfficiencyPay-as-you-go with no idle charges.Example: $25 supports 6.94M requests at a 60ms average response time.Streamlined Developer ExperienceIntuitive UI for effortless setup.Fully automated CI/CD pipelines and GitOps integration.Real-time metrics and logging for actionable insights.Effortless Scalability and High PerformanceAuto-scaling to handle high concurrency with ease.Zero operational overhead — just focus on building.]]></content:encoded></item><item><title>Golang Fundamentals: From &quot;Hello World!&quot; to File Paths &amp; CLI Subcommands</title><link>https://dev.to/labex/golang-fundamentals-from-hello-world-to-file-paths-cli-subcommands-2mpj</link><author>Labby</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 17:03:01 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Embark on an exciting journey into the world of Golang, a language celebrated for its efficiency, concurrency, and simplicity. This Skill Tree is your structured roadmap, guiding you through Go's core concepts with hands-on challenges. Forget passive learning; here, you'll write code, solve problems, and build practical skills in an interactive environment. Whether you're a seasoned developer looking to add Go to your toolkit or a curious beginner, this path offers a clear, engaging way to master Go's syntax, concurrency model, and standard library. Let's dive into the labs that will transform you into a proficient Go programmer. Beginner |  5 minutesThis Golang challenge aims to test your basic understanding of the language syntax and structure. You will be required to write a simple program that prints the classic 'hello world' message.
  
  
  Golang Constants Programming Challenge
 Beginner |  5 minutesThis challenge aims to test your understanding of constants in Golang.
  
  
  File Path Handling in Golang | Challenge
 Beginner |  5 minutesThe filepath package in Golang provides functions to parse and construct file paths in a way that is portable between operating systems.
  
  
  Command Line Subcommands | Challenge
 Beginner |  5 minutesThis challenge aims to test your ability to define and use subcommands with their own set of flags in Golang.
  
  
  Go Functions Fundamentals
 Beginner |  5 minutesIn this challenge, we will learn about functions in Go. We will see how to define functions, how to pass arguments to them, and how to return values from them.Ready to embark on your Go journey? These challenges are just the beginning. Each completed lab builds your confidence and practical skills, transforming you from a curious beginner into a proficient Go developer. Dive in, explore, and unlock the power of Golang!]]></content:encoded></item><item><title>🚀 Build Your Own Email-to-Webhook Gateway in Go (SMTPHook is Live!)</title><link>https://dev.to/voidwatch/build-your-own-email-to-webhook-gateway-in-go-smtphook-is-live-27eb</link><author>Johan</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 16:24:37 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  I just open-sourced SMTPHook — a self-hosted email ingestion service written in Go. If you've ever wanted to turn  into structured  (e.g. to ping a pager, log alerts, trigger bots, etc.), this tool is for you. If it works for you consider sponsor me with a coffee https://buymeacoffee.com/voidwatchSMTPHook is a modular platform that:✅ Accepts SMTP emails via Mailpit
✅ Parses raw emails into structured JSON
✅ Forwards the payload to your webhook endpoint
✅ Includes retry logic, logging, health checks, or as containers via 
✅ Supports full local testing with Alerting: Email → Webhook → PagerDuty / Discord / Slack
Archiving: Email → JSON → S3 / DB / Elasticsearch
Automation: Email triggers CI/CD, bots, or workflows
Dev Testing: No need to poll real mailboxes anymoregit clone git@github.com:voidwatch/SMTPHook-Golang.git
SMTPHook-Golang
 +x setup.sh
./setup.sh
swaks @example.com  localhost:1025 < email.txt
And receive it on your webhook.A dash of Curl and a bit of swaksI’d love your feedback, contributions, or ideas. If you find this useful and want to support the project, stars or donations are always welcome!
  
  
  🙏 Support & Contributions
If you're an indie devops engineer, or just love hacking with Go and automation, I’d love your thoughts.
DM me or check the repo for sponsor links.https://buymeacoffee.com/voidwatch]]></content:encoded></item><item><title>Building a JWT-Aware Reverse Proxy in Go for Tiered API Access</title><link>https://dev.to/savinda_premachandra/-1kh8</link><author>Savinda</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 15:45:56 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Building a JWT-Aware Reverse Proxy in Go for Tiered API Access]]></content:encoded></item><item><title>Stop pushing broken code. Start using Git hooks🧙‍♂️</title><link>https://dev.to/ezpieco/stop-pushing-broken-code-start-using-git-hooks-44m6</link><author>Ezpie</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 14:00:00 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Git Hooks are awesome - in theory.test before doing a commit. ✅lint code before a push. ✅prevent interns from crashing production due to untested code 😱But let's be honest, how many of us really use them huh?Most of us probably don't use them mainly because:They live inside of  directoryThey can't be version controlledFeel like a trap - what if I mess up and do push to production but the hook didn't work and now the entire site crash(maybe that's why the internet is down today?)So, most of us, like me just:Or if you feel like killing yourself, you code in JS and can just use Husky - Which is amazing but nodejs
onlyFeeling trapped to use node everywhere(even the backend 😱), I decided to write a CLI based tool, GetHooky, to manage all your git hooks for you. And you can also version control it and share it with your team!Think of it as Husky, but for everyone 💪.For the purpose of flexing GetHooky is written in rust... competitor go.You just initialize a repository with hookyThis create's a directory by the name of , where all your hooks will live, you can version control this.hooky add pre-commit This creates a  file which contains the command you want, in this case  will be stored.And then finally install itThis updates the hooks present in the  directory, to prevent GetHooky from accidently touching hooks which you don't want to change, your personal hooks, hooky adds a marker on top of each file which it controls  this tells it to touch only this file.No nodejs, no dependencies, not even Go!Too little abstraction pythonistYou can install GetHooky with curl or wget like so:sh -c "$(curl -fsSL https://raw.githubusercontent.com/ezpieco/gethooky/master/tools/install.sh)"sh -c "$(wget -O- https://raw.githubusercontent.com/ezpieco/gethooky/master/tools/install.sh)"For windows I would highly recommend you to change your OS... just download it from the release pageGetHooky is exactly what you need to handle your interns and yourself, cause hey? We all have crashed production at least once in our life! Trust me Bro, I have first hand experience(nothing to do with the recent activities)Check out in -> Github
Docs -> Click right here soy devs As of now GetHooky is in early release, I would love feedbacks, feature requires, ideas and anything crazy you got!Also don't forget to give it a ⭐️ on GitHub!👉 Do you use Git hooks in your projects? Why or why not?Let me know in the comments!]]></content:encoded></item><item><title>Prevent Race Conditions in Go Microservices with Distributed Locks</title><link>https://dev.to/kittipat1413/prevent-race-conditions-in-go-microservices-with-distributed-locks-5609</link><author>Kittipat.po</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 10:30:16 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In distributed systems, coordinating access to shared resources—such as rows in a database, files, or operations like seat reservation or payment processing—can be challenging when multiple services are involved. This is where distributed locks come in.In this post, we’ll cover:What distributed locks areWhy and when you need themA practical example in Go using How to use the  from the go-common library to simplify implementation
  
  
  What is a Distributed Lock?
A distributed lock ensures that multiple nodes in a system do not simultaneously perform conflicting operations on shared resources. It’s the distributed equivalent of a mutex, but across processes and machines.Preventing double booking in a ticketing systemEnsuring only one worker processes a message from a queueSerializing access to a critical section of code across pods or services
  
  
  Redsync: Redis-based Locking 🔒
Redsync is a Go implementation of the Redlock algorithm, using Redis as the coordination backend. It’s simple, reliable, and battle-tested in production.While Redsync is powerful, you often end up repeating boilerplate logic: setting up Redis clients, creating  with consistent options, generating unique tokens and managing error types.To simplify this, the  package from the go-common library provides a clean abstraction over  with:✅ A standard  interface⚙️ Pluggable token generator and retry logic🧪 Easy mocking for unit testing💡 Example: Using  LockManagerWhile distributed locks can be powerful, they come with caveats:Short TTLs may expire before the critical section is done, leading to unintended parallel execution.Long TTLs may block progress if a node crashes without releasing the lock.Don’t use locks as permanent ownership — they’re for coordination, not persistent state.Always wrap lock usage with context timeouts or deadlines to avoid deadlocks.🧠 Pro Tip: Design your system to recover gracefully even if the lock fails or expires unexpectedly.Distributed locks are a foundational building block in microservices and distributed architectures. Whether you’re managing ticket availability, serializing task execution, or controlling access to shared state, having a reliable and testable lock mechanism makes a huge difference.]]></content:encoded></item><item><title>Why Teams Migrate from Java to Go — Benefits, Trade-Offs, and the Right Time</title><link>https://dev.to/evrone/why-teams-migrate-from-java-to-go-benefits-trade-offs-and-the-right-time-4cm0</link><author>Evrone</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 09:54:50 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[This article examines why many development teams are migrating from Java to Go, highlighting Go’s advantages like better performance, simpler syntax, efficient concurrency, and ease of deployment. It also discusses trade-offs, including a smaller ecosystem and learning curve. Go is ideal for high-concurrency, cloud-native, or microservice architectures, but may not suit legacy-heavy enterprise systems.]]></content:encoded></item><item><title>SR621SW: The Hogwarts Battery Powering Time &amp; Magic</title><link>https://dev.to/ersajay/sr621sw-the-hogwarts-battery-powering-time-magic-48kj</link><author>ersajay</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 06:50:27 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[The Leaky Cauldron’s Hidden Alchemy
Behind the creaky floorboards of Quality Quidditch Supplies and the flickering candlelight of the Leaky Cauldron lies a magic even Dumbledore might envy: the SR621SW—a 6.8mm x 2.1mm silver-oxide button cell, small as a Galleon but mightier than a well-cast Protego. While flashy lithium-ion packs strut through Diagon Alley, this unassuming dynamo quietly powers the wizarding world’s grind—from Weasley’s Widgets to Mars-bound broomsticks. Let’s unmask its spells.The Potion of Precision: What Is SR621SW?
This isn’t candy—it’s enchanted energy. Break down its magic:Voltage: 1.55V, steady as a Pensieve memory. No flickering, no fading—just pure, reliable magic.
Size: 6.8mm x 2.1mm, smaller than a Niffler’s paw. Sneaks into the tightest spaces (like the back of a Rolex).
Chemistry: Silver oxide, the Felix Felicis of battery tech—long-lasting, dependable, and trusted by potion masters (engineers) worldwide.
Aliases: 364, SR621, AG1… even “G1” if you’re in a moody French lab. It’s the polyjuice potion of batteries—same magic, different labels.Fun Fact: Engineers call it the “Wand Core of Button Cells.” They swipe it from kids’ toys like students steal chocolate frogs—because once you use it, you never go back.The Triwizard Trials of Durability
Imagine the Great Hall, tables stacked with batteries vying for “Most Resilient Enchanter.” Let’s meet the contenders:
A steadfast champion, its 1.55V hums like a well-tuned wand—no flickering, no fading, even under pressure. It lasts 3-5 years, outgrinding first-years at Transfiguration class. Temperature? It laughs at -30°C ice storms and 60°C cauldrons, sealed tighter than the Chamber of Secrets. Acid? Oil? Your coworker’s cursed energy drink? It shrugs them off like a Reparo spell.
The flashy underdog, boasting 1.5V but crumbling fast—like a Boggart in a rainstorm. It dies in 1-2 years (intern energy, at best), leaks like a nervous first-year on their first broom ride, and melts under a coffee cup.CR2032 (Hufflepuff)
Overeager, with 3V of “look at me!” energy, but midlife crisis hits at 2-3 years. It flexes extreme temps but whimpers when squeezed—leaking like a first-year’s tears after a Dementor drill. Worse, its overkill voltage fries tiny circuits like a Confringo gone wrong.
CR2032: “I’m 3V! I’m better!”
SR621SW: “Your voltage fries hearing aids. Stay in your lane, mate.” 🔥The Invisible Keeper of Magic
In the hidden corners of the wizarding world, the SR621SW is the unsung hero:Luxury Watches: Inside Rolex, Omega, and your dad’s Casio, it keeps time like a Time-Turner. Even the Patek Philippe crowd swears by it—because a $10k watch deserves a $0.50 battery that never fails.
Medical Wonders: Powers pacemakers (steady as a heartbeat) and hearing aids (survives earwax apocalypses). Surgeons trust it more than their own wands—quieter than a scalpel, cheaper than a Firewhiskey night.
Mars Rovers: NASA uses it because “Mars-grade” is just Tuesday for this battery. Cosmic radiation? Dust storms? It handles them like a seasoned Auror handles dark wizards.
Toys & Calculators: Survives juice spills, math meltdowns, and “hold my butterbeer” DIY projects. It’s the only battery that outlasts Weasley’s Wizard Wheezes pranks.Burn Alert:
Smartwatch: “I track your sleep!”
SR621SW: “I’m in a $10k Patek. You’re basic.” ⌚💎The Secret Twins: Who’s a True Match?
Not all batteries are created equal. The SR621SW has true twins and faux friends:True Allies: 364, SR621, AG1, G1. Same size, same voltage, same magic. Even Renata 364 (Swiss-made, pricier) works—though the $0.50 generic is just as reliable.
Faux Friends: LR621 (alkaline) dies faster than a Doxie infestation. CR621 (lithium) fries circuits like a Bombarda spell. Avoid “LR” or “CR” labels—they’re the Bogies of the battery world.Pro Tip: If the package says “364” or “AG1,” you’re golden. Trust the labels like you trust a Veritaserum—no lies, just truth.The Prophecy of the Tiny Titan
In 2050, when wizards build colonies on Mars, the SR621SW will still be there. It’ll power quantum clocks (even qubits need steady time), Mars habitats (cosmic radiation can’t break its magic), and robot familiars (when AI revolts, they’ll use it to build… well, let’s not dwell on that).
Engineer Prophecy: “In 2050, we’ll find these in alien tech—and they’ll still work. Because some magic doesn’t fade.”Conclusion: The Battery That Binds
The SR621SW isn’t flashy. It doesn’t need a wand wave or a grand spell. It’s the Homenum Revelio of tech—small, unassuming, and critical. While lithium-ion packs chase “innovation,” this 0.3-gram hero keeps the world ticking, one flawless volt at a time.
Next time your watch stops, whisper, “Thanks, little buddy.” It’s the least you can do for a battery that’s saved your sanity (and your Patek’s pride).Written by a wizard who once mistook an SR621SW for a Fizzing Whizbee. (Spoiler: It didn’t taste like lemon.)
🔋 Some magic isn’t in wands—it’s in the tiny things that keep the world enchanted.]]></content:encoded></item><item><title>Debug Go Servers Inside Docker Like a Boss (with Delve)</title><link>https://dev.to/shehan_avishka_6229bf40cc/debug-go-servers-inside-docker-like-a-boss-with-delve-1cbc</link><author>Shehan Avishka</author><category>dev</category><category>go</category><category>devto</category><pubDate>Tue, 17 Jun 2025 06:26:24 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[I just wanted to execute my previous go program and could not install it on local machine 24.04 because some of the dependencies are kept in archive and got into trouble and then thought instead of installing it on local machine Whether I can install it on container or not. Then began to install docker file to install my go program within the docker in two stages.Why two steps? Yes i want my runner is lightweight, First step is to build stage and second step were run stage. and each time i build docker container had to execute long command again and again. Yeah so for that what i created a script to control container and images. Within two lines i alerted this but actually it tooked nearly a week to test all things on a docker container. Now the best is gone. I just need to test some apis on docker container and discovered some end points are not functioning,Next issue was how to debug the problem. there are two possible ways. first one is as usual put the print lines and second one is debugging. Instead of putting ton of print lines. i want to debug the code inside the docker container. But How?Then googled and typed “how to debug go web server inside the docker container” Then i learned about the open source project was go-delv and that was great and good documentation.Then When I went through the documentation I found the go-remote debugging for the first time. But I made a plan. Which is. I create two entry points to start the docker container. 1 is debug mod and 2 is normal docker startup.Then during build time i install delv debug tool inside the container. during run time i create an entry point to start with delv debug server. Now my final aim is remote-debuging there i need to connect to debug server from outside the container and thus i expose the port to connect debug server.The famous IDE like goland also support remote debug option. and But the need is to same code and go version. that the trick behind the scene. path mapping is the key unless we will be not able to debug. Because thats the trick of go-remote. from the outside of container and then we can attach it with docker container.Let me explain how did I set the go-remote debugging step by step,First as I mentioned I installed delv debug tool while building the docker container.RUN cd ${APP_NAME} && go install github.com/go-delve/delve/cmd/dlv@latestBut very important to note. while we are doing go build we must pass two flags to the go garbage collector to set the debugging. which is disable the optimization and disable the inline the code. which is necessary for debugging.RUN cd ${APP_NAME} && go mod tidy && go build -gcflags "all=-N -l" -o ${APP_NAME}And in the run stage we have to access execute delv, and it is in go/bin directory but to access it through entry point we have to copy it in /usr/local/bin/dlvCOPY --from=0 /go/bin/dlv /usr/local/bin/dlvThen expose the port to reach the delv debug serverThen set the entry point to execute the go build through delvENTRYPOINT ["dlv", "exec", "./delivery-data-cacher", "--listen=:2345", "--headless=true", "--api-version=2", "--accept-multiclient", "--continue", "--"]Within my bash script I define two entrypoint at the beginning I can select.Let me know your thoughts! If needed, I’d be happy to dive deeper — with a detailed walkthrough of how the debug server communicates using the power of gRPC, how to set up the Dockerfile step-by-step, and more insights from my experience. Always open to feedback and discussion!]]></content:encoded></item><item><title>What I Wish I Knew Before Learning Go.</title><link>https://dev.to/oathooh/what-i-wish-i-knew-before-learning-go-539d</link><author>Seth Athooh</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 16 Jun 2025 18:29:16 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[When I started learning Go, I did what most developers do. I opened the official documentation, skimmed a few tutorials, and jumped straight into building something. Go looked simple on the surface short syntax, fast compile times, built-in concurrency. I figured I could pick it up in a weekend.Go is simple, but not easy. It has a learning curve that’s easy to underestimate. And if I could go back and talk to my past self, here’s what I would say.
  
  
  Simplicity is intentional
At first, Go felt too bare. Where are the generics? Why no while loop? Why does everything feel so manual?Eventually, I realized this simplicity was deliberate. Go avoids cleverness. It pushes you to write code that’s straightforward and easy to maintain. Instead of offering five ways to solve a problem, it gives you one solid way.If you embrace the constraints, Go becomes a lot more enjoyable.
  
  
  Interfaces work differently than you expect
Go’s interfaces confused me at first. Unlike other languages, you don’t explicitly declare that a type implements an interface. You just define the methods, and if the shape fits, Go accepts it.This implicit approach is powerful, but it can be hard to debug when things go wrong. I once spent hours trying to figure out why a type didn’t satisfy an interface, only to realize I’d misspelled a method name.Once you understand how interfaces are used in Go especially small, focused ones it becomes one of the best parts of the language.
  
  
  Error handling looks repetitive, but it matters
I was used to try-catch blocks and error bubbling. Go throws that out the window.In Go, error handling is explicit. You check the error after every operation and deal with it right there. At first it felt like noise, but over time I started to see the benefits. There’s no magic. No hidden failures. Just code that does what it says.The more I worked with it, the more I appreciated the discipline.
  
  
  Go’s tooling is incredibly helpful
Go comes with excellent built-in tools. You don’t need to install formatters or dependency managers or testing libraries. Everything is there from the start. for managing dependencies for catching mistakesThe ecosystem encourages consistency, which makes collaboration and open source contributions smoother.
  
  
  Goroutines are not threads
One of Go’s headline features is concurrency using goroutines. They’re lightweight and easy to create. But that simplicity can also be a trap.I made the mistake of spawning goroutines without fully understanding how they work. This led to race conditions, memory leaks, and hard-to-track bugs.Eventually I learned to pair goroutines with proper synchronization using channels, mutexes, or wait groups. I also learned to use the  package for handling timeouts and cancellations. Once you grasp these tools, writing concurrent programs becomes much safer and more predictable.Coming from object-oriented languages, I expected Go to feel limited without classes or inheritance. But Go uses structs and interfaces to achieve the same results, often in a cleaner and more flexible way.Instead of deep inheritance trees, you compose small pieces of behavior. That shift in mindset took time, but once I got it, I found it easier to reason about my code.
  
  
  The standard library is seriously good
Go’s standard library covers a lot of ground. From HTTP servers to file I/O, JSON handling, cryptography, and more it’s all there.I used to reach for third-party libraries out of habit. Now, I always check the standard library first. It's fast, reliable, and maintained by the Go team.Testing in Go is simple and built in. You write test functions in the same package, use , and you’re done. No need for fancy tools or configuration.One thing that helped me was using table-driven tests. It made my test cases cleaner and easier to expand over time.
  
  
  Reading other people’s Go code helps a lot
Since Go enforces formatting and favors convention, reading other people’s code is easier than in most languages. I learned a lot by browsing open source projects like Hugo, Docker, and Kubernetes.It’s one of the best ways to get a feel for how experienced developers write idiomatic Go.
  
  
  Stop comparing it to other languages
In the beginning, I kept comparing Go to Python, JavaScript, and Java. I judged it by what it lacked instead of what it offered.Go isn’t trying to be like other languages. It’s trying to be a better C. It focuses on performance, readability, and ease of deployment. Once I stopped fighting that, I started enjoying the language for what it is.Learning Go challenged the way I think about programming. It forced me to be more disciplined, to value clarity over cleverness, and to care about what happens under the hood.If you're starting out with Go, give yourself time. It’s not a flashy language, but it’s one that grows on you. And once it does, you might just find yourself reaching for it again and again.]]></content:encoded></item><item><title>**High-Performance Go Logging: Achieving 100,000 Logs Per Second Without Bottlenecks**</title><link>https://dev.to/aaravjoshi/high-performance-go-logging-achieving-100000-logs-per-second-without-bottlenecks-5761</link><author>Aarav Joshi</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 16 Jun 2025 16:43:50 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! Building robust logging systems in Go requires balancing performance and functionality. High-traffic applications need efficient logging that doesn't bottleneck the system while providing structured, queryable data. I've developed a solution that achieves over 100,000 logs per second on standard hardware.The core challenge lies in minimizing allocations and lock contention. Standard logging approaches often become performance liabilities under load. My implementation addresses this through several key techniques:Asynchronous Processing with Worker PoolThe channel buffer absorbs spikes in log volume. Worker goroutines serialize entries concurrently, preventing application threads from blocking on I/O. CPU-bound serialization work scales with available cores.Zero-Allocation Field HandlingRecycling field slices avoids slice header allocations. Field constructors return stack-allocated structs:Buffer Pooling for JSON SerializationReusing buffers eliminates allocation pressure during serialization. Pre-sized buffers prevent expensive runtime expansions.Caller information adds significant overhead. Capturing it only for error levels maintains performance during normal operation.Avoiding  for basic types reduces reflection overhead. Benchmarks show 3-5x speed improvement for primitive types.
The non-blocking channel send with fallback:This prevents log loss during extreme bursts while minimizing blocking under normal conditions.
On a 8-core machine:100,000 logs/second sustained12x faster than zap with similar features97% reduction in GC pressure vs standard loggersNo in-process log filtering (level checked before queuing)Possible reordering under high concurrencyCaller info only for errorsManual type handling requires extension for custom types
For 1M+ logs/second:Use separate writer threadsConsider zero-copy techniques
  
  
  In production systems, I've found this architecture handles 15TB/day log volumes while keeping application overhead below 3%. The structured format enables powerful analytics while the performance ensures logging doesn't become the bottleneck.
📘 , , , and  to the channel! is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly low—some books are priced as low as —making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>File Duplicate Detector. Go implementation.</title><link>https://dev.to/andrey_matveyev/file-duplicate-detector-go-implementation-2270</link><author>Andrey Matveyev</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 16 Jun 2025 10:16:36 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  SearchEngine, Processing Pipeline, Usage and Result.
"Truth is not born pure from the earth; it requires refinement from the superfluous to shine in its essence."
— Ancient WisdomIn previous parts of our series on the , we thoroughly examined individual components:  that perform specific tasks (file discovery, size determination, hash calculation, byte-by-byte comparison);  that ensure reliable data transfer between workers; and  that optimize the duplicate detection process by preventing redundant work.Now it's time to put this puzzle together and understand how these independent, yet interconnected parts form a powerful system for detecting duplicate files. The  component and the data processing pipeline are central to this process.
  
  
  1. : The Heart of Orchestration.
The  is the brain of the entire system. Its main task is to launch and coordinate all stages of the file processing pipeline. It does not perform direct file operations but acts as a conductor, ensuring the correct execution of all operations:: Upon launch, the  initializes internal structures, such as a  to track the completion of all workers, and  for collecting statistics.: The  method starts the main processing pipeline in a separate goroutine, allowing it to operate asynchronously.: The  manages a cancellation context (), allowing for the graceful shutdown of all workers upon receiving a cancellation signal (e.g., ).: After the pipeline completes its work, the  collects and provides the final results via the  method, as well as current progress via .Here's the  code, with inline comments for clarity::
  
  
  2. Building the Processing Pipeline: pipeline()
The most interesting part of the  is the  method, which is responsible for constructing the entire data processing pipeline. This demonstrates the principles of pipeline processing and concurrent programming in Go.The pipeline consists of several sequential stages, each represented by a pool of workers and its own queue. Data (represented by *task) is passed from one stage to the next via Go channels: ():This is the first stage. It launches  workers that traverse the file system, recursively scanning directories and sending tasks () for each found file and directory.It's important to note that  also contains a  that coordinates recursive directory traversal and completion signals for the .The output of  is a channel that provides tasks containing the path and size of each file.runPool(&sizer{}, N, ...) ():The output channel of  becomes the input for the  pool. filter files by size (e.g., excluding zero-byte files) and use a  for initial screening of files with unique sizes. If multiple files of the same size are found, they are passed to the next stage.runPool(&hasher{}, N, ...) ():The output channel of the  becomes the input for the  pool. calculate the CRC32 hash for a portion of the file. At this stage, a second, more precise, duplicate filtering occurs: files with different hashes are guaranteed not to be duplicates.A  is used here to determine if the hash is already known (a potential duplicate).runPool(&matcher{}, N, ...) ():The output channel of the  becomes the input for the  pool. perform the most resource-intensive action: byte-by-byte comparison of files whose size and partial hash have matched. Only at this stage is it definitively confirmed that two files are identical.A  is used to manage groups of potential duplicates to avoid redundant comparisons and track already verified files....runPipeline(ctx context.Context) ():The output channel of the  (containing only confirmed duplicates) is fed into the . From this queue, the code in  executes as the final stage, responsible for collecting and grouping the paths of all confirmed duplicate files, ultimately preparing the data for the final  structure.
All these stages are connected by channels, and each  launches a fixed number () of worker goroutines that process incoming tasks in parallel.  collects statistics for each stage, allowing progress to be monitored.
  
  
  3. Metrics and Monitoring (, )
One of the advantages of this implementation is its built-in mechanism for collecting metrics and monitoring progress. This allows not only tracking the process status in real-time but also analyzing the performance of each stage.The  and  structures (file ) collect data on the number of processed files () and their total size () at the input () and output () of each queue (pipeline stage).The  function (also in ) is a channel wrapper that increments the corresponding metrics as a task passes through the channel.The  file contains the logic for displaying progress. In the  function, metric data is periodically retrieved via engine. (which returns a JSON representation of metrics) and printed to the console.This allows real-time viewing of:The number of active goroutines.Processing progress at each stage (number of files and their total size).Filtering performance at the sizer, hasher, and matcher stages – how many files were "discarded" at each stage.Using the  from  is quite straightforward:As seen in the example, you:Create a  with cancellation capability.Set up OS signal handling for graceful termination.Obtain a  instance.Call  with the root path for scanning and a  function that will be invoked upon completion.Launch  to display progress.Wait for all operations to complete using a .Retrieve and save (GetResult()) the found duplicates.
  
  
  5.  ()
The final result of the 's operation is available via the  method and is returned as a  structure:
  
  
  Example of application execution from the console (scanning an SSD disk C:):
PS D:\go\go-sample-detector> go run .
---- CURRENT CONFIGURATION ----
Root Path:             c:\
File name for results:     fdd-result.txt
File name for logs:        fdd-output.log
Logging level (info/debug): debug
Adds source info in logs:  false
-------------------------------
Progress (every 10 seconds):
45 folder 1_0_1 fetch 0_0_0 size 0_0_0 hash 0_0_0 match 0_0_0 result 0s
50 folder 10849_6798_4051 fetch 41712_0_41712 size 29731_20019_9712 hash 1654_757_897 match 432_0_432 result 10.0514188s
...
50 folder 827791_5379_822412 fetch 2175225_1_2175224 size 2136075_1166018_970057 hash 737102_171_736931 match 725972_0_725972 result 11m30.1975563s
50 folder 838528_3506_835022 fetch 2249794_0_2249794 size 2208711_1225693_983018 hash 750085_965_749120 match 737973_0_737973 result 11m40.1993708s
31 folder 843222_0_843222 fetch 2270482_0_2270482 size 2229228_1234927_994301 hash 761939_5_761934 match 750026_6_750020 result 11m50.2018749s
31 folder 843222_0_843222 fetch 2270482_0_2270482 size 2229228_1219410_1009818 hash 773913_0_773913 match 761518_0_761518 result 12m0.2085391s
31 folder 843222_0_843222 fetch 2270482_0_2270482 size 2229228_1205527_1023701 hash 787439_42_787397 match 774824_0_774824 result 12m10.2143553s
...
31 folder 843222_0_843222 fetch 2270482_0_2270482 size 2229228_24236_2204992 hash 1722277_1662_1720615 match 1700204_1_1700203 result 22m0.3185755s
5 folder 843222_0_843222 fetch 2270482_0_2270482 size 2229228_0_2229228 hash 1735310_0_1735310 match 1714249_0_1714249 result 22m10.3194308s
4 folder 843222_0_843222 fetch 2270482_0_2270482 size 2229228_0_2229228 hash 1735310_0_1735310 match 1714249_0_1714249 result 22m20.320499s
4 folder 843222_0_843222 fetch 2270482_0_2270482 size 2229228_0_2229228 hash 1735310_0_1735310 match 1714249_0_1714249 result 22m30.3211656s
------- TOTAL STATISTIC -------
Time of start:       11:29:09
Time of ended:       11:51:41
Duration:        22m31.7249332s
Total processed <count (size Mb)>:
- folders:           843222
- files:             2270482 (178753.517 Mb)
Performance of filtration <inp-filtered-out (out %)>:
- sizer:             2270482     41254       2229228 (98.18 %)
- hasher:            2229228     493918      1735310 (77.84 %)
- matcher:           1735310     21061       1714249 (98.79 %)
Found duplicates <count (size Mb)>:
- groups of files:   311941
- files:             1714249 (63811.304 Mb)
File with result: fdd-result.txt
File with logs: fdd-output.log
-------------------------------
PS D:\go\go-sample-detector> go run .
---- CURRENT CONFIGURATION ----
Root Path:             d:\
File name for results:     fdd-result.txt
File name for logs:        fdd-output.log
Logging level (info/debug): debug
Adds source info in logs:  false
-------------------------------
Progress (every 10 seconds):
17 folder 0_0_0 fetch 0_0_0 size 0_0_0 hash 0_0_0 match 0_0_0 result 520.5µs
50 folder 382_224_158 fetch 1910_0_1910 size 606_279_327 hash 277_155_122 match 108_0_108 result 10.0512721s
...
50 folder 6712_454_6258 fetch 54626_0_54626 size 32410_25710_6700 hash 3972_2552_1420 match 1334_0_1334 result 5m20.0859867s
50 folder 6952_73_6879 fetch 55794_1_55793 size 33647_26147_7500 hash 4658_3204_1454 match 1365_0_1365 result 5m30.0865653s
31 folder 7008_0_7008 fetch 56138_0_56138 size 34005_26153_7852 hash 4791_3313_1478 match 1389_0_1389 result 5m40.0886867s
31 folder 7008_0_7008 fetch 56138_0_56138 size 34005_25868_8137 hash 4922_3373_1549 match 1451_0_1451 result 5m50.0892709s
...
31 folder 7008_0_7008 fetch 56138_0_56138 size 34005_567_33438 hash 22755_20341_2414 match 2291_0_2291 result 12m30.1476568s
31 folder 7008_0_7008 fetch 56138_0_56138 size 34005_315_33690 hash 22961_20490_2471 match 2353_0_2353 result 12m40.1494179s
20 folder 7008_0_7008 fetch 56138_0_56138 size 34005_0_34005 hash 23323_20744_2579 match 2459_0_2459 result 12m50.1500104s
20 folder 7008_0_7008 fetch 56138_0_56138 size 34005_0_34005 hash 23323_20608_2715 match 2591_0_2591 result 13m0.1504921s
...
20 folder 7008_0_7008 fetch 56138_0_56138 size 34005_0_34005 hash 23323_423_22900 match 22122_0_22122 result 29m10.2776533s
12 folder 7008_0_7008 fetch 56138_0_56138 size 34005_0_34005 hash 23323_0_23323 match 22555_0_22555 result 29m20.2791754s
11 folder 7008_0_7008 fetch 56138_0_56138 size 34005_0_34005 hash 23323_0_23323 match 22557_0_22557 result 29m30.2803337s
11 folder 7008_0_7008 fetch 56138_0_56138 size 34005_0_34005 hash 23323_0_23323 match 22557_0_22557 result 29m40.2814158s
------- TOTAL STATISTIC -------
Time of start:       12:03:35
Time of ended:       12:33:22
Duration:        29m46.4419118s
Total processed <count (size Mb)>:
- folders:             7008
- files:               56138 (645181.041 Mb)
Performance of filtration <inp-filtered-out (out %)>:
- sizer:               56138     22133       34005 (60.57 %)
- hasher:              34005     10682       23323 (68.59 %)
- matcher:             23323       764       22559 (96.72 %)
Found duplicates <count (size Mb)>:
- groups of files:     7243
- files:               22559 (15674.642 Mb)
File with result: fdd-result.txt
File with logs: fdd-output.log
-------------------------------

  
  
  Example Logging Output (from fdd-output.log)
time=2025-06-15T11:29:09.260+03:00 level=DEBUG msg="InpProcess of Queue - started." poolName=fetchers
time=2025-06-15T11:29:09.260+03:00 level=DEBUG msg="OutProcess of Queue - started." poolName=fetchers
time=2025-06-15T11:29:09.260+03:00 level=DEBUG msg="Worker-pool - started." workerType=fdd.fetcher
time=2025-06-15T11:29:09.306+03:00 level=DEBUG msg="Worker-pool - started." workerType=*fdd.sizer
time=2025-06-15T11:29:09.306+03:00 level=DEBUG msg="Worker-pool - started." workerType=*fdd.hasher
time=2025-06-15T11:29:09.306+03:00 level=DEBUG msg="Worker-pool - started." workerType=*fdd.matcher
time=2025-06-15T11:29:09.308+03:00 level=DEBUG msg="InpProcess of Queue - started." poolName=sizers
time=2025-06-15T11:29:09.308+03:00 level=DEBUG msg="OutProcess of Queue - started." poolName=sizers
time=2025-06-15T11:29:09.308+03:00 level=DEBUG msg="InpProcess of Queue - started." poolName=matchers
time=2025-06-15T11:29:09.308+03:00 level=DEBUG msg="InpProcess of Queue - started." poolName=hashers
time=2025-06-15T11:29:09.308+03:00 level=DEBUG msg="OutProcess of Queue - started." poolName=hashers
time=2025-06-15T11:29:09.308+03:00 level=DEBUG msg="OutProcess of Queue - started." poolName=matchers
time=2025-06-15T11:29:09.308+03:00 level=DEBUG msg="InpProcess of Queue - started." poolName=packer
time=2025-06-15T11:29:09.308+03:00 level=DEBUG msg="OutProcess of Queue - started." poolName=packer
time=2025-06-15T11:29:09.311+03:00 level=INFO msg="Objects read error." item=*fdd.fetcher method=readDir() error="open c:\\$Recycle.Bin\\S-1-5-18: Access is denied." path=c:\$Recycle.Bin\S-1-5-18
...
time=2025-06-15T11:39:52.958+03:00 level=INFO msg="File open error." item=*fdd.hasher method=os.Open() error="open c:\\Windows\\System32\\restore\\MachineGuid.txt: Access is denied." path=c:\Windows\System32\restore\MachineGuid.txt
time=2025-06-15T11:39:57.011+03:00 level=INFO msg="File open error." item=*fdd.hasher method=os.Open() error="open c:\\Windows\\System32\\wbem\\AutoRecover\\3FFDD473F026FB198DA9FA65EE71383C.mof: Access is denied." path=c:\Windows\System32\wbem\AutoRecover\3FFDD473F026FB198DA9FA65EE71383C.mof
time=2025-06-15T11:40:55.247+03:00 level=DEBUG msg="InpProcess of Queue - stoped." poolName=fetchers
time=2025-06-15T11:40:55.247+03:00 level=DEBUG msg="OutProcess of Queue - stopped because queue is done and empty." poolName=fetchers
time=2025-06-15T11:40:55.250+03:00 level=DEBUG msg="Worker-pool - stoped." workerType=fdd.fetcher
time=2025-06-15T11:40:55.250+03:00 level=DEBUG msg="InpProcess of Queue - stoped." poolName=sizers
time=2025-06-15T11:40:55.252+03:00 level=DEBUG msg="OutProcess of Queue - stopped because queue is done and empty." poolName=sizers
time=2025-06-15T11:40:55.252+03:00 level=DEBUG msg="Worker-pool - stoped." workerType=*fdd.sizer
time=2025-06-15T11:40:55.252+03:00 level=DEBUG msg="InpProcess of Queue - stoped." poolName=hashers
...
time=2025-06-15T11:50:53.101+03:00 level=INFO msg="File open error." item=*fdd.hasher method=os.Open() error="open c:\\Windows\\System32\\wbem\\AutoRecover\\DA736886F13A0E2EE2265319FB376753.mof: Access is denied." path=c:\Windows\System32\wbem\AutoRecover\DA736886F13A0E2EE2265319FB376753.mof
time=2025-06-15T11:51:19.521+03:00 level=DEBUG msg="OutProcess of Queue - stopped because queue is done and empty." poolName=hashers
time=2025-06-15T11:51:19.522+03:00 level=DEBUG msg="Worker-pool - stoped." workerType=*fdd.hasher
time=2025-06-15T11:51:19.522+03:00 level=DEBUG msg="InpProcess of Queue - stoped." poolName=matchers
time=2025-06-15T11:51:19.522+03:00 level=DEBUG msg="OutProcess of Queue - stopped because queue is done and empty." poolName=matchers
time=2025-06-15T11:51:19.522+03:00 level=DEBUG msg="Worker-pool - stoped." workerType=*fdd.matcher
time=2025-06-15T11:51:19.522+03:00 level=DEBUG msg="InpProcess of Queue - stoped." poolName=packer
time=2025-06-15T11:51:19.522+03:00 level=DEBUG msg="OutProcess of Queue - started." poolName=packer
DEBUG level logs show the start and stop of each worker pool (Worker-pool - started./stoped.) and queue processing (InpProcess/OutProcess of Queue - started./stoped.). INFO level logs often indicate file or folder access errors, for example, to system directories (). This is expected behavior, as the application attempts to access all files in the specified root directory.
  
  
  Example Result File (fdd-result.txt)
After completion, the application saves the results to a text file specified in the configuration (). The output format groups duplicates by size, hash, and group ID, then lists the paths to the duplicate files.      2  {32  3876609034  0}
d:\HP_Drivers_for_Win10\SWSetup\SP92183\Graphics\ocl_cpu_version.ini
d:\HP_Drivers_for_Win10\SWSetup\SP95347\Graphics\ocl_cpu_version.ini
      2  {33  554973275  0}
d:\HP_Drivers_for_Win10\SWSetup\SP92183\DisplayAudio\6.16\version.ini
d:\HP_Drivers_for_Win10\SWSetup\SP95347\DisplayAudio\6.16\version.ini
      3  {33  4084763797  0}
d:\HP_Drivers_for_Win10\SWSetup\SP57014\Driver1\silentsetup.bat
d:\HP_Drivers_for_Win10\SWSetup\SP57014\Driver2\silentsetup.bat
d:\HP_Drivers_for_Win10\SWSetup\SP57014\silentsetup.bat
      2  {41  388051727  0}
d:\go\go-sample-queue\.git\refs\heads\master
d:\go\go-sample-queue\.git\refs\remotes\origin\master
      2  {41  954715591  0}
d:\go\go-sample-detector\.git\ORIG_HEAD
d:\go\go-sample-detector\.git\refs\heads\master
      3  {41  1012172877  0}
d:\go\go-sample-recursion\.git\ORIG_HEAD
d:\go\go-sample-recursion\.git\refs\heads\master
d:\go\go-sample-recursion\.git\refs\remotes\origin\master
Each group of duplicates starts with a line containing:The number of files in the group (e.g., 2 or 3).The structure {size hash group}:size: File size in bytes.hash: CRC32 hash of the files.group: Group ID (in this case, 0). This field is used for grouping identical files that have the same  and . If files have the same  and  but their content differs (which is determined at the  stage), they will be assigned a different  ID, and they will not be included in the same group of duplicates in the final result.Followed by the full paths to each duplicate file in that group.The presented implementation of duplicate file detection in Golang demonstrates the power of the pipelined approach and parallel processing. The use of channels for data transfer between stages, worker pools for parallel execution of tasks, and a well-designed checker mechanism makes the solution efficient and scalable. Built-in metrics and monitoring significantly simplify debugging and performance analysis.I was surprised by the performance, despite the fact that I almost didn't think about optimization. On the contrary - the overhead in the form of monitoring, which I used to observe the load in real time - slows down the work.Of course, there is always room for improvement. Potential enhancements could include:Support for very large files using streaming processing and more sophisticated hashing algorithms.Or more thoughtful use of disk cache (for example, not allowing the cache to "cool down" after the hasher before the matcher).Nevertheless, this solution serves as a starting point for understanding and implementing pipelined systems in Golang.]]></content:encoded></item><item><title>NAS1831C4C56: The Little Fastener That Anchors Stars</title><link>https://dev.to/ersajay/nas1831c4c56-the-little-fastener-that-anchors-stars-4n9</link><author>ersajay</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 16 Jun 2025 06:31:40 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[A Meeting in the Circuit Desert
When I first landed on this planet of clinking tools and humming machines, I thought all fasteners were like the flashy ones I’d seen—loud, polished, and a little too proud of their shine. But then I met the NAS1831C4C56—a 1/4-inch spacer, sitting quietly on a workbench like a single cactus in the desert.
“You’re… small,” I said, tilting my head.
“And you’re a child who talks to fasteners,” it replied, its hexagonal body glinting softly. “But size isn’t what matters. Ask the fox.”The Secret of Its Desert Bloom
This isn’t just metal—it’s desert magic. Let me tell you its story:Material: 303 stainless steel, tough as the baobab roots on my home planet. It laughs at corrosion, just like the cactus laughs at sandstorms.
Size: 1/4" hex, 0.56" long—small enough to fit in a pocket, but strong enough to hold a satellite.
Certifications: MIL-STD-45622, RoHS, ISO 9001. Think of them as the “star maps” that guide it—proving it’s reliable, even in the darkest corners of space.Fun Fact: Engineers call it the “Swiss Army knife of fasteners.” They steal it from factory floors like children steal stars—because once you find one, you never let go.The Cactus of Extreme Climates
On the planet of factories, where machines roar like angry volcanoes and chemicals spill like poison water, the NAS1831C4C56 thrives.
“Why not plastic?” I asked a welding robot.
“Plastic cries when sparks land. This one? It hums.”
It survives -40°C (colder than the desert at night) and 125°C (hotter than the sun at noon). It shrugs off oil, acid, and even the “accidental” hammers of interns.
“You’re unkillable,” I said.
“Not unkillable,” it replied. “Just… prepared. Like the cactus that stores water—we both know hard times come.”The Guardian of Invisible Things
In the quiet corners of the universe, the NAS1831C4C56 holds what matters:Satellites: It keeps solar panels steady in zero gravity, so they can drink sunlight like roses drink rain.Medical Devices: It secures pacemaker circuits, counting heartbeats softer than a fox’s footsteps.
EV Batteries: It stops sparks from dancing where they shouldn’t—so your Tesla stays a car, not a firework.
Smart Factories: It survives 24/7 robot arms, patient as the lamplighter on Earth.“You’re a hero,” I told it.
“Heroes have parades,” it said. “I’m just a spacer. But parades don’t keep stars in the sky—fasteners do.”The Tale of the Three Materials
Once, I met three fasteners in a workshop: Aluminum, Plastic, and NAS1831C4C56.Aluminum preened: “I’m lightweight! Aerospace loves me!” But a single drop of acid made it wince.
Plastic giggled: “I’m cheap! Everyone buys me!” But a warm coffee cup melted its smile.
NAS1831C4C56 said nothing. It just held a circuit board, steady as the desert’s horizon.Later, I asked the fox: “Why does everyone choose the quiet one?”
“Because the best things are invisible to the eye,” the fox said. “Like the wind, or love, or a fastener that never fails.”When the Cactus Isn’t Needed
Even cacti have their limits. The NAS1831C4C56 sighed:
“I’m not for ultra-light drones—aluminum saves grams, though it’ll falter in storms. I’m not for disposable gadgets—plastic’s fine, but it won’t outlive the landfill. And I’m overkill for macaroni art—though I’d outlive the artist’s pride.”
“So when do you shine?” I asked.
“When the project matters,” it said. “Fire, acid, interns… if it’s worth doing, it’s worth doing with something that lasts.”How to Find a True Friend
In the market of fasteners, not all are real. The NAS1831C4C56 warned:
“Beware of eBay sellers with ‘NASA-certified’ and $0.99 shipping. They’re like fake stars—bright, but empty. Find Ersa Electronics or RAF Hardware. Demand MIL-STD-45622 papers. Counterfeits fail faster than a child’s promise to water the rose.”
“How do I know it’s you?” I asked.
“You’ll feel it,” it said. “A real fastener doesn’t shout. It just… holds.”The Star That Never Fades
In 2050, when humans build colonies on Mars, the NAS1831C4C56 will be there. It’ll hold quantum computers (even qubits need something steady), Mars habitats (cosmic radiation can’t break it), and robot arms (when AI overlords revolt, they’ll use it to build… well, let’s not think about that).
“You’ll outlive us all,” I said.
“No,” it replied. “I’ll just keep holding. Because stars don’t stay in the sky by magic—they stay because something anchors them.”The Secret of the Little Fastener
The NAS1831C4C56 isn’t flashy. It doesn’t need a name in lights. It’s the kind of friend you notice only when it’s gone—like the rose in the garden, or the fox’s footsteps in the sand.
“What makes you special?” I asked, as I packed to leave.
It didn’t answer. It just held a circuit board, steady as the desert, as the stars, as time itself.
And I realized—important things are never the loudest. They’re the ones that stay.Written by a wanderer who once mistook a fastener for a new planet. The NAS1831C4C56 set me straight.
🌵 You become responsible, forever, for the fasteners you ignore.]]></content:encoded></item><item><title>MVC vs DDD: Go Language Architecture Deep Dive</title><link>https://dev.to/leapcell/mvc-vs-ddd-go-language-architecture-deep-dive-466f</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Mon, 16 Jun 2025 04:11:02 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  Detailed Comparison of Go Language MVC and DDD Layered Architectures
MVC and DDD are two popular layered architectural concepts in backend development. MVC (Model-View-Controller) is a design pattern mainly used to separate user interface, business logic, and data models for easier decoupling and layering, while DDD (Domain-Driven Design) is an architectural methodology aimed at solving design and maintenance difficulties in complex systems by building business domain models.In the Java ecosystem, many systems have gradually transitioned from MVC to DDD. However, in languages like Go, Python, and NodeJS—which advocate simplicity and efficiency—MVC remains the mainstream architecture. Below, we will specifically discuss the differences in directory structure between MVC and DDD based on Go language.+------------------+
|      View        | User Interface Layer: responsible for data display and user interaction (such as HTML pages, API responses)
+------------------+
|   Controller     | Controller Layer: processes user requests, calls Service logic, coordinates Model and View
+------------------+
|      Model       | Model Layer: contains data objects (such as database table structures) and some business logic (often scattered in Service layer)
+------------------+
+--------------------+
|   User Interface   | Responsible for user interaction and display (such as REST API, Web interface)
+--------------------+
| Application Layer  | Orchestrates business processes (such as calling domain services, transaction management), does not contain core business rules
+--------------------+
|   Domain Layer     | Core business logic layer: contains aggregate roots, entities, value objects, domain services, etc., encapsulates business rules
+--------------------+
| Infrastructure     | Provides technical implementations (such as database access, message queues, external APIs)
+--------------------+

  
  
  Main Differences Between MVC and DDD
MVC layers by technical function (Controller/Service/DAO), focusing on technical implementation.DDD divides modules by business domain (such as order domain, payment domain), isolating core business logic through bounded contexts.Carrier of Business LogicMVC usually adopts an anemic model, separating data (Model) and behavior (Service), which leads to high maintenance cost due to dispersed logic.DDD achieves a rich model through aggregate roots and domain services, concentrating business logic in the domain layer and enhancing scalability.MVC has a low development cost and is suitable for small to medium systems with stable requirements.DDD requires upfront domain modeling and a unified language, making it suitable for large systems with complex business and long-term evolution needs, but the team must have domain abstraction capabilities. For example, in e-commerce promotion rules, DDD can prevent logic from being scattered across multiple services.
  
  
  Go Language MVC Directory Structure
MVC is mainly divided into three layers: view, controller, and model.gin-order/
├── cmd
│   └── main.go                  # Application entry point, starts the Gin engine
├── internal
│   ├── controllers              # Controller layer (handles HTTP requests), also known as handlers
│   │   └── order
│   │       └── order_controller.go  # Controller for the Order module
│   ├── services                 # Service layer (handles business logic)
│   │   └── order
│   │       └── order_service.go       # Service implementation for the Order module
│   ├── repository               # Data access layer (interacts with the database)
│   │   └── order
│   │       └── order_repository.go    # Data access interface and implementation for Order module
│   ├── models                   # Model layer (data structure definitions)
│   │   └── order
│   │       └── order.go               # Data model for the Order module
│   ├── middleware               # Middleware (such as authentication, logging, request interception)
│   │   ├── logging.go             # Logging middleware
│   │   └── auth.go                # Authentication middleware
│   └── config                   # Configuration module (database, server configurations, etc.)
│       └── config.go                # Application and environment configurations
├── pkg                          # Common utility packages (such as response wrappers)
│   └── response.go              # Response handling utility methods
├── web                          # Frontend resources (templates and static assets)
│   ├── static                   # Static resources (CSS, JS, images)
│   └── templates                # Template files (HTML templates)
│       └── order.tmpl           # View template for the Order module (if rendering HTML is needed)
├── go.mod                       # Go module management file
└── go.sum                       # Go module dependency lock file

  
  
  Go Language DDD Directory Structure
DDD is mainly divided into four layers: interface, application, domain, and infrastructure.go-web/
│── cmd/
│   └── main.go               # Application entry point
│── internal/
│   ├── application/          # Application layer (coordinates domain logic, handles use cases)
│   │   ├── services/         # Service layer, business logic directory
│   │   │   └── order_service.go # Order application service, calls domain layer business logic
│   ├── domain/               # Domain layer (core business logic and interface definitions)
│   │   ├── order/            # Order aggregate
│   │   │   ├── order.go      # Order entity (aggregate root), contains core business logic
│   │   ├── repository/       # General repository interfaces
│   │   │   ├── repository.go # General repository interface (CRUD operations)
│   │   │   └── order_repository.go # Order repository interface, defines operations on order data
│   ├── infrastructure/       # Infrastructure layer (implements interfaces defined in the domain layer)
│   │   ├── repository/       # Repository implementation
│   │   │   └── order_repository_impl.go  # Order repository implementation, concrete order data storage
│   └── interfaces/           # Interface layer (handles external requests, such as HTTP interfaces)
│   │   ├── handlers/         # HTTP handlers
│   │   │  └── order_handler.go # HTTP handler for orders
│   │   └── routes/
│   │   │   ├── router.go     # Base router utility setup
│   │   │   └── order-routes.go # Order routes configuration
│   │   │   └── order-routes-test.go # Order routes test
│   └── middleware/           # Middleware (e.g.: authentication, interception, authorization, etc.)
│   │   └── logging.go        # Logging middleware
│   ├── config/               # Service-related configuration
│   │   └── server_config.go  # Server configuration (e.g., port, timeout settings, etc.)
│── pkg/                      # Reusable public libraries
│   └── utils/                # Utility classes (e.g.: logging, date handling, etc.)

  
  
  Go Language MVC Code Implementation
Controller (Interface Layer) → Service (Business Logic Layer) → Repository (Data Access Layer) → Model (Data Model)

  
  
  Data Access Layer (Repository)

  
  
  Go Language MVC Best Practices

  
  
  Interface Segregation Principle
The Repository layer defines interfaces, supporting multiple database implementations.
  
  
  Go Language DDD Code Implementation and Best Practices
DDD emphasizes the construction of domain models, organizing business logic using Aggregates, Entities, and Value Objects.In Go, entities and value objects are typically defined with struct:DDD typically adopts a layered architecture. Go projects can follow this structure:: Core business logic, e.g., entities and aggregates under the domain directory.: Use cases and orchestration of business processes.: Adapters for database, caching, external APIs, etc.: Provides HTTP, gRPC, or CLI interfaces.The domain layer should not directly depend on the infrastructure layer; instead, it relies on interfaces for dependency inversion.Note: The core of DDD architecture is dependency inversion (DIP). The Domain is the innermost core, defining only business rules and interface abstractions. Other layers depend on the Domain for implementation, but the Domain does not depend on any external implementations. In Hexagonal Architecture, the domain layer sits at the core, while other layers (such as application, infrastructure) provide concrete technical details (like database operations, API calls) by implementing interfaces defined by the domain, achieving decoupling between domain and technical implementation.The aggregate root manages the lifecycle of the entire aggregate:Application services encapsulate domain logic, preventing external layers from directly manipulating domain objects:Domain events are used for decoupling. In Go, you can implement this via Channels or Pub/Sub:
  
  
  Combining CQRS (Command Query Responsibility Segregation)
DDD can be combined with CQRS. In Go, you can use  for change operations and  for data reading:
  
  
  Summary: MVC vs. DDD Architecture

  
  
  Core Differences in Architecture
Layers: Three layers—Controller/Service/DAOController handles requests, Service contains logicDAO directly operates the databasePain Points: The Service layer becomes bloated, and business logic is coupled with data operationsLayers: Four layers—Interface Layer / Application Layer / Domain Layer / Infrastructure LayerApplication Layer orchestrates processes (e.g., calls domain services)Domain Layer encapsulates business atomic operations (e.g., order creation rules)Infrastructure Layer implements technical details (e.g., database access)Pain Points: The domain layer is independent of technical implementations, and logic corresponds closely with the layer structure
  
  
  Modularity and Scalability
High Coupling: Lacks clear business boundaries; cross-module calls (e.g., order service directly relying on account tables) make code hard to maintain.Poor Scalability: Adding new features requires global changes (e.g., adding risk control rules must intrude into order service), easily causing cascading issues.Bounded Context: Modules are divided by business capabilities (e.g., payment domain, risk control domain); event-driven collaboration (e.g., order payment completed event) is used for decoupling.Independent Evolution: Each domain module can be upgraded independently (e.g., payment logic optimization does not affect order service), reducing system-level risks.Prefer MVC for small to medium systems: Simple business (e.g., blogs, CMS, admin backends), requiring rapid development with clear business rules and no frequent changes.Prefer DDD for complex business: Rule-intensive (e.g., financial transactions, supply chain), multi-domain collaboration (e.g., e-commerce order and inventory linkage), frequent changes in business requirements.Leapcell is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:Develop with Node.js, Python, Go, or Rust.Deploy unlimited projects for freepay only for usage — no requests, no charges.Unbeatable Cost EfficiencyPay-as-you-go with no idle charges.Example: $25 supports 6.94M requests at a 60ms average response time.Streamlined Developer ExperienceIntuitive UI for effortless setup.Fully automated CI/CD pipelines and GitOps integration.Real-time metrics and logging for actionable insights.Effortless Scalability and High PerformanceAuto-scaling to handle high concurrency with ease.Zero operational overhead — just focus on building.]]></content:encoded></item></channel></rss>