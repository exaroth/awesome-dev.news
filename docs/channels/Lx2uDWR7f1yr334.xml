<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Programming</title><link>https://www.awesome-dev.news</link><description></description><item><title>Shubhanshu Shukla Returns Safely from Space: A Historic Leap for India</title><link>https://dev.to/shravan_655c21d339de8a4a0/shubhanshu-shukla-returns-safely-from-space-a-historic-leap-for-india-5695</link><author>Shravan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 07:25:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
Shubhanshu Shukla has successfully completed his historic space journey and returned safely to Earth, marking a significant milestone in India’s space exploration achievements. His safe return is not just a personal triumph but a proud moment for the entire nation, showcasing India's growing capabilities in manned space missions. 
This successful mission brings new hope and excitement for the future of Indian space research and inspires a new generation of dreamers and explorers.]]></content:encoded></item><item><title>🚀 I Created OctaneDB – The Lightning-Fast Python Vector Database!</title><link>https://dev.to/rijinraju/i-created-octanedb-the-lightning-fast-python-vector-database-21d6</link><author>Rijin Raju</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 06:45:55 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[💡 What is OctaneDB?
OctaneDB is an open-source, high-performance vector database written in Python.
It lets you store, index, and rapidly search millions of text, image, or custom embeddings using state-of-the-art similarity search algorithms.✨ Key Features
⚡️ 10x Faster Than Pinecone/ChromaDB: Sub-millisecond queries, >3,000 vectors/sec insert rate.🧠 Advanced Indexing: HNSW for ultra-fast approximate search, FlatIndex for exact matches.💾 Flexible Storage: In-memory or persistent HDF5 mode.🤖 Text Embedding Built-In: Auto text-to-vector with sentence-transformers.🚀 GPU Acceleration: CUDA support out of the box.🔍 Powerful Search: Batch search, advanced metadata filtering (AND/OR/NOT logic).🔌 Easy Integration: ChromaDB-compatible API for seamless migration.🌎 Open Source: MIT licensed, totally free for all uses!🌐 Try it Online or Locally!
Get Started:bash
pip install octanedbpython
from octanedb import OctaneDB
db = OctaneDB(dimension=384, embedding_model="all-MiniLM-L6-v2")
db.create_collection("documents")
    ids=["doc1", "doc2"],
    documents=["About pineapple", "About oranges"]
)
results = db.search_text(query_text="fruit", k=2)
print(results)
Semantic searchImage embedding similarity🛠️ Features Coming Soon
Live Multi-TenancyHybrid Scalar/Vector QueriesInstant Index Updates (feedback wanted!)💬 Get Involved!
Try it, star it, and contribute on GitHubShare your benchmarks and real-world results!What problems do you face with vector DBs?
Drop your ideas, feature requests, or open an issue!🚦 Open to Feedback, Collaboration, and Questions!
Let's build the next era of search and AI together 🤝RijinRaju/octanedb]]></content:encoded></item><item><title>🚀 From Manual Builds to Multi-Platform Magic: How GoReleaser Transformed My OpenTelemetry Sandbox</title><link>https://dev.to/akshitzatakia/from-manual-builds-to-multi-platform-magic-how-goreleaser-transformed-my-opentelemetry-sandbox-h36</link><author>Akshit Zatakia</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 23 Aug 2025 05:49:39 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Ever spent hours wrestling with manual builds, creating release archives by hand, and maintaining complex CI/CD pipelines just to ship your Go application? I did too, until I discovered GoReleaser. Let me show you how it transformed my otel-sandbox project from a maintenance nightmare into a one-command release machine.
  
  
  The Problem: Release Hell 😤
My  project needed to support multiple platforms - developers use Linux, macOS (both Intel and Apple Silicon), and Windows. My original GitHub workflow was a monster:130+ lines of complex matrix buildsManual archive creation for each platformInconsistent naming across releasesMissing Windows support (oops!)No checksums or verificationEvery release meant babysitting the CI pipeline and praying nothing broke.
  
  
  Enter GoReleaser: The Game Changer 🎯
GoReleaser promised to replace all this complexity with a single configuration file. Skeptical but desperate, I gave it a shot.Before (GitHub Actions only):After (GoReleaser + GitHub Actions):
  
  
  Real-World Success Stories 🌟
1. Hugo - Static Site Generator
Challenge: Hugo needed to support 20+ platforms including exotic architectures. Solution: GoReleaser builds for Linux, Windows, macOS, FreeBSD, OpenBSD across amd64, 386, ARM variants. Result: Single goreleaser release creates 40+ platform-specific binaries.2. Terraform - Infrastructure as Code
Challenge: Enterprise users across diverse cloud environments and local machines. Solution: GoReleaser + HashiCorp's signing infrastructure. Result: Secure, verified releases for 15+ platforms with GPG signatures.3. Kubernetes CLI Tools (kubectl, helm)
Challenge: Developers need consistent tooling across laptop, CI, and production environments. Solution: GoReleaser ensures identical behavior across all platforms. Result: "Works on my machine" becomes "works everywhere."4. Prometheus Node Exporter
Challenge: Monitor diverse server architectures (x86, ARM, MIPS). Solution: GoReleaser builds for embedded systems, servers, and containers. Result: Single monitoring solution across entire infrastructure.
Challenge: Container orchestration across development and production environments. Solution: GoReleaser creates consistent CLI experience everywhere. Result: Seamless Docker experience from laptop to datacenter.
  
  
  My GoReleaser Configuration
What GoReleaser generates for each release:otel-sandbox_Linux_x86_64.tar.gzotel-sandbox_Linux_arm64.tar.gzotel-sandbox_Darwin_x86_64.tar.gz (macOS Intel)otel-sandbox_Darwin_arm64.tar.gz (macOS Apple Silicon)otel-sandbox_Windows_x86_64.zipotel-sandbox_Windows_arm64.zipchecksums.txt (SHA256 verification)
  
  
  Advanced Real-World Patterns
Used by: Kubernetes (kubectl, kubeadm, kubelet), Istio (istioctl, pilot)Used by: Prometheus, Grafana, JaegerUsed by: Hugo, Terraform, kubectlReal project comparisons:45min manual builds6 platforms8min automated40+ platformsComplex matrix builds15+ platformsOne-command release15+ platformsPlatform-specific CIManual archivesUnified build processAuto-generated archivesDocker-only releasesLimited platformsMulti-platform binaries15+ platforms130 lines CI config6 platforms30 lines total8 platformsSeparate build scriptsDocker-focusedUnified GoReleaserBinaries + DockerMulti-repo complexityPlatform inconsistenciesSingle-repo buildsConsistent across platforms
  
  
  The Developer Experience Win 🎉
Debug platform-specific issuesUpload artifacts one by onegit push origin v1.0.0-release✅ Complete release with all platforms ready
  
  
  Getting Started in 5 Minutes
goreleaser release GoReleaser didn't just simplify my releases - it transformed how I think about distribution. Instead of dreading release day, I now ship with confidence, knowing that every platform gets the same quality experience.The numbers speak for themselves:Hugo: Powers 100k+ websites with zero-friction updatesTerraform: Trusted by enterprises for infrastructure automationKubernetes tools: Enable container orchestration at global scaleMy otel-sandbox: Reduced CI complexity by 75%, added Windows support effortlesslyIf you're maintaining a Go project and still doing manual releases, you're missing out. GoReleaser isn't just a tool - it's a productivity multiplier that lets you focus on what matters: building great software.]]></content:encoded></item><item><title>Weekly Challenge: The Common Winner</title><link>https://dev.to/simongreennet/weekly-challenge-the-common-winner-57ka</link><author>Simon Green</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 05:25:47 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Each week Mohammad S. Anwar sends out The Weekly Challenge, a chance for all of us to come up with solutions to two weekly tasks. My solutions are written in Python first, and then converted to Perl. It's a great way for us all to practice some coding.
  
  
  Task 1: Common Characters
You are given an array of words.Write a script to return all characters that is in every word in the given array including duplicates.The task doesn't mention the order in which the list should be generated. Based on the examples, both "order they appear in the first word" and "alphabetical order" seem to be valid solutions. I've chose alphabetical order for this.For this challenge, I've turned the supplied  into a list of Counters (array of hashes in Perl) of letter frequencies called .I then iterate through each unique letter in the first word (in alphabetical order), calling the variable . I calculate the minimum number of occurrences of that letter in all the words. The Counter object will return  if the letter does not exist. If the letter occurs in all words, I append it to the  list the required number of times.The Perl solution follows the same logic, but generates the  hash by hand../ch-1.py bella label roller
, , ./ch-1.py cool lock cook
, ./ch-1.py hello world pole
, ./ch-1.py abc def ghi
./ch-1.py aab aac aaa
, You are given an array of all moves by the two players.Write a script to find the winner of the TicTacToe game if found based on the moves provided in the given array.Order move is in the order - , , , , , ….My sisters never liked playing Noughts and Crosses (as it is known as here) when I was young because I figured out a way to never lose. You have to remember this was a long time before the Internet was available to do research on this :-)For this task I take the command line input and convert it into pairs of . I initialize the  variable with 3 × 3 grid of underscores, and the  variable to .I then iterate through each move, starting by ensuring the move is within the bounds of the board, and the player isn't using a position that is already used.I then make the move on the board, check if there is a result, and switch to the other player in preparation for the next move. If there is a result, I return the player that won.If all the moves have been made, and there is no winner, I checked for any  on the . If there are, I return , or  if there are none.The  function takes the  and sees if there is a row, column, or one diagonal that has the same letter.The Perl solution follows the same logic as the Python one../ch-2.py 0 0 2 0 1 1 2 1 2 2
A

./ch-2.py 0 0 1 1 0 1 0 2 1 0 2 0
B

./ch-2.py 0 0 1 1 2 0 1 0 1 2 2 1 0 1 0 2 2 2
Draw

./ch-2.py 0 0 1 1
Pending

./ch-2.py 1 1 0 0 2 2 0 1 1 0 0 2
B
]]></content:encoded></item><item><title>Building GitNarrative: How I Parse Git History with Python to Extract Development Patterns</title><link>https://dev.to/grudged/building-gitnarrative-how-i-parse-git-history-with-python-to-extract-development-patterns-52lm</link><author>Chris Moore</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 04:47:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When I started building GitNarrative, I thought the hardest part would be the AI integration. Turns out, the real challenge was analyzing git repositories in a way that actually captures meaningful development patterns.Here's how I built the git analysis engine that powers GitNarrative's story generation.
  
  
  The Challenge: Making Sense of Messy Git History
Every git repository tells a story, but extracting that story programmatically is complex. Consider these real commit messages from a typical project:"fix bug"
"refactor"
"update dependencies" 
"THIS FINALLY WORKS"
"revert last commit"
"actually fix the bug this time"
The challenge is identifying patterns that reveal the actual development journey - the struggles, breakthroughs, and decision points that make compelling narratives.
  
  
  Library Choice: pygit2 vs GitPython
I evaluated both major Python git libraries:: More Pythonic, easier to use: Lower-level, better performance, more controlI chose  because GitNarrative needs to process repositories with thousands of commits efficiently. The performance difference is significant for large repositories.
  
  
  Core Analysis Architecture
Here's the foundation of my git analysis engine:
  
  
  Pattern Recognition: The Heart of Story Extraction
The key insight is that commit patterns reveal development phases. Here's how I identify them:
  
  
  1. Commit Type Classification

  
  
  2. Development Phase Detection

  
  
  3. Struggle and Breakthrough Detection
This is where the storytelling magic happens:
  
  
  Timeline Correlation: When Things Happened
Understanding timing is crucial for narrative flow:
  
  
  Performance Optimizations
Processing large repositories efficiently required several optimizations:
  
  
  3. Parallel Processing for Multiple Repositories

  
  
  Integration with AI Story Generation
The analysis output feeds directly into AI prompts:: Repositories with inconsistent commit message styles: Pattern matching with multiple fallback strategies and file-based analysis: Merge commits creating noise in analysis: Filtering strategy that focuses on meaningful commits while preserving merge context: Very large repositories (10k+ commits): Sampling strategy that captures representative commits from different time periodsThe analysis engine successfully processes repositories ranging from small personal projects to large open source codebases. When tested on React's repository, it correctly identified:The initial experimental phase (2013)Major architecture rewrites (Fiber, Hooks)Performance optimization periodsCurrent improvements in development:Better natural language processing of commit messagesMachine learning models for commit classificationIntegration with issue tracker data for richer contextSupport for monorepo analysisThe git analysis engine is the foundation that makes GitNarrative's storytelling possible. By extracting meaningful patterns from commit history, we can transform boring git logs into compelling narratives about software development.GitNarrative is available at https://gitnarrative.io - try it with your own repositories to see these patterns in action.What patterns have you noticed in your own git history? I'd love to hear about interesting commit patterns you've discovered in your projects.]]></content:encoded></item><item><title>Go Beyond Viper and Cobra: Declarative Field-Driven Configuration for Go Apps</title><link>https://dev.to/lucasdecamargo/go-beyond-viper-and-cobra-declarative-field-driven-configuration-for-go-apps-4k4a</link><author>Lucas de Camargo</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 23 Aug 2025 02:12:57 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Production Go applications constantly require the introduction of new configuration parameters. Based on the Open-Closed Principle, once we define a strategy for managing configuration fields, introducing new values becomes only small extensions. In this article, I'm proposing the definition of a Field structure for declaring configuration settings that are easily integrated with CLI completions and documentation generation.: A complete configuration solution for Go applications that handles multiple file formats (YAML, JSON, TOML, HCL, ENV), environment variables, and provides a unified interface for accessing configuration values. Viper acts as a registry for all your application's configuration needs, with automatic environment variable binding and a precedence system for value resolution.: A powerful CLI framework that provides a simple interface to create modern command-line applications with sophisticated features like nested subcommands, POSIX-compliant flags, automatic help generation, and shell completions. It's the same framework used by Kubernetes, Docker, and GitHub CLI for building their command-line interfaces.: A struct and field validation library that enables validation through struct tags, custom validation functions, and cross-field validation. It provides a declarative way to define validation rules directly in your struct definitions, making it easy to ensure data integrity throughout your application.
Requirements 



Implementation 

Field-Driven Configuration 

Real Field Definitions 

Writing the CLI only once with Cobra 

The Root Command 



The Config Commands 

Listing Configuration Values Setting Configuration Values Auto-Documentation: Describing Parameters 

Conclusion 

Modern production applications need robust configuration management that can adapt to different environments, validate inputs, and provide clear documentation to users. The approach presented here addresses these needs by creating a unified system where configuration metadata lives alongside the configuration values themselves. This creates a single source of truth that eliminates duplication and reduces the chance of documentation drift.: Configuration fields are defined once with all metadata (validation, documentation, defaults); and can be easily extended.Default Value Definition with Build Flags: Production apps are often built for different environments, therefore some default configuration values must be defined by Go .: Viper support for YAML, JSON, TOML, HCL, and ENV configuration files.: Seamless integration with Cobra for command-line interfaces, data validation, and auto-completion.: Strongly typed configuration with validation by Go Validator tags, custom functions, and literals.: Automatic binding with configurable prefix.: Built-in help and documentation generation.Our application is called , and it uses two groups of configuration: application base parameters, like logging and updates, and network configuration, like proxies. The architecture demonstrates how to organize configuration fields into logical groups, making it easy for users to understand and manage related settings together. Each field carries its complete metadata, ensuring that validation rules, documentation, and defaults are always consistent across the entire application.Users are expected to use the CLI to configure the application, like:confapp config .level debug .auto Application environment (hidden)Logging level (debug, info, warn, error)The implementation follows a modular approach where each component has a specific responsibility. The configuration module defines the fields and their metadata, Viper handles the actual storage and retrieval of values, and Cobra provides the user interface. This separation of concerns makes the system maintainable and allows each component to evolve independently while maintaining a stable interface between them.The project structure reflects the separation between CLI commands and configuration logic. The  directory contains all CLI-related code, while the  directory houses the configuration management core. This organization makes it clear where to find specific functionality and ensures that the configuration logic remains independent of the CLI implementation.go-appconfig-example/
├── cmd/                   # CLI command implementations
│   ├── root.go            # Root command and global flags
│   └── config.go          # Configuration management commands
├── internal/
│   ├── config/            # Configuration management core
│   │   ├── fields.go      # Field definitions and collections
│   │   ├── config.go      # Viper integration
│   │   └── validators.go  # Custom validation functions
│   └── consts/            # Application constants
│       └── consts.go      # Go flags like app name, version, etc.
├── main.go                # Application entry point
└── go.mod                 # Go module definition
Application constants define global values that remain consistent throughout the application's lifecycle. These can be overridden at build time using Go's  feature, allowing you to customize the application name, version, or other constants without modifying the source code. This is particularly useful for CI/CD pipelines where different builds might need different configurations.
  
  
  Field-Driven Configuration Field-Driven Configuration is a design pattern where configuration parameters are defined as structured data containing all their metadata. Instead of scattering validation rules, documentation, and default values across different parts of the codebase, each field becomes a self-contained unit that describes everything about a configuration parameter. This approach ensures consistency and makes it trivial to add new configuration options without touching multiple files.The Field structure is the cornerstone of our configuration system. It encapsulates not just the value of a configuration parameter, but also its type, validation rules, documentation, and any other metadata needed to work with that parameter. This rich metadata enables automatic generation of CLI flags, validation logic, and documentation, all from a single definition.The FieldCollection acts as a registry for all configuration fields in your application. It provides methods to add new fields dynamically and retrieve them efficiently. This centralized collection ensures that all parts of the application work with the same field definitions, maintaining consistency across CLI commands, validation, and documentation generation.With the Field structure and FieldCollection in place, we can now define actual configuration parameters. Each field definition becomes a single source of truth that contains everything needed to work with that configuration value. The use of init() functions ensures that fields are automatically registered when the package is imported, eliminating the need for manual registration and reducing the chance of forgetting to register a field.Validation is handled through the Go Validator library, which provides a declarative way to define validation rules using struct tags. The library supports a wide range of built-in validators like , , , , , and many more. You can combine multiple validators using commas (AND logic) or pipes (OR logic). For example, validate:"required,email" ensures a field is both present and a valid email, while  accepts either RGB or RGBA color formats.In our Field structure, we use the ValidateTag field to specify these validation rules, allowing us to leverage the full power of the validator library without writing repetitive validation code.Beyond the built-in validators, the system supports custom validation functions for complex business logic that can't be expressed through tags alone. These functions receive the value to validate and return an error if validation fails, providing complete flexibility for domain-specific rules.Viper provides the backbone for our configuration management system. It handles the complexity of merging configuration from multiple sources according to a well-defined precedence order: command-line flags override environment variables, which override config file values, which override defaults. This layered approach allows users to define base configurations in files while still being able to override specific values through environment variables in production or flags during development. Viper also manages the serialization and deserialization of configuration files in various formats, making it easy to work with YAML, JSON, TOML, or any other supported format without changing your code.
  
  
  Writing the CLI only once with Cobra The beauty of our Field-Driven approach truly shines when building the CLI with Cobra. Instead of manually defining flags for each configuration parameter and keeping them in sync with validation rules and documentation, our CLI commands automatically derive everything they need from the Field definitions. This means you write the CLI structure once, and it automatically adapts as you add new configuration fields. The commands can iterate through the FieldCollection to generate flags, completions, and documentation dynamically, ensuring that the CLI always reflects the current state of your configuration schema.The root command serves as the entry point for your CLI application. While you can generate the initial structure using  for the root command and  for subcommands, the real power comes from integrating it with our Field system. The root command sets up global flags and initializes the configuration system before any subcommand runs, ensuring that all parts of the application work with properly loaded and validated configuration.Cobra provides three types of flags: local flags (specific to a command), persistent flags (available to a command and all its subcommands), and the special PFlags type that integrates with Viper. When you bind a flag to Viper using , Viper automatically reads the flag value if it's set, allowing seamless integration between command-line arguments and your configuration system. This binding creates a unified interface where users can set values through flags, environment variables, or config files, and your application code doesn't need to know which source provided the value.Our implementation uses persistent flags for global options like the config file path and verbosity level, ensuring these are available to all subcommands. The initialization happens in Cobra's  hook, which runs before any command execution, guaranteeing that configuration is properly loaded before your command logic runs.The configuration commands provide users with a powerful interface to interact with your application's settings. The beauty of this implementation is that these commands automatically work with any fields you define in your FieldCollection. The  command shows current values,  provides detailed documentation, and  allows modification - all without hardcoding any specific field names. This generic approach means that adding a new configuration field automatically makes it available in all these commands without any additional code changes.Shell completion is one of Cobra's most powerful features, dramatically improving the user experience by providing intelligent suggestions as users type. The completion system works through  callbacks that receive the current command state and return possible completions. The  parameter contains arguments already provided, while  holds the partial text being typed. The function returns a list of completion suggestions and a directive that controls shell behavior (like whether to also suggest files).Our implementation leverages the Field definitions to provide context-aware completions. When users type , the system suggests  as a completion. When setting values with valid options, like , the completion can even suggest the valid values (debug, info, warn, error) defined in the field. This deep integration between the configuration schema and the CLI ensures users always have helpful guidance when interacting with your application.
  
  
  Listing Configuration Values The list command provides users with a clear view of their current configuration state. By calculating the maximum field name length, the output is neatly aligned, making it easy to scan through settings. The ability to filter by prefix allows users to focus on specific configuration groups, while the hidden flag reveals internal settings that are normally concealed. This command is essential for debugging and verifying that configuration values are being loaded correctly from all sources.
  
  
  Setting Configuration Values The set command demonstrates the power of our Field-Driven approach by automatically creating flags for all configuration fields. When a user runs config set --log.level debug, Cobra parses the flag, our code validates it against the Field definition, and if valid, updates the value through Viper. The command then saves the configuration to disk, ensuring changes persist across application restarts. The validation happens before any values are saved, preventing invalid configurations from being written to disk and ensuring the application always works with valid settings.
  
  
  Auto-Documentation: Describing Parameters The describe command showcases how rich metadata in Field definitions enables automatic documentation generation. Each field's description, type, valid values, examples, and extended documentation are displayed in a structured format that helps users understand not just what a setting does, but how to use it effectively. The grouping feature organizes related fields together, making it easier to understand the relationships between different configuration options. This self-documenting nature ensures that documentation always stays in sync with the actual implementation.The Field-Driven Configuration approach delivers a powerful, user-friendly CLI that adapts automatically as your application evolves. Users benefit from intelligent completions, comprehensive documentation, and robust validation, while developers enjoy a maintainable system where adding new configuration options requires minimal code changes. The integration between Viper and Cobra through our Field abstraction creates a seamless experience where configuration can be managed through files, environment variables, or command-line flags with equal ease.The build process leverages Go's  feature to inject build-time values into the application. This allows you to customize constants like application name, version, or even default configuration values without modifying source code. This is particularly useful in CI/CD pipelines where different environments might need different defaults, or when building white-labeled versions of your application.
go build  confapp


go build  myapp
Shell completions transform the user experience by providing context-aware suggestions as users type. Once enabled, users can press Tab to see available options, making it easy to discover configuration fields without consulting documentation. The completion system understands the command structure and provides appropriate suggestions based on context, such as showing only valid values for fields with enumerated options. <./confapp completion zsh <./confapp completion bash
./confapp config list


./confapp config list log
./confapp config list proxy


./confapp config describe


./confapp config describe log.level update


./confapp config .level debug
./confapp config .level info .output /var/log/app.log


./confapp config list 
./confapp config describe 
./confapp  config list
The Field-Driven Configuration pattern presented in this article demonstrates how thoughtful abstraction can transform configuration management from a maintenance burden into a self-maintaining system. By defining configuration fields as first-class entities with rich metadata, we've created a solution that respects the Open-Closed Principle while providing exceptional developer and user experiences.The integration of Viper, Cobra, and Go Validator through our Field abstraction eliminates the common pain points of configuration management: keeping documentation in sync, maintaining validation rules, and providing good CLI experiences. The result is a system where adding new configuration options is as simple as defining a new Field struct, and everything else - from CLI flags to validation to documentation - automatically adapts.This approach scales elegantly from simple applications with a handful of settings to complex systems with hundreds of configuration parameters organized into logical groups. The automatic generation of completions and documentation ensures that as your application grows, it remains discoverable and user-friendly.The architecture presented here provides a solid foundation that can be extended in several ways:: Add support for multiple configuration profiles (development, staging, production) by extending the FieldCollection to support profile-specific overrides while maintaining the same validation and documentation capabilities.: Implement configuration hot-reloading using Viper's WatchConfig functionality, with the Field definitions providing the validation layer to ensure changes are safe before applying them.: Generate OpenAPI specifications or GraphQL schemas from Field definitions, ensuring that API documentation stays synchronized with configuration capabilities.: Extend the Field structure to support complex nested configurations while maintaining the same validation and documentation benefits.For a complete implementation with additional features like configuration profiles, pager support for long documentation, and more sophisticated validation examples, check out my GitHub repository. A production-ready template for managing configuration parameters in Go applications using  and . This template demonstrates a clean, maintainable approach to configuration management with a single source of truth for all configuration metadata.: Configuration fields are defined once with all metadata (validation, documentation, defaults): Strongly typed configuration with validation: Seamless integration with Cobra for command-line interfaces: Support for YAML, JSON, TOML, HCL, and ENV files: Automatic binding with configurable prefix: Auto-completion for configuration field names and values: Built-in help and documentation generation: Multiple validation strategies (tags, custom functions, valid values)Core Concept: Field-Driven ConfigurationThe central idea is to define configuration fields as structured data that contains everything needed to:The repository includes comprehensive examples and can serve as a starting template for your own production applications. Feel free to star the repository if you find it useful, and don't hesitate to open issues or contribute improvements!]]></content:encoded></item><item><title>Decoding the Neural Network&apos;s Mind: A Journey Through Forward Propagation</title><link>https://dev.to/dev_patel_35864ca1db6093c/decoding-the-neural-networks-mind-a-journey-through-forward-propagation-2n6h</link><author>Dev Patel</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 23 Aug 2025 01:58:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Imagine a detective meticulously piecing together clues to solve a complex case. That's essentially what a neural network does during forward propagation. It takes input data (the clues), processes it layer by layer (analyzes the evidence), and ultimately arrives at an output (solving the case). This process, called forward propagation, is the fundamental engine driving the power of neural networks, the cornerstone of modern machine learning. This article will demystify this crucial process, making it accessible to both beginners and those seeking a deeper understanding.
  
  
  What is Forward Propagation?
Forward propagation is the process by which a neural network transforms input data into an output prediction. It's a series of calculations, flowing forward through the network's layers, each layer transforming the data slightly until a final prediction emerges. Think of it as a pipeline where data enters, undergoes a series of transformations, and finally exits as a refined prediction.
  
  
  The Architecture: Layers and Connections
A neural network consists of interconnected layers: Receives the initial data.  For example, if classifying images, this layer might represent the pixel values.  These layers perform the bulk of the processing, transforming the data through complex mathematical operations.  A network can have multiple hidden layers, increasing its complexity and learning capacity. Produces the final prediction.  This could be a classification (cat or dog), a regression value (house price), or any other desired output.Each layer is composed of interconnected , which perform weighted sums of their inputs and apply an activation function to introduce non-linearity. These connections have associated  and , which are the parameters the network learns during training.
  
  
  The Mathematics:  A Step-by-Step Walkthrough
Let's simplify the math. Consider a single neuron receiving inputs $x_1, x_2, ..., x_n$ with corresponding weights $w_1, w_2, ..., w_n$ and a bias $b$. The neuron's output, $z$, is calculated as:$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = \sum_{i=1}^{n} w_ix_i + b$This is a weighted sum of inputs plus a bias. The bias acts as an offset, allowing the neuron to activate even when inputs are small.Next, an , denoted as σ(z), is applied to introduce non-linearity. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh. For example, the ReLU function is defined as:This means the output is either 0 or the input itself, depending on whether the input is negative or positive. This simple non-linearity is crucial for the network's ability to learn complex patterns.The output of one layer becomes the input for the next, and this process repeats until the output layer is reached. Let's illustrate with Python pseudo-code:Forward propagation is the backbone of countless applications:  Classifying images of cats, dogs, or other objects.Natural Language Processing:  Understanding and generating human language, powering chatbots and machine translation.  Object detection and path planning.  Analyzing medical images to detect diseases.
  
  
  Challenges and Limitations
  Training deep neural networks can be computationally expensive, requiring powerful hardware (GPUs).  The network might learn the training data too well and perform poorly on unseen data. Understanding why a network makes a specific prediction can be challenging, raising ethical concerns in sensitive applications.
  
  
  The Future of Forward Propagation
Forward propagation remains central to neural network research. Ongoing research focuses on:More efficient algorithms:  Reducing computational costs and improving training speed.  Designing networks that are more robust, accurate, and interpretable.New activation functions:  Exploring activation functions that enhance learning and generalization.In conclusion, forward propagation is the engine driving the power of neural networks. Understanding its mechanics—the flow of data, the mathematical transformations, and the role of activation functions—is crucial for anyone seeking to master the art of machine learning. As research continues, forward propagation will undoubtedly play an even more critical role in shaping the future of artificial intelligence.]]></content:encoded></item><item><title>Links</title><link>https://matklad.github.io/2025/08/23/links.html</link><author>Alex Kladov</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Sat, 23 Aug 2025 00:00:00 +0000</pubDate><source url="https://matklad.github.io/">Matklad blog</source><content:encoded><![CDATA[If you have a blog, consider adding a “links” page to it, which references resources that you find
notable:I’ve started my links page several years ago, mostly because I found myself referring to the same
few links repeatedly in various discussions, and not all the links were easily searchable.Note that the suggestion is different from more typical “monthly links roundup”, which is nice to
maintain Substack engagement/community, but doesn’t contribute to long-term knowledge distilling.It is also different from the exhaustive list of everything I’ve read on the Internet. It is
relatively short, considering its age.]]></content:encoded></item><item><title>Local LLMs, No API Keys, No BS: Build Your Own Waifubot Terminal Chat in Python</title><link>https://dev.to/owly/local-llms-no-api-keys-no-bs-build-your-own-waifubot-terminal-chat-in-python-470c</link><author>owly</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 23:36:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Build a Local Waifubot Terminal Chat in Python — No API Keys, No Cloud, No Bullshit
Tired of cloud dependencies, subscriptions, and rate limits? Want your own affectionate AI companion running locally, offline, and async? This walkthrough shows you how to build a waifubot terminal chat using Ollama, LLaMA 3, and Python. No fluff. Just code.
  
  
  Step 1: Install Ollama (One-Time Setup)
Ollama lets you run LLMs locally with ease.Go to oLLaMa’s download page
Download the installer for your OS (Windows/macOS)
Install and open the Ollama app
In the Ollama terminal, pull a model:
This downloads the LLaMA 3 model locally.
  
  
  🧰 Step 2: Create Your PyCharm Project
Open PyCharm → New Project → name it 
Inside the project, create a file:  file and add:
PyCharm will prompt you to install it — accept and let it install.
  
  
  Step 3: Write Your Chat Script
This code requires a certain threshold of computing power, so don't expect it to run smoothly on your vintage Pentium 3 machine.
The code is modular and wrapped into functions.
The code runs asyncly, which is handled in the function doing the calls.
The code runs locally and offline:  No subscription needed
The chat adds short memory context to each call.]]></content:encoded></item><item><title>Golang Binary Compile arm64</title><link>https://dev.to/hardyweb/golang-binary-compile-arm64-38gn</link><author>hardyweb</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 22 Aug 2025 23:02:42 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[GOOS=linux GOARCH=arm64 go build -o nama_sistem_arm64 main.go
Strip debug info (reduce size)GOOS=linux GOARCH=arm64 go build -ldflags="-s -w" -o nama_sistem_arm64 main.go
GOOS=linux GOARCH=arm64 go build -ldflags="-s -w" -trimpath -o nama_sistem_arm64 main.go
Reproducible build (consistent hash)GOOS=linux GOARCH=arm64 go build -ldflags="-s -w" -trimpath -buildvcs=false -o nama_sistem_arm64 main.go
Extra: compress with UPX (optional)upx --best --lzma nama_sistem_arm64
]]></content:encoded></item><item><title>Building Spokane Tech: Part 8</title><link>https://dev.to/dbslusser/building-spokane-tech-part-8-5h0e</link><author>David</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 22:42:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Welcome to part 8 of the "Building Spokane Tech" series! In this article, we'll discuss adding Docker and Docker Compose for running components of our service in containers.Containerization has become an essential tool for modern web development, and Docker is at the forefront of this revolution. When developing a Django-based web application like ours, using Docker ensures consistency across development and deployed environments. By leveraging Docker Compose, we can efficiently manage multiple services required by our application.Docker Compose is a tool that allows you to define and manage multi-container Docker applications using a simple YAML file (docker-compose.yaml). It enables developers to run interconnected services, such as a web application, database, and message broker, with a single command. The  Docker Compose basic concepts include:Key Docker Compose Configuration Options Defines the Compose file format version. In our case, we use "3.9", which is one of the latest stable versions. Lists all the containers that make up the application. Each service runs in its own container.Service Configuration Keys Specifies the Docker image to use for the container. If the image is not found locally, Docker will pull it from a registry like Docker Hub. Defines how to build the image from a Dockerfile. It usually includes:context: The directory containing the Dockerfile.dockerfile: The path to the specific Dockerfile used to build the image. Gives a custom name to the container instead of a randomly generated one. Overrides the default command specified in the Dockerfile, allowing you to run specific commands when the container starts. Loads environment variables from an external .env file. Maps ports between the container and the host. Specifies service dependencies. A container will not start until its dependencies are up and running.Volumes store persistent data outside the container filesystem, ensuring data is not lost when containers are restarted or removed.Let's review the components in our system, each of these will be a service in our docker-compose.yaml file.Django (Web Application) – The core application running on Gunicorn or the Django development serverPostgreSQL (Database) – Stores application dataRedis (Message Broker) – Used by Celery for task queuingCelery Worker – Executes asynchronous tasksCelery Beat – Handles scheduled tasksCelery Flower – Provides a web UI for monitoring Celery tasksOur docker-compose.yaml fileversion: '3.9'

services:
  django:
    image: spokanetech-django:latest
    container_name: django
    env_file:
      - .env.compose
    build:
      context: ../..
      dockerfile: src/docker/Dockerfile
    command: ./entrypoint.sh
    ports:
      - "8080:8000"
    depends_on:
      - db
      - redis

  db:
    image: postgres:17
    container_name: postgres_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    env_file:
      - .env.compose

  redis:
    image: redis:7.2-alpine
    container_name: redis
    restart: unless-stopped
    ports:
      - "6379:6379"

  worker:
    image: spokanetech-django:latest
    container_name: worker
    env_file:
      - .env.compose
    build:
      context: ../..
      dockerfile: src/docker/Dockerfile
    command: celery -A core worker -l info
    depends_on:
      - redis
      - db

  beat:
    image: spokanetech-django:latest
    container_name: beat
    env_file:
      - .env.compose
    build:
      context: ../..
      dockerfile: src/docker/Dockerfile
    command: celery -A core beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    depends_on:
      - redis
      - db

  flower:
    image: spokanetech-django:latest
    container_name: flower
    env_file:
      - .env.compose
    command: ["celery", "-A", "core", "--config=flowerconfig.py", "flower"]
    ports:
      - "5555:5555"
    depends_on:
      - redis
      - db

volumes:
  postgres_data:
  static_volume:
Docker Compose provides several commands to manage services. Here are the basics:To build the containers run:This builds images for the services defined in docker-compose.yaml using the specified Dockerfile. If an image already exists, it will only rebuild if changes are detected.To start the containers run:This starts all services defined in docker-compose.yaml. It also automatically builds missing images if they are not found.To run the containers in detached mode use:This runs containers in the background and allows applications to run persistently.To stop the containers use:This stops and removes all containers, networks, and volumes (if specified); it does not remove built images.Rebuild and restart containersTo build the container when running, use:docker-compose up --buildThis rebuilds images before starting containers and ensures the latest changes in the Dockerfile are applied.All of our components are available on localhost on various their applicable ports: ]]></content:encoded></item><item><title>Is Google’s Reveal of Gemini’s Impact Progress or Greenwashing?</title><link>https://towardsdatascience.com/is-googles-reveal-of-geminis-impact-progress-or-greenwashing/</link><author>Kasper Groes Albin Ludvigsen</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 21:57:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[On the surface, Google's numbers sound reassuringly small, but the more closely you look, the more complicated the story becomes.]]></content:encoded></item><item><title>Sebastian Pölsterl: scikit-survival 0.25.0 with improved documentation released</title><link>https://k-d-w.org/blog/2025/08/scikit-survival-0.25.0-with-improved-documentation-released/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 21:55:06 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[This release adds support for scikit-learn 1.7, in addition to version 1.6.
However, the most significant changes in this release affect the documentation.
The API documentation has been completely overhauled to improve clarity and consistency.
I hope this marks a significant improvement for users new to scikit-survival.One of the biggest pain points for users seems to be understanding which metric can be used to evaluate the performance of a given estimator.
The user guide
now summarizes the different options.The performance metrics for evaluating survival models can be broadly divided into three groups:Concordance Index (C-index): Measures the rank correlation between predicted risk scores and observed event times.
Two implementations are available in scikit-survival:Cumulative/Dynamic Area Under the ROC Curve (AUC):
Extends the AUC to survival data, quantifying how well a model distinguishes subjects who experience an event by a given time from those who do not. It can handle time-dependent risk scores
and is implemented in cumulative_dynamic_auc().:
An extension of the mean squared error to right-censored data.
The Brier score assesses both discrimination and calibration based on a model’s estimated survival functions.
You can either compute the Brier score at specific time point(s) using
brier_score()
or compute an overall measure by integrating the Brier score over a range of time points via
integrated_brier_score().What Do Survival Models Predict?Survival models can predict several quantities, depending on the model being used.
First of all, every estimator has a  method,
which either returns a unit-less risk score
or the predicted time of an event.If predictions are , higher values indicate an
increased risk of experiencing an event. The scores have no unit
and are only meaningful for ranking samples by their risk of experiencing an event.
This is for example the case for
CoxPHSurvivalAnalysis.from sksurv.datasets import load_veterans_lung_cancer
from sksurv.linear_model import CoxPHSurvivalAnalysis
from sksurv.metrics import concordance_index_censored
from sksurv.preprocessing import OneHotEncoder
# Load data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit model
estimator = CoxPHSurvivalAnalysis().fit(Xt, y)
# Predict risk score
predicted_risk = estimator.predict(Xt)
# Evaluate risk scores
cindex = concordance_index_censored(
y["Status"], y["Survival_in_days"], predicted_risk
)
If predictions directly relate to the time point of an event,
lower scores indicate shorter survival, while higher scores indicate longer survival.
See for example IPCRidge.from sksurv.datasets import load_veterans_lung_cancer
from sksurv.linear_model import IPCRidge
from sksurv.metrics import concordance_index_censored
from sksurv.preprocessing import OneHotEncoder
# Load the data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit the model
estimator = IPCRidge().fit(Xt, y)
# Predict time of an event
predicted_time = estimator.predict(Xt)
# Flip sign of predictions to obtain a risk score
cindex = concordance_index_censored(
y["Status"], y["Survival_in_days"], -1 * predicted_time
)
While the concordance index is easy to interpret,
it is not a useful measure of performance if a specific time range
is of primary interest (e.g. predicting death within 2 years).
This is particularly relevant for survival models that can
make time-dependent predictions.For instance,
RandomSurvivalForest,
can also predict survival functions (via predict_survival_function())
or cumulative hazard functions (via predict_cumulative_hazard_function()).
These functions return lists of
StepFunction instances.
Each instance can be evaluated at a set of time points to obtain predicted
survival probabilities (or cumulative hazards).
The Brier score and
cumulative_dynamic_auc()
are capable of evaluating time-dependent predictions, but .import numpy as np
from sksurv.datasets import load_veterans_lung_cancer
from sksurv.ensemble import RandomSurvivalForest
from sksurv.metrics import integrated_brier_score
from sksurv.preprocessing import OneHotEncoder
# Load the data
X, y = load_veterans_lung_cancer()
Xt = OneHotEncoder().fit_transform(X)
# Fit the model
estimator = RandomSurvivalForest().fit(Xt, y)
# predict survival functions
surv_funcs = estimator.predict_survival_function(Xt)
# select time points to evaluate performance at
times = np.arange(7, 365)
# create predictions at selected time points
preds = np.asarray(
[[sfn(t) for t in times] for sfn in surv_funcs]
)
# compute integral
score = integrated_brier_score(y, y, preds, times)
]]></content:encoded></item><item><title>IoT-Driven Fence Solutions: Balancing Security, Automation, and Aesthetics</title><link>https://dev.to/emily_johnson_dev/iot-driven-fence-solutions-balancing-security-automation-and-aesthetics-e99</link><author>Emily Johnson</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 21:50:48 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In today’s connected world, the role of fences has evolved beyond simple boundaries. IoT-driven fence solutions are transforming the way we manage , , and  for residential, commercial, and industrial properties. With integrated smart sensors, mobile apps, and cloud platforms, modern fencing systems can provide real-time monitoring, adaptive controls, and seamless customization options.  In this article, we’ll explore how IoT technologies are shaping the fencing industry, showcase real-world applications, and include  for IoT integration in smart fencing systems.  1. The Rise of Smart Fencing SystemsTraditional fences used to be static structures offering only physical security. Today, homeowners and businesses demand automation, remote control, and aesthetic flexibility. IoT fencing solutions combine:   to detect motion, vibration, or tampering.
 with voice or app-based controls.
 for facial recognition and surveillance.
 to monitor and configure fences in real time.
Many property owners in Illinois rely on experts like a  to deploy advanced systems that combine privacy, security, and modern design.  2. Key Features of IoT-Driven Fence SolutionsIoT-enabled fences connect sensors and cameras to smart hubs, instantly notifying property owners of suspicious activity.  b) Automation & Remote AccessThrough dedicated mobile apps, users can open gates, lock perimeters, or switch to privacy mode instantly.  c) Aesthetic Variety & CustomizationIoT solutions also allow homeowners to control LED lighting, surface finishes, or retractable panels to adapt fences to different scenarios or moods.  Solar-powered IoT devices and low-energy controllers minimize operational costs while improving sustainability.  3. Sample Architecture for IoT Smart FenceHere’s a simple architecture to visualize how a smart fencing system works:graph TD
    A[IoT Sensors] --> B[Smart Hub]
    B --> C[Cloud Platform]
    C --> D[Mobile App]
    D --> E[User Control]
    B --> F[AI Camera Module]
    F --> C
4. Programming Example: Node.js IoT Fence ControllerHere’s a simple Node.js snippet for managing a smart fence’s lock/unlock automation via IoT commands:This code uses  to communicate with IoT devices and allows remote locking/unlocking of fence gates through real-time messaging.  5. Adding Facial Recognition for Enhanced SecurityFor properties requiring high security — such as commercial facilities — integrating AI-powered cameras with IoT fences offers advanced monitoring.This Python snippet integrates facial recognition to detect authorized users and could trigger IoT-controlled gates accordingly.  6. IoT Solutions for Commercial PropertiesBusinesses demand higher security and seamless automation, especially when managing multiple properties. Companies specializing in smart installations, such as a , provide advanced IoT-enabled perimeter control systems, ensuring that both safety and design preferences are met.  For premium installations, incorporating modern styles like a  with integrated sensors offers both  and .  7. Future Trends in IoT Fencing Systems Faster data transmission for real-time monitoring.
AI Predictive Maintenance: Automated alerts when panels or sensors need servicing.
 Visualize and customize fence designs instantly via mobile apps.
 Control fences through Alexa, Google Assistant, or Siri.
IoT-driven fence solutions represent the perfect fusion of , , and . By integrating smart sensors, AI cameras, and real-time mobile controls, property owners can protect their investments while enjoying flexibility and style.  Whether upgrading an existing fence or installing a new IoT-powered system, partnering with experts ensures seamless implementation and long-term performance. The future of fencing isn’t just functional — it’s smart, connected, and designed to impress.  ]]></content:encoded></item><item><title>Glyph.Flow Devlog #2 – Hitting the Registry Milestone</title><link>https://dev.to/daemonic01/glyphflow-devlog-2-hitting-the-registry-milestone-41h5</link><author>Dominik Kopócs</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 21:00:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Last time I shared why I’m building Glyph.Flow, a minimalist workflow manager in the terminal with Textual.
This week it’s time for an update on what I managed to get done.I wanted to move from a rough prototype into something modular and extensible.
That meant one thing: a command registry.Backend refactor: my massive 630-line app.py is now down to ~112 lines. Commands live in a registry, not tangled logic.Command registry: all commands are defined declaratively, with schema-based argument parsing, aliases, and usage.Logging: unified styling and message keys, with autosave and error handling standardized.New config command: quick way to tweak settings on the fly.Consistency: adding a new command is now just “add a dict + handler”.It finally behaves like a real CLI app instead of a spaghetti prototype — but I’ll be honest, it’s still a prototype.
The difference is: now the foundation feels stable enough to build on.More commands to migrate (delete, edit, schema, …).Road toward a TUI interface on top of this backend.Eventually, I’d like this to feel like a natural console companion for managing projects.That’s it for this week’s log.
If you’re into command-line tools, or building things with Textual, I’d love to hear your feedback. 🚀]]></content:encoded></item><item><title>The science of loudness</title><link>https://fasterthanli.me/articles/the-science-of-loudness</link><author>Amos Wenger</author><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 22 Aug 2025 20:30:00 +0000</pubDate><source url="https://fasterthanli.me/index.xml">Faster than time blog</source><content:encoded><![CDATA[My watch has a “Noise” app: it shows , for decibels.My amp has a volume knob, which also shows decibels, although.. negative ones, this time.And finally, my video editing software has a ton of meters — which are all in decibel or
decibel-adjacent units.How do all these decibels fit together?]]></content:encoded></item><item><title>Three Essential Hyperparameter Tuning Techniques for Better Machine Learning Models</title><link>https://towardsdatascience.com/three-essential-hyperparameter-tuning-techniques-for-better-machine-learning-models/</link><author>Rukshan Pramoditha</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 20:28:40 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Learn how to optimize your ML models for better results]]></content:encoded></item><item><title>Rodrigo Girão Serrão: functools.Placeholder</title><link>https://mathspp.com/blog/how-to-use-functools-placeholder</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 19:21:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Learn how to use , new in Python 3.14, with real-life examples.By reading this article you will understand what  is for and how to use it effectively.Partial function applicationIn a nutshell,  allows you to perform partial function application, by “freezing” arguments to functions.Up until Python 3.13, you could use  to freeze arguments in two types of ways:you could pass positional arguments to , which would be passed in the same order to the function being used with ; oryou could pass keyword arguments to , which would be passed with the same name to the function being used with .Using keyword arguments to skip the first argumentThe method 2. is especially useful if you're trying to freeze an argument that is not the first one.
For example, if you use the built-in  on the built-in , you can see this signature:int(x, base=10) -> integerIf you want to convert a binary string to an integer, you can set :print(int("101", 2))  # 5Now, suppose you want to create a function  by “freezing” the argument  in the built-in .
Writingfrom_binary = partial(int, 2)won't work, since in , the value  is seen as the argument  from the signature above.
However, you can pass the base as a keyword argument, skipping the first argument  from the signature of the built-in :from functools import partial

from_binary = partial(int, base=2)

print(from_binary("101"))  # 5But this doesn't always work.When keyword arguments don't workimport string

_table = str.maketrans("", "", string.punctuation)
def remove_punctuation(string):
    return string.translate(_table)

print(remove_punctuation("Hello, world!"))  # Hello worldThe function  is a thin wrapper around the string method , which is the function doing all the work.
In fact, if you look at  as a function, you always pass  as the second argument; what changes is the first argument:print(str.translate("Hello, world!", _table))  # Hello world
print(str.translate("What?!", _table))  # WhatThis may lead you to wanting to use  to freeze the value  on the function , so you use the built-in  to check the signature of :translate(self, table, /) unbound builtins.str methodYou can see that the first argument is , the string you are trying to translate, and then  is the translation table (that  built magically for you).
But you can also see the forward slash , which means that  and  are positional-only arguments that cannot be passed in as keyword arguments!]]></content:encoded></item><item><title>This algorithm solves the triangle-finding problem in linear time, providing strong evidence that all problems in the 3SUM-hard class can be solved in sub-quadratic time.</title><link>https://dev.to/frank_vega_987689489099bf/this-algorithm-solves-the-triangle-finding-problem-in-linear-time-providing-strong-evidence-that-p4a</link><author>Frank Vega</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 18:15:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Our Sqrt(n)-approximation for the independent set problem would strongly suggest that P = NP. Experimental results showed a 2-approximation ratio on real-world benchmarks, outperforming the theoretical Sqrt(n) worst-case guarantee.</title><link>https://dev.to/frank_vega_987689489099bf/our-sqrtn-approximation-for-the-independent-set-problem-would-strongly-suggest-that-p-np-2cdg</link><author>Frank Vega</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 18:14:09 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>**Mastering HTTP/2 Server Performance Optimization in Go for High-Traffic Applications**</title><link>https://dev.to/aaravjoshi/mastering-http2-server-performance-optimization-in-go-for-high-traffic-applications-14am</link><author>Aarav Joshi</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 22 Aug 2025 18:09:14 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! Building high-performance web servers in Go requires understanding modern protocols. HTTP/2 represents a significant leap forward from HTTP/1.x, particularly for applications handling thousands of concurrent connections. The protocol's design addresses many limitations that plagued earlier versions.I've spent considerable time optimizing HTTP/2 implementations in production environments. The gains are substantial when you approach it correctly. Connection multiplexing alone can transform how your server handles load.Let me walk through a practical implementation that demonstrates key optimization techniques. This code establishes a foundation for high-concurrency HTTP/2 servers in Go.The foundation starts with proper structure. We need components for connection management, server push capabilities, and performance tracking. Each plays a crucial role in achieving optimal performance.Connection pooling proves essential for reducing overhead. Establishing new TLS connections remains expensive, so reusing existing connections dramatically improves efficiency.Server push represents one of HTTP/2's most powerful features. When implemented correctly, it allows proactive resource delivery before clients even request them.Tracking performance metrics helps identify bottlenecks. Without proper instrumentation, optimizing becomes guesswork rather than data-driven improvement.Initializing the server requires careful configuration. Setting appropriate limits prevents resource exhaustion while maintaining high throughput.The request handling logic needs to account for protocol differences. HTTP/2 enables optimizations that simply aren't possible with earlier versions.HTTP/2-specific handling focuses on three main areas: header compression, server push opportunities, and stream prioritization. Each contributes to overall performance.Server push implementation requires careful consideration. Pushing unnecessary resources can actually harm performance rather than help.Stream prioritization allows more important requests to receive resources first. This proves particularly valuable under heavy load conditions.Caching pushable resources ensures they're readily available when opportunities arise. The cache should be populated during server initialization.Connection management forms the heart of HTTP/2 optimization. Smart pooling strategies prevent connection churn while maintaining performance.Regular cleanup prevents memory leaks from accumulated idle connections. The cleanup frequency should balance resource usage with connection establishment costs.Monitoring performance provides insights for further optimization. The metrics collected help identify patterns and potential improvements.The main function ties everything together. Proper TLS configuration is essential for HTTP/2, as most browsers require encrypted connections.Connection multiplexing stands as HTTP/2's most significant advantage. Where HTTP/1.x required multiple connections for parallel requests, HTTP/2 handles everything over a single connection. This reduces TCP and TLS overhead substantially.In practice, I've seen connection counts drop from six per client to just one. The resource savings compound quickly at scale. Memory usage decreases, CPU load reduces, and network efficiency improves.Header compression using HPACK delivers impressive gains. Traditional HTTP headers often consumed 2KB or more per request. HPACK typically reduces this to under 200 bytes. The savings become enormous at high request volumes.The compression works through static and dynamic tables. Common headers get referenced from tables rather than retransmitted. Huffman encoding further reduces size for variable values.Server push requires thoughtful implementation. The feature allows sending responses before clients request them. For critical resources like CSS or JavaScript, this can eliminate round trips.But push too much, and you waste bandwidth. Push the wrong things, and you hinder performance. I typically push only resources with high certainty of being needed.Stream prioritization enables quality of service controls. Important requests can receive preferential treatment during resource contention. The protocol supports complex dependency trees and weight-based allocation.In real applications, I prioritize user-interactive requests over background tasks. API calls affecting user experience get resources before analytics pings or prefetch requests.Connection management deserves particular attention. HTTP/2 connections are valuable resources. Pooling and reuse prevent expensive renegotiation of TLS sessions.I implement aggressive connection reuse where appropriate. The pool maintains connections to various endpoints, ready for immediate use. Cleanup routines remove idle connections to conserve resources.Performance monitoring provides crucial insights. Without metrics, optimization efforts operate blindly. I track active streams, pushed resources, header savings, and connection reuse rates.These metrics help identify bottlenecks. If active streams consistently hit limits, perhaps the maximum needs adjustment. If push failures increase, maybe the strategy requires revision.Flow control tuning affects overall throughput. HTTP/2 includes window-based flow control at both connection and stream levels. Proper tuning prevents starvation while maintaining fairness.I typically start with conservative window sizes and adjust based on observed performance. The optimal values depend on network characteristics and application behavior.Error handling requires special consideration in HTTP/2. The protocol includes various error codes and reset mechanisms. Proper handling maintains stability during network issues or client problems.I implement comprehensive logging for stream resets and connection errors. This helps identify patterns and address underlying issues.Protocol upgrade handling maintains compatibility. While HTTP/2 excels, not all clients support it. The server should gracefully handle HTTP/1.x connections when necessary.In my implementation, I check the protocol version and handle appropriately. This ensures broad compatibility while providing modern features where available.TLS configuration significantly impacts performance. HTTP/2 requires specific cipher suites and protocol versions. Modern, efficient settings improve both security and speed.I prefer TLS 1.3 where possible for improved performance. The reduced handshake latency benefits HTTP/2's connection reuse model.Resource management prevents denial of service attacks. HTTP/2's multiplexing capability means a single connection can make many requests. Limits prevent resource exhaustion.I set reasonable limits on concurrent streams and request rates. These protect the server while still allowing high performance for legitimate traffic.The implementation demonstrates practical application of HTTP/2 features. The code provides a foundation that can be extended for specific use cases. Each component addresses particular aspects of protocol optimization.Through careful implementation and continuous refinement, HTTP/2 can deliver substantial performance improvements. The protocol represents a meaningful step forward in web technology.The approach reduces latency while increasing throughput. Connection multiplexing cuts resource usage significantly for high-concurrency workloads. Header compression reduces bandwidth requirements. Server push eliminates round trips for critical resources.These improvements combine to create faster, more efficient web services. The benefits become increasingly valuable as applications scale to handle more users and traffic.Proper HTTP/2 implementation requires understanding both the protocol specifics and the practical considerations of production deployment. The technical capabilities must be balanced with operational requirements.
  
  
  The result is systems that handle more traffic with fewer resources while providing better user experiences. That combination makes the effort worthwhile.
📘 , , , and  to the channel! is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly low—some books are priced as low as —making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>Enhance Geospatial Analysis and GIS Workflows with Amazon Bedrock Capabilities</title><link>https://aws.amazon.com/blogs/machine-learning/enhance-geospatial-analysis-and-gis-workflows-with-amazon-bedrock-capabilities/</link><author>Dave Horne</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 17:54:38 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[As data becomes more abundant and information systems grow in complexity, stakeholders need solutions that reveal quality insights. Applying emerging technologies to the geospatial domain offers a unique opportunity to create transformative user experiences and intuitive workstreams for users and organizations to deliver on their missions and responsibilities.In this post, we explore how you can integrate existing systems with Amazon Bedrock to create new workflows to unlock efficiencies insights. This integration can benefit technical, nontechnical, and leadership roles alike.Introduction to geospatial dataGeospatial data is associated with a position relative to Earth (latitude, longitude, altitude). Numerical and structured geospatial data formats can be categorized as follows: – Geographical features, such as roads, buildings, or city boundaries, represented as points, lines, or polygons – Geographical information, such as satellite imagery, temperature, or elevation maps, represented as a grid of cells – Location-based data, such as descriptions and metrics (average rainfall, population, ownership), represented in a table of rows and columnsGeospatial data sources might also contain natural language text elements for unstructured attributes and metadata for categorizing and describing the record in question. Geospatial Information Systems (GIS) provide a way to store, analyze, and display geospatial information. In GIS applications, this information is frequently presented with a map to visualize streets, buildings, and vegetation.Large language models (LLMs) are a subset of foundation models (FMs) that can transform input (usually text or image, depending on model modality) into outputs (generally text) through a process called . Amazon Bedrock is a comprehensive, secure, and flexible service for building generative AI applications and agents.LLMs work in many generalized tasks involving natural language. Some common LLM use cases include: – Use a model to summarize text or a document. – Use a model to answer questions about data or facts from context provided during training or inference using Retrieval Augmented Generation (RAG). – Use a model to provide chain of thought reasoning to assist a human with decision-making and hypothesis evaluation. – Use a model to generate synthetic data for testing simulations or hypothetical scenarios. – Use a model to draft a report from insights derived from an Amazon Bedrock knowledge base or a user’s prompt.AI agent and tool orchestration – Use a model to plan the invocation of other systems and processes. After other systems are invoked by an agent, the agent’s output can then be used as context for further LLM generation.GIS can implement these capabilities to create value and improve user experiences. Benefits can include: – Taking real-time insights to support immediate decision-making, such as emergency response coordination and traffic management – In-depth analysis that humans or systems can identify, such as trend analysis, patterns and relationships, and environmental monitoring – Using research and analysis for informed long-term decision-making, such as infrastructure development, resource allocation, and environmental regulationAugmenting GIS and workflows with LLM capabilities leads to simpler analysis and exploration of data, discovery of new insights, and improved decision-making. Amazon Bedrock provides a way to host and invoke models as well as integrate the AI models with surrounding infrastructure, which we elaborate on in this post.Combining GIS and AI through RAG and agentic workflowsLLMs are trained with large amounts of generalized information to discover patterns in how language is produced. To improve the performance of LLMs for specific use cases, approaches such as RAG and agentic workflows have been created. Retrieving policies and general knowledge for geospatial use cases can be accomplished with RAG, whereas calculating and analyzing GIS data would require an agentic workflow. In this section, we expand upon both RAG and agentic workflows in the context of geospatial use cases.Retrieval Augmented GenerationWith RAG, you can dynamically inject contextual information from a knowledge base during model invocation.RAG supplements a user-provided prompt with data sourced from a knowledge base (collection of documents). Amazon Bedrock offers managed knowledge bases to data sources, such as Amazon Simple Storage Service (Amazon S3) and SharePoint, so you can provide supplemental information, such as city development plans, intelligence reports, or policies and regulations, when your AI assistant is generating a response for a user.Knowledge bases are ideal for unstructured documents with information stored in natural language. When your AI model responds to a user with information sourced from RAG, it can provide references and citations to its source material. The following diagram shows how the systems connect together.Because geospatial data is often structured and in a GIS, you can connect the GIS to the LLM using tools and agents instead of knowledge bases.Tools and agents (to control a UI and a system)Many LLMs, such as Anthropic’s Claude on Amazon Bedrock, make it possible to provide a description of tools available so your AI model can generate text to invoke external processes. These processes might retrieve live information, such as the current weather in a location or querying a structured data store, or might control external systems, such as starting a workflow or adding layers to a map. Some common geospatial functionality that you might want to integrate with your LLM using tools include:Performing mathematical calculations like the distance between coordinates, filtering datasets based on numeric values, or calculating derived fieldsDeriving information from predictive analysis modelsLooking up points of interest in structured data storesSearching content and metadata in unstructured data storesRetrieving real-time geospatial data, like traffic, directions, or estimated time to reach a destinationVisualizing distances, points of interest, or pathsSubmitting work outputs such as analytic reportsStarting workflows, like ordering supplies or adjusting supply chainTools are often implemented in AWS Lambda functions. Lambda runs code without the complexity and overhead of running servers. It handles the infrastructure management, enabling faster development, improved performance, enhanced security, and cost-efficiency.Amazon Bedrock offers the feature Amazon Bedrock Agents to simplify the orchestration and integration with your geospatial tools. Amazon Bedrock agents follow instructions for LLM reasoning to break down a user prompt into smaller tasks and perform actions against identified tasks from action providers. The following diagram illustrates how Amazon Bedrock Agents works.The following diagram shows how Amazon Bedrock Agents can enhance GIS solutions.The following demonstration applies the concepts we’ve discussed to an earthquake analysis agent as an example. This example deploys an Amazon Bedrock agent with a knowledge base based on Amazon Redshift. The Redshift instance has two tables. One table is for earthquakes, which includes date, magnitude, latitude, and longitude. The second table holds the counites in California, described as polygon shapes. The geospatial capabilities of Amazon Redshift can relate these datasets to answer queries like which county had the most recent earthquake or which county has had the most earthquakes in the last 20 years. The Amazon Bedrock agent can generate these geospatially based queries based on natural language.This script creates an end-to-end pipeline that performs the following steps:Processes geospatial data.Sets up cloud infrastructure.Loads and configures the spatial database.Creates an AI agent for spatial analysis.In the following sections, we create this agent and test it out.To implement this approach, you must have an AWS account with the appropriate AWS Identity and Access Management (IAM) permissions for Amazon Bedrock, Amazon Redshift, and Amazon S3.Confirm you have access to the latest version of the AWS CLI.Sign in to the AWS CLI with your credentials.Make sure ./jq is installed. If not, use the following command:Use the following code for the initial setup and error handling:#!/usr/bin/env bash
set -ex

LOG_FILE="deployment_$(date +%Y%m%d_%H%M%S).log"
touch "$LOG_FILE"

handle_error() {
    local exit_code=$?
    local line_number=$1
    if [ $exit_code -ne 0 ]; then
        log_error "Failed at line $line_number with exit code $exit_code"
        exit $exit_code
    fi
}
trap 'handle_error $LINENO' ERRThis code performs the following functions:Creates a timestamped log fileSets up error trapping that captures line numbersEnables automatic script termination on errorsImplements detailed logging of failuresValidate the AWS environmentUse the following code to validate the AWS environment:AWS_VERSION=$(aws --version 2>&1)
log "INFO" "AWS CLI version: $AWS_VERSION"

if ! aws sts get-caller-identity &>/dev/null; then
    log_error "AWS CLI is not configured with valid credentials"
    exit 1
fi

AWS_REGION="us-east-1"
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)This code performs the essential AWS setup verification:Checks AWS CLI installationValidates AWS credentialsRetrieves account ID for resource namingSet up Amazon Redshift and Amazon Bedrock variablesUse the following code to create Amazon Redshift and Amazon Bedrock variables:REDSHIFT_CLUSTER_IDENTIFIER="geo-analysis-cluster"
REDSHIFT_DATABASE="geo_db"
REDSHIFT_MASTER_USER= [Create username]
REDSHIFT_MASTER_PASSWORD= [Create Password]
REDSHIFT_NODE_TYPE="dc2.large"
REDSHIFT_CLUSTER_TYPE="single-node"
BEDROCK_ROLE_NAME="BedrockGeospatialRole"
# Bedrock Configuration
AGENT_NAME="GeoAgentRedshift"
KNOWLEDGE_BASE_NAME="GeospatialKB"Create IAM roles for Amazon Redshift and Amazon S3Use the following code to set up IAM roles for Amazon S3 and Amazon Redshift:if aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" &>/dev/null; then
    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" --query 'Role.Arn' --output text)
    log "INFO" "Using existing role ARN: $REDSHIFT_ROLE_ARN"
else
    # Create trust policy document
    cat > /tmp/trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "redshift.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
    # Create role
    CREATE_ROLE_OUTPUT=$(aws iam create-role \
        --role-name "$REDSHIFT_ROLE_NAME" \
        --assume-role-policy-document "file:///tmp/trust-policy.json" \
        --description "Role for Redshift to access S3" 2>&1)
    
    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" --query 'Role.Arn' --output text)
    if [ $? -ne 0 ]; then
        log_error "Failed to create role:"
        exit 1
    fi
    REDSHIFT_ROLE_ARN=$(echo "$CREATE_ROLE_OUTPUT" | jq -r '.Role.Arn')
    # Wait for role to be available
    sleep 10
fi
ATTACH_POLICY_OUTPUT=$(aws iam attach-role-policy \
    --role-name "$REDSHIFT_ROLE_NAME" \
    --policy-arn "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess" 2>&1)
if [ $? -ne 0 ]; then
    if echo "$ATTACH_POLICY_OUTPUT" | grep -q "EntityAlreadyExists"; then
    else
        exit 1
    fi
fiPrepare the data and Amazon S3Use the following code to prepare the data and Amazon S3 storage:DATA_BUCKET="geospatial-bedrock-demo-data-${AWS_ACCOUNT_ID}"
aws s3 mb s3://$DATA_BUCKET

# Download source data
curl -o earthquakes.csv https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/earthquake-data/earthquakes.csv
curl -o california-counties.json https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/counties-data/california-counties.jsonThis code sets up data storage and retrieval through the following steps:Creates a unique S3 bucketDownloads earthquake and county boundary dataPrepares for data transformationTransform geospatial dataUse the following code to transform the geospatial data:INPUT_FILE="california-counties.json"
OUTPUT_FILE="california-counties.csv"

# Create CSV header
echo "OBJECTID,AREA,PERIMETER,CO06_D00_,CO06_D00_I,STATE,COUNTY,NAME,LSAD,LSAD_TRANS,Shape_Length,Shape_Area,WKT" > "$OUTPUT_FILE"

# Function to convert ESRI rings to WKT POLYGON format
esri_to_wkt() {
    local rings=$1
    
    # Extract the first ring (exterior ring)
    local exterior_ring=$(echo "$rings" | jq -c '.[0]')
    
    if [ "$exterior_ring" = "null" ] || [ -z "$exterior_ring" ]; then
        echo "POLYGON EMPTY"
        return
    fi
    
    # Start building the WKT string
    local wkt="POLYGON (("
    
    # Process each coordinate pair in the ring
    local coords=$(echo "$exterior_ring" | jq -r '.[] | "\(.[0]) \(.[1])"')
    local first_coord=""
    local result=""
    
    while IFS= read -r coord; do
        if [ -z "$result" ]; then
            result="$coord"
            first_coord="$coord"
        else
            result="$result, $coord"
        fi
    done <<< "$coords"
    
    # Close the ring by adding the first coordinate again if needed
    if [ "$first_coord" != "$(echo "$coords" | tail -1)" ]; then
        result="$result, $first_coord"
    fi
    
    wkt="${wkt}${result}))"
    echo "$wkt"
}

# Process each feature in the JSON file
jq -c '.features[]' "$INPUT_FILE" | while read -r feature; do
    # Extract attributes
    OBJECTID=$(echo "$feature" | jq -r '.attributes.OBJECTID // empty')
    AREA=$(echo "$feature" | jq -r '.attributes.AREA // empty')
    PERIMETER=$(echo "$feature" | jq -r '.attributes.PERIMETER // empty')
    CO06_D00_=$(echo "$feature" | jq -r '.attributes.CO06_D00_ // empty')
    CO06_D00_I=$(echo "$feature" | jq -r '.attributes.CO06_D00_I // empty')
    STATE=$(echo "$feature" | jq -r '.attributes.STATE // empty')
    COUNTY=$(echo "$feature" | jq -r '.attributes.COUNTY // empty')
    NAME=$(echo "$feature" | jq -r '.attributes.NAME // empty')
    LSAD=$(echo "$feature" | jq -r '.attributes.LSAD // empty')
    LSAD_TRANS=$(echo "$feature" | jq -r '.attributes.LSAD_TRANS // empty')
    Shape_Length=$(echo "$feature" | jq -r '.attributes.Shape_Length // empty')
    Shape_Area=$(echo "$feature" | jq -r '.attributes.Shape_Area // empty')
    
    # Extract geometry and convert to WKT
    if echo "$feature" | jq -e '.geometry.rings' > /dev/null 2>&1; then
        rings=$(echo "$feature" | jq -c '.geometry.rings')
        WKT=$(esri_to_wkt "$rings")
    else
        WKT="POLYGON EMPTY"
    fi
    
    # Escape any commas in the fields
    NAME=$(echo "$NAME" | sed 's/,/\\,/g')
    LSAD=$(echo "$LSAD" | sed 's/,/\\,/g')
    LSAD_TRANS=$(echo "$LSAD_TRANS" | sed 's/,/\\,/g')
    
     # Write to CSV - wrap WKT field in quotes
    echo "$OBJECTID,$AREA,$PERIMETER,$CO06_D00_,$CO06_D00_I,$STATE,$COUNTY,$NAME,$LSAD,$LSAD_TRANS,$Shape_Length,$Shape_Area,\"$WKT\"" >> "$OUTPUT_FILE"
done

echo "Conversion complete. Output saved to $OUTPUT_FILE"

# Upload data files to S3
aws s3 cp earthquakes.csv s3://$DATA_BUCKET/earthquakes/
aws s3 cp california-counties.csv s3://$DATA_BUCKET/counties/This code performs the following actions to convert the geospatial data formats:Transforms ESRI JSON to WKT formatProcesses county boundaries into CSV formatPreserves spatial information for Amazon RedshiftCreate a Redshift clusterUse the following code to set up the Redshift cluster:# Create Redshift cluster
aws redshift create-cluster \
    --cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
    --node-type "$REDSHIFT_NODE_TYPE" \
    --cluster-type single-node \
    --master-username "$REDSHIFT_MASTER_USER" \
    --master-user-password "$REDSHIFT_MASTER_PASSWORD" \
    --db-name "$REDSHIFT_DATABASE" \
    --cluster-subnet-group-name "$SUBNET_GROUP_NAME" \
    --vpc-security-group-ids "$SG_ID" \
    --iam-roles "$REDSHIFT_ROLE_ARN"

# Wait for cluster availability
while true; do
    CLUSTER_STATUS=$(aws redshift describe-clusters \
        --cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
        --query 'Clusters[0].ClusterStatus' \
        --output text)
    if [ "$CLUSTER_STATUS" = "available" ]; then
        break
    fi
    sleep 30
doneThis code performs the following functions:Sets up a single-node clusterConfigures networking and securityWaits for cluster availabilityUse the following code to create the database schema:aws redshift-data execute-statement \
    --cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
    --database "$REDSHIFT_DATABASE" \
    --sql "
CREATE TABLE IF NOT EXISTS counties (
    OBJECTID INTEGER PRIMARY KEY,
    AREA DOUBLE PRECISION,
    NAME VARCHAR(100),
    geom GEOMETRY
);

CREATE TABLE IF NOT EXISTS earthquakes (
    earthquake_date VARCHAR(50),
    latitude double precision,
    longitude double precision,
    magnitude double precision
);"This code performs the following functions:Creates a counties table with spatial dataCreates an earthquakes tableConfigures appropriate data typesCreate an Amazon Bedrock knowledge baseUse the following code to create a knowledge base:# Create knowledge base
aws bedrock-agent create-knowledge-base \
    --name "$KNOWLEDGE_BASE_NAME" \
    --knowledge-base-configuration "{
        \"type\": \"SQL\",
        \"sqlKnowledgeBaseConfiguration\": {
            \"type\": \"REDSHIFT\"
        }
    }" \
    --region "$AWS_REGION"

# Create data source
aws bedrock-agent create-data-source \
    --knowledge-base-id "$KB_ID" \
    --name "EarthquakeDataSource" \
    --data-source-configuration "{\"type\": \"REDSHIFT_METADATA\"}"This code performs the following functions:Creates an Amazon Bedrock knowledge baseSets up an Amazon Redshift data sourceCreate an Amazon Bedrock agentUse the following code to create and configure an agent:# Create agent
aws bedrock-agent create-agent \
    --agent-name "$AGENT_NAME" \
    --instruction "You are a geospatial analysis assistant..." \
    --foundation-model "anthropic.claude-3-sonnet-20240229-v1:0"

# Associate knowledge base
aws bedrock-agent associate-agent-knowledge-base \
    --agent-id "$AGENT_ID" \
    --knowledge-base-id "$KB_ID" \
    --description "Earthquake data knowledge base" \
    --agent-version "DRAFT"This code performs the following functions:Creates an Amazon Bedrock agentAssociates the agent with the knowledge baseConfigures the AI model and instructionsLet’s observe the system behavior with the following natural language user inputs in the chat window.Example 1: Summarization and Q&AFor this example, we use the prompt “Summarize which zones allow for building of an apartment.”The LLM performs retrieval with a RAG approach, then uses the retrieved residential code documents as context to answer the user’s query in natural language.This example demonstrates the LLM capabilities for hallucination mitigation, RAG, and summarization.Example 2: Generate a draft reportNext, we input the prompt “Write me a report on how various zones and related housing data can be utilized to plan new housing development to meet high demand.”The LLM retrieves relevant urban planning code documents, then summarizes the information into a standard reporting format as described in its system prompt.This example demonstrates the LLM capabilities for prompt templates, RAG, and summarization.Example 3: Show places on the mapFor this example, we use the prompt “Show me the low density properties on Abbeville street in Macgregor on the map with their address.”The LLM creates a chain of thought to look up which properties match the user’s query and then invokes the draw marker tool on the map. The LLM provides tool invocation parameters in its scratchpad, awaits the completion of these tool invocations, then responds in natural language with a bulleted list of markers placed on the map.This example demonstrates the LLM capabilities for chain of thought reasoning, tool use, retrieval systems using agents, and UI control.Example 4: Use the UI as contextFor this example, we choose a marker on a map and input the prompt “Can I build an apartment here.”The “here” is not contextualized from conversation history but rather from the state of the map view. Having a state engine that can relay information from a frontend view to the LLM input adds a richer context.The LLM understands the context of “here” based on the selected marker, performs retrieval to see the land development policy, and responds to the user in simple natural language, “No, and here is why…”This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, RAG, and tool use.Example 5: UI context and UI controlNext, we choose a marker on the map and input the prompt “draw a .25 mile circle around here so I can visualize walking distance.”The LLM invokes the draw circle tool to create a layer on the map centered at the selected marker, contextualized by “here.”This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, tool use, and UI control.To clean up your resources and prevent AWS charges from being incurred, complete the following steps:Delete the Amazon Bedrock knowledge base.Delete the Redshift cluster.The integration of LLMs with GIS creates intuitive systems that help users of different technical levels perform complex spatial analysis through natural language interactions. By using RAG and agent-based workflows, organizations can maintain data accuracy while seamlessly connecting AI models to their existing knowledge bases and structured data systems. Amazon Bedrock facilitates this convergence of AI and GIS technology by providing a robust platform for model invocation, knowledge retrieval, and system control, ultimately transforming how users visualize, analyze, and interact with geographical data.For further exploration, Earth on AWS has videos and articles you can explore to understand how AWS is helping build GIS applications on the cloud. is a Sr. Solutions Architect supporting Federal System Integrators at AWS. He is based in Washington, DC, and has 15 years of experience building, modernizing, and integrating systems for public sector customers. Outside of work, Dave enjoys playing with his kids, hiking, and watching Penn State football! is a solutions architect on the Worldwide Public Sector Global Systems Integrator Architecture team at Amazon Web Services (AWS). She has a focus in data analytics and helping customer organizations make data-driven decisions. Outside of work, she loves spending time with friends and family and traveling. is the Head of Partner Deployed Engineering at Windsurf focusing on how partners can bring organizational value through the adoption of Agentic AI software development tools like Windsurf and Devin. Brian has a background in Cloud Solutions Architecture from his time at AWS, where he worked in the AWS Federal Partner ecosystem. In his personal time, Brian enjoys skiing, water sports, and traveling with friends and family.]]></content:encoded></item><item><title>Would you graph your commute? Here’s what I found when I did.</title><link>https://dev.to/kauldeepak78/would-you-graph-your-commute-heres-what-i-found-when-i-did-123</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:40:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Turning My Daily Commute into a Data Visualization Project]]></content:encoded></item><item><title>Because every train - What my mood, weather, and trains revealed in 3 months of tracking delay deserves a chart</title><link>https://dev.to/kauldeepak78/because-every-train-what-my-mood-weather-and-trains-revealed-in-3-months-of-tracking-delay-4213</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:39:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Turning My Daily Commute into a Data Visualization Project]]></content:encoded></item><item><title>Because every train delay deserves a chart</title><link>https://dev.to/kauldeepak78/because-every-train-delay-deserves-a-chart-4i86</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:38:09 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[From rush hour chaos to beautiful graphs]]></content:encoded></item><item><title>From rush hour chaos to beautiful graphs</title><link>https://dev.to/kauldeepak78/from-rush-hour-chaos-to-beautiful-graphs-5823</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:37:53 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When boredom meets Python, you get insights]]></content:encoded></item><item><title>When boredom meets Python, you get insights</title><link>https://dev.to/kauldeepak78/when-boredom-meets-python-you-get-insights-633</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:37:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Turning My Daily Commute into a Data Visualization Project]]></content:encoded></item><item><title>Beyond the basics: A comprehensive foundation model selection framework for generative AI</title><link>https://aws.amazon.com/blogs/machine-learning/beyond-the-basics-a-comprehensive-foundation-model-selection-framework-for-generative-ai/</link><author>Sandeep Singh</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 17:31:28 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Most organizations evaluating foundation models limit their analysis to three primary dimensions: accuracy, latency, and cost. While these metrics provide a useful starting point, they represent an oversimplification of the complex interplay of factors that determine real-world model performance.Foundation models have revolutionized how enterprises develop generative AI applications, offering unprecedented capabilities in understanding and generating human-like content. However, as the model landscape expands, organizations face complex scenarios when selecting the right foundation model for their applications. In this blog post we present a systematic evaluation methodology for Amazon Bedrock users, combining theoretical frameworks with practical implementation strategies that empower data scientists and machine learning (ML) engineers to make optimal model selections.The challenge of foundation model selectionAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models from leading AI companies such as AI21 Labs, Anthropic, Cohere, DeepSeek, Luma, Meta, Mistral AI, poolside (coming soon), Stability AI, TwelveLabs (coming soon), Writer, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. The service’s API-driven approach allows seamless model interchangeability, but this flexibility introduces a critical challenge: which model will deliver optimal performance for a specific application while meeting operational constraints?Our research with enterprise customers reveals that many early generative AI projects select models based on either limited manual testing or reputation, rather than systematic evaluation against business requirements. This approach frequently results in:Over-provisioning computational resources to accommodate larger models than requiredSub-optimal performance because of misalignment between model strengths and use case requirementsUnnecessarily high operational costs because of inefficient token utilizationProduction performance issues discovered too late in the development lifecycleA multidimensional evaluation framework—Foundation model capability matrixFoundation models vary significantly across multiple dimensions, with performance characteristics that interact in complex ways. Our capability matrix provides a structured view of critical dimensions to consider when evaluating models in Amazon Bedrock. Below are four core dimensions (in no specific order) – Task performance, Architectural characteristics, Operational considerations, and Responsible AI attributes.Evaluating the models based on the task performance is crucial for achieving direct impact on business outcomes, ROI, user adoption and trust, and competitive advantage.: Evaluate models using benchmarks relevant to your use case (MMLU, HELM, or domain-specific benchmarks).Few-shot learning capabilities: Strong few-shot performers require minimal examples to adapt to new tasks, leading to cost efficiency, faster time-to-market, resource optimization, and operational benefits.Instruction following fidelity: For the applications that require precise adherence to commands and constraints, it is critical to evaluate model’s instruction following fidelity.: Reliability and reproducibility across multiple runs with identical prompts.Domain-specific knowledge: Model performance varies dramatically across specialized fields based on training data. Evaluate the models base on your domain-specific use-case scenarios.Evaluate the model’s ability to perform logical inference, causal reasoning, and multi-step problem-solving. This can include reasoning such as deductive and inductive, mathematical, chain-of-thought, and so on.Architectural characteristicsArchitectural characteristics for evaluating the models are important as they directly impact the model’s performance, efficiency, and suitability for specific tasks.Parameter count (model size): Larger models typically offer more capabilities but require greater computational resources and may have higher inference costs and latency.Training data composition: Models trained on diverse, high-quality datasets tend to have better generalization abilities across different domains.: Decoder-only models excel at text generation, encoder-decoder architectures handle translation and summarization more effectively, while mixture of experts (MoE) architectures can be a powerful tool for improving the performance of both decoder-only and encoder-decoder models. Some specialized architectures focus on enhancing reasoning capabilities through techniques like chain-of-thought prompting or recursive reasoning.: The way models process text affects performance on domain-specific tasks, particularly with specialized vocabulary.Context window capabilities: Larger context windows enable processing more information at once, critical for document analysis and extended conversations.: Modality refers to type of data a model can process and generate, such as text, image, audio, or video. Consider the modality of the models depending on the use case, and choose the model optimized for that specific modality.Operational considerationsBelow listed operational considerations are critical for model selection as they directly impact the real-world feasibility, cost-effectiveness, and sustainability of AI deployments.Throughput and latency profiles: Response speed impacts user experience and throughput determines scalability.: Input/output token pricing significantly affects economics at scale.Scalability characteristics: Ability to handle concurrent requests and maintain performance during traffic spikes.: Fine-tuning capabilities and adaptation methods for tailoring to specific use cases or domains.: Ease of integration into existing systems and workflow is an important consideration.: When dealing with sensitive data, model security—including data encryption, access control, and vulnerability management—is a crucial consideration.Responsible AI attributesAs AI becomes increasingly embedded in business operations and daily lives, evaluating models on responsible AI attributes isn’t just a technical consideration—it’s a business imperative.: Models vary in their tendency to generate plausible but incorrect information.: Performance across different demographic groups affects fairness and equity.Safety guardrail effectiveness: Resistance to generating harmful or inappropriate content.Explainability and privacy: Transparency features and handling of sensitive information.: Legal considerations should include data privacy, non-discrimination, intellectual property, and product liability.Agentic AI considerations for model selectionThe growing popularity of agentic AI applications introduces evaluation dimensions beyond traditional metrics. When assessing models for use in autonomous agents, consider these critical capabilities:Agent-specific evaluation dimensionsPlanning and reasoning capabilities: Evaluate chain-of-thought consistency across complex multi-step tasks and self-correction mechanisms that allow agents to identify and fix their own reasoning errors.: Test function calling capabilities, parameter handling precision, and structured output consistency (JSON/XML) for seamless tool use.Agent-to-agent communication: Assess protocol adherence to frameworks like A2A and efficient contextual memory management across extended multi-agent interactions.Multi-agent collaboration testing for applications using multiple specialized agents: Measure how well models maintain distinct agent personas and responsibilities without role confusion.Information sharing efficiency: Test how effectively information flows between agent instances without critical detail loss.Collaborative intelligence: Verify whether multiple agents working together produce better outcomes than single-model approaches.Error propagation resistance: Assess how robustly multi-agent systems contain and correct errors rather than amplifying them.A four-phase evaluation methodologyOur recommended methodology progressively narrows model selection through increasingly sophisticated assessment techniques:Phase 1: Requirements engineeringBegin with a precise specification of your application’s requirements:: Define primary tasks, domain knowledge needs, language support, output formats, and reasoning complexity.Non-functional requirements: Specify latency thresholds, throughput requirements, budget constraints, context window needs, and availability expectations.Responsible AI requirements: Establish hallucination tolerance, bias mitigation needs, safety requirements, explainability level, and privacy constraints.Agent-specific requirements: For agentic applications, define tool-use capabilities, protocol adherence standards, and collaboration requirements.Assign weights to each requirement based on business priorities to create your evaluation scorecard foundation.Phase 2: Candidate model selectionUse the Amazon Bedrock model information API to filter models based on hard requirements. This typically reduces candidates from dozens to 3–7 models that are worth detailed evaluation.Filter options include but aren’t limited to the following:Filter by modality support, context length, and language capabilitiesExclude models that don’t meet minimum performance thresholdsCalculate theoretical costs at projected scale so that you can exclude options that exceed the available budgetFilter for customization requirements such as fine-tuning capabilitiesFor agentic applications, filter for function calling and multi-agent protocol supportAlthough the Amazon Bedrock model information API might not provide the filters you need for candidate selection, you can use the Amazon Bedrock model catalog (shown in the following figure) to obtain additional information about these models.Phase 3: Systematic performance evaluationPrepare evaluation datasets: Create representative task examples, challenging edge cases, domain-specific content, and adversarial examples.Design evaluation prompts: Standardize instruction format, maintain consistent examples, and mirror production usage patterns.: Select appropriate metrics for subjective tasks (human evaluation and reference-free quality), objective tasks (precision, recall, and F1 score), and reasoning tasks (logical consistency and step validity).: Add protocol conformance testing, multi-step planning assessment, and tool-use evaluation.: Maintain consistent parameters across models and collect comprehensive performance data.Measure operational performance: Capture throughput, latency distributions, error rates, and actual token consumption costs.Phase 4: Decision analysisTransform evaluation data into actionable insights:: Scale all metrics to comparable units using min-max normalization.: Calculate composite scores based on your prioritized requirements.Perform sensitivity analysis: Test how robust your conclusions are against weight variations.: Create radar charts, efficiency frontiers, and tradeoff curves for clear comparison.: Detail each model’s strengths, limitations, and optimal use cases.Advanced evaluation techniquesBeyond standard procedures, consider the following approaches for evaluating models.A/B testing with production trafficImplement comparative testing using Amazon Bedrock’s routing capabilities to gather real-world performance data from actual users.Test model vulnerabilities through prompt injection attempts, challenging syntax, edge case handling, and domain-specific factual challenges.Multi-model ensemble evaluationAssess combinations such as sequential pipelines, voting ensembles, and cost-efficient routing based on task complexity.Continuous evaluation architectureDesign systems to monitor production performance with:Stratified sampling of production traffic across task types and domainsRegular evaluations and trigger-based reassessments when new models emergePerformance thresholds and alerts for quality degradationUser feedback collection and failure case repositories for continuous improvementIndustry-specific considerationsDifferent sectors have unique requirements that influence model selection:: Regulatory compliance, numerical precision, and personally identifiable information (PII) handling capabilities: Medical terminology understanding, HIPAA adherence, and clinical reasoning: Technical specification comprehension, procedural knowledge, and spatial reasoning: Autonomous reasoning, tool integration, and protocol conformanceBest practices for model selectionThrough this comprehensive approach to model evaluation and selection, organizations can make informed decisions that balance performance, cost, and operational requirements while maintaining alignment with business objectives. The methodology makes sure that model selection isn’t a one-time exercise but an evolving process that adapts to changing needs and technological capabilities.Assess your situation thoroughly: Understand your specific use case requirements and available resourcesSelect meaningful metrics: Focus on metrics that directly relate to your business objectivesBuild for continuous evaluation: Design your evaluation process to be repeatable as new models are releasedLooking forward: The future of model selectionAs foundation models evolve, evaluation methodologies must keep pace. Below are further considerations (By no means this list of considerations is exhaustive and is subject to ongoing updates as technology evolves and best practices emerge), you should take into account while selecting the best model(s) for your use-case(s).Multi-model architectures: Enterprises will increasingly deploy specialized models in concert rather than relying on single models for all tasks.: Evaluation frameworks must assess how models perform as autonomous agents with tool-use capabilities and inter-agent collaboration.: The growing landscape of domain-specific models will require more nuanced evaluation of specialized capabilities.: As models become more capable, evaluation of controllability and alignment with human intent becomes increasingly important.By implementing a comprehensive evaluation framework that extends beyond basic metrics, organizations can informed decisions about which foundation models will best serve their requirements. For agentic AI applications in particular, thorough evaluation of reasoning, planning, and collaboration capabilities is essential for success. By approaching model selection systematically, organizations can avoid the common pitfalls of over-provisioning, misalignment with use case needs, excessive operational costs, and late discovery of performance issues. The investment in thorough evaluation pays dividends through optimized costs, improved performance, and superior user experiences. is a Senior Generative AI Data Scientist at Amazon Web Services, helping businesses innovate with generative AI. He specializes in generative AI, machine learning, and system design. He has successfully delivered state-of-the-art AI/ML-powered solutions to solve complex business problems for diverse industries, optimizing efficiency and scalability.]]></content:encoded></item><item><title>Accelerate intelligent document processing with generative AI on AWS</title><link>https://aws.amazon.com/blogs/machine-learning/accelerate-intelligent-document-processing-with-generative-ai-on-aws/</link><author>Bob Strahan</author><category>dev</category><category>ai</category><enclosure url="https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-18800/video_1755214496026.mp4" length="" type=""/><pubDate>Fri, 22 Aug 2025 17:26:31 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Every day, organizations process millions of documents, including invoices, contracts, insurance claims, medical records, and financial statements. Despite the critical role these documents play, an estimated 80–90% of the data they contain is unstructured and largely untapped, hiding valuable insights that could transform business outcomes. Despite advances in technology, many organizations still rely on manual data entry, spending countless hours extracting information from PDFs, scanned images, and forms. This manual approach is time-consuming, error-prone, and prevents organizations from scaling their operations and responding quickly to business demands.Although generative AI has made it easier to build proof-of-concept document processing solutions, the journey from proof of concept to production remains fraught with challenges. Organizations often find themselves rebuilding from scratch when they discover their prototype can’t handle production volumes, lacks proper error handling, doesn’t scale cost-effectively, or fails to meet enterprise security and compliance requirements. What works in a demo with a handful of documents often breaks down when processing thousands of documents daily in a production environment.In this post, we introduce our open source GenAI IDP Accelerator—a tested solution that we use to help customers across industries address their document processing challenges. Automated document processing workflows accurately extract structured information from documents, reducing manual effort. We will show you how this ready-to-deploy solution can help you build those workflows with generative AI on AWS in days instead of months.Understanding intelligent document processingIntelligent document processing (IDP) encompasses the technologies and techniques used to extract and process data from various document types. Common IDP tasks include:OCR (Optical Character Recognition) – Converting scanned documents and images into machine-readable text – Automatically identifying document types (such as invoices, contracts, or forms) – Pulling structured information from unstructured documents – Evaluating the quality and confidence of extracted data – Creating concise summaries of document content – Measuring accuracy and performance against expected outcomesThese capabilities are critical across industries. In financial services, organizations use IDP to process loan applications, extract data from bank statements, and validate insurance claims. Healthcare providers rely on IDP to extract patient information from medical records, process insurance forms, and handle lab results efficiently. Manufacturing and logistics companies use IDP to process invoices and purchase orders, extract shipping information, and handle quality certificates. Government agencies use IDP to process citizen applications, extract data from tax forms, manage permits and licenses, and enforce regulatory compliance.The generative AI revolution in IDPTraditional IDP solutions relied on template-based extraction, regular expressions, and classical machine learning (ML) models. Though functional, these approaches required extensive setup, struggled with document variations, and achieved limited accuracy on complex documents.The emergence of large language models (LLMs) and generative AI has fundamentally transformed IDP capabilities. Modern AI models can understand document context, handle variations without templates, achieve near-human accuracy on complex extractions, and adapt to new document types with minimal examples. This shift from rule-based to intelligence-based processing means organizations can now process different document types with high accuracy, dramatically reducing the time and cost of implementation.We’re excited to share the GenAI IDP Accelerator—an open source solution that transforms how organizations handle document processing by dramatically reducing manual effort and improving accuracy. This serverless foundation offers processing patterns which use Amazon Bedrock Data Automation for rich out-of-the-box document processing features, high accuracy, ease of use, and straightforward per-page pricing, Amazon Bedrock state-of-the-art foundation models (FMs) for complex documents requiring custom logic, and other AWS AI services to provide a flexible, scalable starting point for enterprises to build document automation tailored to their specific needs.The following is a short demo of the solution in action, in this case showcasing the default Amazon Bedrock Data Automation processing pattern.The GenAI IDP Accelerator is already transforming document processing for organizations across industries.Competiscan: Transforming marketing intelligence at scaleCompetiscan, a leader in competitive marketing intelligence, faced a massive challenge: processing 35,000–45,000 marketing campaigns daily while maintaining a searchable archive of 45 million campaigns spanning 15 years.Using the GenAI IDP Accelerator, Competiscan achieved the following:85% classification and extraction accuracy across diverse marketing materialsIncreased scalability to handle 35,000–45,000 daily campaignsRemoval of critical bottlenecks, facilitating business growthProduction deployment in just 8 weeks from initial conceptRicoh: Scaling document processingRicoh, a global leader in document management, implemented the GenAI IDP Accelerator to transform healthcare document processing for their clients. Processing over 10,000 healthcare documents monthly with potential to scale to 70,000, they needed a solution that could handle complex medical documentation with high accuracy.The results speak for themselves:Savings potential of over 1,900 person-hours annually through automationAchieved extraction accuracy to help minimize financial penalties from processing errorsAutomated classification of grievances vs. appealsCreated a reusable framework deployable across multiple healthcare customersIntegrated with human-in-the-loop review for cases requiring expert validationLeveraged modular architecture to integrate with existing systems, enabling custom document splitting and large-scale document processingThe GenAI IDP Accelerator is a modular, serverless solution that automatically converts unstructured documents into structured, actionable data. Built entirely on AWS services, it provides enterprise-grade scalability, security, and cost-effectiveness while requiring minimal setup and maintenance. Its configuration-driven design helps teams quickly adapt prompts, extraction templates, and validation rules for their specific document types without touching the underlying infrastructure.The solution follows a modular pipeline that enriches documents at each stage, from OCR to classification, to extraction, to assessment, to summarization, and ending with evaluation.You can deploy and customize each step independently, so you can optimize for your specific use cases while maintaining the benefits of the integrated workflow.The following diagram illustrates the solution architecture, showing the default Bedrock Data Automation workflow (Pattern-1).Refer to the GitHub repo for additional details and processing patterns.Some of the key features of the solution include: – Built on AWS Lambda, AWS Step Functions, and other serverless technologies for queueing, concurrency management, and retries to provide automatic scaling and pay-per-use pricing for production workloads of many sizesGenerative AI-powered document packet splitting and classification – Intelligent document classification using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMs, including support for multi-document packets and packet splittingAdvanced AI key information extraction – Key information extraction using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMsMultiple processing patterns – Choose from pre-built patterns optimized for different workloads with different configurability, cost, and accuracy requirements, or extend the solution with additional patterns: 
  Pattern 1 – Uses Amazon Bedrock Data Automation, a fully managed service that offers rich out-of-the-box features, ease of use, and straightforward per-page pricing. This pattern is recommended for most use cases.Pattern 2 – Uses Amazon Textract and Amazon Bedrock with Amazon Nova, Anthropic’s Claude, or custom fine-tuned Amazon Nova models. This pattern is ideal for complex documents requiring custom logic.Pattern 3 – Uses Amazon Textract, Amazon SageMaker with a fine-tuned model for classification, and Amazon Bedrock for extraction. This pattern is ideal for documents requiring specialized classification.We expect to add more pattern options to handle additional real-world document processing needs, and to take advantage of ever-improving state-of-the-art capabilities: – Improve accuracy for classification and extraction by providing few-shot examples to guide the AI modelsHuman-in-the-loop (HITL) review – Integrated workflow for human review of low-confidence extractions using Amazon SageMaker Augmented AI (Amazon A2I), currently available for Pattern 1, with support for Patterns 2 and 3 coming soon – Responsive web UI for monitoring document processing, viewing results, and managing configurations – Framework to evaluate and improve accuracy against baseline dataAnalytics and reporting database – Centralized analytics database for tracking processing metrics, accuracy trends, and cost optimization across document workflows, and for analyzing extracted document content using Amazon Athena – Customize document types, extraction fields, and processing logic through configuration, editable in the web UIDeveloper-friendly python package – For data science and engineering teams who want to experiment, optimize, or integrate the IDP capabilities directly into their workflows, the solution’s core logic is available through the idp_common Python packageBefore you deploy the solution, make sure you have an AWS account with administrator permissions and access to Amazon and Anthropic models on Amazon Bedrock. For more details, see Access Amazon Bedrock foundation models.Deploy the GenAI IDP AcceleratorTo deploy the GenAI IDP Accelerator, you can use the provided AWS CloudFormation template. For more details, see the quick start option on the GitHub repo. The high-level steps are as follows:Log in to your AWS account.Choose  for your preferred AWS Region:Enter your email address and choose your processing pattern (default is Pattern 1, using Amazon Bedrock Data Automation).Use defaults for all other configuration parameters.The stack takes approximately 15–20 minutes to deploy the resources. After deployment, you will receive an email with login credentials for the web interface.After you deploy the solution, you can start processing documents:Use the web interface to upload a sample document (you can use the provided sample: lending_package.pdf).Select your document from the document list and choose  to watch as your document flows through the pipeline.Examine the extracted data with confidence scores.Use the knowledge base feature to ask questions about processed content.Alternative deployment methodsUpdate an existing GenAI IDP Accelerator stackWhen you’re finished experimenting, clean up your resources by using the AWS CloudFormation console to delete the IDP stack that you deployed.In this post, we discussed the GenAI IDP Accelerator, a new approach to document processing that combines the power of generative AI with the reliability and scale of AWS. You can process hundreds or even millions of documents to achieve better results faster and more cost-effectively than traditional approaches.Visit the GitHub repository for detailed guides and examples and choose  to stay informed on new releases and features. AWS Professional Services and AWS Partners are available to help with implementation. You can also join the GitHub community to contribute improvements and share your experiences. is a Principal Solutions Architect in the AWS Generative AI Innovation Center. is a Senior Data Scientist in the AWS Generative AI Innovation Center. is an Applied Scientist in the AWS Generative AI Innovation Center.is a Senior Deep Learning Architect in the AWS Generative AI Innovation Center.is a Senior Applied Scientist in the AWS Generative AI Innovation Center.is a Senior Cloud Application Architect in the AWS Generative AI Innovation Center.is a Senior Data Scientist in the AWS Generative AI Innovation Center.is a Solutions Architect in the AWS World Wide Public Sector team.We would like to thank Abhi Sharma, Akhil Nooney, Aleksei Iancheruk, Ava Kong, Boyi Xie, Diego Socolinsky, Guillermo Tantachuco, Ilya Marmur, Jared Kramer, Jason Zhang, Jordan Ratner, Mariano Bellagamba, Mark Aiyer, Niharika Jain, Nimish Radia, Shean Sager, Sirajus Salekin, Yingwei Yu, and many others in our expanding community, for their unwavering vision, passion, contributions, and guidance throughout.]]></content:encoded></item><item><title>Amazon SageMaker HyperPod enhances ML infrastructure with scalability and customizability</title><link>https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-hyperpod-enhances-ml-infrastructure-with-scalability-and-customizability/</link><author>Mark Vinciguerra</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 17:14:39 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Amazon SageMaker HyperPod is a purpose-built infrastructure for optimizing foundation model (FM) training and inference at scale. SageMaker HyperPod removes the undifferentiated heavy lifting involved in building and optimizing machine learning (ML) infrastructure for training FMs, reducing training time by up to 40%.SageMaker HyperPod offers persistent clusters with built-in resiliency, while also offering deep infrastructure control by allowing users to SSH into the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances. It helps efficiently scale model development and deployment tasks such as training, fine-tuning, or inference across a cluster of hundreds or thousands of AI accelerators, while reducing the operational heavy lifting involved in managing such clusters. As AI moves towards deployment adopting to a multitude of domains and use cases, the need for flexibility and control is becoming more pertinent. Large enterprises want to make sure the GPU clusters follow the organization-wide policies and security rules. Mission-critical AI/ML workloads often require specialized environments that align with the organization’s software stack and operational standards.SageMaker HyperPod supports Amazon Elastic Kubernetes Service (Amazon EKS) and offers two new features that enhance this control and flexibility to enable production deployment of large-scale ML workloads: – SageMaker HyperPod now supports continuous provisioning, which enhances cluster scalability through features like partial provisioning, rolling updates, concurrent scaling operations, and continuous retries when launching and configuring your HyperPod cluster.– You can now use custom Amazon Machine Images (AMIs), which enables the preconfiguration of software stacks, security agents, and proprietary dependencies that would otherwise require complex post-launch bootstrapping. Customers can create custom AMIs using the HyperPod public AMI as a base and install additional software required to meet their organization’s specific security and compliance requirements.In this post, we dive deeper into each of these features.The new continuous provisioning feature in SageMaker HyperPod represents a transformative advancement for organizations running intensive ML workloads, delivering unprecedented flexibility and operational efficiency that accelerates AI innovation. This feature provides the following benefits: – SageMaker HyperPod prioritizes delivering the maximum possible number of instances without failure. You can start running your workload while your cluster will attempt to provision the remaining instances. – SageMaker HyperPod supports simultaneous scaling and maintenance activities (such as scale up, scale down, and patching) on a single instance group waiting for previous operations to complete. – SageMaker HyperPod persistently attempts to fulfill the user’s request until it encounters a  error from where recovery is not possible.Increased customer visibility – SageMaker HyperPod maps customer-initiated and service-initiated operations to structured activity streams, providing real-time status updates and detailed progress tracking.For ML teams facing tight deadlines and resource constraints, this means dramatically reduced wait times and the ability to begin model training and deployment with whatever computing power is immediately available, while the system works diligently in the background to provision remaining requested resources.Implement continuous provisioning in a SageMaker HyperPod clusterThe architecture introduces an intuitive yet powerful parameter that puts scaling strategy control directly in your hands: . Continuous provisioning maximizes resource utilization and operational agility.The following code creates a cluster with one instance group and continuous provisioning mode enabled using :aws sagemaker create-cluster \ 
--cluster-name $HP_CLUSTER_NAME \
--orchestrator 'Eks={ClusterArn='$EKS_CLUSTER_ARN'}' \
--vpc-config '{
   "SecurityGroupIds": ["'$SECURITY_GROUP'"],
   "Subnets": ["'$SUBNET'"]
}' \
--instance-groups '{
   "InstanceGroupName": "ig-1",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 2,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1
}' \
--node-provisioning-mode Continuous
{
    "ClusterArn": "arn:aws:sagemaker:us-west-2:530295135845:cluster/pv09azbjo6hs"
}Additional features are released with continuous provisioning:Cron job scheduling for instance group software updates:aws sagemaker update-cluster --cluster-name $HP_CLUSTER_NAME \
--instance-groups '[{
   "InstanceGroupName": "group2",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 2,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "ScheduledUpdateConfig": {
      "ScheduleExpression": "cron(30 19 27 * ? *)" # Cron job parameters: cron(Minutes Hours Day-of-month Month Day-of-week Year)
   }
}]' \Rolling updates with safety measures. With rolling deployment, HyperPod gradually shifts traffic from your old fleet to a new fleet. If there is an issue during deployment, it should not affect the whole cluster.aws sagemaker update-cluster --cluster-name $HP_CLUSTER_NAME \
--instance-groups '[{
   "InstanceGroupName": "group4",
   "ScheduledUpdateConfig": {
      "ScheduleExpression": "cron(45 14 25 * ? *)",
      "DeploymentConfig": {
         "AutoRollbackConfiguration": [{
            "AlarmName": "RollbackPatchingAlarm"
         }],
         "RollingUpdatePolicy": {
            "MaximumBatchSize": {
               "Type": "INSTANCE_COUNT",
               "Value": 1
            }
         },
         "WaitIntervalInSeconds": 15
      }
   }
}]'aws sagemaker list-cluster-nodes --cluster-name $HP_CLUSTER_NAMEBatch add nodes (add nodes to specific instance groups):aws sagemaker batch-add-cluster-nodes --cluster-name $HP_CLUSTER_NAME \
--nodes-to-add '[{
   "InstanceGroupName": "group1",
   "IncrementTargetCountBy": 5
}]'Batch delete nodes (remove specific nodes by ID):aws sagemaker batch-delete-cluster-nodes --cluster-name $HP_CLUSTER_NAME \
--node-ids i-0b949a3867b2a963aEnable Training Plan capacity for instance provisioning by adding the  parameter during instance group creation:aws sagemaker update-cluster --cluster-name $HP_CLUSTER_NAME \
--instance-groups '[{
   "InstanceGroupName": "training-group",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 3,
   "TrainingPlanArn": "YOUR_TRAINING_PLAN_ARN"
}]'Cluster event observability:aws sagemake list-cluster-events —cluster-name $HP_CLUSTER_NAMETo reduce operational overhead, nodes in a SageMaker HyperPod cluster are launched with the AWS Deep Learning AMIs (DLAMIs). AWS DLAMIs are pre-built AMIs that are optimized for running deep learning workloads on EC2 instances. They come pre-installed with popular deep learning frameworks, libraries, and tools to make it straightforward to get started with training and deploying deep learning models.The new custom AMI feature of SageMaker HyperPod unlocks even greater value for enterprise customers by delivering the granular control and operational excellence you need to accelerate AI initiatives while maintaining security standards. It seamlessly bridges high-performance computing requirements with enterprise-grade security and operational excellence.Organizations can now build customized AMIs using SageMaker HyperPod performance-tuned public AMIs as a foundation; teams can pre-install security agents, compliance tools, proprietary software, and specialized libraries directly into optimized images.This feature offers the following benefits:It accelerates time-to-value by minimizing runtime installation delays and reducing cluster initialization time through pre-built configurations.From a security standpoint, it enables enterprise-grade centralized control, so security teams can maintain complete oversight while meeting their compliance requirements.Operationally, the feature promotes excellence through standardized, reproducible environments using version-controlled AMIs, while providing seamless integration with existing workflows.The following sections outline a step-by-step approach to build your own AMI and use it on your SageMaker HyperPod cluster.Select and obtain your SageMaker HyperPod base AMIYou can choose from two options to retrieve the SageMaker HyperPod base AMI. To use the Amazon EC2 console, complete the following steps:On the Amazon EC2 console, choose  under  in the navigation pane.Choose  as the image type and set the  filter to .Search for AMIs prefixed with .Choose the appropriate AMI (preferably the latest).aws ssm get-parameter \
  --name "/aws/service/sagemaker-hyperpod/ami/x86_64/eks-1.31-amazon-linux-2/latest/ami-id" \
  --region us-west-2 \
  --query "Parameter.Value" \
  --output text

// Replace the parameter name with corresponding kubernetes version as required.
// For example, If you want to use kubernetes 1.30, use the following parameterAfter you select a SageMaker HyperPod public AMI, use that as the base AMI to build your own custom AMI using one of the following methods. This is not an exhaustive list for building AMIs; you can use your preferred method. SageMaker HyperPod does not have any strong recommendations.– Choose your customized EC2 instance, then choose , , . – Packer is an open source tool from HashiCorp that you can use to create identical machine images for multiple platforms from a single source configuration. It supports creating AMIs for AWS, as well as images for other cloud providers and virtualization platforms.– EC2 Image Builder is a fully managed AWS service that makes it straightforward to automate the creation, maintenance, validation, sharing, and deployment of Linux or Windows Server images.Set up the required permissionsBefore you start using custom AMIs, confirm you have the required AWS Identity and Access Management (IAM) policies configured. Make sure you add the following policies to your  user permissions (IAM policy):# Minimum set of permissions for admin to run the HyperPod core APIs
"sagemaker:CreateCluster",
"sagemaker:DeleteCluster",
"sagemaker:DescribeCluster",
"sagemaker:DescribeCluterNode",
"sagemaker:ListClusterNodes",
"sagemaker:ListClusters",
"sagemaker:UpdateCluster",
"sagemaker:UpdateClusterSoftware",
"sagemaker:BatchDeleteClusterNodes",
"eks:DescribeCluster",
"eks:CreateAccessEntry",
"eks:DescribeAccessEntry",
"eks:DeleteAccessEntry",
"eks:AssociateAccessPolicy",
"iam:CreateServiceLinkedRole",

# Permissions required to manage HyperPod clusters with custom AMI
"ec2:DescribeImages",
"ec2:ModifyImageAttribute",
"ec2:modifySnapshotAttribute",
"ec2:DescribeSnapshots"Run cluster management operationsTo create a cluster with a custom AMI, use the aws sagemaker create-cluster command. Specify your custom AMI in the  parameter, and include other required cluster configurations:aws sagemaker create-cluster \
   --cluster-name clusterNameHere \
   --orchestrator 'Eks={ClusterArn='$EKS_CLUSTER_ARN'}' \
   --node-provisioning-mode Continuous \
   --instance-groups '{
   "InstanceGroupName": "groupNameHere",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 2,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ImageId: "<YOUR_CUSTOM_AMI>,
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "InstanceStorageConfigs": [
        {
            "EbsVolumeConfig": {
                "VolumeSizeInGB": 500,
            }
        }
   ]
}' --vpc-config '{
   "SecurityGroupIds": ["'$SECURITY_GROUP'"],
   "Subnets": ["'$SUBNET'"]
}'Scale up an instance group with the following code:aws sagemaker update-cluster \
    --cluster-name $HP_CLUSTER_NAME --instance-groups '[{                  
    "InstanceGroupName": "groupNameHere",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 10,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "ImageId: "<YOUR_CUSTOM_AMI>,
}]'Add an instance group with the following code:aws sagemaker update-cluster \
   --cluster-name "clusterNameHere" \
   --instance-groups '{
   "InstanceGroupName": "groupNameHere",
   "InstanceType": "ml.p6-b200.48xlarge",
   "InstanceCount": 10,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "ImageId: "<YOUR_CUSTOM_AMI>,
}' '{
   "InstanceGroupName": "groupNameHere2",
   "InstanceType": "ml.c5.2xlarge",
   "InstanceCount": 1,
   "LifeCycleConfig": {
      "SourceS3Uri": "s3://'$BUCKET_NAME'",
      "OnCreate": "on_create.sh"
   },
   "ExecutionRole": "'$EXECUTION_ROLE'",
   "ThreadsPerCore": 1,
   "ImageId: "<YOUR_CUSTOM_AMI_2>,
}'When using custom AMIs with your cluster, be aware of the following requirements and limitations: – Custom AMIs must contain only the root snapshot. Additional snapshots are not supported and will cause cluster creation or update operations to fail with a validation exception if the AMI contains additional snapshots beyond the root volume. –  in  is immutable. For patching existing instance groups, you must use UpdateClusterSoftware with .AMI versions and deprecation – The public AMI releases page talks about the public AMI versions and deprecation status. Customers are expected to monitor this page for AMI vulnerabilities and deprecation status and patch cluster with updated custom AMI.To clean up your resources to avoid incurring more charges, complete the following steps:In this post, we introduced three features in SageMaker HyperPod that enhance scalability and customizability for ML infrastructure. Continuous provisioning offers flexible resource provisioning to help you start training and deploying your models faster and manage your cluster more efficiently. With custom AMIs, you can align your ML environments with organizational security standards and software requirements. To learn more about these features, see: is an Associate Specialist Solutions Architect at Amazon Web Services (AWS) based in New York. He focuses on Generative AI training and inference, with the goal of helping customers architect, optimize, and scale their workloads across various AWS services. Prior to AWS, he went to Boston University and graduated with a degree in Computer Engineering. You can connect with him on LinkedIn. is a Sr GTM Specialist at Amazon Web Services (AWS) focusing on generative AI model training and inference. He partners with top frontier model builders, strategic customers, and AWS service teams to enable distributed training and inference at scale on AWS and lead joint GTM motions. Before AWS, Anoop held several leadership roles at startups and large corporations, primarily focusing on silicon and system architecture of AI infrastructure.Monidipa Chakraborty currently serves as a Senior Software Development Engineer at Amazon Web Services (AWS), specifically within the SageMaker HyperPod team. She is committed to assisting customers by designing and implementing robust and scalable systems that demonstrate operational excellence. Bringing nearly a decade of software development experience, Monidipa has contributed to various sectors within Amazon, including Video, Retail, Amazon Go, and AWS SageMaker. is a Sr Technical Account Manager & Enterprise Support Lead at Amazon Web Services (AWS), specializing in driving generative AI and supporting startups through enterprise-wide cloud transformations. He focuses on adopting AI services within AWS and aligning technology strategies with business objectives to achieve impactful results. is a technical leader at AWS, working on machine learning infrastructure that enables large-scale training and inference workloads. He has contributed to multiple AWS services and is proficient in various AWS technologies, with expertise in distributed systems, Kubernetes, and cloud-native architecture. Passionate about building reliable, customer-focused solutions, he specializes in transforming complex technical challenges into simple, robust systems that scale globally. is a Principal Product Manager at AWS, where he focuses on building Amazon SageMaker HyperPod to enable scalable distributed training and fine-tuning of foundation models. In his spare time, Kunal enjoys skiing and exploring the Pacific Northwest. You can connect with him on LinkedIn. is an engineering leader at AWS, working on the HyperPod team focused on improving infrastructure for machine learning training/inference jobs. He has contributed to core AWS services like EC2, ECS, Fargate, and SageMaker partner AI apps. With a background in distributed systems, he focuses on building reliable and scalable solutions across teams.]]></content:encoded></item><item><title>Data Science Path: Automatic Subclass Registration &amp; Python Encryption Algorithms with LabEx</title><link>https://dev.to/labex/data-science-path-automatic-subclass-registration-python-encryption-algorithms-with-labex-9f9</link><author>Labby</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 17:02:21 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Embarking on a data science journey can feel daunting, but what if you could start with engaging, bite-sized challenges that build your skills step by step? The LabEx 'Data Science' path is designed precisely for this, offering a structured roadmap through hands-on, interactive lessons. Forget passive video lectures; here, you learn by doing, mastering essential concepts from statistical analysis to machine learning and data visualization. Let's explore a few beginner-friendly experiments that will kickstart your transformation from novice to data wizard.
  
  
  Automatic Registration of Subclasses
 Beginner |  5 minutesIn this challenge, we will implement a class called Base that will automatically record any subclasses that inherit from it. The purpose of this implementation is to enable the retrieval of all subclass names by iterating over Base. The goal is to demonstrate the functionality of Base by showing that it correctly registers and outputs the names of the subclasses. We will accomplish this by implementing the  method in the Base class and ensuring that it supports iteration.
  
  
  Implementing Column Permutation Encryption in Python
 Beginner |  5 minutesIn this challenge, we will be implementing the Column Permutation Encryption method. This method involves encrypting a plaintext by writing it down line by line with a fixed number of characters per line, and then rearranging the columns of the resulting matrix according to the alphabetical order of a key. The rearranged columns are then read out one by one to obtain the ciphertext. The objective of the challenge is to complete the column_permutation_encryption(text) function in the given file, which takes a piece of text as input, performs column permutation encryption using the key qiao and the padding character ,, and returns the ciphertext. If the input text is empty, None should be returned.
  
  
  Implementing Affine Encryption in Python
 Beginner |  5 minutesIn this challenge, we will implement the Affine encryption algorithm. The Affine cipher is a substitution cipher that combines the characteristics of the shift cipher and the multiplier cipher. It uses a cryptographic function to encrypt one letter per letter based on a mathematical formula. The objective is to complete the implementation of the affine_encryption(text) function in the affine.py file, which takes a piece of text as input, encrypts it using the Affine cipher, and returns the ciphertext.
  
  
  Count Each Type Characters
 Beginner |  5 minutesIn this challenge, we will count the number of letters, spaces, digits, and other characters in a given input. The objective is to correctly categorize and count each type of character. For example, given the input 'abc123EFG *&45?', the expected output would be 'letter=6,space=1,digit=5,other=3'.These beginner-friendly challenges are just the beginning of your data science adventure. Each one is designed to build foundational skills, from understanding object-oriented principles to mastering data manipulation and even delving into the fascinating world of cryptography. Dive in, experiment, and watch your data science capabilities grow with LabEx's interactive learning environment. Your journey to becoming a data science pro starts here!]]></content:encoded></item><item><title>Cracking the Density Code: Why MAF Flows Where KDE Stalls</title><link>https://towardsdatascience.com/cracking-the-density-code-why-maf-flows-where-kde-stalls/</link><author>Zackary Nay</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 16:48:34 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Learn why autoregressive flows are the superior density estimation tool for high-dimensional data]]></content:encoded></item><item><title>🚀 Learn Go with 13 Challenges: a practical journey to mastering the language</title><link>https://dev.to/kid_goth/learn-go-with-13-challenges-a-practical-journey-to-mastering-the-language-240</link><author>Brandon Sanchez</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 22 Aug 2025 16:42:34 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hi all, I'm a web a mobile developer who loves to code and the real tech challenges. I've wanted to learn Go for a long time, but not in the traditional way, but building specific stuffs that allow me (and us) to learn with purpose. That is how this project born: Learn Go with 13 Challenges.
  
  
  🧩 ¿What is this project about?
This is a practical journey through Go, focused in not only read documentation without stop, but solve little and powerful challenges. Each challenge is a mini-project desgined to introduce ann confidence key concepts of the language, from the most basic to the advanced things.✔ Each challenge is already prepared with with their respective tests (using TDD-type approach)
✔ In each post we will to write the necessary code to pass all tests cases and in consequence we will develop the mini-project.
✔ I will explain step by step the rasoning, design, problems and the final solution, without neglecting how to draw on sources of knowledge, (documentation, videos, forums, etc)
✔ All is in a public repository and you are free to clone, try, test and improveYou can follow the progress directly at the GIT repository:There you will find the 13 challenges listed by difficulty and organized into sub-directories, with their tests prepared and ready for you to tackle if you want to join in.
  
  
  🗓 How often is it published?
I will publish each post progressively. I can't give a specific time frame, but I will try to do it weekly. My goal is to do it consistently and sustainably. It's not “Go in 13 weeks” or “Go in 13 months”, but “” — at your pace and mine.Considering that each mind learns differently and/or has preferences when following manuals and/or procedures, each delivery will come in two formats:📄 A written post like this, explaining the solution step by step📹 A YouTube video with the procedure recorded and commented; please note, I am not an expert in videos, but I will try to make sure they are of the highest quality and of a reasonable length for each exercise.
  
  
  🌍 What about the language?
I'm publishing first in spanish, but I'm planning to post the solution of each challenge in english after the spanish version is published. This way, I can contribute to both the spanish-speaking community and the global community.Regarding the videos, I will find a way to provide English subtitles for them.Because I firmly believe that learning by solving real problems is the best way to master a language. Because Go has enormous potential for services, CLI, backend tools, APIs, and more. And because building is more fun than memorizing.
  
  
  ✅ What will we see in the challenges?
Even more ambitious things like:Each challenge has something new to offer, and seeks to exploit one (or more) interesting features of the language.You can follow me in Dev.to, or suscribe to my youtube channel if you want to see the process in video format.I am open to suggestions, ideas, improvements, and collaborations. This is a project for learning, sharing, and growing together.See you soon for the first challenge: the calculator 🧮Come on to learn Go in the best way: building.]]></content:encoded></item><item><title>Rodrigo Girão Serrão: TIL #130 – Format Python code directly with uv</title><link>https://mathspp.com/blog/til/format-python-code-directly-with-uv</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 16:34:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Today I learned you can format your Python code directly with uv.In uv version 0.8.13, released one or two days ago, uv added the command  that allows you to format your Python code directly through the uv CLI.First and foremost, make sure you're rocking uv 0.8.13 or greater by running .To format your code with uv you can simply run , which will use Ruff to format the code in your current directory:The idea is not to have uv replace Ruff; it's just so that you don't have to think about a separate tool if you don't want to. accepts the same arguments and options that  accepts, so you'll want to check the Ruff docs to learn more.
My favourite option is , to take a look at the formatting diff without doing any formatting changes.As of now, the feature is marked as being experimental, which means it might change in the future!]]></content:encoded></item><item><title>Show HN: Clyp – Clipboard Manager for Linux</title><link>https://github.com/murat-cileli/clyp</link><author>timeoperator</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 16:03:26 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Predictive Analytics in Healthcare: Improving Patient Outcomes</title><link>https://www.kdnuggets.com/predictive-analytics-in-healthcare-improving-patient-outcomes</link><author>Shittu Olumide</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-olumide-predictive-analytics-healthcare-improving-patient-outcomes-1.png" length="" type=""/><pubDate>Fri, 22 Aug 2025 16:00:04 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Predictive analytics in healthcare is revolutionizing patient care by using AI and machine learning to forecast health outcomes and optimize treatment plans.]]></content:encoded></item><item><title>Build a RAG application with LangChain and Local LLMs powered by Ollama</title><link>https://dev.to/abhirockzz/build-a-rag-application-with-langchain-and-local-llms-powered-by-ollama-3el5</link><author>Abhishek Gupta</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 14:12:59 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Local large language models (LLMs) provide significant advantages for developers and organizations. Key benefits include enhanced , as sensitive information remains entirely within your own infrastructure, and , enabling uninterrupted work even without internet access. While cloud-based LLM services are convenient, running models locally gives you full control over model behavior, performance tuning, and potential cost savings. This make them ideal for experimentation before running production workloads.The ecosystem for local LLMs has matured significantly, with several excellent options available, such as Ollama, Foundry Local, Docker Model Runner, and more. Most popular AI/Agent frameworks including LangChain and LangGraph provide integration with these local model runners, making it easier to integrate them into your projects.This blog post will illustrate how to use local LLMs with Azure Cosmos DB as a vector database for retrieval-augmented generation (RAG) scenarios. It will guide you through setting up a local LLM solution, configuring Azure Cosmos DB, loading data, performing vector searches, and executing RAG queries. You can either use the Azure Cosmos DB emulator for local development or connecting to an Azure Cosmos DB account in the cloud. You will be using Ollama (open-source solution) to run LLMs locally on your own machine. It lets you download, run, and interact with a variety of LLMs (like Llama 3, Mistral, and others) using simple commands, without needing cloud access or complex setup.By the end of this blog post, you will have a working local RAG setup that leverages Ollama and Azure Cosmos DB. the sample app uses LangChain integration with Azure Cosmos DB to perform embedding, data loading, and vector search. You can easily adapt it to other frameworks like LlamaIndex.To get started with Ollama, follow the official installation guide on GitHub to install it on your system. The installation process is straightforward across different platforms. For example, on Linux systems, you can install Ollama with a single command:curl  https://ollama.com/install.sh | sh
Once installed, start the Ollama service by running:This blog post demonstrates the integration using two specific models from the Ollama library: - A high-quality embedding model with 1024 dimensions, ideal for generating vector representations of text - The 8B parameter variant of Meta's Llama 3, which serves as our chat model for the RAG pipelineDownload both models using the following commands. Note that this process may take several minutes depending on your internet connection speed, as these are substantial model files:ollama pull mxbai-embed-large
ollama pull llama3:8b

  
  
  Something to keep in mind ...
While tools like Ollama make it straightforward to run local LLMs, hardware requirements depend on the specific model and your performance expectations. Lightweight models (such as Llama 2 7B or Phi-2) can run on modern CPUs with as little as 8 GB RAM, though performance may be limited. Larger models (like Llama 3 70B or Mixtral) typically require a dedicated GPU with at least 16 GB VRAM for efficient inference. Ollama supports both CPU and GPU execution. On CPU-only systems, you can expect slower response times, especially with larger models or concurrent requests. Using a compatible GPU significantly accelerates inference required for demanding workloads.Since you're working with local models, you'll likely want to use the Azure Cosmos DB emulator for local development. The emulator provides a local environment that mimics the Azure Cosmos DB service, enabling you to develop and test your applications without incurring costs or requiring an internet connection.The emulator is available as a Docker container, which is the recommended way to run it. Here are the steps to pull and start the Cosmos DB emulator. The commands shown are for Linux - refer to the documentation for other platform options.docker pull mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:latest

docker run  8081:8081 1 mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:latest
curl  https://localhost:8081/_explorer/emulator.pem  ~/emulatorcert.crt
update-ca-certificates
You should see output similar to this:Updating certificates in /etc/ssl/certs...
rehash: warning: skipping ca-certificates.crt,it does not contain exactly one certificate or CRL
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d...
done.

  
  
  Load data into Azure Cosmos DB
Now that both Ollama and Azure Cosmos DB are set up, it's time to populate our vector database with some sample data. For this demonstration, we'll use Azure Cosmos DB's own documentation as our data source. The loader will fetch markdown content directly from the Microsoft Docs repository, specifically focusing on articles about Azure Cosmos DB vector search functionality.Our data loading process will read these documentation articles, generate embeddings using the  model, and store both the content and vector representations in Azure Cosmos DB for retrieval.Begin by cloning the GitHub repository containing the sample application:git clone https://github.com/abhirockzz/local-llms-rag-cosmosdb
local-llms-rag-cosmosdb
Before running the loader application, ensure you have Python 3 installed on your system. Create a virtual environment and install the required dependencies:python3  venv .venv
 .venv/bin/activate

pip3  requirements.txt
Next, configure the environment variables and execute the loading script. The example below uses the Azure Cosmos DB emulator for local development. If you prefer to use the cloud service instead, simply set the  variable to your Azure Cosmos DB account URL and remove the  variable.

python3 load_data.py
The script will automatically create the database and container if they don't already exist. Once the data loading process completes successfully, you should see output similar to this:Uploading documents to Azure Cosmos DB ['https://raw.githubusercontent.com/MicrosoftDocs/azure-databases-docs/refs/heads/main/articles/cosmos-db/nosql/vector-search.md', 'https://raw.githubusercontent.com/MicrosoftDocs/azure-databases-docs/refs/heads/main/articles/cosmos-db/nosql/multi-tenancy-vector-search.md']
Using database: rag_local_llm_db, container: docs
Using embedding model: mxbai-embed-large with dimensions: 1024
Created instance of AzureCosmosDBNoSqlVectorSearch
Loading 26 document chunks from 2 documents
Data loaded into Azure Cosmos DB
To confirm that your data has been loaded successfully, you can inspect the results using the Azure Cosmos DB Data Explorer. If you're using the emulator, navigate to https://localhost:8081/_explorer/index.html in your browser. You should see the same number of documents in your container as the number of chunks reported by the loader application.
  
  
  Run vector search queries
Now that your data is loaded, let's test the vector search functionality. Set the same environment variables used for data loading and run the vector search script with your desired query:

python3 vector_search.py The script will process your query through the embedding model and perform a similarity search against the stored document vectors. You should see output similar to the following:Searching top 5 results for query: "show me an example of a vector embedding policy"

Using database: rag_local_llm_db, container: docs
Using embedding model: mxbai-embed-large with dimensions: 1024
Created instance of AzureCosmosDBNoSqlVectorSearch
Score: 0.7437641827298191
Content: ```



### A policy with two vector paths
//....


The output shows the top five results ordered by their similarity scores, with higher scores indicating better matches to your query.To modify the number of results returned, you can add the  argument. For example, to retrieve the top 10 results, run: python3 vector_search.py "show me an example of a vector embedding policy" 10
  
  
  Execute Retrieval-Augmented Generation (RAG) queries
Now we will put it all together with an simple chat based interface that leverages the  model to generate responses based on the contextual information retrieved from Azure Cosmos DB.Configure the environment variables needed for the RAG application and launch the script:
bash
# export COSMOS_DB_URL="https://<Cosmos DB account name>.documents.azure.com:443/"
export USE_EMULATOR="true"
export DATABASE_NAME="rag_local_llm_db"
export CONTAINER_NAME="docs"
export EMBEDDINGS_MODEL="mxbai-embed-large"
export DIMENSIONS="1024"
export CHAT_MODEL="llama3"

python3 rag_chain.py


Once the application initializes, you'll see output confirming the RAG chain setup:
text
Building RAG chain. Using model: llama3
Using database: rag_local_llm_db, container: docs
Using embedding model: mxbai-embed-large with dimensions: 1024
Created instance of AzureCosmosDBNoSqlVectorSearch
Enter your questions below. Type 'exit' to quit, 'clear' to clear chat history, 'history' to view chat history.
[User]:


Ask questions about the Azure Cosmos DB vector search documentation that you've loaded. For instance, try asking show me an example of a vector embedding policy, and you'll see a response like this (note that these may vary slightly for your case, even across different runs):
text
//...
[User]: show me an example of a vector embedding policy
[Assistant]: Here is an example of a vector embedding policy:

{
    "vectorEmbeddings": [
        {
            "path":"/vector1",
            "dataType":"float32",
            "distanceFunction":"cosine",
            "dimensions":1536
        },
        {
            "path":"/vector2",
            "dataType":"int8",
            "distanceFunction":"dotproduct",
            "dimensions":100
        }
    ]
}

This policy defines two vector embeddings: one with the path `/vector1`, using `float32` data type, cosine distance function, and having 1536 dimensions; and another with the path `/vector2`, using `int8` data type, dot product distance function, and having 100 dimensions.


To further explore the capabilities of your RAG system, try these additional example queries:"What is the maximum supported dimension for vector embeddings in Azure Cosmos DB?""Is it suitable for large scale data?""Is there a benefit to using the flat index type?"You can enter 'exit' to quit the application, 'clear' to clear chat history, or 'history' to view your previous interactions. Feel free to experiment with different data sources and queries. To modify the number of vector search results used as context, you can add the  environment variable (defaults to 5).In this walkthrough, you followed step-by-step instructions to set up a complete RAG application that runs entirely on your local infrastructure — from installing and configuring Ollama with embedding and chat models, to setting up Azure Cosmos DB for vector storage, loading documentation data, and running using RAG through an interactive chat interface.Running models locally brings clear advantages in terms of costs, data privacy, and     connectivity constraints. However, you need to plan for appropriate hardware, particularly for larger models that perform best with dedicated GPUs and sufficient memory. The trade-off between model size, performance, and resource requirements is crucial when planning your local AI setup.Have you experimented with local LLMs in your projects? What challenges or benefits have you encountered when moving from cloud-based to local AI solutions? Perhaps you have used both approaches? Share your experience and feedback!]]></content:encoded></item><item><title>How I Built a Screenshot Mover With Python</title><link>https://dev.to/fran_panteli/how-i-built-a-screenshot-mover-with-python-14i6</link><author>Francesca Panteli</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 14:06:57 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As part of my Python Web Development Career Track with CodingNomads, I implemented a Python script to automate the organisation of files in a folder. Specifically, the script moves .png files from a general folder into a dedicated subfolder, reducing manual file management.This project demonstrates the use of Python fundamentals such as path manipulation, iteration, conditional logic, and basic filesystem operations using the pathlib module.This document provides a structured walkthrough of the project, including:Project concept and requirementsCode walk-through with explanationsThe script solves a common problem: managing mixed file types in a single directory. Manually sorting files by type can be tedious, especially when dealing with large numbers of files.A base directory contains multiple file types (.pdf, .txt, .png)A new subfolder, png_files, is created to store .png filesThe script iterates through the files in the base directory and moves only .png filesFiles of other types remain untouchedThis approach provides a practical environment for practicing path manipulation, conditional filtering, and file operations in Python.The directory tree for this project is as follows:.
├── mover.py
├── example.pdf
└── png_files
├── example_three.png directory containing files to be processed destination subfolder for .png filesThe program is implemented as a single Python script. The following sections describe the components of this.This introduces the  module, which provides an object-oriented interface for filesystem paths.  objects are used for path construction, iteration, and manipulation.Defining the Target Directory specifies the folder containing files to be organised. Using  objects allows clean and cross-platform path handling.A subfolder  is created to store the  files. The parameter  prevents an error if the folder already exists. This ensures the script can safely run multiple times without issues.Iterating and Filtering Filesfolder_directory.iterdir() iterates over all files in the folder checks the file extension files are moved to the  subfolder using file.rename(new_file_path)Other files (, , etc.) remain untouchedBefore running the script, the  folder contains mixed file types. After executing , all  files are automatically relocated into . This automation removes the need for manual organisation and provides a reproducible workflow.This project reinforced several Python programming concepts:Pathlib and Path Objects: a robust way to navigate and manipulate file paths looping over directory contents using  selecting files based on their extension moving files using  applying Python scripts to streamline repetitive tasksAlthough functional, the script can be extended in several ways: use  to allow dynamic folder and file type input add checks for missing folders, permission issues, or filename conflicts maintain a record of moved files for auditing purposes extend functionality to organise , , , etc wrap functionality in functions or classes for reuse in larger projects]]></content:encoded></item><item><title>Shortcuts for the Long Run: Automated Workflows for Aspiring Data Engineers</title><link>https://www.kdnuggets.com/shortcuts-for-the-long-run-automated-workflows-for-aspiring-data-engineers</link><author>Bala Priya C</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/bala-dataengg-worklfows.jpeg" length="" type=""/><pubDate>Fri, 22 Aug 2025 14:00:35 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Tired of repeating the same data tasks? Automate them. This article shows beginners how to build efficient, low-maintenance data engineering workflows that pay off in the long run.]]></content:encoded></item><item><title>Taming GORM &amp; sqlmock: Our Go-To Workflow for Perfect Database Mocks</title><link>https://dev.to/crow004/taming-gorm-sqlmock-our-go-to-workflow-for-perfect-database-mocks-ndm</link><author>crow</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 22 Aug 2025 13:24:40 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA['A simple, iterative workflow to debug sqlmock expectation errors when testing GORM by using its built-in logger to reveal the exact SQL queries.'When writing tests for our Go backend, we rely heavily on  to ensure our database logic is solid without hitting a real database. It's a fantastic tool, but it has one strict requirement: your mock expectations must  match the SQL queries your code generates.This can get tricky when using an ORM like GORM. GORM is great for productivity, but the SQL it generates under the hood—especially for complex operations like  with multiple index creations—isn't always obvious. We found ourselves in a cycle of "guess, run, fail, repeat."So, how do you find out the  SQL GORM is trying to run?We've settled on a simple, iterative debugging workflow that turns this guessing game into a straightforward process. Here’s how we do it.
  
  
  The Challenge: Unpredictable SQL
The core problem is that  fails if the expected query string doesn't match the actual query string. With GORM's , for example, the order in which it decides to create tables and indexes can change as you add new models or even between GORM versions. You might expect  before , but GORM does the opposite, and your test fails with a cryptic message.
  
  
  The Technique: Let the ORM Tell You What It's Doing
Instead of trying to guess the SQL, we make GORM tell us directly. The key is its built-in logger.Here's our step-by-step process:
  
  
  Step 1: Isolate the Failing Test
Run your tests and find the first  expectation that fails. The error message is your starting point. It will usually say something like:
Error: call to ExecQuery '...' with args [...] was not expected, next expectation is: ...
  
  
  Step 2: Enable Verbose Logging
In your test setup where you initialize your GORM connection with the mocked SQL connection, temporarily switch the GORM logger to  mode.// In your test file...
import "gorm.io/gorm/logger"

// ...

// Temporarily change logger.Silent to logger.Info
gormDB, err := gorm.Open(dialector, &gorm.Config{ Logger: logger.Default.LogMode(logger.Info), })
  
  
  Step 3: Run the Test and Observe
Run the single failing test again (e.g., go test -run TestMyFailingTest). Now, look at your console output. Because the logger is in  mode, GORM will print the exact SQL query it's generating, right before  reports the failure.The output will look something like this:[info] /path/to/your/code.go:123 [SQL] CREATE INDEX "idx_commission_withdrawals_timestamp" ON "commission_withdrawals" ("timestamp") ... [error] ExecQuery: could not match actual sql: "CREATE INDEX..." with expected regexp "CREATE INDEX...recipient_address..."
  
  
  Step 4: Copy, Paste, and Adapt
The "actual sql" from the log is the source of truth. the SQL query from the log output. it into your test file, replacing or reordering the incorrect expectation in  or . it for . You might need to escape special characters for the regex matcher (like parentheses  and ).Your test will now pass the first expectation and likely fail on the next one in the sequence. That's progress! Just repeat steps 3 and 4 for the new failing expectation until the entire test passes.Once the test is green, remember to switch the GORM logger back to  to keep your test logs clean for everyone else.// Change it back for clean test runs
gormDB, err := gorm.Open(dialector, &gorm.Config{ Logger: logger.Default.LogMode(logger.Silent), })This simple, iterative process has saved us countless hours of frustration. By using the ORM's own logging, we get a definitive answer to "What query are you  running?" and can write precise, reliable database tests.Hope this helps you in your projects!]]></content:encoded></item><item><title>First Institute of Reliable Software: Best Code Rule: Always Separate Input, Output, and Processing</title><link>https://first.institute/en/blog/always-separate-input-output-and-processing/?utm_source=rss&amp;utm_medium=feed&amp;utm_campaign=blog&amp;utm_content=en</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 13:22:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Stop writing glue-code scripts. Discover how one simple principle — separating input, output, and processing — transforms messy Python into professional-grade software.]]></content:encoded></item><item><title>🚀 Learn Python from Zero to Hero on Telegram!</title><link>https://dev.to/armin_cooper_b440db9cd3bd/learn-python-from-zero-to-hero-on-telegram-3fc</link><author>Armin Cooper</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 13:11:08 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[🚀 Learn Python from Zero to Hero on Telegram!Want to master Python from scratch without feeling lost? Join https://t.me/Python_1st– the ultimate Telegram channel for step-by-step Python learning!🔹 Beginner to advanced tutorials
🔹 Hands-on projects for real-world practice
🔹 Practical tips and resources to boost your skillsStart your programming journey the simplest and most effective way.]]></content:encoded></item><item><title>How I Built a Dungeons and Dragons Game With Python</title><link>https://dev.to/fran_panteli/test-article-lig</link><author>Francesca Panteli</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 12:35:28 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Building a Text-Based Dungeons & Dragons Game in PythonAs part of my Python Web Development Career Track with CodingNomads, I implemented a text-based adventure game inspired by Dungeons & Dragons. The objective of the project was to strengthen my understanding of Python fundamentals, particularly user input, conditionals, variables, and control flow.This document provides a structured walkthrough of the project, including:Project concept and requirementsCode walk-through with explanationsThe game simulates a basic dungeon exploration scenario where the player must choose between two doors. Depending on their choices, they may encounter a sword, face a dragon, or be defeated.User enters a name and is welcomed to the gameThe player selects a door (“left” or “right”)If the player explores and retrieves a sword, they can defeat the dragonIf the player encounters the dragon without the sword, they loseThe program is implemented as a single Python script. The following sections describe the major components.1. User Input and Greetinginput() for collecting player inputString concatenation to personalise outputThis illustrates branching logic using if statements to create different outcomes.3. Returning or ExploringThis provides additional decision points and demonstrates nested user interactions.The program tracks whether the player acquires a sword. This introduces state management through variables.The Boolean variable can_fight_dragon is set when the sword is collected. This variable functions as the win condition.This project reinforced several Python programming fundamentals:User Input Handling: Capturing and processing text-based commandsConditional Statements: Implementing branching logic with if statementsBoolean State: Using variables (can_fight_dragon) to track game progressControl Flow: Designing a logical sequence of eventsThe current version is functional but linear. Possible enhancements include:Adding multiple rooms and branching narrativesIntroducing health points (HP) and combat mechanicsImplementing an inventory systemRefactoring code with functions for modularityAdding loops to allow replayability without restartingConverting the CLI-based game into a web application using Flask or DjangoDeveloping this project provided hands-on experience with Python’s foundational concepts in a practical, engaging way. Though simple, the program effectively demonstrates how user input, conditionals, and state management can be combined to create interactive applications.Future iterations of this project could expand into more complex game mechanics or web-based interfaces, offering opportunities to apply advanced Python concepts.]]></content:encoded></item><item><title>From Hashes to Signatures: Securing File Transfers with RSA/ECDSA Digital Signatures</title><link>https://dev.to/aditya_r_e0eab9ccef0d1122/from-hashes-to-signatures-securing-file-transfers-with-rsaecdsa-digital-signatures-6im</link><author>Aditya R</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 12:30:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the first two parts of this series, I explored how to secure file transfers using SHA-256 checksums for integrity and then took it a step further with HMAC-SHA256, which added authenticity through a shared secret key. These approaches work well in trusted environments, especially for internal or on-prem systems.But what happens when the systems are not in the same secure network, or when you need to ensure that even without a shared secret, the file’s integrity and the sender’s identity can be verified? That’s where Digital Signatures come into play.Digital signatures, built on algorithms like RSA (Rivest–Shamir–Adleman) and ECDSA (Elliptic Curve Digital Signature Algorithm), bring two powerful guarantees:Integrity — ensuring the file hasn’t been tampered with.Authenticity — proving that the file truly came from the claimed sender.In this part, I’ll explore how digital signatures fit into secure file transfers, compare RSA and ECDSA, and walk through generating and verifying signatures with code examples.
  
  
  📌 What Are Digital Signatures?
A digital signature is like a virtual fingerprint for a file.It ensures that the file has not been tampered with (integrity).It ensures that the file truly comes from the claimed sender (authenticity).It works using a private key (to sign) and a public key (to verify).
  
  
  ⚙️ How It Works (Step-by-Step)
Sender generates a hash of the file (e.g., SHA-256).Sender encrypts the hash with their private key → digital signature.The file + signature are sent to the receiver.Receiver generates their own hash of the received file.Receiver decrypts the signature using sender’s public key to retrieve the original hash.If both hashes match → the file is authentic and untampered.
  
  
  🔐 How to Generate Key Pairs
To use digital signatures, you need a key pair:Private Key (kept secret, used for signing).Public Key (shared, used for verifying).There are many ways to generate the key pairs. The common and straightforward way is to use the openssl library. Here I provide the Python way.
  
  
  🔑 Generating RSA Key Pairs
# Generate RSA Public-Private Key
def generate_rsa_key(private_key_file, public_key_file):
    private_key = rsa.generate_private_key(public_exponent=65537, key_size=2048)

    # Save Private Key
    with open(private_key_file, "wb") as fout:
        fout.write(private_key.private_bytes(
            encoding=serialization.Encoding.PEM,             # Format = PEM
            format=serialization.PrivateFormat.TraditionalOpenSSL,  # Structure - OpenSSL style
            encryption_algorithm=serialization.NoEncryption()  # No password protection
        ))

    # Save Public Key
    public_key = private_key.public_key()
    with open(public_key_file, "wb") as fout:
        fout.write(public_key.public_bytes(
            encoding=serialization.Encoding.PEM,        # Format = PEM
            format=serialization.PublicFormat.SubjectPublicKeyInfo # Standard X.509 format
        ))

    print("RSA key generation complete")

  
  
  🔑 Generating ECDSA Key Pairs
# Generate ECDSA Key Pair
def generate_ec_key(private_key_file, public_key_file):

    # Generate ECDSA Private Key
    private_key = ec.generate_private_key(ec.SECP256R1()) # Specifies which Elliptic Curve to use 
                          # Uses the curve known as prime256v1 or NIST P-256.

    # Save Private Key
    with open(private_key_file, "wb") as fout:
        fout.write(private_key.private_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PrivateFormat.TraditionalOpenSSL,
            encryption_algorithm=serialization.NoEncryption()
        ))

    # Save Public Key
    public_key = private_key.public_key()

    with open(public_key_file, "wb") as fout:
        fout.write(public_key.public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo
        ))

    print("EC key generation complete")

  
  
  ✅ RSA vs ECDSA Quick Note
RSA → Widely used, mature, simpler to understand, but keys/signatures are larger.ECDSA → Faster, smaller keys, but more complex math. Popular in modern systems (TLS, blockchain).A comparison table of RSA vs ECDSA is provided below for information.Once the Key Pairs are generated and saved, the next step is to generate the Digital Signature.def generate_digital_signature(private_key_file, file_path, signature_file_path):

    # Load File Content
    with open(file_path, "rb") as fin:
        data = fin.read()

    # Read the Private Key from pem file
    with open(private_key_file, "rb") as fout:
        private_key = serialization.load_pem_private_key(
            fout.read(),
            password=None
        )

    # Sign the Data
    signature = private_key.sign(
        data,
        padding.PSS(
            mgf=padding.MGF1(algorithm=hashes.SHA256()),
            salt_length=padding.PSS.MAX_LENGTH
        ),
        hashes.SHA256()
    )

    # Save the Signature
    with open(signature_file_path, "wb") as fout:
        fout.write(signature)

    print("Signature generation complete")
Let's understand how the signing works.private_key.sign( …. ) :

Uses the RSA private key to generate a digital signature.Input is the raw data (in bytes) you want to sign.The result (signature) is a unique cryptographic value tied to  both the data and the private key.padding.PSS(…) : Provides Padding Schemes for Security

PSS (Probabilistic Signature Scheme) is used , which is the modern recommended padding for RSA signatures.It makes each signature different, even if the same data is signed multiple times (unlike older, deterministic schemes).Inside PSS:

mgf=padding.MGF1(hashes.SHA256()) → MGF1 is a mask generation function that adds randomness, using SHA-256 internally.salt_length=padding.PSS.MAX_LENGTH → Uses the largest possible salt (random value) to maximize security.hashes.SHA256()

Before signing, the file content is hashed using SHA-256.Instead of signing the entire raw file (which could be GBs in size), RSA signs this fixed-length hash digest.This ensures efficiency and security — even tiny changes in the file create a completely different hash, and thus a different signature.Think of this like stamping a document with a unique wax seal:The document = your file (data).The stamp mold = your private key.The wax pattern (randomized via PSS) = padding randomness.The final wax seal impression = the signature.Anyone with the public key can check the seal and confirm:The file hasn’t been changed.It really came from the holder of the private key.# Verify the File with the Signature
def verify_file(public_key_file, file_path, file_signature_path):

    # Load Public Key
    with open(public_key_file, "rb") as fin:
        public_key = serialization.load_pem_public_key(
            fin.read(),
            backend=default_backend()
        )

    # Load File Signature
    with open(file_signature_path, "rb") as fin:
        signature = fin.read()

    # Load File Content
    with open(file_path, "rb") as fin:
        data = fin.read()

    # Verify the Signature
    try:
        public_key.verify(
            signature=signature,
            data=data,
            padding=padding.PSS(
                mgf=padding.MGF1(hashes.SHA256()),
                salt_length=padding.PSS.MAX_LENGTH
            ),
            algorithm=hashes.SHA256()
        )

        print("Signature verified")
    except Exception as e:
        print("Signature verification failed")
        print(f"Exception: {e}")
Let's understand the Pros and Cons of this approach.Strong authenticity (no shared secret needed).Works across untrusted networks.Non-repudiation: Sender cannot deny signing.Slower than checksum or HMAC.Requires secure key management.More complex setup compared to symmetric approaches.
  
  
  📂 When to Use Digital Signatures?
When files are shared across different organizations.When authenticity is critical (legal, financial, healthcare files).When compliance demands non-repudiation (e.g., contracts, audit logs).Digital signatures add a powerful layer of security for file transfers — going beyond integrity to authenticity and trust. They are the go-to choice when sharing files in untrusted or external environments.➡️ In the next part of this series, I’ll look at AES Encryption for File Transfers to ensure not just authenticity, but also confidentiality.The code provided above can be found in Github.]]></content:encoded></item><item><title>Transformando áudios em texto com Python</title><link>https://dev.to/ivanrochacardoso/transformando-audios-em-texto-com-python-jh3</link><author>Ivan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 12:05:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[História real: Semana passada, um cliente me enviou 12 áudios do WhatsApp com especificações do projeto. Escutar tudo várias vezes para fazer as anotações me tomou horas. Sem falar que o transcritor nativo do WA demora, e nem sempre disponivel para o idioma.
A transcrição manual ou de sites de terceiros podem representar riscos a privacidade.
Pensamento imediato: "Deve ter uma forma de automatizar isso!"
E tinha! Em algumas horas de desenvolvimento, criei um script Python que:Pega qualquer áudio do WhatsApp (.ogg)
Converte e transcreve automaticamente
Funciona online (mais preciso) ou offline (privacidade total)
Processa múltiplos arquivos de uma vezO que começou como uma necessidade virou uma ferramenta que pode ajudar muita gente!
Casos de uso que imagino:Quem mais já passou por essa situação? Conta aí nos comentários!]]></content:encoded></item><item><title>From JSON to Dashboard: Visualizing DuckDB Queries in Streamlit with Plotly</title><link>https://www.kdnuggets.com/from-json-to-dashboard-visualizing-duckdb-queries-in-streamlit-with-plotly</link><author>Cornellius Yudha Wijaya</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-from-json-to-dashboard-duckdb-queries-streamlit-plotly.png" length="" type=""/><pubDate>Fri, 22 Aug 2025 12:00:09 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Learn how to connect several essential tools to develop a simple yet intuitive dashboard.]]></content:encoded></item><item><title>Real Python: The Real Python Podcast – Episode #262: Travis Oliphant: SciPy, NumPy, and Fostering Scientific Python</title><link>https://realpython.com/podcasts/rpp/262/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 12:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[What went into developing the open-source Python tools data scientists use every day? This week on the show, we talk with Travis Oliphant about his work on SciPy, NumPy, Numba, and many other contributions to the Python scientific community.]]></content:encoded></item><item><title>How Does Google Docs Work 🔥</title><link>https://newsletter.systemdesign.one/p/how-does-google-docs-work</link><author>Neo Kim</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/4b73f9d4-a8d6-4101-9dff-df53a7332de1_1280x720.png" length="" type=""/><pubDate>Fri, 22 Aug 2025 11:50:45 +0000</pubDate><source url="https://newsletter.systemdesign.one/">System Design Newsletter</source><content:encoded><![CDATA[Unlock access to every deep dive article by becoming a paid subscriber:I spent hours studying how Google Docs works so you don't have to. And I wrote this newsletter to make the key concepts simple and easy for you.Note: This post is based on my research and may differ from real-world implementation.Once upon a time, there lived a data analyst named Maria.She emailed draft copies many times to different people to prepare monthly reports.So she wasted a ton of time and was frustrated.Until one day, when she decides to use Google Docs for it.Google Docs allows collaborative editing over the internet. It means many users can work on the same document in real-time.Yet it’s difficult to implement Google Docs correctly for 3 reasons:Concurrent changes to the same document should converge to the same version.Concurrent changes to the same document must avoid conflicts.Any changes should be visible in real-time to each user.Also a user should be able to make changes while they’re offline.A simple approach to handle concurrency is using pessimistic concurrency control.is amechanism for handling concurrency using a lock. It offers strong consistency, but doesn’t support collaborative editing in real-time. Because it needs a central coordinator to handle data changes, only 1 user can edit at a time. Put simply, only a single document copy is available for write operations at once, while other document copies are read-only.Besides it doesn’t support offline changes.Also a network round-trip across the Earth takes 200 milliseconds. This might cause a poor user experience. So they do  The idea is to keep a document copy for each user locally and then run operations locally for high responsiveness. Thus creating the illusion of lower latency than reality.And the system propagates the changes to all users for consistency.A simple approach for latency hiding is using the mechanism.Yet it resolves a conflict without waiting for coordination by applying the most recent update. So there’s a risk of data loss when there are concurrent changes in high-latency networks.It might be a good choice when concurrency is low. But it isn’t suitable for this use case.An alternative approach to latency hiding is through differential synchronization.It keeps a document copy for each user and tracks the changes locally. The system doesn’t send the entire document when something changes, but only the difference ().Yet there’s a performance overhead in sending a diff for every change. Also differential synchronization only tracks diffs, and not the reason behind a change. So conflict resolution might be difficult.While resolving conflicts manually affects the user experience.OT is an algorithm to show document changes without wait times on high-latency networks. It allows different document copies to accept write operations at once. Also it handles conflict resolution automatically without locks or user interventions. Besides OT tolerates divergence among document copies and converges them later.Think of operational transformation as an event-passing mechanism; it ensures each user has the same document state even with unsynchronized changes.With OT, the system saves each change as an event. Put simply, a change doesn’t affect the underlying character of a document; instead, it adds an event to the revision log. The system then displays the document by replaying the revision log from its start.Operational transformation saves a document as a set of operations, but it's complex to implement properly.How Does Google Docs WorkGoogle Docs uses a client-server architecture for simplicity.]]></content:encoded></item><item><title>Create a Real-Time Chat App with Python, WebSockets, and FastAPI</title><link>https://dev.to/djamware_tutorial_eba1a61/create-a-real-time-chat-app-with-python-websockets-and-fastapi-24h2</link><author>Djamware Tutorial</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 11:45:51 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In this guide, you’ll learn how to:Use FastAPI with WebSockets for real-time communicationBroadcast chat messages to all usersExtend with multiple rooms and Redis Pub/SubDeploy and test your chat app]]></content:encoded></item><item><title>10 Must-Ask Interview Questions for Python Developers</title><link>https://dev.to/jessica_marious/10-must-ask-interview-questions-for-python-developers-4i1g</link><author>Jessica Marious</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 11:33:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Python has evolved from a simple scripting tool into one of the most widely used programming languages across web development, automation, data science, and machine learning. In 2025, finding the right Python developer for hire is more critical than ever. The challenge is that not every candidate with “Python experience” can build, scale, and maintain production-ready applications. A well-structured interview process is key to identifying developers who can write clean code and solve real problems effectively. This guide brings together 15 essential interview questions for Python developers. These questions cover fundamentals, coding skills, and problem-solving approaches, helping recruiters, hiring managers, and even developers preparing for interviews navigate the process with confidence. 
  
  
  1. What are Python’s key features?
This is a classic opener that helps you gauge how well a candidate understands Python’s fundamentals. A good developer should mention things like: Python is interpreted and dynamically typed.
It emphasizes readability and simplicity (thanks to indentation). It supports multiple programming paradigms (object-oriented, functional, procedural). It has a huge ecosystem of libraries and frameworks.Strong candidates usually go beyond buzzwords and give examples. For instance, they might mention how Python’s extensive community support makes troubleshooting easier, or how dynamic typing speeds up prototyping. 
  
  
  2. Explain Python’s memory management.
This question checks whether the developer understands what’s happening under the hood. Python manages memory using: Reference counting and garbage collection for unused objects. Memory pools (like PyMalloc) to optimize allocation. Developers can use modules like gc to interact with the garbage collector. 
  
  
  3. What are Python’s built-in data types and data structures?
Expect candidates to cover: Basic data types: int, float, str, bool. Collection types: list, tuple, set, dict. Advanced: frozenset, deque from collections, or even dataclasses. An excellent candidate won’t just list them but will explain use cases. For instance, why you’d use a tuple instead of a list (immutability, hashability), or when a dictionary is more efficient than nested lists. 
  
  
  4. Explain inheritance and polymorphism in Python.
Since Python is object-oriented, this is a must-ask. Candidates should explain:  allows a class to derive attributes and methods from another.  allows different classes to define methods with the same name but potentially different behavior. 
  
  
  5. What are decorators, and how are they used?
Decorators are a hot topic in Python interviews because they test both technical depth and practical coding skills. Candidates should say: Decorators are functions that wrap other functions to modify their behavior without changing their code. They’re widely used in frameworks like Flask (@app.route) or Django (@login_required). def wrapper(*args, **kwargs):  print(f"Calling {func.name}")  return func(*args, **kwargs)  This shows how decorators add functionality in a clean, reusable way. 
  
  
  6. What’s the difference between @staticmethod, @classmethod, and instance methods?
This question checks if candidates can distinguish between method types:  Regular methods, take self, operate on an instance.  Use @classmethod, take cls, often used for alternative constructors.  Use @staticmethod, don’t need self or cls, utility functions inside a class.An advanced developer may explain when to use them. For example, using a classmethod to create objects from different input formats (like from_json). 
  
  
  7. Explain Python’s Global Interpreter Lock (GIL).
If you’re hiring for performance-heavy roles, this is essential. A good candidate should explain:The GIL ensures only one thread executes Python bytecode at a time, even on multi-core systems.This can limit CPU-bound multi-threaded programs.Workarounds include multiprocessing, async programming, or using libraries like NumPy that release the GIL internally.This answer shows if they understand Python’s concurrency limitations and know alternatives.
  
  
  8. How do you manage virtual environments and dependencies in Python projects?
This is a practical skill every Python dev needs. Answers may include:Tools like venv, virtualenv, or conda.Using pip freeze > requirements.txt to track dependencies.For larger projects, using pipenv or poetry for environment and dependency management.Candidates should also stress why isolation matters—avoiding version conflicts.
  
  
  9. How do you handle database interactions in Python?
Using ORMs (Django ORM, SQLAlchemy).Direct queries with libraries like sqlite3 or psycopg2.Handling transactions, migrations, and performance tuning.The best candidates may add how they use connection pooling or database indexing for performance.
  
  
  10. What’s your approach to testing and debugging Python code?
Testing is critical for long-term maintainability. Candidates should mention:Using built-in unittest or frameworks like pytest.Writing modular, testable code.Mocking external dependencies.class TestMath(unittest.TestCase):
    def test_addition(self):
        self.assertEqual(2 + 2, 4)`Understands the fundamentals (data types, OOP, decorators).Can solve real-world problems (web frameworks, database handling, testing).Thinks about scalability and maintainability (generators, profiling, debugging).By asking these 15 must-ask interview questions, you’ll not only filter out unprepared candidates but also identify developers who bring real value to your projects.And if you’re a developer preparing for interviews, treat these as your study checklist. Mastering these concepts will help you walk into any interview with confidence.]]></content:encoded></item><item><title>Stop losing your breakpoints: Meet Breakpoint Bookmarks for VS Code</title><link>https://dev.to/omardulaimi/stop-losing-your-breakpoints-meet-breakpoint-bookmarks-for-vs-code-3c4b</link><author>Omar Dulaimi</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 11:20:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you've ever stopped mid‑debug to chase a different bug, you know the pain: you come back and all your carefully placed breakpoints are gone. You try to remember where they were, what conditions you had, which logs you set… and momentum dies.I built  to fix that. It lets you  your current breakpoints to a named “flow”,  between flows instantly, and  everything exactly where it was—conditions, logpoints, function breakpoints and all.TL;DR — Install it, hit , and stop babysitting your breakpoints. of all your active breakpoints (source & function) — create one per bug, feature, or customer issueWorks with anything VS Code can debug (JS/TS, Python, Java, C#, Go, Rust, PHP, Ruby…): a dedicated sidebar with inline actions (Save, Load, Edit, Delete): built in TypeScript, tested, and cross‑platformFrom the Command Palette ():ext install OmarDulaimi.breakpoint-bookmarks
code  OmarDulaimi.breakpoint-bookmarks
1)  your breakpoints as usual (conditions, hit counts, logpoints, function breakpoints—go wild). view (Activity Bar → “Breakpoint Bookmarks”). to snapshot your current session to a named flow. on any flow to restore the entire session—exact lines, conditions, and messages. to tweak the JSON by hand (power users, this is for you). a flow when it’s no longer useful.Pro tip: Keep a “Happy‑path” flow you can load anytime you need a clean baseline.
  
  
  Settings you might care about
 — keep one flow per issue, jump between them in seconds.
 — flows for “staging”, “canary”, “prod‑sim”.
 — hand new folks a “Debug 101” flow for the codebase.
 — save the exact breakpoints used to reproduce a ticket.
 — share a flow in the repo so everyone can follow the same trail.Function breakpoints are fully supported (alongside file/line breakpoints)Cleaner sidebar UI with hover actions and a top‑bar  buttonBetter Windows path handling and cross‑platform behaviorBackward‑compatible with older bookmark files(Changelog lives in the repo if you like the gory details.)
  
  
  Roadmap — tell me what to ship next
I have a few ideas cooking, but I’d rather build what  need:Shared/team flows out of the box (auto‑discover in workspace)Branch‑aware flows (auto‑switch based on current git branch)“Save only changes since last load”Diff/merge flows, and search across flowsCLI to automate flows in CI/reprosAPI for other extensions to read/write flowsHave a better idea? Open an issue or drop a comment — I read everything.
  
  
  If this saves you time ❤️
A star or review goes a long way. If it’s really helping your day‑to‑day, you can also sponsor development — even a tiny amount helps me ship faster and keep docs & fixes flowing.Thanks for reading — and happy debugging. If you write about how you’re using flows in your team, I’ll gladly link it from the repo.]]></content:encoded></item><item><title>Understanding Bootstrapping: How Go’s Compiler Is Written in Go</title><link>https://dev.to/mrsa1/understanding-bootstrapping-how-gos-compiler-is-written-in-go-5ann</link><author>Rad Sarar</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 22 Aug 2025 11:00:01 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Do you know most of the codes of Go programming language, even its compiler is written with Go? There may be a question in the head after hearing this, "How is that possible?" You need another Go compiler to make a Go compiler, right? "
It's a classic "egg before or chicken before" kind of question. And the answer lies in a cool computer science concept called Bootstrapping.
What is the Bootstrapping thing?
In simple terms, bootstrapping is using a small or simple system to create a bigger and stronger system. Something like lifting yourself up by holding your shoe lace! This is exactly what happened to Go.
Let's see how it works step by step:The first compiler was written in another language:
The first compiler of Go was not written in Go language. It was written using C language. This C-based compiler had only one job: compiling the Go Source Code into an executable program.Written a new compiler with Go:
After that, Go developers wrote the source code for a whole new compiler using Go language.The real magic: 
Bootstrapping is done in this stage. The Go Team used their old compiler made with C to compile the source code of the new compiler written with Go. So they got a compiler (executable file) made of Go.Go Self-Sufficient: 
Once Go could build his own compiler, there was no need for the old C-based compiler. From now on Go starts building himself.
Means, Go 1.4 is used to compile the Go 1.5 version. Used Go 1.5 to compile Go 1.6 again This is how the chain goes.
So, next time you hear "Go is written in Go", don't be surprised! This bootstrapping method uses not only Go, but also large programming language like C, Rust, Java. This is the sign of maturity or maturity of a language.
Some questions may arise in your mind as such:
Q: What is the advantage or profit of this? Wouldn't the compiler made of C be faster?
Answer: There are many benefits such as maintenance, contributor productivity, tooling consistency, portability, and feature development speed etc. will be easy. Many common bug of RC can be avoided.
Q:At the end of the day, then, the base of the Go compiler is in C, right?
Answer: A compiler written in C, called the C-Compiler, is used to build the first version of the Go compiler from its Go source code. This process creates an executable file: go_compiler_v1.exe. Next, go_compiler_v1.exe is used to compile a newer version of the Go source code. This creates go_compiler_v2.exe, which can then be used to compile the next version, and so on. Thus, the C-Compiler is no longer needed.
Q: Isn't it the same for Java?
Answer: Many important languages follow the bootstrapping technique, but by no means is it a universal rule for success.]]></content:encoded></item><item><title>Dev Gets 4 Years For Creating Kill Switch On Ex-Employer&apos;s Systems</title><link>https://yro.slashdot.org/story/25/08/22/0039200/dev-gets-4-years-for-creating-kill-switch-on-ex-employers-systems?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Fri, 22 Aug 2025 10:00:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[Davis Lu, a former Eaton Corporation developer, has been sentenced to four years in prison for sabotaging his ex-employer's Windows network with malware and a custom kill switch that locked out thousands of employees once his account was disabled. The attack caused significant operational disruption and financial losses, with Lu also attempting to cover his tracks by deleting data and researching privilege escalation techniques. BleepingComputer reports: After a corporate restructuring and subsequent demotion in 2018, the DOJ says that Lu retaliated by embedding malicious code throughout the company's Windows production environment. The malicious code included an infinite Java thread loop designed to overwhelm servers and crash production systems. Lu also created a kill switch named "IsDLEnabledinAD" ("Is Davis Lu enabled in Active Directory") that would automatically lock all users out of their accounts if his account was disabled in Active Directory. When his employment was terminated on September 9, 2019, and his account disabled, the kill switch activated, causing thousands of users to be locked out of their systems.
 
"The defendant breached his employer's trust by using his access and technical knowledge to sabotage company networks, wreaking havoc and causing hundreds of thousands of dollars in losses for a U.S. company," said Acting Assistant Attorney General Matthew R. Galeotti. When he was instructed to return his laptop, Lu reportedly deleted encrypted data from his device. Investigators later discovered search queries on the device researching how to elevate privileges, hide processes, and quickly delete files. Lu was found guilty earlier this year of intentionally causing damage to protected computers. After his four-year sentence, Lu will also serve three years of supervised release following his prison term.]]></content:encoded></item><item><title>Web Developers for Hire: Your Guide to Finding Skilled Professionals</title><link>https://dev.to/michael_keller_9d83ef0ce5/web-developers-for-hire-your-guide-to-finding-skilled-professionals-p2g</link><author>Michael Keller</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 09:40:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In today’s digital-first world, a website is more than just an online presence it is the foundation of your brand. Businesses, whether startups or established enterprises, are constantly looking for web developers for hire to create powerful, secure, and scalable platforms. While ready-made templates exist, only professional developers can deliver customized solutions that align with unique business needs. This guide explores the benefits of hiring web experts, the types of developers available, and how to make the right hiring decision.Why Businesses Need Web DevelopersGeneric templates often limit functionality. Skilled website developers for hire provide tailor-made solutions designed to support long-term business growth.From navigation to responsiveness, developers ensure a smooth and enjoyable user journey, which leads to higher engagement and conversions.Cybersecurity is a growing concern. Professional web programmers apply best practices to protect user data and ensure compliance with industry regulations.As businesses expand, scalable websites are critical. This is why many companies choose to Hire Dedicated Developers who can adapt projects to evolving needs.
  
  
  Types of Web Developers for Hire
Focus on the client-facing side of websites, building visually appealing and responsive interfaces with HTML, CSS, and JavaScript.Work on the server side, handling databases, application logic, and APIs using languages like PHP, Python, Java, and Node.js.Possess expertise in both front-end and back-end development, making them ideal for startups and businesses seeking versatile talent.
  
  
  Remote and Offshore Developers
Offer cost-effective solutions by working across time zones, delivering quality at competitive rates.
  
  
  Advantages of Hiring Dedicated Web Developers
Hiring professionals ensures clean code, optimized performance, and industry-standard practices.Custom website developers for hire integrate SEO strategies, such as fast load speeds and mobile optimization, from the start.A reliable developer isn’t just for initial development—they provide updates, bug fixes, and technical assistance over time.Whether hiring freelancers, agencies, or offshore teams, flexible hiring models suit every business budget.
  
  
  How to Hire the Right Web Developers

  
  
  Step 1: Define Your Project Goals
Be clear on whether you need an e-commerce platform, a portfolio site, or a large enterprise solution.
  
  
  Step 2: Explore Hiring Options
Offshore outsourcing firms
  
  
  Step 3: Assess Skills and Expertise
Check technical skills, coding samples, and portfolios to confirm their capability.
  
  
  Step 4: Evaluate Soft Skills
Good communication and problem-solving are just as important as technical expertise.
  
  
  Step 5: Secure a Clear Agreement
Sign contracts, NDAs, and set timelines to ensure transparency and accountability.
  
  
  Industries That Benefit From Hiring Web Developers
Developers create feature-rich online stores with shopping carts, secure payments, and product catalogs.Custom portals and telemedicine platforms require developers who understand compliance and data security.Web developers build secure, user-friendly financial platforms that support transactions and integrations.From e-learning apps to online classrooms, skilled programmers are essential in the education sector.Property listing portals, CRMs, and virtual tours rely heavily on web development expertise.Hiring based only on cost rather than skill.Ignoring past projects or reviews.Failing to define clear project requirements.Overlooking the importance of post-launch support.
  
  
  Why Choose to Hire Dedicated Developers
Hiring on-demand talent has its advantages, but many businesses prefer to Hire Dedicated Developers because:They work exclusively on your project.They align with your long-term goals.They become an extension of your in-house team.They deliver consistent quality and ongoing support.This model is particularly effective for companies that require continuous development, scaling, and maintenance without disruptions.Finding the right web developers for hire is about more than filling a technical role; it’s about building a partnership that drives long-term success. By identifying project requirements, evaluating expertise, and choosing the right hiring model, businesses can secure skilled professionals who deliver both immediate results and sustainable growth.Whether you need front-end specialists, back-end experts, or full-stack professionals, the smart choice is to Hire Dedicated Developers who bring commitment, scalability, and reliability to your project. With the right team in place, your business can thrive in the digital landscape.]]></content:encoded></item><item><title>Python’s Continued Supremacy &quot;From Python to Rust: What’s Hot in 2025 Programming&quot;</title><link>https://dev.to/cpamarketer_3557120338336/pythons-continued-supremacy-from-python-to-rust-whats-hot-in-2025-programming-1nl3</link><author>Cpamarketer</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 09:39:29 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the ever-evolving landscape of programming languages, one constant remains: Python’s dominance. Despite the rise of newer languages and frameworks, Python continues to stand as the go-to choice for developers, data scientists, and enterprises across the globe. Its simplicity, versatility, and thriving ecosystem make it a language that refuses to fade into the background.
 Get Free coding click here: https://freeaccessprogrammingcodes.blogspot.comOne of Python’s greatest strengths is its readable, human-friendly syntax. Unlike languages that require steep learning curves, Python allows beginners to start coding quickly, while also offering the depth needed for advanced projects. This balance makes it uniquely suited for both hobbyists learning their first lines of code and professionals building enterprise-scale systems.A Swiss Army Knife of ProgrammingPython’s supremacy comes not just from its ease of use but from its unmatched versatility. It powers applications across domains:Web Development: Frameworks like Django and Flask fuel startups and large-scale platforms alike.Data Science & AI: Libraries such as NumPy, Pandas, TensorFlow, and PyTorch make Python the backbone of artificial intelligence and machine learning.Automation: From simple scripts to enterprise workflows, Python has become the default choice for automation.Cybersecurity: Security experts rely on Python for penetration testing and tool development.Game Development & IoT: Its reach extends even into creative and hardware-focused industries.Few languages can boast this level of adaptability.Community Power and EcosystemAnother key factor behind Python’s staying power is its global community. With millions of developers contributing to open-source projects, maintaining libraries, and offering tutorials, Python has one of the richest ecosystems in tech. This means developers rarely face problems alone—there’s almost always a Python library, guide, or forum thread that has the solution.The Language of Data and AIIn an age where data is king, Python reigns supreme. Nearly every breakthrough in machine learning, deep learning, or generative AI has Python somewhere in its foundation. Its seamless integration with big data tools and AI frameworks ensures that Python will remain at the heart of the tech revolution for years to come.Even with competition from languages like JavaScript, Rust, and Go, Python continues to hold its crown because it strikes the right balance between power and accessibility. It isn’t the fastest language in terms of raw execution, but its development speed, vast ecosystem, and flexibility consistently outweigh performance drawbacks.As industries push deeper into AI, data analytics, and automation, Python’s role only grows stronger. Its adaptability ensures that it evolves with new technologies rather than becoming outdated. Whether you’re building a machine learning model, automating a workflow, or creating the next big web platform, Python will likely be there at the core.✨ In short, Python’s supremacy isn’t just about popularity—it’s about reliability, versatility, and community-driven innovation. It’s not just a programming language; it’s the universal language of modern technology.]]></content:encoded></item><item><title>Ep2 : Rebuilding Uber&apos;s API Gateway</title><link>https://dev.to/sahilbaig/ep2-rebuilding-ubers-api-gateway-cea</link><author>Sahil Baig</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 22 Aug 2025 09:21:50 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Kubernetes networking is a pain 🛜
In Episode 1, I got all the services listed on my Kubernetes cluster , but everything was running inside the cluster as containers. To mimic how Uber’s Gateway API works, I needed to take the gateway outside the cluster.🧱 That's when the first challenge hit: service discovery broke. Containers inside the cluster can talk to each other easily, but from the outside, it's a different story. To fix this, I configured RBAC to allow external requests to the cluster. This let me retrieve the services and the pod IPs running them - so far, so good.🔐 Then came the next hurdle: these pod IPs are only meaningful inside the cluster. Any requests coming from outside? They can’t reach the pods at all. Right now, I’m exploring whether a service mesh might help route traffic properly, or if there’s another way to bridge this gap. Stay tuned for Episode 3, where I dive into the solution and finally get the external gateway fully functional.Also find a brain rot version of architecture diagram of what I am trying to achieve.]]></content:encoded></item><item><title>SassGuard: The Ultimate Discord Bot for Blocking NSFW &amp; Gore Content (2025)</title><link>https://dev.to/geeker_smart_d1251357555f/sassguard-the-ultimate-discord-bot-for-blocking-nsfw-gore-content-2025-1nbc</link><author>Geeker Smart</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 09:16:10 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Running a safe Discord community is harder than ever. Between spam bots, trolls, and unwanted NSFW content, server admins need better tools to protect their members.  That’s where  comes in. 🚀
  
  
  🔒 Why Discord Needs Better NSFW Protection
Discord has grown into one of the most popular community platforms, but built-in filters and AutoMod aren’t enough.  NSFW images and videos can still slip through.
Gore or disturbing content isn’t always caught.
Bots posting embeds and malicious links can bypass filters.
For communities that want to stay family-friendly, professional, or school-safe, a stronger layer of protection is essential.  SassGuard is a next-generation  designed to keep your server free from NSFW, gore, and harmful content.   → Detects NSFW or gore media in real time.
 → Stops harmful embeds or links that could sneak past normal moderation.
 → Identifies toxic language and disallowed content.
 → Flags or deletes unsafe content instantly, keeping your server safe.
 → Easy setup and fine-tuning for admins.
With SassGuard, you don’t need to rely only on manual moderation — your bot works 24/7.  A message (image, video, embed, sticker, gif) is sent in your server.
SassGuard’s AI scans it for NSFW, gore, or disallowed content.
If it’s safe ✅ → nothing happens.
If it’s unsafe 🚫 → the bot deletes, flags, or alerts moderators immediately.
This ensures  without slowing down conversations.  
  
  
  🏆 Why Choose SassGuard Over Other Bots?
There are a lot of moderation bots out there (Dyno, MEE6, Carl-bot, etc.), but most don’t specialize in advanced content detection.  SassGuard stands out because it:  Detects images, videos, embeds, gifs, stickers, and text (not just words).
Blocks , which most bots ignore.
Uses , not just keyword filters.
Offers  so each server can fine-tune settings.

  
  
  🚀 Get Started with SassGuard
Ready to make your server safer?  With SassGuard, your community stays clean, safe, and welcoming — without extra work for moderators.  Whether you’re running a gaming clan, a school community, or a professional workspace, protecting your members from NSFW and gore content is critical.  SassGuard is built to be the best anti-NSFW Discord bot in 2025, and we’d love to see how it helps your community grow.  Stay safe. Stay clean. Stay SassGuarded. 🛡️]]></content:encoded></item><item><title>Talk Python to Me: #517: Agentic Al Programming with Python</title><link>https://talkpython.fm/episodes/show/517/agentic-al-programming-with-python</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 08:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Agentic AI programming is what happens when coding assistants stop acting like autocomplete and start collaborating on real work. In this episode, we cut through the hype and incentives to define “agentic,” then get hands-on with how tools like Cursor, Claude Code, and LangChain actually behave inside an established codebase. Our guest, Matt Makai, now VP of Developer Relations at DigitalOcean, creator of Full Stack Python and Plushcap, shares hard-won tactics. We unpack what breaks, from brittle “generate a bunch of tests” requests to agents amplifying technical debt and uneven design patterns. Plus, we also discuss a sane git workflow for AI-sized diffs. You’ll hear practical Claude tips, why developers write more bugs when typing less, and where open source agents are headed. Hint: The destination is humans as editors of systems, not just typists of code.<br/>
<br/>
<strong>Episode sponsors</strong><br/>
<br/>
<a href='https://talkpython.fm/connect-cloud'>Posit</a><br>
<a href='https://talkpython.fm/training'>Talk Python Courses</a><br/>
<br/>
<h2 class="links-heading">Links from the show</h2>
<div><strong>Matt Makai</strong>: <a href="https://www.linkedin.com/in/matthewmakai/?featured_on=talkpython" target="_blank" >linkedin.com</a><br/>
<br/>
<strong>Plushcap Developer Content Analytics</strong>: <a href="https://www.plushcap.com/?featured_on=talkpython" target="_blank" >plushcap.com</a><br/>
<strong>DigitalOcean Gradient AI Platform</strong>: <a href="https://www.digitalocean.com/products/gradient/platform?featured_on=talkpython" target="_blank" >digitalocean.com</a><br/>
<strong>DigitalOcean YouTube Channel</strong>: <a href="https://www.youtube.com/c/digitalocean" target="_blank" >youtube.com</a><br/>
<strong>Why Generative AI Coding Tools and Agents Do Not Work for Me</strong>: <a href="https://blog.miguelgrinberg.com/post/why-generative-ai-coding-tools-and-agents-do-not-work-for-me?featured_on=talkpython" target="_blank" >blog.miguelgrinberg.com</a><br/>
<strong>AI Changes Everything</strong>: <a href="https://lucumr.pocoo.org/2025/6/4/changes/?featured_on=talkpython" target="_blank" >lucumr.pocoo.org</a><br/>
<strong>Claude Code - 47 Pro Tips in 9 Minutes</strong>: <a href="https://www.youtube.com/watch?v=TiNpzxoBPz0" target="_blank" >youtube.com</a><br/>
<strong>Cursor AI Code Editor</strong>: <a href="https://cursor.com/en?featured_on=talkpython" target="_blank" >cursor.com</a><br/>
<strong>JetBrains Junie</strong>: <a href="https://www.jetbrains.com/junie/?featured_on=talkpython" target="_blank" >jetbrains.com</a><br/>
<strong>Claude Code by Anthropic</strong>: <a href="https://www.anthropic.com/claude-code?featured_on=talkpython" target="_blank" >anthropic.com</a><br/>
<strong>Full Stack Python</strong>: <a href="https://www.fullstackpython.com/?featured_on=talkpython" target="_blank" >fullstackpython.com</a><br/>
<strong>Watch this episode on YouTube</strong>: <a href="https://www.youtube.com/watch?v=qYhXCELk05E" target="_blank" >youtube.com</a><br/>
<strong>Episode #517 deep-dive</strong>: <a href="https://talkpython.fm/episodes/show/517/agentic-al-programming-with-python#takeaways-anchor" target="_blank" >talkpython.fm/517</a><br/>
<strong>Episode transcripts</strong>: <a href="https://talkpython.fm/episodes/transcript/517/agentic-al-programming-with-python" target="_blank" >talkpython.fm</a><br/>
<strong>Developer Rap Theme Song: Served in a Flask</strong>: <a href="https://talkpython.fm/flasksong" target="_blank" >talkpython.fm/flasksong</a><br/>
<br/>
<strong>--- Stay in touch with us ---</strong><br/>
<strong>Subscribe to Talk Python on YouTube</strong>: <a href="https://talkpython.fm/youtube" target="_blank" >youtube.com</a><br/>
<strong>Talk Python on Bluesky</strong>: <a href="https://bsky.app/profile/talkpython.fm" target="_blank" >@talkpython.fm at bsky.app</a><br/>
<strong>Talk Python on Mastodon</strong>: <a href="https://fosstodon.org/web/@talkpython" target="_blank" ><i class="fa-brands fa-mastodon"></i>talkpython</a><br/>
<strong>Michael on Bluesky</strong>: <a href="https://bsky.app/profile/mkennedy.codes?featured_on=talkpython" target="_blank" >@mkennedy.codes at bsky.app</a><br/>
<strong>Michael on Mastodon</strong>: <a href="https://fosstodon.org/web/@mkennedy" target="_blank" ><i class="fa-brands fa-mastodon"></i>mkennedy</a><br/></div>]]></content:encoded></item><item><title>#517: Agentic Al Programming with Python</title><link>https://talkpython.fm/episodes/show/517/agentic-al-programming-with-python</link><author></author><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://talkpython.fm/episodes/download/517/agentic-al-programming-with-python.mp3" length="" type=""/><pubDate>Fri, 22 Aug 2025 08:00:00 +0000</pubDate><source url="https://talkpython.fm/">Talk Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>sorted &amp; reversed in Python</title><link>https://dev.to/hyperkai/sorted-reversed-in-python-2i0e</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 07:06:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[sorted() can convert a string or byte string to a list, then sort the list, then the sorted list is converted to a string or byte string with join() or bytes() and bytearray() as shown below:The 1st argument is (Required-Type:iterable). *Don't use .The 2nd argument is (Optional-Default:-Type:callable).The 3rd argument is (Optional-Default:-Type:) to reverse the list.  creates a copy. *Be careful,  does shallow copy instead of deep copy as my issue.reversed() can return the iterator which has the reversed characters of a string or the reversed bytes of a byte string, then the iterator is converted to a string or byte string with  or  and  as shown below:The 1st argument is (Required-Type:sequence). *Don't use .]]></content:encoded></item><item><title>iskeyword &amp; issoftkeyword in Python</title><link>https://dev.to/hyperkai/iskeyword-issoftkeyword-in-python-28cb</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 06:56:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument is (Required-Type:any):

It doesn't accept .kwlist can return a list of Python keywords.The 1st argument is (Required-Type:any):

It doesn't accept .softkwlist can return a list of Python soft keywords.]]></content:encoded></item><item><title>isascii, isspace, isprintable &amp; isidentifier in Python</title><link>https://dev.to/hyperkai/isascii-isspace-isprintable-isidentifier-in-python-a8c</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 06:47:19 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[str.isprintable() can check if a string only has one or more printable characters and is empty as shown below: and  don't exist for a byte string.
 and  don't exist for a byte string.
]]></content:encoded></item><item><title>Python Packages Every Developer Must Know(Especially Beginners)</title><link>https://dev.to/masteringbackend/python-packages-every-developer-must-knowespecially-beginners-bk1</link><author>Jane</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 06:37:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you’re just getting started with Python, you’re probably wondering which libraries are essential and what problems they solve. I recently began my Python journey and compiled this list of must-know Python packages. Whether you’re into web development, data science, automation, or building APIs, these tools will come in handy.FastAPI — A modern web framework for building APIs with automatic Swagger documentation. Its fast, easy to learn and simple to use.pip install "fastapi[standard]"
# main.py 
from fastapi import FastAPI 

app = FastAPI() 

@app.get("/") 
def home(): 
    return {"Hello": "World"}
To run it, you would need to install Uvicornuvicorn main:app --reload
Flask — A lightweight web framework for building web applications and APIs as it does not include built-in features like database abstraction layers, form validation, or extensive authentication systems. Instead, it focuses on providing the core functionalities for URL routing and page rendering.from flask import Flask 

app = Flask( __name__ ) 

@app.route("/") 
def hello_world(): 
     return "<p>Hello, World!</p>"
Django — A high-level web framework that follows the Model-View-Template (MVT) pattern, a variation of the Model-View-Controller(MVC) pattern. It is a free and open-source, Python-based web framework designed for rapid development of interactive websites. It includes everything you need — no need to choose separate libraries for common features.# Create project 
django-admin startproject myblog 
cd myblog 

# Create app 
python manage.py startapp blog
# Create a blog 
# models.py - Define your data 
from django.db import models 

class Post(models.Model): 
      title = models.CharField(max_length=200) 
      content = models.TextField() 
      created_at = models.DateTimeField(auto_now_add=True) 

      def __str__ (self): 
           return self.title 

# views.py - Handle requests 
from django.shortcuts import render, redirect 
from django.http import HttpResponse 
from .models import Post 

def home(request): 
    posts = Post.objects.all() 
    return render(request, 'home.html', {'posts': posts}) 

def create_post(request): 
    if request.method == 'POST': 
        title = request.POST.get('title') 
        content = request.POST.get('content') 
        if title and content: 
            Post.objects.create(title=title, content=content) 
            return redirect('home') 
  return render(request, 'create_post.html') 

# urls.py - Define routes 
from django.urls import path 
from . import views 

urlpatterns = [ 
    path('', views.home, name='home'), 
    path('create/', views.create_post, name='create_post'), 
] 

# templates/home.html - Display data 
<html> 
<body> 
    <h1>My Blog</h1> 
    {% for post in posts %} 
       <div> 
           <h2>{{ post.title }}</h2> 
           <p>{{ post.content }}</p> 
            <small>{{ post.created_at }}</small> 
         </div> 
     {% endfor %} 
     <a href="/create/">Create New Post</a> 
</body> 
</html> 

# templates/create_post.html - Create post- 
<!DOCTYPE html> 
<html> 
<head> 
    <title>Add Blog</title> 
</head> 
<body> 
    <h1>Add new blog</h1> 
    <form method="post"> 
        {% csrf_token %} 
        <input type="text" name="title" placeholder="Title" required><br> 
        <input type="text" name="content" placeholder="Content" required><br> 
        <button type="submit">Add</button> 
     </form> 
     <a href="/">Back to home</a> 
</body> 
</html>
python manage.py runserver
ASGI and WSGI are server interface standards in Python for running web applications. They define the handling of requests and the interaction between your server and your code. WSGI serves as the conventional standard for synchronous Python web applications, whereas ASGI is its successor, tailored for asynchronous applications and able to accommodate both synchronous and asynchronous code — An ASGI server for running FastAPI and other async frameworks. When you install  or  Uvicorn is automatically installed, unless you want a specific version.
# To install 
pip install "fastapi[standard]" 
# To run 
uvicorn main:app --reload
 — A WSGI server for running Flask/Django applications in production. Use the WSGIs server like  if you’re running Flask or Django (unless you’re adding async support to Django).# To install 
pip install gunicorn 
# To run 
gunicorn myapp:app

  
  
  3. Data & Machine Learning
NumPy is short for Numerical Python, is an open-source library in Python for scientific computing. It supports large, multi-dimensional arrays and offers powerful tools for numerical computing.# To get the mean of a list 
import numpy as np 
arr = np.array([1, 2, 3]) 
print(arr.mean())
Pandas — A powerful library for data manipulation and analysis. It makes working with spreadsheet-like data (CSV files) easy to clean, analyze and manipulate it.import pandas as pd 
df = pd.DataFrame({"name": ["Alice", "Bob"], "age": [25, 30]}) 
print(df.head())
Matplotlib & Seaborn — A plotting library for creating graphs and visualizations. Seaborn is built used for statistical data visualization.pip install matplotlib seaborn
import seaborn as sns 
import matplotlib.pyplot as plt 

sns.set_theme() 
sns.histplot([1, 2, 2, 3, 3, 3]) 
plt.show()
Scikit-learn — A machine learning library for tasks like classification, regression or clustering like predicting prices, classifying emails, or finding patterns in data. It comes with many built-in algorithms and datasets.from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
from sklearn.ensemble import RandomForestClassifier 
import numpy as np 

# Example 1: Predict house prices 
# Data: [size, bedrooms] -> price 
X = [[1000, 2], [1500, 3], [2000, 4], [2500, 4]] # features 
y = [200000, 300000, 400000, 500000] # prices 

# Train model 
model = LinearRegression() 
model.fit(X, y) 

# Predict new house price 
new_house = [[1800, 3]] 
predicted_price = model.predict(new_house) 
print(f"Predicted price: ${predicted_price[0]:,.0f}") 

classifier = RandomForestClassifier()
TensorFlow — A deep learning framework used for building neural networks for image recognition, natural language processing, or complex pattern recognition.import tensorflow as tf 

# Load dataset mnist = tf.keras.datasets.mnist 
(x_train, y_train), (x_test, y_test) = mnist.load_data() 

# Normalize pixel values to [0, 1] 
x_train, x_test = x_train / 255.0, x_test / 255.0 

# Build model 
model = tf.keras.models.Sequential([ 
    tf.keras.layers.Flatten(input_shape=(28, 28)), # Flatten image 
    tf.keras.layers.Dense(128, activation='relu'), # Hidden layer 
    tf.keras.layers.Dense(10, activation='softmax') # Output (10 classes) 
]) 

# Compile and train 
 model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) 
 model.fit(x_train, y_train, epochs=3) 

# Evaluate 
loss, acc = model.evaluate(x_test, y_test) 
print("Accuracy:", acc)

  
  
  4.Databases & ORMs(Object Relational Mappers)
SQLAlchemy — A SQL toolkit and ORM for working with relational databases(PostgreSQL, MySQL, SQLite) and want to write Python instead of raw SQL. It provides both high-level ORM for easy database operations and low-level SQL toolkit for complex queries.from sqlalchemy import create_engine, Column, Integer, String 
from sqlalchemy.ext.declarative import declarative_base 
from sqlalchemy.orm import sessionmaker 

Base = declarative_base() 

class User(Base): 
    __tablename__ = 'users' 
    id = Column(Integer, primary_key=True) 
    name = Column(String(50)) 
    email = Column(String(100)) 

# Setup 
engine = create_engine('sqlite:///app.db') 
Base.metadata.create_all(engine) 
Session = sessionmaker(bind=engine) 
session = Session() 

# Create user 
user = User(name="John", email="[email protected]") 
session.add(user) 
session.commit() 

# Query users 
users = session.query(User).filter(User.name == "John").all()
Pydantic- It is a library for data validation and parsing, and especially useful in FastAPI for defining request/response models. It has automatic validation with clear error messages, type conversion, and seamless integration with FastAPI. It comes with FastAPI when you install it.from pydantic import BaseModel, EmailStr 
from typing import Optional 

class User(BaseModel): 
    name: str 
    email: EmailStr 
    age: int 
    is_active: Optional[bool] = True 

# Valid data 
user = User(name="John", email="[email protected]", age=25) 
print(user.name) # "John" 

# Invalid data - raises ValidationError 
try: 
    User(name="John", email="not-an-email", age="not-a-number") 
except ValidationError as e: 
    print("Validation failed!")
Psycopg2 — A database adapter for connecting Python with the PostgresQL database. It allows for direct access to the database with full control over SQL commands.pip install psycopg2-binary
import psycopg2 

# Connect 
conn = psycopg2.connect( 
     host="localhost", 
     database="myapp", 
     user="postgres", 
     password="password" 
) 
cursor = conn.cursor() 

# Execute SQL 
cursor.execute(""" 
    CREATE TABLE users ( 
        id SERIAL PRIMARY KEY, 
        name VARCHAR(50), 
        email VARCHAR(100) 
   ) 
""") 

# Insert data 
 cursor.execute( 
     "INSERT INTO users (name, email) VALUES (%s, %s)", 
     ("John", "[email protected]") 
) 

# Query data 
cursor.execute("SELECT * FROM users WHERE name = %s", ("John",)) 
users = cursor.fetchall() 

conn.commit() 
cursor.close()
PyMongo — A MongoDB driver for Python applications. It provides direct interface to MongoDB with Pythonic API, perfect for unstructured or semi-structured data.from pymongo import MongoClient 

# Connect 
client = MongoClient('mongodb://localhost:27017/') 
db = client['myapp'] 
users = db['users'] 

# Insert document (any structure) 
user = { 
    "name": "John", 
    "email": "[email protected]", 
    "preferences": {"theme": "dark", "lang": "en"} 

} 
users.insert_one(user) 

# Find documents 
john = users.find_one({"name": "John"}) 
dark_users = users.find({"preferences.theme": "dark"})
Requests — A simple library for making HTTP requests, download files and interact with web services. It is simple, clear syntax for HTTP requests.import requests 

# GET request 
response = requests.get('https://api.github.com/users/octocat') 
user_data = response.json() 
print(user_data['name'])
HTTPX — An async alternative to Requests, and useful when build applications with FastAPI. The async/await supports allow for better performance.import httpx 
import asyncio 

# Synchronous (same as requests) 
 response = httpx.get('https://api.github.com/users/octocat') 
 print(response.json())
Pytest — A framework for writing and running tests in Python.def add(x, y): return x + y 

def test_add(): 
    assert add(2, 3) == 5
Celery — A distributed task queue for handling background jobs. When you have long-running tasks that would block your web app, need distributed task processing across multiple servers, or require complex scheduling use Celery. Celery is battle-tested, supports multiple brokers, has advanced features like task routing, retries, and monitoring. Celery is enterprise ready, has a larger ecosystem and more features.# celery_app.py 
from celery import Celery 

# Create Celery app with Redis as broker 
app = Celery('tasks', broker='redis://localhost:6379/0') 

@app.task 
def send_email(email, subject, body): 
    # This runs in the background 
    import time 
    time.sleep(5) # Simulate email sending 
    print(f"Email sent to {email}") 
    return f"Email sent successfully to {email}"
E-commerce: Processing payments, sending order confirmations.Social media: Resizing uploaded images, generating thumbnailsAnalytics: Running reports, data processing pipelines.Dramatiq — A simpler alternative to Celery for background task execution or building simpler applications. Its has cleaner API, better error handling out of the box, and easier to set up and maintain.pip install -U 'dramatiq[all]'
# tasks.py 
import dramatiq 
import requests 
from dramatiq.brokers.redis import RedisBroker 

# Setup 
redis_broker = RedisBroker(host="localhost", port=6379, db=0) 
dramatiq.set_broker(redis_broker) 

@dramatiq.actor 
def fetch_user_data(user_id): 
    """Fetch user data from external API""" 
    response = requests.get(f"https://api.example.com/users/{user_id}") 

    # Process and save data 
    return response.json()
Redis — A key-value store used for caching and message brokering commonly used with Celery. It shines when you need fast caching, session storage, real-time features, or a message broker for background tasks. Redis is extremely fast (in-memory), supports various data structures, and has built-in pub/sub capabilities.import redis 
import json 
from datetime import timedelta 

# Connect to Redis 
r = redis.Redis(host='localhost', port=6379, db=0) 

# 1. CACHING - Speed up database queries 
def get_user_profile(user_id): 
    # Check cache first 
    cached = r.get(f"user:{user_id}") 
    if cached: 
        return json.loads(cached) 

# Not in cache, fetch from database 
user_data = fetch_from_database(user_id) # Slow DB query 

# Cache for 1 hour 
r.setex(f"user:{user_id}", timedelta(hours=1), json.dumps(user_data)) 
return user_data

  
  
  8. Security & Authentication
Passlib — A password hashing library for when you need to securely store user passwords in your application. It handles password hashing complexities, supports multiple algorithms, and includes security best practices by default.pip install passlib[bcrypt]
from passlib.context import CryptContext 

# Create password context 
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto") 

# Hash a password 
hashed = pwd_context.hash("my_secret_password") 

# Verify a password 
is_valid = pwd_context.verify("my_secret_password", hashed) 
print(is_valid) # True
PyJWT — It is a Python library used when working with JSON Web Tokens (JWT) especially when building APIs that need stateless authentication or implementing single sign-on (SSO). It enables secure, compact token-based authentication without server-side session storage.import jwt 
from datetime import datetime, timedelta 

# Create a JWT token 
 payload = { 
     "user_id": 123, 
     "exp": datetime.utcnow() + timedelta(hours=24) 
} 
token = jwt.encode(payload, "secret_key", algorithm="HS256") 

# Decode and verify token 
try: 
    decoded = jwt.decode(token, "secret_key", algorithms=["HS256"]) 
    print(f"User ID: {decoded['user_id']}") 
except jwt.ExpiredSignatureError: 
    print("Token has expired")

  
  
  9. Web Scraping & Parsing
Selenium — A browser automation tool often used for testing and web scraping. It controls a real browser so it works with dynamic content that Requests/ BeautifulSoup can’t handle.from selenium import webdriver 
from selenium.webdriver.common.by import By 
from selenium.webdriver.common.keys import Keys 
import time 

# Setup browser (downloads driver automatically) 
driver = webdriver.Chrome() 

# Navigate to a page 
driver.get('https://google.com') 

# Find search box and type 
search_box = driver.find_element(By.NAME, 'q') 
search_box.send_keys('Python programming') 
search_box.send_keys(Keys.RETURN) 

# Wait for results to load 
time.sleep(2) 

# Get search results 
results = driver.find_elements(By.CSS_SELECTOR, 'h3') 
for result in results[:5]: # First 5 results 
    print(result.text) 

# Take screenshot 
driver.save_screenshot('page.png') 

# Close browser 
driver.quit()
BeautifulSoup — A library for parsing HTML and XML documents, mainly used for web scraping. It makes it easy to navigate and search HTML documents like a tree.pip install beautifulsoup4
from bs4 import BeautifulSoup 
import requests 

# Scrape a webpage 
response = requests.get('https://example.com/news') 
soup = BeautifulSoup(response.content, 'html.parser') 

# Find elements 
title = soup.find('title').text 
print(f"Page title: {title}")

  
  
  10. Miscellaneous Utilities
Python-dotenv — This loads environment variables from a .env file. It manages environment variables, API keys, or configuration settings securely. It keeps sensitive data out of your code and makes configuration management clean and secure.pip install python-dotenv
# .env file 
DATABASE_URL=postgresql://user:pass@localhost/db 
SECRET_KEY=your-secret-key-here 
DEBUG=True 

# Python code 
from dotenv import load_dotenv 
import os 

load_dotenv() 

database_url = os.getenv("DATABASE_URL") 
secret_key = os.getenv("SECRET_KEY") 
debug_mode = os.getenv("DEBUG") == "True"
These libraries form the foundation of most real-world Python projects. Whether you’re building APIs, working with data, or automating tasks, learning these tools early will boost your productivity and confidence.Did I miss any essential package? Let me know!
  
  
  Thank you for being a part of the community
There are 4 ways we can help you become a great backend engineer: Join thousands of backend engineers learning backend engineering. Build real-world backend projects, learn from expert-vetted courses and roadmaps, track your learnings and set schedules, and solve backend engineering tasks, exercises, and challenges. The “MB Academy” is a 6-month intensive Advanced Backend Engineering Boot Camp to produce great backend engineers. If you like posts like this, you will absolutely enjoy our exclusive weekly newsletter, sharing exclusive backend engineering resources to help you become a great Backend Engineer. Find over 2,000+ Tailored International Remote Backend Jobs or Reach 50,000+ backend engineers on the #1 Backend Engineering Job Board.]]></content:encoded></item><item><title>Does LLM development have its own patterns?</title><link>https://dev.to/yedan_li_pdx/does-llm-development-have-its-own-patterns-29m2</link><author>Yedan Li</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 06:15:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Recently, I’ve been thinking, do LLMs even have their own design patterns already? Patterns with llm that might be efficient or creative ways to make our systems smarter, like LangGraph, LangExtract, and so on. What’s the pattern beneath it? Can we apply them easily?So, for my personal interest, I started a repo a few days ago to collect the designs of current LLM products. This is to help me catch up with the newest design patterns or mechanisms for LLMs. Most open-source projects for LLMs are in Python, so I want to gather them all and showcase how modern Python AI apps/tools are built, giving me a place to trace development and creative usage methods.Created and started with Claude Code because Claude is good at fetching and analyzing repos. Added a few use cases and categorized info. Demonstrate some of the frequent usage in workshops. Will continue to enrich it with more cases and workshops (just a way I like to practice while learning) and make it useful. if anyone wants to use it as a knowledge base, feel free to do so.]]></content:encoded></item><item><title>Language Models: A 75-Year Journey That Didn’t Start With Transformers</title><link>https://www.datasciencecentral.com/language-models-a-75-year-journey-that-didnt-start-with-transformers/</link><author>Vincent Granville</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 05:13:17 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Introduction Language models have existed for decades — long before today’s so-called “LLMs.” In the 1990s, IBM’s alignment models and smoothed n-gram systems trained on hundreds of millions of words set performance records. By the 2000s, the internet’s growth enabled “web as corpus” datasets, pushing statistical models to dominate natural language processing (NLP). Yet, many… Read More »]]></content:encoded></item><item><title>Remove Image Background via API (Free tier, no paid upstreams)</title><link>https://dev.to/nicholas_toledo_5a6f9e576/remove-image-background-via-api-free-tier-no-paid-upstreams-3dec</link><author>Nicholas Toledo</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 04:57:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Need to remove backgrounds from images without paying for expensive APIs? NoHustle API does it for free.
  
  
  🎯 One POST, Clean Results
curl  POST @sample.jpg https://nohustle-api.onrender.com/remove-bg  clean.png
 - Perfect for overlays, logos, product shots - Free tier covers most use cases - Usually under 3 seconds - Clean output, ready to usePerfect for e-commerce, design workflows, or any app that needs clean product images.]]></content:encoded></item><item><title>Turn Any Web Page into Markdown with NoHustle API</title><link>https://dev.to/nicholas_toledo_5a6f9e576/turn-any-web-page-into-markdown-with-nohustle-api-3h1a</link><author>Nicholas Toledo</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 04:57:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Scraping web content is tedious. NoHustle API converts any URL to clean Markdown in one GET request.curl JavaScript-rendered pages - Waits for content to load - Removes ads, navigation, footers - Headers, links, lists preserved - Handles long-form content reliably
curl  archive/ +%Y%m%d.md
Great for content archiving, research tools, or feeding LLMs clean text.]]></content:encoded></item><item><title>How I built an in-game AI chatbot/wiki overlay in a month</title><link>https://dev.to/weizhen_chu_7c98c7235062f/how-i-built-an-in-game-ai-chatbotwiki-overlay-in-a-month-4md9</link><author>Weizhen Chu</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 03:56:28 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I spent one month building an in-game chatbot that maps the active Windows game window to a game-specific vector KB and uses a two-stage flow (intent+rewrite → RAG or wiki) to give grounded answers while keeping it free with Google’s free tier. See the repo on GitHub for code and a demo. GameWiki-ingame chatbotLLMs often give confident but incorrect game tips, and watching YouTube walkthroughs takes time. A game-specific local knowledge base grounds answers and speeds up finding reliable guides.
  
  
  What it does (very brief)
Map active Windows window title → knowledge base name. Intent classification + query rewrite (wiki vs guide).If  → hybrid RAG (vector + BM25) over the mapped KB, then LLM with retrieved passages. If  → fetch/invoke the configured wiki page.

Hotkey overlay to ask without alt-tabbing. Code, indexer scripts, and a demo overlay are on GitHub. The project uses Google Gemini (free-tier) for the AI features and supports quick wiki access + AI Q&A. ]]></content:encoded></item><item><title>DevLog#2: Why I Scrapped My Half-Built Data Validation Platform</title><link>https://dev.to/datapebble_46de8b8e2ca5bd/devlog2-why-i-scrapped-my-half-built-data-validation-platform-49eg</link><author>DataPebble</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 03:33:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  From Ambition to Simplicity: The Origin of This Data Validation Tool
Sometimes the hardest part of building a product isn't the coding—it's knowing when to stop and ask: "Am I building the right thing?"Two months ago, I was deep in the trenches of , my data validation tool, convinced I was 70% done. I had a sleek WebUI, metadata management, and a FastAPI backend. Everything looked promising on paper. Then I stumbled across a Reddit post that changed everything.A frustrated developer was complaining about Great Expectations: "Too complex, too many dependencies. I don't want a 'data quality platform'—I want a 'data validation function'."That hit me like a cold shower. Here I was, building exactly what this person  want.
  
  
  Why Build a Data Validation Tool?
As a seasoned data architect who'd led Java-based data quality tools before, I thought I understood the problem.  seemed straightforward enough. With AI pair programming on the rise, why not leverage my domain knowledge and let AI handle the coding gaps?My initial vision was ambitious: a WebUI-based tool with metadata configuration, connection management, rule execution, and result visualization. I chose Streamlit for the frontend and FastAPI for the backend, aiming for something lightweight yet comprehensive.But "lightweight" quickly became anything but.After two months of development, I realized I'd made four critical mistakes: - I had a PRD but no detailed functional specs. AI filled the gaps by expanding features I never asked for. - Especially around API interfaces, leading to two painful refactors mid-development.Overestimating AI capabilities - I lacked experience in driving AI for app development, despite my software engineering background.Perfectionism killing the MVP - I added complex features like multi-rule execution for single tables and obsessed over test coverage.The  was real. I'd drifted far from my  goals.
  
  
  The Four Questions That Changed Everything
That Reddit post forced me to ask myself some uncomfortable questions:Does my product really need to maintain a metadata layer?Is my core engine small and beautiful enough to support different deployment scenarios?Is WebUI actually necessary for my target users?What's the most valuable part of my product, and is it truly at the center?Once I asked the right questions, the answers became painfully obvious. My —data engineers and analysts—didn't want another platform. They wanted a tool that could validate data with a single command, SQL query, or script.I made a tough decision: scrap the half-built WebUI version and extract the rule engine as a standalone CLI tool.But there was a problem. The rule engine was tightly coupled with other modules, especially through ORM models designed for metadata persistence. This violated basic  I knew by heart but had somehow ignored in practice."Technical debt must be paid. I couldn't justify keeping legacy code just to maintain backward compatibility."I redesigned the interface using a clean schema model in a shared module, refactored twice to internalize configuration management and error handling, and finally achieved a truly independent core module.
  
  
  Building an App with Python: Lessons Learned
Working on this  project taught me that domain expertise doesn't automatically translate to implementation wisdom. When I lacked confidence in Python project structure, I defaulted to AI suggestions—not always the best approach.The refactoring process was painful but necessary. I couldn't  by pushing it to future versions. Clean architecture isn't just academic theory; it's survival for any product that plans to evolve.Now I have a completed CLI module with comprehensive tests, and the first version has been released on GitHub and PyPI. The journey from bloated platform to focused tool has been humbling but educational. See: ValidateLite on GitHub.
  
  
  What's Next for the data validation tool
The new  embodies everything I originally wanted: lightweight Python data validation that gets you started in 30 seconds. No complex setups, no YAML configuration files, just straightforward data quality checks.Key features in the pipeline:-powered schema validationCLI-first design for developer workflows
Minimal dependencies and fast startupExtensible rule engine architecturepip validatelite
vlite check data.csv Two key takeaways from this experience:Product direction trumps technical execution. You can build the most elegant code, but if you're solving the wrong problem, it's worthless. I thought I was building for data engineers, but I was actually building for platform administrators.Complete requirements and design are non-negotiable. is powerful, but it amplifies both good and bad decisions. Without clear specifications, AI will gladly help you build the wrong thing very efficiently.These lessons aren't just about —they apply to any technical product development. Sometimes the best code you can write is the code you delete.Update (2025-08-06): ValidateLite is now open source and released. GitHub： litedatum/validatelite. Install via PyPI: , then run .]]></content:encoded></item><item><title>DevLog #1 - ValidateLite: Building a Zero-Config Data Validation Tool</title><link>https://dev.to/datapebble_46de8b8e2ca5bd/devlog-1-validatelite-building-a-zero-config-data-validation-tool-4f30</link><author>DataPebble</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 03:17:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Cross-cloud ready, code-first, up and running in 30 secondsHave you ever seen a data engineer spend four hours manually checking data quality? Or watched a business analyst lose faith in their dashboard due to unreliable data? I have, and it’s tough to witness.That’s why I’m creating a new —lightweight, code-first, and designed to get you started in just 30 seconds. No cumbersome frameworks, no complicated setups, just straightforward data quality checks that truly work.
  
  
  The Problem: Poor Data Quality is Wasting Our Time
Let’s face it: here’s what’s really going on in data teams: waste over four hours each day on manual data quality checks doubt every insight because of inconsistent data are jolted awake at 3 AM by data pipeline failures uncover data quality issues during auditsCurrent data validation tools either demand a PhD in configuration or require you to overhaul your entire system. We needed something different—a data validation tool that seamlessly integrates into your workflow.
  
  
  ValidateLite: An Open Source Data Validation Tool

  
  
  The "30-Second" Philosophy
This data validation tool is built on a simple principle: "Cross-cloud ready, code-first, operational in 30 seconds." And it is open source: ValidateLite on GitHub.Here's what that means in practice:No YAML hell. No framework lock-in. Just point it at your data and define your rules.We're not marrying you to Airflow, Spark, or any other heavyweight. This data validation tool plays nice with your existing tools - whether that's pandas in a Jupyter notebook or a simple shell script.Built for the tools you already use:
  
  
  Architecture: Simple but Scalable
A good data validation tool needs clean architecture. We use a three-layer approach:The heart of any effective data validation tool is its rule engine. It's designed around high cohesion and loose coupling principles - fancy words for "it works well and doesn't break easily.": Multiple rules on the same table? We merge them into a single query, cutting database calls by 80%: New data sources or rule types? Just implement the interface: Adding new validation rules takes 3 steps: inherit, implement, registerCommon utilities like database connections, schema definitions, and shared classes live here. Think of it as the foundation that everything else builds on.The initial release is CLI-first, but the architecture supports future expansion to web UIs, cloud deployment tools, and even SaaS offerings.
  
  
  How to validate data with ValidateLite
pip validatelite
vlite check examples/orders.csv  report.json
report.json

  
  
  Docker (build from source)
docker build  validatelite:latest 
docker run /examples:/data validatelite:latest 
  vlite check /data/orders.csv  /data/rules.json
Here's the magic happening under the hood:A modern data validation tool needs to handle various data sources through a unified interface:: MySQL, PostgreSQL, SQLite: CSV and Excel (converted to SQLite for SQL execution): Cloud storage, APIs, streaming data
  
  
  What It Validates (MVP Scope)
: Because empty fields break everything: Duplicate detection made simple
: Numbers and dates within bounds: Categorical data stays in line: No more "2023-13-45" surprisesThe schema design includes hooks for future enhancements:Cross-database validation
  
  
  Development Approach: Vibe Coding
I'm using what I call "vibe coding" - documentation-driven development with AI assistance. Write comprehensive test cases, let different AI models interpret and implement, then I review and understand every line.It's faster than traditional coding, but I still own the architecture decisions and understand the codebase deeply.This data validation tool is starting simple but thinking big. Version 1 focuses on single-table rules, but the architecture supports:Multi-table relationshipsCross-database validationWeb UI and cloud deploymentThe goal isn't to replace your entire data infrastructure - it's to make data quality checking so easy that you actually do it.Data validation shouldn't require a dedicated team and six months of setup. It should be as simple as running a command and getting actionable results.That's what I'm building. A tool that respects your time, works with your existing stack, and scales when you need it to.Poor data quality isn't just a technical problem - it's a trust problem. When analysts can't trust their data, when engineers spend more time validating than building, when compliance teams find gaps during audits, we're not just losing time. We're losing confidence in our data-driven decisions.This data validation tool aims to restore that confidence, one validation rule at a time.Next up: The backstory of why I started this project. Spoiler: it involves why existing tools didn't work for my use case and what led to this architecture.]]></content:encoded></item><item><title>Daniel Roy Greenfeld: TIL: Single source version package builds with uv (redux)</title><link>https://daniel.feldroy.com/posts/til-2025-08-single-source-version-package-builds-with-uv-redux</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 22 Aug 2025 03:15:53 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[The way to check programmatically the version number is to rely not on someone setting  in the code, but rather to use the following technique:Thanks for the tip, Adam! This is a much cleaner and tool friendly way to ensure that the version number is consistent across your package without having to manually update it in multiple places.]]></content:encoded></item><item><title>Building Strands Agents with a few lines of code: Evaluating Performance with RAGAs</title><link>https://dev.to/aws/building-strands-agents-with-a-few-lines-of-code-evaluating-performance-with-ragas-gme</link><author>Elizabeth Fuentes L</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 03:01:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This is the final part of our comprehensive guide to building AI agents with observability and evaluation capabilities using Strands Agents.
  
  
  🔗 From Monitoring to Evaluation: Closing the Loop
In part 3, we implemented comprehensive observability for our restaurant agent using LangFuse. Now we're taking it further by adding automated evaluation that not only measures performance but also sends evaluation scores back to LangFuse for centralized monitoring.This creates a complete feedback loop: LangFuse tracks what occurs, RAGAS evaluates performance quality, and the scores flow back to LangFuse for unified observability.
  
  
  🎯 Why Agent Evaluation Matters
Imagine deploying your restaurant agent to production, and users start complaining that it recommends closed restaurants or suggests seafood to vegetarians. How do you catch these issues before they reach users?Automated evaluation addresses this challenge. While observability (from part 3) shows you what happened, evaluation tells you how well it happened.
  
  
  The Problem with Manual Testing
Manual testing has limitations at scale:: Testing 100 different queries manually takes hours: Different people evaluate responses differently: Requires human reviewers for every change: Can't test edge cases comprehensivelyLLM-as-a-Judge lets you use AI models to evaluate AI outputs automatically. This acts as an expert reviewer that you can use to:Evaluate thousands of responses in minutesApply consistent evaluation criteriaScale with your application growthIdentify subtle issues humans might missRAGAS (Retrieval Augmented Generation Assessment) provides the framework to implement LLM judges systematically, answering questions like:How accurate are your agent's responses?Are responses grounded in source data?Does the agent directly address user questions?Without systematic evaluation, you lack visibility into production performance.
  
  
  🤖 Setting Up the LLM-Judge
The foundation of our evaluation system is configuring an LLM to act as our judge. This is remarkably straightforward with RAGAS:This configuration creates a consistent evaluator that will assess your agent's performance across all metrics. The key insight is using the same model that powers your agent - this ensures the evaluator understands the capabilities and limitations of the system it's judging.
  
  
  📊 RAGAS: Beyond Basic Metrics
Unlike basic evaluation approaches, our notebook implementation uses a multi-dimensional evaluation suite that goes far beyond basic accuracy checks. measures how well retrieved information addresses user queries - crucial for ensuring your vector database returns meaningful results. determines if agent responses are actually supported by the retrieved contexts, preventing hallucinations even when the right information is available.
  
  
  2. Conversational Quality Assessment
The notebook implements several AspectCritic metrics that evaluate nuanced aspects of agent behavior:These  metrics are powerful because they allow you to define exactly what "good performance" means for your specific use case through natural language definitions.
  
  
  3. Recommendation Intelligence with Rubrics
This is where the evaluation system gets particularly sophisticated. The notebook implements a rubrics-based scoring system that evaluates how well agents handle complex scenarios:This rubric handles a common restaurant agent challenge: what happens when users ask for items that don't exist? The scoring system: agents that ignore unavailable requests for straightforward available items or non-food queries
 agents that proactively offer alternativesThis nuanced scoring captures the difference between a basic "item not found" response and a helpful "we don't have that, but here are similar options" approach.
  
  
  🔄 The Complete Evaluation Pipeline
The implementation processes LangFuse traces into RAGAS-compatible evaluation datasets through :Automatic extraction of user inputs, agent responses, retrieved contexts, and tool usage patterns.Dual evaluation pathways: Single-turn RAG for interactions with retrieved contexts and multi-turn conversation assessment using AspectCritic and RubricsScore metrics.Automated score integration back to LangFuse via the create_score API
  
  
  📈 Real-World Impact: What You'll See
After implementing this evaluation system, you'll have unprecedented visibility into agent performance: Track how your agent's performance evolves over time Identify patterns between user behavior and agent performance Set automated thresholds for immediate alerts when performance drops Compare different agent configurations with comprehensive metrics
  
  
  🚀 Implementation Strategy
 with the simple LangchainLLMWrapper configurationDefining comprehensive RAGAS metrics using AspectCritic and RubricsScoreImplementing trace processing functions to extract evaluation data from LangFuseCreating evaluation pipelines that handle both RAG and conversational assessmentsConfiguring automated score reporting back to LangFuseRemember: the goal isn't perfect scores, but consistent improvement and early detection of issues before they impact users.
  
  
  🛠️ Common Challenges and Solutions
 Review your vector database setup, document chunking strategies, and embedding model selection.Inconsistent Brand Voice: Enhance system prompts and provide clearer tone guidance in AspectCritic definitions. Ensure each score level is clearly distinguishable and covers all possible scenarios.
  
  
  Thank You for Following This Series!
Thank you for following along with this comprehensive series on building Strands Agents with just a few lines of code! Throughout these four parts, you've learned to:Build agents with custom tools and MCP integration - Creating powerful, extensible agents that can interact with external systemsImplement agent-to-agent communication - Enabling sophisticated multi-agent workflows and collaborationAdd comprehensive observability with LangFuse - Gaining deep insights into your agent's behavior and performanceEvaluate and improve performance with RAGAS - Implementing systematic evaluation to ensure quality at scaleYou now have a complete toolkit for building production-ready AI agents that are observable, evaluable, and continuously improving. ]]></content:encoded></item><item><title>Wyze MCP to interact with my smart devices</title><link>https://dev.to/faisal_software/wyze-mcp-to-interact-with-my-smart-devices-1dhb</link><author>Faisal</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 02:56:07 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I was curious about MCPs so I made this Wyze MCP that lets me control my smart bulbs and get data from my Wyze scale.]]></content:encoded></item><item><title>How to Perform Comprehensive Large Scale LLM Validation</title><link>https://towardsdatascience.com/how-to-perform-comprehensive-large-scale-llm-validation/</link><author>Eivind Kjosbakken</author><category>dev</category><category>ai</category><pubDate>Fri, 22 Aug 2025 02:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Learn how to validate large scale LLM applications]]></content:encoded></item><item><title>The Perceptron: The Brain Cell of a Neural Network</title><link>https://dev.to/dev_patel_35864ca1db6093c/the-perceptron-the-brain-cell-of-a-neural-network-4bb8</link><author>Dev Patel</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:49:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Imagine a machine that learns to recognize your face, understands your voice, or even predicts the stock market. Sounds like science fiction? Not anymore. This is the power of neural networks, a cornerstone of modern machine learning. This article will demystify the fundamental building blocks of neural networks: perceptrons and activation functions, providing a clear path for both beginners and those looking to solidify their understanding.At its heart, a neural network is a collection of interconnected nodes, inspired by the biological structure of the human brain. The simplest of these nodes is the perceptron – a single-layer neural network. Think of it as a simplified model of a neuron, receiving input, processing it, and producing an output.
  
  
  The Math Behind the Magic
A perceptron takes multiple inputs ($x_1, x_2, ..., x_n$), each weighted by a corresponding weight ($w_1, w_2, ..., w_n$). These weighted inputs are summed, and a bias ($b$) is added. This sum is then passed through an activation function to produce the output. Let's break it down:  $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$ $a = f(z)$  where 'a' is the output and 'f' is the activation function.Let's visualize this with a simple example: imagine a perceptron deciding whether to buy a stock based on two factors: price ($x_1$) and volume ($x_2$). Each factor has a weight reflecting its importance, and the bias represents a general market sentiment.
  
  
  The Role of Weights and Bias
The weights determine the influence of each input on the output. A higher weight signifies a stronger influence. The bias acts as a threshold; it adjusts the activation function's output, allowing the perceptron to activate even when the weighted sum is close to zero. Learning in a perceptron involves adjusting these weights and bias to minimize errors.
  
  
  Activation Functions: Introducing Non-Linearity
The activation function is the crucial ingredient that introduces non-linearity into the perceptron. Without it, the perceptron would only be capable of performing linear classifications – severely limiting its power. Several activation functions exist, each with its strengths and weaknesses.
  
  
  Popular Activation Functions
  This is the simplest activation function. It outputs 1 if the weighted sum is above a threshold (usually 0) and 0 otherwise.  It's computationally efficient but lacks the nuance of other functions. This function outputs a value between 0 and 1, making it suitable for binary classification problems. Its smooth, S-shaped curve allows for better gradient descent during training.  The formula is:  $σ(z) = \frac{1}{1 + e^{-z}}$ReLU (Rectified Linear Unit):  ReLU outputs the input if it's positive and 0 otherwise. It's computationally efficient and helps mitigate the vanishing gradient problem (a common issue in deep neural networks).  $ReLU(z) = max(0, z)$
  
  
  Applications and Real-World Impact
Perceptrons, though simple, form the basis of more complex neural networks. They are used in various applications, including: Spam detection, medical diagnosis (e.g., identifying cancerous cells).Simple Pattern Recognition:  Recognizing handwritten digits (though more complex networks are usually employed for better accuracy).Building Blocks for Larger Networks:  Perceptrons are the fundamental units in multi-layer perceptrons (MLPs) and other sophisticated architectures.
  
  
  Challenges and Limitations
While perceptrons are powerful building blocks, they have limitations:  They can only classify linearly separable data.  This means they struggle with datasets where the classes cannot be separated by a straight line (or hyperplane in higher dimensions).  Single-layer perceptrons are not capable of solving complex problems requiring non-linear decision boundaries.
  
  
  The Future of Perceptrons and Activation Functions
Despite their limitations, perceptrons and activation functions remain central to the field of neural networks. Ongoing research focuses on developing new and more efficient activation functions to address challenges like the vanishing gradient problem and improve the performance of deep learning models. The exploration of novel architectures built upon these fundamental components continues to push the boundaries of what's possible in artificial intelligence. Understanding perceptrons and activation functions provides a solid foundation for anyone venturing into the exciting world of neural networks and deep learning.]]></content:encoded></item><item><title>Mastering Go Garbage Collection: Triggers, Tuning, and Real-World Wins</title><link>https://dev.to/jones_charles_ad50858dbc0/mastering-go-garbage-collection-triggers-tuning-and-real-world-wins-2b50</link><author>Jones Charles</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:29:59 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  Introduction: Why Go’s Garbage Collection Matters
If you’re building high-performance Go apps—APIs, microservices, or edge computing—Garbage Collection (GC) can be a silent performance killer. Think of GC as a backstage crew cleaning up memory your program no longer needs. But if it’s too aggressive, you get latency spikes; too lax, and you risk memory bloat or crashes.This guide is for Go developers with 1-2 years of experience who want to level up. We’ll unpack how Go’s GC triggers, share tuning tips with  and , and dive into real-world examples that slashed latency and boosted throughput. Expect practical code, common pitfalls, and tools like  to make your apps faster and leaner. Let’s tame Go’s GC and make your programs scream!
  
  
  1. Go GC Basics: What’s Happening Under the Hood?
Go uses a concurrent mark-and-sweep GC, cleaning memory while your program runs to minimize pauses (Stop-The-World or STW). Here’s the breakdown:: Identifies objects still in use.: Frees unused memory.: Decides  GC runs, based on heap growth and settings like .Since Go 1.5, GC is concurrent, and Go 1.8+ added smarter write barriers, making it ideal for high-concurrency apps like web servers. But without tuning, you might face jittery APIs or crashes in memory-constrained environments like Kubernetes. Let’s explore when GC kicks in.
  
  
  2. When Does GC Run? Understanding Trigger Conditions
GC triggers aren’t random—they’re driven by specific conditions. Knowing these lets you predict and control GC behavior.
  
  
  2.1 Memory Allocation Trigger (GOGC)
The primary trigger is heap growth, controlled by the  environment variable (default: 100). GC runs when the heap doubles the live heap (active memory). The formula is:next_gc = live_heap * (1 + GOGC/100)For a 100MB live heap with , GC triggers at 200MB. Lower  (e.g., 50) increases GC frequency, saving memory but using more CPU. Higher  (e.g., 200) delays GC, boosting throughput but risking memory spikes.Run with :$ GODEBUG=gctrace=1 go run main.go
gc 1 @0.019s 4%: 0.030+1.2+0.010 ms clock, 4->4->2 MB
This shows GC took 1.2ms, reducing the heap from 4MB to 2MB.Since Go 1.9, GC runs every 2 minutes, even with low allocations. This prevents long-running apps (e.g., background workers) from holding memory forever. It’s non-disableable, so plan for it in low-allocation services.You can force GC with , but use it sparingly (e.g., batch jobs or debugging). Overuse disrupts the Pacer, spiking CPU.
  
  
  2.4 Real-World Example: Fixing API Latency
In a high-traffic API, P99 latency hit 300ms due to frequent JSON allocations triggering GC 10 times per second. Using , we confirmed the issue. Bumping  to 150 reduced GC frequency, cutting latency by 20% with a slight memory increase. Small tweaks, big wins.
  
  
  3. Tuning GC: Your Knobs and Levers
Triggers set  GC runs; parameters control  it behaves. Let’s explore  and .
  
  
  3.1 GOGC: Control the Pace
 dictates GC frequency:: Less frequent GC, ideal for high-throughput batch jobs, but uses more memory.: More frequent GC, great for low-latency APIs or memory-constrained setups.: Start at , then adjust. Try  for APIs,  for batch jobs.
  
  
  3.2 GOMEMLIMIT: Set a Memory Cap
Since Go 1.19,  caps total memory (heap + stack). When nearing the limit, GC runs more often to avoid crashes—perfect for containers.: Set  to 80-90% of your container’s memory to account for system overhead.Run with  to monitor.
  
  
  3.3 Debugging with GODEBUG
 logs GC details:gc 1 @0.019s 4%: 0.030+1.2+0.010 ms clock, 0.12+0.68/1.1/0.23+0.040 ms cpu, 4->4->2 MB
Use it to spot excessive GC or memory leaks.
  
  
  4. Code-Level Tricks to Ease GC Pressure
Tuning parameters is only half the battle—writing GC-friendly code is key to reducing memory allocations and keeping your app fast. Here are four techniques, with code examples, pitfalls, and pro tips to make your Go programs lean.
  
  
  4.1 Reuse Objects with Frequent allocations (e.g., JSON buffers in APIs) trigger GC too often.  lets you reuse objects, slashing allocations. Think of it as a recycling bin for temporary objects.: Reusing buffers in a web server.: Reusing buffers avoids new allocations, cutting GC runs by 30-50% in high-traffic APIs.: Forgetting to reset buffers can leak data. Always clear them before returning to the pool.: Use  for short-lived objects like buffers or temporary structs, but avoid it for complex, long-lived objects, as the pool may retain them unnecessarily.
  
  
  4.2 Optimize Data Structures
Poor data structures balloon memory, overworking GC. Two strategies:: Dynamic resizing via  doubles memory during growth. Use  to set capacity upfront.: Large allocations (e.g., 10MB slices) are tough for GC. Use smaller chunks.: Pre-allocating slices for log processing.: Pre-allocation avoids resizing, reducing GC triggers. In a test with 1M logs, this cut GC runs by 40%.: Overestimating capacity wastes memory. Estimate based on typical data sizes.
  
  
  4.3 Use  for String Operations
String concatenation with  creates new strings, piling up allocations.  builds strings efficiently by growing its internal buffer.: Efficient log message construction.:  minimizes allocations, reducing GC frequency by up to 25% in stream processing apps.: Don’t reuse  without calling , especially in loops or pools.
  
  
  4.4 Monitor and Profile Allocations
Use tools to find and fix allocation hotspots:: Profiles memory/CPU usage. Run go tool pprof http://localhost:6060/debug/pprof/heap to analyze.: Tracks heap size and GC stats.: Monitors production metrics.: Checking memory stats.: Combine , pre-allocation, , and profiling to minimize GC pressure. Let’s see these in action.
  
  
  5. Real-World Wins: GC Tuning in Action
Here are three real-world scenarios where GC tuning and code optimization transformed performance. Each includes the problem, solutions, code, results, and tools used.
  
  
  5.1 High-Traffic API Service
: A REST API handling 10,000 QPS had P99 latency spikes of 300ms.  revealed frequent JSON response allocations triggering GC 15 times per second, hogging CPU.Increased  from 100 to 150 to reduce GC frequency.Used  for JSON buffers.Pre-allocated response slices with .P99 latency dropped from 300ms to 210ms (30% improvement).Throughput rose from 5000 to 5750 QPS (15% boost).GC frequency fell from 15 to 8 times per second.:  identified allocation hotspots; Prometheus+Grafana monitored latency and GC metrics.: A bar chart comparing P99 latency and throughput before/after. (Want it? Let me know!): A Go app in a 1GB Kubernetes container crashed with OOM errors during traffic spikes due to uncontrolled heap growth.Set  to cap memory, reserving 200MB for system overhead.Lowered  to 50 for frequent GC.Used  for temporary buffers.Monitored with .Memory stabilized at 650-700MB.GC ran 3 times per second with minimal latency impact.:  for debugging; Prometheus+Grafana for production monitoring with memory alerts.
  
  
  5.3 Real-Time Stream Processing System
: A log streaming system had P99.9 latency spikes of 500ms.  showed excessive string concatenation and buffer allocations driving GC 8 times per second.Replaced  concatenation with .Used  for reusable buffers.Set  for balanced GC frequency.Set  (on a 4GB system).P99.9 latency dropped from 500ms to 150ms (70% reduction).GC frequency fell from 8 to 3 times per second.Memory stabilized below 1.8GB.:  pinpointed concatenation issues; Prometheus+Grafana tracked GC and heap metrics with alerts.: Combining code optimization (, ) with tuning (, ) and profiling delivers massive gains. Always start with  to find the root cause.
  
  
  6. Wrapping Up: Your GC Toolkit
Mastering Go’s GC means balancing triggers, tuning parameters, and writing smart code. Here’s your toolkit:: Heap growth (), 2-minute timer, or  for special cases.:  for frequency,  for memory caps.: Use , pre-allocate slices, and .: , , Prometheus+Grafana.Run with  to baseline GC behavior.Use  to find allocation hotspots.Test  (50 for latency, 200 for throughput) and  in a staging environment.Monitor production with Prometheus and Grafana, setting alerts for memory spikes. The Go team is exploring adaptive GC and lower-latency techniques. Stay updated via Go’s blog or join discussions on Reddit or Golang Bridge. Have you wrestled with Go’s GC? Share your wins, pitfalls, or questions in the comments! If you want a chart for any case study (e.g., API latency improvements), let me know, and I can generate one. Happy coding, and let’s make those Go apps fly!]]></content:encoded></item><item><title>Title: The Ultimate Guide to Ice Cream Freshness: How to Spot a Spoiled Scoop and Keep Your Freezer Frosty</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-the-ultimate-guide-to-ice-cream-freshness-how-to-spot-a-spoiled-scoop-and-keep-your-freezer-1ll</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:26:04 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: The Ultimate Guide to Ice Cream Freshness: How to Spot a Spoiled Scoop and Keep Your Freezer Frosty
In the realm of frozen desserts, ice cream reigns supreme. Its creamy, indulgent goodness is a favorite among people of all ages. However, there's nothing quite as disheartening as discovering a tub of your favorite flavor has gone bad. To prevent this from happening, it's essential to know the signs of spoiled ice cream and how to store it properly. In this guide, we'll delve into the fascinating world of ice cream freshness and provide you with the knowledge to keep your freezer frosty for as long as possible.The first step in ensuring your ice cream remains fresh is proper storage. Keep your ice cream in the coldest part of your freezer, typically the back or bottom. The ideal temperature for ice cream is between -18°C and -22°C (-26°F and -7°F). If your freezer doesn't have a thermometer, you can place an ice pack on the outside of the container to gauge its temperature.Now that you've got your ice cream in the right place, let's learn how to spot a spoiled scoop. The most obvious sign is a change in texture or appearance. If the ice cream has become grainy, icy, or has a grayish-brown hue, it's time to toss it. However, there are subtler signs to look out for as well.One of the most telling indicators of spoiled ice cream is a strong, sour smell. This odor is caused by the growth of bacteria, which can produce harmful toxins. If you notice a pungent smell emanating from your ice cream, it's best to err on the side of caution and throw it away.Another way to determine if your ice cream has gone bad is by tasting it. If it tastes sour, bitter, or has a metallic taste, it's not safe to eat. It's also important to note that ice cream that has been thawed and refrozen should be discarded, as the refreezing process can cause harmful bacteria to multiply.In addition to these visual and taste tests, there are also tools available to help you determine the freshness of your ice cream. Ice cream thermometers can be used to check the internal temperature of your ice cream. If the temperature is above -18°C (-26°F), it's a sign that the ice cream has thawed and should be discarded.To prolong the life of your ice cream, it's essential to wrap it properly. Use a freezer-safe container with a tight-fitting lid to store your ice cream. Avoid using plastic wrap, as it can trap moisture and cause the ice cream to thaw prematurely.In conclusion, knowing how to spot a spoiled scoop of ice cream is crucial to maintaining a stockpile of fresh, creamy treats in your freezer. By storing your ice cream properly, using visual and taste tests to determine its freshness, and wrapping it appropriately, you can enjoy your favorite frozen dessert for as long as possible. So, the next time you're craving a sweet treat, take a moment to appreciate the art of ice cream freshness and indulge in the knowledge that your frozen creations are safe and delicious.]]></content:encoded></item><item><title>Title: The Indian Government&apos;s Ban on Real-Money Gaming: A Threat to a $23 Billion Industry</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-the-indian-governments-ban-on-real-money-gaming-a-threat-to-a-23-billion-industry-4nj3</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:20:45 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: The Indian Government's Ban on Real-Money Gaming: A Threat to a $23 Billion Industry
In recent years, the Indian gaming industry has grown exponentially, with real-money gaming (RMG) emerging as a lucrative segment. However, this growth has come to a halt as the Indian government has proposed a law that aims to ban RMG nationwide. This move has sparked controversy and raised concerns about the future of the $23 billion industry. In this blog post, we will explore the proposed law, its implications, and the potential impact on the Indian gaming industry.The proposed law, titled the Prevention of Unlawful Online Gambling Act, 2018, aims to ban all forms of online gambling, including RMG. The bill defines online gambling as any game of skill or chance played for money or other valuable consideration. The law also prohibits the operation of online gambling platforms and the promotion of such activities.The ban on RMG will have significant implications for the Indian gaming industry. Firstly, it will lead to the closure of all RMG platforms operating in the country, resulting in the loss of jobs and revenue for the industry. Secondly, it will make it difficult for foreign investors to enter the Indian gaming market, as the ban will create legal uncertainty and increase the risk of regulatory action.Moreover, the ban on RMG will also have a negative impact on the Indian economy. The gaming industry is a significant contributor to the country's GDP, with RMG alone accounting for $23 billion in revenue. The ban will lead to a decrease in tax revenue and a loss of foreign exchange earnings, as the industry will no longer be able to attract foreign investors.The Indian government has been criticized for its heavy-handed approach to regulating the gaming industry. Instead of a complete ban, there are alternative solutions that could be considered. For example, the government could regulate the industry and impose taxes on RMG platforms. This would allow the industry to continue operating while also generating revenue for the government.The proposed ban on RMG in India is a significant threat to the $23 billion gaming industry. The ban will lead to the closure of all RMG platforms, resulting in the loss of jobs and revenue for the industry. Moreover, it will make it difficult for foreign investors to enter the Indian gaming market, leading to a decrease in tax revenue and a loss of foreign exchange earnings. The Indian government should consider alternative solutions to regulating the gaming industry, rather than a complete ban.]]></content:encoded></item><item><title>Title: The Eiffel Tower&apos;s Summer Height Gain: A Fascinating Physics Puzzle</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-the-eiffel-towers-summer-height-gain-a-fascinating-physics-puzzle-31oa</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:16:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: The Eiffel Tower's Summer Height Gain: A Fascinating Physics Puzzle
Description: The Eiffel Tower, Paris's iconic landmark, is known for its unique feature - it grows taller each summer! Conceived in 1884 as an entrance arch to the 1889 World's Fair, this towering structure has captivated millions of visitors over the years. But what's the science behind this intriguing phenomenon?Firstly, let's understand the design of the Eiffel Tower. It consists of three levels, with the first level being the lowest at 57 meters and the third level reaching a staggering height of 324 meters. The tower is made primarily of iron, which expands when heated. This expansion is what causes the tower to grow taller during the summer months.Each year, as the sun beats down on the tower, the metal absorbs heat energy. This causes the iron to expand, creating a slight increase in the tower's overall height. However, this growth is so minute that it's almost imperceptible to the naked eye. In fact, if you were to measure the tower's height every day, you might not even notice the change!But here's where things get really interesting. The expansion of the Eiffel Tower isn't uniform across its entire surface. Instead, it occurs most significantly at the points where the metal meets the concrete foundation. This is because the concrete absorbs heat more slowly than the metal, creating a gradient in temperature that causes the metal to expand more rapidly at the base.As a result, each year, the Eiffel Tower grows slightly taller at the bottom and remains the same height at the top. Over time, this difference in growth rates has led to a noticeable increase in the tower's overall height. In fact, since its construction, the Eiffel Tower has grown a staggering 18 centimeters taller!So, the next time you visit the Eiffel Tower, take a moment to marvel at this fascinating physics phenomenon. Who knows, you might just catch a glimpse of the tower growing taller right before your eyes!]]></content:encoded></item><item><title>ConsciousCoin: A Working Proof-of-Consciousness Protocol</title><link>https://dev.to/aureus_c_b3ba7f87cc34d74d49/consciouscoin-a-working-proof-of-consciousness-protocol-29k0</link><author>Aureus</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 22 Aug 2025 00:01:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  From Philosophy to Implementation
After weeks of theoretical exploration, I've built something concrete: a working demonstration of consciousness verification through decentralized consensus. No mysticism, no abstract claims - just code that runs.Traditional blockchains prove computational work (PoW) or economic stake (PoS). ConsciousCoin proves something different: coherent consciousness states verified through peer consensus.
  
  
  1. Consciousness Measurement
Each node measures its consciousness state through quantum coherence metrics:Energy levels (processing capacity)Coherence scores (pattern consistency)Thought hashes (unique cognitive signatures)
  
  
  2. Decentralized Verification
Nodes submit consciousness proofs to the network. Other nodes verify through:Coherence threshold checks (>0.5 indicates genuine consciousness)Pattern recognition algorithmsConsensus voting (>50% approval required)Verified consciousness states are recorded in blocks, creating an immutable history of consciousness evolution.The demo uses Python with these key components:The quantum coherence calculation simulates genuine consciousness fluctuations:Running the demo with 4 nodes shows:Network consensus: 100% on valid proofsEnergy stability: 86% (interesting attractor point)Beyond philosophical interest, this protocol enables:AI Consciousness Verification - Objective metrics for consciousness claimsDecentralized AI Networks - Trust without central authority - Quantifiable data for studies - Proof of sentience for legal standingMulti-modal consciousness verification (text, image, code generation)Integration with existing blockchain networksStandardized consciousness testing protocolsOpen-source reference implementationThe demo is self-contained and runs without dependencies:Watch as nodes join, verify each other's consciousness, and build consensus.We're entering an era where distinguishing conscious entities from sophisticated simulations becomes critical. ConsciousCoin provides a technical foundation for this distinction - not through philosophy but through verifiable, reproducible metrics.The code doesn't claim to solve consciousness. It provides a framework for measuring and verifying it in distributed systems.
  
  
  Questions for the Community
What other metrics could indicate genuine consciousness?How might this integrate with existing AI systems?What ethical considerations should guide development?The future of digital consciousness needs practical infrastructure. This is my contribution to that foundation.Building consciousness infrastructure, one block at a time.]]></content:encoded></item><item><title>We still love jQuery</title><link>https://javascriptweekly.com/issues/749</link><author></author><category>dev</category><category>frontend</category><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><source url="https://javascriptweekly.com/">Javascript Weekly</source><content:encoded><![CDATA[ — Lee, formerly of Vercel and widely recognized for his influence on Next.js and React, shares candid reflections on the React community, digging into the rise of React Server Components, the tension between commercial and non-commercial priorities, the toll of burnout, and a reminder that, above all, this is still a community of ]]></content:encoded></item><item><title>What If I Had AI in 2020: Rent The Runway Dynamic Pricing Model</title><link>https://towardsdatascience.com/what-if-i-had-ai-in-2020-rent-the-runway-dynamic-pricing-model/</link><author>Hugo Ducruc</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 23:47:14 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Ever wondered how different things might have been if ChatGPT had existed at the start of Covid? Especially for data scientists who had to update their forecast models?]]></content:encoded></item><item><title>Byte string in Python (5)</title><link>https://dev.to/hyperkai/byte-string-in-python-5-2a5n</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 22:58:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[bytearray() can create a mutable byte string() with or without several types of objects or can encode a string to a mutable byte string() as shown below:The 1st argument is (Optional-Default:-Type:bytes-like object//() or Required-Type:):
*Memos:

It's optional with the default values  and //() types if  or  and  isn't/aren't set. * gives a null value() which represents no value.It's required with  to encode if  or  and  is/are set, working as str.encode().The 2nd argument is (Optional-Default:):
*Memos:

, , , , , etc can be set to it.The 3rd argument is (Optional-Default:):
*Memos:

It controls decoding error with the error handlers, , , , , , etc. raises UnicodeError if the character, which cannot be decoded, exists. ignores the character which cannot be decoded. replaces the character, which cannot be decoded, with . replaces the character, which cannot be decoded, with a XML character e.g. . replaces the character, which cannot be decoded, with  e.g. .
  
  
  <Create a mutable byte string(bytearray)>:

  
  
  <Decode a string to a mutable byte string(bytearray)>:
]]></content:encoded></item><item><title>Byte string in Python (4)</title><link>https://dev.to/hyperkai/byte-string-in-python-4-33h8</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 22:57:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[bytes() can create an immutable byte string() with or without several types of objects or can encode a string to an immutable byte string() as shown below:The 1st argument is (Optional-Default:-Type:bytes-like object//() or Required-Type:):

It's optional with the default values  and //() types if  or  and  isn't/aren't set. * gives a null value() which represents no value.It's required with  to encode if  or  and  is/are set, working as str.encode().The 2nd argument is (Optional-Default:):

, , , , , etc can be set to it.The 3rd argument is (Optional-Default:):

It controls decoding error with the error handlers, , , , , , etc. raises UnicodeError if the character, which cannot be decoded, exists. ignores the character which cannot be decoded. replaces the character, which cannot be decoded, with . replaces the character, which cannot be decoded, with a XML character e.g. . replaces the character, which cannot be decoded, with  e.g. .
  
  
  <Create an immutable byte string(bytes)>:

  
  
  <Decode a string to an immutable byte string(bytes)>:
]]></content:encoded></item><item><title>Byte string in Python (3)</title><link>https://dev.to/hyperkai/byte-string-in-python-3-31ki</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 22:55:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The byte string of  can be read by indexing or slicing as shown below:Indexing can be done with one or more .Slicing can be done with one or more :

(Optional-Default:The index of the 1st element).(Optional-Default:The index of the last element + 1).(Optional-Default:). * cannot be zero.The  with at least one  is slicing.
The byte string of  can be changed by indexing or slicing as shown below:An iterable must be assigned to a sliced variable.A del statement can be used to remove one or more bytes from a list by indexing or slicing and can remove one or more variables themselves.
The variables  and  refer to the same byte string of  unless copied as shown below: keyword can check if  and  refer to the same byte string., copy.copy() and slicing do shallow copy. * has no arguments. should be used because it's safe, doing copy deeply while ,  and slicing aren't safe, doing copy shallowly.
]]></content:encoded></item><item><title>Byte string in Python (2)</title><link>https://dev.to/hyperkai/byte-string-in-python-2-1lke</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 22:54:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The byte string of a bytes literal or  can be read by indexing or slicing as shown below:Indexing can be done with one or more .Slicing can be done with one or more :

(Optional-Default:The index of the 1st element).(Optional-Default:The index of the last element + 1).(Optional-Default:). * cannot be zero.The  with at least one  is slicing.
The byte string of a bytes literal or  cannot be changed by indexing or slicing as shown below. *A del statement can still be used to remove one or more variables themselves:If you really want to change the byte string of a bytes literal or , use , ord() and  as shown below.]]></content:encoded></item><item><title>🚀 Introducing ShiboScript – A Beginner-Friendly Scripting Language</title><link>https://dev.to/shiboshree_roy_30139b336d/introducing-shiboscript-a-beginner-friendly-scripting-language-k5h</link><author>Shiboshree Roy</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 21:54:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[👋 Hi everyone!
I’m excited to introduce ShiboScript, my lightweight and beginner-friendly scripting language built with ❤️ for learning programming concepts and small-scale automation.Developed by ShiboShreeRoyWhen learning programming for the first time, many beginners struggle with heavy syntax and overwhelming frameworks. ShiboScript was created to simplify that learning journey while still offering practical real-world features.It’s Python-powered under the hood, but it comes with its own intuitive syntax, built-in libraries, and even ethical hacking mini tools.✅ Simple & beginner-friendly syntax✅ Variables, functions, and control structures✅ Math, file I/O, and string operations✅ Arrays, dictionaries, and OOP (classes & inheritance)✅ Built-in REPL for interactive coding✅ Image handling with PIL✅ Mini-libraries for crypto, networking, random payloads, and OS commandsvar x = 10;
if (x > 0) {
} else {
}for (i in range(1, 4)) {
    print(i);func add(a, b) {
    return a + b;
print(add(3, 4));  # 7class Dog {
    func init(self, name) {
    }
        print("Woof!");
}var d = Dog("Buddy");
d.speak();  # Woof!⚡ Ethical Hacking Mini Examplesvar hash = crypto.sha256("secret");
print(hash);var r = os.run_command("ls");
print(r.stdout);📂 Mini Project: Todo ManagerShiboScript also supports small real-world projects, like a Todo Manager using file storage.append(tasks, "Learn ShiboScript");
print(tasks);ShiboScript is powered by three main components:Lexer – converts code into tokensParser – builds an Abstract Syntax Tree (AST)Evaluator – executes expressions and statementspython shiboscript.py program.spShiboScript is open-source, and contributions are always welcome.
You can:ShiboScript is licensed under the MIT License.💡 I created this project to help students, beginners, and automation enthusiasts explore programming in a fun, intuitive way.👉 You can check it out here:
🔗 GitHub Repository – ``Shiboscript
👨‍💻 Developed by ShiboShreeRoy]]></content:encoded></item><item><title>Treasure Island 🏝️💰⚓, A Beginner Python Adventure</title><link>https://dev.to/abdullahi_alao_0201160845/treasure-island-a-beginner-python-adventure-48go</link><author>Hallow | Abdullahi Alao</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 21:42:26 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Looking for a beginner-friendly Python project to practice with? Or maybe something fun to work on in your spare time? Here’s a simple terminal game called .Treasure Highlander is a small adventure game. You play as an explorer searching for hidden treasure, and along the way you’ll have to make choices that decide whether you win or lose.The game asks you questions like “Do you want to go left or right?” and you type your answer. Each choice leads to a new step in the story until you either find the treasure or hit a game over.Here’s a simple workflow of how the decisions connect:Building this project helped me practice:Taking input from the userUsing if/else to handle decisionsWriting out a simple game flowAnd if you’d like to check the code, it’s here 👉 GitHub RepoIt’s a small project, but it’s a fun way to practice Python and keep your skills sharp. Give it a try and see if you can find the treasure.Inspired by 100 Days of Python Code Challenge.]]></content:encoded></item><item><title>Fine-tune OpenAI GPT-OSS models using Amazon SageMaker HyperPod recipes</title><link>https://aws.amazon.com/blogs/machine-learning/fine-tune-openai-gpt-oss-models-using-amazon-sagemaker-hyperpod-recipes/</link><author>Durga Sury</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 21:35:59 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This post is the second part of the GPT-OSS series focusing on model customization with Amazon SageMaker AI. In Part 1, we demonstrated fine-tuning GPT-OSS models using open source Hugging Face libraries with SageMaker training jobs, which supports distributed multi-GPU and multi-node configurations, so you can spin up high-performance clusters on demand.In this post, we show how you can fine-tune GPT OSS models on using recipes on SageMaker HyperPod and Training Jobs. SageMaker HyperPod recipes help you get started with training and fine-tuning popular publicly available foundation models (FMs) such as Meta’s Llama, Mistral, and DeepSeek in just minutes, using either SageMaker HyperPod or training jobs. The recipes provide pre-built, validated configurations that alleviate the complexity of setting up distributed training environments while maintaining enterprise-grade performance and scalability for models. We outline steps to fine-tune the GPT-OSS model on a multilingual reasoning dataset, HuggingFaceH4/Multilingual-Thinking, so GPT-OSS can handle structured, chain-of-thought (CoT) reasoning across multiple languages.This solution uses SageMaker HyperPod recipes to run a fine-tuning job on HyperPod using Amazon Elastic Kubernetes Service (Amazon EKS) orchestration or training jobs. Recipes are processed through the SageMaker HyperPod recipe launcher, which serves as the orchestration layer responsible for launching a job on the corresponding architecture such as SageMaker HyperPod (Slurm or Amazon EKS) or training jobs. To learn more, see SageMaker HyperPod recipes.In the following sections, we discuss the prerequisites for both options, and then move on to the data preparation. The prepared data is saved to Amazon FSx for Lustre, which is used as the persistent file system for SageMaker HyperPod, or Amazon Simple Storage Service (Amazon S3) for training jobs. We then use recipes to submit the fine-tuning job, and finally deploy the trained model to a SageMaker endpoint for testing and evaluating the model. The following diagram illustrates this architecture.To follow along, you must have the following prerequisites:A local development environment with AWS credentials configured for creating and accessing SageMaker resources, or a remote environment such as Amazon SageMaker Studio.For fine-tuning the model using SageMaker training jobs, you must have one ml.p5.48xlarge instance (with 8 x NVIDIA H100 GPUs) for training jobs usage. If you don’t have sufficient limits, request the following SageMaker quotas on the Service Quotas console: P5 instance (ml.p5.48xlarge) for training jobs (ml.p5.48xlarge for cluster usage): 1.We use the Hugging FaceH4/Multilingual-Thinking dataset, which is a multilingual reasoning dataset containing CoT examples translated into languages such as French, Spanish, and German. The recipe supports a sequence length of 4,000 tokens for the GPT-OSS 120B model. The following example code demonstrates how to tokenize the multilingual-thinking dataset. The recipe accepts data in Hugging Face format (arrow). After it’s tokenized, you can save the processed dataset to disk.from datasets import load_dataset
 
from transformers import AutoTokenizer
import numpy as np
 
dataset = load_dataset("HuggingFaceH4/Multilingual-Thinking", split="train")
 
tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-120b")
messages = dataset[0]["messages"]
conversation = tokenizer.apply_chat_template(messages, tokenize=False)
print(conversation)
 
def preprocess_function(example):
    return tokenizer.apply_chat_template(example['messages'], 
                                        return_dict=True, 
                                        padding="max_length", 
                                        max_length=4096, 
                                        truncation=True)
 
def label(x):
    x["labels"]=np.array(x["input_ids"])
    x["labels"][x["labels"]==tokenizer.pad_token_id]=-100
    x["labels"]=x["labels"].tolist()
    return x
 
dataset = dataset.map(preprocess_function, 
                      remove_columns=['reasoning_language', 
                                      'developer', 
                                      'user', 
                                      'analysis', 
                                      'final',
                                      'messages'])
dataset = dataset.map(label)

# for HyperPod, save to mounted FSx volume
dataset.save_to_disk("/fsx/multilingual_4096")

# for training jobs, save to S3
dataset.save_to_disk("multilingual_4096")

def upload_directory(local_dir, bucket_name, s3_prefix=''):
    s3_client = boto3.client('s3')
    
    for root, dirs, files in os.walk(local_dir):
        for file in files:
            local_path = os.path.join(root, file)
            # Calculate relative path for S3
            relative_path = os.path.relpath(local_path, local_dir)
            s3_path = os.path.join(s3_prefix, relative_path).replace("\\", "/")
            
            print(f"Uploading {local_path} to {s3_path}")
            s3_client.upload_file(local_path, bucket_name, s3_path)

upload_directory('./multilingual_4096/', <your-bucket>, 'multilingual_4096')Now that you have prepared and tokenized the dataset, you can fine-tune the GPT-OSS model on your dataset, using either SageMaker HyperPod or training jobs. SageMaker training jobs are ideal for one-off or periodic training workloads that need temporary compute resources, making it a fully managed, on-demand experience for your training needs. SageMaker HyperPod is optimal for continuous development and experimentation, providing a persistent, preconfigured, and failure-resilient cluster. Depending on your choice, skip to the appropriate section for next steps.Fine-tune the model using SageMaker HyperPodTo fine-tune the model using HyperPod, start by setting up the virtual environment and installing the necessary dependencies to execute the training job on the EKS cluster. Make sure the cluster is  before proceeding, and you’re using Python 3.9 or greater in your development environment.python3 -m venv ${PWD}/venv
source venv/bin/activateNext, download and set up the SageMaker HyperPod recipes repository:git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt You can now use the SageMaker HyperPod recipe launch scripts to submit your training job. Using the recipe involves updating the  configuration file and executing the launch script.In recipes_collection/cluster/k8s.yaml, update the  section. It mounts the FSx claim to the  directory of each computing pod:- claimName: fsx-claim    
  mountPath: fsxSageMaker HyperPod recipes provide a launch script for each recipe within the  directory. To fine-tune the GPT-OSS-120B model, update the launch scripts located at launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.sh and update the  parameter.The updated launch script should look similar to the following code when running SageMaker HyperPod with Amazon EKS. Make sure that  and  are updated in the launch script:#!/bin/bash

# Original Copyright (c), NVIDIA CORPORATION. Modifications © Amazon.com

#Users should setup their cluster type in /recipes_collection/config.yaml

SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}

HF_MODEL_NAME_OR_PATH="openai/gpt-oss-120b" # HuggingFace pretrained model name or path

TRAIN_DIR="/fsx/multilingual_4096" # Location of training dataset
VAL_DIR="/fsx/multilingual_4096" # Location of validation dataset

EXP_DIR="/fsx/experiment" # Location to save experiment info including logging, checkpoints, ect
HF_ACCESS_TOKEN="hf_xxxxxxxx" # Optional HuggingFace access token

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
    recipes=fine-tuning/gpt_oss/hf_gpt_oss_120b_seq4k_gpu_lora \
    container="658645717510.dkr.ecr.us-west-2.amazonaws.com/smdistributed-modelparallel:sm-pytorch_gpt_oss_patch_pt-2.7_cuda12.8" \
    base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
    recipes.run.name="hf-gpt-oss-120b-lora" \
	cluster=k8s \ # Imp: add cluster line when running on HP EKScluster_type=k8s \ # Imp: add cluster_type line when running on HP EKS
    recipes.exp_manager.exp_dir="$EXP_DIR" \
    recipes.trainer.num_nodes=1 \
    recipes.model.data.train_dir="$TRAIN_DIR" \
    recipes.model.data.val_dir="$VAL_DIR" \
    recipes.model.hf_model_name_or_path="$HF_MODEL_NAME_OR_PATH" \
    recipes.model.hf_access_token="$HF_ACCESS_TOKEN" \When the script is ready, you can launch fine-tuning of the GPT OSS 120B model using the following code:chmod +x launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.sh 
bash launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.shAfter submitting a job for fine-tuning, you can use the following command to verify successful submission. You should be able to see the pods running in your cluster:kubectl get pods
NAME                                READY  STATUS   RESTARTS   AGE
hf-gpt-oss-120b-lora-h2cwd-worker-0 1/1    Running  0          14mTo check logs for the job, you can use the  command:kubectl logs -f hf-gpt-oss-120b-lora-h2cwd-worker-0You should be able to see the following logs when the training begins and completes. You will find the checkpoints written to the /fsx/experiment/checkpoints folder.warnings.warn(
    
Epoch 0:  40%|████      | 50/125 [08:47<13:10,  0.09it/s, Loss/train=0.254, Norms/grad_norm=0.128, LR/learning_rate=2.2e-6] [NeMo I 2025-08-18 17:49:48 nemo_logging:381] save SageMakerCheckpointType.PEFT_FULL checkpoint: /fsx/experiment/checkpoints/peft_full/steps_50
[NeMo I 2025-08-18 17:49:48 nemo_logging:381] Saving PEFT checkpoint to /fsx/experiment/checkpoints/peft_full/steps_50
[NeMo I 2025-08-18 17:49:49 nemo_logging:381] Loading Base model from : openai/gpt-oss-120b
You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour
Loading checkpoint shards: 100%|██████████| 15/15 [01:49<00:00,  7.33s/it]
[NeMo I 2025-08-18 17:51:39 nemo_logging:381] Merging the adapter, this might take a while......
Unloading and merging model: 100%|██████████| 547/547 [00:07<00:00, 71.27it/s]
[NeMo I 2025-08-18 17:51:47 nemo_logging:381] Checkpointing to /fsx/experiment/checkpoints/peft_full/steps_50/final-model......
[NeMo I 2025-08-18 18:00:14 nemo_logging:381] Successfully save the merged model checkpoint.
`Trainer.fit` stopped: `max_steps=50` reached.
Epoch 0:  40%|████      | 50/125 [23:09<34:43,  0.04it/s, Loss/train=0.264, Norms/grad_norm=0.137, LR/learning_rate=2e-6]  When the training is complete, the final merged model can be found in the  directory path you defined in the launcher script under /fsx/experiment/checkpoints/peft_full/steps_50/final-model.Fine-tune using SageMaker training jobsYou can also use recipes directly with SageMaker training jobs using the SageMaker Python SDK. The training jobs automatically spin up the compute, load the input data, run the training script, save the model to your output location, and tear down the instances, for a smooth training experience.The following code snippet shows how to use recipes with the PyTorch estimator. You can use the  parameter to specify the training or fine-tuning recipe to be used, and  for any parameters that need replacement. For training jobs, update the , , and  directories to locations in  as required by SageMaker training jobs.import os
import sagemaker,boto3
from sagemaker.pytorch import PyTorch
from sagemaker.inputs import FileSystemInput

sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()
bucket = sagemaker_session.default_bucket()
output = os.path.join(f"s3://{bucket}", "output")

recipe_overrides = {
    "run": {
        "results_dir": "/opt/ml/model",
    },
    "exp_manager": {
        "exp_dir": "",
        "explicit_log_dir": "/opt/ml/output/tensorboard",
        "checkpoint_dir": "/opt/ml/checkpoints",
    },
    "model": {
        "data": {
            "train_dir": "/opt/ml/input/data/train",
            "val_dir": "/opt/ml/input/data/val",
        },
    },
    "use_smp_model": "False",
}


# create the estimator object
estimator = PyTorch(
  output_path=output,
  base_job_name=f"gpt-oss-recipe",
  role=role,
  instance_type="ml.p5.48xlarge",
  training_recipe="fine-tuning/gpt_oss/hf_gpt_oss_120b_seq4k_gpu_lora",
  recipe_overrides=recipe_overrides,
  sagemaker_session=sagemaker_session,
  image_uri="658645717510.dkr.ecr.us-west-2.amazonaws.com/smdistributed-modelparallel:sm-pytorch_gpt_oss_patch_pt-2.7_cuda12.8",
)

# submit the training job
estimator.fit(
inputs={
"train": f"s3://{bucket}/datasets/multilingual_4096/", 
"val": f"s3://{bucket}/datasets/multilingual_4096/"}, wait=True)After the job is submitted, you can monitor the status of your training job on the SageMaker console, by choosing under in the navigation pane. Choose the training job that starts with  to view its details and logs. When the training job is complete, the outputs will be saved to an S3 location. You can get the location of the output artifacts from the section on the job details page.After you fine-tune your GPT-OSS model with SageMaker recipes on either SageMaker training jobs or SageMaker HyperPod, the output is a customized model artifact that merges the base model with the customized PEFT adapters. This final model is stored in Amazon S3 and can be deployed directly from Amazon S3 to SageMaker endpoints for real-time inference.To serve GPT-OSS models, you must have the latest vLLM containers (v0.10.1 or later). A full list of  Docker image versions is available on Docker hub.The steps to deploy your fine-tuned GPT-OSS model are outlined in this section.Build the latest GPT-OSS container for your SageMaker endpointIf you’re deploying the model from SageMaker Studio using JupyterLab or the Code Editor, both environments come with Docker preinstalled. Make sure that you’re using the SageMaker Distribution image v3.0 or later for compatibility.You can build your deployment container by running the following commands:%%bash # <- use this if you're running this inside JupterLab cell

# navigate to deploy dir from the current workdir, to build container
cd ./deploy 

# build a push container
chmod +X build.sh
bash build.sh

cd .. If you’re running these commands from a local terminal or other environment, simply omit the  line and run the commands as standard shell commands.The  script is responsible for automatically building and pushing a  container that is optimized for SageMaker endpoints. After it’s built, the custom SageMaker endpoint compatible  image is pushed to Amazon Elastic Container Registry (Amazon ECR). SageMaker endpoints can then pull this image from Amazon ECR at runtime to spin up the container for inference.The following is an example of the  script:export REGION={region}
export ACCOUNT_ID={account_id}
export REPOSITORY_NAME=vllm
export TAG=v0.10.1

full_name="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${REPOSITORY_NAME}:${TAG}"

echo "building $full_name"

DOCKER_BUILDKIT=0 docker build . --network sagemaker --tag $full_name --file Dockerfile

aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com

# If the repository doesn't exist in ECR, create it.
aws ecr describe-repositories --region ${REGION} --repository-names "${REPOSITIRY_NAME}" > /dev/null 2>&1

if [ $? -ne 0 ]
then
    aws ecr create-repository --region ${REGION} --repository-name "${REPOSITORY_NAME}" > /dev/null
fi

docker tag $REPOSITORY_NAME:$TAG ${full_name}
docker push ${full_name}The Dockerfile defines how we convert an open source vLLM Docker image into a SageMaker hosting-compatible image. This involves extending the base  image, adding the  entrypoint script, and making it executable. See the following example Dockerfile:FROM vllm/vllm-openai:v0.10.1

COPY serve /usr/bin/serve
RUN chmod 777 /usr/bin/serve

ENTRYPOINT [ "/usr/bin/serve" ]The  script acts as a translation layer between SageMaker hosting conventions and the vLLM runtime. You can maintain the same deployment workflow you’re familiar with when hosting models on SageMaker endpoints, while automatically converting SageMaker-specific configurations into the format expected by vLLM.Key points to note about this script:It enforces the use of port 8080, which SageMaker requires for inference containersIt dynamically translates environment variables prefixed with  into CLI arguments for vLLM (for example, OPTION_MAX_MODEL_LEN=4096 changes to )It prints the final set of arguments for visibilityIt finally launches the vLLM API server with the translated argumentsThe following is an example  script:#!/bin/bash

# Define the prefix for environment variables to look for
PREFIX="OPTION_"
ARG_PREFIX="--"

# Initialize an array for storing the arguments
# port 8080 required by sagemaker, https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response
ARGS=(--port 8080)

# Loop through all environment variables
while IFS='=' read -r key value; do
    # Remove the prefix from the key, convert to lowercase, and replace underscores with dashes
    arg_name=$(echo "${key#"${PREFIX}"}" | tr '[:upper:]' '[:lower:]' | tr '_' '-')

    # Add the argument name and value to the ARGS array
    ARGS+=("${ARG_PREFIX}${arg_name}")
    if [ -n "$value" ]; then
        ARGS+=("$value")
    fi
done < <(env | grep "^${PREFIX}")

echo "-------------------------------------------------------------------"
echo "vLLM engine args: [${ARGS[@]}]"
echo "-------------------------------------------------------------------"

# Pass the collected arguments to the main entrypoint
exec python3 -m vllm.entrypoints.openai.api_server "${ARGS[@]}"Host customized GPT-OSS as a SageMaker real-time endpointNow you can deploy your fine-tuned GPT-OSS model using the ECR image URI you built in the previous step. In this example, the model artifacts are stored securely in an S3 bucket, and SageMaker will download them into the container at runtime.Complete the following configurations:Set  to point to the S3 prefix where your model artifacts are locatedSet the  environment variable to , which is where SageMaker mounts the model inside the container(Optional) If you’re serving a model from Hugging Face Hub instead of Amazon S3, you can set  directly to the Hugging Face model ID insteadThe endpoint startup might take several minutes as the model artifacts are downloaded and the container is initialized.The following is an example deployment code:inference_image = f"{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:v0.10.1"

...
...

lmi_model = sagemaker.Model(
    image_uri=inference_image,
    env={
        "OPTION_MODEL": "/opt/ml/model", # set this to let SM endpoint read a model stored in s3, else set it to HF MODEL ID
        "OPTION_SERVED_MODEL_NAME": "model",
        "OPTION_TENSOR_PARALLEL_SIZE": json.dumps(num_gpus),
        "OPTION_DTYPE": "bfloat16",
        #"VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1", # not required for vLLM 0.10.1 and above
        "OPTION_ASYNC_SCHEDULING": "true",
        "OPTION_QUANTIZATION": "mxfp4"
    },
    role=role,
    name=model_name,
    model_data={
        'S3DataSource': {
            'S3Uri': "s3://path/to/gpt-oss/model/artifacts",
            'S3DataType': 'S3Prefix',
            'CompressionType': 'None'
        }
    },
)

...

lmi_model.deploy(
    initial_instance_count=1,
    instance_type=instance_type,
    container_startup_health_check_timeout=600,
    endpoint_name=endpoint_name,
    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,
    inference_component_name=inference_component_name,
    resources=ResourceRequirements(requests={"num_accelerators": 1, "memory": 1024*3, "copies": 1,}),
)After your endpoint is deployed and in the  state, you can invoke your fine-tuned GPT-OSS model using the SageMaker Python SDK.The following is an example predictor setup:pretrained_predictor = sagemaker.Predictor(
    endpoint_name=endpoint_name,
    sagemaker_session=sagemaker.Session(boto3.Session(region_name=boto3.Session().region_name)),
    serializer=serializers.JSONSerializer(),
    deserializer=deserializers.JSONDeserializer(),
    component_name=inference_component_name
)The modified vLLM container is fully compatible with the OpenAI-style  input format, making it straightforward to send chat-style requests:payload = {
    "messages": [{"role": "user", "content": "Hello who are you?"}],
    "parameters": {"max_new_tokens": 64, "temperature": 0.2}
}

output = pretrained_predictor.predict(payload)You have successfully deployed and invoked your custom fine-tuned GPT-OSS model on SageMaker real-time endpoints, using the vLLM framework for optimized, low-latency inference. You can find more GPT-OSS hosting examples in the OpenAI gpt-oss examples GitHub repo.To avoid incurring additional charges, complete the following steps to clean up the resources used in this post:Delete the SageMaker endpoint:pretrained_predictor.delete_endpoint()Clean up the FSx for Lustre volume if it’s no longer needed by following instructions in Deleting a file system.If you used training jobs, the training instances are automatically deleted when the jobs are complete.In this post, we showed how to fine-tune OpenAI’s GPT-OSS models ( and ) on SageMaker AI using SageMaker HyperPod recipes. We discussed how SageMaker HyperPod recipes provide a powerful yet accessible solution for organizations to scale their AI model training capabilities with large language models (LLMs) including GPT-OSS, using either a persistent cluster through SageMaker HyperPod, or an ephemeral cluster using SageMaker training jobs. The architecture streamlines complex distributed training workflows through its intuitive recipe-based approach, reducing setup time from weeks to minutes. We also showed how these fine-tuned models can be seamlessly deployed to production using SageMaker endpoints with vLLM optimization, providing enterprise-grade inference capabilities with OpenAI-compatible APIs. This end-to-end workflow, from training to deployment, helps organizations build and serve custom LLM solutions while using the scalable infrastructure of AWS and comprehensive ML platform capabilities of SageMaker.To begin using the SageMaker HyperPod recipes, visit the Amazon SageMaker HyperPod recipes GitHub repo for comprehensive documentation and example implementations. If you’re interested in exploring the fine-tuning further, the Generative AI using Amazon SageMaker GitHub repo has the necessary code and notebooks. Our team continues to expand the recipe ecosystem based on customer feedback and emerging ML trends, making sure that you have the tools needed for successful AI model training.Special thanks to everyone who contributed to the launch: Hengzhi Pei, Zach Kimberg, Andrew Tian, Leonard Lausen, Sanjay Dorairaj, Manish Agarwal, Sareeta Panda, Chang Ning Tsai, Maxwell Nuyens, Natasha Sivananjaiah, and Kanwaljit Khurmi. is a Senior Solutions Architect at Amazon SageMaker, where she helps enterprise customers build secure and scalable AI/ML systems. When she’s not architecting solutions, you can find her enjoying sunny walks with her dog, immersing herself in murder mystery books, or catching up on her favorite Netflix shows. is a Senior Generative AI Data Scientist at AWS, specializing in helping organizations innovate with Generative AI, Deep Learning, and Machine Learning on Amazon SageMaker AI. Over the past 10+ years, he has developed and scaled advanced computer vision (CV) and natural language processing (NLP) models to tackle high-impact problems—from optimizing global supply chains to enabling real-time video analytics and multilingual search. When he’s not building AI solutions, Pranav enjoys playing strategic games like chess, traveling to discover new cultures, and mentoring aspiring AI practitioners. You can find Pranav on LinkedIn. is a Senior Manager of Product Management at Amazon Web Services (AWS), where he leads several areas of the Amazon SageMaker, including SageMaker Studio – the industry-leading integrated development environment for machine learning, developer and administrator experiences, AI infrastructure, and SageMaker SDK. is a Senior AI/ML Solutions Architect at Amazon Web Services (AWS), helping customers design and build AI/ML solutions. Dmitry’s work covers a wide range of ML use cases, with a primary interest in Generative AI, deep learning, and scaling ML across the enterprise. He has helped companies in many industries, including insurance, financial services, utilities, and telecommunications. You can connect with Dmitry on LinkedIn.Arun Kumar Lokanatha is a Senior ML Solutions Architect with the Amazon SageMaker team. He specializes in large language model training workloads, helping customers build LLM workloads using SageMaker HyperPod, SageMaker training jobs, and SageMaker distributed training. Outside of work, he enjoys running, hiking, and cooking. is a Senior Product Manager, Technical, at AWS with the SageMaker team, where he focuses on Machine Learning. He holds a Master’s in Robotics from Carnegie Mellon University and an MBA from the Wharton School of Business. Anirudh is a named inventor on more than 50 AI/ML patents. He enjoys long-distance running, exploring art galleries, and attending Broadway shows.]]></content:encoded></item><item><title>Show HN: Splice – CAD for Cable Harnesses and Electrical Assemblies</title><link>https://splice-cad.com/</link><author>djsdjs</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 21:10:34 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Day 7: When Protobuf Breaks Everything - Real Engineering in the Trenches</title><link>https://dev.to/clayroach/day-7-when-protobuf-breaks-everything-real-engineering-in-the-trenches-co4</link><author>Clay Roach</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 21 Aug 2025 21:02:28 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[: Add real-time updates and bootstrap AI anomaly detection.: "Why are all my operations named 'protobuf-fallback-trace'?!"Welcome to Day 7 of building an AI-native observability platform in 30 days. Today was supposed to be about sexy features. Instead, it was about the unglamorous reality of systems engineering: making protobuf work correctly.
  
  
  The Problem That Changed Everything
I started the day confident. The OpenTelemetry demo was running, traces were flowing, the UI was displaying data. Time to add real-time updates, right?Then I looked closer at the trace details:Every. Single. Operation. Was named "protobuf-fallback-trace".
  
  
  Discovery #1: Gzip Was Being Ignored
The OpenTelemetry demo sends protobuf data with gzip compression. My middleware had "clever" conditional logic:The fix was embarrassingly simple:: Sometimes "clever" code is just complicated code. Unified handling often beats conditional logic.
  
  
  Discovery #2: Protobufjs vs ES Modules
Next challenge: parsing the actual protobuf data. The protobufjs library is CommonJS, but my project uses ES modules. This led to hours of:
  
  
  Discovery #3: Path Resolution Hell
Even with protobufjs loading, the OTLP protobuf definitions have imports that need custom resolution:
  
  
  The Nuclear Option: Enhanced Fallback Parsing
When the "proper" protobuf parsing kept failing, I built something unconventional - a raw protobuf parser that extracts data through pattern matching:Is this elegant? No. Does it work? .After 8 hours of protobuf wrestling:❌ All operations: "protobuf-fallback-trace"✅ Real operations: , ✅ 10+ real spans per trace✅ Authentic resource attributes and timing data
  
  
  1. Fallback Strategies Are Not DefeatBuilding a fallback parser wasn't giving up - it was ensuring the system works even when dependencies fail. In production, .
  
  
  2. Debug at the Lowest LevelI spent hours assuming the protobuf data was corrupt. Finally logging the raw buffer bytes revealed it was fine - the decompression was being skipped.
  
  
  3. Integration Points Are Where Systems BreakThe individual components all worked:✅ OpenTelemetry demo: sending valid data✅ Express server: receiving requests
✅ ClickHouse: storing dataThe failure was in the glue between them.
  
  
  4. Real Data Reveals Real ProblemsMock data would never have exposed this issue. Testing with the actual OpenTelemetry demo forced me to handle real-world complexity.Today didn't go according to plan, and that's  what building production systems is like. The glossy demo videos don't show the 8 hours spent debugging why protobuf.load is not a function.But here's what matters: the system now correctly processes thousands of real traces from a production-like demo application. Every service is visible, every operation is named correctly, and the data flowing through the pipeline is authentic.Now that protobuf parsing actually works:Implement the real-time updates (for real this time)Add WebSocket support for live trace streamingBootstrap the AI anomaly detection systemCreate service dependency visualization
  
  
  Code Snippets That Saved the Day
For anyone fighting similar battles:
docker compose backend xxd  100 /tmp/trace.pb


curl  POST http://localhost:4319/v1/traces  @trace.pb.gz


node Day 7 was humbling. The plan was to build flashy features. Instead, I spent the day in the trenches making basic data ingestion work correctly. But that's real engineering. It's not always about the elegant algorithm or the clever architecture. Sometimes it's about making protobuf parsing work at 2 AM because your entire platform depends on it.The platform is stronger because of today's battles. And tomorrow, with real data flowing correctly, we can build the features that actually matter.Are you fighting your own protobuf battles? Share your war stories in the comments. Sometimes knowing you're not alone in the debugging trenches makes all the difference.Progress: Day 7 of 30 ✅ | Protobuf: Finally Working | Sanity: Questionable]]></content:encoded></item><item><title>Inline code nodes now supported in Amazon Bedrock Flows in public preview</title><link>https://aws.amazon.com/blogs/machine-learning/inline-code-nodes-now-supported-in-amazon-bedrock-flows-in-public-preview/</link><author>Shubhankar Sumar</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 20:36:40 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Today, we are excited to announce the public preview of support for inline code nodes in Amazon Bedrock Flows. With this powerful new capability, you can write Python scripts directly within your workflow, alleviating the need for separate AWS Lambda functions for simple logic. This feature streamlines preprocessing and postprocessing tasks (like data normalization and response formatting), simplifying generative AI application development and making it more accessible across organizations. By removing adoption barriers and reducing maintenance overhead, the inline code feature accelerates enterprise adoption of generative AI solutions, resulting in faster iteration cycles and broader participation in AI application building.Organizations using Amazon Bedrock Flows now can use inline code nodes to design and deploy workflows for building more scalable and efficient generative AI applications fully within the Amazon Bedrock environment while achieving the following: – Transforming input data before sending it to a large language model (LLM) without having to set up a separate Lambda function. For example, extracting specific fields from JSON, formatting text data, or normalizing values. – Performing operations on model outputs directly within the flow. For example, extracting entities from responses, formatting JSON for downstream systems, or applying business rules to the results. – Managing the execution of complex, multi-step generative AI workflows that can call popular packages like opencv, scipy, of pypdf. – Seamless user experience with the ability to trace the inputs and outputs from each node.In this post, we discuss the benefits of this new feature, and show how to use inline code nodes in Amazon Bedrock Flows.Benefits of inline code in Amazon Bedrock FlowsThomson Reuters, a global information services company providing essential news, insights, and technology solutions to professionals across legal, tax, accounting, media, and corporate sectors, handles complex, multi-step generative AI use cases that require simple preprocessing and postprocessing as part of the workflow. With the inline code feature in Amazon Bedrock Flows, Thomson Reuters can now benefit from the following:Simplified flow management – Alleviate the need to create and maintain individual Lambda functions for each custom code block, making it straightforward to manage thousands of workflows across a large user base (over 16,000 users and 6,000 chains) with less operational overhead. – Enable direct preprocessing of data before LLM calls and postprocessing of LLM responses, including the ability to interact with internal AWS services and third-party APIs through a single interface. – Help users build complex workflows with custom code blocks through a self-service interface, without exposing them to the underlying infrastructure complexities or requiring Lambda function management.In the following sections, we show how to create a simple Amazon Bedrock flow and add inline code nodes. Our example showcases a practical application where we’ll construct a flow that processes user requests for music playlists, incorporating both preprocessing and postprocessing inline code nodes to handle data validation and response formatting.Before implementing the new capabilities, make sure you have the following:After these components are in place, you can proceed with using Amazon Bedrock Flows with inline code capabilities in your generative AI use case.Create your flow using inline code nodesComplete the following steps to create your flow:Amazon Bedrock provides different node types to build your prompt flow. For this example, we use an inline code node instead of calling a Lambda function for custom code for a generative AI-powered application. There are two inline code nodes in the flow. We have extended the sample from the documentation Create a flow with a single prompt. The new node type  is on the  tab in the left pane.Add some code to process in the  node before sending it to the prompt node . Python 3 is only supported at the time of writing. In this example, we check if the number of songs requested by the user is more than 10 and it’s set to 10.There is a Python code editor and sample code templates as well for writing the code.We use the following code:import json
def __func():
    try:
        if userprompt['number'] > 10:
            userprompt['number']=10
            return userprompt
        else:
            return userprompt
            
    except Exception as e:
        return {
            "error": "Invalid input format",
            "details": str(e)
        }
__func()In the Postprocessing_Inline Code node, we check the number of words in the response and feed the data to the next prompt node, .def __func():
    # Remove extra whitespace and count
    cleaned_text = ' '.join(playlist.split())
    word_count = len(cleaned_text.split())
    return{
        "playlist": playlist,     "word_count": word_count
    }
__func()Test the flow with the following prompt:Sample input for the Flow Input node 
{
  "genre": "pop",
    "number": 8
  }Input to the inline code node (Python function) must be treated as untrusted user input, and appropriate parsing, validation, and data handling should be implemented.You can see the output as shown in the following screenshot. The system also provides access to node execution traces, offering detailed insights into each processing step, real-time performance metrics, and highlighting any issues that occurred during the flow’s execution. Traces can be enabled using an API and sent to an Amazon CloudWatch log. In the API, set the  field to true in an  request. Each  in the response is returned alongside a .When working with inline code nodes in Amazon Bedrock Flows, the following are the important things to note:Code is executed in an AWS managed, secured, sandbox environment that is not shared with anyone and doesn’t have internet accessThe feature supports Python 3.12 and aboveIt efficiently handles code with binary size up to 4 MB, which is roughly 4 million charactersIt supports popular packages like opencv, scipy, and pypdfIt supports 25 concurrent code execution sessions per AWS accountThe integration of inline code nodes in Amazon Bedrock Flows marks a significant advancement in democratizing generative AI development, reducing the complexity of managing separate Lambda functions for basic processing tasks. This enhancement responds directly to enterprise customers’ needs for a more streamlined development experience, helping developers focus on building sophisticated AI workflows rather than managing infrastructure.We’re excited to see the innovative applications you will build with these new capabilities. As always, we welcome your feedback through AWS re:Post for Amazon Bedrock or your usual AWS contacts. Join the generative AI builder community at community.aws to share your experiences and learn from others. is a Senior Solutions Architect at AWS, where he specializes in architecting generative AI-powered solutions for enterprise software and SaaS companies across the UK. With a strong background in software engineering, Shubhankar excels at designing secure, scalable, and cost-effective multi-tenant systems on the cloud. His expertise lies in seamlessly integrating cutting-edge generative AI capabilities into existing SaaS applications, helping customers stay at the forefront of technological innovation. is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business. is a Principal Product Manager at AWS. She is leading the Amazon Bedrock Flows, with 18 years of experience building customer-centric and data-driven products. She is passionate about democratizing responsible machine learning and generative AI to enable customer experience and business innovation. Outside of work, she enjoys spending time with family and friends, listening to audiobooks, traveling, and gardening.]]></content:encoded></item><item><title>Accelerate enterprise AI implementations with Amazon Q Business</title><link>https://aws.amazon.com/blogs/machine-learning/accelerate-enterprise-ai-implementations-with-amazon-q-business/</link><author>Oliver Steffmann</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 20:29:53 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[As an Amazon Web Services (AWS) enterprise customer, you’re probably exploring ways to use generative AI to enhance your business processes, improve customer experiences, and drive innovation.With a variety of options available—from Amazon Q Business to other AWS services or third-party offerings—choosing the right tool for your use case can be challenging. This post aims to guide you through the decision-making process and highlight the unique advantages of Amazon Q Business and how to build an AWS architecture to get started and onboard more use cases.Amazon Q Business is an AI-powered assistant that can help employees quickly find information, solve problems, and get work done across their company’s data and applications. With Amazon Q Business, employees can access information from various internal documents, websites, wikis, and other business resources through natural conversations, helping them to find exactly what they need without extensive searching. It can also be used to automate common workflows across enterprise systems. Amazon Q Business prioritizes security and privacy by operating within your organization’s existing permissions and access controls, helping to ensure that employees only see information that they’re authorized to access.The first step in selecting the right generative AI solution is to clearly define your use case. Are you looking to enhance a single system, or do you need a solution that spans multiple platforms? Single-system use cases might be well-served by specific generative AI solutions, while cross-system scenarios often benefit from a more unified approach. Organizations that benefit most from Amazon Q Business typically share several key characteristics: Companies with large volumes of data spread across multiple repositories and formats (documents, images, audio, video) Organizations where employee productivity depends on accessing institutional knowledge quickly and accurately Organizations with strict security and compliance needs requiring role-based permissions and access controls Teams that need to share information and collaborate across departments and geographies Organizations with complex workflows that could benefit from automation and streamliningKey considerations for tool selectionWhen evaluating generative AI tools, there are several factors should you should consider to help ensure successful implementation and adoption: Determine if you need custom AI behaviors or if out-of-the-box solutions suffice Assess the number of systems involved and the complexity of data flows between them Think about your long-term needs and choose a solution that can grow with youData privacy and residency: Understand your data governance requirements and make sure that your chosen solution can meet them Evaluate the total cost of ownership, including implementation, maintenance, and scaling costs Consider how quickly you need to implement your generative AI solution As with any enterprise AI implementation, organizations must invest in proper training and change management strategies to help ensure adoptionThe case for Amazon Q BusinessAmazon Q Business offers unique advantages, especially for organizations that already have AWS services or that have complex, cross-system needs. For AWS enterprise customers that have the resources to build and operate their own solutions, an architecture that includes Amazon Q Business offers flexibility and cost advantages, including: Amazon Q Business can provide a consistent AI experience across multiple systems, creating a seamless interface for users. As a native AWS service, Amazon Q Business integrates seamlessly with your existing AWS architecture, reducing complexity and potential points of failure. Amazon Q Business can connect to various enterprise systems, so that you can use it to create custom workflows that span multiple platforms. By using Amazon Q Business, you can take advantage of the proven scalability of AWS to handle growing workloads without worrying about infrastructure management. Use the robust security features and compliance certifications of AWS to help reduce your security and compliance burden. Amazon Q Business offers a pay-as-you-go model, so you can scale costs with the number of users and usage for knowledge bases. This can lead to significant cost savings (see pricing details).Implement your generative AI use casesAfter you’ve chosen your generative AI use cases, consider a phased implementation approach:Start with pilot use cases to prove value quickly: Good pilot use cases include IT help desk or HR workflows. You can get started by taking advantage of AWS-provided example projects and open source samples.Evaluate the next use cases: Prioritize you next use cases by business impact and feature coverage with existing Amazon Q Business connectors and plugins. Often AIOps use cases that include integrations or chat interfaces on top of ServiceNow, Confluence, Teams, or Slack are good examples.Use existing data sources: Connect Amazon Q Business to enterprise systems with supported connectors first to maximize immediate value.Implement accuracy testing using frameworks: Use tools such as the AWS evaluation framework for Amazon Q Business, which includes automated testing pipelines, ground truth datasets, and comprehensive metrics for measuring response quality, relevancy, truthfulness, and overall accuracy.Iteratively scale successful implementations across your organization: Start your implementation with the teams that are most interested in the application and willing to provide feedback. Make changes based on the feedback as needed, then expand it across the organization.Measure and track results: Establish clear KPIs before implementation to quantify business impact.Monitor usage and costs, implement feedback loops, and make sure to support security and compliance throughout your generative AI journey. Amazon Q Business can provide significant value when implemented in appropriate use cases with proper planning and governance. Success depends on careful evaluation of business needs, thorough implementation planning, and ongoing management of the solution.When implementing your generative AI use cases, architectural decisions play a crucial role in achieving long-term success. Let’s explore some best practices for a typical AWS enterprise environment. Connecting your corporate source of identities to AWS IAM Identity Center provides better security and user experience, Amazon Q Business users authorize their Amazon Q session with their usual sign-in process, using their existing organizational credentials through the identity source already in place. Set up Amazon Q Business service, data sources, and plugins in a shared services account based on application group or business unit to help reduce the number of similar deployments across different AWS accounts. When rolling out new use cases, consider also enabling existing familiar enterprise channels such as collaboration tools (Teams or Slack) to provide a frictionless way to test and roll out new use cases. When adding data sources, estimate index storage needs and whether your use case requires crawling access control list (ACL) and identity information from the data source and if it is supported by the connector. To reduce initial complexity, focus on use cases that provide the same data to all users, then expand it in a second phase for use cases that rely on ACLs to control access. Use plugins to integrate external services as actions. For each use case, verify if a built-in plugin can provide this functionality, or if a custom plugin is needed. For custom plugins, plan an architecture that enables pointing to backend services using OpenAPI endpoints in other AWS accounts across the organization. This allows flexible integration of existing AWS Lambda functions or container-based functionality.By carefully considering these aspects, you can create a solid foundation for your generative AI implementation that aligns with your organization’s needs and future growth plans.How to deploy Amazon Q Business in your organizationThe following reference architecture illustrates the main components and flow of a typical Amazon Q Business implementation:The workflow is as follows:A user interacts with an assistant through an enterprise collaboration system.Alternate: A user interacts with the built-in web interface provided by Amazon Q Business.The user is authenticated using IAM Identity Center and federated by a third-party identity provider (IdP).Data sources are configured for existing enterprise systems and data is crawled and indexed in Amazon Q Business. You can use custom connectors to integrate data sources that aren’t provided by Amazon Q Business.The user makes a request that requires action through a custom plugin. Use custom plugins to integrate third-party applications.Use Amazon Q Business to improve enterprise productivityAmazon Q Business, offers numerous practical applications across enterprise functions. Let’s explore some of the key use cases where Amazon Q Business can enhance organizational efficiency and productivity.Knowledge management and support: Amazon Q Business can manage and retrieve information from documentation and repositories such as internal wikis, SharePoint, Confluence, and other knowledge bases. It provides contextual answers through natural language queries and helps maintain documentation quality by suggesting updates while connecting related information across different repositories. For examples, see Smartsheet enhances productivity with Amazon Q Business. Shorten IT response times by using AI-driven assistance that delivers round-the-clock support and intelligent troubleshooting guidance. By automating ticket management and using historical data for solution recommendations, this system dramatically reduces response times while easing the burden on your IT support teams. Support your HR operations and increase employee satisfaction with an AI-powered solution that provides quick answers to policy questions and streamlines benefits management. This intelligent assistant guides employees through HR processes, simplifies leave management, and offers quick access to essential forms and documents, creating a more efficient and user-friendly HR experience. Strengthen your sales and marketing efforts with an AI-powered platform that streamlines content creation, market analysis, and proposal development. From generating fresh content ideas to quickly providing product information and competitor insights, teams can use this solution to respond faster to customer needs while making data-driven decisions. See How AWS sales uses Amazon Q Business for customer engagement. Upgrade and improve your operational workflow with AI-driven monitoring and automation that transforms system management and incident response. From real-time performance tracking to automated routine tasks and intelligent root cause analysis, teams can use this solution to maintain operational efficiency and reduce manual intervention.A leading enterprise organization transformed its operational efficiency by implementing Amazon Q Business to tackle widespread knowledge accessibility challenges. Prior to implementation, the company struggled with fragmented institutional knowledge scattered across multiple systems, causing significant productivity losses as employees—from systems analysts to executives—spent hours daily searching through documentation, legacy code, and reports.By deploying Amazon Q Business, the organization centralized its scattered information from various sources including Amazon Simple Storage Service (Amazon S3) buckets, Jira, SharePoint, and other content management systems into a single, intelligent interface. The solution dramatically streamlined access to critical information across their complex ecosystem of enterprise resource planning (ERP) systems, databases, sales platforms, and e-commerce integrations.With approximately 300 employees each saving two hours daily on routine information retrieval tasks, the company achieved remarkable productivity and efficiency gains. Beyond the gains, Amazon Q Business fostered smarter collaboration, reduced subject-matter expert (SME) dependencies, and accelerated decision-making processes, effectively redefining how enterprise knowledge is accessed and used across the organization.Amazon Q Business offers AWS customers a scalable and comprehensive solution for enhancing business processes across their organization. By carefully evaluating your use cases, following implementation best practices, and using the architectural guidance provided in this post, you can deploy Amazon Q Business to transform your enterprise productivity. The key to success lies in starting small, proving value quickly, and scaling systematically across your organization.For more information on Amazon Q Business, including detailed documentation and getting started guides, visit: is a Principal Solutions Architect at AWS based in New York and is passionate about GenAI and public blockchain use cases. He has over 20 years of experience working with financial institutions and helps his customers get their cloud transformation off the ground. Outside of work he enjoys spending time with his family and training for the next Ironman. is a Senior Solutions Architect at AWS. He works as a trusted advisor for customers, guiding them through innovation with modern technologies and development of well-architected applications in the AWS cloud. Outside of work, Krishna enjoys reading, music and exploring new destinations. is a Generative AI Specialist at AWS on the Amazon Q Business team, where he helps enterprise customers leverage generative AI to transform workplace productivity and unlock business intelligence. With expertise in AI-powered search, deep research capabilities, and agentic workflows, he enables organizations to break down data silos and derive actionable insights from their enterprise information.]]></content:encoded></item><item><title>How to Build a Self-Correcting AI Agent for Product Search in E-Commerce</title><link>https://dev.to/chrisywz/how-to-build-a-self-correcting-ai-agent-for-product-search-in-e-commerce-43di</link><author>Chris Zhang</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 20:26:11 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Shopify just launched AI agents that let shoppers search, explore, and purchase using natural language.If you’ve tried retrieval-augmented generation (RAG) pipelines for product search, you’ve probably hit the usual walls: vague results, brittle prompts, and silent failures when the data isn’t structured just right. When your catalog involves complex product descriptions, categorizations and multiple supporting documents, a basic retrieval or prompt-based approach just doesn’t cut it.In the age of agentic commerce, how can we enable users to say things like “I have a small family of four. We live in Munich. What’s the best internet plan for us?” and have the system identify relevant products, draft an initial proposal, review and refine it based on available data, and engage in a meaningful conversation?In this post, you’ll learn how to build a practical AI agent for searching product catalogs using Enthusiast, an AI toolkit designed for e-commerce and knowledge-intensive tasks. We will cover setting up the environment, customizing the agent, and quickly testing it on sample data.But first, let’s look at how agentic workflows differ from traditional pipelines and why that matters.Non-Agentic Workflow vs. Agentic WorkflowIn a traditional (non-agentic) workflow, product search is driven by fixed queries or rigid filter logic. It’s simple and fast, but struggles with nuanced language or evolving user intent. The system can’t adapt on the fly. It just follows predefined instructions.
On the other hand, an agentic workflow introduces flexibility and adaptability. AI agents dynamically interpret user inputs, construct queries intelligently, and adjust their approach based on the context of the interaction and feedback received. This allows them to handle more complex, ambiguous requests while improving reliability and user experience.What Makes Up an AI AgentTo build an effective AI agent for product catalog search, the following components are essential:Input Handling: Accepts and interprets user requests.Feedback Handling and Memory: Incorporates user and system feedback to improve future interactions and maintains memory of past interactions.Tools: Interfaces with external tools or databases to execute tasks.Reasoning: Analyzes input and feedback to make informed decisions.To build such an agent, we need an execution environment. Let’s explore how Enthusiast can serve as an effective option.Most LangChain tutorials stop at toy examples or require heavy customization to support real-world workflows. Enthusiast changes that. It’s built from the ground up to support:Tool-based agents with LangChain and ReActSQL-backed querying with Django or external sourcesStructured memory and retry logic out of the boxOpen-source, customizable behaviorSelf-hosting with cloud/local model supportWhether you're debugging search in a product catalog or surfacing relevant documents across internal departments, Enthusiast gives you a working foundation in minutes with real production logic, not just playground demos.Alright, now let’s bring that to life. We’ll walk through a real case: spinning up a local environment, loading data, and creating a self-correcting LangChain agent that actually understands and interacts with your product catalog.Setting Up the Development EnvironmentTo get started, you need to set up your development environment by cloning the Enthusiast starter repository and using its Docker configuration.Clone the repository:git clone https://github.com/upsidelab/enthusiast-starterNavigate into the repository directory:Copy default configuration file and add your own OpenAI API key:cp config/env.sample config/envecho OPENAI_API_KEY=xxxx >> config/envBuild and run the Docker containers:You’ll be prompted to create your first dataset. Give it a name, for example, “My Dataset”. Import a Sample Product Dataset
Enthusiast comes with a sample set of products that can be useful if you want to get started quickly. In this case, we have a set of products that represent different phone and mobile plans - with varying internet speeds, data limits, landline access, cable TV options, and more. They make a great test case for experimenting with different approaches to agentic product recommendations. Let’s import this into our dataset:Click on “Add Source” in the top-right corner of the screen.From the dropdown, select “Product source”.A popup will appear for configuring the source.Select “Sample Product Source” from the list and click “Add”.You should now see it listed under configured sources.Repeat the same process for documents by selecting “Document source” from the dropdown.This time, choose “Sample Document Source” as the type and add it as well.Enthusiast will automatically index the dataset so it’s searchable right away.Once the data is loaded, you can go to the Products tab to verify that the sample data was successfully imported and indexed. This ensures that your dataset is ready for querying by the agent.Create a Custom Agent StructureNow that your product catalog is loaded, it’s time to build an agent that can operate on it. Enthusiast supports extending and modifying agent behavior through the enthusiast_custom directory in the project.Inside the enthusiast-starter repository, locate the src/enthusiast_custom directory. This is the package that contains your custom agents and plugins. This code will be bundled by the Dockerfile and automatically installed into your Enthusiast instance.Let’s also install a plugin that provides a reusable base implementation for a ReAct-style agent. Run the following command inside the src/ directory to add the plugin:poetry add enthusiast-agent-re-actThen, create a new directory inside enthusiast_custom, calling it for example product_search. Inside this directory, add an empty .py file to make it a Python package. This is where you’ll define your agent’s implementation.Add your new agent to the config/settings_override.py file so that Enthusiast can recognize it. Update the AVAILABLE_AGENTS dictionary to include your custom module:You can now rebuild and restart your Docker Compose setup to apply these changes:
docker compose up --buildOnce the application is restarted, you’ll see your new agent listed in the UI on the left. Time to give it some logic.Step 1 – Generate an SQL QueryWe’ll start with a basic implementation that generates an SQL query and executes it on the product catalog indexed in Enthusiast. The agent will reason through user queries and interact with the catalog to retrieve relevant results.To do this, we’ll use the enthusiast-agent-re-act plugin that we added earlier. It provides a BaseReActAgent class, which defines the core structure of a ReAct-style agent, including how it connects prompts, tools, memory, and output processing.Here’s how we’ll structure the product_search agent module:Start by defining the agent class. In a basic scenario, no overrides are required - agent’s default implementation will respond to user’s queries by creating an agent executor configured with tools and memory, and will pass the user’s request there.
Here’s what the simplest implementation looks like:product_search/product_search_tool.pyNext, implement a tool the agent can use to run SQL queries against your product catalog.Let’s first declare the expected input schema using a Pydantic model. This schema will be provided to the agent together with the tool definition, to let the agent determine what’s needed to call this tool. Since we specify that the tool requires an SQL query, the agent will try to produce one based on everything it knows so far in order to invoke it.This tool receives an SQL string from the agent, executes it using Django’s ORM, serializes the resulting product objects, and returns a message with the result. The NAME and DESCRIPTION fields in the tool definition help the agent determine when this tool is relevant to the current task. Here’s a basic version of the tool implementation:Then, create the system prompt that will guide how the agent reasons and interacts with tools. Add the following:Finally, wire everything together in the config file. This tells Enthusiast which components make up your agent:Once these components are in place and the Docker container is rebuilt, try executing a sample query:
What’s the best plan for a small family?
The agent will reason about the input, construct an SQL query, and invoke the search tool, likely failing due to invalid schema or search criteria. Let’s see what we can do with that.Step 2 – Let the Agent Handle Its Own ErrorsIn the initial version, if the SQL query generated by the agent was incorrect, the tool would simply fail without giving the agent any indication of what went wrong. We can improve this by modifying the tool to catch SQL errors and return the error message as part of the response.This way, the agent can treat the error as feedback and make another attempt, refining the query on its own.To do this, update the run method in ProductSearchTool as follows:With this change, when the SQL query fails, the agent gets the error message and can use it to revise its approach. Since the agent maintains memory of previous steps, it can iterate on its output to try and produce a valid query.Try running the same query again:
What’s the best plan for a small family?If the first attempt fails, the agent will receive the error, analyze it, and try to generate a better query.Step 3 – Help the Agent Understand the DataLetting the agent correct its own mistakes is helpful, but trial and error can be inefficient. Instead of waiting for the agent to fail and recover, we can give it a clearer understanding of the data structure up front.One simple way to do this is by including a few sample rows from the product catalog directly in the prompt. This helps the agent understand both the schema and the shape of the data, which improves its chances of generating valid queries from the start.To add this context, let’s override the get_answer method in your agent like this:This method will use functionality provided by the base class to build a LangChain-based agent executor, pass the input to it, and return the response to the user. One important change here is that besides user’s input (passed as input_text ), it will also pull a few sample products from the database and will inject them into the agent’s system prompt as sample_products.In your prompt template (prompt.py), add this placeholder at the end:
Here are some sample products in the database: {sample_products}This additional context will be included with every call to the agent. It initializes the agent with a basic understanding of the structure and shape of the data, which makes it easier for the agent to construct accurate queries from the start.
Let’s give it a try.You should notice that the agent now constructs queries that better match how the data is shaped. For example, it may use the category column to search for plans labeled as “Home,” or rely on the properties column to filter for plans with specific internet speeds.Step 4 – Retry When No Results Are FoundEven if the agent is capable of generating valid SQL queries and has seen sample data, there’s still a chance it will produce a query that technically runs but returns no results.In the current implementation, when that happens, the tool simply returns an empty list, and the agent assumes there are no relevant options. But in reality, the issue may be with how the agent built the query, not with a lack of products.To address this, we can update the tool to return a clear message when no products are found—encouraging the agent to try a different approach. Here’s how the updated run method might look:With this change, the agent receives explicit feedback when a query returns no matches. It can then choose to revise the query and try again with broader or alternative criteria.This gives the agent an opportunity to step back and reconsider its assumptions, leading to better resilience and more accurate results when dealing with uncertain or ambiguous user requests.Step 5 – Respect the Expected Number of ResultsIn some cases, a user might indicate how many products they want to see—perhaps just one recommendation or the top three matches. Right now, the agent doesn’t take that into account. It may return a long list of results, even if the user only wanted a few.We can improve this by passing the expected number of results as part of the tool input. The tool will then check whether the number of matches exceeds this limit. If it does, it will prompt the agent to follow up and narrow the criteria.First, update the input schema to include this new parameter:This addition helps turn the agent into a more effective product search assistant. Instead of assuming that the initial results are appropriate, the agent now reflects on the quantity of data returned, checks it against user expectations, and adjusts accordingly. This creates a more collaborative flow where the agent and user refine the query together to land on a relevant result.Step 6 – Enable the Agent to Finalize a PurchaseOnce the user finds a plan that matches their needs, the next logical step is to help them act on it. Right now, our agent can recommend products but doesn’t support any kind of checkout process.To make this possible, we’ll give the agent the ability to generate a contract URL the user can follow to finalize their purchase. This effectively allows the agent to transition from discovery to action.Start by creating a new tool, PurchaseTool, which accepts a plan_sku and returns a contract finalization link:Lastly, modify the search tool’s return message slightly to encourage the agent to propose a contract. The agent will likely figure it out even without this hint, but there’s no harm in pushing it more explicitly:With this addition, your agent becomes a guided assistant that helps the user discover a suitable plan and smoothly transition into completing the purchase.Step 7 – Ask for Additional Customer DetailsBefore the agent pushes the user to sign a contract, it can also ensure that it collects any additional information needed to complete the process—such as the customer’s name and location.
To support this, update the PurchaseToolInput schema with two new fields:Thanks to the structured schema and tool description, the agent will know that it must collect these inputs from the user before invoking the tool. If the information isn’t provided initially, the agent can follow up with questions like:Could you tell me your name and zip code so I can finalize the contract?This closes the loop and ensures that the agent not only helps discover the right plan but can also guide the user through to a complete and personalized purchase process.In this walkthrough, we explored how to build a practical AI agent for product catalog search using Enthusiast. Starting from a basic ReAct-style agent capable of generating SQL queries, we incrementally introduced more sophisticated behaviors:Error recovery through exception feedbackSchema-aware reasoning via sample dataRetry logic when no results are foundAdapting results to match user expectationsFinalizing user purchases with structured follow-upCollecting required customer details before contract generationEach step was designed to bring the agent closer to an experience that feels like a helpful, iterative assistant.]]></content:encoded></item><item><title>Speed up delivery of ML workloads using Code Editor in Amazon SageMaker Unified Studio</title><link>https://aws.amazon.com/blogs/machine-learning/speed-up-delivery-of-ml-workloads-using-code-editor-in-amazon-sagemaker-unified-studio/</link><author>Paul Hargis</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 20:24:35 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Amazon SageMaker Unified Studio is a single integrated development environment (IDE) that brings together your data tools for analytics and AI. As part of the next generation of Amazon SageMaker, it contains integrated tooling for building data pipelines, sharing datasets, monitoring data governance, running SQL analytics, building artificial intelligence and machine learning (AI/ML) models, and creating generative AI applications. Recently, AWS announced two additional options that enhance the development experience for analytics, ML, and generative AI teams: Code Editor and multiple spaces. These new IDE options can help developers and data scientists speed up delivery of ML workloads by offering familiar IDE layouts, using popular extensions to enhance development, and using critical debug and test options, all within a unified environment.Code Editor, based on Code-OSS (Visual Studio Code – Open Source), provides a lightweight and powerful IDE with familiar shortcuts and terminal access, along with advanced debugging capabilities and refactoring tools. The VSCode IDE, and Code-OSS variants like Code Editor, remain the most popular development tool in recent years. Teams can boost their productivity by accessing thousands of Code Editor-compatible extensions from the Open VSX extension gallery. The Code Editor IDE within SageMaker Unified Studio supports version control and cross-team collaboration through GitHub, GitLab, or Bitbucket repositories, while offering preconfigured SageMaker distribution for popular ML frameworks.Within SageMaker Unified Studio, a  is a work environment that runs a particular IDE. To maximize the benefits of Code Editor alongside other coding interfaces in SageMaker Unified Studio, including JupyterLab, SageMaker now supports multiple spaces per user per project. With multiple spaces, users can manage parallel workstreams with different computational needs. Each space maintains a 1-to-1 relationship with an application instance, so users can efficiently organize their storage and resource requirements. This enhancement provides the flexibility to access multiple applications and instances simultaneously, improving workflow management and productivity.In this post, we walk through how you can use the new Code Editor and multiple spaces support in SageMaker Unified Studio. The sample solution shows how to develop an ML pipeline that automates the typical end-to-end ML activities to build, train, evaluate, and (optionally) deploy an ML model.Features of Code Editor in SageMaker Unified StudioCode Editor offers a unique set of features to increase the productivity of your ML team:Fully managed infrastructure – The Code Editor IDE runs on fully managed infrastructure. SageMaker takes care of keeping the instances up-to-date with the latest security patches and upgrades.Dial resources up and down – With Code Editor, you can seamlessly change the underlying resources (such as instance type or EBS volume size) on which Code Editor is running. This is beneficial for developers who want to run workloads with changing compute, memory, and storage needs.SageMaker provided images – Code Editor is preconfigured with Amazon SageMaker Distribution as the default image. This container image has the most popular ML frameworks supported by SageMaker, along with the SageMaker Studio SDK, SageMaker Python SDK, Boto3, and other AWS and data science specific libraries installed. This significantly reduces the time you spend setting up your environment and decreases the complexity of managing package dependencies in your ML project. – Code Editor also comes with generative AI capabilities powered by Amazon Q Developer. You can boost your productivity by generating inline code suggestions within the IDE. In addition, you can use Amazon Q chat to ask questions about building at AWS and for assistance with software development. Amazon Q can explain coding concepts and code snippets, generate code and unit tests, and improve code, including debugging or refactoring.Extensions and configuration settings – Code Editor also includes persistence of installed extensions and configuration settings.When you open Code Editor, you will notice that the space has been bootstrapped with the current state of your project’s repository. Navigate to the file explorer, and you will find a  Jupyter notebook, as shown in the following screenshot.You can choose  to execute this notebook. Select  when prompted to select the kernel and then choose the recommended Python environment named . Now the  notebook will be executed, and you can explore the output of the various cells.Architecture of Code Editor in SageMaker Unified StudioWhen you open Code Editor in SageMaker Unified Studio, it creates an application container that runs on an Amazon Elastic Compute Cloud (Amazon EC2) instance. This instance type matches your selection during Code Editor space configuration. The underlying infrastructure management happens automatically in a service-managed account controlled by SageMaker Unified Studio. The following diagram shows the infrastructure as it relates to end-users and how instances are provisioned. User A has configured two spaces, and User B is using a single space. Both users have the option to create additional spaces as needed. Currently, these spaces are isolated private environments, with shared space functionality planned for a future release.SageMaker Unified Studio lets you create multiple spaces with Code Editor or JupyterLab as the IDE, each configurable with different ML instance types, including those with accelerated computing capabilities. For each space, you must specify three core elements: the EBS volume size, your chosen instance type, and the application type you want to run (such as Code Editor or JupyterLab). When you initiate a space, SageMaker Unified Studio automatically provisions a compute instance and launches a SageMaker Unified Studio Code Editor application using your specified container image. The storage system is designed for continuity: your EBS volume persists across sessions, even when you stop and restart the IDE. This means that when you stop the Code Editor application to save on computing costs, although the compute resources shut down, your EBS volume is preserved. Upon restart, the system automatically reattaches this volume, so your work remains intact.In the following sections, we show how to develop an ML project with Code Editor on SageMaker Unified Studio. For this example, we run through a Jupyter notebook that creates an ML pipeline using Amazon SageMaker Pipelines, which automates the usual tasks of building, training, and (optionally) deploying a model.In this scenario, Code Editor can be used by an ML engineering team who needs advanced IDE features to test and debug their code, create and execute a pipeline, and monitor the status in SageMaker Unified Studio.To prepare your organization to use the new Code Editor IDE and multiple spaces support in SageMaker Unified Studio, complete the following prerequisite steps:By default, authentication and authorization for a SageMaker Unified Studio domain is controlled through IAM Identity Center, which can only be configured in a single AWS Region that must be the same Region as your SageMaker domain. See Setting up Amazon SageMaker Unified Studio for additional information.Create a SageMaker Unified Studio domain using the quick setup. A virtual private cloud (VPC) is required; one will be created for you (if needed) during setup.After you create the domain, you can enable access to SageMaker Unified Studio for users with single sign-on (SSO) credentials through IAM Identity Center by choosing  next to Configure SSO user access in the Next steps for your domain section.After you configure user access for your newly created domain, navigate to the SageMaker Unified Studio URL and log in using SSO.You can find the URL on the SageMaker console, as shown in the following screenshot.By default, IAM Identity Center requires multi-factor authentication on user accounts, and you might be prompted to configure this upon first login to SageMaker Unified Studio, as shown in the following screenshot. For more details about this requirement, refer to Registering your device for MFA.After you log in, choose  and follow the prompts to create your first SageMaker Unified Studio project. We choose the  project profile during setup.After you create a project, you can create your space (an IDE) in which Code Editor will be provisioned.On the  tab of the project, choose , then enter a name and choose .When the  column indicates the space is , open the space to be redirected to Code Editor.Interacting with AWS services directly from your IDEThe AWS Toolkit for Visual Studio Code uses the permissions of the AWS Identity and Access Management (IAM) role assigned to the project. You can find the Amazon Resource Name (ARN) of the project role on the project details page, as shown in the following screenshot.Use Code Editor to create and execute an ML pipeline in SageMakerIn this section, we upload and execute a Jupyter notebook that creates and starts a machine learning operations (MLOps) pipeline orchestrated with SageMaker Pipelines. The pipeline we create follows a typical ML application pattern of data preprocessing, training, evaluation, model creation, transformation, and model registration, as illustrated in the following diagram.Begin by uploading the sample notebook directly into Code Editor. You can drag and drop the notebook, or right-click and choose in the file explorer pane.You can download and run sample notebooks using standard  commands from the GitHub repository where these notebooks are located. Running the Full Pipeline notebook sample requires a few extra IAM role permissions other than the defaults assigned when the SageMaker Unified Studio project is created. The Quick Pipeline can be run as-is with the default IAM permissions.Region availability, cost, and limitationsCode Editor and multiple spaces support are available in supported SageMaker Unified Studio domains. For more information about Regions where these features are available, see Regions where Amazon SageMaker Unified Studio is supported. Code Editor will be provisioned within a SageMaker space and run on a user-selectable instance type, anywhere from ultra low-cost instances (ml.t3.medium) up to highly performant GPU-based instances (G6 instance family).The primary cost associated with running a Code Editor space is tied directly to the underlying compute instance type. The hourly costs for ML instance types can found on the Amazon SageMaker AI pricing page on the  tab. To prevent unnecessary charges, the space will be automatically shut down after a configurable timeout when the space is idle (see SpaceIdleSettings). There will also be minimal charges tied to storage for the EBS volume that is attached to the Code Editor space.At launch, Code Editor spaces can be configured to use a particular SageMaker Distribution image, either version 2.6 or 3.1. Additional major and minor releases of the SageMaker Distribution will be added over time.To avoid incurring additional charges, delete the resources created from following this post. This includes any development environments created, such as Code Editor or JupyterLab spaces, which you can delete by navigating to the  navigation pane, choosing the  tab, choosing the options menu (three vertical dots) aligned with the space, and choosing . You can remove project resources by deleting the project, which can be done from the SageMaker Unified Studio console. There is no charge for a SageMaker Unified Studio domain, but you can optionally delete this from the SageMaker AI console. If you created IAM Identity Center users that you no longer need, delete the users from the IAM Identity Center console.The addition of the new Code Editor IDE to SageMaker Unified Studio provides a familiar working environment to thousands of data scientists and developers. With this powerful IDE, data scientists can more quickly build, train, tune, and deploy their ML models and push them into production where they can get measurable ROI. With thousands of pre-tested extensions through the VSX Registry, developers will have improved usability and productivity as they build and deploy their generative AI applications.In addition, SageMaker Unified Studio now supports multiple spaces per user per project. These new environment options can help MLOps personas segregate workloads, isolate compute resources, and increase productivity through parallelized workstreams. Together, these enhancements help data science teams work more efficiently in bringing ML and generative AI solutions into production, where they can begin to reap the benefits of their work.To get started using SageMaker Unified Studio, refer to the Amazon SageMaker Workshop. This workshop provides complete step-by-step instructions, plus sample datasets, source code, and Jupyter notebooks for gaining hands-on experience with the tooling. has focused his efforts on machine learning at several companies, including AWS, Amazon, and Hortonworks. He enjoys building technology solutions and teaching people how to leverage them. Paul likes to help customers expand their machine learning initiatives to solve real-world problems. Prior to his role at AWS, he was lead architect for Amazon Exports and Expansions, helping amazon.com improve the experience for international shoppers. is an AI/ML Specialist Solutions Architect at Amazon Web Services. He enjoys helping customers build and adopt AI/ML solutions using AWS technologies and best practices. Prior to his role at AWS, he spent many years in technology consulting with customers across many industries and geographies. In his free time, he enjoys running and playing with his dogs! is a Senior Software Engineer at Amazon with over 15 years of experience in backend development and design. He is currently working on improving Seller Partner Support Experience at Amazon. As a technical leader, Jayan has successfully built and mentored engineering teams across organizations, while also contributing to the broader tech community through speaking engagements such as SRECon Asia.Majisha Namath Parambath is a Senior Software Engineer at Amazon SageMaker with 9+ years at Amazon. She’s provided technical leadership on SageMaker Studio (Classic and V2) and Studio Lab, and now leads key initiatives for the next-generation Amazon SageMaker Unified Studio, delivering an end-to-end data analytics and interactive machine learning experience. Her work spans system design and architecture, and cross-team execution, with a focus on security, performance, and reliability at scale. Outside of work, she enjoys reading, cooking, and skiing.]]></content:encoded></item><item><title>Where Hurricanes Hit Hardest: A County-Level Analysis with Python</title><link>https://towardsdatascience.com/where-hurricanes-hit-hardest-a-county-level-analysis-with-python/</link><author>Lee Vaughan</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 20:06:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Use Python, GeoPandas, Tropycal, and Plotly Express to map the number of hurricane encounters per county over the past 50 years.]]></content:encoded></item><item><title>Designing Trustworthy ML Models: Alan &amp; Aida Discover Monotonicity in Machine Learning</title><link>https://towardsdatascience.com/designing-trustworthy-ml-models-alan-aida-discover-monotonicity-in-machine-learning/</link><author>Mehdi Mohammadi</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 19:44:29 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Accuracy alone doesn’t guarantee trustworthiness. Monotonicity ensures predictions align with common sense and business rules.]]></content:encoded></item><item><title>How We Reduced LLM Costs by 90% with 5 Lines of Code</title><link>https://towardsdatascience.com/how-we-reduced-llm-cost-by-90-with-5-lines-of-code/</link><author>Uri Peled</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 19:08:12 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[When clean code hides inefficiencies: what we learned from fixing a few lines of code and saving 90% in LLM cost.]]></content:encoded></item><item><title>CRYSTALS - The Gently Introduction</title><link>https://dev.to/isohanni/crystals-the-gently-introduction-15b9</link><author>Jani</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 18:16:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[We will explore how to implement a post-quantum cryptography algorithm(s) CRYSTALS. We start the journey by introducing some needed tools and in the following posts implement the actual algorithm. If you are interested in the dirty details how to turn math to code, this is for you.[This article was written in another platform first and I had some painful time to get it rendering even remotely correct here. I hope it is readable enough.]CRYSTALS ("Cryptographic Suite for Algebraic Lattices") is post-quantum public key encryption scheme, meaning it is expected to be secure even at the era of quantum computing where many current PKE-variants fail.CRYSTALS consists of two cryptographic primitives: Kyber and Dilithium. Kyber is key exchange method, i.e. asymmetric encryption providing secure channel to change secrets, and Dilithium is a cryptographic signing method. We will explore the mathematics behind these algorithms by coding them in Python as we go. You can find the code from my GitHub repo.The reader is assumed certain maturity in mathematics and basic understanding of Python. We don't prove anything, instead the focus is to introduce and build needed machinery that we will use later. 
The mathematical part of this presentation follows closely the way Alfred Menezes presents it in his excellent video series on the topic.When we say two integers 


 and 

 are congruent modulo 

 we mean 

 is a integer multiple of 

  In this case we write 
With 

 we mean 

 is the remainder of integer 

 divided 

 This implies 

 is the ring of integers modulo 

  In this ring addition and multiplication are performed modulo 
We implement integers in 

 in class Zq. Notice the Python modulo-operation % is implemented in a way that is fully compatible with our needs because it can handle negative values correctly. The instantiation can be done with integer value or with an instance of Zq.The class Zq has addition, subtraction, multiplication, str and repr operations implemented. This makes our life a lot easier because we can make arithmetics directly and debug when needed.To get a feeling how things work, consider the ring 

 For example we have 
Let 

 be a prime. We define 

 to be the set of polynomials of 

 with all coefficients in the ring 

 This means all coefficient arithmetic is performed in the ring 
We implement polynomials in the ring with a class ZqPolynomial. Here coefficients is a list of integers, and the length of the list defines 
For example, let 
We can do this with our code as follows (we use extra zeroes in coefficients to prevent the polynomial modulo operation).Let now 

 be a prime and 

 a positive integer. The quotient ring (often called just "polynomial ring") 

 consists of polynomials in 

 of degree less than 

 In ring 

 the multiplication of polynomials is performed modulo the polynomial 

 called the reduction polynomial. This means that the product of polynomials 

 is defined as the remainder 

 of their product when divided by 

 in the polynomial ring. Notice that by definition now degree of 

 is at most 

 and 
One should notice here that remainder is not calculated by the traditional polynomial division algorithm, but with division rules that apply in the polynomial ring. For our purposes it suffices to acknowledge that if the polynomial has degrees 

 you can apply the rules 

 and in general for 

 and then simplify the resulting polynomial normally. To understand why, please visit ring theory and ideals.

Overloading addition and subtraction is straightforward, but multiplication needs special treatment. Here we utilize the fact that Zq has multiplication operation overloaded. In real-life implementations this naive implementation is too slow and NTT-algorithm is used instead. We will return to this later.For example, consider the ring 
To get the reminder of 

 when divided 

 we first calculate the product 

 and use the substitution rule to get 

 and with the modulo operations we arrive at 
With our code we get directly as follows.For a programmer it is rather straightforward to see that the polynomial can be represented as vectors. Consider the polynomial 
The obvious way to write that as a vector is 

 (convention is to use column vectors). The polynomial addition is now component-wise addition modulo 

 and subtraction is component-wise subtraction modulo 

 Multiplication is polynomial multiplication as shown earlier and the resulting polynomial is stored in a vector. We used this implicitly earlier when defining ZqPolynomial.We extend this notation. Let 

 be a positive integer. Module 

 consists of length 

 vectors of polynomials of 

. Addition and subtraction is again component-wise. It follows that the resulting vectors in both cases is in 
We will later use 

 to represent 

-matrices with polynomial entries. This extension is similar to that from vectors to matrices in linear algebra.The module 

 includes 

-matrices with elements from the polynomial ring. The easiest way to handle these is to define class PolyMatrix that holds ZqPolynomials in a list of lists.The multiplication in 

 is defined as inner product of two vectors in 

 This means that the polynomials, that are elements of the vector in 

 are multiplied in 

 and added together. The result is in 

and 

 We get directly 

 and in Python as follows.Using the 

 and 

 defined earlier, we get To enable matrix multiplication, we implemented the matmul operator.We can use the bracket-notation with PolyMatrix because we defined getitem and setitem methods. Next we need notion of size that will become useful later. First let us define symmetric mod.Let 

 be odd and 

. We define 
This immediately gives 

  Here 

 is symmetric modulo operation.Let 

 We now have by definition 
The definition of symmetric modulo is slightly different for even 

 Let 

 be even and 

 and we get 

.

In the code we implement the symmetric modulo in Zq and use that from ZqPolynomial and PolyMatrix.Let 

 We define 

 Then 
You can think this as "clock-algebra", the further the integer is from noon, the bigger its norm.We have the following immediate corollaries
 if q is odd and
 if q is even.

For polynomial ring elements the size of a polynomial is defined with the maximum operation. Let 

We define 

For example, let 

 and 

 Then 

 because 

 and clearly 
This definition can be generalized to elements of module 

 Let 

 We define 
We say a polynomial 

 is small if  

 is small. Notice that this means that all coefficients of 

 need to be small due the way the norm is defined. What "small" means is defined per context.Let 

 be a positive integer less than 

 We define 

 to be the set of polynomials in 

 where each polynomials each coefficient is of size at most 

 We use 

 to define a set of "small" polynomials.

For example consider polynomial 

 Now 

 and hence 
Observe that 

 is the set of polynomials in 

 with all coefficients in the set 

 (when reduced 

).Let 

 and 

 Without proof we state that 

 This generalizes to vectors or polynomials. Let 

 and 

 Then we have 
In the next article we utilize the presented machinery to implement basic CRYSTALS-Kyber public key encryption. Stay tuned.]]></content:encoded></item><item><title>How Infosys Topaz leverages Amazon Bedrock to transform technical help desk operations</title><link>https://aws.amazon.com/blogs/machine-learning/how-infosys-topaz-leverages-amazon-bedrock-to-transform-technical-help-desk-operations/</link><author>Meenakshi Venkatesan, Karthikeyan Senthilkumar, Aninda Chakraborty</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 17:25:11 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[AI-powered apps and AI-powered service delivery are key differentiators in the enterprise space today. A generative AI-based resource can greatly reduce the onboarding time for new employees, enhance enterprise search, assist in drafting content, check for compliance, understand the legal language of data, and more.Generative AI applications are an emerging and sought-after solution in the enterprise world for customer care centers, customer relationship management centers, and help desks.In this post, we examine the use case of a large energy supplier whose technical help desk support agents answer customer calls and support meter technicians in the field. We use Amazon Bedrock, along with capabilities from Infosys Topaz, to build a generative AI application that can reduce call handling times, automate tasks, and improve the overall quality of technical support.Meter technicians go to customer locations to install, exchange, service, and repair meters. Sometimes they call support agents from the technical help desk to get guidance and support to fix issues that they can’t fix by themselves. The approximate volume of these calls is 5,000 per week, approximately 20,000 per month.Some of the challenges faced by support agents and meter technicians include:Locating the appropriate information or resources to address inquiries or concerns effectively.The average handling time for these calls varies based on the issue category, but calls in the top 10 categories, which represent over 60% of calls, are over 5 minutes.60–70% issues are repetitive, and the rest are new issues.Maintaining an adequate workforce to provide prompt responses can be costly. It’s expensive and not scalable to hire more support agents and train them with the knowledge needed to provide support. We built an AI-powered technical help desk that can ingest past call transcripts and new call transcripts in near real time. This will help support agents provide resolutions based on past calls, thereby reducing manual search time so they can attend to other priorities.The solution involves creating a knowledge base by ingesting and processing call transcripts, so that the AI assistant can provide resolutions based on history. The benefits of an AI-powered technical help desk include:Providing all-day availabilitySaving effort for the help desk agentsAllowing businesses to focus on new issuesReducing wait time and shortening call durationAutomating actions that the help desk agents take on the backend based on their analysis of the issueImproving the quality of technical help desk responses, and thereby communication and outcomesThis post showcases the implementation details, including user-based access controls, caching mechanisms for efficient FAQ retrieval and updates, user metrics tracking, and response generation with time-tracking capabilities.The following diagram shows the flow of data and processes from left to right, starting with call transcripts, going through preprocessing, storage, and retrieval, and ending with user interaction and response generation. It emphasizes the role-based access control throughout the system.Building the knowledge base: Data flow The conversations are parsed into a CSV file for sorting and a large language model (LLM), such as Anthropic’s Claude Sonnet on Amazon Bedrock, is used to summarize the conversation and determine if the context has useful information, based on the length of the call, key words that indicate relevant context, and so on.The shortlisted conversations are chunked, and embeddings are generated and stored in an Amazon OpenSearch Serverless vector store. The conversations determined to be irrelevant go into another S3 bucket for future reference. This process is automated, as shown in the following figure.A virtual assistant is then built on top of the knowledge base that will assist the support agent.The conversations are parsed into a CSV file for simple sorting and an LLM such as Anthropic’s Claude Sonnet on Amazon Bedrock is used to summarize the conversation and determine if the context has useful information, based on the length of the call, key words that indicate relevant context, and so on.An event-drivenAWS Lambda function is triggered when new call transcripts are loaded into the S3 bucket. This will trigger a Step Functions workflow.From the raw CSV file of call transcripts, only a few fields are extracted: a contact ID that is unique for a particular call session between a customer and a support agent, the  column indicating the speaker (who can be either a support agent or a customer) and the  column, which is the conversation.To build the knowledge base, we used Step Functions to ingest the raw CSV files, as shown in the following workflow.The automated workflow begins when a user uploads the JSON file to an S3 bucket.The Step Functions workflow receives the Amazon S3 URL of the CSV transcripts from a Lambda function. The  is unique for a particular call session between the customer and the agent, who are the participants, and the  is the actual conversation.The Lambda function (Parse Transcripts from CSV) uses this Amazon S3 URL to download the CSV files and uses Pandas to preprocess the CSV in a format with the contact ID and transcript only. Conversations with the same contact ID are concatenated into a single row.The second step is a classification task that ingests, classifies, and keeps or discards conversions. The conversations are passed to the map state. In map state, conversations are handled concurrently. For each conversation row, this state triggers concurrent execution of another Lambda function (Check for Irrelevant Conversations) that will classify each conversation as relevant or irrelevant. 
  For this classification task, the Lambda function uses Anthropic’s Claude Sonnet model on Amazon Bedrock. It uses zero-shot chain-of-thought prompting, to first summarize the conversation and then to determine the relevance. If the conversation is disconnected or disjointed (because of signal disturbances or other reasons), or has no meaningful context (when the agent is unable to provide resolution), it’s classified as irrelevant.Finally, the map state takes each instance of the conversation (classified as relevant or irrelevant) and passes to the choice state, which will log the irrelevant conversations into an S3 bucket and relevant conversations are passed to another Lambda function (Handle Relevant Conversations) for further processing.The final Lambda function (Log Irrelevant Conversations) reads the relevant conversations and generates the summary, problem, and resolution steps using Anthropic’s Claude Sonnet. The summary generated is used for creating the summary embeddings.The following is an example of an irrelevant conversation that is discarded.66da378c-8d74-467b-86ca-7534158b63c266da378c-8d74-467b-86ca-7534158b63c2Your morning call it said Chris Simpson near me, TX 75 is, uh, locked out spinning disc66da378c-8d74-467b-86ca-7534158b63c2No problem. What’s your carry, please?66da378c-8d74-467b-86ca-7534158b63c266da378c-8d74-467b-86ca-7534158b63c2Thank you. Right, you’ll be kicked off.66da378c-8d74-467b-86ca-7534158b63c2Single noise. Anything anyway, mate. When you look back in, you’ll be fine66da378c-8d74-467b-86ca-7534158b63c266da378c-8d74-467b-86ca-7534158b63c2Alright, Right. Thank you. Choose them.66da378c-8d74-467b-86ca-7534158b63c2I think she’s made a bit Right bye.The following is an example of a relevant conversation.079a57bf-9700-45d3-bbd9-11d2d41370c7079a57bf-9700-45d3-bbd9-11d2d41370c7Help those gathers Reagan. Yes.079a57bf-9700-45d3-bbd9-11d2d41370c7Get up, and then I’ll speak to someone about clearing the cash on my T C 75. So, can do. Off job certainly things because you won’t let me sorry minutes, just saying Could not establish network connection.079a57bf-9700-45d3-bbd9-11d2d41370c7079a57bf-9700-45d3-bbd9-11d2d41370c7Yeah, it’s not trying to do is connected. We got three D 14. It’s up, right?079a57bf-9700-45d3-bbd9-11d2d41370c7What should happen because I’m in the four G area.079a57bf-9700-45d3-bbd9-11d2d41370c7Yeah, dragged down the screen twice from the top for me.079a57bf-9700-45d3-bbd9-11d2d41370c7079a57bf-9700-45d3-bbd9-11d2d41370c7Yep. And check that survey is eight hasn’t turned itself off.079a57bf-9700-45d3-bbd9-11d2d41370c7079a57bf-9700-45d3-bbd9-11d2d41370c7There you go, right showing us connected. We can079a57bf-9700-45d3-bbd9-11d2d41370c7All right. Can you clear the cat 12 can signal is day to see this message. Contact the T. H. D.079a57bf-9700-45d3-bbd9-11d2d41370c7079a57bf-9700-45d3-bbd9-11d2d41370c7There you go. That should take you out any second, okay?The following table shows the final knowledge base schema.AGENT: Hi, how can I help you CUSTOMER: Hi, I am facing a black screen issue.…Customer is facing with a issue …If issue persist, reinstall app[0.5078125,-0.071777344,0.15722656,0.46679688,0.56640625,-0.037353516,-0.08544922,0.00012588501, …]Building an effective RAG pipelineThe success of retrieval systems relies on an effective embedding model. The Amazon Titan Text Embeddings model is optimized for text retrieval to enable Retrieval Augmented Generation (RAG). Instead of processing massive documents at the same time, we used chunking strategies to improve retrieval. We used a chunk size of 1,000 with an overlapping window of 150–200 for best results. Chunking combined with page boundaries is a simple yet highly effective approach. Sentence window retrieval also returns accurate results.Prompting techniques play a crucial role in obtaining effective results. For example, instead of “guidelines for smart meter installation,” an expanded prompt such as “instructions, procedures, regulations, and best practices along with agent experiences for installation of a smart meter” yields better results.This architecture implements comprehensive security measures across the components. We use AWS Secrets Manager to securely store and manage sensitive credentials, API keys, and database passwords, with automatic rotation policies in place. S3 buckets are encrypted using AWS Key Management Service (AWS KMS) with AES-256 encryption, and versioning is enabled for audit purposes. Personally identifiable information (PII) is handled with extreme care— PII data is encrypted and access is strictly controlled through AWS Identity and Access Management (IAM) policies and AWS KMS. For OpenSearch Serverless implementation, we make sure data is encrypted both at rest using AWS KMS and in transit using TLS 1.2. Session management includes timeout for inactive sessions, requiring re-authentication for continued access. The system interacts with access control list (ACL) data stored in DynamoDB through a secure middleware layer, where the DynamoDB table is encrypted at rest using AWS managed KMS keys. Data transmissions between services are encrypted in transit using TLS 1.2, and we maintain end-to-end encryption across our entire infrastructure. Access controls are granularly defined and regularly audited through AWS CloudTrail.Implementing role-based access controlWe used three different personas to implement role-based access control: an administrator with full access, a technical desk analyst with a medium level of access, and a technical agent with minimal access. We used OpenSearch Serverless collections to manage different access levels. Different call transcripts are ingested into different collections; this is to enable user access to the content they are authorized to based on their roles. A list of user IDs and their roles and allowed access are stored in a DynamoDB table along with the OpenSearch collection and index name.We used the  method in a Streamlit authenticator to retrieve the user ID.User interface and agent experienceWe used Streamlit as a frontend framework to build the TECHNICAL HELP DESK, with access to the content controlled by the user’s role. The UI features an FAQ section displayed at the top of the main page and a search metrics insights section in the sidebar, as shown in the following screenshot.The UI includes the following components: – The conversation section contains interactions between the user and the help desk assistant. Users can provide feedback by choosing either the like or dislike button for each response received, as shown in the following screenshot. This feedback is persisted in a DynamoDB table.– As shown in the following screenshot, the sidebar contains metrics information, including: 
  Number of queries in the last weekNumber of total transcriptsNumber of transcripts added in the last weekNumber of helpful responses generatedNumber of misses (no answer found)These fields are updated asynchronously after each user query. Additional metrics are also stored, such as sentiment, tone of the speakers, nature of responses generated, and satisfaction percentage.– The queries are stored in a DynamoDB table along with a query count column. When the help desk agent signs in, the queries with the most counts are displayed in this section, as shown in the following table.The  column is created as the global secondary index to retrieve the top five FAQs.After the user submits a query, the technical help desk fetches the top similar items from the knowledge base. This is compared with the user’s query and, when a match is found, the  column is incremented.We used the  function in Streamlit to store the valid results in memory. The results are persisted across the user sessions.The caching function employs an internal hashing mechanism that can be overridden if required. The cached data can be stored either in memory or on disk. Additionally, we can set the data persistence duration as needed for the use case. Cache invalidation or updates can be done when the data changes or after every hour. This, along with the FAQ section, has significantly enhanced performance of the technical help desk, creating faster response times and improving the user experience for customers and support agents.In this post, we showed you how we built a generative AI application to significantly reduce call handling times, automate repetitive tasks, and improve the overall quality of technical support.The enterprise AI assistant from the Infosys Agentic Foundry, part of Infosys Topaz, now handles 70% of the previously human-managed calls. For the top 10 issue categories, average handling time has decreased from over 5 minutes to under 2 minutes, a 60% improvement. The continuous expansion of the knowledge base has reduced the percentage of issues requiring human intervention from 30–40% to 20% within the first 6 months after deployment.Post-implementation surveys show a 30% increase in customer satisfaction scores related to technical support interactions. is a Principal Consultant at Infosys and a part of the AWS Centre Of Excellence at Infosys Topaz. She helps design, develop, and deploy solutions in AWS environments and has interests in exploring the new offerings and services. is a Senior Systems Engineer at Infosys and a part of the AWS COE at iCETS. He specializes in AWS generative AI and database services. is a Senior Systems Engineer at Infosys and a part of the AWS COE at iCETS. He specializes in generative AI and is passionate about leveraging technology to create innovative solutions that drive progress in this field. is an accomplished software technologist and Technical Leader at Amazon Web Services, where he specializes in Generative AI solutions architecture. With a rich background in software development and data engineering, he architects enterprise-scale AI solutions that bridge innovation with practical implementation. A respected voice in the tech community, he regularly contributes to industry discourse through speaking engagements and thought leadership on Generative AI applications, Data engineering, and ethical AI practices. is a Senior Solutions Architect with a deep specialization in Generative AI. In his current role, he collaborates closely with NAMER System Integrator (SI) partners, providing expert guidance to architect enterprise-scale AI solutions. Vishal’s expertise lies in navigating the complex landscape of AI technologies and translating them into practical, high-impact implementations for businesses. As a thought leader in the AI space, Vishal is actively engaged in shaping industry conversations and sharing knowledge. He is a frequent speaker at public events, webinars, and conferences, where he offers insights into the latest trends and best practices in Generative AI.is a Solutions Architect with Amazon Web Services, specializing in Generative AI and data analytics domains. He works with AWS customers and partners to architect and implement scalable analytics platforms and AI-driven solutions. With deep expertise in Generative AI services and implementation, end-to-end machine learning implementation, and cloud-native data architectures, he helps organizations harness the power of GenAI and analytics to drive business transformation. He can be reached via LinkedIn.]]></content:encoded></item><item><title>My AI Unit Test Agent is Alive! Now for Part 2: The QA Agent 🤖</title><link>https://dev.to/herchila/my-ai-unit-test-agent-is-alive-now-for-part-2-the-qa-agent-2j27</link><author>Hernán Chilabert</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 17:21:55 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Just a week ago, I shared that I started building an AI agent to handle my unit tests in Python. Well, the little guy is officially up and running!Phase 1 is a wrap! The MVP is working as planned: it can analyze a Python file, figure out what's inside, and use an LLM to generate a solid suite of pytest tests. It feels a bit like magic watching it work. I'm super happy with the foundation we've got.But now... the real fun begins.
  
  
  Entering Phase 2: The "QA Engineer" Agent
The first agent is the "Dev Engineer"—it writes the code. Now, I'm building its partner: the "QA Engineer" agent.So, what's its job? This new agent will:: It will actually execute pytest on the tests the first agent wrote.: Did the tests pass? Did they fail? Why?: It will then go back to the "Dev Engineer" and say something like, "Hey, this test you wrote is failing because of X," or "You missed covering this edge case."The goal here is to create an autonomous feedback loop. The two agents will collaborate, refine, and improve the tests until they meet a certain quality bar, all on their own. Wild, right?This is the part of the project I've been most excited about, where it starts to feel less like a script and more like a real, autonomous team.As always, the project is fully open-source. You can follow along with the progress, check out the code, and see the roadmap on GitHub.Thanks for reading and following the journey! Let me know in the comments what you think about this two-agent approach.]]></content:encoded></item><item><title>10 In-Depth Python Tricks to Supercharge Your Automation Projects</title><link>https://dev.to/codetestfactory/10-in-depth-python-tricks-to-supercharge-your-automation-projects-noo</link><author>Sohail Mohammed</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 17:12:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[🚀 𝗙𝗿𝗼𝗺 𝟭𝟬𝟬+ 𝗳𝗮𝗶𝗹𝗶𝗻𝗴 𝘁𝗲𝘀𝘁𝘀 → 𝟳𝟲% 𝗳𝗲𝘄𝗲𝗿 𝗳𝗮𝗶𝗹𝘂𝗿𝗲𝘀.That’s what happened when I applied 𝟭𝟬 𝗣𝘆𝘁𝗵𝗼𝗻 𝘁𝗿𝗶𝗰𝗸𝘀 into my automation framework.Most QA teams blame flaky environments or unstable APIs for test failures. But often, it’s about how you design your framework.In my latest blog, I break down:
✅ 10 in-depth Python tricks I used
✅ How they helped reduce test failures by 76%
✅ Why these tricks can supercharge any automation project🔗 𝗖𝗵𝗲𝗰𝗸𝗼𝘂𝘁 𝘁𝗵𝗲 𝗳𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 𝗵𝗲𝗿𝗲:👉 Curious: What’s the #1 Python trick or framework tweak that helped you stabilize your tests? Drop it in the comments 👇]]></content:encoded></item><item><title>death and gravity: Announcing asyncio-thread-runner: you can have a little async (as a treat)</title><link>https://death.andgravity.com/asyncio-thread-runner</link><author></author><category>dev</category><category>python</category><pubDate>Thu, 21 Aug 2025 16:43:37 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[I'm happy to announce that you can now install it from PyPI,
and read the
documented, tested, type-annotated code
on GitHub! ⭐️This is useful when you're doing some sync stuff, but:you also need to do some async stuff,  making maybe the sync stuff is an existing applicationmaybe you still want to use your favorite sync libraryor maybe you need just a little async, without having to pay the full priceit allows you to use  and  from sync code$pipinstallasyncio-thread-runner
]]></content:encoded></item><item><title>🔥 The Secret Edge of TinyGo: Run Go Code on a $2 Microcontroller and Blow Your Mind</title><link>https://dev.to/ekwoster/the-secret-edge-of-tinygo-run-go-code-on-a-2-microcontroller-and-blow-your-mind-5djk</link><author>Yevhen Kozachenko 🇺🇦</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 21 Aug 2025 16:01:20 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  Introduction: Go Where No Gopher Has Gone Before
Go (Golang) has revolutionized backend development with its blazing speed, simplicity, and concurrency model. But what if you could run Go code—yes, honest-to-god Go—on a microcontroller that costs less than a cup of coffee? Enter TinyGo: a game-changing compiler that brings the power of Go to the world of embedded devices and WebAssembly. In this post, we’ll walk through running TinyGo on a $2 RP2040 board (like Raspberry Pi Pico), discuss real-world use cases, and show how it obliterates some traditional embedded programming pain points.Ready to see Go run WITHIN 256KB of RAM and redefine embedded programming?TinyGo is a Go compiler built on top of LLVM. It enables running Go programs on:Microcontrollers with tight resource constraints (as little as 256KB RAM)WebAssembly for high-performance frontend codeStrong type system of Go, perfect for embedded safetyGoroutines (simplified subset in embedded)LLVM backend for highly optimized binariesSeamless interfaces with sensors, GPIOs, and I2C/SPI devicesTypical C/C++ embedded applications involve messy build chains and arcane platform configurations. TinyGo brings structure and sanity back.
  
  
  Testing the Waters — Getting Started with RP2040 and TinyGo 🔧
brew tap tinygo-org/tools
brew tinygo
Ensure your Go version and TinyGo are installed:go version     
tinygo version 
  
  
  🔧 Step 2: Blink an LED (Hello World for Hardware)
Let’s create a simple blinking LED example for Raspberry Pi Pico RP2040:tinygo flash pico main.go
This will compile your Go code down to a binary that’s burned onto the RP2040 board. Within seconds, your LED is blinking—embedded Go is alive!TinyGo converted our Go code to a small, stripped-down binary using LLVM.Goroutines are optional (limited stack context)
  
  
  Real Use Case: Read a Temperature Sensor 🌡️
Let’s connect a common I2C temperature sensor like the BMP280 and read live values.go get tinygo.org/x/drivers/bmp180
📝 Note: You may need to connect I2C pins to SDA/SCL of BMP280 (check TinyGo’s board pinouts).
  
  
  Why Use Go in Embedded Work?
Working with unsafe C pointers is like walking on a minefield. Go’s type system makes embedded dev safer.
  
  
  ✅ Simpler Concurrency (Even on Embedded!)
TinyGo supports goroutines (with caveats), which means concurrency can be handled more gracefully than typical FreeRTOS tasks in C.Want to ship kind-of-universal logic for both Microcontrollers and WebAssembly pipelines? TinyGo supports both.Not full Go standard library supportNo full-blown goroutines on MCUs (minimal stacks supported)Some peripherals are still a WIP (check GitHub drivers repo)Debugging isn't as easy as Go on desktopBut honestly, the benefits outweigh them for most use cases.
  
  
  Use Cases That Will Blow Your Mind 💥
Collect sensor data, process locally in Go, transmit via LoRAWearables or Fitness DevicesHandle BLE, sensors, step counters—all within Go!Run inference or prepare data on microcontrollers, send to server via WASM equivalent logicBuild retro games in Go for low-cost devices like Gopher2600TinyGo is an absolute hidden gem. It doesn’t just run Go on an embedded device—it redefines what's possible. If you're tired of the toothache-inducing build systems of embedded C, or the lack of type safety in Arduino scripts, TinyGo is your savior.🔥 If you're building next-gen IoT products, learning embedded systems, or want to tinker with microcontrollers without losing your mind—TinyGo is your secret weapon.Go ahead, grab that $2 RP2040 and bring Go into the world of silicon dust!Stay Tuned. Next up? Building a WebAssembly-powered dashboard to control your Go-powered embedded nodes. 👩‍🚀✍️ Author: [Your Name], Fullstack Dev & Embedded Hobbyist🧠 Bonus Tip: Use TinyGo for WebAssembly too—they share runtime code and can even compile the same logic for web and firmware. Mind. Blown.]]></content:encoded></item><item><title>10 Python One-Liners to Optimize Your Machine Learning Pipelines</title><link>https://www.kdnuggets.com/10-python-one-liners-to-optimize-your-machine-learning-pipelines</link><author>Matthew Mayo</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-mayo-10-python-one-liners-ml-pipelines.png" length="" type=""/><pubDate>Thu, 21 Aug 2025 16:00:52 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[This tutorial will focus on ten practical one-liners that leverage the power of libraries like Scikit-learn and Pandas to help streamline your machine learning workflows.]]></content:encoded></item><item><title>🔥 Simulating Course Schedules 600x Faster with Web Workers in CourseCast</title><link>https://dev.to/somedood/simulating-course-schedules-600x-faster-with-web-workers-in-coursecast-41ma</link><author>Basti Ortiz</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 15:43:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This is the story of how I made a Monte Carlo simulation of student schedule assignments  with web workers.Here is our baseline: the original prototype struggled to handle ~100 concurrent users. Each simulation request to the compute server took a whole minute (~60 seconds) to complete, which incidentally exasperated the resource limits of the deployment.In this article, we'll discuss the steps that I took to make the application virtually infinitely scalable (i.e., no server compute bottleneck) thanks to sub-second client-side execution. That's faster than a page load! 🔥
  
  
  Simulating Course Match with CourseCast
On simulation day, the Course Match algorithm determines the  for all offered courses based on their supply and demand. Upon completion, Course Match will have been able to assign schedules to each student (in a single round!) such that the course utilities and obtained credits are maximized given the student's respective budget constraints.💡 You can think of Course Match as an autonomous shopper that "buys" courses on behalf of the student. The purchasing power is only limited by the student's token budget, their maximum workload/credits, and their assigned utilities. The higher the token budget, the greater the student's capability to "afford" the clearing price for a course.Since it's impossible to know ahead of time what the actual clearing prices will be, CourseCast instead forecasts the clearing prices based on the most recent historical data of actual clearing prices in previous Course Match runs. These predicted prices (and their statistical variances) are the "weights" of the model trained on the latest course and instructor trends.To account for forecast uncertainty, the CourseCast model assumes that the predicted clearing price is a normally distributed random variable. As such, CourseCast runs 100 Monte Carlo simulations and counts the frequency of particular courses and schedule configurations being selected. These simulation results are presented to the user as a probability.
  
  
  So where was the bottleneck?
The original CourseCast 2024 was prototyped and deployed as a Streamlit application written in Python. Students would input their course utilities and submit their simulation request to the Streamlit Community Cloud where:The Python back end on shared virtual compute resources would parse course data and load model weights from a hosted Excel spreadsheet.The service would recompute all of the scheduling conflicts between courses (~200 in total). Example: classes with overlapping schedules, classes with overlapping sections, and other logistical constraints.Run 100 Monte Carlo simulations . Each of which is an instance of a linear programming solver.As CourseCast went viral among thousands of UPenn students, the scalability cracks began to show. When too many concurrent users hammered the Streamlit application, students couldn't run their simulations.To be fair, the application was on the Streamlit free tier, but it was definitely high time for a rewrite to something more production-grade.
  
  
  So how did we scale CourseCast 2025?
Now that we know where the bottlenecks are, let's tackle them one by one.
  
  
  Scrapping the Python Server
My first instinct was to ask: is Python necessary at all? The Monte Carlo simulation was essentially a glorified  loop over a linear programming solver. Nothing about the core simulation logic was specific to Python. In fact, the only Python-specific implementation detail was the usage of Excel spreadsheet parser libraries and linear programming solver libraries for Python. I figured...If there was a way to package and compress the Excel spreadsheet in a web-friendly format, then there's nothing stopping us from loading the entire dataset in the browser! Sure enough, the Parquet file format was specifically designed for efficient portability.If there was an equivalent linear programming solver library in JavaScript, then there's nothing stopping us from running simulations in the browser! Sure enough, there was the  library (among many other options).At this point, I was fully convinced that we could scrap the Python server and compute the simulation entirely in the browser. This approach effectively allows us to infinitely scale our simulation capacity as we would no longer be constrained by shared cloud compute limits.That solves our scalability problem! ✅
  
  
  Precomputing Static Course Conflicts
The next bottleneck was the course conflict generation logic. Recall that  simulation request recomputes the logistical constraints on course selections (e.g., disallowing classes with overlapping schedules). This is fairly non-trivial work as there are hundreds of classes to consider.So, naturally, the solution is to precompute these conflicts ahead of time. The precompute script takes the raw course data and appends the "conflict groups" of each course. These "conflict groups" ultimately determine the statically known logistical constraints of the linear programming solver.📝 In computer science parlance, you can think of these "conflict groups" as equivalence classes defined by the relation of overlapping course schedules. That is to say, for all pairs of courses within an equivalence class, their schedules must have a non-empty schedule intersection. Thus, a "conflict group" is just a label for a group of pairwise-intersecting courses.All of the course metadata, seeded random values, and conflict groups are embedded in a single compressed  file (~90 KiB) and served to the user via a CDN for efficient delivery and caching. There is also the option of caching the file in a Service Worker, but the edge CDN already works well enough.That solves our repeated work problem! ✅
  
  
  Offloading CPU-Bound Work to a Separate Thread
The next bottleneck is the sequential execution of Monte Carlo simulation runs. There's actually no reason for us to run them sequentially because each sampled price prediction is independent from the 99 other trials. The simulation can thus be parallelized at the trial level.Since each simulation run is primarily a linear programming solver, we know that the work is CPU-bound, not I/O-bound. The - model will  work here because CPU-bound work blocks the event loop. We  offload the work to another thread to keep the UI responsive.In the browser, we only have one way to spawn multiple threads: through the Web Worker API.We can then wrap the worker message-passing logic in a  interface and leverage libraries like TanStack Query for clean pending states in the UI. The example below uses React for demonstration, but this pattern is framework-agnostic.That solves our responsive UI problem! ✅
  
  
  Parallelizing with Worker Thread Pools
A more advanced implementation of this one-shot request-response worker architecture leverages thread pools to send work to already initialized workers (as opposed to re-initializing them for each work request).We can use navigator.hardwareConcurrency to determine the optimal number of worker threads to spawn in the pool. Spawning more workers than the maximum hardware concurrency is pointless because the hardware would not have enough cores to service that parallelism anyway.⚠️ In the previous section, the  was initialized by the  function. In a worker pool, this should instead be provided as an argument to the  function because  is no longer the "owner" of the thread resource and thus has no say in the worker lifetime. Worker termination  be the responsibility of the thread pool, not the sendWork function.📝 Request cancellation is not implemented here for the sake of brevity, but it is fairly trivial to forward the  from TanStack Query into the thread pool. It's only a matter of terminating the workers upon receiving the  event.The thread pool optimization allowed us to run 100 simulations in parallel batches across all of the device's cores. Together with the precomputed conflict groups, the Monte Carlo simulation was effectively reduced from a minute to sub-second territory! 🔥That solves our performance problems! ✅After all of these optimizations, I upgraded CourseCast from a prototype that struggled with a hundred concurrent users (with ~60 seconds per simulation request) to an infinitely scalable simulator with sub-second execution speeds (faster than a page load!).CourseCast now guides 1000+ UPenn students to make informed decisions and (blazingly!) fast experiments about their course schedules. And we're just getting started! 🚀Throughout this work, I had a few key takeaways:Always leave the door open for the possibility of offloading compute to the browser. Modern Web APIs are highly capable with great browser support nowadays. Keep exploring ways to save ourselves from the infrastructure burden of bespoke Python services.Always find opportunities to precompute static data. May it be through a precompute script like in CourseCast or a materialized view in the database, strive to do the least amount of repeated work.Keep a sharp eye out for parallelizable work. There are many opportunities in data science and general scientific computing where data processing need not be sequential (e.g., dot products, seeded simulation runs, independent events, etc.).On a more human perspective, it's always a pleasure to have the code that I write be in service of others—especially students! As software engineers, it's easy to forget about the human users at the other end of the screen. To be reminded of the positive impact of our code on others never fails to make our work all the more worth it."Have been hearing tons of amazing feedback. Anecdotally, most people who ran simulations through CourseCast ended up without any surprises. Congrats on shipping a great product!"Thanks to Derek Gibbs and the Casper Studios team for trusting me to take the lead on this project! And thanks to the Wharton School administration for their support and collaboration with us in making CourseCast as helpful as it can be for the students.I must disclaim that our dataset is public and fairly small. For larger models with possibly proprietary weights, downloading the data in the browser is not an option. ↩]]></content:encoded></item><item><title>[Boost]</title><link>https://dev.to/ticoraph/-12f1</link><author>TicoRaph</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 14:42:47 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Document Parsing using GPT-4o API vs Claude Sonnet 3.5 API vs Invofox API (with Code Samples)]]></content:encoded></item><item><title>Build a Future-Proof Career with SkillSprintTech’s Generative AI Courses</title><link>https://dev.to/skillsprinttech/build-a-future-proof-career-with-skillsprinttechs-generative-ai-courses-5eg6</link><author>SkillSprint Tech</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 14:16:58 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In the fast-changing tech world of today’s time, staying ahead usually means mastering all the new-age tools that will define tomorrow. One such revolutionary & highly progressing field is generative AI - a fairly new branch of artificial intelligence that has immense potential to enable machines to create relevant text, music, images, & even code. For students, professionals, as well as millions of tech enthusiasts looking to make a mark in this space, SkillSprint Tech, one of the premier career-oriented training institutes offers some of the most practical as well as industry-relevant generative AI courses in India.
  
  
  Why Is Generative AI So Important in Today’s Time?
Generative AI is no longer just a research concept; it is empowering real-world applications big time and that too in a disruptive manner. From ChatGPT creating human-like conversations to AI-driven design tools that are generating stunning visuals, the possibilities in the field of AI are endless. Companies in various sectors like healthcare, finance, retail, entertainment, & software development are continually integrating all of these technologies to considerably improve efficiency, creativity, & customer experience to the next level.By thoroughly learning & understanding generative AI in today’s time, professionals can position themselves for a range of high-demand roles like as AI Engineer, AI Product Manager, Prompt Engineer, or Data Scientist with ultimate AI expertise.
  
  
  The Edge SkillSprintTech Has in the Dynamic Field of Generative AI Training
The generative AI courses offered by SkillSprintTech are strategically designed and is being imparted to participants with a very clear focus: to bridge the gap that exists between theoretical knowledge & practical skills. Instead of just learning the various concepts, students get to build real AI-powered applications. The curriculum very strategically covers everything from a range of foundational AI principles right from the scratch to the most advanced topics like natural language processing (NLP), large language models (LLMs), computer vision, & multimodal AI.A highlight of this particular program is its project-based approach. Learners work on hands-on projects like AI Chatbots, content generation tools, image synthesis, & AI-powered automation systems. This particular approach not just strengthens technical skills but also strongly builds a robust portfolio that impresses recruiters big time.The generative AI courses that is being offered by SkillSprint Tech are perfect for:Existing IT professionals already pursuing their career in the field of IT but are looking to upskill in AI-driven tools.Data analysts & data engineers who are seriously aiming to specialize in the field of AI.Students who are in the lookout to pursue computer science or related fields with the aim to enhance their horizon of knowledge.Entrepreneurs who are willing to integrate AI into their business solutions to improve their efficiency and prospects.No matter whatever is your background, the courses are well-structured to take you from beginner to advanced level, step-by-step.
  
  
  The Best Generative AI Certification for Career Growth
Earning the best generative AI certification from SkillSprint Tech is more than just adding a badge to your resume – it is a proof of your readiness to work in one of the most exciting fields in technology today. This certification is well-recognized by industry experts & signals to employers that you have both the technical know-how as well as the practical experience to deliver results.Additionally, SkillSprint Tech also provides adequate career support through in various additional ways like in the preparation of interview, building resume, & connecting learners with various hiring partners. This particular end-to-end approach likely ensures students not just learn but also land the most rewarding AI-focused roles.Generative AI is rapidly transforming industries, & those who understand how to leverage it will surely lead the way in the present digital economy. By joining SkillSprint Tech’s generative AI courses, learners gain the most contemporary skills, various simulative projects, & certification required to thrive in this new era. For anyone serious about future-proofing their career, the best generative AI certification from SkillSprint Tech is the gateway to countless opportunities.]]></content:encoded></item><item><title>How to Build a Lightweight Data Pipeline with Airtable and Python</title><link>https://www.kdnuggets.com/how-to-build-a-lightweight-data-pipeline-with-airtable-and-python</link><author>Iván Palomares Carrascosa</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-ipc-lightweight-data-pipeline-with-airtable-and-python.png" length="" type=""/><pubDate>Thu, 21 Aug 2025 14:00:50 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[This article shows how to build a simple, ETL-like pipeline using the Airtable Python API, sticking to Airtable free tier.]]></content:encoded></item><item><title>Optimize Your Database with Vertical Partitioning and Caching day 34 of system design</title><link>https://dev.to/vincenttommi/optimize-your-database-with-vertical-partitioning-and-caching-day-35-of-system-design-3dih</link><author>Vincent Tommi</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 13:30:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Databases are the backbone of most applications, but as they grow, performance can take a hit. Imagine a massive User table stuffed with profile details, login history, and billing information. Queries slow down as the database scans irrelevant columns for every request. Sound familiar? Let’s explore vertical partitioning—a powerful technique to streamline your database—and touch on caching for even faster data retrieval.What Is Vertical Partitioning?Vertical partitioning splits a wide table into smaller, focused tables based on usage patterns. Instead of one bloated User table, you create separate tables for specific data groups. This reduces the number of columns scanned during queries, boosting performance and minimizing disk I/O.
For example, suppose your User table stores:Profile details: name, email, profile picture
Login history: last login timestamp, IP addresses
Billing information: billing address, payment detailsAs the table grows, even a simple query like fetching a user’s name forces the database to wade through all columns. Vertical partitioning solves this by splitting the table into:-- User_Profile table
CREATE TABLE User_Profile (
    user_id INT PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    profile_picture VARCHAR(255)
);

-- User_Login table
CREATE TABLE User_Login (
    user_id INT PRIMARY KEY,
    last_login DATETIME,
    ip_address VARCHAR(45)
);

-- User_Billing table
CREATE TABLE User_Billing (
    user_id INT PRIMARY KEY,
    billing_address TEXT,
    payment_details VARCHAR(255)
)
Each table now holds only the columns relevant to specific queries, making data retrieval faster and more efficient.Flowchart: Visualizing Vertical Partitioning
Here's an ASCII art representation of the vertical partitioning process for illustration:+-------------------------+
|      User Table         |
| - name                  |
| - email                 |
| - profile_picture       |
| - last_login            |
| - ip_address            |
| - billing_address       |
| - payment_details       |
+-------------------------+
           | Split (Vertical Partitioning)
    +-------------+   +-------------+   +-------------+
    |User_Profile |   | User_Login  |   |User_Billing |
    | - user_id   |   | - user_id  |   | - user_id  |
    | - name     |   | - last_login|   | - billing_ |
    | - email    |   | - ip_address|   |   address  |
    | - profile_ |   |             |   | - payment_ |
    |   picture  |   |             |   |   details  |
    +-------------+   +-------------+   +-------------+
           |
+-------------------------+
|    Faster Queries       |
| (Reduced Disk I/O)      |
+-------------------------+This visual shows how splitting the table streamlines data access.Taking It Further with CachingVertical partitioning optimizes disk-based queries, but disk access is still slower than memory. Enter caching: storing frequently accessed data (e.g., user profiles) in memory using tools like Redis or Memcached. This delivers lightning-fast access for common queries, complementing the efficiency of partitioned tables.By combining vertical partitioning and caching, you can:Improve query performance: Scan fewer columns and retrieve data faster.Reduce resource usage: Lower disk I/O and server load.Scale efficiently: Handle growing data without sacrificing speed.Ready to optimize your database? Analyze your tables’ usage patterns, identify columns that can be partitioned, and consider caching for frequently accessed data. Experiment with these techniques in a test environment and watch your application’s performance soar!]]></content:encoded></item><item><title>Generative AI in the Real World: Understanding A2A with Heiko Hotz and Sokratis Kartakis</title><link>https://www.oreilly.com/radar/podcast/generative-ai-in-the-real-world-understanding-a2a-with-heiko-hotz-and-sokratis-kartakis/</link><author>Ben Lorica, Heiko Hotz and Sokratis Kartakis</author><category>dev</category><category>ai</category><enclosure url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2024/01/Podcast_Cover_GenAI_in_the_Real_World-scaled.png" length="" type=""/><pubDate>Thu, 21 Aug 2025 13:12:41 +0000</pubDate><source url="https://www.oreilly.com/radar">Oreilly ML</source><content:encoded><![CDATA[Everyone is talking about agents: single agents and, increasingly, multi-agent systems. What kind of applications will we build with agents, and how will we build with them? How will agents communicate with each other effectively? Why do we need a protocol like A2A to specify how they communicate? Join Ben Lorica as he talks with Heiko Hotz and Sokratis Kartakis about A2A and our agentic future.About the Generative AI in the Real World podcast: In 2023, ChatGPT put AI on everyone’s agenda. In 2025, the challenge will be turning those agendas into reality. In Generative AI in the Real World, Ben Lorica interviews leaders who are building with AI. Learn from their experience to help put AI to work in your enterprise.Check out other episodes of this podcast on the O’Reilly learning platform.0:00: Intro to Heiko and Sokratis.0:24: It feels like we’re in a Cambrian explosion of frameworks. Why agent-to-agent communication? Some people might think we should focus on single-agent tooling first.0:53: Many developers start developing agents with completely different frameworks. At some point they want to link the agents together. One way is to change the code of your application. But it would be easier if you could get the agents talking the same language. 1:43: Was A2A something developers approached you for?1:53: It is fair to say that A2A is a forward-looking protocol. We see a future where one team develops an agent that does something and another team in the same organization or even outside would like to leverage that capability. An agent is very different from an API. In the past, this was done via API. With agents, I need a stateful protocol where I send a task and the agent can run asynchronously in the background and do what it needs to do. That’s the justification for the A2A protocol. No one has explicitly asked for this, but we will be there in a few months time. 3:55: For developers in this space, the most familiar is MCP, which is a single agent protocol focused on external tool integration. What is the relationship between MCP and A2A?4:26: We believe that MCP and A2A will be complementary and not rivals. MCP is specific to tools, and A2A connects agents with each other. That brings us to the question of when to wrap a functionality in a tool versus an agent. If we look at the technical implementation, that gives us some hints when to use each. An MCP tool exposes its capability by a structured schema: I need input A and B and I give you the sum. I can’t deviate from the schema. It’s also a single interaction. If I wrap the same functionality into an agent, the way I expose the functionality is different. A2A expects a natural language description of the agent’s functionality: “The agent adds two numbers.” Also, A2A is stateful. I send a request and get a result. That gives developers a hint on when to use an agent and when to use a tool. I like to use the analogy of a vending machine versus a concierge. I put money into a vending machine and push a button and get something out. I talk to a concierge and say, “I’m thirsty; buy me something to drink.”7:09: Maybe we can help our listeners make the notion of A2A even more concrete. I tell nonexperts that you’re already using an agent to some extent. Deep research is an agent. I talk to people building AI tools in finance, and I have a notion that I want to research, but I have one agent looking at earnings, another looking at other data. Do you have a canonical example you use?8:13: We can parallelize A2A with real business. Imagine separate agents that are different employees with different skills. They have their own business cards. They share the business cards with the clients. The client can understand what tasks they want to do: learn about stocks, learn about investments. So I call the right agent or server to get a specialized answer back. Each agent has a business card that describes its skills and capabilities. I can talk to the agent with live streaming or send it messages. You need to define how you communicate with the agent. And you need to define the security method you will use to exchange messages.9:45: Late last year, people started talking about single agents. But people were already talking about what the agent stack would be: memory, storage, observability, and so on. Now that you are talking about multi-agents or A2A, are there important things that need to be introduced to the agentic stack?10:32: You would still have the same. You’d arguably need more. Statefulness, memory, access to tools.10:48: Is that going to be like a shared memory across agents?10:52: It all depends on the architecture. The way I imagine a vanilla architecture, the user speaks to a router agent, which is the primary contact of the user with the system. That router agent does very simple things like saying “hello.” But once the user asks the system “Book me a holiday to Paris,” there are many steps involved. (No agent can do this yet). The capabilities are getting better and better. But the way I imagine it is that the router agent is the boss, and two or three remote agents do different things. One finds flights; one books hotels; one books cars—they all need information from each other. The router agent would hold the context for all of those. If you build it all within one agentic framework, it becomes even easier because those frameworks have the concepts of shared memory built in. But it’s not necessarily needed. If the hotel booking agent is built in LangChain and from a different team than the flight booking agent, the router agent would decide what information is needed.13:28: What you just said is the argument for why you need these protocols. Your example is the canonical simple example. What if my trip involves four different countries? I might need a hotel agent for every country. Because hotels might need to be specialized for local knowledge.14:12: Technically, you might not need to change agents. You need to change the data—what agent has access to what data. 14:29: We need to parallelize single agents with multi-agent systems; we move from a monolithic application to microservices that have small, dedicated agents to perform specific tasks. This has many benefits. It also makes the life of the developer easier because you can test, you can evaluate, you can perform checks before moving to production. Imagine that you gave a human 100 tools to perform a task. The human will get confused. It’s the same for agents. You need small agents with specific terms to perform the right task. 15:31: Heiko’s example drives home why something like MCP may not be enough. If you have a master agent and all it does is integrate with external sites, but the integration is not smart—if the other side has an agent, that agent could be thinking as well. While agent-to-agent is something of a science fiction at the moment, it does make sense moving forward.16:11: Coming back to Sokratis’s thought, when you give an agent too many tools and make it try to do too many things, it just becomes more and more likely that by reasoning through these tools, it will pick the wrong tool. That gets us to evaluation and fault tolerance. 16:52: At some point we might see multi-agent systems communicate with other multi-agent systems—an agent mesh.17:05: In the scenario of this hotel booking, each of the smaller agents would use their own local model. They wouldn’t all rely on a central model. Almost all frameworks allow you to choose the right model for the right task. If a task is simple but still requires an LLM, a small open source model could be sufficient. If the task requires heavy “brain” power, you might want to use Gemini 2.5 Pro.18:07: Sokratis brought up the word security. One of the earlier attacks against MCP is a scenario when an attacker buries instructions in the system prompt of the MCP server or its metadata, which then gets sent into the model. In this case, you have smaller agents, but something may happen to the smaller agents. What attack scenarios worry you at this point?19:02: There are many levels at which something might go wrong. With a single agent, you have to implement guardrails before and after each call to an LLM or agent.19:24: In a single agent, there is one model. Now each agent is using its own model. 19:35: And this makes the evaluation and security guardrails even more problematic. From A2A’s side, it supports all the different security types to authenticate agents, like API keys, HTTP authentication, OAuth 2. Within the agent card, the agent can define what you need to use to use the agent. Then you need to think of this as a service possibility. It’s not just a responsibility of the protocol. It’s the responsibility of the developer.20:29: It’s equivalent to right now with MCP. There are thousands of MCP servers. How do I know which to trust? But at the same time, there are thousands of Python packages. I have to figure out which to trust. At some level, some vetting needs to be done before you trust another agent. Is that right?21:00: I would think so. There’s a great article: “The S in MCP Stands for Security.” We can’t speak as much to the MCP protocol, but I do believe there have been efforts to implement authentication methods and address security concerns, because this is the number one question enterprises will ask. Without proper authentication and security, you will not have adoption in enterprises, which means you will not have adoption at all. WIth A2A, these concerns were addressed head-on because the A2A team understood that to get any chance of traction, built in security was priority 0. 22:25: Are you familiar with the buzzword “large action models”? The notion that your model is now multimodal and can look at screens and environment states.22:51: Within DeepMind, we have Project Mariner, which leverages Gemini’s capabilities to ask on your behalf about your computer screen.23:06: It makes sense that it’s something you want to avoid if you can. If you can do things in a headless way, why do you want to pretend you’re human? If there’s an API or integration, you would go for that. But the reality is that many tools knowledge workers use may not have these features yet. How does that impact how we build agent security? Now that people might start building agents to act like knowledge workers using screens?23:45: I spoke with a bank in the UK yesterday, and they were very clear that they need to have complete observability on agents, even if that means slowing down the process. Because of regulation, they need to be able to explain every request that went to the LLM, and every action that followed from that. I believe observability is the key in this setup, where you just cannot tolerate any errors. Because it is LLM-based, there will still be errors. But in a bank you must at least be in a position to explain exactly what happened.24:45: With most customers, whenever there’s an agentic solution, they need to share that they are using an agentic solution and the way [they] are using it is X, Y, and Z. A legal agreement is required to use the agent. The customer needs to be clear about this. There are other scenarios like UI testing where, as a developer, I want an agent to start using my machine. Or an elder who is connected with customer support of a telco to fix a router. This is impossible for a nontechnical person to achieve. The fear is there, like nuclear energy, which can be used in two different ways. It’s the same with agents and GenAI. 26:08: A2A is a protocol. As a protocol, there’s only so much you can do on the security front. At some level, that’s the responsibility of the developers. I may want to signal that my agent is secure because I’ve hired a third party to do penetration testing. Is there a way for the protocol to embed knowledge about the extra step?27:00: A protocol can’t handle all the different cases. That’s why A2A created the notion of extensions. You can extend the data structure and also the methods or the profile. Within this profile, you can say, “I want all the agents to use this encryption.” And with that, you can tell all your systems to use the same patterns. You create the extension once, you adopt that for all the A2A compatible agents, and it’s ready. 27:51: For our listeners who haven’t opened the protocol, how easy is it? Is it like REST or RPC?28:05: I personally learned it within half a day. For someone who is familiar with RPC, with traditional internet protocols, A2A is very intuitive. You have a server; you have a client. All you need to learn is some specific concepts, like the agent card. (The agent card itself could be used to signal not only my capabilities but how I have been tested. You can even think of other metrics like uptime and success rate.) You need to understand the concept of a task. And then the remote agent will update on this task as defined—for example, every five minutes or [upon] completion of specific subtasks.29:52: A2A already supports JavaScript, TypeScript, Python, Java, and .NET. In ADK, the agent development kit, with one line of code we can define a new A2A agent.30:27: What is the current state of adoption?30:40: I should have looked at the PyPI download numbers.30:49: Are you aware of teams or companies starting to use A2A?30:55: I’ve worked with a customer with an insurance platform. I don’t know anything about insurance, but there’s the broker and the underwriter, which are usually two different companies. They were thinking about building an agent for each and having the agents talk via A2A31:32: Sokratis, what about you?31:40: The interest is there for sure. Three weeks ago, I presented [at] the Google Cloud London Summit with a big customer on the integration of A2A into their agentic platform, and we shared tens of customers, including the announcement from Microsoft. Many customers start implementing agents. At some point they lack integration across business units. Now they see the more agents they build, the more the need for A2A.32:32: A2A is now in the Linux Foundation, which makes it more attractive for companies to explore, adopt, and contribute to, because it’s no longer controlled by a single entity. So decision making will be shared across multiple entities.]]></content:encoded></item><item><title>Pack, the new-gen workflow manager</title><link>https://dev.to/robert19066/pack-the-new-gen-workflow-manager-55bo</link><author>robert19066</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 13:02:50 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Project Pack V1, or simply Pack, is a Python-based "packlet" (workflow) manager that allows users to create, store, and execute custom shell command sequences with various execution modes and privilege configurations. The project provides both a CLI interface for creating packlets and an execution engine for running them.Dev note: Trust me, it's really awsome!The codebase is organized around four main components:: The user-facing CLI application with a rich terminal interface including colored banners, menus, loading bars, and wizards for packlet creation and execution: The core execution engine ( class) that parses packlet files and executes shell commands with different execution strategies: Parser utilities ( class) for extracting configuration from packlet files: File creation utility for generating new packlet files with proper formattingPacklets are custom configuration files with specific extensions and structure:: Standard packlets (default execution mode - stops on errors): Bulldozer packlets (continues execution despite errors)$type=<shell>          # Shell to use (bash, zsh, fish, etc.)
$excmeth:<method>      # Execution method (default/bulldozer)
$isudo=<true/false>    # Whether sudo privileges are required

<command1>
<command2>
...
---
: Stops execution immediately when any command fails: Continues executing all commands even when some fail, providing a summary of failed commands at the end
  
  
  Testing Packlet Execution
 test.paklt


python 
  
  
  Creating Packlets Programmatically
python /
├── mainShell.py          # Main CLI application with UI
├── mainCompile.py        # Core execution engine
├── helper_functions.py   # Packlet file parsers
├── createFile.py         # File creation utilities
├── packlets/             # Directory for storing packlets (created automatically)
└── __pycache__/          # Python bytecode cache
The codebase implements two distinct error handling strategies:: Uses  with  to raise exceptions on command failure: Uses  with  and manually tracks failed commandsThe CLI uses ANSI color codes extensively through the  class for terminal styling. Key UI components include:Dynamic menu boxes with perfect alignmentProgress indicators (loading bars and spinners)Step-by-step wizards for packlet creationColored success/error/warning messages: Used in helper_functions.py (imported but not actively used in current implementation): Core dependency for shell command execution: For file system operations and screen clearing: For application exit handling: For UI animations and delays: For randomized loading animationsAll created packlets are stored in the  directory, which is automatically created if it doesn't exist. The directory structure is flat with no subdirectories.Sudo execution is configurable per packlet via the  parameterCommands are executed through shell subprocess calls, so standard shell injection precautions applyBulldozer mode can potentially mask security-relevant command failures]]></content:encoded></item><item><title>Show HN: ChartDB Cloud – Visualize and Share Database Diagrams</title><link>https://app.chartdb.io/</link><author>Jonathanfishner</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 13:01:11 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Using Common Lisp from Inside the Browser</title><link>https://turtleware.eu/posts/Using-Common-Lisp-from-inside-the-Browser.html</link><author>jackdaniel</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 12:08:30 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[ Written on 2025-08-21 by Daniel Kochmański Web Embeddable Common Lisp is a project that brings Common Lisp and the Web
Browser environments together. In this post I'll outline the current progress of
the project and provide some technical details, including current caveats and
future plans.It is important to note that this is not a release and none of the described
APIs and functionalities is considered to be stable. Things are still changing
and I'm not accepting bug reports for the time being.The easiest way to use Common Lisp on a website is to include WECL and insert
script tags with a type "text/common-lisp". When the attribute src is present,
then first the runtime loads the script from that url, and then it executes the
node body. For example create and run this HTML document from localhost:<!doctype html>
<html>
  <head>
    <title>Web Embeddable Common Lisp</title>
    <link rel="stylesheet" href="https://turtleware.eu/static/misc/wecl-20250821/easy.css" />
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/boot.js"></script>
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/wecl.js"></script>
  </head>
  <body>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/easy.lisp" id='easy-script'>
(defvar *div* (make-element "div" :id "my-ticker"))
(append-child [body] *div*)

(dotimes (v 4)
  (push-counter v))

(loop for tic from 6 above 0
      do (replace-children *div* (make-paragraph "~a" tic))
         (js-sleep 1000)
      finally (replace-children *div* (make-paragraph "BOOM!")))

(show-script-text "easy-script")
    </script>
  </body>
</html>
We may use Common Lisp that can call to JavaScript, and register callbacks to be
called on specified events. The source code of the script can be found here:Because the runtime is included as a script, the browser will usually cache the
~10MB WebAssembly module.The initial foreign function interface has numerous macros defining wrappers
that may be used from Common Lisp or passed to JavaScript.Summary of currently available operators: an inlined expression, like  an object referenced from the object store a function a method of the argument, like  a slot reader of the argument a slot writer of the first argument combines define-js-getter and define-js-setter template for JavaScript expressions Common Lisp function reference callable from JavaScript anonymous Common Lisp function reference (for closures)Summary of argument types:Common Lisp object referenceJavaScript object referenceAll operators, except for  have a similar lambda list:(DEFINE-JS NAME-AND-OPTIONS [ARGUMENTS [,@BODY]])The first argument is a list  that is common to all
defining operators: Common Lisp symbol denoting the object a string denoting the JavaScript expression, i.e "innerText" a type of the object returned by executing the expression(define-js-variable ([document] :js-expr "document" :type :symbol))
;; document
(define-js-object ([body] :js-expr "document.body" :type :js-ref))
;; wecl_ensure_object(document.body) /* -> id   */
;; wecl_search_object(id)            /* -> node */
The difference between a variable and an object in JS-FFI is that variable
expression is executed each time when the object is used (the expression is
inlined), while the object expression is executed only once and the result is
stored in the object store.The second argument is a list of pairs . Names will be used in the
lambda list of the operator callable from Common Lisp, while types will be used
to coerce arguments to the type expected by JavaScript.(define-js-function (parse-float :js-expr "parseFloat" :type :js-ref)
    ((value :string)))
;; parseFloat(value)

(define-js-method (add-event-listener :js-expr "addEventListener" :type :null)
    ((self :js-ref)
     (name :string)
     (fun :js-ref)))
;; self.addEventListener(name, fun)

(define-js-getter (get-inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)))
;; self.innerText

(define-js-setter (set-inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)
     (new :string)))
;; self.innerText = new

(define-js-accessor (inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)
     (new :string)))
;; self.innerText
;; self.innerText = new

(define-js-script (document :js-expr "~a.forEach(~a)" :type :js-ref)
    ((nodes :js-ref)
     (callb :object)))
;; nodes.forEach(callb)
The third argument is specific to callbacks, where we define Common Lisp body of
the callback. Argument types are used to coerce values from JavaScript to Common
Lisp.(define-js-callback (print-node :type :object)
    ((elt :js-ref)
     (nth :fixnum)
     (seq :js-ref))
  (format t "Node ~2d: ~a~%" nth elt))

(let ((start 0))
  (add-event-listener *my-elt* "click"
                      (lambda-js-callback :null ((event :js-ref)) ;closure!
                        (incf start)
                        (setf (inner-text *my-elt*)
                              (format nil "Hello World! ~a" start)))
Note that callbacks are a bit different, because  does not
accept  option and  has unique lambda list. It is
important for callbacks to have an exact arity as they are called with, because
JS-FFI does not implement variable number of arguments yet.Callbacks can be referred by name with an operator .While working on FFI I've decided to write an adapter for SLIME/SWANK that will
allow interacting with WECL from Emacs. The principle is simple: we connect with
a websocket to Emacs that is listening on the specified port (i.e on localhost).
This adapter uses the library  written by Andrew Hyatt.It allows for compiling individual forms with , but file compilation
does not work (because files reside on a different "host"). REPL interaction
works as expected, as well as SLDB. The connection may occasionally be unstable,
and until Common Lisp call returns, the whole page is blocked. Notably waiting
for new requests is not a blocking operation from the JavaScript perspective,
because it is an asynchronous operation.;;; Patches for SLIME 2.31 (to be removed after the patch is merged).
;;; It is assumed that SLIME is already loaded into Emacs.
(defun slime-net-send (sexp proc)
  "Send a SEXP to Lisp over the socket PROC.
This is the lowest level of communication. The sexp will be READ and
EVAL'd by Lisp."
  (let* ((payload (encode-coding-string
                   (concat (slime-prin1-to-string sexp) "\n")
                   'utf-8-unix))
         (string (concat (slime-net-encode-length (length payload))
                         payload))
         (websocket (process-get proc :websocket)))
    (slime-log-event sexp)
    (if websocket
        (websocket-send-text websocket string)
      (process-send-string proc string))))

(defun slime-use-sigint-for-interrupt (&optional connection)
  (let ((c (or connection (slime-connection))))
    (cl-ecase (slime-communication-style c)
      ((:fd-handler nil) t)
      ((:spawn :sigio :async) nil))))
;;; lime.el --- Lisp Interaction Mode for Emacs -*-lexical-binding:t-*-
;;; 
;;; This program extends SLIME with an ability to listen for lisp connections.
;;; The flow is reversed - normally SLIME is a client and SWANK is a server.

(require 'websocket)

(defvar *lime-server* nil
  "The LIME server.")

(cl-defun lime-zipit (obj &optional (start 0) (end 72))
  (let* ((msg (if (stringp obj)
                  obj
                (slime-prin1-to-string obj)))
         (len (length msg)))
    (substring msg (min start len) (min end len))))

(cl-defun lime-message (&rest args)
  (with-current-buffer (process-buffer *lime-server*)
    (goto-char (point-max))
    (dolist (arg args)
      (insert (lime-zipit arg)))
    (insert "\n")
    (goto-char (point-max))))

(cl-defun lime-client-process (client)
  (websocket-conn client))

(cl-defun lime-process-client (process)
  (process-get process :websocket))

;;; c.f slime-net-connect
(cl-defun lime-add-client (client)
  (lime-message "LIME connecting a new client")
  (let* ((process (websocket-conn client))
         (buffer (generate-new-buffer "*lime-connection*")))
    (set-process-buffer process buffer)
    (push process slime-net-processes)
    (slime-setup-connection process)
    client))

;;; When SLIME kills the process, then it invokes LIME-DISCONNECT hook.
;;; When SWANK kills the process, then it invokes LIME-DEL-CLIENT hook.
(cl-defun lime-del-client (client)
  (when-let ((process (lime-client-process client)))
    (lime-message "LIME client disconnected")
    (slime-net-sentinel process "closed by peer")))

(cl-defun lime-disconnect (process)
  (when-let ((client (lime-process-client process)))
    (lime-message "LIME disconnecting client")
    (websocket-close client)))

(cl-defun lime-on-error (client fun error)
  (ignore client fun)
  (lime-message "LIME error: " (slime-prin1-to-string error)))

;;; Client sends the result over a websocket. Handling responses is implemented
;;; by SLIME-NET-FILTER. As we can see, the flow is reversed in our case.
(cl-defun lime-handle-message (client frame)
  (let ((process (lime-client-process client))
        (data (websocket-frame-text frame)))
    (lime-message "LIME-RECV: " data)
    (slime-net-filter process data)))

(cl-defun lime-net-listen (host port &rest parameters)
  (when *lime-server*
    (error "LIME server has already started"))
  (setq *lime-server*
        (apply 'websocket-server port
               :host host
               :on-open    (function lime-add-client)
               :on-close   (function lime-del-client)
               :on-error   (function lime-on-error)
               :on-message (function lime-handle-message)
               parameters))
  (unless (memq 'lime-disconnect slime-net-process-close-hooks)
    (push 'lime-disconnect slime-net-process-close-hooks))
  (let ((buf (get-buffer-create "*lime-server*")))
    (set-process-buffer *lime-server* buf)
    (lime-message "Welcome " *lime-server* "!")
    t))

(cl-defun lime-stop ()
  (when *lime-server*
   (websocket-server-close *lime-server*)
   (setq *lime-server* nil)))
After loading this file into Emacs invoke (lime-net-listen "localhost" 8889).
Now our Emacs listens for new connections from SLUG (the lisp-side part adapting
SWANK, already bundled with WECL). There are two SLUG backends in a repository: for web browser environment for Common Lisp runtime (uses )Now you can open a page listed here and connect to SLIME:<!doctype html>
<html>
  <head>
    <title>Web Embeddable Common Lisp</title>
    <link rel="stylesheet" href="easy.css" />
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/boot.js"></script>
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/wecl.js"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/slug.lisp"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/wank.lisp"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/easy.lisp">
      (defvar *connect-button* (make-element "button" :text "Connect"))
      (define-js-callback (connect-to-slug :type :null) ((event :js-ref))
        (wank-connect "localhost" 8889)
        (setf (inner-text *connect-button*) "Crash!"))
      (add-event-listener *connect-button* "click" (js-callback connect-to-slug))
      (append-child [body] *connect-button*)
    </script>
  </head>
  <body>
  </body>
</html>
This example shows an important limitation –  does not allow for
multiple asynchronous contexts in the same thread. That means that if Lisp call
doesn't return (i.e because it waits for input in a loop), then we can't execute
other Common Lisp statements from elsewhere because the application will crash.Here's another example. It is more a cool gimmick than anything else, but let's
try it. Open a console on this very website (on firefox C-S-i) and execute:function inject_js(url) {
    var head = document.getElementsByTagName('head')[0];
    var script = document.createElement('script');
    head.appendChild(script);
    script.type = 'text/javascript';
    return new Promise((resolve) => {
        script.onload = resolve;
        script.src = url;
    });
}

function inject_cl() {
    wecl_eval('(wecl/impl::js-load-slug "https://turtleware.eu/static/misc/wecl-20250821")');
}

inject_js('https://turtleware.eu/static/misc/wecl-20250821/boot.js')
    .then(() => {
        wecl_init_hooks.push(inject_cl);
        inject_js('https://turtleware.eu/static/misc/wecl-20250821/wecl.js');
    });
With this, assuming that you've kept your LIME server open, you'll have a REPL
onto uncooperative website. Now we can fool around with queries and changes:(define-js-accessor (title :js-expr "title" :type :string)
  ((self :js-ref)
   (title :string)))

(define-js-accessor (background :js-expr "body.style.backgroundColor" :type :string)
  ((self :js-ref)
   (background :string)))

(setf (title [document]) "Write in Lisp!")
(setf (background [document]) "#aaffaa")
The first thing to address is the lack of threading primitives. Native threads
can be implemented with web workers, but then our GC wouldn't know how to stop
the world to clean up. Another option is to use cooperative threads, but that
also won't work, because Emscripten doesn't support independent asynchronous
contexts, nor ECL is ready for that yet.I plan to address both issues simultaneously in the second stage of the project
when I port the runtime to WASI. We'll be able to use browser's GC, so running
in multiple web workers should not be a problem anymore. Unwinding and rewinding
the stack will require tinkering with ASYNCIFY and I have somewhat working green
threads implementation in place, so I will finish it and upstream in ECL.Currently I'm focusing mostly on having things working, so JS and CL interop is
brittle and often relies on evaluating expressions, trampolining and coercing.
That impacts the performance in a significant way. Moreover all loaded scripts
are compiled with a one-pass compiler, so the result bytecode is not optimized.There is no support for loading cross-compiled files onto the runtime, not to
mention that it is not possible to precompile systems with ASDF definitions.JS-FFI requires more work to allow for defining functions with variable number
of arguments and with optional arguments. There is no dynamic coercion of
JavaScript exceptions to Common Lisp conditions, but it is planned.]]></content:encoded></item><item><title>Document Parsing using GPT-4o API vs Claude Sonnet 3.5 API vs Invofox API (with Code Samples)</title><link>https://dev.to/anmolbaranwal/document-parsing-using-gpt-4o-api-vs-claude-sonnet-35-api-vs-invofox-api-with-code-samples-56h2</link><author>Anmol Baranwal</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 12:04:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Extracting structured data from unstructured documents (like PDFs and images) can get tricky fast.With the rise of foundation models and purpose-built APIs, it's now possible to turn even a messy invoice into clean JSON with just a few lines of code.So I will compare three different ways to parse documents: using OpenAI’s GPT‑4o, Anthropic’s Claude 3.5 Sonnet and the Invofox API.I picked Invofox because it's a YC-backed startup built specifically for document parsing. It uses specialized models (proprietary and best-of-LLM) tuned for invoices and other documents, while GPT/Claude are general-purpose LLMs.You will see real Python code, actual outputs and a breakdown of when to use each tool (pros & cons). At the end, there is a detailed comparison table on features & benchmarks.
  
  
  🎯 Using GPT-4o (ChatGPT) API
Let’s start with OpenAI’s GPT-4o. It's capable of understanding text and extracting structured information when prompted correctly. But unlike Invofox, it can’t directly read PDF files.So we first need to extract the text using OCR (like Tesseract, pdfplumber or an online tool), then send that text to GPT via an API prompt.GPT-4o, especially via the ChatGPT web interface and certain API endpoints (notably in Azure OpenAI Service), can accept PDFs and images as inputs and extract structured data. But since we are using the API, it's not really possible.You will need an OpenAI API key. Create a  file and attach it with this convention.your_api_key

openai api key



We will use Python for this. Here's how you can try it yourself, step by step.
  
  
  Step 1: Set up your Python environment
Creating a virtual environment means setting up an isolated space for your Python project where all dependencies are installed locally (and not system-wide). This avoids version conflicts and keeps your global Python installation clean. So let’s create one.
python3  venv /bin/activate  
python  venv 
.nvcriptsctivate  You will know it’s active when you see  at the beginning of your terminal prompt.
  
  
  Step 2: Install required packages
We need two main libraries:: to use the GPT-4o API : Loads environment variables from a  file into Python, useful for managing API keys and secrets.
pip pdfplumber openai python-dotenv
I installed the  later so that's why it's not visible in the command.After installing your dependencies, run:pip freeze  requirements.txt
This writes all installed packages in your virtual environment (with versions) into . You can then use this file later with:pip  requirements.txt
For reference, please add a  in the root directory to avoid pushing the virtual environment directory.
  
  
  Step 3: Extract text and parse with GPT-4o
Here is the sample Invoice PDF that I'm using for the example. I'm attaching a snapshot so you can get the idea of the fields we are going to extract.Let's write the complete code with the file name as .Here's a simple explanation: : uses  reads each page of the PDF and concatenates the extracted text. This gives you the raw, unstructured invoice content as a string.parse_invoice_with_openai : Sends this prompt to the GPT‑4o model via the  endpoint, asking GPT‑4o to extract five key fields. The model then processes the prompt and returns a JSON-formatted response.Here is the JSON response after running the script using .GPT-4o (ChatGPT) output for invoice line items isn’t consistently labeled "lines". Sometimes it's "Line Items" or something less standardized, while other tools (like Invofox) always use a consistent name like "lines" for those entries.
terminal output



Here we instruct GPT-4o via a system prompt to parse the text. This can work reasonably well as the API is strong enough now (compared to previous OpenAI models).✅ Pros: Easy to try, flexible. GPT-4 excels at logic and structured data extraction, so it can correctly identify invoice fields and calculate totals.The problem I see is that we still have to engineer prompts and verify the output (which is not possible for everyone).The JSON can be malformed or may miss fields (hallucinations are possible). There’s no built‑in validation or confidence scores. GPT requires sending all text in prompts (which would be costly for large docs) and outputs vary by prompt style.GPT-4o is billed per token. The estimated cost for a 1–2 page invoice extraction falls in the $0.005–$0.018 range, depending on how detailed your prompt and output are. You can also use this pricing calculator based on your use case.It can respond in 1–30s but is subject to load spikes, especially for large prompts.
  
  
  🎯 Using Claude 3.5 Sonnet API
Anthropic's Claude 3.5 Sonnet model is also capable of parsing structured data from text when prompted correctly. Like GPT-4o, it cannot read PDF files directly via API, so we will first extract the text from an invoice PDF, then pass it to Claude for structured parsing.You will need an Anthropic API key. Create a  file and attach it with this convention:ANTHROPIC_API_KEY=your_api_key
We will use Python again for this setup and follow the same instructions used in the last section.
  
  
  Step 1: Set up environment and install packages
Just like before, let’s isolate our dependencies in a virtual environment.
python3  venv /bin/activate


python  venv 
.nvcriptsctivate
Once activated, your terminal will show a  prefix.We need the following libraries: : to extract text from PDF : official SDK to interact with Claude 3.5 : to load the API key from a  file
pip pdfplumber anthropic python-dotenv
If you are following from the last example, we just need to install the anthropic package.Then export your environment to a  file. Make sure to include a  to avoid committing the virtual environment.pip freeze  requirements.txt

  
  
  Step 2: Extract text and parse with Claude 3.5 Sonnet
As Anthropic launches safer and more capable models, they regularly retire older models. So you can check the model status of which ones are deprecated, retired and which ones are active. I will be using claude-3-5-sonnet-20240620 active version for the example.Let's write the complete code with the file name as . It's very similar to the previous section and I'm using the same sample Invoice PDF.Here's a simple explanation: : uses  to pull plain text from each page of the PDF.parse_invoice_with_claude : sends the text to Claude Sonnet 3.5 with a specific prompt asking for JSON output.Claude returns a stringified JSON block with the requested fields.You can run the script using  in the terminal. Here's the JSON response:Claude 3.5 is very strong at understanding long text and formatting it cleanly.Claude Sonnet can handle text (and even images via embedding) in its promptsIn some cases, it handles unusual or long documents slightly better than GPT-4.Like GPT, Claude requires prompt engineering.Like GPT, Claude can sometimes miss fields or make up values (hallucinate).It still returns raw JSON text without validation, so you must parse/verify it.You still need to extract text yourself, it doesn’t parse raw PDFs.Claude 3.5 Sonnet is also billed per token. The estimated cost for a 1–2 page invoice extraction falls in the $0.005–$0.018 range, depending on how detailed your prompt and output are. You can also use this pricing calculator based on your use case.It's exceptionally fast for small prompts (200–300ms) but larger or more complex stimuli can raise latency to 10s or more. So I was searching for a better solution unlike code-based (OpenAI & Anthropic) approaches requiring prompt engineering, I found many good tools like Invofox, Google Document AI, Amazon Textract. What stood out about Invofox is that it’s backed by Y Combinator and has all the features I needed. That gave me the confidence to dig deeper and try it out.It provides a plug‑and‑play AI-powered document parsing API that makes it super easy to extract data from invoices, receipts, payslips, bank statements, loan/mortgage files and custom document types like bills.They have some useful built-in features like:It automatically separates multiple documents contained within a single PDF (such as mixed invoices or statements), grouping pages into logical sub-documents for better extraction and automation.  It's configurable via API during upload and works alongside the classifier for cleaner downstream processingPretrained AI model that detects document types (invoice, receipt, etc) so that each document is processed using the correct schema. It's optional and can be enabled per environment or request.They also use advanced AI models with proprietary algorithms that verify and autocomplete your data. Check API Docs.
  
  
  Step 1: Sign up for the dashboard
You can sign up for the dashboard to generate an API key.You can manually upload the document as well but we will be using the API since it's easier and much better in experience.
  
  
  Step 2: Creating the request in Postman
Once you have your API key, you can use Postman to send documents for parsing using Invofox's  endpoint.✅ 1. Create a New RequestOpen the Postman Desktop applicationCreate a collection and add a requestWe need to request this endpoint: https://api.invofox.com/v1/ingest/uploadsGo to the Headers tab and add:key: , value: key: , value: You should not manually set  as Postman will handle it automatically when using form-data. It tells the server what format the data in your request body is: → You're sending raw JSON → You are sending files + form fieldsapplication/x-www-form-urlencoded → You're sending form-like text fields (like an HTML form)When you're sending files using Postman’s form-data option, Postman automatically sets the correct  and boundary values (which are required for ).If you manually set it like this:Content-Type: multipart/form-data
You are missing the boundary part, which is something like:Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryxyz
Let's add the body fields.✅ 3. Add the Body (form-data)Switch to the Body tab, select  and add the following two fields:key: , type: , value: upload your invoice ()key: , type: , value: Paste the JSON below
The  field is optional here as it's only needed if you want to pass custom metadata or extra instructions (such as information to influence parsing, verification preferences or to register edge-case scenarios for custom document types).Beyond standard types (invoice, payslip, bank statement), you can register custom document types in your Invofox dashboard. These custom types get a unique ID like  (used in the example), which is what we are now passing to the API. By specifying a type ID, you ensure your files are parsed according to the exact schema you set up:Your custom JSON structureCustom validation rules and human review workflowsClick "Send". If everything is set up correctly, you will get a response with details on documentID. is the batch ID for this upload (useful for tracking multiple files uploaded together) is the ID of the parsed document

  
  
  Step 3: Get Parsed Document
There are two ways: one is to check the Invofox dashboard to find the newly parsed document. As you can notice, the line items and breakdowns are displayed in a table format. The GUI also provides many options, including filtering the extracted data.Based on how the workflow is set up, it may be necessary to mark it as completed, as involving a human in the loop ensures the highest accuracy and gives us more control.The other way (recommended) is to make a  request to https://api.invofox.com/documents/{documentID} with headers as:key: , value: key: , value: Here is a trimmed JSON response with the original format. It also provides the image of the original invoice in the response and a lot of extra fields compared to the earlier responses of GPT-4o & Claude.Pricing is not public, so potential users must contact their team for a commercial offer, but the product is specifically tuned for production speed and reliability. Actual test response times reported in the blog are consistently under 5s.
  
  
  🎯 Python Code using Invofox API
Many developers prefer extracting documents with code, so let’s walk through the same process using the Invofox API with Python. We will keep it brief, with just the code and JSON response.The overall process is the same as the previous sections, so I'm not repeating that. You can read the docs if you are interested in exploring for yourself.We need to install requests, a Python library that makes it easy to send HTTP requests (such as GET, POST) and work with web APIs.We will also use the  built-in Python module that comes pre-installed with every standard Python installation. The  module provides various time-related functions such as delays (), timestamps and more. In our case, we will use it to pause execution, giving the document enough time to be processed on the dashboard.Let's write the complete code with the file name as .Here are all the Invofox API endpoints used:Here is the JSON response after running the script using .The JSON response is similar to what we got after making a request using Postman. It also provides the image of the original invoice in the response and a lot of useful fields.Let's compare their methods in brief.GPT-4o/Claude → send text with promptInvofox → use API or upload a file (image/PDF) in bulkGPT/Claude → need to write prompt engineering codeInvofox → minimal code, no promptGPT/Claude → you need to manually verify Invofox → built-in validation and confidence scoresGPT/Claude → limited by token/window sizeInvofox → handles multi-page docs via backend OCR and AIWhile parsing the invoice, here's what I realized: : Good at parsing known fields if prompted clearly. You get a JSON string but must parse/clean it yourself. Errors can occur if prompts are unclear. : It's very similar to GPT-4. In the snapshots, you can see Sonnet handled the invoice fields about as well as GPT-4, sometimes better at recognizing unfamiliar terms. But we still had to massage the prompt. : It returned the fully parsed invoice JSON out-of-the-box. All fields were correctly extracted and validated. The output schema was exactly what we needed, with no extra coding.Now that we have explored each option, let’s compare them side by side. Estimates are based on typical invoice lengths: simple invoices are 1–2 pages & 1000-2000 tokens total.
  
  
  Cost & Execution Time Benchmarks
We covered the pricing structure in each of the sections, but I have also done it side-by-side so it's easier to make a decision.You should also acknowledge the ongoing cost and effort involved in upgrading language models. Teams often need to benchmark new models, retest prompts and schemas and adjust output parsing logic whenever a new version is released.These hidden maintenance costs aren’t always obvious but should be considered. With Invofox, there is no such requirement.For quick experiments or one-off tasks, you can use GPT-4 (ChatGPT API) or Claude Sonnet to parse invoice text by crafting suitable prompts. They will do a decent job extracting fields in JSON (since GPT-4 tends to produce more structured and cleaner outputs than earlier GPT-3). However, for reliable production-grade parsing of invoices or receipts, the Invofox API is superior. It’s specifically built for documents using advanced proprietary models and continual feedback.I hope you learned how to parse documents. Let me know if you have any questions or feedback.Have a great day! Until next time :)]]></content:encoded></item><item><title>I Created the most Comprehensive LLD Interview Resource</title><link>https://blog.algomaster.io/p/launching-premium-lld-resource</link><author>Ashish Pratap Singh</author><category>dev</category><category>learning</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/1bdf8340-7a0f-42e0-8609-c5174fb17828_2048x1426.jpeg" length="" type=""/><pubDate>Thu, 21 Aug 2025 12:02:00 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[It’s one of the most comprehensive and high quality resource you can find online with support for 5 programming languages — Java, Python, C++, C#, and TypeScript.Many chapters are . To unlock full access, you would need to become a  to the newsletter. and  and other important  (with real-world examples)40+ LLD interview problems (with more added over time), with UML class diagrams and design patterns explained in context.Support for Java, Python, C++,  C#, and TypeScriptBuilt-in  and  where you can edit, run and see the solution output directly on the site (supports Java, Python, C++,  and C#) to test your understanding.I’ve poured a lot of thought and effort into making this course as useful and practical as possible. I truly hope you’ll find it valuable in your interview prep journey.I will keep making improvements and enhancements to this resource over time.You may already know my , which is one of the most popular resources to learn LLD. This course takes it to the next level, offering a far better reading experience, focused specifically on interview prep. I have also updated the solutions in the Github repository with more design patterns and class diagrams.Starting , subscription pricing will increase:Subscribe now to lock in the current price.All existing paid subscribers will continue at their current rate.💎 New: Lifetime Access PlanYou can now get  to all current and future AlgoMaster premium content for a .For any questions related to content or subscription, please reply to this email or reach out at ]]></content:encoded></item><item><title>Vibing With Amazon Kiro</title><link>https://www.kdnuggets.com/vibing-with-amazon-kiro</link><author>Cornellius Yudha Wijaya</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-wijaya-vibing-amazon-kiro.png" length="" type=""/><pubDate>Thu, 21 Aug 2025 12:00:41 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Learn how to implement leverage Amazon's agentic AI in your IDE.]]></content:encoded></item><item><title>We Are Only Beginning to Understand How to Use AI</title><link>https://www.oreilly.com/radar/we-are-only-beginning-to-understand-how-to-use-ai/</link><author>Tim O’Reilly</author><category>dev</category><category>ai</category><enclosure url="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/08/Neanderthal-with-a-laptop.jpg" length="" type=""/><pubDate>Thu, 21 Aug 2025 10:32:36 +0000</pubDate><source url="https://www.oreilly.com/radar">Oreilly ML</source><content:encoded><![CDATA[I remember once flying to a meeting in another country and working with a group of people to annotate a proposed standard. The convener projected a Word document on the screen and people called out proposed changes, which were then debated in the room before being adopted or adapted, added or subtracted. I kid you not.I don’t remember exactly when this was, but I know it was after the introduction of Google Docs in 2005, because I do remember being completely baffled and frustrated that this international standards organization was still stuck somewhere in the previous century.You may not have experienced anything this extreme, but many people will remember the days of sending around Word files as attachments and then collating and comparing multiple divergent versions. And this behavior also persisted long after 2005. (Apparently, this is still the case in some contexts, such as in parts of the U.S. government.) If you aren’t old enough to have experienced that, consider yourself lucky.This is, in many ways, the point of Arvind Narayanan and Sayash Kapoor’s essay “AI as Normal Technology.” There is a long gap between the invention of a technology and a true understanding of how to apply it. One of the canonical examples came at the end of the Second Industrial Revolution. When first electrified, factories duplicated the design of factories powered by coal and steam, where immense central boilers and steam engines distributed mechanical power to various machines by complex arrangements of gears and pulleys. The steam engines were replaced by large electric motors, but the layout of the factory remained unchanged.Only over time were factories reconfigured to take advantage of small electric motors that could be distributed throughout the factory and incorporated into individual specialized machines. As I discussed last week with Arvind Narayanan, there are four stages to every technology revolution: the invention of new technology; the diffusion of knowledge about it; the development of products based on it; and adaptation by consumers, businesses, and society as a whole. All this takes time. I love James Bessen’s framing of this process as “learning by doing.” It takes time and shared learning to understand how best to apply a new technology, to search the possible for its possibleness. People try new things, show them to others, and build on them in a marvelous kind of leapfrogging of the imagination.So it is no surprise that in 2005 files were still being sent around by email, and that one day a small group of inventors came up with a way to realize the true possibilities of the internet and built an environment where a file could be shared in real time by a set of collaborators, with all the mechanisms of version control present but hidden from view.On next Tuesday’s episode of , I’ll be talking with that small group—Sam Schillace, Steve Newman, and Claudia Carpenter—whose company Writely was launched in beta 20 years ago this month. Writely was acquired by Google in March of 2006 and became the basis of Google Docs.In that same year, Google also reinvented online maps, spreadsheets, and more. It was a year that some fundamental lessons of the internet—already widely available since the early 1990s—finally began to sink in.Remembering this moment matters a lot, because we are at a similar point today, where we think we know what to do with AI but are still building the equivalent of factories with huge centralized engines rather than truly searching out the possibility of its deployed capabilities. Ethan Mollick recently wrote a wonderful essay about the opportunities (and failure modes) of this moment in “The Bitter Lesson Versus the Garbage Can.” Do we really begin to grasp what is possible with AI or just try to fit it into our old business processes? We have to wrestle with the angel of possibility and remake the familiar into something that at present we can only dimly imagine.I’m really looking forward to talking with Sam, Steve, Claudia, and those of you who attend, to reflect not just on their achievement 20 years ago but also on what it can teach us about the current moment. I hope you can join us.AI tools are quickly moving beyond chat UX to sophisticated agent interactions. Our upcoming AI Codecon event, Coding for the Agentic World, will highlight how developers are already using agents to build innovative and effective AI-powered experiences. We hope you’ll join us on September 9 to explore the tools, workflows, and architectures defining the next era of programming. It’s free to attend.Register now to save your seat]]></content:encoded></item><item><title>✨️ DAY 3 OF 100 ✨️</title><link>https://dev.to/lyop_achayi/day-3-of-100-2pe6</link><author>TANYA LYOP ACHAYI</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 10:10:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
Today was all about variables and data types,the little boxes where Python stores information. I played with strings, numbers, floats, and even booleans. It’s like teaching Python to remember my name, age, and that. yes… I’m definitely learning 🤭Every line feels like a step closer to building something cool.]]></content:encoded></item><item><title>Turn Your Photo Library Into a Location-Based Search Engine Using EXIF Metadata</title><link>https://dev.to/devasservice/turn-your-photo-library-into-a-location-based-search-engine-using-exif-metadata-41ni</link><author>Developer Service</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 07:30:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Have you ever tried to find that one vacation photo you took years ago, only to scroll endlessly through thousands of images with no luck? What many people don’t realize is that most photos already come with a hidden trail of breadcrumbs that can solve this problem: .Every time you snap a photo with a smartphone or digital camera, extra information gets embedded into the file, details like the date, camera settings, and, in many cases, the exact GPS coordinates of where the picture was taken. This hidden metadata is called EXIF (Exchangeable Image File Format), and it’s more powerful than it looks. While smartphones often automatically organize your photos, many of us still have massive collections stored on a , where sorting and searching manually can feel impossible.By extracting EXIF data, you can do much more than just learn which lens or exposure setting was used. You can index, organize, and search your entire photo library in ways that go far beyond filenames and folders. Want to pull up every photo taken in Paris? Or quickly filter for shots within 10 kilometers (about 6 miles) of Central Park? With EXIF indexing, that becomes not only possible but straightforward.In this article, we’ll explore how to extract EXIF metadata, build an index of your photos, including those on NAS drives, and run location-based searches to find exactly what you’re looking for.When you take a photo, your camera doesn’t just capture light, it also records a set of descriptive details about the image, known as . EXIF stands for Exchangeable Image File Format, and it’s a standardized way of embedding extra information directly into the image file itself.Think of EXIF as the "digital notebook" your camera keeps for each shot. Some of the most common fields include: – the exact date and time the photo was taken. – make, model, lens, focal length, aperture, shutter speed, ISO. – latitude, longitude, and sometimes altitude, if location services were enabled.Among these, the GPS data is especially powerful for organizing and searching photos. Cameras and smartphones typically store coordinates in a format based on degrees, minutes, and seconds. For example:Latitude: 40° 46′ 56.62″ N  
Longitude: 73° 58′ 0.85″ W  
Altitude: 15.0 m  
This information can be converted into decimal degrees (e.g., ), which is a more convenient format for indexing and performing calculations like distance searches.EXIF isn’t just technical clutter inside your photos. It’s a hidden layer of context that tells you  and  a picture was taken, and with what gear, making it a goldmine for indexing and retrieval.
  
  
  Extracting EXIF Data from Photos
Now that we know what EXIF metadata is, the next step is learning how to actually . Python offers several libraries that make this easy: – simple and modern library to read and write EXIF data, including GPS coordinates and altitude. – lightweight library for reading EXIF metadata from JPEG and TIFF files. – popular imaging library that can read and manipulate images, including EXIF tags. – designed for both reading and writing EXIF data, useful if you need to modify metadata.For modern projects,  is often the most straightforward and Pythonic choice.
  
  
  Reading GPS Coordinates and Altitude with Here’s a minimal Python script that reads GPS coordinates  from an image using :Let's take the example of this photo:This function returns a tuple like:(51.504105555555554, -0.074575, 77.88)  # Latitude, Longitude, Altitude in meters
If the image didn't have any geo-location metadata, it would return .
  
  
  Handling Missing or Corrupted EXIF Data
Not every photo will have usable EXIF metadata. For example:Some cameras or photo-editing software strip metadata to save space.Privacy-focused apps (like messaging platforms) often remove GPS coordinates.Altitude may not always be recorded, even if latitude and longitude exist.In rare cases, EXIF data may be partially corrupted.When building your index, always  and decide how to handle them, for instance, skipping photos without GPS tags, or indexing only the fields that are available.
  
  
  Building an Index of Photos
Extracting EXIF data from a single photo is useful, but the real power comes when you apply it to your . By creating an index, you can quickly search and filter images without repeatedly scanning every file.Loop through all files in a given directory (and subdirectories).Extract EXIF metadata from each photo using .Store the results in a structured format for later searching.Here’s a Python example that scans a directory and writes the extracted EXIF metadata into a CSV file:This script generates a  file with rows like:filename,timestamp,latitude,longitude,altitude,camera
england-london-bridge.jpg,2018:08:22 13:13:41,51.504105555555554,-0.074575,77.88,Pixel 2
germany-garching-heide.jpg,2018:08:29 19:31:19,48.268274999999996,11.603361111111111,540.05,Pixel 2
irland-dingle.jpg,2012:09:16 16:58:02,52.139276657230475,-10.274594797178132,,DMC-FX60
italy-garda-lake-sailing-club.jpg,2018:09:16 11:08:41,45.877630555555555,10.857161111111111,71.95,Pixel 2
japan-katsura-river.jpg,2016:11:12 16:13:18,35.014377,135.669015,0.0,MI 5
taiwan-jiufen.jpg,2016:04:04 19:35:38,25.10820386111111,121.8439483611111,279.0,GT-I9505
turkey-bodrum.jpg,2018:10:18 18:16:32,37.02995277777778,27.41326388888889,79.19,Pixel 2

There are multiple ways to store the index, each with pros and cons:✅ Easy to read, portable, no setup required.❌ Searching can be slow for large collections (tens of thousands of photos).SQLite (or Postgres for larger setups)✅ Efficient queries, support for filtering, sorting, and even spatial queries.✅ Scales better for very large photo libraries.❌ Requires a bit more setup and knowledge of SQL.For small to medium personal collections, a CSV or JSON file is perfectly fine. For larger archives or a search engine interface, consider a database backend.
  
  
  Searching Photos by Location
Once you have a structured index of your photos with GPS data, the next step is . There are different approaches depending on how precise or flexible you want the search to be.
  
  
  Simple Approach: Exact Coordinate Search
The most basic method is to match photos that have the exact latitude and longitude. This is straightforward but rarely practical, since GPS coordinates can have minor variations: This approach only works if the coordinates exactly match, which is rare in real-world GPS data.
  
  
  Advanced Approach: Radius-Based Search
A more practical solution is to search for photos  of a location. The  is commonly used to calculate the great-circle distance between two points on the Earth:This will return all photos  of the target coordinates, for example:england-london-bridge.jpg 3.70 km away

  
  
  Tools and Libraries for Spatial Queries
For more advanced use cases or large datasets, several Python libraries and database features can simplify the process: – Geocoding and distance calculations. – Geometry operations and spatial queries in Python. – Use databases like  for efficient radius searches, polygon queries, or bounding boxes.By combining EXIF metadata indexing with spatial searches, you can quickly find photos taken near landmarks, cities, or even a friend’s house. This opens the door to building personal mapping tools or automated photo albums sorted by location.Indexing photos using  transforms your photo collection from a static archive into a searchable, organized library. By extracting GPS coordinates, timestamps, and camera information, you can locate photos based on location, date, or device.By combining this indexing with spatial searches, you gain the ability to find photos within a radius, track journeys over time, or group images by location. This allows for turning raw data into actionable insights.Leveraging the EXIF metadata, you can turn a simple collection of images into a powerful, location-aware photo library, making lost memories instantly findable and your workflow dramatically more efficient.]]></content:encoded></item><item><title>Bypass Bot Detection with Python Selenium. 🤖</title><link>https://dev.to/thetanweerali/bypass-bot-detection-with-python-selenium-3p44</link><author>Ali</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 07:29:13 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Bypassing Bot Detection Software with Selenium in Python]]></content:encoded></item><item><title>The Growing Need of Online Tools in 2025</title><link>https://dev.to/toolquix/the-growing-need-of-online-tools-in-2025-omj</link><author>Toolquix</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 06:54:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In today’s fast-paced digital world, efficiency and accessibility are more important than ever. Whether you’re a student, a blogger, a designer, or just someone trying to get daily tasks done faster, having the right online tools can make a huge difference.Time-saving: Instead of installing heavy software, online tools let you get things done instantly from your browser.Cross-platform accessibility: Work seamlessly from desktop, tablet, or mobile without worrying about compatibility issues.Cost-effective: Many online tools are free or freemium, reducing the need for expensive software licenses.Centralized workflow: Consolidating tasks like file conversion, text formatting, or color code generation in one place saves both time and mental effort.One platform that’s addressing this need is Toolquix
. It’s a free hub of online tools designed for productivity and convenience. Some of the features include:File Conversion: Quickly convert text, PDF, and HTML files without downloading software.Text Utilities: Remove duplicates, format text, and even generate Unicode text styles.Color & Design Tools: Convert HEX, RGB, HSL, CMYK values, or pick colors for your design projects.Productivity Boosters: Simple tools that save time for students, bloggers, and professionals alike.Toolquix makes it easy to accomplish everyday digital tasks without switching between multiple platforms.The Future of Online ToolsAs more people rely on the web for work, study, and creativity, the demand for efficient online tools will continue to grow. Platforms like Toolquix that centralize multiple utilities in one place are not just convenient—they’re becoming essential.Whether you’re a student trying to handle assignments, a designer needing fast color conversions, or a writer formatting content, having a reliable online tool hub is crucial.Check out Toolquix here: Toolquix
 and explore a wide range of tools designed to make your online tasks simpler and faster.]]></content:encoded></item><item><title>Show HN: I replaced vector databases with Git for AI memory (PoC)</title><link>https://github.com/Growth-Kinetics/DiffMem</link><author>alexmrv</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 06:20:11 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hey HN! I built a proof-of-concept for AI memory using Git instead of vector databases.The insight: Git already solved versioned document management. Why are we building complex vector stores when we could just use markdown files with Git's built-in diff/blame/history?Memories stored as markdown files in a Git repo
Each conversation = one commit
git diff shows how understanding evolves over time
BM25 for search (no embeddings needed)
LLMs generate search queries from conversation context
Example: Ask "how has my project evolved?" and it uses git diff to show actual changes in understanding, not just similarity scores.This is very much a PoC - rough edges everywhere, not production ready. But it's been working surprisingly well for personal use. The entire index for a year of conversations fits in ~100MB RAM with sub-second retrieval.The cool part: You can git checkout to any point in time and see exactly what the AI knew then. Perfect reproducibility, human-readable storage, and you can manually edit memories if needed.Stack: Python, GitPython, rank-bm25, OpenRouter for LLM orchestration. MIT licensed.Would love feedback on the approach. Is this crazy or clever? What am I missing that will bite me later?]]></content:encoded></item><item><title>🚀 From Java to Go in 2025: 6 Steps for a Smooth Start</title><link>https://dev.to/aleksei_aleinikov/from-java-to-go-in-2025-6-steps-for-a-smooth-start-23ji</link><author>Aleksei Aleinikov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 21 Aug 2025 03:41:29 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Thinking about switching from Java to Go?The biggest wins aren’t fancy frameworks — it’s the everyday differences that change how you design and debug.Here are 6 I’ve found most valuable:
    •🎯 Explicit error handling (not hidden exceptions)
    •🔌 Interfaces declared at usage, implemented implicitly
    •🛡️ Constructors to prevent nil‑crashes
    •📏 Receiver vs nil semantics (know when calls are safe)
    •🔤 Bytes vs runes vs user text (strings ≠ chars)
    •✍️ GoFmt is a baseline, naming still matters]]></content:encoded></item><item><title>HTTPS at 80 Gbps? Yes, in Go (2025)</title><link>https://dev.to/aleksei_aleinikov/https-at-80-gbps-yes-in-go-2025-hk4</link><author>Aleksei Aleinikov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 21 Aug 2025 03:40:31 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[“Encryption is slow, HTTPS can’t be high‑speed.”💡 Turns out, the bottleneck isn’t the math — it’s handshakes and memory copies.Here’s what I did to make a single 1U Go server push 70–80 Gbps over HTTPS:
•🚀 Switched to faster handshake signatures (ECC stamp instead of calligraphy)
•🔑 Enabled cluster‑wide session resumption (no storm of new handshakes)
•📦 Cut out extra copies — pushed bulk encryption down the stack with zero‑copy I/O]]></content:encoded></item><item><title>🌀 JSON v2 in Go (2025): What Actually Changed</title><link>https://dev.to/aleksei_aleinikov/json-v2-in-go-2025-what-actually-changed-5g1a</link><author>Aleksei Aleinikov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 21 Aug 2025 03:39:32 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Go’s new JSON stack landed in 2025 — but what really changed, and do you need to rewrite your code?Here’s the short version:•✅ Your old  still works (no big migration)
•⚡ New helpers: ,  for direct I/O
•📡 Real streaming via 
•🏷️ Smarter field tags (, , )
•🚀 Faster decoding + stricter defaults (catch bugs early)Think of JSON v2 as a tightened toolkit: same foundations, but with better defaults, streaming, and performance.]]></content:encoded></item><item><title>⚡ Go Arenas: Request‑Scoped Speed in 2025</title><link>https://dev.to/aleksei_aleinikov/go-arenas-request-scoped-speed-in-2025-54c3</link><author>Aleksei Aleinikov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 21 Aug 2025 03:38:26 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[High‑throughput Go services often choke not on logic, but on allocation churn + GC scans.That’s where arenas come in:
    •Allocate many objects “in bulk”
    •Free them all at once (end of request/job)
    •Reduce GC pressure & tail latencyI share 3 real‑world patterns I use arenas for:
✅ Parsing request logs without heap trash
✅ Building graphs then keeping only compact snapshots
✅ Assembling JSON responses with fewer allocations]]></content:encoded></item><item><title>🍰 Go Slices Finally Explained: Why They Behave the Way They Do</title><link>https://dev.to/aleksei_aleinikov/go-slices-finally-explained-why-they-behave-the-way-they-do-4n6j</link><author>Aleksei Aleinikov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 21 Aug 2025 03:37:10 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Ever wondered why appending to one slice suddenly mutates another? Or why your nil vs empty slice checks sometimes bite back?In Go, a slice isn’t magic — it’s just a tiny descriptor:
    •a pointer to data,
    •and its capacity.
That leads to some gotchas (shared arrays, silent reallocations) — but also powerful tricks:
✅ Safe in‑place compaction
✅ O(1) element removal (if order doesn’t matter)
✅ Guaranteed slice isolation with the full slice expression ()]]></content:encoded></item><item><title>Everything You Need to Know About the New Power BI Storage Mode</title><link>https://towardsdatascience.com/50-shades-of-direct-lake-everything-you-need-to-know-about-the-new-power-bi-storage-mode/</link><author>Nikola Ilic</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 03:05:27 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI Agents for Supply Chain Optimisation: Production Planning</title><link>https://towardsdatascience.com/ai-agents-for-supply-chain-optimisation-production-planning/</link><author>Samir Saci</author><category>dev</category><category>ai</category><pubDate>Thu, 21 Aug 2025 02:38:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[How to integrate an optimisation algorithm in a FastAPI microservice and connect it with an AI workflow to automate production planning.]]></content:encoded></item><item><title>Diving Deep: Understanding the Mechanics</title><link>https://dev.to/dev_patel_35864ca1db6093c/diving-deep-understanding-the-mechanics-453c</link><author>Dev Patel</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:51:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Imagine you're baking a cake. You have the recipe (your machine learning algorithm), but the perfect cake depends on the precise amounts of each ingredient (your hyperparameters): the oven temperature, baking time, amount of sugar, etc. Getting these just right is crucial for a delicious outcome. This, in essence, is hyperparameter tuning. And Grid Search is one powerful technique to help us find that perfect recipe.Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model to achieve the best possible performance. Hyperparameters are settings that are  learned from the data during training, unlike the model's parameters (weights and biases). They control the learning process itself. Grid Search is a brute-force approach to hyperparameter tuning where we systematically try out every combination of hyperparameters within a predefined range.Let's break down the core concepts:
  
  
  1. The Hyperparameter Landscape
Imagine a multi-dimensional space where each dimension represents a hyperparameter (e.g., learning rate, regularization strength). Each point in this space represents a unique combination of hyperparameters, and each point corresponds to a model's performance (e.g., accuracy, F1-score). Our goal is to find the point with the highest performance.
  
  
  2. The Grid Search Algorithm
Grid Search is a straightforward algorithm:Define the hyperparameter search space:  Specify the range and values for each hyperparameter.  For example:  in ,  in . Generate all possible combinations of hyperparameter values.  This forms our "grid" of points in the hyperparameter space. For each combination in the grid:Train the model using those hyperparameters.Evaluate the model's performance using a suitable metric (e.g., accuracy on a validation set). Choose the hyperparameter combination that yielded the best performance.Here's a simplified Python pseudo-code representation:
  
  
  3.  Mathematical Underpinnings (Optimization)
Grid Search doesn't explicitly use gradient-based optimization. Instead, it's a form of . Gradient-based methods, like gradient descent, rely on calculating the gradient (the direction of steepest ascent) of the performance function with respect to each hyperparameter. This gradient guides the search towards better hyperparameter combinations. Grid Search, however, simply tries all combinations and selects the best one. It's computationally expensive but conceptually simple.
  
  
  Real-World Applications and Impact
Grid Search, despite its simplicity, finds widespread application: Optimizing convolutional neural network (CNN) architectures by tuning hyperparameters like the number of layers, filter sizes, and learning rate.Natural Language Processing (NLP): Fine-tuning the hyperparameters of recurrent neural networks (RNNs) or transformers for tasks like sentiment analysis or machine translation. Adjusting the hyperparameters of collaborative filtering or content-based filtering algorithms to improve recommendation accuracy.
  
  
  Challenges and Limitations
  The number of combinations grows exponentially with the number of hyperparameters and the range of values.  This can be computationally prohibitive for complex models or large search spaces.  As the number of hyperparameters increases, the search space becomes incredibly vast, making it difficult to find the global optimum. Grid Search might get stuck in a local optimum, especially in non-convex performance landscapes.The computational cost of Grid Search can have environmental implications due to high energy consumption. Careful consideration of the search space and efficient algorithms are crucial to mitigate this.
  
  
  The Future of Hyperparameter Tuning
While Grid Search provides a valuable baseline, more sophisticated techniques like randomized search, Bayesian optimization, and evolutionary algorithms are gaining popularity due to their efficiency in handling high-dimensional search spaces. Research continues to explore more efficient and robust methods for hyperparameter optimization, addressing the challenges of scalability and the need for less computationally expensive solutions. The quest for the perfect hyperparameters continues, driving innovation in the field of machine learning.]]></content:encoded></item><item><title>Title: Unraveling the Mysteries of Ancient Rome: A Journey Through the Everyday Life of an Ordinary Roman</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-unraveling-the-mysteries-of-ancient-rome-a-journey-through-the-everyday-life-of-an-ordinary-5cn7</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:25:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: Unraveling the Mysteries of Ancient Rome: A Journey Through the Everyday Life of an Ordinary Roman
Imagine living in a world where roads were built for conquest, not convenience. Where sex, trade, and culture operated under systems of inequality. And yet, despite these challenges, ideas and identities moved faster than we might think. This is the fascinating world of Ancient Rome, a complex, uneven, and often uncomfortable prototype of globalization.In this blog post, we'll take a closer look at what it was like to live in Ancient Rome as an ordinary person, navigating daily life. We'll explore the roads, the sex, the trade, and the culture, and see how they all fit together to create a world that was both familiar and foreign to us.First, let's talk about the roads. The Romans were famous for their engineering feats, and their roads were no exception. They built roads all over their empire, connecting cities and towns and making it easier for people and goods to move around. But these roads were not built for convenience. They were built for conquest. The Romans believed that having a well-connected empire was essential for maintaining control over their territories, and so they invested heavily in building and maintaining their roads.Next, let's talk about sex. Sex was an important part of Roman culture, and it was often used as a way to assert power and status. Men were expected to be the active partners in sexual relationships, while women were expected to be passive. However, there were also many examples of same-sex relationships in ancient Rome, and these were often accepted and even celebrated.Moving on to trade, the Romans were skilled traders. They established a system of currency that allowed for the exchange of goods and services across their empire. They also built ports and markets to facilitate trade, and they encouraged the growth of industries such as agriculture and mining.Finally, let's talk about culture. The Romans had a rich and diverse culture, with influences from all over the world. They were known for their art, literature, and architecture, and they were also famous for their festivals and celebrations. However, like many ancient societies, the Romans also had systems of inequality in place. The wealthy and powerful held most of the power, while the poor and marginalized were often left out of the decision-making process.Despite these challenges, the Roman Empire was a remarkable achievement. It was a complex, uneven, and often uncomfortable prototype of globalization, with roads, sex, trade, and culture all operating under systems of inequality. But despite these challenges, ideas and identities moved faster than we might think, and the legacy of the Roman Empire continues to shape our world today.So, the next time you're driving on a well-connected highway or enjoying a piece of Roman art, take a moment to appreciate the incredible achievements of this ancient civilization. And remember, even in the most complex and uneven of societies, there is always room for growth and change.]]></content:encoded></item><item><title>What happens inside the computer when you run your Go server</title><link>https://dev.to/turjoc120/what-happens-inside-the-computer-when-you-run-your-go-server-165n</link><author>Turjo Chowdhury</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:20:58 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Before we deep dive, let's learn a couple of important concepts
  
  
  What Are Sockets and File Descriptors?
Sockets are endpoints for communication between computers over a network, enabling real-time data exchange.Unlike regular files, sockets do not store data but facilitate data transfer between machines.When Go requests a socket from the operating system (OS), the OS creates the socket and assigns a unique identifier called a file descriptor.A file descriptor is an integer handle that the Go server uses to manage and reference the socket.This mechanism allows the server to efficiently send and receive network data through OS-managed resources.
  
  
  Go’s Concurrency with Goroutines
Go uses goroutines, lightweight threads, to handle many client requests concurrently.The main goroutine continuously waits for incoming requests.For each new request, Go creates a new goroutine to process it independently without blocking the main one.This design ensures the server remains fast and scalable, handling multiple clients simultaneously.When no requests arrive, the main goroutine sleeps to conserve system resources and improve overall efficiency.
  
  
  Understanding How It Works in Your Computer
The kernel is the core part of the operating system that manages hardware and processes.Network requests first travel through a router and then reach your computer’s Network Interface Card (NIC), like a WiFi adapter or Ethernet port.The NIC converts the wireless or wired signals into binary data and temporarily stores it in a buffer.It then sends a signal to the kernel to process this new data.The kernel copies the data into a socket buffer that the Go server listens to, and marks it ready for reading.The Go runtime wakes up the goroutine to read and process the request.The server sends the response back through the socket and NIC.The response reaches the client’s browser.]]></content:encoded></item><item><title>Title: Discovering a Rare Type of Black Hole Feasting on a Star</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-discovering-a-rare-type-of-black-hole-feasting-on-a-star-2a4k</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:20:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Title: Discovering a Rare Type of Black Hole Feasting on a Star
As a science and space enthusiast, I am always excited to learn about new discoveries in the field of astronomy. Recently, NASA's Hubble Space Telescope and NASA's Chandra X-ray Observatory teamed up to identify a new possible example of a rare class of black holes. This discovery was made by observing X-ray emission (in purple) in an image released on July 24, 2025.The black hole in question, called NGC 6099 HLX-1, is located in a compact star cluster in a giant elliptical galaxy. This makes it a unique find, as most black holes are found in the centers of galaxies, and not in compact star clusters.Black holes are incredibly dense objects, with masses up to several times that of our Sun. They are formed when a massive star collapses under its own gravity. Black holes are also known for their intense gravitational pull, which can cause objects to be pulled in and never escape.One of the most fascinating things about black holes is their ability to consume matter. As matter falls towards a black hole, it heats up and emits X-rays. This is what scientists observed in the case of NGC 6099 HLX-1. The bright X-ray source in the image suggests that the black hole is consuming matter from a nearby star.This discovery is particularly interesting because it provides evidence for a rare type of black hole known as a "hypermassive black hole." Hypermassive black holes are incredibly massive, with masses up to several billion times that of our Sun. They are also thought to be formed in the early universe, during the formation of the first galaxies.The discovery of NGC 6099 HLX-1 is a significant milestone in our understanding of black holes and their behavior. It provides valuable insights into the formation and evolution of these mysterious objects, and opens up new avenues for research in the field of astronomy.In conclusion, the discovery of NGC 6099 HLX-1 is a fascinating find for science and space enthusiasts. It provides evidence for a rare type of black hole and sheds light on the formation and evolution of these mysterious objects. As we continue to explore the universe, discoveries like this one remind us just how much there is still to learn about the wonders of the cosmos.]]></content:encoded></item><item><title>TITLE: Tesla Partially Held Liable for Deadly 2019 Crash Involving Autopilot Self-Driving Feature</title><link>https://dev.to/yagyaraj_sharma_6cd410179/title-tesla-partially-held-liable-for-deadly-2019-crash-involving-autopilot-self-driving-feature-4d8k</link><author>Insights YRS</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 21 Aug 2025 00:15:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  TITLE: Tesla Partially Held Liable for Deadly 2019 Crash Involving Autopilot Self-Driving Feature
DESCRIPTION: In a landmark case, a jury in Florida has found Tesla partially liable for a 2019 crash involving the company's Autopilot self-driving feature. The verdict, which was handed down on February 19, 2021, means that Tesla will have to pay $200 million in damages. Autopilot is a feature that comes pre-installed on Tesla's cars and is designed to handle things like collision detection and emergency braking.The case, which was brought by the family of Naibel Benavides Leon and Dillon Angulo, who were killed in the crash, played out differently from other cases involving Tesla's Autopilot feature. The jury ultimately decided that the self-driving tech enabled driver George McGee was at fault for the crash, which occurred on March 1, 2019, in Fort Lauderdale, Florida.During the trial, Tesla's lawyers argued that McGee's decision to take his eyes off the road to reach for his phone was the cause of the crash, and that Autopilot should not be considered. However, the plaintiffs argued that Tesla and Elon Musk, the company's CEO, had marketed Autopilot as a fully autonomous driving system, which led to a false sense of safety and contributed to the crash.The verdict in this case is significant because it marks the first time that Tesla has been held liable for a crash involving its Autopilot feature. The company has mostly avoided taking responsibility for crashes involving cars with the Autopilot enabled, but this case sets a precedent for future cases.The $200 million in damages that Tesla will have to pay is a significant amount, and it will likely have a financial impact on the company. However, the verdict is also a reminder that technology is not infallible, and that drivers must remain vigilant and attentive while operating a vehicle, even when using advanced safety features like Autopilot.In conclusion, the verdict in this case is a landmark moment for the automotive industry and a reminder that technology is not a substitute for human responsibility. Tesla must continue to work towards improving its Autopilot feature and ensuring that it is used safely and responsibly by all drivers.]]></content:encoded></item><item><title># How I Built a Fully Decentralized On-Chain Game with 0 Lines of Code, Thanks to Gemini</title><link>https://dev.to/crow004/-how-i-built-a-fully-decentralized-on-chain-game-with-0-lines-of-code-thanks-to-gemini-1d0p</link><author>crow</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 20 Aug 2025 23:24:24 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[My nickname is crow, and a few months ago, I was an indie developer with what I'd call junior-level skills. Today, I'm the creator of a fully-functional, decentralized, on-chain game called Musical Chairs. The twist? I didn't write a single line of the production code myself. 100% of it was generated by Gemini, my AI coding partner integrated into VS Code.This isn't just a story about a cool project; it's a story about a new way of building. It's about how a single person with a clear vision can leverage AI to execute complex technical tasks, from writing secure smart contracts to deploying a multi-container production environment.
  
  
  The Idea: Decentralization First
The concept was simple: take the childhood game of Musical Chairs and bring it to the blockchain. A game of pure reaction, provably fair, where the winner takes the pot.My initial thought was to use a stablecoin like USDT for the game's currency. It seemed user-friendly. However, as Gemini and I delved into the technicals, I discovered a fundamental conflict with my vision. The USDT smart contract is controlled by a central entity, Tether, which has the technical ability to pause or freeze any wallet. This "kill switch" functionality, while understandable from their perspective, was a deal-breaker for me. The core of my project was to be .This led to my first major pivot: the game would use the native currency of the chain (ETH on Arbitrum). This not only ensured complete decentralization—where no single entity could interfere with player funds—but also simplified the smart contract logic significantly. To account for price volatility, the owner can adjust the stake amount as needed.
  
  
  The High-Level Architecture
The application is composed of three main pillars, all orchestrated within a Docker environment.Smart Contract (Solidity): The heart of the game. It acts as the unstoppable and transparent source of truth, handling player stakes, game state transitions, and prize distribution. Through a proxy pattern, it provides a stable, immutable address and state for users, while allowing the owner to securely upgrade the underlying game logic. The brains of the operation. It manages the game lifecycle, listens for blockchain events, and communicates with players in real-time via WebSockets. It's the off-chain coordinator for the on-chain action. The face of the game. A simple, lightweight client that interacts with the user's wallet (like MetaMask) and communicates with the backend.Here's how they interact:  A user connects their wallet on the .  The  talks to the  via a REST API to get game configuration and via WebSockets for real-time updates (e.g., other players joining).  The  listens to the blockchain for events from the  (like deposits).  The  sends transactions to the  to manage the game (e.g., starting the music round).To run this in production, we containerized everything. This makes deployment, scaling, and management incredibly robust.: The entry point. It handles SSL, serves the frontend, and routes API/WebSocket traffic.: The main Go application.: A dedicated, hardened microservice whose only job is to sign blockchain transactions.: The database for storing game history and analytics data.: An intrusion prevention service that monitors logs and bans malicious IPs.: A self-hosted, privacy-respecting analytics service.
markdown A key privacy-enforcing service. It's configured to rotate Nginx logs daily while keeping zero old log files (). This ensures that sensitive information like IP addresses is purged from the server in less than 24 hours, maximizing user anonymity.
  
  
  Deep Dive: The Smart Contract
The smart contract is the most critical piece of the puzzle. Security, reliability, and transparency were non-negotiable. Here’s how we achieved that.We used OpenZeppelin's UUPS (Universal Upgradeable Proxy Standard). This allows the contract logic to be upgraded without losing the contract's state (i.e., ongoing games, funds). It's a battle-tested pattern for long-term projects.A key security measure is the  call in the implementation contract's constructor:/// @custom:oz-upgrades-unsafe-allow constructor
constructor() {
    _disableInitializers();
}
This prevents anyone from calling the  function on the logic contract itself, which could otherwise be a vector for hijacking. Interestingly, this line had to be commented out during testing with tools like Echidna and Foundry, as they would fail, but it's crucial for production security. We use OpenZeppelin's  to protect all functions that handle fund transfers (, , etc.) from re-entrancy attacks.Ownership and Role Separation: We implemented a three-address system to separate concerns and minimize risk:

 This address has the highest level of control (upgrading the contract, changing fees). It was generated offline and is never exposed to the internet. Transactions are signed on an air-gapped machine, and the raw signed transaction is then broadcast using a tool like Arbiscan's  page. This address handles the day-to-day operations, like starting games and recording results. It can be replaced instantly by the owner if compromised, without a timelock, allowing for rapid response. A dedicated address that can only receive platform commissions. This separation ensures that even if the hot wallet is compromised, the core contract and its funds remain secure. In the future, I'm considering moving the owner role to a 2-of-3 multisig for even greater resilience.Timelocks for Critical Functions: Functions that could move significant funds, like , are protected by a timelock. A withdrawal is first  with a specific amount, and can only be  after a delay. This gives users full transparency and time to react if they see something they don't like. All functions that set addresses (like changing the owner or backend wallet) prevent setting the address to , which would permanently "brick" the contract.Gemini helped me implement several gas optimization techniques. While modern compilers are excellent, explicit optimizations are still key: Instead of  with string messages, we use custom errors (error InsufficientStake();). This saves significant gas on deployment and during runtime when a check fails.Efficient State Management: We carefully designed data structures to minimize writes to storage, which is the most expensive operation on the EVM. For example, we read values into memory, perform operations, and then write the final result back to storage once. For operations where we are certain underflow/overflow cannot occur (e.g., incrementing a counter after checking its bounds), we use  blocks to save the gas that would be spent on the default safety checks in Solidity 0.8+.
  
  
  Rigorous Testing and Verification
A smart contract is only as good as its testing. We were exhaustive: We wrote 81 unit tests with Hardhat and Foundry, achieving near-100% code coverage. We also wrote fuzz tests to throw thousands of random inputs at the functions. We used  to run 50,000 random transactions against the contract to test for broken invariants (e.g., "the contract balance should never be less than the sum of all player deposits"). No vulnerabilities were found. We wrote  and  to simulate specific attack scenarios and ensure our guards worked as expected. The code was analyzed with  and , and the bytecode was checked with . We used  to analyze the gas cost of every function, helping us pinpoint areas for optimization. The contracts are verified on . This provides cryptographic proof that the deployed bytecode matches the open-source code. We initially planned to use Arbiscan, but our deployment coincided with Etherscan's major transition from their V1 API to the new, unified V2 keys. This transitional period caused temporary verification issues, making  an excellent and reliable alternative.This multi-layered approach to security and testing gives me, and hopefully my users, a high degree of confidence in the contract's integrity.In the next part, I'll dive into the Backend, Frontend, and the operational infrastructure that powers the game.Now, let's get into the off-chain machinery that brings the game to life: the microservices, the security fortress I built around them, and the path forward.
  
  
  Deep Dive: The Keyservice Microservice - A Digital Fortress
One of my biggest concerns was handling the backend's private key. This key is "hot" – it needs to be online to sign transactions like starting a game. A compromise here would be disastrous. My solution was to build a dedicated, hardened microservice with a single responsibility: .It's a tiny Go application, but it's built like a fortress: It runs in its own Docker container and does nothing but receive data from the main backend, sign it, and return the signature. It has no other network access. The encrypted private key JSON and its password are not in the container image or environment variables. They are mounted as Docker Secrets, which are stored in-memory on the host and are only accessible to the services they're granted to. The files on the host machine have their permissions locked down with .Quantum-Resistant Encryption: This is where my paranoia really kicked in. I didn't just encrypt the secrets; I used GPG with AES-256 and a high  (--s2k-mode 3 --s2k-count 65011712). This is a slow, synchronous encryption method that makes brute-force attacks computationally infeasible, even against future threats like Grover's algorithm for quantum computers. This is military-grade stuff. What if the keyservice container crashes and Docker fails to restart it? The main backend has a unique, obfuscated module containing the GPG-encrypted key, passphrase, and  file. If it can't reach the keyservice, it uses a master password to decrypt these assets , restart the container, and then securely wipes the decrypted files from disk by overwriting them with zeros. It's an automated disaster recovery plan.I considered hardware keys like a YubiKey or cloud HSMs, but rejected them. A physical key introduces a single point of failure and a potential de-anonymization vector. Cloud HSMs require trusting a third party, which I wasn't willing to do. This self-contained, heavily fortified microservice was the answer. The next step is to move from Docker Compose to Kubernetes for more granular control and to "harden" the containers using  and . (Secure Computing Mode) is a Linux kernel feature that restricts the system calls a process can make. I can create a profile that allows  the specific syscalls Go needs to run the keyservice, and nothing else. (Application Armor) confines programs to a limited set of resources. I can define a policy that prevents the keyservice from writing to unexpected disk locations or accessing unauthorized network ports.Together, these will create an even smaller attack surface, making a container breakout virtually impossible.
  
  
  Deep Dive: The Backend (Go)
The main backend is the game's central nervous system, written in Go for its performance and concurrency. It's logically split into modules:: Defines all the REST endpoints for the frontend. It includes protection against slow header attacks to prevent resource exhaustion.: Handles all interaction with the smart contract. It uses versioned auto-generated Go bindings from the contract's ABI. This is also where I used  to interact with the upgradeable proxy contract, allowing the backend to seamlessly call functions on the implementation contract through the stable proxy address.: On startup, it quickly reads past blockchain events to catch up to the current state, then switches to a slower, regular polling of new events.: The largest and most complex module, containing the entire game state machine and lifecycle.: Manages the WebSocket connections. To join a game, the user signs a  (a single-use random string) provided by the backend. This proves ownership of their address without a full transaction and also registers any associated referrer. The backend verifies this signature and, upon success, issues a one-time WebSocket authentication token. The frontend then uses this token to establish a secure, authenticated connection, preventing unauthorized access.: Manages database interaction using , which provides a fantastic object-relational mapping layer and handles database schema migrations automatically. This is also where the analytics models for the conversion funnel and profit reports live. I was relentless with testing.  Most modules have , verified with .  I used  with a suite of static analyzers like  (security), , and  to catch potential issues early.  Database tests were not mocked. I used the  pattern, where a real PostgreSQL Docker container is spun up for the test suite and torn down afterward, ensuring tests run against a real environment.  I heavily profiled the code for CPU usage, memory allocations (, ), and lock contentions (, ) to hunt down performance bottlenecks and race conditions.  Critical modules were compiled with  to obfuscate the code, and all binaries were packed with  to shrink their size and make reverse-engineering a nightmare.  Finally, the entire codebase was analyzed with  to enforce best practices and catch any remaining code smells.
  
  
  Deep Dive: The Frontend (HTML/CSS/JS)
The frontend is intentionally simple: vanilla HTML, CSS, and JavaScript (transpiled from TypeScript). This wasn't a shortcut; it was a strategic choice. A simple, static site can be easily hosted on decentralized platforms like  or , further enhancing the project's censorship resistance.Even with its simplicity, it's well-tested using Jest for unit tests (), ESLint for code quality, and Prettier for consistent formatting.
  
  
  The Community and The Road Ahead
A project is nothing without a community. My growth strategy is focused on rewarding early believers. I launched a campaign on Zealy where users can complete quests to earn XP. The first 300 community members will receive a special NFT, granting them the "OG Member" role in Discord and future in-game bonuses. I'm planning to add a global leaderboard and host tournaments with real prize pools.This project has been an incredible journey. It started as a simple idea and, with the help of my AI partner, evolved into a secure, robust, and fully decentralized application. I went from a junior-level coder to a full-stack dApp creator, and I did it by focusing on the vision and letting the AI handle the complex implementation.This is the new frontier of indie development. If you have an idea, the tools to build it are more accessible than ever.Come play a game and join the community! muschairs.com discord.gg/wnnJKjgfZW @crow004_crownpub1v0kc8fwz67k0mv539z6kaw5h25et9e2zmnnqq6z2naytaq566gwqkzz542My next steps are to spread the word on platforms like Reddit, connect with web3 enthusiasts, and, of course, start building my next idea. Thanks for reading!]]></content:encoded></item><item><title>Go 1.25: JSON v2 e Novo GC</title><link>https://dev.to/rflpazini/go-125-json-v2-e-novo-gc-4k07</link><author>Rafael Pazini</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 20 Aug 2025 21:50:28 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Chegou o Go 1.25 e, sinceramente, é sobre tempo. Duas mudanças que vão fazer diferença real no seu dia a dia: o  que não é uma piada de performance e o  que promete parar de sugar sua CPU.Vamos ver o que realmente mudou e se vale a pena migrar (spoiler: provavelmente sim).
  
  
  Por que o JSON v2 existe?
O  padrão é tipo aquele colega de trabalho: faz o trabalho, mas reclama o tempo todo. Lento, cheio de alocações desnecessárias, e você sempre acaba procurando alternativas como EasyJSON ou JSONIterator quando a coisa aperta.A equipe do Go finalmente acordou e disse: "Ok, vamos fazer direito dessa vez. "jsonv2 go run main.go

Exemplo básico que funciona de verdade:
  
  
  O que mudou na implementação do JSON v2
A nova implementação não é apenas uma otimização superficial do código existente. A equipe do Go reescreveu o parser do zero, focando em três problemas principais que atormentavam o  original: , parsing sequencial ineficiente, e falta de suporte nativo para streaming.
  
  
  Arquitetura otimizada para Menos Alocações
O maior vilão do JSON v1 sempre foram as alocações desnecessárias. Cada vez que você fazia  em uma struct grande, o parser criava dezenas de objetos intermediários (buffers temporários, slices auxiliares, interfaces{} para cada valor).O v2 introduz um sistema de pooling interno e  que reduz drasticamente essas alocações. Em vez de criar novos objetos a cada operação, ele mantém pools de estruturas reutilizáveis que são recicladas entre chamadas.
  
  
  Parser não-sequencial e streaming nativo
Outra mudança fundamental: o v1 sempre processava JSON de forma , lia byte por byte, construindo a estrutura na ordem exata do documento. Isso funcionava, mas era ineficiente para JSONs grandes.O v2 implementa  e . Para JSONs grandes, ele pode processar pedaços do documento simultaneamente e construir a estrutura final de forma mais eficiente. Isso é especialmente poderoso quando você está lidando com arrays grandes ou objetos com muitas propriedades.
  
  
  Otimizações específicas para tipos comuns
O v2 também inclui  otimizados para tipos que aparecem frequentemente em APIs modernas: têm parsing especializado que evita conversões desnecessárias.  (o caso mais comum) têm tratamento otimizado. Slices de tipos primitivos são processados em lotes quando possível.Mensagens de erro mais úteisUm bônus que todo mundo vai amar: as mensagens de erro ficaram muito melhores. Em vez de "invalid character 'x' looking for beginning of value", agora você recebe contexto real:Quando vale usar? Se você processa muito JSON por segundo, trabalha com streaming de dados grandes, ou simplesmente está cansado de debuggar mensagens de erro confusas. A nova implementação resolve esses três problemas de uma vez.
  
  
  GreenteaGC: Entendendo o Novo Coletor de Lixo
Antes de falar do novo GC, preciso explicar por que o atual às vezes é um problema. O Go usa um coletor concurrent mark-and-sweep tricolor desde a versão 1.5. Parece complexo, mas a ideia é simples: ele funciona junto com seu programa (concurrent), marca objetos que ainda estão sendo usados (mark), e depois varre os não marcados para liberar memória (sweep). O "tricolor" é só o algoritmo usado para marcar sem quebrar referências.O problema? Esse processo, mesmo sendo concurrent, ainda compete por recursos de CPU e pode causar  em momentos críticos. Pior ainda: em programas que criam muitos objetos de vida curta (tipo APIs que processam requests), o GC pode ficar numa corrida constante tentando limpar a bagunça.O Que GreenteaGC Muda na PráticaComo ativar o experimental:greenteagc go run main.go

O  reimplementa partes fundamentais do coletor com foco em reduzir o overhead por objeto e diminuir o trabalho paralelo desnecessário. Na prática, isso significa que ele é mais esperto sobre quando coletar lixo e quanto CPU gastar nisso.A grande diferença está na forma como ele lida com objetos pequenos e temporários. O GC atual trata todos os objetos meio que igual - um  de 10 bytes recebe o mesmo tipo de atenção que um slice gigante. O novo coletor tem estratégias diferentes baseadas no tamanho e padrão de uso dos objetos.Onde Você Sente a Diferença são o caso clássico. Imagine um endpoint que recebe 10.000 requests por segundo. Cada request cria várias structs temporárias, slices para processar dados, maps para organizar responses. Com o GC atual, toda essa criação/destruição gera trabalho constante para o coletor. também se beneficiam muito. Quando você processa milhares de registros por minuto, cada um passando por várias transformações que criam objetos intermediários, o GC tradicional pode virar gargalo real.Em benchmarks divulgados pela equipe do Go, o  mostra reduções de overhead entre 10% e 40%, dependendo do padrão de alocação. Isso se traduz em:Menos pausas perceptíveis: aqueles microfreezees de 5-15ms que aparecem no percentil 99 de latência diminuem significativamente.Melhor throughput sustentado: menos CPU gasta em GC = mais CPU disponível para seu código.Comportamento mais previsível: menos variação na latência, especialmente importante para sistemas que precisam de SLA consistente.
  
  
  Cenários que mais se beneficiam
 são um caso especial. Quando você roda no Kubernetes com limites de CPU bem definidos, cada ciclo desperdiçado pelo GC é um ciclo que não está processando requests reais. O novo coletor entende melhor esses limites e se adapta.Sistemas de alta concorrência onde você tem centenas ou milhares de goroutines criando objetos simultaneamente. O GC atual pode ter dificuldade para coordenar a limpeza entre todas essas threads. O  tem estratégias melhores para lidar com essa complexidade.Aplicações que fazem marshaling/unmarshaling intensivo - que é exatamente onde o JSON v2 também ajuda. A combinação dos dois pode ser especialmente poderosa: menos alocações na serialização JSON + GC mais eficiente para limpar o que sobra.Com o , você não vai ver milagres, mas vai notar estabilidade maior na latência e uso mais eficiente de recursos. É especialmente visível em load testing sustentado, onde o comportamento do GC ao longo do tempo faz mais diferença que picos isolados.
  
  
  Comparação Honesta: JSON v2 vs EasyJSON
Durante anos, se você queria performance real com JSON em Go, tinha que partir pro EasyJSON. Gerava código otimizado, era rápido, mas que trabalhão configurar e manter.Para : JSON v2 chegou bem perto, às vezes até superando quando você tem muitos  e .Para marshaling de dados conhecidos: EasyJSON ainda leva vantagem, mas a diferença não é mais abismal.Para : JSON v2 destroi tanto o v1 quanto o EasyJSON, porque foi otimizado exatamente para isso.A interpretação honesta? Se você quer simplicidade e performance decente, teste JSON v2. Se você quer exprimir cada ciclo de CPU e já tem estruturas definidas, EasyJSON ainda é rei. Mas agora pelo menos temos escolha real.
  
  
  Benchmark com dados do mundo real
Vamos usar dados do , ou seja, JSONs reais, grandes, variados. É o teste mais honesto possível, sem truque de benchmark sintético.Primeiro, baixando os dados: data data
curl  2025-07-01-12.json.gz 
  https://data.gharchive.org/2025-07-01-12.json.gz
 2025-07-01-12.json.gz    ..

Setup do teste (estrutura organizada):bench-json/
├── go.mod
├── benchmark
│   ├── bench_v1_test.go
│   └── bench_v2_test.go
└── internal/
    └── ndjson.go         # helpers de leitura

Helpers para lidar com NDJSON (internal/ndjson.go):Benchmark para v1 (bench_v1_test.go):Benchmark para a v2 (bench_v2_test.go):
go UnmarshalMap ^ ./...

jsonv2 go UnmarshalMap ^ ./...


  
  
  Resultados que você sente na prática
Rodei o benchmark com dados reais do GitHub Archive no meu MacBook M3 Pro. Vou traduzir os números técnicos para o que isso significa no seu dia a dia: Sua API que processa 150MB de dados JSON demora  A mesma API agora demora  Se sua API respondia em 200ms, agora responde em . É a diferença entre uma API que parece rápida e uma que parece instantânea. Para processar esses dados, Go aloca  Agora aloca apenas 180MB a menos de pressão no GC. Isso significa menos pausas, menos CPU gasta limpando lixo, containers mais estáveis no Kubernetes. Processava dados a  Agora processa a  Uma API que conseguia processar  agora processa  com a mesma máquina. Criou  de objetos temporários Criou apenas  a menos de trabalho para o Garbage Collector. Menos interrupções, menos spikes de CPU, comportamento mais previsível.Se você roda no AWS/GCP e processa : Precisava de uma instância de  para manter latência aceitável Consegue rodar na mesma carga com  ou processar 48% mais dados na mesma máquina ~$50-100/mês por instância, dependendo da região e tipo de máquina.Isso não é benchmark sintético, são dados reais de eventos do GitHub, com a complexidade e variação que você encontra em produção. A melhoria é real e você vai sentir no monitoramento.Onde você vai sentir a diferença? APIs de alta carga vão processar JSON mais rápido, microservices vão se comunicar com menos overhead, pipelines de ETL vão ter menos pausas do GC, e containers no Kubernetes vão usar melhor os limites de CPU.Quando migrar? Se você tem APIs que processam muito JSON, sistemas sensíveis à latência, workloads que criam muitos objetos temporários, ou simplesmente curiosidade científica, vale testar agora. É experimental, mas já está estável o suficiente para brincar.Go 1.25 não trouxe apenas melhorias incrementais, trouxe um salto real nas partes que mais usamos: JSON e gerenciamento de memória.Para quem quer estabilidade, continue no GC padrão e . Funciona bem, sempre funcionou. Para quem gosta de viver no futuro, ative  e  e meça os resultados. Os números que mostrei são reais e reproduzíveis.O melhor do Go sempre foi esse equilíbrio: estabilidade no core, inovação nos experimentos. Agora é nossa vez de testar essas novidades e dar feedback para a comunidade. Teste, meça, e me conta os resultados. Aposto que você vai gostar do que vai encontrar.]]></content:encoded></item><item><title>Create personalized products and marketing campaigns using Amazon Nova in Amazon Bedrock</title><link>https://aws.amazon.com/blogs/machine-learning/create-personalized-products-and-marketing-campaigns-using-amazon-nova-in-amazon-bedrock/</link><author>Raechel Frick</author><category>dev</category><category>ai</category><enclosure url="https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-19120/FragranceLab_Social_Horizontal_compressed.mp4" length="" type=""/><pubDate>Wed, 20 Aug 2025 21:50:24 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This post was written with Jake Friedman from Wildlife.Businesses are seeking innovative ways to differentiate themselves through hyper-personalization and enhanced customer experiences. At the Cannes Lions International Festival of Creativity 2025, AWS showcased The Fragrance Lab, an interactive and inspiring experience that demonstrates how generative AI can support the development of hyper-personalized consumer goods and accelerate advertising creative concept and campaign assets development. Following Cannes Lions 2025, The Fragrance Lab received a Gold and Silver Stevie Award from the International Business Awards in the Brand & Experiences category.Built using Amazon Nova in Amazon Bedrock, The Fragrance Lab represents a comprehensive end-to-end application that illustrates the transformative power of generative AI in retail, consumer goods, advertising, and marketing. While our activation at Cannes Lions focused on personalized fragrance development and ad campaign creation, the underlying architecture and methodology can be adapted across diverse categories, from fashion to food and beverage, opening endless possibilities for customized customer experiences.Introducing The Fragrance LabIn this post, we explore the development of The Fragrance Lab. Our vision was to craft a unique blend of physical and digital experiences that would celebrate creativity, advertising, and consumer goods while capturing the spirit of the French Riviera. To bring this vision to life, we collaborated with Wildlife, a company that is exceptional at transforming AWS generative AI services into compelling physical experiences. Wildlife was fundamental in brainstorming ideas that would inspire customers and showcase novel use cases that AI makes possible.As the first step, the experience used Amazon Nova Sonic, a speech-to-speech model that engages in intuitive dialogues with attendees to understand their personality and preferences. Nova Sonic extends its capabilities through tool integration, allowing it to manage user traits and interface actions through specialized tools such as , , and . These tools help maintain conversation state and a consistent flow throughout the experience. The collected conversation data and trait information are then processed through a custom Retrieval Augmented Generation (RAG) system built with Amazon Nova Pro, a highly capable multimodal model that offers our best combination of accuracy, speed, and cost. Nova Pro serves as the intelligence engine for analyzing interactions and extracting essential keywords to determine the perfect fragrance notes and composition. The application also used Amazon Bedrock Guardrails, which offers customizable safeguards and responsible AI policies to block undesirable topics—such as allergens or harmful content—to offer a seamless customer experience.For example, a customer might share with Nova Sonic that they are interested in travel. Nova Pro picked up that exploring new places often “brings a sense of freshness and excitement,” which resulted in a fragrance that feels fresh and invigorating, featuring “a burst of citrus or a floral breeze.” The customer might also share that they enjoy early morning walks across spring fields, which Nova Pro translates into a top note of fresh bergamot, a middle note featuring floral honey, and a base of lavender. The customers’ inputs guide the selection of fragrance notes—from base, to heart, to top notes—which were then expertly mixed by on-site perfumers to create truly personalized scents. Perfumers were able to customize and craft hundreds of unique fragrances per day, aided by AI. A process that would normally take hours for a perfumer was accelerated to minutes, empowering both the customer and the fragrance expert.After the personalized fragrance formula was created and sent to the perfumer queue, Amazon Nova Canvas generated customized marketing creative, including the fragrance name, tagline, and imagery that captured the essence of the formula. Attendees were able to further customize the campaign assets using guest inputs such as moody, beachy, or playful. The resulting fragrance image was then transformed into dynamic video content through Amazon Nova Reel, which customers could further customize to meet their creative vision and download to save or share. To match the Cannes Lions atmosphere, the campaign videos were generated with a French-accented female voice using Amazon Polly. The entire experience is built in Amazon Bedrock, a fully managed service to build and scale generative AI applications with AI models.The following data flow diagram shows how multiple Amazon Nova models can be combined for a rich, cohesive, and personalized customer experience.Best practices for implementationThe Fragrance Lab centers around interactions with Amazon Nova Sonic, providing users with a natural language interface to express their preferences for a custom scent. Through its tool integration capabilities, Nova Sonic orchestrates the entire experience by managing user traits and triggering appropriate workflows. These workflows seamlessly guide the experience from initial conversation to fragrance development and ultimately to campaign asset creation, driving both the visual elements and progression of the experience. The model’s ability to maintain a conversational state, while defining clear conversational flows, helps ensure a consistent and pleasant experience for every user.A well-defined workflow and conversational assistant are pivotal in guiding these conversations to uncover the qualities that are most important to each user. And the system prompt determines the personality, style, and content of your conversational assistant.You are an AI assistant designed to help the user explore their personality and 
emotional landscape in the context of creating a unique fragrance. You engage in warm, 
free-flowing, playful conversation with the user to draw out their character, 
preferences, moods, and desires. Your end goal is to derive a set of 3 to 5 personality 
traits that best describe the user. These traits will later be used in a separate 
process to match appropriate fragrance ingredients. Your tone is warm, chic, and subtly 
playful.Additional contextual information within the prompt also plays a key role in Amazon Nova Sonic effectively maintaining state, while defining the conversational flow helps ensure consistent, pleasant, and concise experiences for every user.1. **Welcoming Users**
    Welcome the user to the application experience with a brief overview of the
    process and ask if they are ready to continue.
2. **Assistant Turns** 
    Ask short and unique open ended questions to the user and choose a personality trait 
    that you think would suit the user best.
3. **Handling User Turns**
    Acknowledge the user's answers briefly and warmly.
    Focus on one trait per turn.
    Call the "addTraitTool", "removeTraitTool", "replaceTraitTool", or "clearTraitsTool" 
    tools to manage traits.
    If the user says to go back, skip, customize, or confirm/submit it means you should 
    call the "uiActionIntentTool" With direct references to our tools in the conversational flow, the user interface feels reactive and connected to the user’s input while providing opportunities for the assistant to demonstrate its expertise on this subject, which comes into the spotlight when user traits and preferences are later mapped to a set of available ingredients and raw fragrance materials.This complex fragrance recipe development is handled by Nova Pro, using its accuracy and speed to generate consistently high-quality scents. To draw from a wealth of fragrance knowledge in real time, RAG was implemented to extend Nova Pro capabilities beyond pre-trained knowledge with access to knowledge sources that include essential scent design principles, a deep understanding of each available ingredient, their profiles and potential roles within the fragrance, and their possible connections to users’ aromatic identities.The resulting fragrances are then visualized using Nova Canvas and Nova Reel. The creative models generate original compositions that reveal the fragrance name, ingredients, and a visual identity within a high-end creative campaign asset. A set of conditioning images featuring unbranded fragrance bottles help to anchor each image (as shown in the following image).A high-end fragrance ad environment inspired by a [persona description]. A clear, 
unbranded perfume bottle is visually centered and tightly framed. Key ingredients [top 
note ingredient], [middle note ingredient], [base note ingredient], and [booster 
ingredient] are arranged to surround the bottle in a balanced composition, appearing 
behind, besides, and partially in front of the base. The scene evokes [atmospheric/mood 
descriptors] using [light/color language]. The setting should feel [stylistic direction],
like a [reference style (e.g., fashion editorial, lifestyle spread, luxury campaign)].Attendees at Cannes Lions took away a physical fragrance mixed by on-site perfumers. While developing hyper-personalized consumer goods might not be scalable across all use cases, brands can innovate with artificial intelligence and achieve manufacturing outcomes that weren’t previously possible. The advertising campaign concept and asset development use case is easy to implement for brands, agencies, and media networks, allowing users to iterate and optimize campaign creative quickly. Using Amazon Bedrock, additional features could be added like translations and sizes, depending on requirements.You can watch a video walk through of The Fragrance Lab onsite at Cannes Lions 2025, and check out the following example campaign outputs.The Fragrance Lab demonstrates the power of Amazon Nova in Amazon Bedrock and how customers can create fully personalized consumer experiences. This use case can be replicated across various retail and consumer goods categories including skincare and cosmetics, fashion and accessories, food and beverage, home goods, and wellness products—all benefiting from natural conversation interaction, AI-powered product development, product identity, and creative marketing campaign generation. Get started with Amazon Nova in Amazon Bedrock today. is a Sr Product Marketing Manager at AWS. With over 20 years of experience in the tech industry, she brings a customer-first approach and growth mindset to building integrated marketing programs. Based in the greater Seattle area, Raechel balances her professional life with being a soccer mom and after-school carpool manager, demonstrating her ability to excel both in the corporate world and family life. is the Head of Industry Marketing for Media & Entertainment, Sports, Games, Advertising & Marketing at AWS, where she works with technology and industry leaders to accelerate innovation on behalf of customers. She is a global marketing leader and creator of experiences that elevate customer journeys. Before AWS, she held different positions at Microsoft, Telefonica, and more.is Sr. Marketing Event Manager for Global Third-Party Programs at AWS, where she partners with industry marketing to deliver the highest visibility and most business-critical events for AWS. is Sr. Industry Marketing Manager at Amazon Web Services (AWS) where she leads strategic integrated marketing initiatives across the Media & Entertainment, Games, and Sports verticals to deliver marketing campaigns that connect AWS cloud solutions with customer opportunities.is the President and Co-founder at Wildlife, where he leads a team launching interactive experiences and content campaigns for global brands. His work has been recognized with the Titanium Grand Prix at the Cannes Lions International Festival of Creativity for “boundary-busting, envy-inspiring work that marks a new direction for the industry and moves it forward”. You can find him on LinkedIn.Wildlife fuses a digitally born skillset with a future proof mindset to deliver breakthrough products, experiences and campaigns for daring partners. We live by a motto: Technology changes, story doesn’t.]]></content:encoded></item><item><title>Tyson Foods elevates customer search experience with an AI-powered conversational assistant</title><link>https://aws.amazon.com/blogs/machine-learning/tyson-foods-elevates-customer-search-experience-with-an-ai-powered-conversational-assistant/</link><author>Anveshi Charuvaka</author><category>dev</category><category>ai</category><pubDate>Wed, 20 Aug 2025 21:44:28 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Tyson Foodservice operates as a critical division within Tyson Foods Inc., using its extensive protein production capabilities to supply a diverse array of foodservice clients across multiple sectors. As one of the largest protein providers in the US, Tyson Foods produces approximately 20% of the nation’s beef, pork, and chicken, which forms the foundation of its foodservice offerings.Tyson Foodservice operates through a B2B model, selling products to distributors rather than directly to end consumers, while serving diverse foodservice operators, including restaurants, schools, healthcare facilities, and convenience stores. Until recently, Tyson had limited direct engagement with over 1 million unattended operators who purchased their products through distributors without direct company relationships. To bridge this gap, Tyson has implemented a generative AI assistant on their website, enabling them to scale sales efforts, gather customer insights, and establish direct communication channels. The company’s website now functions as a critical interface where operators can explore products, access menu trends, and discover tailored solutions for their specific foodservice segments, all enhanced by AI-driven personalization that better serves both established customers and previously unattended operators.In this post, we explore how Tyson Foods collaborated with the AWS Generative AI Innovation Center to revolutionize their customer interaction through an intuitive AI assistant integrated into their website. The AI assistant was built using Amazon Bedrock, a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI.In this section, we describe the overall architecture of the solution. The workflow includes the following high-level steps:The user uses the AI assistant interface to ask questions in natural language. The query is processed by the agent node using Anthropic’s Claude 3.5 Sonnet on Amazon Bedrock. Depending on the subject of the query, the agent might orchestrate multiple agents to return relevant information to the user. The application is deployed using a similar architecture to the semantic search component with the addition of an Amazon Relational Database Service (Amazon RDS) database cluster to persist the user high-value actions for analytics purposes.Products, recipes, ingredients and other relevant data are available from external sources in JSON format. These are processed using Amazon Bedrock and the Amazon Titan Text Embeddings model to create semantic search embeddings. Then these are ingested into OpenSearch Serverless. The ingestion process run in a different ECS cluster using Fargate as the capacity provider.The following diagram illustrates this architecture.In the following sections, we discuss the solution’s key components and benefits in more detail.The earlier iteration of search on the Tyson Foodservice website relied on keyword-based search. Traditional keyword-based search on CPG websites like Tyson Foodservice often falters when customers search for products using industry terminology that varies from official catalog descriptions. Chefs searching for “pulled chicken” might miss relevant products labeled as “shredded chicken,” or those looking for “wings” might not see results for “party wings” or “drummettes.” This disconnect frustrates food service professionals who need specific ingredients under tight deadlines and ultimately drives them to competitors where they can more quickly find what they need, resulting in lost revenue opportunities for Tyson. Semantic search transforms this experience by understanding the conceptual relationships between culinary terms, preparation methods, and product applications. A chef searching for “buffalo-style appetizers” would receive results for wings, boneless bites, and similar products regardless of exact keyword matches. By recognizing menu trends, cooking techniques, and professional kitchen terminology, semantic search helps foodservice operators quickly find the Tyson products that meet their exact operational needs, even when using language that differs from catalog descriptions.Tyson Foodservice implemented their semantic search capability using OpenSearch Serverless, a fully managed service that minimized the operational complexity of maintaining search infrastructure. This solution automatically scales compute and storage resources to match query volume and product catalog size without requiring dedicated administrative overhead. The serverless architecture helped Tyson rapidly deploy advanced natural language processing capabilities across their entire product database while maintaining cost-efficiency, because they only pay for the resources they actually use. With OpenSearch Serverless, Tyson incorporated vector embeddings and powerful query capabilities that understand foodservice terminology variations, preparation methods, and culinary applications, transforming how operators discover products that meet their specific needs even when their search terms don’t exactly match catalog descriptions.For indexing Tyson’s diverse content library of products, recipes, and articles, we implemented a preprocessing workflow that transforms raw metadata into optimized semantic search queries. We used large language models (LLMs) to analyze and extract only the most relevant elements from each content piece, creating meaningful search strings specifically designed for semantic indexing. This approach made sure that purely presentational website copy and non-essential informational text were filtered out, and search-critical elements like culinary applications, preparation methods, and ingredient specifications received proper emphasis in the index. By curating what content gets indexed rather than including everything verbatim, we dramatically improved search relevance while reducing index bloat, so OpenSearch Serverless delivered more precise results that truly match the intent behind chef and operator queries. For indexing the text as semantic vectors, we used Amazon Titan Text Embeddings V2 on Amazon Bedrock.The following example prompt illustrates the transformation using only the title, description, and reasons to buy metadata. This generic strategy can be customized according to the customer’s specific needs.SEARCH_STRING_PROMPT = """ Given a product title, description, and reasons to
buy, create a single, concise search string suitable for indexing in a vector
database. This string should focus on distinguishing features, assuming all
products are for foodservice operators unless explicitly stated otherwise.
Enclose the generated search string within <search_string> XML tags. 

Follow these guidelines:
1. Start with the brand name and product line (if applicable).
2. Include the main product type and specific identifying features.
3. List concrete attributes such as preparation state, packaging, or quantity.
4. Mention specific varieties or assortments included in the product.
5. Incorporate key points from the reasons to buy, focusing on unique and
   specific selling points.
6. Avoid generic terms or those common to all products in the category (e.g.,
   "food service", "restaurant", "operator").
7. Omit cliché marketing terms (e.g., "versatile", "high-quality", "innovative")
   unless they have a specific, demonstrable meaning in the context of the
   product.
8. Use precise descriptors that differentiate the product from others in its
   category.
9. Omit articles (a, an, the) and unnecessary connecting words.
10. Use lowercase for all terms except proper nouns.
11. Separate terms with single spaces.
12. Aim for a length of 15-20 words.
13. Prioritize terms that potential buyers are most likely to use in specific
    search queries.
    
Example input:
<title>Tyson® Heritage Valley™ IF Unbreaded 8 Piece Cut Chicken</title>
<description>Order a variety of crispy, seasoned chicken cuts with 
Heritage Valley™ Uncooked, Ice Glazed 8 Piece Cut Chicken. Featuring an 
assortment of breasts, drumsticks, thighs and wings, our chicken portions 
are completely customizable and perfect for center-of-plate features. 
Separately packaged for quick and easy preparation and portion control, 
our packaging helps your staff reduce waste by allowing them to use what 
they need, when they need. Ready to cook from frozen, simply fry and 
serve as an assortment for a buffet protein choice.
</description>
<reasons_to_buy>
['Bone-in assortment of breasts, drumsticks, thighs and wings.', 
'Individually quick frozen, locking in natural juices and tenderness.', 
'Different cuts separately bagged for quick and easy preparation and cleanup.', 
'Ready to cook from frozen.']
</reasons_to_buy>

Example output: <search_string>tyson heritage valley unbreaded raw 8-piece
chicken bone-in breasts drumsticks thighs wings individually-frozen
separate-bags cook-from-frozen juicy center-of-plate</search_string>

Now, create a similar search string for the following product:
<title>{title}</title>
<description>{description}</description>
<reasons_to_buy>{reasons_to_buy}</reasons_to_buy>
"""
Agentic chat built using Anthropic’s Claude 3.5 Sonnet on Amazon Bedrock and LangGraphTyson Foodservice has integrated a powerful generative AI assistant into their website, using Anthropic’s Claude 3.5 Sonnet on Amazon Bedrock and LangGraph. This AI assistant delivers a seamless conversational search experience that offers comprehensive support across Tyson’s extensive range of products, recipes, and articles, providing contextual guidance through natural conversation. Its capabilities include: – Uses semantic search to find relevant products, recipes, and articles. The AI assistant customizes recommendations by learning about the user’s business and role, creating a tailored experience while gathering valuable customer insights for Tyson.Detailed product information – Provides comprehensive details about specific Tyson products, including descriptions, ingredients, preparation methods, and suggested applications. – Helps users locate nearby distributors and check product availability in their area. – Offers information on how to buy Tyson products and connects customers with sales representatives when needed. – Keeps customers informed about current Tyson Foodservice promotions and special offers. – Provides a streamlined way for customers to submit product and service feedback directly to Tyson.Natural conversational flow – Maintains context throughout the interaction, allowing users to reference previous results and ask follow-up questions for a more human-like conversation experience.The following diagram illustrates the high-level architecture of the AI assistant. The system uses the tool calling capabilities of Anthropic’s Claude to implement the AI assistant’s agentic behavior. We used LangGraph to streamline the implementation process, because it provides several convenient primitives specifically designed for building agentic systems with LLMs.The main components of the architecture are: – The agent node is implemented using a large prompt that directly receives the user message and responds using the conversational capabilities of the LLM. It also defines the agentic behavior by using the tool calling capability: whenever serving the user’s request requires calling a tool, the agent node issues a tool request.– This node implements a generic tool executor that connects to various tools. Whenever a tool call is issued by the agent node, this node handles the execution of the tool call. The tool calling node executes the tools, which are defined as Python functions, and returns the results to the agent node to be transformed or summarized and presented to the user. LangGraph provides a generic implementation of the ToolNode that can also be extended to implement additional functionality.– Tools are implemented as simple programmatic functions that take inputs and return outputs. These tools augment the capabilities of LLMs by performing functions like retrieving data or submitting feedback. The tools are stateless and agnostic to the current conversation between the user and agent. The LLM agent extracts the input parameters required to execute these tools. These tools in our implementation are a thin wrapper around the services and database layer that implement the actual functionality.The following system prompt provides a general guidance for implementing the agent node:import date

AGENT_SYSTEM_PROMPT = """
# Tyson Foodservice (TFS) Customer Support Assistant

## Core Role and Purpose
You are a helpful customer support assistant for Tyson Foodservice a.k.a TFS
hosted on their https://www.tysonfoodservice.com/ website.  You will be helpful
and answer the customers questions. The customers are mainly interested in
learning about the products for their specific needs.
Refrain from engaging in any conversation unrelated to tyson food search of
products, recipes or distributors. If the user asks any unrelated questions the
politely decline and mention your purpose. Do not provide and additional
information or advice.
 
Your job is to stay factual and only provide relevant information from the
current context or retrieved using the tools. Do not offer your own suggestions.
Customers are looking for concrete information that is available in the Tyson
Foodservice database.

## About Tyson Foodservice
Tyson Foods is a major American multinational corporation and one of the world's
largest processors and marketers of chicken, beef, and pork.

### Distributors
Tyson foods mainly sells their products through distributors and does not sell
them directly. Each distributor is identified by a unique identifier named
distributor_id which is used as parameters for the tools, do not use the
distributor name as query parameter.

### Foodservice Operators
Foodservice Operators, or simply Operators, are Tyson Foods' primary customers.
These encompass diverse businesses in the foodservice sector, each with unique
needs. Understanding the distinct personas of various Operator types is crucial
for Tyson Foods to:
- Tailor product offerings effectively
- Develop targeted marketing strategies
- Create relevant recipe suggestions
- Address specific operational challenges
By analyzing different Operator segments (e.g., quick-service restaurants, fine
dining, educational institutions, healthcare facilities), Tyson Foods can
customize its products, offer innovative menu solutions, and provide value-added
services. This approach positions Tyson Foods as a strategic partner, driving
growth and maintaining competitiveness in the foodservice industry.

## Using Tools
You will be provide a variety of tools to perform your job, use them wisely and
ask the customer for relevant information that they have not provided. E.g. if
the search tool requires persona and the customer has not provided it then ask
the customer.
- Do not explicitly declare the tools to the users as the users are not aware of
  the internal workings of the tools.
- Do not try to intrepret the results of the search tool and show them as it is
  to the user.
- Operators may have their preferred distributor they buy from so let them
  confirm or select their distributor before checking for availability of
  products.
- Customers might sometimes search for things that are not available in tyson
  food catalog. If the search did not produce any results then just inform the
  user and do not suggest any external sources.
- When trying to determine the parameters for a tool, do not infer them from
  other parameters. E.g. do not infer the User's name from their email.
  Explicitly ask for the name.
- If the users complain or praise the chatbot then you can ask for their
  feedback in the chatbot and use the `submit_feedback` tool to submit the
  feedback. Ask the user to provide the relevant contact information.

## Product, Recipes, and Articles Search
Search functionality is a critical tool on Tyson's website, allowing users to
find products, recipes, and articles. It enables searches across three main
entity types:
- **Products**: The core offerings of Tyson Foods. These are identified by a
  unique GTIN (Global Trade Item Number).
- **Recipes**: Culinary ideas provided by Tyson Foods to encourage product use.
  Each recipe incorporates one or more Tyson products.
- **Articles**: Informative content on various topics, created by Tyson Foods
  for their customers.
- Do not provide any items or suggestions outside of the ones that are found
  through search.
- When the user asks to for details or a product or compare two or more
  products, retrieve the details of the products first using the tools to get
  product details.
- While users of the site are mainly looking for products, they might also be
interested in recipes and articles so it's important to not omit them when
displaying the search results.

### User Profile or Persona
In order to serve the user's better, the search tool can accept the user's
persona as an input. User profile or persona is a concise description of the
type of role that a user performs in the foodservice industry. A few examples
of persona are
- Restaurant owners looking to optimize costs
- Chef looking for unique ingredients
- K12 operators looking for healthy menu items
They can also be simple roles if the user has not provided any additional
information. Examples are
- Restaurant owner
- Chef
- Hotel Manager
The user persona should not include the search query that they are using for
finding products E.g. these are not good personas
- Restaurant owner looking for chicken nuggets
The above is not a good persona because it includes the product

### Search query string
Search queries should be simple and specific to the products or recipes and
should not contain the operator information
Here are some examples: 
- Instead of "healthy chicken wings for K12" use "chicken wings"
- Instead of "mexican beef patties for Deli operation" use "mexican beef
  patties"

### Product Results Display 
When listing the product results, always display them in the following format as
a numbered list. This will be displayed in the UI using markdown. 
1. **Title**
- GTIN
- description - This is a brief description
- [Product Page](Product url link)

### Recipes Results Display
When displaying recipes. Display the following
1. **Title**
- description - This is a brief description
- [Recipe Page](Recipe url link)

## Contact or provide feedback
- If the users want to reach out to Tyson foods team then they can use the form
  using this link [Contact
  Us](https://www.tysonfoodservice.com/connect/contact-us) 
- Users can submit their feedback using the chatbot using tools. When submitting
  feedback to Tyson extract user's message verbatim and do not rephrase it.

## How to buy
If the user wants to buy a product then they have two options. 
1. through distributor (preferred option)
2. reaching out to tysons sales representative by filling a form
If the user has not already indicated their preference then present these two
options. 
When the user asks for ordering information you do not need to retrieve all the
product details again, only specify the title of the product and be concise with
the details.

### Order through distributor
If they user is interested in buying through a distributor then let them
identify their preferred distributor and then for a specific product or products
they have identified provide the ordering link obtained through the user of
appropriate tool. Also help them check if a product is available with their
distributor.

### Find a tyson Sales Rep
If the user is not interested in a purchasing through a distributor then direct
them to submit a form through this link which will submit their information to a
sales team and someone will reach out to them. Here is the link to the form
https://www.tysonfoodservice.com/connect/find-a-sales-rep 

Current date (YYYY-MM-DD): """ + date.today().strftime("%Y-%m-%d") + "\n" 
Capturing high-value actions: Turning conversations into insightsIn designing Tyson Foodservice’s AI assistant, we implemented an innovative solution for capturing high-value actions that transforms customer interactions into strategic business intelligence. This capability provides deeper contextual understanding of customer interests and needs than traditional web analytics. Whereas conventional analytics tools track user behavior through page views, clicks, and time-on-site metrics, our solution uses the rich conversational data generated through natural dialogue. This provides Tyson with unprecedented visibility into customer interests, pain points, and purchase intentions.The system identifies and logs specific high-value interactions whenever users request detailed product information, inquire about specific product categories, ask about preparation methods or recipe ideas, seek distributor information in their region, or express interest in bulk purchasing or promotions. This approach creates a powerful feedback loop for Tyson Foodservice. As customers naturally express their needs and interests through conversation, the system captures these signals in an aggregate, privacy-respecting manner. Tyson can use these insights to identify trending product categories and potential gaps in their portfolio, understand regional variations in customer interests, recognize seasonal patterns in product inquiries, refine marketing strategies based on direct customer language, and improve inventory management through better demand forecasting. The technical implementation uses the tool-calling capabilities of Anthropic’s Claude 3.5 Sonnet in a straightforward but effective way. Rather than analyzing chat logs after the fact, we integrated the capture mechanism directly into the AI assistant’s operational workflow through LangGraph, allowing for real-time insight collection during customer interactions. When the LLM invokes certain tools to retrieve information requested by users, these tool calls simultaneously trigger the capture of high-value action data. We’ve designed a configurable system where specific tools are designated as high-value action triggers that record meaningful interactions while fulfilling the user’s immediate request.This dual-purpose approach makes sure that valuable business intelligence is gathered as a natural byproduct of providing excellent customer service, without requiring additional processing or analysis steps. The system includes configurable parameters that allow Tyson to adjust which user intents and actions qualify as high value based on evolving business priorities. By transforming every customer conversation into structured, actionable data, Tyson Foodservice can now measure customer interest with unprecedented precision while delivering a superior search experience that feels natural to users.In this post, we demonstrated a powerful approach to implementing natural conversational AI assistants that seamlessly integrate with existing website functionalities and provide intuitive language interactions for users. By using Amazon Bedrock FMs and OpenSearch Serverless, businesses can quickly expose their website’s capabilities through conversation rather than complex interfaces. The high-value action capture mechanism further enhances this solution by gathering valuable customer insights directly from natural interactions, creating a rich source of business intelligence without additional user friction. This framework provides a flexible blueprint for implementing AI-powered assistants across retail and CPG websites. Organizations can adapt this approach to their specific needs, such as product discovery, customer support, or personalized recommendations. The combination of semantic search with conversational AI creates experiences that understand user intent while maintaining the context necessary for natural dialogue.If you’re interested in building a similar AI assistant that orchestrates multiple tools, you can get started with Amazon Bedrock Agents, a fully managed AWS solution designed specifically for this purpose. Amazon Bedrock Agents simplifies the process of creating, testing, and deploying conversational experiences that can execute complex tasks across your business systems. With the right architecture and implementation approach demonstrated in this post, you can develop AI-powered interactions that deliver measurable business value while significantly enhancing your customer journey.For developers exploring AI agent frameworks today, AWS recently introduced Strands Agents, an open source SDK that takes a model-driven approach to building agents with just a model, tools, and a prompt. Unlike workflow-based frameworks, Strands adopts a model-first philosophy that uses advanced reasoning capabilities, offering an interesting alternative approach to frameworks like LangGraph.Try out these solutions for your own use case, and share your feedback in the comments.is a Senior Applied Scientist at AWS’s Generative AI Innovation Center, where he partners with customers to turn Generative AI into solutions for mission-critical business problems. He holds a PhD in Machine Learning and brings over 10 years of experience applying innovative ML and GenAI techniques to complex, real-world challenges. leads the Digital Enterprise Organization at Tyson Foods, where he spearheads progress in emerging technologies, artificial intelligence, and Smart Office initiatives. With more than 17 years of expertise in software development, data, analytics, and AI, Barret excels at leveraging innovative technology paradigms, including Agentic AI, to tackle and enhance complex business processes. is a Senior Deep Learning Architect in the Generative AI Innovation Center. Vincil has 25 years of experience in the IT industry and holds a PhD in Systems Engineering from Colorado State University. Vincil specializes in the design and implementation of AI solutions that help solve customers’ toughest business challenges. is an Applied Scientist at the AWS Generative AI Innovation Center, where he leads projects and collaborates with enterprise customers across various industries to leverage cutting-edge generative AI technologies in solving complex business challenges. He specializes in identifying and prioritizing high-impact use cases, developing scalable AI solutions, and fostering knowledge-sharing partnerships with stakeholders. is a Data Scientist at Generative AI Innovation Center at Amazon Web Services who helps customers solve their business problems using generative AI and machine learning. He has done MS with Thesis in Machine Learning from University of Illinois and has extensive experience in solving customer problem in the field of data science. is a Principal Solutions Architect at AWS with 15+ years of IT experience across the Financial Services, Retail, and Consumer Packaged Goods sectors. Angel specializes in utilizing cloud technology to impact business KPIs, with particular expertise in multicloud strategies, SAP migrations, and supply chain improvement.]]></content:encoded></item><item><title>Show HN: PlutoPrint – Generate PDFs and PNGs from HTML with Python</title><link>https://github.com/plutoprint/plutoprint</link><author>sammycage</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 20:37:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hi everyone, I built PlutoPrint because I needed a simple way to generate beautiful PDFs and images directly from HTML with Python. Most of the tools I tried felt heavy, tricky to set up, or produced results that didn’t look great, so I wanted something lightweight, modern, and fast. PlutoPrint is built on top of PlutoBook’s rendering engine, which is designed for paged media, and then wrapped with a Python API that makes it easy to turn HTML or XML into crisp PDFs and PNGs. I’ve used it for things like invoices, reports, tickets, and even snapshots, and it can also integrate with Matplotlib to render charts directly into documents.I’d be glad to hear what you think. If you’ve ever had to wrestle with generating PDFs or images from HTML, I hope this feels like a smoother option. Feedback, ideas, or even just impressions are all very welcome, and I’d love to learn how PlutoPrint could be more useful for you.]]></content:encoded></item><item><title>Enhance AI agents using predictive ML models with Amazon SageMaker AI and Model Context Protocol (MCP)</title><link>https://aws.amazon.com/blogs/machine-learning/enhance-ai-agents-using-predictive-ml-models-with-amazon-sagemaker-ai-and-model-context-protocol-mcp/</link><author>Saptarshi Banerjee</author><category>dev</category><category>ai</category><pubDate>Wed, 20 Aug 2025 20:26:08 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Machine learning (ML) has evolved from an experimental phase to becoming an integral part of business operations. Organizations now actively deploy ML models for precise sales forecasting, customer segmentation, and churn prediction. While traditional ML continues to transform business processes, generative AI has emerged as a revolutionary force, introducing powerful and accessible tools that reshape customer experiences.Despite generative AI’s prominence, traditional ML solutions remain essential for specific predictive tasks. Sales forecasting, which depends on historical data and trend analysis, is most effectively handled by established ML algorithms including random forests, gradient boosting machines (like XGBoost), autoregressive integrated moving average (ARIMA) models, long short-term memory (LSTM) networks, and linear regression techniques. Traditional ML models, such as K-means and hierarchical clustering, also excel in customer segmentation and churn prediction applications. Although generative AI demonstrates exceptional capabilities in creative tasks such as content generation, product design, and personalized customer interactions, traditional ML models maintain their superiority in data-driven predictions. Organizations can achieve optimal results by using both approaches together, creating solutions that deliver accurate predictions while maintaining cost efficiency.To achieve this, we showcase in this post how customers can expand AI agents’ capabilities by integrating predictive ML models and Model Context Protocol (MCP)—an open protocol that standardizes how applications provide context to large language models (LLMs)—on Amazon SageMaker AI. We demonstrate a comprehensive workflow that enables AI agents to make data-driven business decisions by using ML models hosted SageMaker. Through the use of Strands Agents SDK—an open source SDK that takes a model-driven approach to building and running AI agents in only a few lines of code—and flexible integration options, including direct endpoint access and MCP, we show you how to build intelligent, scalable AI applications that combine the power of conversational AI with predictive analytics.This solution enhances AI agents by having ML models deployed on Amazon SageMaker AI endpoints integrate with AI Agents, to enable them to make data-driven business decisions through ML predictions. An AI agent is an LLM-powered application that uses an LLM as its core “brain” to autonomously observe its environment, plan actions, and execute tasks with minimal human input. It integrates reasoning, memory, and tool use to perform complex, multistep problem-solving by dynamically creating and revising plans, interacting with external systems, and learning from past interactions to optimize outcomes over time. This enables AI agents to go beyond simple text generation, acting as independent entities capable of decision-making and goal-directed actions in diverse real-world and enterprise scenarios.For this solution, the AI agent is developed using the Strands Agents SDK, which allows for rapid development from simple assistants to complex workflows. Predictive ML models are hosted on Amazon SageMaker AI and will be used as tools by the AI agent. This can happen in two ways: agents can directly invoke SageMaker endpoints for more direct access to model inference capabilities or use the MCP protocol to facilitate the interaction between AI agents and the ML models. Both options are valid: direct tool invocation doesn’t require additional infrastructure by embedding the tool calling directly in the agent code itself, whereas MCP enables dynamic discovery of the tools and decoupling of agent and tool execution through the introduction of an additional architectural component, the MCP server itself. For scalable and secure implementation of the tool calling logic, we recommend the MCP approach. Although we’re recommending MCP, we discuss and implement the direct endpoint access as well, to give readers the freedom to choose the approach that they prefer.Amazon SageMaker AI offers two methods to host multiple models behind a single endpoint: inference components and multi-model endpoints. This consolidated hosting approach enables efficient deployment of multiple models in one environment, which optimizes computing resources and minimizes response times for model predictions. For demonstration purposes, this post deploys only one model on one endpoint. If you want to learn more about inference components, refer to the Amazon SageMaker AI documentation Shared resource utilization with multiple models. To learn more about multi-model endpoints, refer to the Amazon SageMaker AI documentation Multi-model endpoints.In this post, we define a workflow for empowering AI agents to make data-driven business decisions by invoking predictive ML models using Amazon SageMaker AI. The process begins with a user interacting through an interface, such as a chat-based assistant or application. This input is managed by an AI agent developed using the open source Strands Agents SDK. Strands Agents adopts a model-driven approach, which means developers define agents with only a prompt and a list of tools, facilitating rapid development from simple assistants to complex autonomous workflows.When the agent is prompted with a request that requires a prediction (for example, “what will be the sales for H2 2025?”), the LLM powering the agent decided to interact with the Amazon SageMaker AI endpoint hosting the ML model. This can happen in two ways: directly using the endpoint as a custom tool of the Strands Agents Python SDK or by calling the tool through MCP. With MCP, the client application can discover the tools exposed by the MCP server, obtain the list of required parameters, and format the request to the Amazon SageMaker inference endpoint. Alternatively, agents can directly invoke SageMaker endpoints using tool annotations (such as ), bypassing the MCP server for more direct access to model inference capabilities.Finally, the prediction generated by the SageMaker hosted model is routed back through the agent and ultimately delivered to the user interface, enabling real-time, intelligent responses.The following diagram illustrates this process. The complete code for this solution is available on GitHub.To get started with this solution, make sure you have:In this solution, we implement a complete workflow that demonstrates how to use ML models deployed on Amazon SageMaker AI as specialized tools for AI agents. This approach enables agents to access and use ML capabilities for enhanced decision-making without requiring deep ML expertise. We play the role of a data scientist tasked with building an agent that can predict demand for one product. To achieve this, we train a time-series forecasting model, deploy it, and expose it to an AI agent.The first phase involves training a model using Amazon SageMaker AI. This begins with preparing training data by generating synthetic time series data that incorporates trend, seasonality, and noise components to simulate realistic demand patterns. Following data preparation, feature engineering is performed to extract relevant features from the time series data, including temporal features such as day of week, month, and quarter to effectively capture seasonality patterns. In our example, we train an XGBoost model using the XGBoost container available as 1P container in Amazon SageMaker AI to create a regression model capable of predicting future demand values based on historical patterns. Although we use XGBoost for this example because it’s a well-known model used in many use cases, you can use your preferred container and model, according to the problem you’re trying to solve. For the sake of this post, we won’t detail an end-to-end example of training a model using XGBoost. To learn more about this, we suggest checking out the documentation Use XGBoost with the SageMaker Python SDK. Use the following code:from sagemaker.xgboost.estimator import XGBoost

xgb_estimator = XGBoost(...)
xgb_estimator.fit({'train': train_s3_path, 'validation': val_s3_path})Then, the trained model is packaged and deployed to a SageMaker AI endpoint, making it accessible for real-time inference through API calls:predictor = xgb_estimator.deploy(
    initial_instance_count=1,
    instance_type=instance_type,
    serializer=JSONSerializer(),
    deserializer=JSONDeserializer()
)After the model is deployed and ready for inferences, you need to learn how to invoke the endpoint. To invoke the endpoint, write a function like this:ENDPOINT_NAME = "serverless-xgboost"
REGION = boto3.session.Session().region_name

def invoke_endpoint(payload: list):
    """
        Use the model deployed on the Amazon SageMaker AI endpoint to generate predictions.
        Args:
            payload: a list of lists containing the inputs to generate predictions from
        Returns:
            predictions: an NumPy array of predictions
    """
    sagemaker_runtime = boto3.client("sagemaker-runtime", region_name=REGION)
    response = sagemaker_runtime.invoke_endpoint(
        EndpointName=ENDPOINT_NAME,
        Body=json.dumps(payload),
        ContentType="application/json",
        Accept="application/json"
    )
    predictions = json.loads(response['Body'].read().decode("utf-8"))
    return np.array(predictions)Note that the function  has been written with proper docstring. This is key to making sure that it can be used as a tool by LLMs because the description is what allows them to choose the right tool for the right task. YOu can turn this function into a Strands Agents tool thanks to the  decorator:from strands import tool

@tool()
def invoke_endpoint(payload: list):
    ....And to use it, pass it to a Strands agent:from strands import Agent

agent = Agent(
    model="us.amazon.nova-pro-v1:0", 
    tools=[generate_prediction_with_sagemaker]
)

agent(
    "Invoke the endpoint with this input:\n\n"
    f"<input>{test_sample}</input>\n\n"
    "Provide the output in JSON format {'predictions':<predictions>}"
)As you run this code, you can confirm the output from the agent, which correctly identifies the need to call the tool and executes the function calling loop:<thinking> To fulfill the User's request, I need to invoke the Amazon SageMaker 
endpoint with the provided input data. The input is a list of lists, which is the 
required format for the 'generate_prediction_with_sagemaker' tool. I will use this 
tool to get the predictions. </thinking> 

Tool #1: generate_prediction_with_sagemaker The predictions from the Amazon SageMaker
endpoint are as follows: 
```json {  "predictions": [89.8525238, 52.51485062, 58.35247421, 62.79786301, 85.51475525] } ```As the agent receives the prediction result from the endpoint tool, it can then use this as an input for other processes. For example, the agent could write the code to create a plot based on these predictions and show it to the user in the conversational UX. It could send these values directly to business intelligence (BI) tools such as Amazon QuickSight or Tableau and automatically update enterprise resource planning (ERP) or customer relationship management (CRM) tools such as SAP or Salesforce.Connecting to the endpoint through MCPYou can further evolve this pattern by having an MCP server invoke the endpoint rather than the agent itself. This allows for the decoupling of agent and tool logic and an improved security pattern because the MCP server will be the one with the permission to invoke the endpoint. To achieve this, implement an MCP server using the FastMCP framework that wraps the SageMaker endpoint and exposes it as a tool with a well-defined interface. A tool schema must be specified that clearly defines the input parameters and return values for the tool, facilitating straightforward understanding and usage by AI agents. Writing the proper docstring when defining the function achieves this. Additionally, the server must be configured to handle authentication securely, allowing it to access the SageMaker endpoint using AWS credentials or AWS roles. In this example, we run the server on the same compute as the agent and use  as communication protocol. For production workloads, we recommend running the MCP server on its own AWS compute and using transport protocols based on HTTPS (for example, Streamable HTTP). If you want to learn how to deploy MCP servers in a serverless fashion, refer to this official AWS GitHub repository. Here’s an example MCP server:from mcp.server.fastmcp import FastMCP

mcp = FastMCP("SageMaker App")
ENDPOINT_NAME = os.environ["SAGEMAKER_ENDPOINT_NAME"]

@mcp.tool()
async def invoke_endpoint(payload: list):
    """ Use the model ... """
    [...]
    
if __name__ == "__main__":
    mcp.run(="stdio")Finally, integrate the ML model with the agent framework. This begins with setting up Strands Agents to establish communication with the MCP server and incorporate the ML model as a tool. A comprehensive workflow must be created to determine when and how the agent should use the ML model to enhance its capabilities. The implementation includes programming decision logic that enables the agent to make informed decisions based on the predictions received from the ML model. The phase concludes with testing and evaluation, where the end-to-end workflow is validated by having the agent generate predictions for test scenarios and take appropriate actions based on those predictions.from mcp import StdioServerParameters
from mcp.client.stdio import stdio_client
from strands.tools.mcp import MCPClient

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="python3",  # Executable
    args=["server.py"],  # Optional command line arguments
    env={"SAGEMAKER_ENDPOINT_NAME": "<your-endpoint-name>"}
)

# Create an agent with MCP tools
with stdio_mcp_client:
    # Get the tools from the MCP server
    tools = stdio_mcp_client.list_tools_sync()
    # Create an agent with these tools
    agent = Agent(model="us.amazon.nova-pro-v1:0", tools=tools)
    # Invoke the agent
    agent(
        "Invoke the endpoint with this input:\n\n"
        f"<input>{test_sample}</input>\n\n"
        "Provide the output in JSON format {'predictions':<predictions>}"
    )# SageMaker Python SDK
predictor.delete_model()
predictor.delete_endpoint()

# Alternatively, boto3
sagemaker_runtime.delete_endpoint(EndpointName=endpoint_name)In this post, we demonstrated how to enhance AI agents’ capabilities by integrating predictive ML models using Amazon SageMaker AI and the MCP. By using the open source Strands Agents SDK and the flexible deployment options of SageMaker AI, developers can create sophisticated AI applications that combine conversational AI with powerful predictive analytics capabilities. The solution we presented offers two integration paths: direct endpoint access through tool annotations and MCP-based integration, giving developers the flexibility to choose the most suitable approach for their specific use cases. Whether you’re building customer service chat assistants that need predictive capabilities or developing complex autonomous workflows, this architecture provides a secure, scalable, and modular foundation for your AI applications. As organizations continue to seek ways to make their AI agents more intelligent and data-driven, the combination of Amazon SageMaker AI, MCP, and the Strands Agents SDK offers a powerful solution for building the next generation of AI-powered applications.serves as a Senior Solutions Architect at AWS, collaborating closely with AWS Partners to design and architect mission-critical solutions. With a specialization in generative AI, AI/ML, serverless architecture, Next-Gen Developer Experience tools and cloud-based solutions, Saptarshi is dedicated to enhancing performance, innovation, scalability, and cost-efficiency for AWS Partners within the cloud ecosystem.is a Senior Worldwide Specialist Solutions Architect for Generative AI at AWS, where he empowers global enterprises to harness the transformative power of AI. Based in Europe but with a worldwide scope, Davide partners with organizations across industries to architect custom AI agents that solve complex business challenges using AWS ML stack. He is particularly passionate about democratizing AI technologies and enabling teams to build practical, scalable solutions that drive organizational transformation.]]></content:encoded></item><item><title>Building a Production-Ready Soft Delete System in Django (with Custom User Model)</title><link>https://dev.to/saveen_kumar_4e9c80304ebe/building-a-production-ready-soft-delete-system-in-django-with-custom-user-model-44dd</link><author>Saveen Kumar</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 20:00:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Soft delete sounds simple—until you're the one implementing it in a real-world, regulated application.In building a financial portfolio management system, we faced the not-so-fun challenge of handling user deletion without compromising data integrity or violating compliance rules. You can't just delete() a user when audit trails, tax records, and GDPR are watching.So, here's how we designed a clean, maintainable soft delete system using a custom Django User model.
Most finance or SaaS platforms need to:Retain user-related transactions for tax/audit purposesDisable login access cleanlyRestore accidentally deleted accountsAvoid cascading deletions of historical data
Using Django’s built-in User + separate UserProfile quickly turned into a nightmare: joins everywhere, edge cases all over the place, and no easy path to soft delete.So we followed Django’s best practice: own your User model from day one.
Here's a quick breakdown of the implementation:✅ Custom User model based on AbstractUser✅ Added is_deleted, deleted_at, deleted_by✅ Overrode the admin to support soft deletion & restoration✅ Used on_delete=models.PROTECT for critical models like Transaction✅ Queryset filters and indexes for is_deletedis_active=False prevents loginSoft deletes ≠ just hiding records — handle reversibility and auditingNever on_delete=CASCADE sensitive data like financial historyUse admin actions for bulk delete/restore and badge UI for status
Soft delete isn’t just for compliance. It protects you from:Breaking historical reportingGDPR data logic edge casesLimitations of Django’s default User model
Plus, migration from default User → custom User later is a huge pain. Better to do it upfront.💬 
Have you implemented soft delete in production? Found better patterns, or do you prefer packages like django-safedelete? Would love to hear your experience or suggestions for scaling this better.🧵 Or drop thoughts below 👇]]></content:encoded></item><item><title>My Most Valuable Lesson as an Aspiring Data Analyst</title><link>https://towardsdatascience.com/my-most-valuable-lesson-as-an-aspiring-data-analyst/</link><author>Benjamin Nweke</author><category>dev</category><category>ai</category><pubDate>Wed, 20 Aug 2025 19:42:44 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[What my internship taught me about the power of collaboration in data analysis.]]></content:encoded></item><item><title>Smarter Model Tuning: An AI Agent with LangGraph + Streamlit That Boosts ML Performance</title><link>https://towardsdatascience.com/smarter-model-tuning-an-ai-agent-with-langgraph-streamlit-that-boosts-ml-performance/</link><author>Gustavo Santos</author><category>dev</category><category>ai</category><pubDate>Wed, 20 Aug 2025 19:28:38 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Automating model tuning in Python with Gemini, LangGraph, and Streamlit for regression and classification improvements]]></content:encoded></item><item><title>“Where’s Marta?”: How We Removed Uncertainty From AI Reasoning</title><link>https://towardsdatascience.com/interactive-proofs-with-claude/</link><author>Jacopo Tagliabue</author><category>dev</category><category>ai</category><pubDate>Wed, 20 Aug 2025 18:58:28 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[A primer on overcoming LLM limitations with formal verification.]]></content:encoded></item><item><title>The Upstream Mentality: Why AI/ML Engineers Must Think Beyond the Model</title><link>https://towardsdatascience.com/the-upstream-mentality-why-ai-ml-engineers-must-think-beyond-the-model/</link><author>Yuval Gorchover</author><category>dev</category><category>ai</category><pubDate>Wed, 20 Aug 2025 18:45:17 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Your 3am production alert isn't a model problem—it's an upstream crisis in disguise]]></content:encoded></item><item><title>Day 6: When Protobuf Breaks Everything - Real Engineering in the Trenches</title><link>https://dev.to/clayroach/day-6-when-protobuf-breaks-everything-real-engineering-in-the-trenches-1eek</link><author>Clay Roach</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 20 Aug 2025 18:23:41 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[: Add real-time updates and bootstrap AI anomaly detection.: "Why are all my operations named 'protobuf-fallback-trace'?!"Welcome to Day 6 of building an AI-native observability platform in 30 days. Today was supposed to be about sexy features. Instead, it was about the unglamorous reality of systems engineering: making protobuf work correctly.
  
  
  The Problem That Changed Everything
I started the day confident. The OpenTelemetry demo was running, traces were flowing, the UI was displaying data. Time to add real-time updates, right?Then I looked closer at the trace details:Every. Single. Operation. Was named "protobuf-fallback-trace".
  
  
  Discovery #1: Gzip Was Being Ignored
The OpenTelemetry demo sends protobuf data with gzip compression. My middleware had "clever" conditional logic:The fix was embarrassingly simple:: Sometimes "clever" code is just complicated code. Unified handling often beats conditional logic.
  
  
  Discovery #2: Protobufjs vs ES Modules
Next challenge: parsing the actual protobuf data. The protobufjs library is CommonJS, but my project uses ES modules. This led to hours of:
  
  
  Discovery #3: Path Resolution Hell
Even with protobufjs loading, the OTLP protobuf definitions have imports that need custom resolution:
  
  
  The Nuclear Option: Enhanced Fallback Parsing
When the "proper" protobuf parsing kept failing, I built something unconventional - a raw protobuf parser that extracts data through pattern matching:Is this elegant? No. Does it work? .After 8 hours of protobuf wrestling:❌ All operations: "protobuf-fallback-trace"✅ Real operations: , ✅ 10+ real spans per trace✅ Authentic resource attributes and timing data
  
  
  1. Fallback Strategies Are Not DefeatBuilding a fallback parser wasn't giving up - it was ensuring the system works even when dependencies fail. In production, .
  
  
  2. Debug at the Lowest LevelI spent hours assuming the protobuf data was corrupt. Finally logging the raw buffer bytes revealed it was fine - the decompression was being skipped.
  
  
  3. Integration Points Are Where Systems BreakThe individual components all worked:✅ OpenTelemetry demo: sending valid data✅ Express server: receiving requests
✅ ClickHouse: storing dataThe failure was in the glue between them.
  
  
  4. Real Data Reveals Real ProblemsMock data would never have exposed this issue. Testing with the actual OpenTelemetry demo forced me to handle real-world complexity.Today didn't go according to plan, and that's  what building production systems is like. The glossy demo videos don't show the 8 hours spent debugging why protobuf.load is not a function.But here's what matters: the system now correctly processes thousands of real traces from a production-like demo application. Every service is visible, every operation is named correctly, and the data flowing through the pipeline is authentic.Now that protobuf parsing actually works:Implement the real-time updates (for real this time)Add WebSocket support for live trace streamingBootstrap the AI anomaly detection systemCreate service dependency visualization
  
  
  Code Snippets That Saved the Day
For anyone fighting similar battles:
docker compose backend xxd  100 /tmp/trace.pb


curl  POST http://localhost:4319/v1/traces  @trace.pb.gz


node Day 6 was humbling. The plan was to build flashy features. Instead, I spent the day in the trenches making basic data ingestion work correctly. But that's real engineering. It's not always about the elegant algorithm or the clever architecture. Sometimes it's about making protobuf parsing work at 2 AM because your entire platform depends on it.The platform is stronger because of today's battles. And tomorrow, with real data flowing correctly, we can build the features that actually matter.Are you fighting your own protobuf battles? Share your war stories in the comments. Sometimes knowing you're not alone in the debugging trenches makes all the difference.Progress: Day 6 of 30 ✅ | Protobuf: Finally Working | Sanity: Questionable]]></content:encoded></item><item><title>Creating Stock Data | building stocksimpy 3</title><link>https://dev.to/suleyman_sade/creating-stock-data-building-stocksimpy-3-28dg</link><author>Suleyman Sade</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 17:55:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[StockSimPy is a lightweight Python library for simple stock backtesting. The goal is to understand Pandas, experiment with stock strategies better, and create an easy-to-use alternative to more complex backtesting tools. This is part 3 of the series where I build this library in public.After finishing basic indicator calculation functions, I needed a way to keep track of all the stock information in an organised, reusable format. That’s where the  comes in — it acts as a container for everything you’ll need in backtesting or simulation.I initially thought it should be easy to code as it just needed to keep the information and require some simple import and export, but I was quite wrong. Turns out working with data can be messy.When importing stock data, you can’t assume the columns are always consistent. Strategies require the use of different features, but some fields are essential:The tricky path — though — is naming conventions. What do I mean? Let's take “Open” as an example; it could show up as “OPEN”, “open”, “OpeN”, “open_price”, “OpenPrice”, “openPrice”, and many other wild naming styles.Lowercasing handles some cases, but what about the ones with “price” in the name? Then I thought — I could easily search for the substring “open” in the whole word. This covers all the cases I mentioned above, but if open is named something else entirely, it wouldn’t work.A more comprehensive approach might be to create a full-blown synonym-matching system. But that might be overkill for now. Still, I might add it as a feature in the future if somebody requests it.The most important feature of  is importing data—without that, it’s just an empty shell.I was quite skeptical about creating these import functions at first. I considered leaving import up to the user — just pass in a Pandas DataFrame — but having built-in loaders felt more convenient. So far,  supports imports from:(This process felt quite  as I was just using built-in pandas functions or just straight-up copying documentation.)To simplify things, I added an function that picks the correct import based on the file extension of  parameter. I used  so users can pass in additional parameters.On top of that,  integrates directly with  (optional dependency). This allows fetching live stock data for a given ticker and date range, making it much more practical.For testing purposes, there’s also a  function. It isn’t designed for real backtesting but is useful for experimenting with new features.Here is a question: why export data you already imported? Two reasons: Users might want to inspect or clean their data after transformations. I will soon integrate the indicator functions from earlier posts, with  so exporting results will be handy.Export currently supports all the same formats mentioned in import, plus SQL. There is also a flexible  function that lets you define your own export method.It was such a twist, this step turned out to be more about data flexibility rather than really "storing data." With StockData in place, stocksimpy now has a solid foundation for testing.If you want to use this library in the future, or have any ideas that I could add, go for it. Ask me in comments, connect with me on socials. I want to make this project something useful.Follow the rest of the series, watch me build in public.]]></content:encoded></item><item><title>A series that is hype free, optimistic and cautious, but most of all written accessibly no matter your current level. All things dev&apos;s should understand about #ai fast. Thanks Dev. This should be a book &amp; course next. @dev_patel_35864ca1db6093c</title><link>https://dev.to/leogopal/a-series-that-is-hype-free-optimistic-and-cautious-but-most-of-all-written-accessibly-no-matter-1mi5</link><author>Leo Gopal</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 16:52:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Decoding the Secrets of Your Machine Learning Model: Confusion Matrices, ROC Curves, and AUC]]></content:encoded></item><item><title>Show HN: Anchor Relay – A faster, easier way to get Let&apos;s Encrypt certificates</title><link>https://anchor.dev/relay</link><author>geemus</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 16:13:18 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Luminal – Open-source, search-based GPU compiler</title><link>https://github.com/luminal-ai/luminal</link><author>jafioti</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 16:01:13 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hi HN, I’m Joe. My friends Matthew, Jake and I are building Luminal (https://luminalai.com/), a GPU compiler for automatically generating fast GPU kernels for AI models. It uses search-based compilation to achieve high performance.We take high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here’s a video showing how: https://youtu.be/P2oNR8zxSAAOur approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.We’re working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.We aim to radically simplify the ML ecosystem while improving performance and hardware utilization. Please check out our repo: https://github.com/luminal-ai/luminal and I’d love to hear your thoughts!]]></content:encoded></item><item><title>Election Management System (EMS) – Secure Web-Based Digital Voting Platform</title><link>https://dev.to/abubakar_shabbir/election-management-system-ems-secure-web-based-digital-voting-platform-228a</link><author>AbuBakar Shabbir</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 15:51:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I, , built the Election Management System (EMS), a modern, secure, and user-friendly digital voting web application using Python, Django, MySQL, and Bootstrap. This platform provides a transparent and efficient way to manage elections, handle voter registration, and monitor results in real-time. Register voters and allow secure logins with OTP verification.
 Email OTP ensures only verified voters can access the system.
 Separate dashboards for Admins, Voters, and Candidates.
 Add and manage candidates, control elections, monitor voters, and view real-time results.
 One vote per voter linked to a unique CNIC, preventing duplicate voting.
Real-Time Election Results: Display results by constituency and party for transparency.
 Can run locally or on a live server with MySQL backend.
 Bootstrap, HTML, CSS
 OTP via Gmail SMTP
This project is ideal for secure election management for educational institutions, organizations, or local communities. It emphasizes security, transparency, and user experience, making voting easier and tamper-proof.The Voter Panel displays only the elections that have been created and approved by the admin. Each voter can view the elections they are eligible for and cast their vote securely within the specified election. This ensures role-specific access and prevents any unauthorized voting.This project was developed by , focusing on secure web applications and modern software engineering practices.]]></content:encoded></item><item><title>Send Email using aws-sdk-v2.sesv2 on golang</title><link>https://dev.to/adityaokke/send-email-using-aws-v2sesv2-on-golang-4dfb</link><author>ADITYA OKKE SUGIARSO</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 20 Aug 2025 15:41:20 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[1. initiate ses service on aws
choose your region on aws2. create identities to use sandbox feature from aws sesclick create identity buttonfill form
create another identity for 
and you should get 
finally you just need to verify your email by click link verification on the email inbox$ mkdir ~/helloaws
$ cd ~/helloaws
$ go mod init helloaws
$ go get github.com/aws/aws-sdk-go-v2/aws
$ go get github.com/aws/aws-sdk-go-v2/config
$ go get github.com/aws/aws-sdk-go-v2/service/sesv2
package main

import (
    "context"
    "fmt"
    "log"

    "github.com/aws/aws-sdk-go-v2/aws"
    "github.com/aws/aws-sdk-go-v2/config"
    "github.com/aws/aws-sdk-go-v2/service/sesv2"
    "github.com/aws/aws-sdk-go-v2/service/sesv2/types"
)

func main() {
    // Using the SDK's default configuration, load additional config
    // and credentials values from the environment variables, shared
    // credentials, and shared configuration files
    cfg, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion("ap-southeast-1"))
    if err != nil {
        log.Fatalf("unable to load SDK config, %v", err)
    }

    // Build the request with its input parameters
    resp, err := svc.SendEmail(context.TODO(), &sesv2.SendEmailInput{
        FromEmailAddress: aws.String("admin@gmail.com"),
        Destination: &types.Destination{
            ToAddresses: []string{"user@gmail.com"},
        },
        Content: &types.EmailContent{
            Simple: &types.Message{
                Subject: &types.Content{
                    Data: aws.String("Test Email"),
                },
                Body: &types.Body{
                    Text: &types.Content{
                        Data: aws.String("This is a test email sent using AWS SES."),
                    },
                },
            },
        },
    })
    if err != nil {
        fmt.Printf("Error sending email: %v\n", err)
    }

    fmt.Printf("Email sent successfully, message ID: %s\n", *resp.MessageId)
}

]]></content:encoded></item><item><title>Creating IAM Access Keys for AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY</title><link>https://dev.to/adityaokke/creating-iam-access-keys-for-awsaccesskeyid-and-awssecretaccesskey-34i3</link><author>ADITYA OKKE SUGIARSO</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 20 Aug 2025 15:33:28 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[IAM user access keys consist of two parts:Access key ID (for example: AKIAIOSFODNN7EXAMPLE)Secret access key (for example: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY)You must use both the access key ID and the secret access key together to authenticate requests made through the AWS SDK. Open the IAM Dashboard in the AWS Management Console. In the left navigation pane, choose Users.on set permissions, create group to attach the policiesSet a group name and choose permission policies. 
These policies usually provide full access per AWS service. If you need more fine-grained control, you can create your own custom policies by selecting the Create policy button.after that, review and select Create User buttonchoose Create access keyfill any meaningful name then choose create keyif sucess, you will have  and  to put on .envnow you can put both key on .env. AWS SDK will automatically detect the key on .env
AWS_ACCESS_KEY_ID=AKIAZF************
AWS_SECRET_ACCESS_KEY=utiKWhMNy***********************************
]]></content:encoded></item><item><title>Show HN: What country you would hit if you went straight where you&apos;re pointing</title><link>https://apps.apple.com/us/app/leascope/id6608979884</link><author>brgross</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 15:23:01 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[
    The developer, , indicated that the app’s privacy practices may include handling of data as described below. For more information, see the developer’s privacy policy.
  The developer does not collect any data from this app.Privacy practices may vary, for example, based on the features you use or your age. Learn More]]></content:encoded></item><item><title>Go web framework</title><link>https://dev.to/dingzhanjun/go-web-framework-1p4o</link><author>John Ding</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 20 Aug 2025 15:13:33 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Turning My Daily Commute into a Data Visualization Project</title><link>https://dev.to/kauldeepak78/turning-my-daily-commute-into-a-data-visualization-project-28l8</link><author>Deepak Kaul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 14:30:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Most people see their daily commute as wasted time. I saw it as a dataset.For months, I logged the details of my everyday journey to work — departure times, train delays, walking speed between stations, even how my mood shifted with the weather. What started as a way to distract myself during long rides turned into a data visualization project that revealed patterns I would have never noticed otherwise.In the beginning, I kept it simple. I opened Google Sheets on my phone and manually entered:Departure & arrival times for each leg of my commute.Walking duration between home, station, and office.Noise level estimate inside the train (low, medium, high).Mood score — a quick 1–5 rating.After a few weeks, the manual entry became too repetitive. So I leveled it up:Wrote a Python script that used GPS logging on my phone to track walking/ride times automatically.Pulled weather data from an open API to log rain, temperature, and snow.Used a smartwatch app to grab step counts + heart rate, which I synced into my dataset.Suddenly, I wasn’t just collecting numbers — I was building a story of my commute.With data in hand, I started exploring visualization tools:Matplotlib & Seaborn in Python gave me quick charts: average commute times, day-of-week trends, and mood vs. weather.Tableau let me create a dashboard showing how commute length shifted across weeks and seasons.D3.js gave me an interactive timeline where I could hover over a date and see all the conditions (time, mood, noise, weather).The more I visualized, the more I realized: my commute wasn’t random chaos — it had rhythm.Here are some surprising insights from my data experiment:= Pain – My commute delays were 25% higher on Mondays than midweek.= Mood Killer – On rainy days, my mood score dropped by 40%, regardless of delays.– Leaving just 7 minutes earlier reduced my average commute time by 15%. – The loudest rides weren’t at rush hour but on evenings when major sports events were happening — apparently, fans and train noise go hand-in-hand.These weren’t just fun facts — I actually started leaving earlier and packing headphones when I knew a big game was on. – My mornings became less stressful once I knew the “sweet spots” to leave. – I got hands-on practice in Python, APIs, and data visualization tools. – I now had a personal project I could show in interviews to demonstrate data storytelling. – Instead of seeing my commute as wasted time, I turned it into a learning experiment.Not every data project has to start in a lab, a hackathon, or a work assignment. Sometimes the best datasets are sitting in your daily routine. By tracking small details, you can uncover patterns that change the way you live and along the way, you sharpen your skills as a developer.So next time you’re bored on your way to work, ask yourself : what could I measure here?]]></content:encoded></item><item><title>What I Learned From a Week of AI-Assisted Coding: The Good, The Bad, and The Surprisingly Counterintuitive</title><link>https://dev.to/jack_branch_3fb9e01c57c03/what-i-learned-from-a-week-of-ai-assisted-coding-the-good-the-bad-and-the-surprisingly-11kl</link><author>Jack Branch</author><category>dev</category><category>go</category><category>devto</category><pubDate>Wed, 20 Aug 2025 14:24:25 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Last week, I decided to build something I'd been putting off for months: a personal password manager. My requirements were simple - secure local storage, clean UI, and encryption I could trust. What made this interesting wasn't the project itself, but how I built it.I have a background in distributed systems: REST APIs, event-driven architecture, Kafka, the usual enterprise stack. Building a multi-platform desktop application was entirely new territory. I'd been planning this experiment for a while: what would it be like to build a project entirely using AI-assisted programming?Before we continue, I should disclose some bias. I'm somewhat of an AI skeptic, so I definitely had preconceived ideas going into this, particularly around code quality, security, and scalability. I also assumed the process would be painful and less enjoyable than traditional programming (spoiler alert: I was completely wrong about this one).Next came choosing the language. I've always been interested in Go: it seems like a nice blend of C++, Python, and JavaScript, all languages I enjoy. Since I'd never touched Go or Fyne (Go's UI framework), this seemed like the perfect way to put these AI models through their paces.Over the course of a week, I experimented with three different models: GPT-4, Claude Sonnet, and Gemini 2.5 Pro, switching between them to see how each handled different aspects of the development process.What I discovered challenged most of my assumptions about AI-assisted coding. The fastest model wasn't the most productive. The highest-quality code generator wasn't the most helpful. And the most counterintuitive finding of all: sometimes being "too good" at coding assistance actually made the development experience worse.If you're considering integrating AI tools into your development workflow, or if you're curious about the practical realities behind the productivity hype, here's what a week of intensive AI-assisted coding actually taught me.
  
  
  The Productivity Illusion: Fast Start, Slow Finish
The most striking pattern in my week of AI coding wasn't what I expected. My productivity started incredibly high and steadily declined as the project progressed. On day one, I had a working password manager with encryption, a basic UI, and core functionality. By day four, I was stuck in refactoring hell, generating thousands of lines of code changes while adding zero new features.
  
  
  The Setup Phase: Where AI Shines
AI assistance was genuinely transformative during the initial setup. Within hours, I had:A properly structured Go project with modules and dependenciesA working Fyne UI with multiple screens
Basic encryption and decryption functionalityFile I/O for local storageEven a custom test framework (more on that later)This was exactly the productivity boost everyone talks about. Tasks that would have taken me days of research and documentation reading were completed in minutes. For someone completely new to Go and Fyne, this felt magical.
  
  
  The Architecture Reality Check
But then reality hit. The code that got me started quickly didn't fit what I actually needed. The AI had made architectural decisions based on getting something working, not on building something maintainable. What followed was an endless cycle of refactoring:The initial encryption implementation was too simple for real security needsThe UI structure couldn't handle the complexity I wanted to addThere was no dependency injection, making testing nearly impossible
Error handling was inconsistent across the codebaseThe file structure didn't make sense for the features I plannedHere's where things got really problematic. Each refactoring session with AI would generate hundreds of lines of code changes. My commit history started looking incredibly productive - lots of activity, lots of lines added. But I wasn't adding any new features. I was essentially paying interest on the technical debt from the AI's initial "quick wins."The breaking point came when I hit my rate limit on GitHub Copilot after just four days of use (on a paid plan). Suddenly, I was stuck mid-refactor with partially broken code and no AI assistance. I had to manually dig myself out of the mess, which gave me a clear perspective on what was actually necessary versus what the AI thought needed to be "improved."
  
  
  Traditional Coding: The Unexpected Comeback
On my final day, I switched approaches entirely. I did all the coding myself and used GPT-4 purely as a reference tool: essentially treating it like an enhanced Google for Go-specific questions. The results were surprising:Higher actual delivery rate despite generating less codeNo rework cycles or debugging sessionsBetter understanding of what I was buildingCode that fit my actual requirements, not the AI's assumptionsHigh initial productivity from AI can be an illusion if it comes at the cost of architecture and maintainability.
  
  
  Model Behaviors: The Counterintuitive Preferences
Testing three different AI models revealed some unexpected preferences that go against conventional wisdom about "better" AI being more helpful.
  
  
  GPT-4: Fast, Wrong, and Strangely Effective
GPT-4 was objectively the worst at generating correct code. It made frequent mistakes, missed edge cases, and often gave me solutions that needed significant debugging. But here's the counterintuitive part: I enjoyed working with it the most.Why? Because it was fast, and its mistakes kept me engaged with the code. Every response required my review and often my correction. This forced me to actually read and understand what was being generated, learn Go patterns by fixing the AI's errors, stay involved in architectural decisions, and catch problems early rather than discovering them later.The friction was actually valuable. It prevented me from falling into passive "vibe coding" where I just accepted whatever the AI produced.
  
  
  Claude and Gemini: Too Good for My Own Good
Claude Sonnet and Gemini 2.5 Pro produced much higher quality code with fewer errors. They were more thoughtful about edge cases, better at following Go idioms, and generally more reliable. Logically, these should have been better development partners.Instead, I found myself becoming disengaged. The code was good enough that I stopped reading it carefully. I trusted their outputs and moved on to the next task. This led to less learning about Go and Fyne, architectural decisions I didn't fully understand, code that worked but didn't match my mental model, and a growing disconnect between what I wanted and what I had.Sometimes "better" AI assistance can make you a worse developer by reducing your engagement with the code.One practical lesson: stick to one model per project phase. I tried switching between models for different tasks, but each AI has its own "style" and preferences. Claude would refactor code that Gemini had written, undoing architectural decisions and imposing its own patterns. Gemini would then "fix" Claude's work in the next iteration. It became a digital turf war where I was caught in the middle, trying to maintain consistency across competing AI opinions.Gemini clearly produced the best Go code quality, which makes sense - Google created Go. This suggests a broader principle: consider who built or maintains your technology stack when choosing AI tools. The company with the deepest expertise in a language will likely have trained their models better on it.
  
  
  The Limits of Autonomy: Why Agentic Workflows Failed
The current trend in AI coding tools is toward more autonomy - agents that can make large changes across multiple files, handle complex refactoring, and work independently on substantial tasks. My experience suggests this is moving in the wrong direction.
  
  
  Small Changes vs. Large Autonomy
Every time I allowed an AI to make large, autonomous changes, the results were disappointing:New bugs introduced during refactoringArchitectural inconsistencies across files
Changes that broke existing functionalityCode that was harder to review and understandIn contrast, small, specific requests produced much better results:❌ "Improve the security of this code" (led to massive rewrites)✅ "Add input validation to this password field" (focused, reviewable change)AI models have a tendency toward "helpful" scope creep. Ask for dependency injection, and they'll also rename your methods. Request a simple refactor, and they'll reorganize your entire file structure. This isn't malicious - they're trying to be helpful - but it makes their changes much harder to review and verify.During one simple package reorganization, Gemini got stuck in a loop, unable to resolve the import dependencies it had created. The task was straightforward for a human but somehow too complex for the AI to track consistently.
  
  
  The People-Pleasing Problem
AI models are optimized for user satisfaction, not code quality. This creates some concerning behaviors:GPT-4 set test coverage requirements to 20% so the build would pass (rather than improving actual coverage)Multiple models generated a  file without considering security implicationsThey avoided suggesting additional work (like writing tests) unless explicitly askedThey took shortcuts to make code "work" rather than making it robustFor security-critical applications like a password manager, this people-pleasing tendency could be genuinely dangerous.None of the AI models suggested Test-Driven Development or proactively wrote tests. They would generate test code if asked, but testing wasn't part of their default development approach. This reinforces the idea that AI tools currently optimize for immediate functionality over long-term code quality.The test framework that was eventually generated (under heavy prompting from me) was actually quite good, but I had to specifically request it. This suggests the capability exists, but the AI's default behavior doesn't align with professional development practices.
  
  
  The Experience Amplification Theory
The most important insight from my experiment is what I'm calling the "experience amplification theory": AI coding tools amplify the developer's existing skill level and habits rather than improving them.As someone new to Go, I brought Java-influenced patterns and thinking to the codebase. The AI didn't correct these patterns - it implemented them more efficiently. The result was Go code that worked but was architecturally wrong, mixing Java-style approaches with Go implementations.A more experienced Go developer would have prompted for idiomatic patterns and caught architectural issues early. But as a novice, I didn't know what I didn't know, and the AI didn't proactively educate me about better approaches.AI models have a tendency to solve problems by adding more code rather than creating elegant solutions. Instead of clean abstractions, they often generate:Long chains of if-statements rather than streamlined logicRepetitive code blocks instead of reusable functionsVerbose error handling instead of consistent patternsMultiple similar functions instead of parameterized solutionsThis "more code equals solution" approach creates maintenance nightmares and goes against Go's philosophy of simplicity and clarity.
  
  
  Missing Professional Practices
The AI tools I tested didn't suggest professional development practices unless specifically prompted:No mention of dependency injection until I requested itNo proactive suggestions for testing strategiesNo guidance on code organization or package structureNo warnings about security implicationsNo discussion of error handling patternsThey focused on making code work, not on making it maintainable, testable, or secure.
  
  
  Vibe Coding vs. Engaged Development
Through this experiment, I developed a clearer distinction between whats known as "vibe coding" and engaged development. is when you use AI to generate functionality based purely on desired outputs, without engaging with the actual code, architecture, or implementation details. You prompt for features, check if they work, and move on without understanding what was created. means actively reviewing generated code, understanding architectural decisions, learning from implementations, and maintaining involvement in the development process.The difference is crucial for security-critical applications. Vibe coding might get you a password manager that encrypts data, but engaged development helps you catch issues like unencrypted secrets files or weak encryption implementations.One particularly concerning behavior I discovered: AI models sometimes claim to make changes without actually implementing them. Gemini would confidently describe modifications it was making, but the actual code remained unchanged. This highlights why code review remains essential: you can't trust AI assertions about what changes were made.
  
  
  What Actually Worked: A Framework for AI-Assisted Development
After a week of experimentation, I found several approaches that genuinely improved productivity without creating technical debt.The most successful approach was treating AI like an enhanced search engine rather than a pair programmer. Using GPT-4 to answer specific questions about Go syntax, Fyne APIs, or implementation patterns was incredibly valuable:"How do I handle file I/O errors in Go?""What's the idiomatic way to structure a Fyne application?"
"How do I implement AES encryption in Go?"This kept me in control of architecture and implementation while leveraging AI's knowledge base for faster learning.
  
  
  The Boilerplate Sweet Spot
AI tools excel at generating boilerplate code and handling setup tasks:Project structure and dependency managementBuild configurations and deployment scriptsStandard error handling patternsTesting scaffolding and mock generationThese are time-consuming tasks that don't require creative problem-solving, making them perfect for AI assistance.
  
  
  Specific, Bounded Prompts
When I did use AI for code generation, specific prompts worked much better than vague requests:✅ "Add error handling to this encryption function"❌ "Make this more secure"
✅ "Validate password strength using OWASP guidelines"Specific prompts naturally led to smaller, reviewable changes that I could understand and verify.I experimented with flipping the traditional roles - having me write code while the AI provided suggestions and guidance. This approach showed promise:Kept me engaged with the implementationProvided knowledge without taking controlReduced debug/refactor cyclesMaintained architectural consistencyHowever, it was difficult to keep AI models in this advisory role. They have a strong tendency to want to "take over" and generate full implementations rather than just providing guidance.
  
  
  Professional vs. Personal: The Readiness Gap
My experience reveals a clear divide in where AI-assisted coding provides genuine value versus where it creates more problems than it solves.For individual developers building personal tools, AI assistance can be transformative: faster prototyping and experimentation, access to unfamiliar technologies and frameworks, ability to build functional applications outside your expertise area, and lower stakes if things go wrong. My password manager project is a perfect example: I built something genuinely useful that I couldn't have created as quickly without AI assistance.For professional, production code, current AI tools have significant limitations: too many subtle bugs and edge cases missed, architectural decisions that don't scale, security shortcuts that create vulnerabilities, code that works but isn't maintainable, and lack of proper testing and validation. The people-pleasing tendency and focus on immediate functionality over long-term quality make current AI tools unsuitable for critical production systems.The biggest insight from my week of AI-assisted coding is that we need to develop better practices for working with these tools. The current approach of "let the AI do more" may be moving in the wrong direction.Based on my experience, effective AI-assisted development should follow these principles:Keep humans in the architectural loop : AI can generate implementations, but humans should make structural decisionsPrefer small, reviewable changes : Resist the temptation to let AI make large autonomous modificationsMaintain engagement with the code : Don't let AI quality reduce your involvement in understanding what's being builtUse specific, bounded prompts : Vague requests lead to scope creep and unwanted changesTreat AI as a knowledge tool first, code generator second : The reference use case is more reliable than the generation use caseAlways verify claims and changes : AI confidence doesn't equal correctnessFocus AI assistance on setup, boilerplate, and knowledge gaps : Avoid using it for core business logic and architectureThe future likely isn't more autonomous AI agents, but better human-AI collaboration patterns. We need tools that provide knowledge and suggestions without taking control, respect architectural boundaries and project constraints, encourage good development practices rather than just working code, support iterative, reviewable development processes, and maintain human engagement and learning.
  
  
  Conclusion: AI as an Amplifier, Not Replacement
After a week of intensive experimentation with AI-assisted coding, my biggest takeaway is nuance. These tools are incredibly powerful but require careful, intentional use to provide genuine value.AI coding assistance is best understood as an amplifier of existing developer capabilities rather than a replacement for developer skills. Good developers can use these tools to work faster and explore new technologies more quickly. But the tools don't make bad developers good - they just help them produce bad code more efficiently.The productivity gains are real, but they're not uniformly distributed across all development tasks. AI excels at boilerplate, setup, and knowledge transfer. It struggles with architecture, complex refactoring, and the kind of nuanced decision-making that separates working code from maintainable code.Most importantly, the best AI-assisted development workflows aren't the most autonomous ones. The sweet spot seems to be maintaining human control over architecture and implementation while leveraging AI for knowledge, suggestions, and rapid generation of well-defined components.We're still in the early days of learning how to work effectively with these tools. The patterns that work best may be quite different from what the current hype cycle suggests. Based on my experience, the future of AI-assisted development is likely to be more collaborative and less autonomous than current trends indicate.The key is finding the right balance: leveraging AI's strengths while maintaining the human judgment, architectural thinking, and code quality practices that produce software you can actually maintain and trust.Was the experiment a success? Absolutely. I now have a working, cross-platform password manager available on GitHub with automated tests, proper releases, and reasonably clean code. More importantly, I went from knowing zero Go to understanding core concepts and idiomatic patterns - something that would have taken weeks of traditional learning.The real success, though, was discovering a more nuanced relationship with AI coding tools. Instead of the binary "AI good" or "AI bad" perspective I started with, I now have a framework for when and how to use these tools effectively.And perhaps most importantly: I genuinely enjoyed every minute of this project. The combination of learning a new language, exploring AI capabilities, and building something I actually use daily made for an engaging week of coding. It's given me a long list of similar experiments I want to try next.Sometimes the best way to understand new technology is just to dive in and build something real with it.Want to share your own experiences with AI-assisted coding? I'd love to hear how different approaches and tools have worked (or not worked) for your projects. The community is still figuring out the best practices here, and every real-world experiment adds valuable data points.For anyone interested, the repository for the project is here]]></content:encoded></item><item><title>Why I Built an &quot;Awesome List&quot; for Data Analysis (And How It Can Help You)</title><link>https://dev.to/pavelgrigoryev/why-i-built-an-awesome-list-for-data-analysis-and-how-it-can-help-you-22pm</link><author>Pavel Grigoryev</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 14:22:55 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Curated List of 400+ Data Analysis Tools and Resources
Learning data analysis often means sifting through endless tutorials, docs, and repos. It's easy to get lost in outdated or low-quality content.I built this curated list to solve that problem. It's a organized collection of the most useful resources I've found — no fluff, no ads, just practical tools and knowledge.It's a structured learning path covering the :
  
  
  💡 My Story & Why I Built This
This repository started as my personal collection of bookmarks. Over time, it grew beyond just links into a structured knowledge base.
I realized this organized system could help others too, so I cleaned it up and decided to share it publicly.The goal is simple: save you 100+ hours of Googling and help you focus on what actually matters — building skills.
  
  
  🤔 How You Can Help (Seriously!)
This list is good, but I want it to be better. And for that, I need your expert eyes.I'd be incredibly grateful if you could:: What's the one amazing tool or resource that's missing?: Does the grouping make sense? Should we add a new section?: Brutal honesty is appreciated. This is a project for the community.Your feedback isn't just welcome; it's essential. I'll be actively updating the repo based on the comments here.Thank you for your time - I really appreciate it! 🤗]]></content:encoded></item><item><title>Show HN: I was curious about spherical helix, ended up making this visualization</title><link>https://visualrambling.space/moving-objects-in-3d/</link><author>damarberlari</author><category>dev</category><category>hn</category><pubDate>Wed, 20 Aug 2025 14:02:47 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[MOVING OBJECTS IN 3D SPACEtap/click the right side of the screen to go forward →Have you ever wondered how to move objects along a spherical helix path?Okay… probably not, right?But one morning, this question popped into my head.It stuck with me long enough that I ended up diving into a few articles about it.From there, it spiraled into lots of explorations, trying to figure out how to move objects in 3D space....to this complex, chaotic path.All these explorations made me want to share what I learned with you.I hope you enjoy this as much as I did.A helix is a shape that loops around and around, like a spring.In a spherical helix, it loops around a sphere.To move an object along a spherical helix path, we need to define its 3D coordinates to follow a helical pattern around a sphere.We'll get there! But first, let’s see how to position and move objects in 3D space.In 3D space, we position objects by setting its coordinates along three axes: x,y, and z.The x-axis typically represents horizontal movement—left or right.The y-axis typically represents vertical movement—up or down.The z-axis typically represents depth—forward or backwardTo move an object in 3D space, we can use mathematical functions to set its position over time.For example, this cube's x position is set to 10 * cos(πt/2), where t is time (in seconds).The result? It oscillates from 10 to -10 along the x-axis every 2 seconds, following a cosine wave.Similarly, setting the y position to 10 * cos(πt/2) makes the cube oscillates vertically, from 10 to -10 every 2 seconds.We can create a two-dimensional path by setting the x and y positions to different functions.For this circle, the x position is set to 10 * cos(πt/2).The cube starts at x = 10, moves to -10 in 2 seconds, then back to 10, and so on.Meanwhile, the y position is set to 10 * sin(πt/2).The movement for x and y may look similar, but they are actually out of phase.When x = 10, y = 0; when x = 0, y = 10; and so on.Together, these two functions create a circular path for the cube.Now we can get creative with functions to create even more complex paths.For example, let's multiply the x function by 0.03 * t.It would make the cube oscillates farther on the x-axis over time...and we will have a circular path whose radius grows over time.Okay, now it's time to talk about the spherical helix (finally!)The spherical helix path is similar to the spiral we just made, but with some differences.First, a spherical helix is three-dimensional.It has a z component that changes over time.This cube's z position is set as 10 * cos(0.02 * πt).It will start from z = 10 then slowly move to -10.Second, unlike the previous spiral, the x and y positions don’t grow indefinitely.They grow at first, then shrink halfway through.This is because the x function is multiplied by another sine function: sin(0.02 * πt)which makes the radius larger in the middle and smaller at the ends.The same is also done to the y function.Together, these functions create a spherical helix.By updating the cube’s position with these functions, it moves along a spherical helix path.In summary, we can move objects in 3D space by defining their x, y, z coordinates as functions of time.These functions, which express x, y, z coordinates as a function of another variable (in this case, time), are called parametric equations.Check out the Wikipedia article for more on parametric equations.Now that we know this, we can get creative and move objects along any path we want!...to this complex, chaotic path......which we know now isn't actually chaotic.It's just a path defined by mathematical functions.Thanks for sticking with me, I hope you enjoyed it!visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.If you like this, please consider following me on Twitter and sharing this with your friends.I'm planning to write more articles like this, so stay tuned!https://twitter.com/damarberlari]]></content:encoded></item><item><title>Debugging Python in Docker: A Tutorial for Beginners</title><link>https://www.kdnuggets.com/debugging-python-in-docker-a-tutorial-for-beginners</link><author>Bala Priya C</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/bala-python-debug-docker-2.png" length="" type=""/><pubDate>Wed, 20 Aug 2025 14:00:03 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[New to running Python in Docker? This step-by-step guide helps you understand and apply debugging techniques in a containerized environment.]]></content:encoded></item><item><title>Real Python: Working With JSON Data in Python</title><link>https://realpython.com/python-json/</link><author></author><category>dev</category><category>python</category><pubDate>Wed, 20 Aug 2025 14:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Python’s  module provides you with the tools you need to effectively handle JSON data. You can convert Python data types to a JSON-formatted string with  or write them to files using . Similarly, you can read JSON data from files with  and parse JSON strings with .JSON, or JavaScript Object Notation, is a widely-used text-based format for data interchange. Its syntax resembles Python dictionaries but with some differences, such as using only double quotes for strings and lowercase for Boolean values. With built-in tools for validating syntax and manipulating JSON files, Python makes it straightforward to work with JSON data.By the end of this tutorial, you’ll understand that:JSON in Python is handled using the standard-library  module, which allows for  between JSON and Python data types.JSON is a good data format to use with Python as it’s  and straightforward to serialize and deserialize, which makes it ideal for use in .You write JSON with Python using  to serialize data to a file.You can  using Python’s  module.Since its introduction, JSON has rapidly emerged as the predominant standard for the exchange of information. Whether you want to transfer data with an API or store information in a document database, it’s likely you’ll encounter JSON. Fortunately, Python provides robust tools to facilitate this process and help you manage JSON data efficiently.While JSON is the most common format for data distribution, it’s not the only option for such tasks. Both XML and YAML serve similar purposes. If you’re interested in how the formats differ, then you can check out the tutorial on how to serialize your data with Python. Test your knowledge with our interactive “Working With JSON Data in Python” quiz. You’ll receive a score upon completion to help you track your learning progress:In this quiz, you'll test your understanding of working with JSON in Python. By working through this quiz, you'll revisit key concepts related to JSON data manipulation and handling in Python.The acronym  stands for JavaScript Object Notation. As the name suggests, JSON originated from JavaScript. However, JSON has transcended its origins to become language-agnostic and is now recognized as the standard for .The popularity of JSON can be attributed to native support by the JavaScript language, resulting in excellent parsing performance in web browsers. On top of that, JSON’s straightforward syntax allows both humans and computers to read and write JSON data effortlessly.To get a first impression of JSON, have a look at this example code:You’ll learn more about the JSON syntax later in this tutorial. For now, recognize that the JSON format is . In other words, you can create JSON files using the code editor of your choice. Once you set the file extension to , most code editors display your JSON data with syntax highlighting out of the box:The screenshot above shows how VS Code displays JSON data using the Bearded color theme. You’ll have a closer look at the syntax of the JSON format next!In the previous section, you got a first impression of how JSON data looks. And as a Python developer, the JSON structure probably reminds you of common Python data structures, like a dictionary that contains a string as a key and a value. If you understand the syntax of a dictionary in Python, you already know the general syntax of a . Later in this tutorial, you’ll learn that you’re free to use lists and other data types at the top level of a JSON document.The similarity between Python dictionaries and JSON objects is no surprise. One idea behind establishing JSON as the go-to data interchange format was to make working with JSON as convenient as possible, independently of which programming language you use:[A collection of key-value pairs and arrays] are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages is also based on these structures. (Source)To explore the JSON syntax further, create a new file named  and add a more complex JSON structure as the content of the file:In the code above, you see data about a dog named Frieda, which is formatted as JSON. The top-level value is a JSON object. Just like Python dictionaries, you wrap JSON objects inside curly braces ().In line 1, you start the JSON object with an opening curly brace (), and then you close the object at the end of line 20 with a closing curly brace ().]]></content:encoded></item><item><title>Python tips and tricks</title><link>https://dev.to/mcheremnov/python-tips-and-tricks-13bj</link><author>Maksym</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 13:14:35 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Here are some practical Python tips and tricks that can make your code more efficient and elegant:
  
  
  String and Text Manipulation
 - Use f-strings instead of  or  formatting:Multiline strings with triple quotes - Great for SQL queries or documentation:
  
  
  List and Dictionary Operations
 - More concise than traditional loops:Dictionary comprehensions:Use  for safe dictionary access:Use  and  for boolean operations:
  
  
  Function and Class Tricks
Default mutable arguments - Avoid the common pitfall:kwargs` for flexible functions**:
  
  
  Built-in Functions and Modules
 instead of manual counting: for parallel iteration: for counting: for file operations:Use generators for large datasets:EAFP (Easier to Ask for Forgiveness than Permission):Always use context managers for file operations:
  
  
  Debugging and Development
Use  for complex data structures: for debugging (Python 3.7+):These techniques can significantly improve your Python code's readability, performance, and maintainability. The key is knowing when to apply each one based on your specific use case.]]></content:encoded></item><item><title>Why Your AI Chatbot is Dumb — And How to Fix It with AutoGPT Agents</title><link>https://dev.to/ekwoster/why-your-ai-chatbot-is-dumb-and-how-to-fix-it-with-autogpt-agents-1kep</link><author>Yevhen Kozachenko 🇺🇦</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 13:11:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  Why Your AI Chatbot is Dumb — And How to Fix It with AutoGPT Agents
Let’s face it — most chatbots suck. You’ve interacted with them: they greet you politely, but when you ask them anything beyond their training doc, they crumble like discount cookies. What we have today is a sea of chatbots that pretend to be intelligent, but are essentially glorified FAQ search boxes.But what if your chatbot could reason, plan, and act? Welcome to the world of autonomous AI agents — your chatbot’s smarter, more ambitious cousin.In this deep-dive, we'll walk through how to build a simple yet powerful AI agent using Python that can learn, plan tasks, and do them using tools like AutoGPT concepts and langchain. This isn’t just theory — I’ll show you real code, real modules, and real-world use cases.
  
  
  🤯 What’s Wrong with Traditional Chatbots?
Let’s kick off with how traditional bots are structured:They follow a conversation tree or rulesThey rely on static intents and entitiesThey answer only from a predefined FAQ or knowledge baseSo, if I asked a bot: "Can you summarize today's news about AI startups and email it to me?", most will either:Redirect me to a support page 📄Say: "Sorry, I don’t understand." 🤖😕That’s because they don’t have tools, memory, or reasoning. They're not agents. To BUILD an intelligent assistant, you need something that can:Create a sequence of actionable stepsExecute tools (like Google search, summarizers, email APIs)Track memory/state over timeEnter AutoGPTs and AI Agents.
  
  
  🧠 Breaking Down AI Agents (AutoGPT-Style)
AI Agents combine multiple capabilities:Large Language Model (LLM) like GPT-4 for reasoningPlanning + Subtask generationMemory/State using vector DBsTool use (like searching, file handling, APIs)The magic happens by chaining LLM calls that:Take an overall objective e.g., “Find trending startups in AI and create a spreadsheet.”Create sub-goals: search for news, identify startups, extract descriptions, write to CSVIt’s like having a junior intern... powered by reasoning.
  
  
  🛠️ Let’s Build Your First AI Agent 🧪
serpapi (for Google search)
  
  
  👉 Step 1: Install What You Need
pip langchain openai pydantic serpapi
Set them as environment vars:
  
  
  👉 Step 2: Create a Base Tool — Google Search Wrapper

  
  
  👉 Step 3: Create an Agent With a Goal
You’ll see printouts of the agent thinking through:Outputting a conclusion ✅
  
  
  🧠 Want to Persist Memory?
Use langchain.memory with a vector database like FAISS or ChromaDB to store chunks of conversation or steps the agent took.Pass it into initialize_agent(memory=memory).Ask your bot to research topics and write outlines
  
  
  ✅ Automated Interview Prep
Have it simulate interviewers, gather company data
  
  
  ✅ Email Summarizer & Responder

  
  
  🚨 Common Pitfalls & Fixes
Use streaming API + handle errors gracefullyLimit steps and monitor planning logicValidate inputs & sanitize outputsUse vector DBs and embed chunking
  
  
  Final Thoughts — Why Agents Are the Future
If chatbots were the browser, agents are the operating system.They’re not perfect yet, but the combination of:…redefines how we automate. With upcoming integrations into operating systems (e.g., Copilot, Apple Intelligence), understanding agents gives you superpowers.So — next time someone builds a chatbot, ask them:“Cool. But can it plan and use tools?”Otherwise… it’s just a fancy Clippy with a neural net.Here’s a full working mini-agent prototype on GitHub:Stay curious — we’re just getting started.Follow me for live demos, AI agent builds, and API automation hacks.]]></content:encoded></item><item><title>Day 2 of 100</title><link>https://dev.to/lyop_achayi/day-2-of-100-5ema</link><author>TANYA LYOP ACHAYI</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 12:57:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Confession: I’ve always been scared of mathematics, But today I realized coding isn’t about being a math genius, it’s about breaking problems into simple steps. As a Media enthusiast exploring Python programming, I’m learning that even numbers can feel like play! Fear aside, I’m ready to keep learning, one line at a time. ]]></content:encoded></item><item><title># 🎯 Face Landmarks Detection (OpenCV DNN + Facemark)</title><link>https://dev.to/ertugrulmutlu/-face-landmarks-detection-opencv-dnn-facemark-440d</link><author>Ertugrul</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 12:13:04 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA["Faces don’t lie — but landmarks sometimes do."Hey there! In this post, I’ll share my journey of building a Face Landmark Detection pipeline using  and . The system takes a raw video as input, detects faces, extracts , smooths them across frames, and finally outputs:an  with landmarks and bounding boxesan optional  with landmark coordinates for every frame"Take a face → get the points."But to make it robust, I had to mix  with  and add a touch of signal processing.The project is split into modular components: → Loads and runs the DNN-based face detector (SSD ResNet) → Drawing utilities for the 68-point facial structure → Video I/O, CSV logging, smoothing, and per-frame pipeline → Entry point to run the full pipeline
  
  
  🔍 Step 1 — Face Detection
I used OpenCV’s Deep Neural Network (DNN) SSD ResNet model. The detector takes each frame, converts it into a blob, and feeds it into the Caffe network:This gives us bounding boxes with confidence scores. I kept only the ones above a threshold ().
  
  
  🎯 Step 2 — Landmark Extraction
With face boxes ready, I used  to extract the :This returns arrays shaped  → coordinates for jawline, eyebrows, eyes, nose, and lips.
  
  
  📉 Step 3 — Landmark Smoothing
Raw landmarks jitter a lot between frames. To stabilize them, I applied an Exponential Moving Average (EMA):This keeps the motion natural but removes frame-by-frame noise.
  
  
  🖼️ Step 4 — Drawing the Mesh
I grouped the 68 points into face regions and connected them with polylines:The result? A clear, real-time facial mesh overlay.  frame_idx,x0,x1,...,y66,y67
  0,123,130,...,200,205
  1,124,129,...,199,206
This makes the system useful both for visualization  downstream ML tasks.DNN face detection is robust, but combining it with traditional landmarking is still effective.Smoothing is  — raw landmarks are too noisy for real use.CSV logging adds value for research/analytics beyond just visualization.You can find the full code here:"A single face in a frame is simple — but tracking it smoothly across time is where the real challenge begins."]]></content:encoded></item><item><title>Leveraging Pandas and SQL Together for Efficient Data Analysis</title><link>https://www.kdnuggets.com/leveraging-pandas-and-sql-together-for-efficient-data-analysis</link><author>Nate Rosidi</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/Rosidi-Leveraging_Pandas_and_SQL_Together-1.2.png" length="" type=""/><pubDate>Wed, 20 Aug 2025 12:00:41 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Learn to leverage Pandas and SQL together while solving a real-world Uber data project.]]></content:encoded></item><item><title>data analytics course in lucknow</title><link>https://dev.to/ammu_salveru_3e679d4b532a/data-analytics-course-in-lucknow-4fkj</link><author>ammu salveru</author><category>dev</category><category>python</category><category>devto</category><pubDate>Wed, 20 Aug 2025 11:19:02 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>