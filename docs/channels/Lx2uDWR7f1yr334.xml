<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Programming</title><link>https://www.awesome-dev.news</link><description></description><item><title>Quiz: How to Use sorted() and .sort() in Python</title><link>https://realpython.com/quizzes/python-sort/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Sun, 23 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[By working through this quiz, youâ€™ll revisit how to sort various types of data in different data structures, customize the order, and work with two different ways of sorting in Python.]]></content:encoded></item><item><title>This is don&apos;t my first lerning.</title><link>https://dev.to/consolvex/this-is-dont-my-first-lerning-279c</link><author>Consolex</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 23 Feb 2025 11:59:06 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[I'am learning GOlang. 
My English is bad, but i'am also learning Eng.]]></content:encoded></item><item><title>native WebP encoding version 1.0! ðŸš€</title><link>https://www.reddit.com/r/golang/comments/1iw8a68/native_webp_encoding_version_10/</link><author>/u/Pretend-Ad1926</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 11:38:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Iâ€™m excited to announce nativewebp v1.0, a major milestone for our WebP encoder in Go! This version marks 1.0 because we now fully support the VP8L format, making nativewebp a complete solution for lossless WebP encoding. Alongside this, weâ€™ve added better compression, improved Go integration, and important bug fixes.Here are some highlights of this release:Full VP8L Feature SupportThis release now fully supports all VP8L features, including LZ77, Color Caching, and transforms, ensuring more accurate and efficient encoding.Smarter Compression with Filter Selection for Predictor TransformWe now analyze block entropy and automatically select the best filter per block, leading to much better compression.nativewebp now includes a wrapper for golang.org/x/image/webp, so you can use Decode and image.Decode out of the box without extra imports.Looking forward to your thoughts and feedback on the new release!]]></content:encoded></item><item><title>Agricultural Product Classification</title><link>https://dev.to/agam_singh_2f66d3d454144d/agricultural-product-classification-1nco</link><author>Agam Singh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 11:30:08 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Agricultural Product Classification Using Machine LearningðŸŒ¾ Introduction
Agriculture plays a vital role in feeding the global population, making it essential to improve productivity and efficiency in this sector. One of the key challenges in modern agriculture is the accurate classification of agricultural products, which directly impacts quality control, pricing, and supply chain management. Traditional methods of classification often rely on manual inspection, which can be time-consuming and prone to errors.With the rise of data-driven technologies, Machine Learning (ML) offers a powerful alternative to automate and enhance agricultural product classification. By analyzing dimensional and shape factors, ML algorithms can accurately classify various agricultural products, reducing manual labor and increasing efficiency. This project explores how machine learning techniques can be applied to classify agricultural products using real-world data, leading to smarter agricultural practices and better decision-making.ðŸ“Š Project Overview
This project focuses on classifying agricultural products based on dimensional and shape factors using machine learning. The goal is to develop an accurate and efficient classification system by exploring data preprocessing techniques, visualizations, and multiple machine learning algorithms.ðŸ“ Dataset
Note: The Data was taken fromEntries: 13,611
Columns: 17
Data Types: 14 float64, 2 int64, 1 object
The dataset contains detailed information about agricultural plants, including area, perimeter, major axis length, and shape factors. Preprocessing steps included:Removing 68 duplicate entries.
Encoding the categorical â€˜Classâ€™ column.
Ensuring no missing values.ðŸ“Š Data Preprocessing
Data Cleaning: Removed duplicates and encoded categorical data.
Feature Scaling: Applied standard scaling for improved model performance.
Data Splitting: Divided the dataset into training and testing sets.ðŸ“ˆ Data Visualization
Utilized Matplotlib and Seaborn for:Pair Plots: To visualize feature relationships.
Histograms & Distribution Plots: To understand feature distributions.
Scatter Plots: To observe trends and correlations.
Heatmap: To display feature correlation.
Box Plots: To examine feature distribution across classes.ðŸ¤– Machine Learning Models Used
Random Forest Classifier
Precision: 92.65%
F1-Score: 92.12%K-Nearest Neighbors (KNN) (Best Performer)Accuracy: 91.95%
Precision: 92.62%
F1-Score: 92.41%Support Vector Classifier (SVC)Accuracy: 87.93%
Precision: 92.62%
F1-Score: 92.41%âš–ï¸ Model Comparison
KNN outperformed the other models, making it the most suitable for classifying agricultural products in this project.
Random Forest followed closely, while SVC showed comparatively lower accuracy.ðŸ¤– Model Training & Evaluationfrom sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Feature and target split
X = df.drop(['Class_BO', 'Class_CA', 'Class_DE', 'Class_HO', 'Class_SE', 'Class_SI'], axis=1)
y = df[['Class_BO', 'Class_CA', 'Class_DE', 'Class_HO', 'Class_SE', 'Class_SI']]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

# KNN
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))

# SVC
svc = SVC(kernel='linear')
svc.fit(X_train, y_train)
y_pred_svc = svc.predict(X_test)
print("SVC Accuracy:", accuracy_score(y_test, y_pred_svc))

âœ… Conclusion
This project demonstrates how machine learning can be applied to classify agricultural products based on shape and dimensional factors. The KNN model proved to be the most effective, showcasing strong performance across all evaluation metrics.ðŸ› ï¸ Tools & Technologies Used
Python
Pandas & NumPy: Data preprocessing
Matplotlib & Seaborn: Data visualization
Scikit-learn: Machine learning algorithms and evaluation]]></content:encoded></item><item><title>PyPy</title><link>https://dev.to/sanika_sreeak_e95609b33/pypy-1o67</link><author>Sanika Sree A.K</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 11:13:47 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[While Python 2.7 and older versions are officially unsupported, a different unofficial Python putting into use, PyPy, continues to support Python 2.PyPy is faster than CPython because it uses JIT compiler.]]></content:encoded></item><item><title>Port Forwarding with Ngrok ðŸš€: Quick Guide</title><link>https://dev.to/victorchiaka/port-forwarding-with-ngrok-quick-guide-j52</link><author>Victor Chiaka</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 10:23:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[You're building a mobile app solo. The backend is ready, and now you're working on the UI. You run your server locally and try making requests from the appâ€”nothing works.Why? Your base URL ( or ) points to your local machine, but your mobile device has its own localhost.Realizing this, you deploy your backend to a free online service, but then:The server shuts down after minutes of inactivity.Network requests are painfully slow. Ngrok, is a tool that allows you expose your local server to the internet securely.Get your auth_token from the dashboardbrew ngrok/ngrok/ngrok
ngrok config add-authtoken <your_auth_token>
ngrok config add-authtoken <your_auth_token>
wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-stable-linux-amd64.deb
dpkg  ngrok-stable-linux-amd64.deb
ngrok config add-authtoken <your_auth_token>
ngrok http http://127.0.0.1:<your-port>
Now ngrok will do it's thing ðŸ˜ and give you the following results.ngrok                                                        Ctrl+C to quit

Session Status                online
Account                       your-email@example.com Plan: Free
Version                       3.x.x
Region                        United States us
Latency                       15ms
Web Interface                 http://127.0.0.1:4040
Forwarding                    https://random-subdomain.ngrok-free.app -> http://127.0.0.1:8000
Forwarding                    http://random-subdomain.ngrok-free.app  -> http://127.0.0.1:8000

Connections                   ttl     opn     rt1     rt5     p50     p90
                              0       0       0.00    0.00    0.00    0.00
Add the following to your apps build configuration.https://random-subdomain.ngrok-free.app
Ngrok is a game-changer when it comes to local development and testing. It eliminates the hassle of exposing your local server to the internet, making it easy to test APIs, webhooks, and mobile applications without deployment delays. With just a few commands, you get a secure, public URL that seamlessly tunnels traffic to your local machine.Now, instead of struggling with localhost limitations or slow, unreliable free hosting, you can integrate Ngrok into your workflow and focus on building your app. ðŸš€Give it a try, and happy coding! ðŸ’»ðŸ”¥]]></content:encoded></item><item><title>How to be Test Driven with Spark: 2 - CI</title><link>https://dev.to/nda_27/how-to-be-test-driven-with-spark-2-ci-4a28</link><author>Nicoda-27</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 10:21:54 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This goal of this tutorial is to provide a way to easily be test driven with spark on your local setup without using cloud resources.This is a series of tutorials and the initial chapters can be found in:
  
  
  Chapter 2: Continuous Integration (ci)
Having a ci is mandatory for any project that aims at having multiple contributors. In the following chapter, a proposal ci will be implemented.As ci implementation is specific to a collaborative platform being , , ,  etc. The following chapter will try to provide a technology agnostic ci as much as possible.Similar concepts are available in all ci, you will have to transpose the concepts that will be used here.The ci here will be very minimal but showcases concepts that you implemented in Chapter 1, namely:There are many more addition to the continuous integration that will not be tackled here. A minimal ci is required to guarantee non regressions in terms of:code styling rules to guarantee no indivual contributors diverge from the coding styletests, namely all tests must be passing is expecting ci files to be provided at a specific location, you can therefore create a file in .github/workflows/ci.yaml.In this file, you can addThe  and  define the names of the pipeline that will run.The  defines the event that will trigger the pipeline to run,  means that for every commit the pipeline will run.The  defines a list of jobs, the ci is made of one job with multiple steps for the sake of simplicity.The  defined the docker image used to run (the runner) the environment against, it's a list of docker images maintained by .Now into the steps section we can add:The  is the  action that checkout the current branch of the repository.The  is the  action that will read the  and install everything for us.The  step will install the dependencies and run the formatting. It there is an error, the command will fail and the pipeline too.The  step will run the tests. It there is an error, the command will fail and the pipeline too.As it was stated, the ci is the only source of truth. If it passes on ci, it should pass on your local setup. If not, it means there are discrepancies between the ci setup and yours.Going through the ci implementation will help you on reproducibility. Maybe you're not using the same way to install python version, or the same dependency management tool. You need to align your tools and the ones presented in chapter 1 help not to conflict with your local setup. You might have installed python package globally or you might have manually changed  or your  and this can easily be a mess.To help on reproducibility, a dev container approach can be used. It means, the ci will run inside a container and this container can be reused as a developer environment. This will not be implemented for the moment.To improve readability and segregates between code formatting and testing,  actions can be implemented as job with interdependencies. Then, the workflow becomes:In here we added the  to create dependencies between ci job. It means, we will not run the tests until the code style is compliant; this will save some time and resources. Indeed, if the code is not formatted, don't even bother running the tests. The execution graph will be like:We can see here some duplication, which is not ideal as for future code improvements, you will have to do it at two places at the same time. This is technical debt that one would have to tackle using composite action. We will consider it's ok for now.
  
  
  Caching dependency resolution
You will see additional steps in the , namely related to cacheThese steps aim at caching the  when there are no changes on the  and reusing it. The intent is to speed up the ci execution as dependency resolution and installation can be time consuming.An extra step to minimize caching size is added as  proposes such feature, namely an extra step and an environment variable is added to configure the location of the cache.On the next chapter, you will implement your first spark code and implement a way to guarantee test automation of it. This is long overdue as we spent 3 chapters on setup...You can find the original materials in spark_tdd. This repository exposes what's the expected repository layout at the end of each chapter in each branch:]]></content:encoded></item><item><title>DataWars.io: 7 free Machine Learning projects to practice using Python | DataWars</title><link>https://www.datawars.io/articles/7-free-machine-learning-projects-to-practice-using-python</link><author></author><category>dev</category><category>python</category><pubDate>Sun, 23 Feb 2025 09:48:16 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Finding UI libraries is easy, but discovering components visually is still a challenge. A curated list + an idea to fix this.</title><link>https://github.com/sanjay10985/animated-react-collection</link><author>/u/Mobile_Candidate_926</author><category>dev</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 09:22:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[P] See the idea development of academic papers visually</title><link>https://www.reddit.com/r/MachineLearning/comments/1iw5lgj/p_see_the_idea_development_of_academic_papers/</link><author>/u/MadEyeXZ</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 08:30:10 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Optimizing Rust Compilation: Smaller, Faster, or Both?</title><link>https://dev.to/leapcell/optimizing-rust-compilation-smaller-faster-or-both-16pe</link><author>Leapcell</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sun, 23 Feb 2025 08:18:56 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[You have completed writing a Rust project and are now working on compilation. How can you make the compiled file as small as possible? How can you make it run as fast as possible? Or how can you achieve both small size and high speed?You may have these considerations:: Suitable for embedded development, where the project is small and not complex, and execution speed is already fast. The main goal is to reduce the file size as much as possible.Maximizing execution speed: Suitable for network services where file size is not a concern, but maximizing concurrency is the top priority.Balancing both size and speed: A middle ground that is suitable for various types of projects.You only need to add the following configuration to your  file and run:
  
  
  Generate a Smaller Executable

  
  
  Generate a Faster Executable

  
  
  Balance Between Size and Speed

  
  
  Explanation of Configurations
: Specifies the level of compiler optimizations.: No optimization, fastest compilation time.: Optimize for faster compilation.: Balance between compilation speed and runtime performance (default).: Optimize for maximum runtime performance.: Optimize for smaller code size.: Further optimize for code size, more aggressively than .: Use  to generate the smallest executable; use  to generate the fastest executable.: Enables Link Time Optimization (LTO).: Disable LTO (default).: Enable the most aggressive LTO.: Enabling LTO reduces binary size and improves runtime performance.  is a moderate choice, while  provides the best optimization but increases compilation time.: Controls the number of code generation units.: Usually . Setting it to  enables the highest level of optimization.: Reducing the number of code generation units gives the compiler more information for global optimizations, resulting in a smaller and faster executable. Setting it to  maximizes optimization but increases compilation time.: Controls panic behavior.: Unwind the stack (default).: Directly abort the process.: Using  reduces the executable size and improves performance in some cases since it eliminates the need for stack unwinding information.: Controls which debug and symbol information is removed.: Keep all information (default).: Remove debug information.: Remove symbol tables but retain necessary debug information.: Remove all optional information, including debug and symbol data.: Removing unnecessary debug and symbol information significantly reduces executable size.These are the optimization techniques for compiling a Rust project. Have you mastered them?Leapcell is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:Develop with Node.js, Python, Go, or Rust.Deploy unlimited projects for freepay only for usage â€” no requests, no charges.Unbeatable Cost EfficiencyPay-as-you-go with no idle charges.Example: $25 supports 6.94M requests at a 60ms average response time.Streamlined Developer ExperienceIntuitive UI for effortless setup.Fully automated CI/CD pipelines and GitOps integration.Real-time metrics and logging for actionable insights.Effortless Scalability and High PerformanceAuto-scaling to handle high concurrency with ease.Zero operational overhead â€” just focus on building.]]></content:encoded></item><item><title>Getting micrograd to Perfectly Predict Answers To A Sample Problem</title><link>https://dev.to/shrsv/getting-micrograd-to-perfectly-predict-answers-to-a-sample-problem-3cib</link><author>Shrijith Venkatramana</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 08:18:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, Iâ€™m building LiveAPI, a tool that makes generating API docs from your code ridiculously easy.
  
  
  Training: Changing  based on  slightly (accorindg to Learning Rate)
From the previous post - we created   a list of all the nodes in the neural network.In total we have  neurons:One of the neuron's data value is shown below:Now the goal is to change the data value of this neuron, in accordance with the gradient feedback.for p in n.parameters():
    p.data += 0.01 * p.grad # something like that (wip)
We also note that the gradient is negative for that neuron:
  
  
  Determining the sign of the step factor So there's a bit of reasoning to do here, to determine the sign of step factor.The goal is to minimize the loss (bring loss = 0). is negative - .In , the result is  is decreased a bit, increasing the loss.But in , the result is  is increased a bit, reducing the loss.So therefore, the correct option is to have a negative step factor.Now we can see that the loss before/after weight adjustment and conclude that - through the backward pass plus gradient descent, we got a more accurate result:
  
  
  Automating Gradient Descent To Get a Highly Accurate Network
Setting the right learning rate value is a subtle art. If it is too low, it takes too long to converge. If it is too large a step size, the process gets unstable and may explode the loss. So finding the perfect rate is a subtle art.
  
  
  Implementing a Training Loop
We put a loop repeating the forward pass, backward pass and weight updates process:The training gives an output like this:0 4.149044341397712
1 2.8224176124705482
2 1.0767374634555338
3 0.4436221441110331
4 0.048639680823661345
5 0.0007984305003777319
6 5.758159329954795e-06
7 1.1072290005342024e-07
8 1.1331571852917713e-08
9 1.8004031247688252e-09
10 3.886667439780539e-10
11 1.190170455797565e-10
12 5.491701244447392e-11
13 4.086071696354591e-11
14 5.2487460541263784e-11
15 1.235857710202349e-10
16 5.557297068527374e-10
17 4.829530833029305e-09
18 7.912558681799505e-08
19 2.2910484425631455e-06
You can see that the loss is getting to really small numbers near the final passes.Now we compare actual y to predicted y:And we get perfect results:actual [1.0, -1.0, -1.0, 1.0]
predicted [
    Value(data=1.0, grad=0.0, label=''), 
    Value(data=-0.9986538494836703, grad=0.0026923010326593833, label=''), 
    Value(data=-0.9993079543151291, grad=0.0013840913697418245, label=''), 
    Value(data=1.0, grad=0.0, label='')
]

  
  
  Fixing a subtle bug in the training loop
Each of the neurons in the net has weight and grad attributes.In our training loop, the first iteration is fine - when we do  we fill in the grad values for each neuron.But on the second iteration and next, we keep accumulating the grad values (and are never reset to 0).So the feedback given to each neuron could be slightly wrong. We have to reset grad to 0.We get a similar result as above in this case, since the problem was quite a simple one. It so happens in neural network that sometimes, we seem to get a successful result even when the logic is a bit buggy. For complex problems, these sorts of issues/bugs can derail the solution process - and one has to watch out for common mistakes.]]></content:encoded></item><item><title>Rust, ROS, and dynamic typing https://open.substack.com/pub/intrepidai/p/rust-ros-and-dynamic-typing?r=7n2a9&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false</title><link>https://dev.to/fgadaleta/rust-ros-and-dynamic-typing-42n9</link><author>frag</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sun, 23 Feb 2025 07:36:01 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] Relevance-Guided Parameter Optimization for Efficient Control in Diffusion Transformers</title><link>https://www.reddit.com/r/MachineLearning/comments/1iw46oq/r_relevanceguided_parameter_optimization_for/</link><author>/u/Successful-Western27</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 06:50:56 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[The key technical contribution here is a relevance-guided architecture that makes diffusion transformers more computationally efficient by selectively allocating processing power based on region importance. It combines DiT (Diffusion Transformers) with ControlNet approaches while introducing a relevance prior mechanism.Main technical points: - Introduces a two-stage relevance assessment system: lightweight networks evaluate region importance, followed by adaptive computation allocation - Integrates with existing diffusion pipelines through modular design - Relevance prior guides transformer attention mechanisms - Compatible with standard diffusion transformer architecturesKey results: - 30-50% reduction in computational overhead - Maintains or improves image quality compared to baselines - More precise control over generated content - Effective handling of complex scenesI think this could have meaningful impact on making high-quality image generation more accessible, especially for resource-constrained applications. The approach seems particularly promising for deployment scenarios where computational efficiency is crucial.I think the relevance-guided approach could extend beyond image generation - the core idea of selective computation based on importance could benefit other transformer applications where attention mechanisms are computationally expensive.TLDR: Novel architecture that makes diffusion transformers more efficient by focusing computational resources on important image regions, reducing compute needs by 30-50% while maintaining quality.]]></content:encoded></item><item><title>Build Your Own AI Agents - From Scratch &amp; with Frameworks! (YouTube Tutorial)</title><link>https://dev.to/bytesinstitute/build-your-own-ai-agents-from-scratch-with-frameworks-youtube-tutorial-d94</link><author>Bytes Institute</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 06:37:53 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Want to dive into AI Agents but don't know where to start?  I just released a new YouTube video guiding you through building AI Agents from the ground up!  Fundamentals of AI Agents  Building from scratch (no frameworks!)  Building with the   Leveraging  for powerful agentsReady to build intelligent agents?  Check out the full tutorial on YouTube.]]></content:encoded></item><item><title>Folang: Transpiler for F#-like functional languages â€‹â€‹to Go</title><link>https://www.reddit.com/r/golang/comments/1iw3tz7/folang_transpiler_for_flike_functional_languages/</link><author>/u/karino2012</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 06:26:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I wrote a transpiler in Go that transpiles F#-like functional languages â€‹â€‹to Go.I design the language specifications from scratch to match Go, and named it Folang.There are still many NYIs, but I have implemented it to the extent that it can be self-hosted, so I will post it on reddit.A transpiler that does not require anything other than Go to tryArgument types are inferred using F#-like syntax, and the arguments are generalized to become generic functionsThe transpiler itself is 3600 lines of Folang code and about 500 lines of Go code]]></content:encoded></item><item><title>A simple VSCode extension to remember which virtual desktop each editor window is on in Linux</title><link>https://marketplace.visualstudio.com/items?itemName=mathiscode.remember-desktops</link><author>/u/FatherCarbon</author><category>dev</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 05:56:05 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[On some Linux desktop managers, Visual Studio Code editors don't remember their last desktop. This extension uses  to save the desktop of each open editor window, and restore them when the editor starts.There are commands to save the editor locations, and to restore them, but by default the extension will start working automatically when it is installed.]]></content:encoded></item><item><title>A Practical Guide to ORM in GoFrame: From Basics to Advanced Relationships</title><link>https://dev.to/jones_charles_ad50858dbc0/a-practical-guide-to-orm-in-goframe-from-basics-to-advanced-relationships-17fk</link><author>Jones Charles</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 23 Feb 2025 05:45:07 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[If you're working with Go and looking for a powerful yet lightweight web framework, GoFrame might be just what you need. One of its standout features is the  package, which provides a robust ORM (Object-Relational Mapping) system. In this guide, I'll walk you through everything you need to know to effectively use ORM in your GoFrame projects.Setting up database connectionsWorking with transactionsHandling relationships (one-to-one, one-to-many, many-to-many)Basic knowledge of Go programmingGo installed on your machineMySQL or compatible databaseBasic understanding of ORM concepts
  
  
  Getting Started: Database Configuration
First things first, let's set up our database connection. GoFrame makes this super straightforward with a YAML configuration:ðŸ’¡ : The  setting is fantastic during development as it shows you the actual SQL queries being executed. Remember to disable this in production!
  
  
  Defining Your First Model
Let's start with a simple user model. In GoFrame, models are just Go structs with special tags:
  
  
  CRUD Operations Made Easy
More complex query with conditions:
  
  
  Working with Transactions
Transactions are crucial for maintaining data integrity. Here's how to use them in GoFrame:
  
  
  Advanced Feature: Relationships
Perfect for user profiles or detailed information:
  
  
  One-to-Many Relationships
Great for handling user comments or posts:
  
  
  Many-to-Many Relationships
Perfect for scenarios like course enrollment systems:
  
  
  Best Practices and Tips ðŸ’¡
: Include context in your database operations for better control and cancellation capabilities.: Don't ignore error returns from database operations. for operations that modify multiple tables. appropriately based on your query patterns.: Avoid putting business logic in your model structs.
  
  
  Common Gotchas to Watch Out For âš ï¸
Remember to properly close database connectionsBe careful with large result sets - use paginationWatch out for N+1 query problemsDon't forget to handle null values appropriatelyGoFrame's ORM system provides a powerful yet intuitive way to work with databases in Go. It strikes a great balance between functionality and simplicity, making it a solid choice for both small and large projects.Explore GoFrame's caching capabilitiesLook into query optimization techniquesLearn about GoFrame's migration toolsLet me know in the comments if you have any questions or if you'd like to see more GoFrame content! ðŸš€]]></content:encoded></item><item><title>Font for programming mathematics</title><link>https://www.reddit.com/r/rust/comments/1iw2ovd/font_for_programming_mathematics/</link><author>/u/okimusix</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 05:14:27 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[So I am a physics undergrad and I've been using Rust for a few years now. It's my favorite language and I use it for everything, from personal apps using Tauri to taking advantage of its speed for computations and using it in my school assignments.Since I often find myself writing math code, I found naming variables "lambda_squared", for example, looks really clunky and makes it harder to read the code. For this, I implemented a Live Templates group on RustRover that replaced lambda, for example, with its equivalent unicode character. However, Rust did complain a little.Finally, though, I found the solution. I had been trying to do this for a while with no luck, but I found a way to make it work. I used the ligature system on the FiraCode font to implement ligatures for every greek letter and some mathematical symbols, this way you get the readability of actual math, but for the compiler, it still looks like plain text. Here's an exampleThe text for the sum variable, for example, is just "SUMxu2", and both the compiler and I are happier. I don't know if anyone has done this before, I tried to look for it but never found anything. If you find this something that could be useful for you or others, I can share a link to a drive or something where you can download the font, as well as the guide to every symbol I included. If so, please comment and share your thoughts on this too :)]]></content:encoded></item><item><title>Write AI agent from scratch without LangChain and CrewAI</title><link>https://dev.to/franzwong/write-ai-agent-from-scratch-without-langchain-and-crewai-2bop</link><author>Franz Wong</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 05:12:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[We are going to create an AI agent which can perform timezone conversion with tools / function calling. Only  library is used.Full source code will be given at the end.We only need to install  library.We will define 2 tools. One is to get today's date and the other is to convert the timezone.We also define  which stores the details about the tools. The key of the map is the function name.  is the details we will send to LLM and  is the function we will call.If your tools perform sensitive operations, you should verify or sanitize the parameters first.We apply "ReAct Prompting" to enable act and reasoning abilities. This also allows us to use tools.In order to make LLM stop generating after proposing an action, we ask it to output "PAUSE" text after an action.Before we see the actual code of AI agent, we check the pseudocode first.We have a loop to keep sending messages to LLM and performing actions.messages = [system_prompt, question_prompt]

completion = chat(llm, messages)
add completion to messages with role 'assistant'

while 'Final Answer' is not in completion and maximum round is not reached:

  If 'Action' is not found in completion:
    continue
  action = parse_action(completion)

  If 'Action Input' is not in completion:
    continue
  action_input = parse_action_input(completion)

  result = action(acion_input)

  add result to messages with role 'assistant'

  completion = chat(llm, messages)
  add completion to messages with role 'assistant'
 function is straight forward. It only creates chat completion with the messages we have.Stop word "PAUSE" is used to make LLM stop generation after proposing an action.We use regular expression to extract tool function name. Tool function will be returned. We also make sure tool function is defined in  for security reason.You can also change the system prompt to ask LLM returning JSON to ease parsing. function
As same as , we use regular expression to extract parameters of tool.Here is the implementaiton of the above pseudocode.Question: Convert 13:49 today in London time to Japan time. Please consider daylight saving.
Chat completion:
Thought: To accurately convert 13:49 London time to Japan time, it's important to consider the current date and whether daylight saving time is in effect. I'll first fetch the current date in the "Europe/London" timezone to understand if daylight saving time needs to be taken into account.
Action: get_today_date
Action Input: "Europe/London"

Action result: 2025-02-23
Chat completion:
Thought: Today is February 23, 2025. Daylight saving time in London typically starts on the last Sunday in March and ends on the last Sunday in October. Therefore, currently, London is on Greenwich Mean Time (GMT, UTC+0). Japan does not observe daylight saving time and is always on Japan Standard Time (JST, UTC+9). I will now convert 13:49 London time to Japan time for today.
Action: convert_timezone
Action Input: "Europe/London", "Asia/Tokyo", 2025, 2, 23, 13, 49

Action result: 2025-02-23 22:49:00+09:00
Chat completion:
Thought: I have successfully converted 13:49 London time to Japan time. The converted time is 22:49 on February 23, 2025.
Final Answer: 13:49 in London on February 23, 2025, is 22:49 in Japan time.
]]></content:encoded></item><item><title>[D] API platforms vs self-deployment for diffusion models</title><link>https://www.reddit.com/r/MachineLearning/comments/1iw2kbl/d_api_platforms_vs_selfdeployment_for_diffusion/</link><author>/u/crookedstairs</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 05:06:59 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Caveat that Modal is a serverless compute platform! But this post covers when you might choose between API platforms (replicate, fal), traditional cloud (AWS EC2), managed ML platforms (SageMaker, Vertex), and serverless cloud.I often see companies jump to self-deployment even if they're just using off-the-shelf models with a couple of adapters. I think that rarely makes sense from a cost or effort perspective unless you have a high volume of production traffic that you're amortizing those things across. The most compelling reason to move to self-deployment is if you need a high level of control over generated inputs => this requires fine-tuned weights / customer adapters / multi-step generation pipeline => this requires code-level control of your deployment.What do you agree/disagree with? If you've evaluated these categories of providers before, tell me how they stacked up against each other.]]></content:encoded></item><item><title>ElasticTransform in PyTorch (3)</title><link>https://dev.to/hyperkai/elastictransform-in-pytorch-3-56pl</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 03:55:45 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ElasticTransform() can do random morphological transformation for an image as shown below. *It's about  and  argument:]]></content:encoded></item><item><title>ElasticTransform in PyTorch (2)</title><link>https://dev.to/hyperkai/elastictransform-in-pytorch-2-2p5j</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 03:52:59 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ElasticTransform() can do random morphological transformation for an image as shown below. *It's about  and  argument:]]></content:encoded></item><item><title>ElasticTransform in PyTorch (1)</title><link>https://dev.to/hyperkai/elastictransform-in-pytorch-1-m0b</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 03:51:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ElasticTransform() can do random morphological transformation for an image as shown below. *It's about  and  argument:The 1st argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can do morphological transformation.It's the magnitude of displacements .A tuple/list must be the 1D with 1 or 2 elements.A single value(,  or /( or )) means .The 2nd argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It's the smoothness of displacements .A tuple/list must be the 1D with 1 or 2 elements.A single value(,  or /( or )) means .The 3rd argument for initialization is (Optional-Default:InterpolationMode.BILINEAR-Type:InterpolationMode).The 4th argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can change the background of an image. *The background can be seen when doing morphological transformation for an image.A tuple/list must be the 1D with 1 or 3 elements.If all values are , it's black.The 1st argument is (Required-Type: or ()):
*Memos:

]]></content:encoded></item><item><title>What is your logging, monitoring &amp; observability stack for your golang app?</title><link>https://www.reddit.com/r/golang/comments/1iw07rm/what_is_your_logging_monitoring_observability/</link><author>/u/gwwsc</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 23 Feb 2025 02:53:57 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[My company uses papertrail for logging, prometheus and grafana for observability and monitoring.I was not actively involved in the integration as it was done by someone else a few years ago and it works.I want to do the same thing for my side project that I am working on for learning purpose. The thing I am confused about it should I first learn the basics about otel, collector agents etc? Or should I just dive in?As a developer I get an itch if things are too abstracted away and I don't know how things are working. I want to understand the underlying concepts first before relying on abstraction.What tools are you or your company using for this?]]></content:encoded></item><item><title>My &quot;AI Operating System&quot; Can Now Organize My Desktop!</title><link>https://v.redd.it/crjpxmcknske1</link><author>/u/mitousa</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 01:47:12 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Understanding Anonymous Functions in Go: A Practical Guide</title><link>https://dev.to/abstractmusa/understanding-anonymous-functions-in-go-a-practical-guide-57hd</link><author>Md Abu Musa</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 23 Feb 2025 01:11:56 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[What is an Anonymous Function?An  is a function . Instead of being declared like a traditional named function, it is defined inline and assigned to a variable or executed immediately.ðŸ“Œ Here,  holds an anonymous function that takes two integers and returns their sum.Use Cases of Anonymous Functions1ï¸âƒ£ Passing Anonymous Functions as ArgumentsSince functions can be passed as arguments, anonymous functions are useful for higher-order functions.âœ…  Useful in callback functions or custom processing logic.2ï¸âƒ£ Using Anonymous Functions in GoroutinesGoroutines allow concurrent execution, and anonymous functions are a great way to define short-lived concurrent tasks.âœ…  Running background tasks without defining a named function.3ï¸âƒ£ Returning Anonymous Functions (Closures)Closures allow an anonymous function to capture variables from its surrounding scope.âœ…  Creating  that retain state.4ï¸âƒ£ Storing Anonymous Functions in a MapYou can store anonymous functions in a map for dynamic execution.âœ…  Implementing  or .What is an IIFE (Immediately Invoked Function Expression)?An IIFE (Immediately Invoked Function Expression) is an anonymous function that runs immediately after being defined.âœ…  One-time setup logic, reducing unnecessary variable scope.Anonymous functions in Go offer flexibility and concise coding, making them a powerful tool for:Concurrent execution (Goroutines)Closures (Retaining state)One-time execution (IIFE)By understanding and implementing anonymous functions effectively, you can write , , and  Go code.]]></content:encoded></item><item><title>I built WikiTok in 4 hours - A TikTok style feed for Wikipedia</title><link>https://www.reddit.com/r/artificial/comments/1ivy48f/i_built_wikitok_in_4_hours_a_tiktok_style_feed/</link><author>/u/Illustrious-King8421</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 01:03:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[So, I decided to use Replit's AI Agent to create my own version. Took me about 4 hours total, which isn't bad since I don't know any code at all.To be honest, at first it seemed unreal - seeing the AI build stuff just from my instructions. But then reality hit me. With every feature I wanted to add, it became more of a headache. Here's what I mean: I wanted to move some buttons around, simple stuff. But when I asked the AI to realign these buttons, it messed up other parts of the design that were working fine before. Like, why would moving a button break the entire layout?This really sucks because these errors took up most of my time. I'm pretty sure I could've finished everything in about 2 hours if it wasn't for all this fixing of things that shouldn't have broken in the first place.I'm curious about other people's experiences. If you don't code, I'd love to hear about your attempts with AI agents for building apps and websites. What worked best for you? Which AI tool actually did what you needed?What do you think? Would love to hear your stories and maybe get some tips for next time!]]></content:encoded></item><item><title>Fixing Django FieldError at /admin/accounts/customuser/add/</title><link>https://dev.to/wsvincent/fixing-django-fielderror-at-adminaccountscustomuseradd-k9n</link><author>Will Vincent</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 00:48:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you are a Django developer who wants to add a custom user model to your project, you've likely come across this error on Django versions 5.0 and above.FieldError at /admin/accounts/customuser/add/
Unknown field(s) (usable_password) specified for CustomUser. Check fields/fieldsets/exclude attributes of class CustomUserAdmin.The issue is around  In Django versions up to 4.2, you could set your  file to add updated user creation and change forms.However, as of Django 5.0, that leads to the above-mentioned . The fix is straightforward to do, thankfully, which is to swap out  for the newer AdminUserCreationForm instead, which includes the additional  field causing the initial issue.]]></content:encoded></item><item><title>Advanced SQL Tricks (CTEs, Conditional Aggregations, etc)</title><link>https://youtu.be/rDGCOE5YGT0</link><author>/u/Special_Community179</author><category>dev</category><category>reddit</category><pubDate>Sun, 23 Feb 2025 00:18:12 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Por que vocÃª deve repensar o uso de Regex em validaÃ§Ãµes de strings em Go</title><link>https://dev.to/renanbastos93/por-que-voce-deve-repensar-o-uso-de-regex-em-validacoes-de-strings-no-go-1cdi</link><author>renanbastos93</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sun, 23 Feb 2025 00:05:15 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Quando falamos de validaÃ§Ã£o de strings no Go, uma das soluÃ§Ãµes mais comuns Ã© o uso de expressÃµes regulares (regex). No entanto, dependendo do contexto, o uso de regex pode ser menos eficiente do que alternativas mais simples. Em sistemas de alta performance, como os que lidam com grandes volumes de dados ou que precisam ser executados em ambientes com recursos limitados, Ã© importante considerar o impacto de cada escolha de implementaÃ§Ã£o.Embora regex seja uma ferramenta poderosa, sua utilizaÃ§Ã£o indiscriminada pode resultar em impactos de performance, principalmente em Go, onde cada operaÃ§Ã£o Ã© otimizada ao mÃ¡ximo para garantir alta eficiÃªncia. Neste post, vamos abordar um exemplo simples de validaÃ§Ã£o de strings alfanumÃ©ricas, comparando o uso de regex com uma abordagem baseada em Unicode e explicando como otimizar o uso de regex no Go para obter melhores resultados.O que acontece por trÃ¡s das cortinas: Regex vs Unicode
A biblioteca  Ã© bastante Ãºtil, mas pode ser mais lenta do que validaÃ§Ãµes feitas manualmente usando funÃ§Ãµes da biblioteca unicode. Isso ocorre porque o Go precisa compilar o regex e avaliar sua expressÃ£o toda vez que Ã© usado dentro de um mÃ©todo. Isso consome mais tempo de CPU e memÃ³ria, especialmente em validaÃ§Ãµes simples, como a checagem, se uma string contÃ©m apenas caracteres alfanumÃ©ricos.A alternativa, mais eficiente, Ã© iterar sobre a string e validar cada caractere individualmente, utilizando funÃ§Ãµes como unicode.IsLetter e unicode.IsDigit. Isso evita a sobrecarga de compilar o regex toda vez que a funÃ§Ã£o Ã© chamada e pode ser muito mais rÃ¡pido para cenÃ¡rios simples.
  
  
  Exemplo prÃ¡tico: Comparando as abordagens
Aqui estÃ¡ um exemplo em Go para comparar as duas abordagens: uma usando regex e outra utilizando Unicode diretamente.
  
  
  ImplementaÃ§Ã£o usando Unicode:

  
  
  ComparaÃ§Ã£o de Performance
Vamos rodar alguns benchmarks para comparar a performance das duas abordagens.Como vocÃª pode observar nos benchmarks acima, a versÃ£o que usa Unicode Ã© significativamente mais rÃ¡pida do que a versÃ£o com regex. A versÃ£o com Unicode realiza cerca de 18 milhÃµes de operaÃ§Ãµes por segundo, enquanto a versÃ£o com regex realiza apenas cerca de 3 milhÃµes.Quando usamos regex, estamos criando e compilando uma expressÃ£o regular toda vez que chamamos a funÃ§Ã£o. O processo de compilaÃ§Ã£o e execuÃ§Ã£o da expressÃ£o regular envolve mais passos do que simplesmente iterar sobre os caracteres da string com funÃ§Ãµes do unicode. Isso faz com que a execuÃ§Ã£o com regex consuma mais tempo de CPU e memÃ³ria.Isso nÃ£o significa que vocÃª deve parar de usar regex. Em casos mais complexos, onde vocÃª precisa de validaÃ§Ãµes mais sofisticadas, regex pode ser a melhor escolha. No entanto, em validaÃ§Ãµes simples, como a checagem de caracteres alfanumÃ©ricos, o uso de Unicode diretamente Ã© muito mais eficiente.Dica de Performance: Compile Regex fora do mÃ©todo
Se vocÃª optar por usar regex, uma boa prÃ¡tica Ã© compilar a expressÃ£o regular fora do mÃ©todo, para que ela nÃ£o precise ser recompilada a cada chamada. Isso pode ajudar a reduzir o custo de performance de usar regex.Ao compilar a expressÃ£o regular uma vez e reutilizÃ¡-la, vocÃª reduz significativamente o impacto de performance.Embora regex seja uma ferramenta poderosa e Ãºtil, seu uso em validaÃ§Ãµes simples pode ser ineficiente, especialmente quando a performance Ã© uma prioridade. Em Go, alternativas como a utilizaÃ§Ã£o da biblioteca unicode podem oferecer ganhos significativos de performance. Se for necessÃ¡rio usar regex, lembre-se de compilar a expressÃ£o regular fora do mÃ©todo para otimizar o desempenho.Agora, repense como vocÃª estÃ¡ validando suas strings no seu projeto e escolha a abordagem mais eficiente para o seu caso!]]></content:encoded></item><item><title>íŒŒì´ì¬ìœ¼ë¡œ Epitope binning í•˜ê¸°</title><link>https://dev.to/partrita/paisseoneuro-epitope-binning-hagi-1ch1</link><author>Joconan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[ì¹˜ë£Œìš© ë‹¨í´ë¡  í•­ì²´(mAbs)ëŠ” ë°”ì´ì˜¤ì˜ì•½í’ˆ ì‹œìž¥ì˜ 70% ì´ìƒì„ ì°¨ì§€í•˜ë©° ì§€ì†ì ìœ¼ë¡œ ì„±ìž¥í•˜ê³  ìžˆìŠµë‹ˆë‹¤. í•­ì²´ ê°œë°œ ì´ˆê¸° ë‹¨ê³„ì—ì„œ ì¹˜ë£Œì œ ë° ì§„ë‹¨ ë„êµ¬ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì ì ˆí•œ íŠ¹ì„±ì„ ê°€ì§„ í›„ë³´ë¥¼ ì„ ë³„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì—í”¼í† í”„ ë¹ˆë‹ì€ mAbsê°€ í‘œì  ë‹¨ë°±ì§ˆ(í•­ì›)ì— ê²°í•©í•˜ëŠ” íŠ¹ì„±ì„ íŒŒì•…í•˜ëŠ” ë°©ë²•ìž…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë™ì¼í•œ í‘œì  ë‹¨ë°±ì§ˆì— íŠ¹ì´ì ì¸ mAbsë¥¼ ìŒìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ í•­ì›ì˜ íŠ¹ì • ë¶€ìœ„ì— ëŒ€í•œ ê²°í•©ì„ ì„œë¡œ ì°¨ë‹¨í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ê°™ì€ ì—í”¼í† í”„ì— ëŒ€í•œ ê²°í•©ì„ ì°¨ë‹¨í•˜ëŠ” mAbsëŠ” í•¨ê»˜ â€œë¹ˆâ€ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. ê°™ì€ ë¹ˆì— ì†í•œ mAbsëŠ” ì¢…ì¢… ìœ ì‚¬í•œ ê¸°ëŠ¥ì„ í•˜ë¯€ë¡œ ì—í”¼í† í”„ ë¹ˆì„ í†µí•´ í›„ë³´ í•­ì²´ì˜ ë‹¤ì–‘ì„±ì„ í™•ì¸ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì—í”¼í† í”„ ë‹¤ì–‘ì„±ì€ ì§€ì  ìž¬ì‚°ê¶Œ ë³´í˜¸ë¥¼ í™•ëŒ€í•˜ëŠ” ë°ë„ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ í•­ì²´ë“¤ì´ ê°™ì€ í•­ì›ì— ê²°í•©í•˜ë”ë¼ë„ ìž‘ìš© ë©”ì»¤ë‹ˆì¦˜ì´ ë‹¤ë¥¼ ìˆ˜ ìžˆëŠ”ë° ì´ëŠ” ì¼ë¶€ ì•”ê³¼ ê°ì—¼ì„± ì§ˆí™˜ ì¹˜ë£Œì— ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.ì—í”¼í† í”„ ë¹ˆë‹ì€ ì—í”¼í† í”„ ë§¤í•‘ê³¼ í˜¼ë™í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ì—í”¼í† í”„ ë§¤í•‘ì—ì„œëŠ” í•­ì›ì˜ ê°œë³„ ë‹¨íŽ¸ì— ëŒ€í•œ í•­ì²´ ê²°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ í•­ì²´ê°€ ê²°í•©í•˜ëŠ” í•­ì›ì˜ íŠ¹ì • ì—í”¼í† í”„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.SPRì„ ì´ìš©í•œ ì—í”¼í† í”„ ë¹ˆë‹ì˜ ì£¼ìš” ìž¥ì ì€ í•­ì›ê³¼ ì†ŒëŸ‰ì˜ ì •ì œëœ í•­ì²´ë§Œ ìžˆìœ¼ë©´ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìžˆë‹¤ëŠ” ê²ƒìž…ë‹ˆë‹¤. SPRì„ í†µí•œ ì—í”¼í† í”„ ë¹„ë‹ì˜ ì›ë¦¬ë¥¼ ê°„ëžµí•˜ê²Œ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì²«ë²ˆì§¸ í•­ì²´ë¥¼ ê³ ì •ì‹œì¼œ ë†“ê³  í•­ì›ê³¼ ë‘ë²ˆì§¸ í•­ì²´ë¥¼ ë„£ì–´ì„œ RUê°’ì„ ì¸¡ì •í•˜ëŠ”ë°, ì—í”¼í† í”„ê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²½ìš°ì— RU ê°’ì´ ë†’ê²Œ ì¸¡ì •ë©ë‹ˆë‹¤. ì¦‰, ì—í”¼í† í”„ê°€ ë¹„ìŠ·í•œ ê²½ìš°ëŠ” RU ê°’ì´ ë‚®ê²Œ ì¸¡ì •ë˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ì´ì œ SPRì„ í†µí•´ ì–»ì€ epitope binning ë°ì´í„°ë¥¼ íŒŒì´ì¬ìœ¼ë¡œ ë¶„ì„í•´ì„œ ì–´ë–¤ í•­ì²´ ì»¤ë®¤ë‹ˆí‹°ê°€ ìžˆëŠ”ì§€ ì‹ë³„í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.ìœ„ì˜ ê²°ê³¼ë¥¼ í†µí•´ ë°ì´í„°ì˜ í˜•íƒœë¥¼ íŒŒì•…í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ 2563ê°œê°€ ìžˆê³  ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì„ ë³´ì•„í•˜ë‹ˆ ì‹¤ì œ ì‹¤í—˜ ê°’ì´ ì•„ë‹Œ ì¼ì¢…ì˜ í›„ì²˜ë¦¬ê°€ëœ ê°’ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìžˆìŒì„ ì•Œ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.ížˆíŠ¸ë§µì€ ì°¨ë‹¨, ë¹„ì°¨ë‹¨ ë° ë¶ˆí™•ì‹¤í•œ í•­ì²´ ìŒì— ëŒ€í•œ ë¹ ë¥¸ ê°œìš”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ížˆíŠ¸ë§µ ë‚´ ë°ì´í„°ë¥¼ ê°„íŽ¸í•˜ê²Œ ê²€ì‚¬í•  ìˆ˜ ìžˆìœ¼ë©° ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤:ì§ê´€ì  ì´í•´: ë³µìž¡í•œ ë°ì´í„°ë¥¼ ìƒ‰ìƒ ì½”ë“œë¡œ í‘œí˜„í•˜ì—¬ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.íŒ¨í„´ ì‹ë³„: ëŒ€ëŸ‰ì˜ ë°ì´í„°ì—ì„œ íŒ¨í„´ì´ë‚˜ íŠ¸ë Œë“œë¥¼ ì‰½ê²Œ ë°œê²¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.ìœ ì—°í•œ ë¶„ì„: ì»·ì˜¤í”„ ê°’ì„ ì¡°ì •í•¨ìœ¼ë¡œì¨ ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.íš¨ìœ¨ì ì¸ ë°ì´í„° í•´ì„: ë§Žì€ ì–‘ì˜ ì •ë³´ë¥¼ ì••ì¶•ëœ í˜•íƒœë¡œ í‘œí˜„í•˜ì—¬ ë¹ ë¥¸ ì˜ì‚¬ê²°ì •ì„ ë•ìŠµë‹ˆë‹¤.ìœ„ ížˆíŠ¸ë§µ ê²°ê³¼ë¥¼ í†µí•´ í¬ê²Œ 4ê°œì˜ í´ëŸ¬ìŠ¤í„°ê°€ ì¡´ìž¬í•˜ê³  ìžˆë‹¤ëŠ” ê²ƒì„ ì‰½ê²Œ ìœ ì¶”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.K-Nearest Neighbors (KNN) í´ëŸ¬ìŠ¤í„°ë§ì€ ì—í”¼í† í”„ ë¹ˆë‹ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ë° ìœ ìš©í•œ ë°©ë²•ìœ¼ë¡œ ížˆíŠ¸ë§µë³´ë‹¤ ë” ëª…ë£Œí•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.ìœ„ì˜ ê²°ê³¼ë¥¼ í†µí•´ ì´ 4ê°œì˜ í•­ì²´ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ í™•ì¸ í•  ìˆ˜ ìžˆì—ˆê³  ížˆíŠ¸ë§µ ê²°ê³¼ì™€ ìœ ì‚¬í•¨ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.ì—í”¼í† í”„ ë¹ˆë‹ì€ í•­ì²´ì˜ ê²°í•© íŠ¹ì„±ì„ íŒŒì•…í•˜ê³  ë‹¤ì–‘í•œ ì—í”¼í† í”„ë¥¼ í‘œì ìœ¼ë¡œ í•˜ëŠ” í•­ì²´ë¥¼ ì„ ë³„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ížˆíŠ¸ë§µê³¼ KNN í´ëŸ¬ìŠ¤í„°ë§ ë“±ì˜ ì‹œê°í™” ë°©ë²•ìœ¼ë¡œ í•­ì²´ íŒ¨ë„ì˜ ë‹¤ì–‘ì„±ì„ í™•ì¸í•˜ê³  ê°€ìž¥ ìœ ë§í•œ í›„ë³´ë¥¼ ì„ ë³„í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.]]></content:encoded></item><item><title>The unexpected way in which conditional types constrain type variables in TypeScript</title><link>https://2ality.com/2025/02/conditional-type-constraints.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[The TypeScript handbook makes an interesting statement: â€œOften, the checks in a conditional type will provide us with some new information. Just like narrowing with type guards can give us a more specific type, the true branch of a conditional type will further constrain generics by the type we check against.â€In this blog post, weâ€™ll see that this goes further than you may think.]]></content:encoded></item><item><title>Any open source project that shows a good example of unit test and integration test</title><link>https://www.reddit.com/r/golang/comments/1ivv5eq/any_open_source_project_that_shows_a_good_example/</link><author>/u/smartfinances</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 22:40:03 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[As the title suggests. I have been adding various unit tests to my project but I am looking for suggestions/ideas on how to go about writing integration tests.My project mostly entails reading from SQS, batching data based on some parameters and then writing the output to s3. probably, a very common pattern. Extending that the service reads from various SQS and batching is localised to one queue. So 10 queue creats 10 different outputs.I am using localstack for development. I am not looking for examples of exactly the above use case but something similar that is interaction with db/external system and then giving some output would also do.]]></content:encoded></item><item><title>Streaming SQL in Stateful DataFlows</title><link>https://dev.to/debadyuti/streaming-sql-in-stateful-dataflows-3jng</link><author>Deb</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Sat, 22 Feb 2025 22:37:35 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[
  
  
  Streaming SQL Functionality
SQL Streaming Queries and Stream Processing Operations is released in Stateful DataFlow Beta 7 running on Fluvio 0.15.2With SQL Streaming on Stateful DataFlow you can:Run ad-hoc queries on saved state objects and materialized views based live event streams.Use SQL queries to run stream processing operations in data flows.For those who are not aware of Fluvio or Stateful DataFlow yet:Stateful DataFlow - Stream processing layer built on Fluvio built using the wasm component model.
  
  
  SQL: From Static Tables to Streaming Data
Remember when SQL was the only way to talk to your data? It wasn't just a query language - it was  query language. But its story goes deeper than syntax.
  
  
  The Universal Language of Data
Just as merchants in medieval Mediterranean ports needed a shared language to trade (that's where "lingua franca" came from), the tech world needed SQL to make data accessible across different systems and teams.If you're in a room with a DBA, a data analyst, and a business analyst. What's the one language they all speak? Likely SQL.Look familiar? Whether you're running Oracle, Postgres, or MySQL, this just works. Well sort of!Three key factors made SQL a long-term utility that stood the test of time:
Instead of telling machines HOW to get data, you just say WHAT you want. SELECT * FROM users WHERE status = 'active' reads almost like English.
From startups to Fortune 500s, SQL skills travel. Write once, run anywhere - from healthcare to fintech.
Need to analyze sales data? Track user behavior? SQL's got you covered, backed by decades of tooling and optimization.In a world of Artificial Intelligence, Web3, and global markets, event streaming is no longer a luxury - it's a basic need. Ask yourself:Is your application combining data from multiple sources in real-time?Are your customers happy with stale insights?Do you need fresh data on demand?
  
  
  Bridging Static and Streaming
What if you could use familiar SQL syntax for real-time data processing? What if your team could leverage their existing SQL skills for stream processing?We've been exploring these questions and implementing solutions that bring SQL's simplicity to streaming data. Want to see how? Check out the full article where we dive into:Practical examples using NY Transit dataReal-world streaming SQL queries in actionHow to implement stream processing without learning a new language]]></content:encoded></item><item><title>Writing a file system in Go -- not FUSE, but a real FS</title><link>https://www.reddit.com/r/golang/comments/1ivuz61/writing_a_file_system_in_go_not_fuse_but_a_real_fs/</link><author>/u/Rich-Engineer2670</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 22:31:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I would say I'm crazy, but this both well established and redundant.....Assume I wanted to write my own file system (education), with Golang -- not a fuse variant, but I literally am taking a file on a block device and treating it as a disk. If this were C, OK, I'd do the following:Define a binary boot block at LBA 0Define a certain number of LBAs for boot codeDefine a certain number of LBAs for partitionsWithin each partition define the directories and free lists (FATs, clusters, etc...)Have a bunch of free LBAs.In C, I could define structs and just write them out assuming they were packed. In Go, structs aren't C structs, so I need to constantly convert structs to binaries. Sure, I could use the binary package and a lot functions, but someone must have done this in a better way, or is the "better way" "No, write your file systems in C...."I want to stay in Go, because everything else in the OS is in Go...]]></content:encoded></item><item><title>Golang SQLite admin tool</title><link>https://github.com/joelseq/sqliteadmin-go</link><author>/u/lAdddd</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 21:08:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Observability Made Easy: Adding Logs, Traces &amp; Metrics to FastAPI with Logfire</title><link>https://dev.to/devgeetech/observability-made-easy-adding-logs-traces-metrics-to-fastapi-with-logfire-529l</link><author>Joel Gee Roy</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 20:44:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Picture this: You deploy your shiny new application. Everything looks great in dev, the logs are clean, requests are snappy, and life is good. Thenâ€¦ disaster strikes. A user reports a bug. Another complains about slow response times. You check your logsâ€”wait, where are they? You SSH into the server, tail some logs, guess what went wrong, and hope for the best. Sound familiar?Observabilityâ€”knowing whatâ€™s happening inside your app in real timeâ€”shouldnâ€™t be this hard. But setting up an observability stack often feels like assembling IKEA furniture with missing instructions. Thatâ€™s where  comes in.Pydantic Logfire is a platform that makes it ridiculously easy to add observability to your application, no matter the size. In this post, Iâ€™ll show you how to integrate Logfire into a  app to get instant insights into logs, traces, and metricsâ€”without the usual setup headaches. By the end, youâ€™ll have real-time visibility into whatâ€™s happening under the hood, so you can debug, optimize, and sleep better at night.We will build two services that handles orders and shipping for BigHuge Corp Inc. To keep things straightforward, we'll put both the services in the same repo and run a FastAPI for each of them to emulate two services talking to each other.fastapi-logfire
fastapi-logfire
We'll create a virtual environment and activate it:Let's install the necessary packages:pip  and  services
The ordering service will contain just two endpoints - one to place an order and one to get the details of a specific order. Similarly the shipping service will have two endpoints - one to initiate the shipping process and one to get the status of a shipment. The ordering service will invoke the shipping service in both its endpoints. 
Here's the code for both the services:Now we'll create  and  which will be used to start both the servicesYou can run both services on separate terminals using the  command.fastapi dev 8001 app/main2.py
You can go to  to try the ordering service out and see if everything is working as expected.Before we integrate logfire, you need to create a token from Logfire so that you can write data to the logfire dashboard.  You can read on how to generate the token here. Once you have the token, you can save it as an environment variable in a  file (You might need to install  to use it).We'll start small. We'll integrate logfire into our app such that all logs generated by the app will be sent to the logfire dashboard. We'll integrate logfire into the  standard library. (Tip: you can also emit logs directly using logfire methods like )We've defined a  function that we can call anywhere in our project to send logs.You can add similar statements in the  handler as well.In  and , we'll add the logic to configure logfireDo the same for , but with  as shipping.If you run both the apps again, and try out some requests, you will see some logs displaying on the Live tab of the logfire dashboad:Great! Now we see our logging statements in the dashboard. But Logfire lets us instrument fastapi directly to get even more data for each request. Let's implement that. Under the  in both  and , add a new line of code:Now all your requests to both servers are instrumented automatically.Now all the service level logs are neatly nested under the respective requests. But get this - it can get even better. So we set up our application in such a way the  endpoint will call  endpoint inside it. It would be really nice if our dashboard could display this sequential flow instead of showing both the calls as separate (like we saw before). We can do this using tracing. For tracing to work, context has to be propagated across services. This "context" helps keep track of the parent trace/span of a new span/log so that they can be viewed in tandem.  Thankfully logfire gives us an easy way to do this. Since we're using  to use the  service, we'll install the associated library from logfire.pip We'll update the code in  to add logfire.instrument_requests()We don't need to update  (the shipping one) because we aren't sending any requests in that server for now. will make sure that the traceparent header is automatically set when making requests.  makes sure that the  header is extracted correctly from incoming requests. Thats it. !Run the servers again, and try sending some requests. You'll see something like this in your logfire dashboard:And heavens forbid, if something were to go wrong in one of the services, you now know exactly where it went wrong. Let's test this out:If we try  now, we'll see something like this:Metrics signify how much of something exists. When paired with a time series, we can know the metric value at a point in time. This is useful for observing data like number of requests over a period of time, or things like CPU utilisation over time etc.Setting up system metric tracking is pretty straightforward with logfire, so we'll do that first.pip Now go to the logfire platform on your browser, select the "Dashboards" tab. Click on the "New Dashboard" button. Select "Basic System Metrics (Logfire)" from the drop down.Run the server again and you should be seeing the graph populate with your system data.if you choose "Web Server Metrics" from the create dashboard drop down, you'll get another readymade dashboard containing useful metrics from our services.Now let's add a custom metric to see the orders placed over time. We'll use a counter for this one.Now on the logfire platform, create a new dashboard and choose "Start from scratch" from the dropdown. Click on "Add Chart". We have to retrieve the data we need from logfire using SQL. To get an idea of how to structure your SQL queries, use the "Explore" tab in the logfire platform. To get the total orders placed in a time period we'll use the following query:Set the visualisation type as "Values" and save the chart. Now the chart will get updated based on the time period that you select on top.To chart this in a time series, use the following query:Choose the visualisation as "Time Series", and set the metrics to "scalar_value". In this post, we took a  to setting up observability in a FastAPI app using , covering logging, distributed tracing, and real-time metrics with minimal setup. While we focused on automatic instrumentation, thereâ€™s even more you can exploreâ€”like , which give you fine-grained control over spans and logs for deeper insights. If you enjoyed this post, consider subscribing to my newsletter for more dev-focused deep dives.]]></content:encoded></item><item><title>DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask</title><link>https://www.bloomberg.com/news/articles/2025-02-10/deepseek-could-make-founder-liang-wenfeng-one-of-the-world-s-richest-people?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczOTIzNzk1NywiZXhwIjoxNzM5ODQyNzU3LCJhcnRpY2xlSWQiOiJTUjhYTTdUMEcxS1cwMCIsImJjb25uZWN0SWQiOiI0MUVGMDc3MjI0RTM0MDhFOTNFMDdFQkY0RDc3QzI1QiJ9.kqtC_AK59CyhVfXIjYbRqB5ymi-WS52icc0pzlfX74E</link><author>/u/cramdev</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 20:32:49 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Solving The Millionaires&apos; Problem in Rust</title><link>https://vaktibabat.github.io/posts/smpc_circuits/</link><author>/u/vaktibabat</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 20:24:20 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Official /r/rust &quot;Who&apos;s Hiring&quot; thread for job-seekers and job-offerers [Rust 1.85]</title><link>https://www.reddit.com/r/rust/comments/1ivrkhs/official_rrust_whos_hiring_thread_for_jobseekers/</link><author>/u/DroidLogician</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 19:59:34 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Welcome once again to the official r/rust Who's Hiring thread!Before we begin, job-seekers should also remember to peruse the prior thread.This thread will be periodically stickied to the top of r/rust for improved visibility. You can also find it again via the "Latest Megathreads" list, which is a dropdown at the top of the page on new Reddit, and a section in the sidebar under "Useful Links" on old Reddit.The thread will be refreshed and posted anew when the next version of Rust releases in six weeks.Please adhere to the following rules when posting:Don't create top-level comments; those are for employers.Feel free to reply to top-level comments with on-topic questions.Anyone seeking work should reply to my stickied top-level comment.Meta-discussion should be reserved for the distinguished comment at the very bottom.The ordering of fields in the template has been revised to make postings easier to read. If you are reusing a previous posting, please update the ordering as shown below.Remote positions: see bolded text for new requirement.To find individuals seeking work, see the replies to the stickied top-level comment; you will need to click the "more comments" link at the bottom of the top-level comment in order to make these replies visible.To make a top-level comment you must be hiring directly; no third-party recruiters.One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.Proofread your comment after posting it and edit it if necessary to correct mistakes.To share the space fairly with other postings and keep the thread pleasant to browse, we ask that you try to limit your posting to either 50 lines or 500 words, whichever comes first.We reserve the right to remove egregiously long postings. However, this only applies to the content of this thread; you can link to a job page elsewhere with more detail if you like.Please base your comment on the following template:COMPANY: [Company name; optionally link to your company's website or careers page.]TYPE: [Full time, part time, internship, contract, etc.]LOCATION: [Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.]REMOTE: [Do you offer the option of working remotely? Please state clearly if remote work is restricted to certain regions or time zones, or if availability within a certain time of day is expected or required.]VISA: [Does your company sponsor visas?]DESCRIPTION: [What does your company do, and what are you using Rust for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.]ESTIMATED COMPENSATION: [Be courteous to your potential future colleagues by attempting to provide at least a rough expectation of wages/salary. If you are listing several positions in the "Description" field above, then feel free to include this information inline above, and put "See above" in this field. If compensation is negotiable, please attempt to provide at least a base estimate from which to begin negotiations. If compensation is highly variable, then feel free to provide a range. If compensation is expected to be offset by other benefits, then please include that information here as well. If you don't have firm numbers but do have relative expectations of candidate expertise (e.g. entry-level, senior), then you may include that here. If you truly have no information, then put "Uncertain" here. Note that many jurisdictions (including several U.S. states) require salary ranges on job postings by law. If your company is based in one of these locations or you plan to hire employees who reside in any of these locations, you are likely subject to these laws. Other jurisdictions may require salary information to be available upon request or be provided after the first interview. To avoid issues, we recommend all postings provide salary information. You  state clearly in your posting if you are planning to compensate employees partially or fully in something other than fiat currency (e.g. cryptocurrency, stock options, equity, etc). Do  put just "Uncertain" in this case as the default assumption is that the compensation will be 100% fiat. Postings that fail to comply with this addendum . Thank you.]CONTACT: [How can someone get in touch with you?]]]></content:encoded></item><item><title>25+ Little-Known Python Resources That Will Make You a Pro!</title><link>https://dev.to/dev-resources/25-little-known-python-resources-that-will-make-you-a-pro-51ed</link><author>Dev Resources</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 19:53:51 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[1. Wk 3 Orchestration: MLOPs with DataTalksWe've been introduced to the subject of Machine Learning in Operations, familarized with Experiment... 2. RandomAdjustSharpness in PyTorchBuy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   RandomAdjustSharpness() can randomly... 3. RandomSolarize in PyTorchBuy Me a Coffeeâ˜•  *Memos:    My post explains RandomInvert().  My post explains... 4. RandomAutocontrast in PyTorchBuy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   RandomAutocontrast() can randomly... 5. RandomInvert in PyTorchBuy Me a Coffeeâ˜•  *Memos:    My post explains RandomSolarize().  My post explains... 6. RandomPosterize in PyTorchBuy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   RandomPosterize() can randomly... Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   Grayscale() can convert an image to... Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   CenterCrop() can crop an image,... Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   Resize() can resize an image as... 10. RandomPerspective in PyTorchBuy Me a Coffeeâ˜•  *Memos:    My post explains RandomRotation().  My post explains RandomAffine().  My... 11. 5 Best Programming Languages to Learn: Decoding the FutureIn today's tech-driven world, staying ahead of the curve is essential. As we approach 2025, certain... 12. ðŸš€ Day 3 of #100DaysOfCodingToday, I dived into Linked Lists and learned the basics: âœ… Insertion âœ… Traversal âœ… Deletion âœ…... 13. Building a REST API with Django REST Framework: A Beginners GuideIntroduction   Imagine you're building a book management system where users can browse... 14. RandomAffine in PyTorch (6)Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... 15. Open-Source Book Creator with Multi-Agent AII'm excited to share ** ðŸ“ LibriScribe**, an open-source book creation system I've developed that... 16. Seamlessly Compare Maps on QGIS with the QMapCompare PluginIntroduction   When working with QGIS, you often switch between basemaps, but  comparing... 17. How I built an AI-Powered Code Reviewer (and you can too).AI is flipping the game for developers, and honestly,   I got fed up of seeing people waste hours... 18. RandomAffine in PyTorch (5)Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... 19. RandomAffine in PyTorch (4)Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... 20. RandomAffine in PyTorch (3)Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... 21. What is LangGraph and How to Use It for Building AI AgentsI keep finding myself going back to the LangChain documentation to figure out how to use LangGraph.... 22. How to Create Python Virtual Environments on UbuntuWhen working on different Python projects, it's often necessary to create isolated environments for... 23. RandomAffine in PyTorch (2)Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... 24. Project Translate: The Translate API (Part 4)Hey developers! ðŸ‘‹ For the last post in the series, we'll provision the infrastructure on AWS and... 25. Explore the Future: Trending GitHub Projects Revolutionizing Tech ðŸš€âœ¨ðŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-21   Every week, thousands of... 26. **"ðŸš€ Dive into Innovation: Top Trending GitHub Projects Shaping the Future of AI!"**
ðŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-21   Every week, thousands of... 27. Day-03 of Kapilâ€™s learning python programmingThe things i learned from python are:   1.More i.e. depth in list:  In list i learned about many... Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   FiveCrop() can crop an image into 5... 29. Generate Tailored Cover Letters with AI: A Step-by-Step Guide Using FastAPI and OpenAIIn todayâ€™s fast-paced job market, a personalized cover letter can set you apart. ResumeBurgerâ€™s... 30. What is a RESTful API? A Beginnerâ€™s GuideIntroduction   In todayâ€™s digital world, applications need to communicate seamlessly with... 
  
  
  Earn $100 Fast: AI + Notion Templates
Do you want to make extra money quickly? This guide shows you how to create and sell Notion templates step by step. Perfect for beginners or anyone looking for an easy way to start earning online. Follow a simple process to create templates people want and will buy. Learn to use tools like ChatGPT to design and improve templates. More people are using Notion every day, and they need templates to save time and stay organized. Ready-made prompts to spark ideas and create templates faster. Stay on track as you work. Learn everything from idea to sale.How to Find Popular Ideas: Research trends and needs. Tips for improving templates with AI tools.Making Templates User-Friendly: Simple tips for better design. Advice on sharing and selling on platforms like Gumroad or Etsy. Solutions for issues like low sales or tricky designs.Anyone who wants to make extra money online.People who love using Notion and want to share their ideas.Creators looking for a simple way to start selling digital products.]]></content:encoded></item><item><title>ðŸš€ Go Interface Nil Pitfall: Why Your Nil Check is Failing (and How to Fix It!) ðŸ”</title><link>https://dev.to/mx_tech/go-interface-nil-pitfall-why-your-nil-check-is-failing-and-how-to-fix-it-2ie7</link><author>Moksh</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 19:43:27 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Have you ever encountered a situation where you check for nil, but the function still executes unexpectedly? This is a common pitfall in Go when working with interfaces and nil values. Let's break it down. ðŸ‘‡ðŸ” The Problem: A Failing Nil Check
Imagine we have an interface Notifier with a method Notify().package main

import "fmt"

// Define an interface
type Notifier interface {
    Notify()
}

// Define a struct
type Email struct {
    Address string
}

// Implement the Notify method for Email
func (e *Email) Notify() {
    fmt.Println("Sending email to:", e.Address)
}

// Function that processes Notifier entities
func ProcessNotifiers(notifiers ...Notifier) {
    for _, notifier := range notifiers {
        if notifier == nil {
            fmt.Println("Skipping nil notifier")
            continue
        }
        fmt.Println("Processing:", notifier)
        notifier.Notify()
    }
}

func main() {
    var email *Email  // Declaring a nil pointer of type *Email
    var notifier Notifier = email // Assigning it to an interface

    fmt.Println(notifier == nil)  // âŒ False! (unexpected)

    ProcessNotifiers(notifier) // Will still call Notify() and panic!
}
1ï¸âƒ£ email is a nil pointer, but notifier is NOT nil!When assigning email to notifier, Go stores its type info (*Email) in the interface.This means the interface itself is not nil, even though the underlying value is nil.2ï¸âƒ£ Our if notifier == nil check fails!Even though email is nil, notifier still holds a valid interface value, so notifier == nil returns false.3ï¸âƒ£ Calling notifier.Notify() causes a panic!The method is called on a nil receiver, leading to a runtime error.âœ… The Fix: Proper Nil Checking
Instead of checking if notifier == nil, use reflection to properly detect nil interfaces:import "reflect"

// Properly check if an interface is nil
func isNil(i interface{}) bool {
    if i == nil {
        return true
    }
    v := reflect.ValueOf(i)
    return v.Kind() == reflect.Ptr && v.IsNil()
}

func ProcessNotifiersFixed(notifiers ...Notifier) {
    for _, notifier := range notifiers {
        if isNil(notifier) {
            fmt.Println("Skipping nil notifier")
            continue
        }
        fmt.Println("Processing:", notifier)
        notifier.Notify()
    }
}
âœ… Now, it properly skips the nil notifier and avoids the panic!
 âœ” Interfaces holding nil pointers are NOT nil!
 âœ” Always check for nil pointers inside interfaces properly
 âœ” Use reflection to avoid hidden nil bugsHave you run into this before? Let's discuss in the comments! ðŸ’¬ðŸ‘‡]]></content:encoded></item><item><title>What are the odds that Rust is going to have a real competitor?</title><link>https://www.reddit.com/r/rust/comments/1ivqkj1/what_are_the_odds_that_rust_is_going_to_have_a/</link><author>/u/nikitarevenco</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 19:15:59 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[By "Real Competitor" I mean: A language just like Rust with similar goals, but one that people actually prefer to Rust. So it would be a fast, low-level memory safe language with great tooling, great type system and other benefits that Rust offers. But it would need to be better than Rust to actually catch onThis language needs to offer real advantages over Rust to be considered. Of course since Rust has a huge ecosystem that is growing rapidly, it may take a long time. But I am talking on a timescale of 25+ years.Creating a new programming language to compete with Rust would be a massive undertaking and there would have to be some real reason to do it. Rust may be missing some features like higher-kinded types, named function arguments and such but to really catch on the language would need to offer some extremely important feature that Rust doesn't have, as well as offering all of Rust's benefits at the same time.Is there any such language currently in early development? Or perhaps, what would such a language have to look like?]]></content:encoded></item><item><title>How Portsicle works!</title><link>https://dev.to/amitsuthar69/how-portsicle-works-24k6</link><author>Amit Suthar</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 19:09:40 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Portsicle is a reverse tunneling service that creates a public ingress endpoint for local servers, allowing developers to showcase their locally running applications to clients, stakeholders or to test APIs without deploying.We all know that the most obvious way to allow an inbound traffic to a private network is through port forwarding. But we won't sit and configure our routers every time, we need a better solution.But if we're not forwarding the port, then how can an external packet enter our private network without getting chocked by firewalls? Here's where the concept of Reverse Tunneling comes into the picture. Reverse Tunneling is a technique used to establish a secure connection from a remote server back to a local machine. Portsicle beautifully manipulates this same technique to route traffic to local machine.The idea is to have an intermediary remote (relay) server which will act as a bridge between local machine and the internet traffic. Anyone who wants to access someone's local server, will first visit to the remote server, which will then route the request to the local machine.To achieve this, Portsicle provides a client CLI which connects to the remote server. The user can use the CLI command to initiate the tunnel. This means that the connection is now outbound (established by the local machine) and the firewall will just ignore it. This connection is then upgraded to a secure and persistent websocket tunnel.Once a tunnel is established, the relay server provides a 'Public URL' for that session. Anyone on the internet can now access that particular local server with this public URL and the URL is only valid as long as the client is running along.Steps to get the public url:note that 3000 has to be the port of the local server you wanted to expose.You'll then receive the URL:
â¯ ./portsicle http -p 3000
2025/02/23 00:26:33 Connected to remote server.
2025/02/23 00:26:33 Your public url: https://portsicle.koyeb.app/a355bf62-f7c4-46e9-9d9b-1125d6343b3d
Your local server is accessible with this link!But how the traffic is "forwarded"??? Well, we simply convert the HTTP/HTTPS requests and responses into websocket messages and forward them between local client and the remote server.We handle this forwarding for two cycles:Take plain HTTP request => Convert it to a message object => send it across wire => reconstruct an HTTP request => send it to local server. Client gets a response from local server => convert it to a response object => send it across wire => construct a response.  ]]></content:encoded></item><item><title>Estudos em Python - Objeto iterÃ¡vel</title><link>https://dev.to/douglasamarelo/estudos-em-python-objeto-iteravel-4a98</link><author>Douglas &quot;Amarelo&quot; Lopes</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 18:55:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  1. Objeto IterÃ¡vel (Iterable)
Um objeto iterÃ¡vel Ã© qualquer objeto que pode ser percorrido em um loop for ou utilizado com a funÃ§Ã£o . Em Python, para ser considerado iterÃ¡vel, um objeto deve implementar o mÃ©todo especial  ou o mÃ©todo .Exemplos comuns de objetos iterÃ¡veis incluem , , ,  e atÃ© arquivos.Exemplo de objeto iterÃ¡vel:Uma lista Ã© um tipo de dado composto que pode armazenar uma coleÃ§Ã£o ordenada de itens. Ela Ã© mutÃ¡vel, ou seja, pode ser alterada apÃ³s sua criaÃ§Ã£o. Pode armazenar elementos de diferentes tipos, como nÃºmeros, strings ou atÃ© outras listas.Em Python, "objeto" Ã© um termo genÃ©rico que se refere a qualquer instÃ¢ncia de uma classe. Tudo em Python Ã© um objeto, e um objeto possui atributos e mÃ©todos definidos pela classe Ã  qual pertence.Por exemplo, se vocÃª criar uma classe Pessoa, ao criar uma instÃ¢ncia dessa classe (um objeto), ele terÃ¡ atributos como nome e idade e poderÃ¡ ter mÃ©todos como falar() ou andar().Em Python, o termo "Collection" (coleÃ§Ã£o) Ã© uma forma genÃ©rica de se referir a tipos de dados que armazenam mÃºltiplos itens. Isso inclui listas, tuplas, conjuntos (sets), dicionÃ¡rios, entre outros.As coleÃ§Ãµes sÃ£o usadas para armazenar grupos de elementos e podem ser iterÃ¡veis, mutÃ¡veis ou imutÃ¡veis, dependendo do tipo. O mÃ³dulo collections em Python tambÃ©m oferece tipos de dados especializados como deque e Counter.Exemplo de coleÃ§Ã£o (lista):Um iterador Ã© um objeto que permite percorrer um objeto iterÃ¡vel, mas ao contrÃ¡rio do iterÃ¡vel, ele mantÃ©m o estado de onde estÃ¡ no percurso. Ou seja, um iterador sabe onde ele estÃ¡ no processo de iteraÃ§Ã£o e pode continuar de onde parou.Um iterador implementa os mÃ©todos () (que retorna o prÃ³prio iterador) e () (que retorna o prÃ³ximo item da coleÃ§Ã£o ou levanta uma exceÃ§Ã£o StopIteration quando nÃ£o hÃ¡ mais itens).Quando nÃ£o hÃ¡ mais elementos, o next(iterador) levanta uma exceÃ§Ã£o StopIteration. Qualquer objeto que pode ser percorrido em um loop (como listas ou tuplas). Um tipo de coleÃ§Ã£o que armazena elementos ordenados e mutÃ¡veis. Qualquer instÃ¢ncia de uma classe em Python, com atributos e mÃ©todos. Estruturas que armazenam mÃºltiplos itens (listas, tuplas, sets, dicionÃ¡rios). Um objeto que percorre elementos de um iterÃ¡vel e mantÃ©m o estado da iteraÃ§Ã£o.]]></content:encoded></item><item><title>Scrap Your ORMâ€”Replacing Your ORM With Relational Algebra</title><link>https://youtu.be/SKXEppEZp9M?si=wccXwllXm-0M-zOO</link><author>/u/JohnyTex</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 18:39:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Loss Functions and NN Parameter Accumulation in micrograd</title><link>https://dev.to/shrsv/loss-functions-and-nn-parameter-accumulation-in-micrograd-3c13</link><author>Shrijith Venkatramana</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 18:30:22 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, Iâ€™m building LiveAPI, a tool that makes generating API docs from your code ridiculously easy.
  
  
  A Sample Dataset and Comparing With Predictions
In the following dataset, we have 4 example inputs/outputs. And each input has 3 numbers.We use the neural network (MLP object) defined in last post to do some predictions on each input.When I run this I get some predictions which are either a bit less than required or more:
  
  
  A Skeleton to Build a Loss Function
We define a loss variable, and define in the following way:Find the difference between actual output vs predicted outputSquare the difference (to remove negative sum)Sum all such squares of differences for whole input/output set
Once we have the loss node, we can run backward propagation to get gradients for each node.When we do  we get a huge graph consisting of 4 forward passes corresponding to the 4 examples above and loss on top of those.
  
  
  Gathering All Neural Network Parameters And Operating On Them
Pay attention to the  method in each of the classes below - where we collect the various parameters at every level (neuron, layer, MLP) for convenience:Now when I do  I get all the parameters in the neural network:]]></content:encoded></item><item><title>FFmpeg School of Assembly Language</title><link>https://github.com/FFmpeg/asm-lessons/blob/main/lesson_01/index.md</link><author>/u/mitousa</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 18:00:07 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Spectacular Connection Between LLMs, Quantum Systems, and Number Theory</title><link>https://www.datasciencecentral.com/spectacular-connection-between-llms-quantum-systems-and-number-theory/</link><author>Vincent Granville</author><category>dev</category><category>ai</category><pubDate>Sat, 22 Feb 2025 17:44:20 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] Interpreting Deep Neural Networks: Memorization, Kernels, Nearest Neighbors, and Attention</title><link>https://medium.com/@thienhn97/interpreting-deep-neural-networks-memorization-kernels-nearest-neighbors-and-attention-6bf0cefc7619</link><author>/u/ThienPro123</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 17:15:49 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[This means that our positive kernel is actually some inner product of a Hilbert space. Typically, Mercerâ€™s theorem is used for the kernel trick where we can map our input data to richer feature spaces that are potentially infinite dimensional (e.g. Gaussian kernel, polynomial kernel, etc.). However, in our case, we will use it to interpret the other way around.Note the following relationship between distance in the Hilbert space and the kernel function:This means that the closer x is to y in H , the more similar they will be in our similarity measure. So our intuition of the similarity measure being related to some form of distance is formalized by the relationship above.Learned kernel instead of fixing a kernel a prioriIf something within our prediction model is not learnable, then it is a prior that we are imposing on the dataset and the problem.In our previous discussions of soft-kernelized NNs, the kernel K is fixed, meaning that we have some prior on the geometry of the data. That is not always desirable and we want our methods to automatically learn the structure of the data rather than us manually imposing this geometry.Hence, if we want to learn the kernel K instead of imposing a prior fixed kernel, we can write (due to Mercerâ€™s theorem):for some parameterized feature map Ïˆ : X â†’ H from the data space X to some Hilbert space H. Typically, H will just be R^n or the dimension of the latent space. We can then learn the parameters of Ïˆ via standard training techniques (i.e. gradient descent on some loss).This view allows us to connect standard deep learning (or representation learning) to kernel learning.Interpreting attention as soft nearest neighborsNote that the soft kernelized nearest neighbor that we presented earliercan be interpreted as the popular attention mechanism that is ubiquitous today in LLMs and LVMs via the transformers architecture.If we interpret x as some token, e.g. x_c, in the sequence (x_1, â€¦, x_n), K(x_i , x) as the attention dot product i.e.and setting W_{ci} as the normalized values for token at time i i.e.then we would recover the attention computation (bidirectional or autoregressive depending on whether we set the W_{ci} = 0 for i < c) as being the weighted average of the values of other tokens in the sequence.The representer theorem states that there exists an optimal linear solution that lies in the span of the training data. We shall call span (Ïˆ(x_1), â€¦, Ïˆ (x_n)) the training (examples) feature span.]]></content:encoded></item><item><title>i made a list of Tech EU tech projects! for users interested in privacy and sustainability</title><link>https://github.com/uscneps/Awesome-European-Tech</link><author>/u/uscnep</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 17:10:16 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Gitoxide in February</title><link>https://github.com/GitoxideLabs/gitoxide/discussions/1855</link><author>/u/ByronBates</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 16:47:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I made an MMORPG playable with an API. Use any programming language to control your characters with the API.</title><link>https://www.artifactsmmo.com/</link><author>/u/Muigetsu</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 16:24:00 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Day -04 of learning python(02-22-2025)</title><link>https://dev.to/kapil_kumarshahsonar_ad/day-04-of-learning-python02-22-2025-1696</link><author>KAPIL SHAH</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 16:22:13 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The things i learned form todays python course: It is one most important important function used in python programming.on going depth i found that it is anonymous function used in code .It is mainly used in on line function for example :numbers = [1, 2, 3, 4, 5]
squared = list(map(lambda x: x ** 2, numbers))
print(squared)  # Output: [1, 4, 9, 16, 25]
In above example list is given and in that list using  we found the square of the whole list i.e in .The basic idea used in above example or anyother example or  program is used like : lambda(parameter:expression)or lambda(argumensts:expression)It helps to iterate every single value present in the list.In above example map is used for sqauring  of the value in the list .Also some basic stuffs like how swap values in the list ,more about IDctionary and all .This much only for  today.]]></content:encoded></item><item><title>windows firewall for local Go web app</title><link>https://www.reddit.com/r/golang/comments/1ivmbtd/windows_firewall_for_local_go_web_app/</link><author>/u/jeevanism</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 16:18:35 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Every time I start my Go web app locally on Windows, I get a firewall error (see screenshot). The Windows Firewall blocks it, and I have to manually allow access. Why does this keep happening? Is there a way to fix this permanently?NB : I am unable to attach the screenshot here :( ]]></content:encoded></item><item><title>Dynamic Data Tables Concept in Flask</title><link>https://dev.to/sm0ke/dynamic-data-tables-concept-in-flask-39c7</link><author>Sm0ke</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 16:06:21 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This article explains the Dynamic Data Table Pattern and how it can be used to manage information with ease via pagination, search, and filters without any coding effort. For newcomers, Dynamic Programming is a method for solving complex problems by breaking them down into simpler subproblems.Let's see how the dynamic programming concept applies to Dynamic Data Tables, which is the solution we're trying to achieve in this article. Data Tables represent a structured way of organizing and displaying data in rows and columns with built-in functionality for managing large datasets. Think of it as an enhanced table that provides:Breaking down large datasets into smaller, manageable pagesAllows users to navigate through data without loading everything at onceTypically shows X records per page (e.g., 10, 25, 50 entries)Rows represent individual recordsColumns represent attributes or fieldsHeaders define the structure and can enable sortingSuitable for small datasetsAll records are pulled once on the client sideData is processed locally in the browser or phone applicationDatabase queries fetch only the required recordsReduces memory usage and improves performanceHandles large datasets efficientlyThe goal of our research is to provide the above features out of the box using a minimal configuration and only the dynamic features of Python that can detect and manipulate data at runtime. Let's break down the task into smaller pieces and start applying the dynamic programming principles to our specific use case. Users should be able to activate the dynamic pattern for any model using a simple and intuitive syntax. For this, we can use a map that uses the URL as the key, and the target model as value.The above definition will use the routing "products" to build the dynamic table for the Product Model defined in the apps/models.pyThe path for the model is now used to load the model class and analyze the fields and the type. The model can be analyzed using the  and  helpers as showcased below:If we call the  with the  input, we should get the  returned by the  helper.Having the class is the first step. Next is to get all fields, the associated types, and also all the foreign keys mapped to external types we don't know yet.   The ordinary fields can be pulled from the class definition using this code:The above snippet will return all fields defined using ordinary types like Integer, String, DateTime, and Text. For the Foreign key discovery, the code needs to use the model metadata that saves the distant relationships with other Models. The code that provides all associated FKs is the one below:After solving the FKs case, the next challenge is to detect the ENUMS used by the model for mapping the information into combos. Without this detection, the ENUMS will be mapped to input fields and we might have issues during creation or update.The above code snippet iterates on the model fields and saves all values for each field detected as ENUM. At this point, we've achieved the following:The model is automatically discovered using the URI segment and the configuration mappingFields information (name and type) covers all cases: simple types, ENUMS, foreign keysThe information is injected into the Jinja files to be presented to the userThe page provided by the dynamic data table provides the usual controls like search, show hide columns control, filters and export in CSV format.The item creation page is displayed with as below:For more inputs regarding the concept feel free to contact the team in support or simply download and inspect the code in your local system:]]></content:encoded></item><item><title>COMMON MANUAL TESTING TECHNIQUES</title><link>https://dev.to/saikiran_r_63247e3b241a8e/common-manual-testing-techniques-1ngh</link><author>SAIKIRAN R</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 16:04:35 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hi everyone we will look at the common manual testing techniquesThe client will approach the service provider with some requirements and a plan. Based on that plan, the frontend and backend development teams will work on creating the screens. After development, the testing team will begin testing the process to ensure it functions smoothly and meets the clientâ€™s requirements.
  
  
  What are the types of testing
There are 2 types of testingManual testing is the process of manually checking and verifying the functionality of software by using keyboard, mouse and some testing toolsIn business or in a career, there are process steps to follow to achieve success similarly in testing there are different types of testing to followed based on the scenarioLet me give you a simple example to explain Boundary Value Analysis:Imagine you're at a theme park, and there's a roller coaster ride. In the ticket section, there's a condition: the minimum age to get a ticket is 18, and the maximum age is 50.In this case, the system should not allow a ticket to be issued to anyone under 18 (age â‰¥ 18) or anyone over 50 (age â‰¤ 50).
Boundary Value Analysis involves testing the input values at the edges of the allowable range (i.e., at the boundaries), as these are the most likely to cause errors.Testing the input values at the allowable range is called boundary value analysis testing.In a login page, the user is required to input a username and password. The clientâ€™s requirement is that if the user enters the wrong password three times, the user should be blocked.To test this, we will create a table to test different scenarios, such as entering an invalid username or an invalid password. This helps to ensure that the system behaves correctly in all possible situations.
  
  
  Common manual testing techniques
To test the softwareâ€™s performance under different network conditions.
This ensures the system can handle varying loads, network speeds, and other factors that could affect its performance.To explore the software and verify that all buttons, features, and functions are working correctly before proceeding to the next testing phase.
In exploratory testing, testers interact with the software to identify defects, without following a predefined set of test cases.Unit testing is done by developers to test individual components or functions of the software.
It ensures that each part of the code works as intended before the software is handed off to the testing team. Sometimes, the testing team may also perform unit testing.]]></content:encoded></item><item><title>DSBG, an open-source static site generator built with Go</title><link>https://www.reddit.com/r/golang/comments/1ivl3o0/dsbg_an_opensource_static_site_generator_built/</link><author>/u/tarjano</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 15:24:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[This is my first big project built in Go, primarily to see if I could be as productive with it as I am with Python. I wanted to tackle a non-trivial project, so I aimed to include most of the functionality I envisioned for this type of tool. Here's what DSBG offers: Works with both Markdown and HTML source files.Automatic Tagging & Filtering: Tags are generated from paths, with built-in tag filtering.Client-Side Fuzzy Search: Provides fast search over all content within the browser.Automatic RSS Feed Generation: Easily create RSS feeds for your blog.Watch Mode with Auto-Rebuild: For continuous feedback during development. Includes 3 different themes, with the option to add your own custom CSS. For major social networks. Easily add analytics, comments, and more.The code might not be perfectly idiomatic, so any tips, suggestions, and feedback are very welcome!]]></content:encoded></item><item><title>Wk 3 Orchestration: MLOPs with DataTalks</title><link>https://dev.to/afrologicinsect/wk-3-orchestration-mlops-with-datatalks-2057</link><author>Akan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 15:00:02 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[We've been introduced to the subject of Machine Learning in Operations, familarized with Experiment Tracking with MLflow in the past week, now we work with orchestration tools, where we can manage all our processes all the way to deployment of the models - think CI/CD.. Data EngineeringHere, we use the  orchestration tool.This is going to a bit technical, but please follow-through.
There are a few ways to set-up Mage, however, the safest is by using the containerized method - through Docker. Remember to create a separate folder for this week's assignment, just to be organized.0.1 First launch the Docker Engine then from your terminal, or preferably git bash run:docker pull mageai:mageai:latest
We would start to see some logs, wait for this to complete - then you can0.2 Clone quickstart filesgit clone https://github.com/mage-ai/compose-quickstart.git mage-quickstart
cd mage-quickstart
cp dev.env .env
rm dev.env
Update your requirements.txt file with these packages:boto3
fastparquet
graphviz
hyperopt
jupyter
mlflow==2.12.1
pandas
scikit-learn
seaborn
shap
xgboost
This sequence of commands is used to set up a local development environment for a new Mage project:git clone https://github.com/mage-ai/compose-quickstart.git: This command clones the repository from the provided URL into  directory on your local machine. navigates into the directory.: Copies the  file to a new file named , which is used for environment configuration.: Removes the  file, which is no longer needed after copying its contents to .Now, we are ready to face the questions
1.1 Run Mage with Docker Compose.
To do this all we need to do is run  - but not yet.
1.2 What's the version of Mage we run?
We will get this from the page's ui, still, hold on.Question 2. Creating a project
2.1 Create Project Name
Create a new project called "homework_03" - How many lines are in the created metadata.yaml file?Yes, so there are many ways to create a project but since we are yet to launch the servers, let's make this by simply tweaking the  variable in the  file - like so:
2.2 Launch Server: Starts up all the services defined in the  file using Docker Compose. This will build and run the containers necessary for the project.We should start to receive some logs indicating that the server is up on , visit the page and we would have version :And two new folders storing these actions:
2.3 Lines in metadata
To get the number of lines in the  file simply run:wc -l homework_03/metadata.yaml => 55 homework_03/metadata.yamlQuestion 3. Creating a pipeline
3.1 Create Standard Pipeline
Click on the Pipeline Icon, start a new  Pipeline and fill out the details.3.2 Load/Ingest Data takes us to this page:
Select  > Python > API, fill out the required details.We've created our first block! It gets fun from here. We can see some default ingestion code in our block - which we would modify into the this:import io
import pandas as pd
import requests
if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@data_loader
def load_data_from_api(*args, **kwargs):
    """
    Template for loading data from API
    """
    taxi_type = "yellow_tripdata"
    year = "2023"
    month = "03"

    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_{year}-{month}.parquet'
    response = requests.get(url)

    ## Data Types
    taxi_dtypes = {
        'VendorID': 'Int64',
        'store_and_fwd_flag': 'str',
        'RatecodeID': 'Int64',
        'PULocationID': 'Int64',
        'DOLocationID': 'Int64',
        'passenger_count': 'Int64',
        'trip_distance': 'float64',
        'fare_amount': 'float64',
        'extra': 'float64',
        'mta_tax': 'float64',
        'tip_amount': 'float64',
        'tolls_amount': 'float64',
        'ehail_fee': 'float64',
        'improvement_surcharge': 'float64',
        'total_amount': 'float64',
        'payment_type': 'float64',
        'trip_type': 'float64',
        'congestion_surcharge': 'float64'
    }

    parse_dates_taxi = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']

    df = pd.read_parquet(io.BytesIO(response.content))

    ## Convert data types
    for col, dtype in taxi_dtypes.items():
        if col in df.columns:
            df[col] = df[col].astype(dtype)

    ## Parse Dates
    for col in parse_dates_taxi:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col])

    row_count = df.shape[0]
    print(f'Total Number of rows retrieved: {row_count}')

    return df


@test
def test_output(output, *args) -> None:
    """
    Template code for testing the output of the block.
    """
    assert output is not None, 'The output is undefined'
The Python script is designed to load and process data from an API. Here's a breakdown of what it does:Import necessary libraries: It imports , , , and some decorators ( and ) from mage_ai.data_preparation.decorators.Define the  function: This function is decorated with , which suggests that it's used for loading data. The function does the following:Constructs a URL to fetch a specific  file from a cloud storage. The file name is constructed using , , and .Sends a GET request to the constructed URL and receives the response.Defines the data types () for each column in the dataset.Defines the columns () that need to be parsed as dates.Reads the  file from the response content into a pandas DataFrame ().Converts the data types of the columns in the DataFrame as per .Parses the date columns in the DataFrame as per .Prints the total number of rows retrieved.Define the  function: This function is decorated with , which is used for testing. The function checks if the output of the block ( function) is not .Now, click on the  button and wait for the check mark on your trail.We added this line print(f'Total Number of rows retrieved: {row_count}') to print out the rows in the data: => 3403766Question 4. Data preparation
Here and just like before, we add a _transformer _ block then modify the script like this:if 'transformer' not in globals():
    from mage_ai.data_preparation.decorators import transformer
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@transformer
def transform(data, *args, **kwargs):
    """
    Template code for a transformer block.

    Add more parameters to this function if this block has multiple parent blocks.
    There should be one parameter for each output variable from each parent block.

    Args:
        data: The output from the upstream parent block
        args: The output from any additional upstream blocks (if applicable)

    Returns:
        Anything (e.g. data frame, dictionary, array, int, str, etc.)
    """
    # Specify your transformation logic here

    data['duration'] = data.tpep_dropoff_datetime - data.tpep_pickup_datetime
    data.duration = data.duration.dt.total_seconds() / 60

    data = data[(data.duration >= 1) & (data.duration <= 60)]

    categorical = ['PULocationID', 'DOLocationID']
    data[categorical] = data[categorical].astype(str)

    row_count = data.shape[0]
    print(f'Total Number of rows in transformed data: {row_count}')

    return data


@test
def test_output(output, *args) -> None:
    """
    Template code for testing the output of the block.
    """
    assert output is not None, 'The output is undefined'
This script is designed to transform data, specifically the data loaded from the previous block. Here's a breakdown of what it does:Import necessary decorators: It imports  and  decorators from mage_ai.data_preparation.decorators if they are not already in the global scope.Define the  function: This function is decorated with , which suggests that it's used for transforming data. The function does the following:Adds a new column  to the DataFrame . This column is calculated as the difference between  and , converted to minutes.Filters the DataFrame to only include rows where  is between 1 and 60 minutes.Converts the data types of the columns  and  to string.Prints the total number of rows in the transformed data.Returns the transformed DataFrame.Define the  function: This function is decorated with , which suggests that it's used for testing. The function checks if the output of the block (presumably the  function) is not .The size of the result: 3316216Question 5. Train a model
Yet again we build another  block to train a Regression Model:## Linear Regression Model
from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

if 'transformer' not in globals():
    from mage_ai.data_preparation.decorators import transformer
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@transformer
def transform(data, *args, **kwargs):
    """
    Template code for a transformer block.

    Add more parameters to this function if this block has multiple parent blocks.
    There should be one parameter for each output variable from each parent block.

    Args:
        data: The output from the upstream parent block
        args: The output from any additional upstream blocks (if applicable)

    Returns:
        Anything (e.g. data frame, dictionary, array, int, str, etc.)
    """
    # Specify your transformation logic here

    dicts_train = data[['PULocationID', 'DOLocationID']].to_dict(orient='records')
    vec = DictVectorizer(sparse=True)
    feature_matrix = vec.fit_transform(dicts_train)

    # Use `transform` not `fit_transform` the validation data according to the feature space learned from the training data
    feature_matrix_val = vec.fit_transform(dicts_train)
    print(f"Dimension of feature_matrix: {feature_matrix_val.shape} \n")

    ## Target Var
    y = data['duration']

    ## Fit Linear Regression Model
    model = LinearRegression()
    model.fit(feature_matrix, y)

    ## Print Model's intercept
    intercept = model.intercept_
    print(f'Linear Regression Model Intercept: {intercept}')

    return model, vec


@test
def test_output(output, *args) -> None:
    """
    Template code for testing the output of the block.
    """
    assert output is not None, 'The output is undefined'
This Python script is designed to transform data and fit a Linear Regression model:Import necessary libraries and decorators: It imports  from sklearn.feature_extraction,  from ,  from , and  and  decorators from mage_ai.data_preparation.decorators if they are not already in the global scope.Define the  function: This function is decorated with , which suggests that it's used for transforming data. The function does the following:Converts the  and  columns of the DataFrame  to a list of dictionaries ().Initializes a  () and fits and transforms  into a feature matrix.Fits and transforms  again into a validation feature matrix (). Note: This seems to be a mistake. It should be transforming validation data, not the training data again.Sets  as the  column of .Fits a  model () on the feature matrix and .Prints the intercept of the model.Returns the model and the vectorizer.Linear Regression Model Intercept: 24.774803905297286Question 6. Register the model
Here, we would have to make some configurations to the  file to enable us connect to the MLflow servers - follow the next steps:6.1 Stop server
Ctrl + C or :Container mage-quickstart-magic-1  Stopping6.2 Create Dockerfile for MLflow:FROM python:3.10-slim

RUN pip install mlflow==2.12.1

EXPOSE 5000

CMD [ \
    "mlflow", "server", \
    "--backend-store-uri", "sqlite:///home/mlflow/mlflow.db", \
    "--default-artifact-root, ./artifacts", \
    "--host", "0.0.0.0", \
    "--port", "5000" \
]
6.2.2 Modify docker-compose yaml
As seen on the homework open the  file and make the changes, the file should now look like this:version: '3'
services:
  magic:
    image: mageai/mageai:latest
    command: mage start ${PROJECT_NAME}
    env_file:
      - .env
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      USER_CODE_PATH: /home/src/${PROJECT_NAME}
      ENV: ${ENV}
    ports:
      - 6789:6789
    volumes:
      - .:/home/src/
    restart: on-failure:5
  mlflow:
    build:
      context: .
      dockerfile: mlflow.dockerfile
    ports:
      - "5000:5000"
    volumes:
      - "${PWD}/mlflow:/home/mlflow/"
    networks:
      - app-network
networks:
  app-network:
    driver: bridge
6.2.3 Run 
This will start up the server and set-up your new requirements - it might take a while to complete, check that both MLflow and Mage are running. NB. For the Author mlflow worked only on  and not on .6.3 Data Explorer
Create a Generic  and we pull the generated model and vectorizer from the previous  and like the assignment requires push the results to mlflow.Paste the following in your exporter block:import os
import pickle
import click
import mlflow

from mlflow.entities import ViewType
from mlflow.tracking import MlflowClient
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

HPO_EXPERIMENT_NAME = "Linear-Regression"
EXPERIMENT_NAME = "sklearn-track-models"

mlflow.set_tracking_uri("http://127.0.0.1:5000")
mlflow.set_experiment(EXPERIMENT_NAME)
mlflow.sklearn.autolog()

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


@data_exporter
def export_data(data, *args, **kwargs):
    """
    Exports data to some source.

    Args:
        data: The output from the upstream parent block
        args: The output from any additional upstream blocks (if applicable)

    Output (optional):
        Optionally return any object and it'll be logged and
        displayed when inspecting the block run.
    """
    # Start an MLflow run
    with mlflow.start_run():

        # Log the model
        mlflow.sklearn.log_model(data['model'], "model")

        # Save the DictVectorizer as an artifact
        vec = data['vec']
        artifact_path = "dict_vectorizer"
        vec_path = os.path.join(artifact_path, "vec.pkl")
        joblib.dump(vec, vec_path)
        mlflow.log_artifact(vec_path, artifact_path)
Now you are on track:That's it!
Visit wk3_submission to review the codes and Cheers!
Comment below if there are any issues.]]></content:encoded></item><item><title>What is Saga Pattern in Distributed Systems?</title><link>https://newsletter.scalablethread.com/p/what-is-saga-pattern-in-distributed</link><author>/u/scalablethread</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 14:43:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[data consistencyThe Saga pattern is a design pattern that helps manage transaction updates across multiple services by breaking them down into a sequence of small local transactional updates, called "saga steps" or "subtransactions." Each step represents a unit of work that interacts with a single service. Once a step is completed, it triggers the next step in the sequence. If any step fails, the saga executes compensating updates to undo the changes made by the previous steps, ensuring that the system returns to its initial state.There are two main approaches to implementing the Saga Pattern: Orchestration and Choreography.In this approach, a central orchestrator service coordinates the saga steps. The orchestrator tells each service when to execute its local transaction. It maintains the state of the saga and handles any failures by invoking compensating transactions. The orchestrator knows the entire saga flow.The client initiates the saga by communicating with the orchestrator. The orchestrator then invokes the first service. Upon successful completion, the orchestrator moves to the next step, invoking the corresponding service. If a service fails, the orchestrator triggers compensating transactions in reverse order.In the Choreography approach, there is no central coordinator. Instead, each service involved in the saga knows its role and communicates with the other services through events or messages. Each service listens for specific events and performs local transactions when the appropriate event is received. The saga flow is distributed across the services.The client initiates the saga by communicating with the first service. This service performs its transaction and publishes an event. Other services, listening for this event, perform their respective transactions and publish their events. This chain reaction continues until the saga is complete. If a service fails, it publishes a compensating event, triggering other services to execute compensating transactions.Choreography has no single point of failure, as each service manages its part of the saga.Orchestration provides simplified error handling and monitoring with centralized control. In contrast, each service needs to handle its errors in Choreography, which can lead to complex error-handling logic.In Orchestration, the coordinator needs to know about all the services involved in the saga, which can lead to tight coupling. In contrast, in Choreography, services need to agree on the events and the order of transactions, which can lead to overhead in coordination.If you enjoyed this article, please hit the â¤ï¸ like button.If you think someone else will benefit from this, then please ðŸ” share this post.]]></content:encoded></item><item><title>[R] Calculating costs of fine tuning an Vision Language Model</title><link>https://www.reddit.com/r/MachineLearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/</link><author>/u/thekarthikprasad</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 14:21:21 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hello guys, I need help in calculating the cost of fine-tuning a VL model. My image dataset is of size 80+gb (https://huggingface.co/datasets/RussRobin/SpatialQA) The VL model is InternVL's 2B model I am confused about whether to do a full parameter/QLoRA Finetuning. I can't spend more on this, but wish to check the results.If so I could, what would be the cost estimate, also how to estimate cost in general Can I sample the dataset, if it breaks my cost bound and still see the results? Also do suggest the best and cheapest compute platform for my case. Thanks in advance.]]></content:encoded></item><item><title>Show HN: I Built a Visual Workflow Automation Platform â€“ FlowRipple</title><link>https://flowripple.com/</link><author>shivsarthak34</author><category>dev</category><category>hn</category><pubDate>Sat, 22 Feb 2025 14:20:04 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Watch how easy it is to build powerful automation workflows using our visual builder. Drag, drop, and connect nodes to create your perfect workflow.]]></content:encoded></item><item><title>Rustaceans, What are your thoughts on Gleam?</title><link>https://www.reddit.com/r/rust/comments/1ivjcus/rustaceans_what_are_your_thoughts_on_gleam/</link><author>/u/nikitarevenco</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 14:00:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've been writing Rust for a couple months. I absolutely love its monads like Result and Option, pattern-matching, private-by-default, the friendly compiler and its type system. I took a quick look at Gleam and it seems to have those features as well. Its syntax heavily reminds me of Rust's, the major distinction is that Gleam is much higher level (No lifetimes, for example), and also it is a purely functional language. It is still relatively new.For those who have tried it, what do you think about it? Are there situations where you will prefer Gleam over Rust and why. ]]></content:encoded></item><item><title>Almost everyone is under-appreciating automated AI research</title><link>https://www.reddit.com/r/artificial/comments/1ivja6c/almost_everyone_is_underappreciating_automated_ai/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 13:57:12 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>RandomAutocontrast in PyTorch</title><link>https://dev.to/hyperkai/randomautocontrast-in-pytorch-1311</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 13:44:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is inverted or not.The 1st argument is (Required-Type: or ()):
*Memos:

]]></content:encoded></item><item><title>RandomSolarize in PyTorch</title><link>https://dev.to/hyperkai/randomsolarize-in-pytorch-mc5</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 13:40:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomSolarize() can randomly solarize an image with a given probability as shown below:The 1st argument for initialization is (Required-Type: or ). *All pixels equal or above this value are inverted.The 2nd argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is solarized or not.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>RandomInvert in PyTorch</title><link>https://dev.to/hyperkai/randominvert-in-pytorch-40pa</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 13:34:53 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is inverted or not.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>RandomAdjustSharpness in PyTorch</title><link>https://dev.to/hyperkai/randomadjustsharpness-in-pytorch-dd8</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 13:31:23 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Required-Type: or ):
*Memos:

 gives a blurred image. gives an original image. gives a sharpened image.The 2nd argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is solarized or not.The 1st argument is (Required-Type: or ()):
*Memos:

]]></content:encoded></item><item><title>RandomPosterize in PyTorch</title><link>https://dev.to/hyperkai/randomposterize-in-pytorch-24d2</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 13:25:14 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomPosterize() can randomly posterize an image with a given probability as shown below:The 1st argument for initialization is (Required-Type:):
*Memos:

It's the number of bits to keep for each channel.The 2nd argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is posterized or not.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>Grayscale in PyTorch</title><link>https://dev.to/hyperkai/grayscale-in-pytorch-19og</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 12:45:10 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Grayscale() can convert an image to grayscale as shown below:The 1st argument for initialization is (Optional-Default:-Type:). *It must be  or .The 1st argument is (Required-Type: or ()):
*Memos:

]]></content:encoded></item><item><title>Understanding the init Function in Go: Purpose, Execution, and Best Practices</title><link>https://dev.to/abstractmusa/understanding-the-init-function-in-go-purpose-execution-and-best-practices-2i9k</link><author>Md Abu Musa</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 12:33:36 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In Go, the  is a special function that is automatically executed  when a package is initialized. It is primarily used for , such as initializing global variables, opening database connections, or registering dependencies.Key Characteristics of  Function:
No Arguments & No Return Value â€“ The  function does not take parameters or return values. â€“ It runs before  and does not require explicit invocation.Can Have Multiple  Functions â€“ A package can have multiple  functions, even across different files.Executed in Declaration Order â€“ If multiple  functions exist in a package, they are executed in the order in which they appear.Example 1: Basic  UsageInitializing...
Main function running...
âœ… The  function runs .Example 2: Using  for Global Variable InitializationExample 3:  in Multiple FilesðŸ“Œ If a package has multiple files, all  functions run , in the order they appear.Output (execution order is preserved):Init from a.go
Init from b.go
Main function running...
Example 4:  in a Different PackageOutput (Package-Level Execution Order)Initializing utils package...
Initializing main package...
Main function running...
Hello from utils!
âœ… The  function in imported packages runs before the  function in .Initializing global variables.Setting up logging configurations.Registering dependencies (e.g., database connections).Ensuring required setup before  runs.Complex logic (prefer explicit initialization in ).Business logic (should be in  or other functions). runs automatically before .It has  and .Each package can have multiple  functions. in  runs before  in .]]></content:encoded></item><item><title>The Efficiency Paradox: How to Save Yourself &amp; the World â€¢ Holly Cummins</title><link>https://youtu.be/dU_WHead0oY</link><author>/u/goto-con</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 12:29:40 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>API Application Monitoring - OpenTelemetry? Or something else?</title><link>https://www.reddit.com/r/golang/comments/1ivhm7y/api_application_monitoring_opentelemetry_or/</link><author>/u/_nullptr_</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 12:25:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am writing a few different gRPC and HTTP (via gRPC Gateway) API servers for various heavy financial compute/IO operations (trading systems and market data). I am doing this as a single developer. These are mostly for me as a hobbyist, but may become commercial/cloud provided at some point with a nice polished UI frontend.Given the nature of the applications, I want to know what is "going on" and be able to troubleshoot performance bottlenecks as they arise, see how long transactions take, etc. I want to standardize the support for this into my apiserver package so all my apps can leverage and it isn't an afterthought. That said, I don't want some huge overhead either, but just want to know the performance of my app when I want to (and not when I don't). I do think I want to instrument with logs, trace and metrics after thinking what each would give me in value.Right now I am leaning towards just going full OpenTelemetry knowing that it is early and might not be fully mature, but that it likely will over time. I am thinking I will use stdlib  for logs with Otel handler only when needed else default to basic stdout handler. Do I want to use otel metrics/tracing directly? I am also thinking I want these others sent to a  handler by default (even stdout is too much noise), and only to a collector when configured at runtime. Is that possible with the Go Otel packages? Does this seem like the best strategy? How does stdlib  play into this? or doesn't it? Other ideas?]]></content:encoded></item><item><title>CenterCrop in PyTorch</title><link>https://dev.to/hyperkai/centercrop-in-pytorch-16he</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 12:09:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[CenterCrop() can crop an image, centering on it as shown below:The 1st argument for initialization is (Required-Type: or () or size()):
*Memos:

A tuple/list must be the 1D with 1 or 2 elements.A single value( or ()) means .The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>Resize in PyTorch</title><link>https://dev.to/hyperkai/resize-in-pytorch-4nld</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 11:55:55 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Resize() can resize an image as shown below:The 1st argument for initialization is (Required-Type:, () or size()):
*Memos:

 can be explicitly set to it only if  isn't .A tuple/list must be the 1D with 1 or 2 elements.A single value( or ()) is applied to a smaller image's width or height edge, then the other larger width or height edge is also resized:
*Memos:If an image's width is smaller than its height, it's [size * height / width, size].If an image width is larger than its height, it's [size, size * width / height].If an image width is equal to its height, it's .The 2nd argument for initialization is (Optional-Default:InterpolationMode.BILINEAR-Type:InterpolationMode).The 3rd argument for initialization is (Optional-Default:-Type:):
*Memos:

It's only supported if  is a single value( or ()).After  is applied if a larger image's width or height edge exceeds it, it's applied to a larger image's width or height edge to limit the image size, then the other smaller image's width or height edge also becomes smaller than before.The 4th argument for initialization is (Optional-Default:-Type:). *Even if setting  to it, it's always  if  is InterpolationMode.BILINEAR or InterpolationMode.BICUBIC.The 1st argument is (Required-Type: or (, ,  or )):
*Memos:

]]></content:encoded></item><item><title>5 Best Programming Languages to Learn: Decoding the Future</title><link>https://dev.to/anemnavinrao/5-best-programming-languages-to-learn-decoding-the-future-499l</link><author>Navin Rao âœï¸</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 11:50:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In today's tech-driven world, staying ahead of the curve is essential. As we approach 2025, certain programming languages are gaining traction, offering exciting opportunities for developers. Whether you're a seasoned coder or just starting, understanding these languages can be a game-changer. So, what's the buzz about the best programming languages to learn in 2025? Let's dive in!
  
  
  1. Python: The Versatile Vanguard

Python's simplicity and readability have made it a favorite among developers. Its extensive libraries and frameworks make it ideal for web development, data analysis, artificial intelligence, and more. In 2025, Python continues to dominate due to its versatility and the growing demand for AI and machine learning applications. Python's straightforward syntax makes it beginner-friendly. From NumPy for data science to Django for web development, Python has it all. A vast community ensures continuous growth and support. Frameworks like Django and Flask streamline the process. Libraries such as Pandas and Matplotlib are industry standards. Tools like TensorFlow and PyTorch are built on Python.
JavaScript remains the cornerstone of web development. With the rise of frameworks like React and Angular, it's more powerful than ever. In 2025, JavaScript's role in building dynamic and interactive web applications solidifies its position as a must-learn language.Asynchronous Programming: Handles multiple tasks efficiently. Ideal for interactive web pages. Can be used on both client and server sides. Frameworks like React and Angular enhance user interfaces. Node.js allows for server-side scripting. Tools like React Native enable cross-platform apps.
Developed by Google, Go is known for its efficiency and scalability. It's particularly suited for cloud services and microservices architectures. In 2025, Go's performance and simplicity make it a top choice for developers aiming for high-performance applications. Built-in support for concurrent programming. Minimalistic design for easy readability. Compiled language with fast execution. Ideal for building scalable cloud applications. Efficient for developing microservices architectures.Command-Line Tools: Great for creating fast and reliable CLI tools.
  
  
  4. Rust: The Safe Systems Language

Rust offers memory safety without sacrificing performance. It's gaining popularity for system-level programming and is being adopted by major tech companies. In 2025, Rust's focus on safety and performance makes it a compelling choice for developers. Prevents common bugs like null pointer dereferencing. Safe concurrency without data races. Comparable to C and C++ in speed. Ideal for operating systems and embedded systems. Used for high-performance web applications. Popular in blockchain development for its safety features.
  
  
  5. Kotlin: The Modern Java Alternative

Kotlin is a modern, expressive language that runs on the Java Virtual Machine (JVM). It's officially supported for Android development and is gaining traction in other areas. In 2025, Kotlin's concise syntax and interoperability with Java make it a top choice for developers. Reduces boilerplate code. Seamless integration with Java. Prevents null pointer exceptions. Officially supported for Android apps. Frameworks like Ktor enable server-side development. Can be used with JavaFX for desktop apps.
  
  
  6. C++: The High-Performance Heavyweight
C++ offers unparalleled performance, making it suitable for system programming and applications requiring high efficiency. 
PLURALSIGHT.COM Direct access to hardware resources. Optimized for speed and efficiency. Supports object-oriented programming principles. Used in developing high-performance games. Ideal for programming microcontrollers. Employed in high-frequency trading platforms.Choosing the right programming language in 2025 depends on your career goals and interests. Python's versatility, JavaScript's web dominance, Java's enterprise strength, C++'s performance, and Go's cloud capabilities each offer unique advantages. Assess your aspirations and the industry demands to make an informed decision. Remember, the best language to learn is one that aligns with your passion and career objectives.
  
  
  Frequently Asked Questions (FAQs)
1. Which programming language should I learn first in 2025?
If you're new to programming, Python is an excellent starting point due to its simplicity and wide range of applications. 2. Is JavaScript still relevant in 2025?
Absolutely! JavaScript remains essential for web development, powering interactive websites and applications. 3. What are the career prospects for learning C++?
C++ is highly valued in fields like game development, embedded systems, and high]]></content:encoded></item><item><title>RandomPerspective in PyTorch</title><link>https://dev.to/hyperkai/randomperspective-in-pytorch-3i1g</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 11:43:29 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It can do perspective transformation.The 2nd argument for initialization is (Optional-Default:-Type: or ):
*Memos:

It's the probability of whether an image is done with perspective transformation or not.The 3rd argument for initialization is (Optional-Default:InterpolationMode.BILINEAR-Type:InterpolationMode).The 4th argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can change the background of an image. *The background can be seen when doing perspective transformation for an image.A tuple/list must be the 1D with 1 or 3 elements.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>OpenAI bans Chinese accounts using ChatGPT to edit code for social media surveillance</title><link>https://www.engadget.com/ai/openai-bans-chinese-accounts-using-chatgpt-to-edit-code-for-social-media-surveillance-230451036.html</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 11:24:23 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[OpenAI has banned the accounts of a group of Chinese users who had attempted to use ChatGPT to debug and edit code for an AI social media surveillance tool, the company . The campaign, which OpenAI calls Peer Review, saw the group prompt ChatGPT to generate sales pitches for a program those documents suggest was designed to monitor anti-Chinese sentiment on X, Facebook, YouTube, Instagram and other platforms. The operation appears to have been particularly interested in spotting calls for protests against human rights violations in China, with the intent of sharing those insights with the country's authorities."This network consisted of ChatGPT accounts that operated in a time pattern consistent with mainland Chinese business hours, prompted our models in Chinese, and used our tools with a volume and variety consistent with manual prompting, rather than automation," said OpenAI. "The operators used our models to proofread claims that their insights had been sent to Chinese embassies abroad, and to intelligence agents monitoring protests in countries including the United States, Germany and the United Kingdom."According to Ben Nimmo, a principal investigator with OpenAI, this was the first time the company had uncovered an AI tool of this kind. "Threat actors sometimes give us a glimpse of what they are doing in other parts of the internet because of the way they use our AI models," Nimmo told .Much of the code for the surveillance tool appears to have been based on an open-source version of one of Meta's . The group also appears to have used ChatGPT to generate an end-of-year performance review where it claims to have written phishing emails on behalf of clients in China."Assessing the impact of this activity would require inputs from multiple stakeholders, including operators of any open-source models who can shed a light on this activity," OpenAI said of the operation's efforts to use ChatGPT to edit code for the AI social media surveillance tool.Separately, OpenAI said it recently banned an account that used ChatGPT to generate social media posts critical of , a Chinese political scientist and dissident who lives in the US in exile. The same group also used the chatbot to generate articles in Spanish critical of the US. These articles were published by "mainstream" news organizations in Latin America and often attributed to either an individual or a Chinese company.]]></content:encoded></item><item><title>Distributed system courses in Rust?</title><link>https://www.reddit.com/r/rust/comments/1ivgbko/distributed_system_courses_in_rust/</link><author>/u/FeelingAttempt55</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 11:00:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am currently following the pingcap course to learn distributed systems with Rust. So far, I am really enjoying the course, but the course is 5 years old, could you guys suggest some other project-based and more up-to-date courses? ]]></content:encoded></item><item><title>Ensuring Interface Implementation at Compile Time in Go ðŸ› ï¸</title><link>https://dev.to/abgeo/ensuring-interface-implementation-at-compile-time-in-go-3366</link><author>Temuri Takalandze</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 10:43:52 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In Go, we canâ€™t implicitly implement interfaces like in some other languages. To ensure a type implements an interface, we explicitly check this by trying to cast the type to the interface. If the type doesnâ€™t match, Go will give a compile-time error, and the code wonâ€™t compile ðŸš«This compile-time check helps catch errors early, making sure your types conform to the expected interfaces. If Task doesnâ€™t implement Executor properly, Go wonâ€™t compile the code, saving you from potential issues at runtime ðŸ‘¨â€ðŸ’»]]></content:encoded></item><item><title>Dockerize GO environment with only go.mod and go.sum and no source code</title><link>https://www.reddit.com/r/golang/comments/1ivfxtb/dockerize_go_environment_with_only_gomod_and/</link><author>/u/muthunatsharma</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 10:32:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I need a docker container which has go packages downloaded, installed and compiled as mentioned in go.mod and go.sum. All the articles show how to do it but the install/compile actually happens only when the source-code is copied in to the container and "go build" is run in the dockerfile.I see "go download" downloads all pkgs in go.mod to /go/mod/pkg. How do I install these? I can give "go install <pkg>" but that would mean I need to update my Dockerfile each time a new pkg is added to go.mod.What is the one-shot way of installing it in the dockerfile build?Edit: The context is to build a dev container where deps are pre-built saving time when code is mounted on the container and built -- this is the main point to save time. The container wouldn't have the app itself. Only the dependencies fully installed and serve as a standard environment to run.]]></content:encoded></item><item><title>godoc.nvim - Golang docs inside Neovim!</title><link>https://www.reddit.com/r/golang/comments/1ivfv16/godocnvim_golang_docs_inside_neovim/</link><author>/u/ffredrikk</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 10:27:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Protobuf Should Dominate the Data Format Ecosystem</title><link>https://dev.to/leapcell/why-protobuf-should-dominate-the-data-format-ecosystem-4ddd</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 09:42:07 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Protobuf (Google Protocol Buffers), as defined in the official documentation: Protocol buffers is a language-independent, platform-independent, and extensible method for serializing structured data, which can be widely applied in scenarios such as data communication protocols and data storage. It is a tool library provided by Google with an efficient protocol data exchange format, possessing the characteristics of flexible, efficient, and automated structured data serialization mechanisms.Compared with XML, the size of data encoded by Protobuf is smaller, and the encoding and decoding speed is faster. Compared with Json, Protobuf performs more excellently in conversion efficiency, with both its time efficiency and space efficiency reaching 3 to 5 times that of JSON.As the official description states: â€œProtocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data â€“ think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.â€
  
  
  Comparison of Data Formats
Suppose we have a  object, represented by JSON, XML, and Protobuf respectively, and let's see their differences.John24Protobuf directly represents data in binary format, which is not as intuitive as XML and JSON formats. For example:[10 6 69 108 108 122 111 116 16 24]

  
  
  Good Performance/High Efficiency
: The overhead of XML formatting (serialization) is acceptable, but the overhead of XML parsing (deserialization) is relatively large. Protobuf has optimized this aspect and can significantly reduce the time overhead of serialization and deserialization.: Protobuf also greatly reduces the space occupation.
  
  
  Code Generation Mechanism
For example, write the following content similar to a structure:Protobuf can automatically generate the corresponding  file and  file, and encapsulate the operations on the structure  into a class.
  
  
  Support for Backward Compatibility and Forward Compatibility
When the client and the server use a protocol simultaneously, if the client adds a byte in the protocol, it will not affect the normal use of the client.
  
  
  Support for Multiple Programming Languages
In the source code officially released by Google, it includes support for multiple programming languages, such as:
  
  
  Disadvantages of Protobuf

  
  
  Poor Readability Due to Binary Format
To improve performance, Protobuf uses a binary format for encoding, which makes the data less readable and will affect the efficiency during the development and testing phase. However, under normal circumstances, Protobuf performs very reliably, and serious problems generally do not occur.Generally, XML is self-descriptive, while the Protobuf format is not. It is a piece of binary format protocol content, and it is difficult to know its function without matching it with a pre-written structure.Although Protobuf supports serialization and deserialization in multiple languages, it is not a universal transmission standard across platforms and languages. In scenarios of multi-platform message passing, its compatibility with other projects is not good, and corresponding adaptation and transformation work is often required. Compared with json and XML, its universality is slightly insufficient.Proto message type files generally end with . In a  file, one or more message types can be defined.The following is an example of defining a message type for a search query. The  at the beginning of the file is used to describe the version information. Currently, there are two versions of proto, proto2 and proto3.Explicitly set the syntax format to proto3. If the  is not set, it defaults to proto2.  represents the content to be queried,  represents the page number of the query, and  represents the number of items per page.  must be located on the first line of the  file excluding comments and blank lines.The following message contains 3 fields (, , ), and each field has a corresponding type, field name, and field number. The field type can be , , , or a composite type.Each field in the message type needs to be defined with a unique number, and this number is used to identify the field in the binary data. Numbers in the range of [1,15] can be encoded and represented with one byte; in the range of [16,2047], they need to be encoded and represented with two bytes. Therefore, leaving the numbers within 15 for frequently occurring fields can save space. The minimum value of the number is 1, and the maximum value is 2^29 - 1 = 536870911. Numbers in the range of [19000, 19999] cannot be used because these numbers are used internally by the proto compiler. Similarly, other pre-reserved numbers cannot be used either.Each field can be modified by  or . In the proto3 syntax, if the modification type is not specified, the default value is .: It means that the modified field appears at most once, that is, it appears 0 or 1 time.: It means that the modified field can appear any number of times, including 0 times. In the proto3 syntax, fields modified by  use the  encoding by default.You can add comments to the  file. The comment syntax is the same as the C/C++ style, using  or .When deleting or commenting out a field in a , other developers in the future may reuse the previous field number when updating the  definition. If they accidentally load the old version of the  file, it may lead to serious problems, such as data corruption. To avoid such problems, you can specify the reserved field numbers and field names. If someone uses these field numbers in the future, an error will be generated when compiling the proto, thus reminding that there is a problem with the proto.Note: Do not mix the use of field names and field numbers for the same field.
  
  
  Mapping between Field Types and Language Types
The defined  file can generate Go language code through a generator. For example, the Go file generated from the  file is the  file.The mapping between basic types in proto and Go language types is shown in the following table (here only the type mapping between Go and C/C++ is listed, and for other languages, refer to https://developers.google.com/protocol-buffers/docs/proto3):
|.proto Type | Go Type | C++ Type |
| double | float64 | double |
| float | float32 | float |
| int32 | int32 | int32 |
| int64 | int64 | int64 |
| uint32 | uint32 | uint32 |
| uint64 | uint64 | uint64 |
| sint32 | int32 | int32 |
| sint64 | int64 | int64 |
| fixed32 | uint32 | uint32 |
| fixed64 | uint64 | uint64 |
| sfixed32 | int32 | int32 |
| sfixed64 | int64 | int64 |
| bool | bool | bool |
| string | string | string |
| bytes | []byte | string |When defining a message, if you want the value of a field to be only one of the expected values, you can use the enum type.For example, now add the  field to , and its value can only be one of , , , , , , and . This can be achieved by adding an enum to the message definition and adding a constant for each possible enum value.The first constant of the  enum must be mapped to 0, and all enum definitions need to include a constant mapped to 0, and this value is the first line content of the enum definition. This is because 0 is used as the default value of the enum. In the proto2 syntax, the enum value on the first line is always the default value. For the sake of compatibility, the value 0 must be the first line of the definition.Other  files can be imported in a  file, so as to use the message types defined in the imported file.By default, only the message types defined in the directly imported  file can be used. But sometimes it may be necessary to move the  file to a new location. At this time, a virtual  file can be placed in the old location, and the  syntax can be used to forward all imports to the new location, instead of directly moving the  file and updating all call points at once. Any place that imports a proto file containing the  statement can pass on the public dependencies of the imported dependencies.For example, there are  and  files in the current folder, and  is imported in the  file, that is, the  file has the following content:Suppose now we want to put the messages in  into the  file for use in other places. We can modify  and import  in it. Note that we need to use  because a single  can only use the messages defined in  and cannot use the message types in the proto file imported in .When using  for compilation, the option  or  needs to be used to notify  where to find the imported files. If the search path is not specified,  will look for it in the current directory (the path where  is called).Message types in the proto2 version can be imported into a proto3 file for use, and message types in the proto3 version can also be imported into a proto2 file. But the enum types in proto2 cannot be directly applied to the proto3 syntax.Message types can be defined inside another message type, that is, nested definitions. For example, the  type is defined inside , and it supports multiple levels of nesting.When an outer message type uses a message inside another message, such as the  type using , it can use .Unknown fields are fields that the proto compiler cannot recognize. For example, when an old binary file parses the data sent by a new binary file with new fields, these new fields will become unknown fields in the old binary file. In the initial version of proto3, unknown fields were discarded when the message was parsed, but in version 3.5, the retention of unknown fields was reintroduced. Unknown fields are retained during parsing and are included in the serialized output.The key to the high efficiency of Protobuf lies in its TLV (tag-length-value) encoding format. Each field has a unique  value as an identifier,  represents the length of the  data (for a  with a fixed length, there is no ), and  is the content of the data itself.For the  value, it is composed of two parts:  and .  is the number given to each field in the  earlier, and  represents the type (fixed length or variable length). The  currently has 6 values from 0 to 5, and these 6 values can be represented by 3 bits.The values of  are shown in the following table, where 3 and 4 have been deprecated, and we only need to pay attention to the remaining 4 types. For data encoded with Varint, there is no need to store the byte length , and at this time, the TLV encoding format degenerates into TV encoding. For 64-bit and 32-bit data, there is also no need for  because the  value already indicates whether the length is 8 bytes or 4 bytes.int32 int64 uint32 uint64 bool enumstring bytes packed repeated fields embedded
  
  
  Varint Encoding Principle
Varint is a variable-length int, which is a variable-length encoding method. It can make smaller numbers use fewer bytes to represent, and achieve data compression by reducing the number of bytes used to represent numbers. For an int32 type number, it usually requires 4 bytes to represent, but with Varint encoding, an int32 type number less than 128 can be represented with 1 byte. For larger numbers, it may require 5 bytes to represent, but in most messages, very large numbers usually do not appear, so using Varint encoding can use fewer bytes to represent numbers.Varint is a variable-length encoding, and it distinguishes each field through the highest bit of each byte. If the highest bit of a byte is 1, it means that the subsequent byte is also part of the number; if it is 0, it means that this is the last byte, and the remaining 7 bits are all used to represent the number. Although each byte will waste 1 bit of space (that is, 1/8 = 12.5% waste), if there are many numbers that do not need to be fixed as 4 bytes for representation, a large amount of space can still be saved.For example, for an int32 type number 65, its Varint encoding process is as follows, and the 65 that originally occupied 4 bytes only occupies 1 byte after encoding.For an int32 type number 128, it occupies 2 bytes after encoding.Varint decoding is the reverse process of encoding, which is relatively simple, and no example is given here.numbers to unsigned numbers, and then use Varint encoding to reduce the number of bytes after encoding.Zigzag uses unsigned numbers to represent signed numbers, enabling numbers with smaller absolute values to be represented with fewer bytes. Before understanding Zigzag encoding, let's first understand a few concepts:: The highest bit is the sign bit, and the remaining bits represent the absolute value.: Except for the sign bit, invert the remaining bits of the original code one by one.: For positive numbers, the two's complement is itself; for negative numbers, except for the sign bit, invert the remaining bits of the original code one by one and then add 1.Take the int32 type number -2 as an example, and its encoding process is as follows.In summary, for negative numbers, perform arithmetic operations on their two's complement. For a number , if it is of the  type, perform the operation ; if it is of the  type, perform the operation . Through this operation, the negative number is changed to a positive number, and this process is Zigzag encoding. Finally, use Varint encoding.Since Varint and Zigzag encoding can self-parse the content length, the length item can be omitted, and the TLV storage is simplified to TV storage, without the need for the  item.
  
  
  Calculation Methods of tag and value Values
The  stores the identification information and data type information of the field, that is,  (field data type) +  (identification number). The field number can be obtained through the , corresponding to the defined message field. The calculation formula is tag = field_number<<3 | wire_type, and then perform Varint encoding on it.The  is the value of the message field after Varint and Zigzag encoding.
  
  
  string Encoding (continued)
When the field type is the  type, the field value is encoded in UTF-8. For example, there is the following message definition:In the Go language, the sample code for encoding this message is as follows:The binary content after encoding is as follows:[10 14 67 104 105 110 97 228 184 173 144 155 189 228 120 186]
Nested messages mean that the  is another field message. The outer message is stored using TLV storage, and its  is also a TLV storage structure. The schematic diagram of the entire encoding structure is as follows (it can be imagined as a tree structure, where the outer message is the root node, and the nested message inside it is used as a child node, and each node follows the TLV encoding rule):The outermost message has its corresponding ,  (if any), and .When the  is a nested message, this nested message has its own independent ,  (if any), and .By analogy, if there are nested messages within the nested message, continue to encode according to the TLV rule.
  
  
  repeated Fields with packed
The fields modified by  can be with  or without it. For multiple field values of the same  field, their  values are all the same, that is, the data type and field sequence number are the same. If multiple  storages are used, there will be redundancy of the .If  is set, the storage method of the  field will be optimized. That is, the same  is only stored once, and then the total length  of all values under the  field is added to form a  storage structure. This method can effectively compress the length of the serialized data and save transmission overhead. For example:In the above example, the  field does not use , and each  value will have independent  and  storage; while the  field uses , and the  will only be stored once, followed by the total length  of all  values, and then all  values are arranged in sequence. In this way, when the data volume is large, the  field using  can significantly reduce the space occupied by the data and the bandwidth consumption during transmission. With its efficiency (in terms of size) and professionalism (professional types), Protobuf should have a higher coverage in the future data transmission field.Finally, I would like to introduce to you the most suitable platform for deploying services: 
  
  
  1. Multi-Language Support
Develop with JavaScript, Python, Go, or Rust.

  
  
  2. Deploy unlimited projects for free
pay only for usage â€” no requests, no charges.
  
  
  3. Unbeatable Cost Efficiency
Pay-as-you-go with no idle charges.
Example: $25 supports 6.94M requests at a 60ms average response time.

  
  
  4. Streamlined Developer Experience
Intuitive UI for effortless setup.
Fully automated CI/CD pipelines and GitOps integration.
Real-time metrics and logging for actionable insights.

  
  
  5. Effortless Scalability and High Performance
Auto-scaling to handle high concurrency with ease.
Zero operational overhead â€” just focus on building.
]]></content:encoded></item><item><title>Feature Flag Service: Experimenting with New Technologies and Architectures</title><link>https://dev.to/palma99/feature-flag-service-experimenting-with-new-technologies-and-architectures-176p</link><author>Palma99</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 09:36:54 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[I am a junior frontend developer with two years of experience working with Vue.js, I wanted to broaden my knowledge by exploring backend development and experimenting with other frontend frameworks. I decided to create a project from scratch using Go for the backend and Angular 19 for the frontend. The project is a service for managing feature flags, inspired by great existing solutions. My primary focus was on implementing the principles of Clean Architecture while also improving my SQL skills by working directly with PostgreSQL without an ORM.The source code can be found here:The idea was to create a dashboard where users can sign in and manage their projects, environments and flags. Then, project's specific flags can be retrieved by user's app using a public api. The first thing that i wasn't sure about is how to implement this public interaction in a secure way, we will dive into that later, but the idea was to create some sort of library that handle this communication using a public key.For the dashboard, I aimed to create an intuitive and simple UI for basic operations like creating new projects and flags. My main goal was to clearly display the status of the flags in each environment. This was the initial sketch of the UI.I also wanted to implement collaboration, allowing multiple users to access the same project. This necessitates the implementation of a role/permission system, where, for example, only the owner can delete a project.
  
  
  First step: Designing the database
Let's dive into some technical details, starting with database design. I wanted to use a relational database, but at this stage, I didnâ€™t really care which one to choose. So, as a first step, I started thinking about all the entities and the relationships between them:: a person that can sign up and have access to the dashboard: represent a container of environment and flags: each project can have one or more environments, this is useful for a real world project where multiple test environment can exist.: represent a single feature that can be enabled or notI started creating many-to-many relationship between users and project, so we can easily implement collaboration as mentioned before.
The one-to-many relationship between  and  it's pretty straight forward as one project can have multiple environments.Nothing special so far, but things started to get tricky when I encountered the flag table. The first idea that came to mind was that each flag should be directly related to an environment. This seemed logical, given that weâ€™re working in a multi-environment system where the end user needs to request flags for a specific environment. However, I quickly realized that this could introduce some introduce some complexities.Imagine you're inside the dashboard for a specific project that has 3 environments (TEST, QA, PROD), and you want to create a new flag called 'dark_mode_experimental'. This means you would need to create 3 new rows in the  table, one for each environment. Then, if you want to update the flag name to 'dark_mode', you would need to update all the rows accordingly. The same applies if you want to delete a flag. Furthermore, if a project already has some flags and you want to add another environment (let's say 'DEV'), you would need to duplicate all the existing flags for this new environment, which can lead to increased complexity and potential maintenance issues.The final solution is that flags belong to a project while maintaining a many-to-many relationship with environments through the  table. This table allows us to store the flag status for each specific environment. By doing so, we can easily manage flag updates and deletions without needing to duplicate or update rows across multiple environments.What happens when a new environment is created within a project is quite simple: we just add a new row in the  table. The relations in the  table are not created immediately but only after a flag is updated. This ensures that we avoid unnecessary entries for environments where no flags have been modified yet. Once a flag is updated, the corresponding relation in the flag_environment table is established, allowing us to track the flag's status in the new environment. 
  
  
  Step two: Implementing Go backend
As I mentioned, I'm more of a frontend person, but I have a strong interest in learning backend technologies. During my studies, I worked with several different languages, mostly C, Java, and Python. These are great languages, but I wanted to try something new. Recently, I heard a lot of positive things about Go, so I decided to give it a shot.There are a couple of things I want to mention before diving into the code. My main goal here was to structure the code by following clean architecture principles. This approach helps ensure that the code remains modular, maintainable, and testable. Additionally, for handling data, I chose not to use any ORM because I believe that for a study project, writing raw SQL is more instructive and provides a deeper understanding of how data is managed at the database level.Let's start with folder structureIn Go, it's common to have a folder called  that contains the entry point of the program, and a folder called  for all the application code. That's what I did â€” I created two subfolders inside : one for the API version and one for the CLI version of the service.There are some great articles about clean architecture, and the structure I implemented is quite standard, so I won't go into detail about what each folder represents. I just want to highlight some parts of the code that demonstrate how the principles are applied and how the different components interact with each other in the project.As I mentioned, I'm new to the Go world and still getting familiar with the ecosystem. From what I've seen so far, there aren't any major frameworks like those in the .NET or Java ecosystems that handle dependency injection in such a clean way. Therefore, I decided to implement dependency injection in a very 'vanilla' way, without relying on any external frameworks. Here is an exampleAn interactor is a struct with some dependencies that has methods to fulfill some business use cases, e.g.There are several aspects that can be improved, such as error handling or using a factory to create entities. However, the key idea is that the 'Create Environment' use case is managed within this method, which does not rely on any concrete implementation.In this project, I implemented anemic entities, which is not ideal. For example, all fields are public so they can be serialized using Go's standard library, although I'm not fully convinced this is the best approach.The infrastructure folder contains all the implementations related to external dependencies. In this case, it includes the repository implementation for PostgreSQL and the middleware used by the HTTP framework.I decided to use PostgreSQL as the database for no particular reason (well, maybe because I already had a Docker image pulled). However, thanks to clean architecture, it's easy to swap the database by simply implementing a new repository that adheres to the same interface. For example:In this case, interfaces refer to the components that allow external systems to interact with the application. For a web API, this typically consists of a function that, for example, extracts request parameters or body data, creates a DTO, and calls the interactor.
  
  
  Authentication for the admin section
When dealing with authentication in the real world, it's better to relay on well tested library to improve security. However this for this simple project i decided to implement a username and password authentication that works with JWT tokens from scratch, using this popular library 'github.com/golang-jwt/jwt/v5'.
  
  
  Implementation of the "public api"
To allow the end user to access flags for a certain environment, I decided to implement a public key pattern. Essentially, there is a public REST API protected by a middleware that checks for a specific header containing a key. If the key is valid, it grants read-only access to all the enabled flags for that environment. Even if the public key gets stolen, the impact is limited. Since it only grants read-only, the key cannot be used to modify any data or access sensitive information.That's how i implemented the middlewareThen in the route definitionTo interact with the public API I wrote a simple typescript SDK that allow to easily communicate with the service. It provides basic caching and types, and can be used by any javascript app.
  
  
  Admin dashboard in Angular 19
Once I completed the backend, I wanted to interact with my service through a comfortable UI. Angular 19 had just come out with stable signals and other cool features, so I decided to use it for the frontend. I didnâ€™t want to spend too much time designing UI components, hence I decided to use a component library that provides pre-built, customizable components. This allowed me to focus more on the core functionality and user experience. The library i choose is Taiga UI.The frontend is quite simple, a bit different from the initial sketch but it provides all the features i need.List of user projectsAn interesting feature I added consists of a section where you can test the payload of the public api for the environment. It works by making an api call to public endpoint using the environment key of the selected environment, and then it shows the response.Unit testing: Clean architecture is great and makes things easier to test, but I havenâ€™t written a single test for my business logic yet. This is definitely something to focus on moving forward.Collaboration is supported, but there is currently no way to "invite" someone to join a project (just manually on db). In the future, it would be cool to implement an invitation system, allowing users to easily add collaborators and manage team access.]]></content:encoded></item><item><title>I created a fairly extensive cheat sheet for scripting Sieve mail filters. Here&apos;s a link to the Gist if anyone is interested.</title><link>https://gist.github.com/Hotrod369/6b7a24e1ea060e48e0c02459cbb950a0</link><author>/u/StinkyPete312</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 09:01:46 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>This is a minimalist 2-click MSI installer generator for your projects for Windows. Magic works as all you need is to populate _configMSI.yml with your own values, then click 2 bat or sh files (if you use MS Visual Studio or MSYS2/MINGW64). And voila, your MSI Installer is ready!</title><link>https://github.com/windows-2048/Magic-MSI-Installer-Template</link><author>/u/florida-haunted</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 08:51:24 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pixerise v0.12 Release: Python High-Performance 3D Renderer Adds Ray Casting, 1/z Depth Interpolation, and Group Management with Improved Architecture</title><link>https://github.com/enricostara/pixerise</link><author>/u/jumpixel</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 08:47:55 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ðŸš€ Day 3 of #100DaysOfCoding</title><link>https://dev.to/xscoox_ca5e58c796032a1802/day-3-of-100daysofcoding-1008</link><author>xscoox</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 08:44:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Today, I dived into Linked Lists and learned the basics:
âœ… Insertion
âœ… Deletion
âœ… Understanding how pointers workExcited to learn more complex data structures next! ðŸ’¡]]></content:encoded></item><item><title>Confused about &quot;NEW&quot; Rust feature in - Closures in async functions</title><link>https://www.reddit.com/r/rust/comments/1ivdmek/confused_about_new_rust_feature_in_closures_in/</link><author>/u/DataBora</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 07:45:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am reading how new Rust feature is comming for using closures in Async functions. || async whaterver...In my Elusion library implementation i have PipelineScheduler function signature: ```rust pub async fn new<F, Fut>(frequency: &str, job: F) -> ElusionResult<Self> where F: Fn() -> Fut + Send + Sync + 'static, Fut: Future<Output = ElusionResult<()>> + Send + 'static ``` and then for Job creation:```rust let job = Job::new_async(&cron, move |uuid, mut l| { let job_fn = job_fn.clone(); Box::pin(async move { let future = job_fn(); future.await.unwrap_or_else(|e| eprintln!("âŒ Job execution failed: {}", e)); let next_tick = l.next_tick_for_job(uuid).await; match next_tick { Ok(Some(ts)) => println!("Next job execution: {:?} UTC Time", ts), _ => println!("Could not determine next job execution"), } }) }).map_err(|e| ElusionError::Custom(format!("âŒ Job creation failed: {}", e)))?; ``` which user can use like this:let scheduler = PipelineScheduler::new("5min", || async {}) How this new feature will be different?]]></content:encoded></item><item><title>[R] Evaluating LLM Knowledge Across 285 Graduate Disciplines: A Comprehensive Benchmark Using Human-LLM Collaborative Filtering</title><link>https://www.reddit.com/r/MachineLearning/comments/1ivd069/r_evaluating_llm_knowledge_across_285_graduate/</link><author>/u/Successful-Western27</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 07:02:41 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[A new evaluation benchmark tests language models across 285 graduate-level disciplines using an iterative human-AI collaborative approach to generate and validate questions. The methodology combines expert review with model-assisted filtering to ensure high-quality, discipline-appropriate assessment.Key technical points: - Uses a two-stage question generation process: initial AI generation followed by expert review - Implements collaborative filtering where both human experts and LLMs help identify and remove problematic questions - Covers disciplines from traditional academia to specialized industrial fields - Tests both factual knowledge and reasoning capabilities - Evaluated on multiple leading LLMs including GPT-4, Claude 2, and DeepSeekResults: - Best performance: DeepSeek-R1 at 61.82% accuracy - Significant variance in performance across different disciplines - 80+ expert annotators involved in validation - Generated dataset of 2,855 validated questionsI think this benchmark addresses a critical gap in LLM evaluation by going beyond common academic subjects. The methodology of combining human expertise with AI assistance for question validation could be valuable for developing future evaluation datasets.I think the relatively modest performance (62%) on graduate-level questions across diverse fields suggests current LLMs still have significant room for improvement in specialized domains. This could influence how we approach model training and evaluation for domain-specific applications.TLDR: New benchmark tests LLMs across 285 graduate disciplines using human-AI collaborative question generation. Best model achieved 62% accuracy, revealing gaps in specialized knowledge.]]></content:encoded></item><item><title>I Made a Configurable Rate Limiterâ€¦ Because APIs Canâ€™t Say â€˜Chillâ€™</title><link>https://beyondthesyntax.substack.com/p/i-made-a-configurable-rate-limiter?r=4jgehp&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;triedRedirect=true</link><author>/u/Sushant098123</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 06:24:46 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Building a REST API with Django REST Framework: A Beginners Guide</title><link>https://dev.to/kihuni/building-a-rest-api-with-django-rest-framework-a-beginners-guide-1b1n</link><author>kihuni</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 06:04:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Imagine you're building a book management system where users can browse available books and add new ones. To make this possible, we need a REST API to handle book data efficiently.REST APIs are the backbone of modern web applications, enabling seamless communication between frontend and backend systems. Django REST Framework (DRF) simplifies API development by providing a powerful toolkit built on top of Django. In this guide, we'll build a simple REST API to manage books.Before starting this tutorial, you should have:âœ… Basic understanding of Python programming
âœ… Familiarity with web development concepts
âœ… Python 3.8+ installed on your system
âœ… Basic knowledge of the Django framework
âœ… Understanding of HTTP methods (GET, POST, PUT, DELETE)
âœ… Experience using the command-line interfaceBy the end of this tutorial, you will be able to:âœ… Set up a Django REST Framework project from scratch
âœ… Create API endpoints using function-based views (FBVs) and class-based views (CBVs)
âœ… Implement model serialization for JSON responses
âœ… Perform CRUD operations via a REST API
âœ… Test API endpoints using browsable API and cURL commands
  
  
  Why Django REST Framework?
Django REST Framework has become the go-to choice for building APIs with Django because it offers:âœ”ï¸ Powerful serialization capabilities
âœ”ï¸ Built-in authentication and permissions
âœ”ï¸ Browsable API interface for easy testing
âœ”ï¸ Extensive documentation and strong community support
âœ”ï¸ Flexible request/response handling
âœ”ï¸ Support for pagination, filtering, and throttlingLetâ€™s start building our REST API step by step.Step 1: Environment SetupFirst, create a clean development environment:# Create a project folder
mkdir book-api && cd book-api

# Create a virtual environment
python -m venv venv

# Activate the virtual environment (Windows)
venv\Scripts\activate

# Activate the virtual environment (Mac/Linux)
source venv/bin/activate

# Install Django and Django REST Framework
pip install django djangorestframework

Now, create a Django project and an app for managing books.# Create a Django project
django-admin startproject bookapi

cd bookapi

# Create a Django app
python manage.py startapp books

Register Django REST Framework and the books app in settings.py:INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',

    # Third-party apps
    'rest_framework',

    # Local apps
    'books',
]

Define a Book model in books/models.py:from django.db import models

class Book(models.Model):
    title = models.CharField(max_length=255)
    author = models.CharField(max_length=255)
    published_date = models.DateField()
    isbn = models.CharField(max_length=13, unique=True)

    def __str__(self):
        return self.title
python manage.py makemigrations books
python manage.py migrate
Step 5: Serializer CreationCreate books/serializers.py to handle data conversion:from rest_framework import serializers
from .models import Book

class BookSerializer(serializers.ModelSerializer):
    class Meta:
        model = Book
        fields = '__all__'
We'll implement both function-based views (FBVs) and class-based views (CBVs).Function-Based Views (FBVs)from rest_framework.response import Response
from rest_framework.decorators import api_view
from .models import Book
from .serializers import BookSerializer

@api_view(['GET'])
def book_list(request):
    books = Book.objects.all()
    serializer = BookSerializer(books, many=True)
    return Response(serializer.data)

@api_view(['POST'])
def book_create(request):
    serializer = BookSerializer(data=request.data)
    if serializer.is_valid():
        serializer.save()
        return Response(serializer.data, status=201)
    return Response(serializer.errors, status=400)


For a more scalable approach, use Django REST Frameworkâ€™s generic views:from rest_framework import generics
from .models import Book
from .serializers import BookSerializer

class BookListCreateView(generics.ListCreateAPIView):
    queryset = Book.objects.all()
    serializer_class = BookSerializer

Step 7: URL Configurationfrom django.urls import path
from .views import book_list, book_create, BookListCreateView

urlpatterns = [
    # Function-based views
    path('books/', book_list, name='book-list'),
    path('books/create/', book_create, name='book-create'),

    # Class-based views
    path('books/cbv/', BookListCreateView.as_view(), name='book-list-cbv'),
]

**Update bookapi/urls.py:from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('api/', include('books.urls')),
]

Run the server and visit:ðŸ“Œ http://127.0.0.1:8000/api/books/Django REST Framework provides an interactive browsable API that allows you to test endpoints without external tools!You can also test the API using cURL commands:# GET request
curl -X GET http://127.0.0.1:8000/api/books/

# POST request
curl -X POST http://127.0.0.1:8000/api/books/create/ \
     -H "Content-Type: application/json" \
     -d '{"title": "Django for Beginners", "author": "William Vincent", "published_date": "2021-09-01", "isbn": "9781735467207"}'
Next Steps: Enhancing Your APINow that you have a working API, consider improving it with:
âœ”ï¸ Authentication & Permissions (e.g., only authenticated users can add books)
âœ”ï¸ Pagination for Large Datasets
âœ”ï¸ Filtering & Searching
âœ”ï¸ ViewSets & Routers for cleaner URL managementðŸŽ‰ Congratulations! You have successfully built a simple REST API using the Django REST Framework. This foundation can be expanded to develop more complex APIs with additional features and security measures.]]></content:encoded></item><item><title>I have made a pong game in C++ using raylib. So can anyone plz give suggession where I can improve the game and my code?</title><link>https://github.com/EthicalAniruddha/AI-Pong</link><author>/u/Ethical_Aniruddha</author><category>dev</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 06:01:42 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>One-Minute Daily AI News 2/21/2025</title><link>https://www.reddit.com/r/artificial/comments/1ivbt3a/oneminute_daily_ai_news_2212025/</link><author>/u/Excellent-Target-847</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 05:45:10 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/Excellent-Target-847 ]]></content:encoded></item><item><title>RandomAffine in PyTorch (6)</title><link>https://dev.to/hyperkai/randomaffine-in-pytorch-6-4813</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 05:11:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomAffine() can do random rotation or random affine transformation for an image as shown below:]]></content:encoded></item><item><title>Generic Bitfield I had fun implementing</title><link>https://gist.github.com/oplanre/de0bba4f1e2f769458ca1adff7f12280</link><author>/u/ln3ar</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 04:12:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>RandomAffine in PyTorch (5)</title><link>https://dev.to/hyperkai/randomaffine-in-pytorch-5-1c2m</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 03:59:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomAffine() can do random rotation or random affine transformation for an image as shown below:]]></content:encoded></item><item><title>Announcing async-local 3.0 now with async closure support</title><link>https://www.reddit.com/r/rust/comments/1iv8o6v/announcing_asynclocal_30_now_with_async_closure/</link><author>/u/Jester831</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 02:45:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Async-local enables thread locals to be used in an async context across await points or within blocking threads managed by the Tokio runtime without the overhead of `Arc`. The way this is accomplished is by using generativity to create unique invariant lifetimes so that borrows to TLS can't be coerced to a `&'static` lifetime and by configuring the runtime with a barrier to rendezvous worker threads during shutdown. This shutdown barrier makes it such that runtime tasks never outlive TLS data owned by worker threads; this makes every invariant lifetime guaranteed to be valid until no tasks remain. Blocking threads managed by the Tokio runtime cannot outlive worker threads with this configuration, and so pointers to TLS from worker threads can be safely moved to these blocking threads with the lifetime constrained. As the lifetimes cannot be coerced into `&'static`, moving onto other threads is prevented. This crate downgrades to using `Arc` whenever the `barrier-protected-runtime` feature is not enabled, making it the end users choice to opt into this optimization by using async_local to configure the runtime shutdown barrier. ]]></content:encoded></item><item><title>Seamlessly Compare Maps on QGIS with the QMapCompare Plugin</title><link>https://dev.to/mierune/seamlessly-compare-maps-on-qgis-with-the-qmapcompare-plugin-3186</link><author>Raymond Lay</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 02:10:46 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When working with QGIS, you often switch between basemaps, but  comparing small differences between maps can be challenging when only one map can be displayed at a time.However, QGIS has lacked a stable feature for map comparison until now! This post introduces , a new plugin that enables users to compare multiple maps directly within QGIS using various visualization methods.QMapCompare overview. Created by editing GSI TilesFirst, ensure you are using QGIS version 3.34 or later.
Then, you can install plugin by searching  on QGIS plugin manager.Once plugin installed, a new icon will appear on the toolbar.
Clicking the icon will toggle the QMapCompare panel on the left bottom of QGIS window.QMapCompare provides several ways to compare maps. Select layers you want to compare (1), and choose the following comparison method (2-5):1.  (multiple selections allowed)2. : Displays two maps side by side3. : Divides the map vertically4. : Divides the map horizontally5. : Check compare layers with a circle around the mouse cursor6. : Ends the comparison
  
  
  Comparison Methods Overview
The mirror mode places a duplicate of the map canvas on the right side of the map canvas. This is useful when comparing base maps with satellite imagery or other data.The split mode divides the map into two sections, showing different layers side by side.You can choose either a vertical or horizontal split, depending on your needs.This is may be useful for comparing building accuracy in OpenStreetMap with government-provided data as example.Lens mode displays a circular preview of the comparison layers around the mouse cursor.As you move the cursor, the preview updates in real time.This is especially useful for detailed comparisons of specific locations.QMapCompare is valuable for various use cases as below:Comparing historical aerial photographsTokyo Aerial Photo Comparison between 2024(left) and 1987-1990(right).GSI TilesAnalyzing pre- and post-disaster imagesAerial Photo Comparison of Before 2021 Atami Landslip Disaster (left) and after(right).GSI TilesQMapCompare is a powerful tool for comparing different maps and datasets within QGIS. With this plugin, you can easily:Analyze time-series data (e.g., pre- and post-disaster maps)Evaluate data accuracy (e.g., comparing OpenStreetMap with government maps)Support decision-making (e.g., verifying different analytical results and styles)As this is a newly released plugin, there may still be some bugs. If you encounter any issues, please report them on our GitHub Issues page.Your feedback will help improve the tool!]]></content:encoded></item><item><title>from nodejs want to move to golang</title><link>https://www.reddit.com/r/golang/comments/1iv7ngg/from_nodejs_want_to_move_to_golang/</link><author>/u/Spirited-Item1431</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 22 Feb 2025 01:53:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I used to be a web developer who used Node.js as my daily programming language, but now I'm interested in switching to Golang. Aside from the usual fundamentals, what are the most important things to learn in Golang?]]></content:encoded></item><item><title>Understanding Packages in Go: A Comprehensive Guide</title><link>https://dev.to/abstractmusa/fsdfasdf-asfa-3fd1</link><author>Md Abu Musa</author><category>dev</category><category>go</category><category>devto</category><pubDate>Sat, 22 Feb 2025 01:42:36 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In Go, a package is a fundamental concept for organizing and reusing code. This guide explains everything you need to know about Go packages.A package is a collection of source files in the same directory.All files in a package must declare the same package name at the top.It provides modularity, encapsulation, and code reuse.A special package that creates an executable program.Must contain a  function.Used only for executables.Can have any name except .Used to create reusable code.Can be imported by other packages.
  
  
  3. Package Visibility Rules
Names starting with an  letter are .Names starting with a  letter are .To use packages in Go, you import them:
  
  
  5. Package Organization Example
myapp/
â”œâ”€â”€ main.go              // package main
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ math.go         // package utils
â”‚   â””â”€â”€ strings.go      // package utils
â””â”€â”€ models/
    â””â”€â”€ user.go         // package models

  
  
  6. Benefits of Using Packages
All files in the same folder must have the same package name.Package names usually match the directory name.Standard library packages like , , etc., come with Go installation.You can create custom packages for better code structure.Use  to initialize a new module (which can contain multiple packages).By following these best practices, you can effectively manage code in Go using packages.]]></content:encoded></item><item><title>Open-Source Book Creator with Multi-Agent AI</title><link>https://dev.to/guerra2fernando/open-source-book-creator-with-multi-agent-ai-1bnl</link><author>Fernando Guerra</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 01:29:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I'm excited to share ** ðŸ“ LibriScribe**, an open-source book creation system I've developed that demonstrates the power of multiple specialized AI agents working together. 
You can just install it with python and the system will guide you to write a complete book in a few minutes :)
  
  
  The Power of Multi-Agent Architecture
Rather than using a single AI model to handle all aspects of book creation, LibriScribe orchestrates specialized agents:: Develops and refines your initial idea: Structures your book with chapters and scenes: Creates detailed character profiles: Builds rich, consistent settings and lore: Writes scene-by-scene content: Refines and improves the writing: Checks for plot holes and inconsistencies: Polishes the writing style: Prepares the final manuscript
  
  
  Versatile for Multiple Book Types
Fiction (novels, short stories)The system is built in Python with a modular and custom agent architecture. Each agent is a class that inherits from a base Agent class and implements an  method. The system uses a unified LLM client that supports multiple AI providers (OpenAI, Claude, Google AI, DeepSeek, and Mistral).There's several functions:Feedback and contributions are very welcome! Leave a star if you like it :)]]></content:encoded></item><item><title>Ring is unmaintained</title><link>https://rustsec.org/advisories/RUSTSEC-2025-0007.html</link><author>/u/technobicheiro</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 00:44:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[This advisory has been withdrawn and should be ignored. It is kept only for reference.The author has announced an indefinite hiatus in its development, noting that
any reported security vulnerabilities may go unaddressed for prolonged periods
of time.After this advisory was published, the author graciously agreed to give
access to the rustls team. The rustls team is committed to providing
security (only) maintenance for  for the foreseeable future.Advisory available under CC0-1.0
    license.

    
    ]]></content:encoded></item><item><title>RandomAffine in PyTorch (4)</title><link>https://dev.to/hyperkai/randomaffine-in-pytorch-4-570l</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 00:35:25 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomAffine() can do random rotation or random affine transformation for an image as shown below:]]></content:encoded></item><item><title>[P] Decensor AI models Qwen/Deepseek by finetuning with non political data</title><link>https://www.reddit.com/r/MachineLearning/comments/1iv6ckk/p_decensor_ai_models_qwendeepseek_by_finetuning/</link><author>/u/Ambitious_Anybody855</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 22 Feb 2025 00:30:49 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[The best way to decensor a DeepSeek model? Donâ€™t try to decensor it.Fine-tuned OpenThinker on OpenThoughts-114k, a dataset focused on reasoning tasks like math, coding, and graduate-level Q&A, with no political content. Despite using censored base models (Qwen), the fine-tuned OpenThinker-7B and OpenThinker-32B models became decensored without any explicit intervention. Unlike Perplexity, no custom fine-tuning was applied to remove censorship, yet the results remain uncensored. It challenges assumptions about model safety and opens exciting new research directions. AI game is so on]]></content:encoded></item><item><title>RandomAffine in PyTorch (3)</title><link>https://dev.to/hyperkai/randomaffine-in-pytorch-3-3hkm</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Sat, 22 Feb 2025 00:24:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomAffine() can do random rotation or random affine transformation for an image as shown below:]]></content:encoded></item><item><title>The Next AI Revolution: A Tutorial Using VAEs to Generate High-Quality Synthetic Data</title><link>https://towardsdatascience.com/the-next-ai-revolution-a-tutorial-using-vaes-to-generate-high-quality-synthetic-data/</link><author>Torty Sivill</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 23:42:07 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Data created by a computer intended to replicate or augment existing data.We have all experienced the success of ChatGPT, Llama, and more recently, DeepSeek. These language models are being used ubiquitously across society and have triggered many claims that we are rapidly approaching Artificial General Intelligenceâ€Šâ€”â€ŠAI capable of replicating any human function.Â Before getting too excited, or scared, depending on your perspectiveâ€Šâ€”â€Šwe are also rapidly approaching a hurdle to the advancement of these language models. According to a paper published by a group from the research institute, Epoch [1], we are running out of data. They estimate that by 2028 we will have reached the upper limit of possible data upon which to train language models.Â What happens if we run out of data?Well, if we run out of data then we arenâ€™t going to have anything new with which to train our language models. These models will then stop improving. If we want to pursue Artificial General Intelligence then we are going to have to come up with new ways of improving AI without just increasing the volume of real-world training data.Â One potential saviour is synthetic data which can be generated to mimic existing data and has already been used to improve the performance of models like Gemini and DBRX.Â Synthetic data beyond LLMsBeyond overcoming data scarcity for large language models, synthetic data can be used in the following situations:Â â€Šâ€”â€Šif we donâ€™t want to share or use sensitive attributes, synthetic data can be generated which mimics the properties of these features while maintaining anonymity.â€Šâ€”â€Šif collecting data is expensive we can generate a large volume of synthetic data from a small amount of real-world data.â€”â€Šdatasets are biased when there is a disproportionately low number of individual data points from a particular group. Synthetic data can be used to balance a dataset.Â Imbalanced datasets can (*but not always*) be problematic as they may not contain enough information to effectively train a predictive model. For example, if a dataset contains many more men than women, our model may be biased towards recognising men and misclassify future female samples as men.Â In this article we show the imbalance in the popular UCI Adult dataset[2], and how we can use a  to generate Synthetic Data to improve classification on this example.Â We first download the Adult dataset. This dataset contains features such as age, education and occupation which can be used to predict the target outcome â€˜incomeâ€™.Â # Download dataset into a dataframe
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
columns = [
   "age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
   "occupation", "relationship", "race", "sex", "capital-gain",
   "capital-loss", "hours-per-week", "native-country", "income"
]
data = pd.read_csv(url, header=None, names=columns, na_values=" ?", skipinitialspace=True)

# Drop rows with missing values
data = data.dropna()

# Split into features and target
X = data.drop(columns=["income"])
y = data['income'].map({'>50K': 1, '<=50K': 0}).values

# Plot distribution of income
plt.figure(figsize=(8, 6))
plt.hist(data['income'], bins=2, edgecolor='black')
plt.title('Distribution of Income')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()In the Adult dataset, income is a binary variable, representing individuals who earn above, and below, $50,000. We plot the distribution of income over the entire dataset below. We can see that the dataset is heavily imbalanced with a far larger number of individuals who earn less than $50,000.Â Despite this imbalance we can still train a machine learning classifier on the Adult dataset which we can use to determine whether unseen, or test, individuals should be classified as earning above, or below, 50k.Â # Preprocessing: One-hot encode categorical features, scale numerical features
numerical_features = ["age", "fnlwgt", "education-num", "capital-gain", "capital-loss", "hours-per-week"]
categorical_features = [
   "workclass", "education", "marital-status", "occupation", "relationship",
   "race", "sex", "native-country"
]

preprocessor = ColumnTransformer(
   transformers=[
       ("num", StandardScaler(), numerical_features),
       ("cat", OneHotEncoder(), categorical_features)
   ]
)

X_processed = preprocessor.fit_transform(X)

# Convert to numpy array for PyTorch compatibility
X_processed = X_processed.toarray().astype(np.float32)
y_processed = y.astype(np.float32)
# Split dataset in train and test sets
X_model_train, X_model_test, y_model_train, y_model_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42)


rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_model_train, y_model_train)

# Make predictions
y_pred = rf_classifier.predict(X_model_test)

# Display confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()Printing out the confusion matrix of our classifier shows that our model performs fairly well despite the imbalance. Our model has an overall error rate of 16% but the error rate for the positive class (income > 50k) is 36% where the error rate for the negative class (income < 50k) is 8%.Â This discrepancy shows that the model is indeed biased towards the negative class. The model is frequently incorrectly classifying individuals who earn more than 50k as earning less than 50k.Â Below we show how we can use a Variational Autoencoder to generate synthetic data of the positive class to balance this dataset. We then train the same model using the synthetically balanced dataset and reduce model errors on the test set.Â How can we generate synthetic data?There are lots of different methods for generating synthetic data. These can include more traditional methods such as SMOTE and Gaussian Noise which generate new data by modifying existing data. Alternatively Generative models such as Variational Autoencoders or General Adversarial networks are predisposed to generate new data as their architectures learn the distribution of real data and use these to generate synthetic samples.In this tutorial we use a variational autoencoder to generate synthetic data.Variational Autoencoders (VAEs) are great for synthetic data generation because they use real data to learn a continuous latent space. We can view this latent space as a magic bucket from which we can sample synthetic data which closely resembles existing data. The continuity of this space is one of their big selling points as it means the model generalises well and doesnâ€™t just memorise the latent space of specific inputs.A VAE consists of an , which maps input data into a probability distribution (mean and variance) and a , which reconstructs the data from the latent space.Â For that continuous latent space, VAEs use a reparameterization trick where a random noise vector is scaled and shifted using the learned mean and variance, ensuring smooth and continuous representations in the latent space.Below we construct a  class which implements this process with a simple architecture. compresses the input into a smaller, hidden representation, producing both a mean and log variance that define a Gaussian distribution aka creating our magic sampling bucket. Instead of directly sampling, the model applies the reparameterization trick to generate latent variables, which are then passed to the decoder.Â  reconstructs the original data from these latent variables, ensuring the generated data maintains characteristics of the original dataset.Â class BasicVAE(nn.Module):
   def __init__(self, input_dim, latent_dim):
       super(BasicVAE, self).__init__()
       # Encoder: Single small layer
       self.encoder = nn.Sequential(
           nn.Linear(input_dim, 8),
           nn.ReLU()
       )
       self.fc_mu = nn.Linear(8, latent_dim)
       self.fc_logvar = nn.Linear(8, latent_dim)
      
       # Decoder: Single small layer
       self.decoder = nn.Sequential(
           nn.Linear(latent_dim, 8),
           nn.ReLU(),
           nn.Linear(8, input_dim),
           nn.Sigmoid()  # Outputs values in range [0, 1]
       )

   def encode(self, x):
       h = self.encoder(x)
       mu = self.fc_mu(h)
       logvar = self.fc_logvar(h)
       return mu, logvar

   def reparameterize(self, mu, logvar):
       std = torch.exp(0.5 * logvar)
       eps = torch.randn_like(std)
       return mu + eps * std

   def decode(self, z):
       return self.decoder(z)

   def forward(self, x):
       mu, logvar = self.encode(x)
       z = self.reparameterize(mu, logvar)
       return self.decode(z), mu, logvarGiven our BasicVAE architecture we construct our loss functions and model training below.Â def vae_loss(recon_x, x, mu, logvar, tau=0.5, c=1.0):
   recon_loss = nn.MSELoss()(recon_x, x)
 
   # KL Divergence Loss
   kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
   return recon_loss + kld_loss / x.size(0)

def train_vae(model, data_loader, epochs, learning_rate):
   optimizer = optim.Adam(model.parameters(), lr=learning_rate)
   model.train()
   losses = []
   reconstruction_mse = []

   for epoch in range(epochs):
       total_loss = 0
       total_mse = 0
       for batch in data_loader:
           batch_data = batch[0]
           optimizer.zero_grad()
           reconstructed, mu, logvar = model(batch_data)
           loss = vae_loss(reconstructed, batch_data, mu, logvar)
           loss.backward()
           optimizer.step()
           total_loss += loss.item()

           # Compute batch-wise MSE for comparison
           mse = nn.MSELoss()(reconstructed, batch_data).item()
           total_mse += mse

       losses.append(total_loss / len(data_loader))
       reconstruction_mse.append(total_mse / len(data_loader))
       print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, MSE: {total_mse:.4f}")
   return losses, reconstruction_mse

combined_data = np.concatenate([X_model_train.copy(), y_model_train.cop
y().reshape(26048,1)], axis=1)

# Train-test split
X_train, X_test = train_test_split(combined_data, test_size=0.2, random_state=42)

batch_size = 128

# Create DataLoaders
train_loader = DataLoader(TensorDataset(torch.tensor(X_train)), batch_size=batch_size, shuffle=True)
test_loader = DataLoader(TensorDataset(torch.tensor(X_test)), batch_size=batch_size, shuffle=False)

basic_vae = BasicVAE(input_dim=X_train.shape[1], latent_dim=8)

basic_losses, basic_mse = train_vae(
   basic_vae, train_loader, epochs=50, learning_rate=0.001,
)

# Visualize results
plt.figure(figsize=(12, 6))
plt.plot(basic_mse, label="Basic VAE")
plt.ylabel("Reconstruction MSE")
plt.title("Training Reconstruction MSE")
plt.legend()
plt.show()consists of two components: , which measures how well the generated data matches the original input using Mean Squared Error (MSE), and , which ensures that the learned latent space follows a normal distribution. optimises the VAE using the Adam optimizer over multiple epochs. During training, the model takes mini-batches of data, reconstructs them, and computes the loss using . These errors are then corrected via backpropagation where the model weights are updated. We train the model for 50 epochs and plot how the reconstruction mean squared error decreases over training.We can see that our model learns quickly how to reconstruct our data, evidencing efficient learning.Â Now we have trained our BasicVAE to accurately reconstruct the Adult dataset we can now use it to generate synthetic data. We want to generate more samples of the positive class (individuals who earn over 50k) in order to balance out the classes and remove the bias from our model.To do this we select all the samples from our VAE dataset where income is the positive class (earn more than 50k). We then encode these samples into the latent space. As we have only selected samples of the positive class to encode, this latent space will reflect properties of the positive class which we can sample from to create synthetic data.Â We sample 15000 new samples from this latent space and decode these latent vectors back into the input data space as our synthetic data points.Â # Create column names
col_number = sample_df.shape[1]
col_names = [str(i) for i in range(col_number)]
sample_df.columns = col_names

# Define the feature value to filter
feature_value = 1.0  # Specify the feature value - here we set the income to 1

# Set all income values to 1 : Over 50k
selected_samples = sample_df[sample_df[col_names[-1]] == feature_value]
selected_samples = selected_samples.values
selected_samples_tensor = torch.tensor(selected_samples, dtype=torch.float32)

basic_vae.eval()  # Set model to evaluation mode
with torch.no_grad():
   mu, logvar = basic_vae.encode(selected_samples_tensor)
   latent_vectors = basic_vae.reparameterize(mu, logvar)

# Compute the mean latent vector for this feature
mean_latent_vector = latent_vectors.mean(dim=0)


num_samples = 15000  # Number of new samples
latent_dim = 8
latent_samples = mean_latent_vector + 0.1 * torch.randn(num_samples, latent_dim)

with torch.no_grad():
   generated_samples = basic_vae.decode(latent_samples)Now we have generated synthetic data of the positive class, we can combine this with the original training data to generate a balanced synthetic dataset.Â new_data = pd.DataFrame(generated_samples)

# Create column names
col_number = new_data.shape[1]
col_names = [str(i) for i in range(col_number)]
new_data.columns = col_names

X_synthetic = new_data.drop(col_names[-1],axis=1)
y_synthetic = np.asarray([1 for _ in range(0,X_synthetic.shape[0])])

X_synthetic_train = np.concatenate([X_model_train, X_synthetic.values], axis=0)
y_synthetic_train = np.concatenate([y_model_train, y_synthetic], axis=0)

mapping = {1: '>50K', 0: '<=50K'}
map_function = np.vectorize(lambda x: mapping[x])
# Apply mapping
y_mapped = map_function(y_synthetic_train)

plt.figure(figsize=(8, 6))
plt.hist(y_mapped, bins=2, edgecolor='black')
plt.title('Distribution of Income')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()We can now use our balanced training synthetic dataset to retrain our random forest classifier. We can then evaluate this new model on the original test data to see how effective our synthetic data is at reducing the model bias.rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_synthetic_train, y_synthetic_train)

# Step 5: Make predictions
y_pred = rf_classifier.predict(X_model_test)

cm = confusion_matrix(y_model_test, y_pred)

# Create heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()Our new classifier, trained on the balanced synthetic dataset makes fewer errors on the original test set than our original classifier trained on the imbalanced dataset and our error rate is now reduced to 14%.However, we have not been able to reduce the discrepancy in errors by a significant amount, our error rate for the positive class is still 36%. This could be due to to the following reasons:Â We have discussed how one of the benefits of VAEs is the learning of a continuous latent space. However, if the majority class dominates, the latent space might skew towards the majority class.The model may not have properly learned a distinct representation for the minority class due to the lack of data, making it hard to sample from that region accurately.In this tutorial we have introduced and built a BasicVAE architecture which can be used to generate synthetic data which improves the classification accuracy on an imbalanced dataset.Â Follow for future articles where I will show how we can build more sophisticated VAE architectures which address the above problems with imbalanced sampling and more.[1] Villalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., & Hobbhahn, M. (2024). Will we run out of data? Limits of LLM scaling based on human-generated data. arXiv preprint arXiv:2211.04325, .]]></content:encoded></item><item><title>Lightweight Real-Time System Stats for VS Code</title><link>https://marketplace.visualstudio.com/items?itemName=odangoo.otak-monitor</link><author>/u/Wise_Bug47</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 23:39:37 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A lightweight system monitor for VS Code - Track CPU, memory, and disk usage with efficient 5-second updates and 1-minute averages.Find the system monitor in your VS Code status barView CPU usage percentageHover to see detailed current and average metricsotak-monitor is a lightweight VS Code extension that helps you monitor system resources without leaving your editor.Status bar display of CPU usage percentageAggregated across all CPU coresPrecise to one decimal placeCurrent CPU clock speed (MHz)Detailed memory informationShows used and total memory in MBCross-platform disk space monitoring
Windows: C: drive (home directory in Codespaces)Linux: Root filesystem (workspace root in Codespaces)Shows used and total space in GBClean status bar integrationDetailed hover tooltip showing:
Current CPU, memory, and disk metricsVisual Studio Code ^1.90.0Supported environments:
Local: Windows, macOS, LinuxRemote: GitHub CodespacesInstall the extension from VS Code MarketplaceLook for the CPU usage display in your status barHover over it to see detailed system informationThe extension shows the following information in your status bar:With a detailed tooltip showing:Current:
CPU Usage: 45.3% (2400 MHz)
Memory Usage: 1024 MB / 2048 MB (50.0%)
Disk Usage: 150 GB / 500 GB (30.0%)
Note: For disk usage, the monitored path varies by environment:Windows:
Codespaces: Home directoryLinux:
Local: Root filesystem (/)Codespaces: Workspace rootCPU usage is calculated by comparing idle and total CPU time differencesMemory values are shown in MB and percentageDisk usage monitoring adapts to the environment:
Local machines: Monitors system root or C: driveCodespaces: Monitors relevant workspace pathsMoving averages are calculated using 12 data points (5-second intervals over 1 minute)Updates occur every 5 seconds for efficient monitoringMinimal performance impact on the systemGitHub Codespaces SupportThe extension automatically detects when running in GitHub Codespaces and adjusts its behavior:Monitors the workspace root directory in Linux environmentsUses home directory for Windows-based CodespacesMaintains consistent monitoring experience across all environmentsProvides accurate disk usage information for containerized developmentContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.This project is licensed under the MIT License - see the LICENSE file for details.]]></content:encoded></item><item><title>I made an AirDrop server that uses URL Requests to accept data from anywhere</title><link>https://github.com/gnhen/SkyDrop</link><author>/u/GranttH</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 23:06:34 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Comments on Executive Order 14168</title><link>https://aphyr.com/posts/380-comments-on-executive-order-14168</link><author>Aphyr</author><category>dev</category><category>go</category><pubDate>Fri, 21 Feb 2025 23:04:55 +0000</pubDate><source url="http://aphyr.com/posts.atom">Aphyr</source><content:encoded><![CDATA[Executive order 14168 is biologically incoherent and socially cruel. All passport applicants should be allowed to select whatever gender markers they feel best fit, including M, F, or X.In humans, neither sex nor gender is binary at any level. There are several possible arrangements of sex chromosomes: X, XX, XY, XXY, XYY, XXX, tetrasomies, pentasomies, etc. A single person can contain a mosaic of cells with different genetics: some XX, some XYY. Chromosomes may not align with genitalia: people with XY chromosomes may have a vulva and internal testes. People with XY chromosomes and a small penis may be surgically and socially reassigned female at birthâ€”and never told what happened. None of these biological dimensions necessarily align with oneâ€™s internal concept of gender, or oneâ€™s social presentation.The executive order has no idea how biology works. It defines â€œfemaleâ€ as â€œa person belonging, at conception, to the sex that produces the large reproductive cellâ€. Zygotes do not produce reproductive cells at all: under this order none  of us have a sex. Oogenesis doesnâ€™t start until over a month into embryo development. Even if people were karyotyping their zygotes immediately after conception so they could tell what â€œlegalâ€ sex they were going to be, they could be wrong: which gametes we produce depends on the formation of the genital ridge.All this is to say that if people fill out these forms using this definition of sex, theyâ€™re guessing at a question which is both impossible to answer and socially irrelevant. You might be one of the roughly two percent of humans born with an uncommon sexual development and not even know it. Moreover, the proposed change fundamentally asks the wrong question: gender markers on passports are used by border control agents, and are expected to align with how those agents read the passport holderâ€™s gender. A mismatch will create needless intimidation and hardship for travelers.Of course most of us will not have our identities challenged under this order. That animus is reserved for trans people, for gender-non-conforming people, for anyone whose genetics, body, dress, voice, or mannerisms donâ€™t quite fit the mold. Those are the people who will suffer under this order. That cruelty should be resisted.]]></content:encoded></item><item><title>How to Create Python Virtual Environments on Ubuntu</title><link>https://dev.to/lgerthal/creating-python-virtual-environments-on-ubuntu-5an7</link><author>Luiz Gustavo Erthal</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 22:34:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[When working on different Python projects, it's often necessary to create isolated environments for specific tasks. This is where Python's Virtual Environments package comes in handy.Use this post as a guide or a cheat sheet for future reference.
  
  
  Creating a Virtual Environment
To create a new virtual environment, use the following command:The .venv directory will contain your virtual environment. Using a dot (.) before the name makes it a hidden folder, which is a common practice. However, you can name it anything you like to suit your project structure.
  
  
  Activating the Virtual Environment
To activate the virtual environment, run:source .venv/bin/activate
Once activated, you can install any required packages and libraries. These installations will be isolated within the .venv directory and will not affect global Python packages.For example, to install pandas:
  
  
  Deactivating the Virtual Environment
To deactivate the virtual environment, simply run:
  
  
  Listing Installed Packages
While the virtual environment is activated, you can list all installed packages using:The only difference between each command is that  will display a human-readable list while  will outputs installed packages in a machine-readable format.
  
  
  Sharing Your Virtual Environment
If you need to share your project, ensure that others can replicate your environment. You can do this by exporting the installed packages to a requirements.txt file:Save your venv into a .txtpip freeze > requirements.txt
If you receive a requirements.txt file from a colleague or friend, you can install all required packages by running:Import the packages from a requirements.txtpip install -r requirements.txt
By following these steps, you can effectively manage your Python environments and ensure consistency across different projects.]]></content:encoded></item><item><title>Rust Rant Contest: std::io::Error, the oversized junk drawer of failure</title><link>https://www.reddit.com/r/rust/comments/1iv3rb3/rust_rant_contest_stdioerror_the_oversized_junk/</link><author>/u/OliveTreeFounder</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 22:33:02 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've been coding in Rust for five years, and  has never been anything but a headache. The error code? Never useful. Itâ€™s impossible to handleâ€”too big, too vagueâ€”so we all end up just passing this bloated mess back to the caller without even knowing whatâ€™s inside or what actually caused the error.But it gets worse. Traits, instead of being parameterized over an  type, just return Result<..., std::io::Error>. Once a trait like this becomes popularâ€”like  or â€”you're stuck. You canâ€™t handle errors properly unless you rewrite every crate that depends on these traits. is a contagious disease infecting the entire ecosystem. We need to stop this pandemic!]]></content:encoded></item><item><title>RandomAffine in PyTorch (2)</title><link>https://dev.to/hyperkai/randomaffine-in-pytorch-2-3d3a</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 22:07:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomAffine() can do random rotation or random affine transformation for an image as shown below:]]></content:encoded></item><item><title>RandomAffine in PyTorch (1)</title><link>https://dev.to/hyperkai/randomaffine-in-pytorch-1-2j6j</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 22:03:16 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[RandomAffine() can do random rotation or random affine transformation for an image as shown below:The 1st argument for initialization is (Required-Type:,  or /( or )):
*Memos:

It's the range of the degrees  so it must be .A degrees value is randomly taken from the range of .A tuple/list must be the 1D with 2 elements.A single value( or ) means [-degrees(min), +degrees(max)].A single value( or ) must be .The 2nd argument for initialization is (Optional-Default:-Type:/( or )):
*Memos:

It must be the 1D with 2 elements.The 1st element is for the horizontal shift randomly taken in the range of -img_width * a < horizontal shift < img_width * a.The 2nd element is for the vertical shift randomly taken in the range of -img_height * b < vertical shift < img_height * b.The 3rd argument for initialization is (Optional-Default:-Type:/( or )):
*Memos:

It's  so it must be .It must be the 1D with 2 elements.A scale value is randomly taken from the range of .The 4th argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can do affine transformation with  and .It's  so it must be .
*Memos:The 1st two elements are the range of .The 2nd two elements are the range of . value is randomly taken from the range of the 1st two elements. value is randomly taken from the range of the 2nd two elements.A tuple/list must be the 1D with 2 or 4 elements.The tuple/list of 2 elements means [shear[0](min), shear[1](max), 0.0(min), 0.0(max)].A single value means [-shear(min), +shear(max), 0.0(min), 0.0(max)].A single value must be .The 5th argument for initialization is (Optional-Default:InterpolationMode.NEAREST-Type:InterpolationMode).The 6th argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can change the background of an image. *The background can be seen when doing rotation or affine transformation for an image.A tuple/list must be the 1D with 1 or 3 elements.The 7th argument for initialization is (Optional-Default:-Type:/( or )):
*Memos:

It can change the center position of an image.It must be the 1D with 2 elements.The 1st argument is (Required-Type: or ()):
*Memos:

]]></content:encoded></item><item><title>Do European M&amp;Ms Actually Taste Better than American M&amp;Ms?</title><link>https://towardsdatascience.com/do-european-mms-actually-taste-better-than-american-mms/</link><author>Erin Wilson</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 21:52:58 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[(Oh, I am the only one whoâ€™s been asking this questionâ€¦? Hm. Well, if you have a minute, please enjoy this exploratory Data Analysis â€” featuring experimental design, statistics, and interactive visualization â€” applied a bit too earnestly to resolve an international debate.)1.1 Background and motivationChocolate is enjoyed around the world. From ancient practices harvesting organic cacao in the Amazon basin, to chocolatiers sculpting edible art in the mountains of Switzerland, and enormous factories in Hershey, Pennsylvania churning out 70 million kisses per day, the nuanced forms and flavors of chocolate have been integrated into many cultures and their customs. While quality can greatly vary across chocolate products, a well-known, shelf-stable, easily shareable form of chocolate are M&Ms. Readily found by convenience store check-out counters and in hotel vending machines, the brightly colored pellets are a popular treat whose packaging is re-branded to fit nearly any commercializable American holiday.While living in Denmark in 2022, I heard a concerning claim: M&Ms manufactured in Europe taste different, and arguably â€œbetter,â€ than M&Ms produced in the United States. While I recognized that fancy European chocolate is indeed quite tasty and often superior to American chocolate, it was unclear to me if the same claim should hold for M&Ms. I learned that many Europeans perceive an â€œunpleasantâ€ or â€œtangyâ€ taste in American chocolate, which is largely attributed toÂ butyric acid, a compound resulting from differences in how milk is treated before incorporation into milk chocolate.But honestly, how much of a difference could this make for M&Ms?Â ? I imagined M&Ms would retain a relatively processed/mass-produced/cheap candy flavor wherever they were manufactured. As the lone American visiting a diverse lab of international scientists pursuing cutting-edge research in biosustainability, I was inspired to break out my data science toolbox and investigate this M&M flavor phenomenon.To quote a European woman, who shall remain anonymous, after she tasted an American M&M while traveling in New York:â€œThey taste so gross. Like vomit. I donâ€™t understand how people can eat this. I threw the rest of the bag away.â€Vomit? Really? In my experience, children raised in the United States had no qualms about eating M&Ms. Growing up, I was accustomed to bowls of M&Ms strategically placed in high traffic areas around my house to provide readily available sugar. Clearly American M&Ms are edible. But are they significantly different and/or inferior to their European equivalent?In response to the anonymous European womanâ€™s scathing report, myself and two other Americans visiting Denmark sampled M&Ms purchased locally in the Lyngby Storcenter FÃ¸tex. We hoped to experience the incredible improvement in M&M flavor that was apparently hidden from us throughout our youths. But curiously, we detected no obvious flavor improvements.Unfortunately, neither preliminary study was able to conduct a side-by-side taste test with proper controls and randomized M&M sampling. Thus, we turn to science.This study seeks to remedy the previous lack of thoroughness and investigate the following questions:Is there aÂ Â that European M&Ms are in fact better than American M&Ms?Can Europeans actually detect a differenceÂ between M&Ms purchased in the US vs in Europe when they donâ€™t know which one they are eating? Or is this aÂ Â amongst Europeans to make Americans feel embarrassed?Are Americans actually taste-blindÂ to American vs European M&Ms? Or can they taste a difference but simply donâ€™t describe this difference as â€œan improvementâ€ in flavor?Can these alleged taste differences beÂ perceived by citizens of other continents? If so, do they find one flavor obviously superior?2.1 Experimental design and data collectionParticipants were recruited by luring â€” er,Â Â them to a social gathering (with the promise of free food) that was conveniently co-located with the testing site. Once a participant agreed to pause socializing and join the study, they were positioned at a testing station with a trained experimenter who guided them through the following steps:Participants sat at a table and received two cups: 1 empty and 1 full of water. With one cup in each hand, the participant was asked to close their eyes, and keep them closed through the remainder of the experiment.The experimenter randomly extracted one M&M with a spoon, delivered it to the participantâ€™s empty cup, and the participant was asked to eat the M&M (eyes still closed).After eating each M&M, the experimenter collected the taste response by asking the participant to report if they thought the M&M tasted: Especially Good, Especially Bad, or Normal.Each participant received a total of 10 M&Ms (5 European, 5 American), one at a time, in a random sequence determined by random.org.Between eating each M&M, the participant was asked to take a sip of water to help â€œcleanse their palate.â€: for each participant, the experimenter recorded the participantâ€™sÂ if this was ambiguous, the participant was asked to list the continent on which they have the strongest memories of eating candy as a child). For each of the 10 M&Ms delivered, the experimenter recorded theÂ Â (â€œDenmarkâ€ or â€œUSAâ€), theÂ Â and the participantâ€™sÂ . Experimenters were also encouraged to jot down any amusing phrases uttered by the participant during the test, recorded underÂ (data availableÂ here).2.2 Sourcing materials and recruiting participantsTwo bags of M&Ms were purchased for this study. The American-sourced M&Ms (â€œUSA M&Mâ€) were acquired at the SFO airport and delivered by the authorâ€™s parents, who visited her in Denmark. The European-sourced M&Ms (â€œDenmark M&Mâ€) were purchased at a local FÃ¸tex grocery store in Lyngby, a little north of Copenhagen.Experiments were conducted at two main time points. The first 14 participants were tested in Lyngby, Denmark in August 2022. They mostly consisted of friends and housemates the author met at the Novo Nordisk Foundation Center for Biosustainability at the Technical University of Denmark (DTU) who came to a â€œgoing away partyâ€ into which the experimental procedure was inserted. A few additional friends and family who visited Denmark were also tested during their travels (e.g. on the train).The remaining 37 participants were tested in Seattle, WA, USA in October 2022, primarily during a â€œTGIF happy hourâ€ hosted by graduate students in the computer science PhD program at the University of Washington. This second batch mostly consisted of students and staff of the Paul. G. Allen School of Computer Science & Engineering (UW CSE) who responded to the weekly Friday summoning to the Allen Center atrium for free snacks and drinks.While this study set out to analyze global trends, unfortunately data was only collected from 51 participants the author was able to lure to the study sites and is not well-balanced nor representative of the 6 inhabited continents of Earth (Figure 1). We hope to improve our recruitment tactics in future work. For now, our analytical power with this dataset is limited to response trends for individuals from North America, Europe, and Asia, highly biased by subcommunities the author happened to engage with in late 2022.While we did not acquire formal approval for experimentation with human test subjects, there were minor risks associated with this experiment: participants were warned that they may be subjected to increased levels of sugar and possible â€œunpleasant flavorsâ€ as a result of participating in this study. No other risks were anticipated.After the experiment however, we unfortunately observed several cases of deflated pride when a participant learned their taste response was skewed more positively towards the M&M type they were not expecting. This pride deflation seemed most severe among European participants who learned their own or their fiancÃ©â€™s preference skewed towards USA M&Ms, though this was not quantitatively measured and cannot be confirmed beyond anecdotal evidence.3.1 Overall response to â€œUSA M&Msâ€ vs â€œDenmark M&Msâ€3.1.1 Categorical response analysis â€” entire datasetIn our first analysis, we count the total number of â€œBadâ€, â€œNormalâ€, and â€œGoodâ€ taste responses and report the percentage of each response received by each M&M type. M&Ms from Denmark more frequently received â€œGoodâ€ responses than USA M&Ms but also more frequently received â€œBadâ€ responses. M&Ms from the USA were most frequently reported to taste â€œNormalâ€ (Figure 2). This may result from the elevated number of participants hailing from North America, where the USA M&M is the default and thus more â€œNormal,â€ while the Denmark M&M was more often perceived as better or worse than the baseline.Figure 2. Qualitative taste response distribution across the whole dataset. The percentage of taste responses for â€œBadâ€, â€œNormalâ€ or â€œGoodâ€ was calculated for each type of M&M. Figure made with Altair.Now letâ€™s break out some Statistics, such as aÂ -squared (X2) test to compare our observed distributions of categorical taste responses. Using the scipy.statsÂ chi2_contingencyÂ function, we built contingency tables of the observed counts of â€œGood,â€ â€œNormal,â€ and â€œBadâ€ responses to each M&M type. Using the X2 test to evaluate the null hypothesis that there is no difference between the two M&Ms, we found theÂ -value for the test statistic to be 0.0185, which is significant at the commonÂ -value cut off of 0.05, but not at 0.01. So a solid â€œmaybe,â€ depending on whether youâ€™d like this result to be significant or not.3.1.2 Quantitative response analysis â€” entire dataset.The X2 test helps evaluate if there is a difference in categorical responses, but next, we want to determine a relative tasteÂ Â between the two M&M types. To do this, we converted taste responses to a quantitative distribution and calculated aÂ Briefly, â€œBadâ€ = 1, â€œNormalâ€ = 2, â€œGoodâ€ = 3. For each participant, we averaged the taste scores across the 5 M&Ms they tasted of each type, maintaining separate taste scores for each M&M type.With the average taste score for each M&M type in hand, we turn to scipy.statsÂ ttest_indÂ (â€œT-testâ€) to evaluate if the means of the USA and Denmark M&M taste scores are different (the null hypothesis being that the means are identical). If the means are significantly different, it would provide evidence that one M&M is perceived as significantly tastier than the other.We found the average taste scores for USA M&Ms and Denmark M&Ms to be quite close (Figure 3), and not significantly different (T-test:Â = 0.721). Thus, across all participants, we do not observe a difference between the perceived taste of the two M&M types (or if you enjoy parsing triple negatives: â€œweÂ Â the null hypothesis that there isÂ Â a differenceâ€).But does this change if we separate participants by continent of origin?3.2 Continent-specific responses to â€œUSA M&Msâ€ vs â€œDenmark M&Msâ€We repeated the above X2 and T-test analyses after grouping participants by their continents of origin. The Australia and South America groups were combined as a minimal attempt to preserve data privacy. Due to the relatively small sample size of even the combined Australia/South America group (=3), we will refrain from analyzing trends for this group but include the data in several figures for completeness and enjoyment of the participants who may eventually read this.3.2.1 Categorical response analysis â€” by continentIn Figure 4, we display both the taste response counts (upper panel,Â note the interactive legend) and the response percentages (lower panel) for each continent group. Both North America and Asia follow a similar trend to the whole population dataset: participants report Denmark M&Ms as â€œGoodâ€ more frequently than USA M&Ms, but also report Denmark M&Ms as â€œBadâ€ more frequently. USA M&Ms were most frequently reported as â€œNormalâ€ (Figure 4).On the contrary, European participants report USA M&Ms as â€œBadâ€ nearly 50% of the time and â€œGoodâ€ only 18% of the time, which is the most negative and least positive response pattern, respectively (when excluding the under-sampled Australia/South America group).Figure 4. Qualitative taste response distribution by continent. Upper panel: counts of taste responses â€” click the legend to interactively filter! Lower panel: percentage of taste responses for each type of M&M. Figure made with Altair.This appeared striking in bar chart form, however only North America had a significant X2Â -value () when evaluating each continentâ€™s difference in taste response profile between the two M&M types. The EuropeanÂ -value is perhaps â€œapproaching significanceâ€ in some circles, but weâ€™re about to accumulate several more hypothesis tests and should be mindful of multiple hypothesis testing (Table 1). A false positive result here would be devastating.When comparing the taste response profiles between two continents for the same M&M type, there are a couple interesting notes. First, we observed no major taste discrepancies between all pairs of continents when evaluating Denmark M&Ms â€” the world seems generally consistent in their range of feelings about M&Ms sourced from Europe (right column X2Â -values, Table 2). To visualize this comparison more easily, we reorganize the bars in Figure 4 to group them by M&M type (Figure 5).Figure 5. Qualitative taste response distribution by M&M type, reported as percentages. (Same data as Figure 4 but re-arranged). Figure made with Altair.However, when comparing continents to each other in response to USA M&Ms, we see larger discrepancies. We found one pairing to be significantly different: European and North American participants evaluated USA M&Ms very differently () (Table 2). It seems very unlikely that this observed difference is by random chance (left column, Table 2).3.2.2 Quantitative response analysis â€” by continentWe again convert the categorical profiles to quantitative distributions to assess continentsâ€™ relative preference of M&M types. For North America, we see that the taste score means of the two M&M types are actually quite similar, but there is a higher density around â€œNormalâ€ scores for USA M&Ms (Figure 6A). The European distributions maintain a bit more of a separation in their means (though not quite significantly so), with USA M&Ms scoring lower (Figure 6B). The taste score distributions of Asian participants is most similar (Figure 6C).Reorienting to compare the quantitative means between continentsâ€™ taste scores for the same M&M type, only the comparison between North American and European participants on USA M&Ms is significantly different based on a T-test () (Figure 6D), though now weÂ Â are in danger of multiple hypothesis testing! Be cautious if you are taking this analysis at all seriously.At this point, I feel myself considering that maybe Europeans are not just making this up. Iâ€™m not saying itâ€™s as dramatic as some of them claim, but perhaps a difference does indeed existâ€¦ To some degree, North American participants also perceive a difference, but the evaluation of Europe-sourced M&Ms is not consistently positive or negative.3.3 M&M taste alignment chartIn our analyses thus far, we did not account for the baseline differences in M&M appreciation between participants. For example, say Person 1 scored all Denmark M&Ms as â€œGoodâ€ and all USA M&Ms as â€œNormalâ€, while Person 2 scored all Denmark M&Ms as â€œNormalâ€ and all USA M&Ms as â€œBad.â€ They would have the same relative preference for Denmark M&Ms over USA M&Ms, but Person 2 perhaps just does not enjoy M&Ms as much as Person 1, and the relative preference signal is muddled by averaging the raw scores.Inspired by the Lawful/Chaotic x Good/Evil alignment chart used in tabletop role playing games like Dungeons & DragonsÂ©, in Figure 7, we establish an M&M alignment chart to help determine the distribution of participants across M&M enjoyment classes.Notably, the upper right quadrant where both M&M types are perceived as â€œGoodâ€ to â€œNormalâ€ is mostly occupied by North American participants and a few Asian participants. All European participants land in the left half of the figure where USA M&Ms are â€œNormalâ€ to â€œBadâ€, but Europeans are somewhat split between the upper and lower halves, where perceptions of Denmark M&Ms range from â€œGoodâ€ to â€œBad.â€An interactive version of Figure 7 is provided below for the reader to explore the counts of various M&M alignment regions.Figure 7 (interactive): click and brush your mouse over the scatter plot to see the counts of continents in different M&M enjoyment regions. Figure made with Altair.3.4 Participant taste response ratioNext, to factor out baseline M&M enjoyment and focus on participantsâ€™ relative preference between the two M&M types, we took the log ratio of each personâ€™sÂ USA M&M taste score averageÂ divided by theirÂ Denmark M&M taste score average.As such, positive scores indicate a preference towards USA M&Ms while negative scores indicate a preference towards Denmark M&Ms.On average, European participants had the strongest preference towards Denmark M&Ms, with Asians also exhibiting a slight preference towards Denmark M&Ms (Figure 8). To the two Europeans who exhibited deflated pride upon learning their slight preference towards USA M&Ms, fear not: you did not think USA M&Ms were â€œGood,â€ but simply ranked them as less bad than Denmark M&Ms (see participant_id 4 and 17 in the interactive version of Figure 7). If you assert that M&Ms are a bad American invention not worth replicating and return to consuming artisanal European chocolate, your honor can likely be restored.North American participants are pretty split in their preference ratios: some fall quite neutrally around 0, others strongly prefer the familiar USA M&M, while a handful moderately prefer Denmark M&Ms. Anecdotally, North Americans who learned their preference skewed towards European M&Ms displayed signals of inflated pride, as if their results signaled posh refinement.Overall, a T-test comparing the distributions of M&M preference ratios shows a possibly significant difference in the means between European and North American participants (), but come on, this is like the 20th p-value Iâ€™ve reported â€” this one is probably too close to call.3.5 Taste inconsistency and â€œPerfect Classifiersâ€For each participant, we assessed their taste score consistency by averaging the standard deviations of their responses to each M&M type, and plotting that against their preference ratio (Figure 9).Figure 9. Participant taste consistency by preference ratio. The x-axis is a participantâ€™s relative M&M preference ratio. The y-axis is the average of the standard deviation of their USA M&M scores and the standard deviation of their Denmark M&M scores. A value of 0 on the y-axis indicates perfect consistency in responses, while higher values indicate more inconsistent responses. Figure made with Altair.Most participants were somewhat inconsistent in their ratings, ranking the same M&M type differently across the 5 samples. This would be expected if the taste difference between European-sourced and American-sourced M&Ms is not actually all that perceptible. Most inconsistent were participants who gave the same M&M type â€œGoodâ€, â€œNormalâ€,Â Â â€œBadâ€ responses (e.g., points high on the y-axis, with wider standard deviations of taste scores), indicating lower taste perception abilities.Intriguingly, four participants â€” one from each continent group â€” were perfectly consistent: they reported the same taste response for each of the 5 M&Ms from each M&M type, resulting in an average standard deviation of 0.0 (bottom of Figure 9). Excluding the one of the four who simply rated all 10 M&Ms as â€œNormalâ€, the other three appeared to be â€œPerfect Classifiersâ€ â€” either rating all M&Ms of one type â€œGoodâ€ and the other â€œNormalâ€, or rating all M&Ms of one type â€œNormalâ€ and the other â€œBad.â€ Perhaps these folks are â€œsuper tasters.â€Another possible explanation for the inconsistency in individual taste responses is that there exists a perceptible taste difference based on the M&M color. Visually, the USA M&Ms were noticeably more smooth and vibrant than the Denmark M&Ms, which were somewhat more â€œsplotchyâ€ in appearance (Figure 10A). M&M color was recorded during the experiment, and although balanced sampling was not formally built into the experimental design, colors seemed to be sampled roughly evenly, with the exception of Blue USA M&Ms, which were oversampled (Figure 10B).We briefly visualized possible differences in taste responses based on color (Figure 11), however we do not believe there are enough data to support firm conclusions. After all, on average each participant would likely only taste 5 of the 6 M&M colors once, and 1 color not at all. We leave further M&M color investigations to future work.We assured each participant that there was no â€œright â€œanswerâ€ in this experiment and that all feelings are valid. While some participants took this to heart and occasionally spent over a minute deeply savoring each M&M and evaluating it as if they were a sommelier, many participants seemed to view the experiment as a competition (which occasionally led to deflated or inflated pride). Experimenters wrote down quotes and notes in conjunction with M&M responses, some of which were a bit â€œcolorful.â€ We provide a hastily rendered word cloud for each M&M type for entertainment purposes (Figure 12) though we caution against reading too far into them without diligent sentiment analysis.Overall, there does not appear to be a â€œglobal consensusâ€ that European M&Ms are better than American M&Ms. However, European participants tended to more strongly express negative reactions to USA M&Ms while North American participants seemed relatively split on whether they preferred M&Ms sourced from the USA vs from Europe. The preference trends of Asian participants often fell somewhere between the North Americans and Europeans.Therefore, Iâ€™ll admit that itâ€™s probable that Europeans are not engaged in a grand coordinated lie about M&Ms. The skew of most European participants towards Denmark M&Ms is compelling, especially since I was the experimenter who personally collected much of the taste response data. If they found a way to cheat, it was done well enough to exceed my own passive perception such that I didnâ€™t notice. However, based on this study, it would appear that a strongly negative â€œvomit flavorâ€ is not universally perceived and does not become apparent to non-Europeans when tasting both M&Ms types side by side.We hope this study has been illuminating! We would look forward to extensions of this work with improved participant sampling, additional M&M types sourced from other continents, and deeper investigations into possible taste differences due to color.Thank you to everyone who participated and ate M&Ms in the name of science!Article by Erin H. Wilson, Ph.D.[1,2,3] who decided the time between defending her dissertation and starting her next job would be best spent on this highly valuable analysis. Hopefully it is clear that this article is intended to be comedicâ€” I do not actually harbor any negative feelings towards Europeans who donâ€™t like American M&Ms, but enjoyed the chance to be sassy and poke fun at our lively debates with overly-enthusiastic data analysis.Shout out to Matt, Galen, Ameya, and Gian-Marco for assisting in data collection![1] Former Ph.D. student in the Paul G. Allen School of Computer Science and Engineering at the University of Washington[2] Former visiting Ph.D. student at the Novo Nordisk Foundation Center for Biosustainability at the Technical University of Denmark[3] Future data scientist at LanzaTech]]></content:encoded></item><item><title>Progzee: Simplifying Proxy Management for Developers</title><link>https://dev.to/progzee/progzee-simplifying-proxy-management-for-developers-5b22</link><author>aldin</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 21:39:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As developers, we often have to navigate the complexities of web scraping, API interactions, and other tasks that require HTTP requests. One of the most recent aspects of these tasks I have dealt with is managing IP proxies. Whether you're rotating proxies to avoid rate limits or ensuring your requests come from different IP addresses, proxy management can quickly become a headache.Hence the Progzee, a Python library designed to simplify IP proxy usage and rotation, making your life as a developer significantly easier.Progzee is a Python library that simplifies the use of IP proxies in HTTP requests. It offers a simplified approach to proxy management, allowing developers to focus on their core tasks rather than getting bogged down by the intricacies of proxy rotation and configuration. With simple features like config file support, CLI integration, and automatic retries for failed requests, Progzee is a neat tool for anyone working with proxies.: Automatically rotate through a list of proxies to distribute your requests.: Easily manage your proxies and settings through a simple  file.: Perform quick tasks like updating proxies or fetching data directly from the command line.: Automatically retries failed requests with the next proxy in the rotation.
  
  
  Simplifying Proxy Management
Managing proxies can be a tedious task. You need to keep track of multiple IP addresses, handle failures, and ensure that your requests are distributed evenly. Progzee abstracts away these complexities, providing a clean and intuitive interface for proxy management.For example, initializing Progzee with a list of proxies is as simple as:If you prefer using a configuration file, Progzee has you covered:And for those who love the command line, Progzee offers CLI support:
progzee update-proxies 
progzee fetch While Progzee is a handy tool, it's essential to emphasize the importance of ethical use. The library is designed for legitimate purposes such as educational projects, testing, and lawful API interactions. Misusing Progzee for activities like unauthorized scraping, bypassing rate limits, or engaging in malicious activities is strictly prohibited.The disclaimer in the README file is clear:This tool is intended for ethical use cases only, including educational purposes, testing, and legitimate API interactions.As developers, we have a responsibility to use our tools ethically and in compliance with all applicable laws and regulations. Progzee is no exception. Always ensure that your usage complies with the Terms of Service of the APIs you interact with.: By automating proxy rotation and error handling, Progzee saves you valuable time that you can spend on more critical aspects of your project.: The intuitive API and configuration options make it easy to integrate Progzee into your existing workflows.: With automatic retries and proxy rotation, Progzee ensures that your requests are more likely to succeed, even in the face of network issues or rate limits.
  
  
  Getting Started with Progzee
Installing Progzee is straightforward. Simply use pip:Here's a quick example to get you started:Or, using a configuration file:Progzee also offers CLI commands for quick tasks:
progzee update-proxies 
progzee fetch Progzee is a simplification for developers who need to manage IP proxies in their HTTP requests. By simplifying proxy management, offering robust error handling, and providing easy-to-use configuration options, Progzee allows you to focus on what really matters: building great software.However, with great power comes great responsibility. Always use Progzee ethically and in compliance with the law. Whether you're working on a web scraping project, interacting with APIs, or testing your applications, Progzee is here to make your life easierâ€”just remember to use it wisely.]]></content:encoded></item><item><title>Tk9.0 canvas demo</title><link>https://opu.peklo.biz/p/25/02/21/1740170028-43ac6.png</link><author>/u/0xjnml</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 20:38:31 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Installing Golang on Windows WSL/WSL2</title><link>https://dev.to/sonishivam10/installing-golang-on-windows-wslwsl2-3pn5</link><author>ShivamS</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 20:23:31 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Official Go documentation might not provide the steps to install Go on WSL/WSL2. If you want to install GoLang and set up the development environment, follow these steps:Open your terminal and use the command to download the specific version.
Note: Replace go1.24.0.linux-amd64.tar.gz with the latest version.wget https://dl.google.com/go/go1.24.0.linux-amd64.tar.gzUnzip:sudo tar -xvf go1.24.0.linux-amd64.tar.gzMove to the correct path.echo "export GOROOT=/usr/local/go" >> ~/.bashrc
echo "export GOPATH=\$HOME/go" >> ~/.bashrc
echo "export PATH=\$GOPATH/bin:\$GOROOT/bin:\$PATH" >> ~/.bashrc
Refresh the terminal:Verify the Go version.]]></content:encoded></item><item><title>Show HN: Slime OS â€“ An open-source app launcher for RP2040 based devices</title><link>https://github.com/abeisgoat/slime_os</link><author>abeisgreat</author><category>dev</category><category>hn</category><pubDate>Fri, 21 Feb 2025 20:22:57 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hey all - this is the software part of my cyberdeck, called the Slimedeck Zero.The Slimedeck Zero is based around this somewhat esoteric device called the PicoVision which is a super cool RP2040 (Raspberry Pi Pico) based device. It outputs relatively high-res video over HDMI while still being super fast to boot with low power consumption.The PicoVision actually uses two RP2040 - one as a CPU and one as a GPU. This gives the CPU plenty of cycles to run bigger apps (and a heavy python stack) and lets the GPU handle some of the rendering and the complex timing HDMI requires. You can do this same thing on a single RP2040, but we get a lot of extra headroom with this double setup.The other unique thing about the PicoVision is it has a physical double-buffer - two PSRAM chips which you manually swap between the CPU and GPU. This removes any possibility of screen tearing since you always know the buffer your CPU is writing to is not being used to generate the on-screen image.For my cyberdeck, I took a PicoVision, hacked a QWERTY keyboard from a smart TV remote, added an expansion port, and hooked it all up to a big 5" 800x480 screen (interlaced up from 400x240 internal resolution).I did a whole Slimedeck Zero build video ( https://www.youtube.com/watch?v=rnwPmoWMGqk ) over on my channel but I really hope Slime OS can have a life of it's own and fit onto multiple form-factors with an ecosystem of apps.I've tried to make it easy and fun to write apps for. There's still a lot broken / missing / tbd but it's enough of a base that, personally, it already sparks that "programming is fun again" vibe so hopefully some other folks can enjoy it!Right now it only runs on the PicoVision but there's no reason it couldn't run on RP2350s or other hardware - but for now I'm more interested in adding more input types (we're limited to the i2c TV remote keyboard I hacked together) and fleshing out the internal APIs so they're stable enough to make apps for it!]]></content:encoded></item><item><title>How I built an AI-Powered Code Reviewer (and you can too).</title><link>https://dev.to/manasmoon_/how-i-built-an-ai-powered-code-reviewer-and-you-can-too-2fk4</link><author>Manas Moon</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 20:07:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[AI is flipping the game for developers, and honestly, I got fed up of seeing people waste hours debugging code what a machine could do it in seconds. So, I thoughtâ€”why not build an AI (agent type program) that does the boring stuff.It all started when I was working on a project and constantly had to review my own code. While AI-powered coding assistants like GitHub Copilot help with writing code, I wondered, why isnâ€™t there an AI to review my code? Thatâ€™s when I decided to build one. So yeah, AI-powered code review isnâ€™t just a convenienceâ€”itâ€™s a lifesaver.Create a command-line tool where I can input a code snippet.Use OpenAIâ€™s GPT-4 to analyze and review the code.(If you donâ€™t know where to find your secret API key: .Return a detailed review with suggestions, best practices, and potential bug fixes.After some research, I decided on the following stack: Python (Standalone CLI tool) OpenAIâ€™s GPT-4 API Terminal-based command-line applicationYou can subscribe to   for more such cool AI prompts.
  
  
  Hereâ€™s your premium prompt:
Please generate a Python script for a terminal-based AI code reviewer application that uses OpenAI's GPT model. 
The script should allow users to paste their code into the terminal, and upon submitting it, receive a detailed review of their code. 

The review should include feedback on:

1. Code Quality
2. Best Practices
3. Potential Bugs
4. Performance Improvements
5. Security Concerns

The script should load the OpenAI API key from a .env file and use it to call OpenAI's GPT-4 model. 
The program should allow the user to input code directly into the terminal, and when they press Enter twice, the review should be generated and displayed.

The Python script should include proper error handling and user prompts for a smooth user experience. 
Also, ensure the code is clean, well-commented, and modular for easy understanding. 
The review output should be structured and clear, providing actionable insights for the user.
Hereâ€™s your hands-on tutorial:ðŸ“Œ Step 1: Setting Up the ProjectI started by creating a new Python project and installing the necessary libraries:ai-code-reviewer ai-code-reviewer
python  venv venv
venv/bin/activate  
pip openai python-dotenv
Next, I created a  file to store my OpenAI API key. You need to edit this file and add your own API key:OPENAI_API_KEY=your_openai_api_key_here
ðŸ“Œ Step 2: Writing the AI Review LogicI created  to handle the review process:ðŸ“Œ Step 3: Running the Code ReviewerTo run the script, simply execute:Then, paste your code into the terminal and press  to get a review.Thatâ€™s how I built my AI-powered code reviewer! ðŸš€Next Steps? Try running it on different code snippets and see how it performs. Let me know if you use it! ðŸ˜ŠðŸš§ PS: Want more AI tips, tricks, and in-depth tutorials? Stay tuned for the next issue, where Iâ€™ll share something more useful for you.Got a favorite AI prompt? Or an AI tool you swear by? Let me know (@Manas Moon)â€”Iâ€™m always excited to learn new ways to use AI.Once again, you can subscribe to   for more such cool AI prompts.]]></content:encoded></item><item><title>Learn how to set up and deploy apps to your own VPS or bare metal using Sidekick. Sidekick is an alternative to Kamal and Coolify, with over 6.5k GitHub stars</title><link>https://dev.to/pmbanugo/learn-how-to-set-up-and-deploy-apps-to-your-own-vps-or-bare-metal-using-sidekick-sidekick-is-an-42od</link><author>Peter Mbanugo</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 19:31:30 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Self-hosting on bare metal and Cloud VM - Deploy like a Pro with Sidekick]]></content:encoded></item><item><title>[First crate] derive_regex: construct a type by parsing a string with regular expressions</title><link>https://www.reddit.com/r/rust/comments/1iuzg1i/first_crate_derive_regex_construct_a_type_by/</link><author>/u/TitaniumBrain</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 19:31:27 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I had an idea and decided it was simple enough to publish my first crate and contribute to the Rust ecosystem.I'm still relatively new to Rust (coming from a few years of Python but I fell in love with the language), so any feedback is welcome. I'm confident my code isn't , but I want to make sure I follow best practices and learn about any Rust .Using this crate - and the associated derive proc macro - you can derive  on an enum or struct to automatically derive the  constructor method.Copied from the readme, here's a couple examples if you don't to click away from Reddit:```rust use derive_regex::FromRegex;pattern = r"^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(?P<level>[A-Z]+)\] (?P<message>.+)$" )] struct LogEntry { timestamp: String, level: String, message: String, }fn main() { let log = "2025-02-20 15:30:00 [INFO] Server started successfully"; let entry = LogEntry::parse(log).expect("Failed to parse log entry"); println!("Parsed log entry: {:#?}", entry); // Parsed log entry: LogEntry { // timestamp: "2025-02-20 15:30:00", // level: "INFO", // message: "Server started successfully", // } } ``````rust use derive_regex::FromRegex;enum CookingCommand { // Parses a command like "chop 3 carrots" #[regex(pattern = r"chop (?P<quantity>\d+) (?P<ingredient>\w+)")] Chop { quantity: u32, ingredient: String },// Parses a command like "boil for 10 minutes" #[regex(pattern = r"boil for (?P<minutes>\d+) minutes")] Boil(u32), // Parses a command like "bake at 375.0 degrees for 25 minutes" #[regex(pattern = r"bake at (?P<temperature>\d+\.\d+) degrees for (?P<minutes>\d+) minutes")] Bake { temperature: f64, minutes: u32 }, // Parses a command like "mix salt and pepper" #[regex(pattern = r"mix (?P<ingredient1>\w+) and (?P<ingredient2>\w+)")] Mix { ingredient1: String, ingredient2: String, }, fn main() { let commands = [ "First, chop 3 carrots", "Don't forget to boil for 10 minutes", "I guess I'll bake at 375.0 degrees for 25 minutes", "mix salt and pepper now", ];for cmd in &commands { if let Ok(command) = CookingCommand::parse(cmd) { match command { CookingCommand::Chop { quantity, ingredient, } => { println!("Chop {} {}(s)", quantity, ingredient); } CookingCommand::Boil(minutes) => { println!("Boil for {} minutes", minutes); } CookingCommand::Bake { temperature, minutes, } => { println!("Bake at {} degrees for {} minutes", temperature, minutes); } CookingCommand::Mix { ingredient1, ingredient2, } => { println!("Mix {} and {}", ingredient1, ingredient2); } } } else { eprintln!("Failed to parse command: {}", cmd); } } // Chop 3 carrots(s) // Boil for 10 minutes // Bake at 375 degrees for 25 minutes // Mix salt and pepper ]]></content:encoded></item><item><title>Self-hosting on bare metal and Cloud VM - Deploy like a Pro with Sidekick</title><link>https://dev.to/pmbanugo/self-hosting-on-bare-metal-and-cloud-vm-deploy-like-a-pro-with-sidekick-2b27</link><author>Peter Mbanugo</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 19:21:08 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Talking about Games</title><link>https://towardsdatascience.com/talking-about-games/</link><author>Dorian Drost</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 19:14:25 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Game theory is a field of research that is quite prominent in Economics but rather unpopular in other scientific disciplines. However, the concepts used in game theory can be of interest to a wider audience, including data scientists, statisticians, computer scientists or psychologists, to name just a few. This article is the opener to a four-chapter tutorial series on the fundamentals of game theory, so stay tuned for the upcoming articles.Â In this article, I will explain the kinds of problems Game Theory deals with and introduce the main terms and concepts used to describe a game. We will see some examples of games that are typically analysed within game theory and lay the foundation for deeper insights into the capabilities of game theory in the later chapters. But before we go into the details, I want to introduce you to some applications of game theory, that show the multitude of areas game-theoretic concepts can be applied to.Â Applications of game theoryDoes it make sense to vote for a small party in an election if this party may not have a chance to win anyway? Is it worth starting a price war with your competitor who offers the same goods as you? Do you gain anything if you reduce your catch rate of overfished areas if your competitors simply carry on as before? Should you take out insurance if you believe that the government will pay for the reconstruction after the next hurricane anyway? And how should you behave in the next auction where you are about to bid on your favourite Picasso painting?Â All these questions (and many more) live within the area of applications that can be modelled with game theory. Whenever a situation includes strategic decisions in interaction with others, game-theoretic concepts can be applied to describe this situation formally and search for decisions that are not made intuitively but that are backed by a notion of rationality. Key to all the situations above is that your decisions depend on other peopleâ€™s behaviour. If everybody agrees to conserve the overfished areas, you want to play along to preserve nature, but if you think that everybody else will continue fishing, why should you be the only one to stop? Likewise, your voting behaviour in an election might heavily depend on your assumptions about other peopleâ€™s votes. If nobody votes for that candidate, your vote will be wasted, but if everybody thinks so, the candidate doesnâ€™t have a chance at all. Maybe there are many people who say â€œI would vote for him if others vote for him tooâ€. Similar situations can happen in very different situations. Have you ever thought about having food delivered and everybody said â€œYou donâ€™t have to order anything because of me, but if you order anyway, Iâ€™d take some french friesâ€? All these examples can be applications of game theory, so letâ€™s start understanding what game theory is all about.Â When you hear the word , you might think of  such as Minecraft,  such as Monopoly, or  such as poker. There are some common principles to all these games: We always have some  who are allowed to do certain things determined by the gameâ€™s . For example, in poker, you can raise, check or give up. In Monopoly, you can buy a property you land on or donâ€™t buy it. What we also have is some notion of how to  the game. In poker, you have to get the best hand to win and in Monopoly, you have to be the last person standing after everybody went bankrupt. That also means that some actions are better than others in some scenarios. If you have two aces on the hand, staying in the game is better than giving up.Â When we look at games from the perspective of game theory, we use the same concepts, just more formally.A game consists of a set of  = {1, .., n}, where each player has a set of  and a . The set of strategies is determined by the rules of the games. For example, it could be S = {check, raise, give-up} and the player would have to decide which of these actions they want to use. The utility function u (also called ) describes how valuable a certain action of a player would be, given the actions of the other players. Every player wants to maximize their utility, but now comes the tricky part: The utility of an action of yours depends on the other playersâ€™ actions. But for them, the same applies: Their actionsâ€™ utilities depend on the actions of the other players (including yours).Â Letâ€™s consider a well-known game to illustrate this point. In rock-paper-scissors, we have n=2 players and each player can choose between three actions, hence the strategy set is S={rock, paper, scissors} for each player. But the utility of an action depends on what the other player does. If our opponent chooses rock, the utility of paper is high (1), because paper beats rock. But if your opponent chooses scissors, the utility of paper is low (-1), because you would lose. Finally, if your opponent chooses paper as well, you reach a draw and the utility is 0.Â Instead of writing down the utility function for each case individually, it is common to display games in a matrix like this:The first player decides for the row of the matrix by selecting his action and the second player decides for the column. For example, if player 1 chooses paper and player 2 chooses scissors, we end up in the cell in the third column and second row. The value in this cell is the utility for both players, where the first value corresponds to player 1 and the second value corresponds to player 2. (-1,1) means that player 1 has a utility of -1 and player 2 has a utility of 1. Scissors beat paper.Â Now we have understood the main components of a game in game theory. Let me add a few more hints on what game theory is about and what assumptions it uses to describe its scenarios.Â We often assume that the players select their actions at the same time (like in rock-paper-scissors). We call such games  games. There are also  games in which players take turns deciding on their actions (like in chess). We will consider these cases in a later chapter of this tutorial.Â In game theory, it is typically assumed that the players  with each other so they canâ€™t come to an agreement before deciding on their actions. In rock-paper-scissors, you wouldnâ€™t want to do that anyway, but there are other games where communication would make it easier to choose an action. However, we will always assume that communication is not possible.Â Game theory is considered a  theory, not a descriptive one. That means we will analyse games concerning the question â€œWhat would be the rational solution?â€ This may not always be what people do in a likewise situation in reality. Such descriptions of real human behaviour are part of the research field of behavioural economics, which is located on the border between Psychology and economics.Â Let us become more familiar with the main concepts of game theory by looking at some typical games that are often analyzed. Often, such games are derived from are story or scenario that may happen in the real world and require people to decide between some actions. One such story could be as follows:Â Say we have two criminals who are suspected of having committed a crime. The police have some circumstantial evidence, but no actual proof for their guilt. Hence they question the two criminals, who now have to decide if they want to confess or deny the crime. If you are in the situation of one of the criminals, you might think that denying is always better than confessing, but now comes the tricky part: The police propose a deal to you. If you confess while your partner denies, you are considered a crown witness and will not be punished. In this case, you are free to go but your partner will go to jail for six years. Sounds like a good deal, but be aware, that the outcome also depends on your partnerâ€™s action. If you both confess, there is no crown witness anymore and you both go to jail for three years. If you both deny, the police can only use circumstantial evidence against you, which will lead to one year in prison for both you and your partner. But be aware, that your partner is offered the same deal. If you deny and he confesses, he is the crown witness and you go to jail for six years. How do you decide?The game derived from this story is called the  and is a typical example of a game in game theory. We can visualize it as a matrix just like we did with rock-paper-scissors before and in this matrix, we easily see the dilemma the players are in. If both deny, they receive a rather low punishment. But if you assume that your partner denies, you might be tempted to confess, which would prevent you from going to jail. But your partner might think the same, and if you both confess, you both go to jail for longer. Such a game can easily make you go round in circles. We will talk about solutions to this problem in the next chapter of this tutorial. First, letâ€™s consider some more examples.Â You and your friend want to go to a concert together. You are a fan of Bachâ€™s music but your friend favors the Russian 20th. century composer Igor Stravinsky. However, you both want to avoid being alone in any concert. Although you prefer Bach over Stravinsky, you would rather go to the Stravinsky concert with your friend than go to the Bach concert alone. We can create a matrix for this game:Â You decide for the row by going to the Bach or Stravinsky concert and your friend decides for the column by going to one of the concerts as well. For you, it would be best if you both chose Bach. Your reward would be 2 and your friend would get a reward of 1, which is still better for him than being in the Stravinsky concert all by himself. However, he would be even happier, if you were in the Stravinsky concert together.Â Do you remember, that we said players are not allowed to communicate before making their decision? This example illustrates why. If you could just call each other and decide where to go, this would not be a game to investigate with game theory anymore. But you canâ€™t call each other so you just have to go to any of the concerts and hope you will meet your friend there. What do you do?Â A third example brings us to the realm of international politics. The world would be a much happier place with fewer firearms, wouldnâ€™t it? However, if nations think about disarmament, they also have to consider the choices other nations make. If the USA disarms, the Soviet Union might want to rearm, to be able to attack the USAâ€Šâ€”â€Šthat was the thinking during the Cold War, at least. Such a scenario could be described with the following matrix:Â As you see, when both nations disarm, they get the highest reward (3 each), because there are fewer firearms in the world and the risk of war is minimized. However, if you disarm, while the opponent upgrades, your opponent is in the better position and gets a reward of 2, while you only get 0. Then again, it might have been better to upgrade yourself, which gives a reward of 1 for both players. That is better than being the only one who disarms, but not as good as it would get if both nations disarmed.Â All these examples have one thing in common: There is no single option that is always the best. Instead, the utility of an action for one player always depends on the other playerâ€™s action, which, in turn, depends on the first playerâ€™s action and so on. Game theory is now interested in finding the optimal solution and deciding what would be the rational action; that is, the action that maximizes the expected reward. Different ideas on how exactly such a solution looks like will be part of the next chapter in this series.Â Before continuing with finding solutions in the next chapter, let us recap what we have learned so far.Â A game consists of , that decide for , which have a  or .Â The utility/reward of an action  on the other playersâ€™ actions.Â In  games, players decide for their actions simultaneously. In  games, they take turns.Â The  is a very popular example of a game in game theory.Games become increasingly interesting if there is no single action that is better than any other.Â Now that you are familiar with how games are described in game theory, you can check out the next chapter to learn how to find solutions for games in game theory.Â The topics introduced here are typically covered in standard textbooks on game theory. I mainly used this one, which is written in German though:Â Bartholomae, F., & Wiens, M. (2016). Spieltheorie. Ein anwendungsorientiertes Lehrbuch. Wiesbaden: Springer Fachmedien Wiesbaden.An alternative in English language could be this one:Â Espinola-Arredondo, A., & MuÃ±oz-Garcia, F. (2023). Game Theory: An Introduction with Step-by-step Examples. Springer Nature.Game theory is a rather young field of research, with the first main textbook being this one:Â Von Neumann, J., & Morgenstern, O. (1944). Theory of games and economic behavior. to be notified of my future posts.]]></content:encoded></item><item><title>How Rocket Companies modernized their data science solution on AWS</title><link>https://aws.amazon.com/blogs/machine-learning/how-rocket-companies-modernized-their-data-science-solution-on-aws/</link><author>Dian Xu, Joel Hawkins</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 18:45:46 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This post was written with Dian Xu and Joel Hawkins of Rocket Companies.Rocket Companies is a Detroit-based FinTech company with a mission to â€œHelp Everyone Homeâ€. With the current housing shortage and affordability concerns, Rocket simplifies the homeownership process through an intuitive and AI-driven experience. This comprehensive framework streamlines every step of the homeownership journey, empowering consumers to search, purchase, and manage home financing effortlessly. Rocket integrates home search, financing, and servicing in a single environment, providing a seamless and efficient experience.The Rocket brand is a synonym for offering simple, fast, and trustworthy digital solutions for complex transactions. Rocket is dedicated to helping clients realize their dream of homeownership and financial freedom. Since its inception, Rocket has grown from a single mortgage lender to an network of businesses that creates new opportunities for its clients.Rocket takes a complicated process and uses technology to make it simpler. Applying for a mortgage can be complex and time-consuming. Thatâ€™s why we use advanced technology and data analytics to streamline every step of the homeownership experience, from application to closing. By analyzing a wide range of data points, weâ€™re able to quickly and accurately assess the risk associated with a loan, enabling us to make more informed lending decisions and get our clients the financing they need.Our goal at Rocket is to provide a personalized experience for both our current and prospective clients. Rocketâ€™s diverse product offerings can be customized to meet specific client needs, while our team of skilled bankers must match with the best client opportunities that align with their skills and knowledge. Maintaining strong relationships with our large, loyal client base and hedge positions to cover financial obligations is key to our success. With the volume of business we do, even small improvements can have a significant impact.In this post, we share how we modernized Rocketâ€™s data science solution on AWS to increase the speed to delivery from eight weeks to under one hour, improve operational stability and support by reducing incident tickets by over 99% in 18 months, power 10 million automated data science and AI decisions made daily, and provide a seamless data science development experience.Rocketâ€™s legacy data science environment challengesRocketâ€™s previous data science solution was built around Apache Spark and combined the use of a legacy version of the Hadoop environment and vendor-provided Data Science Experience development tools. The Hadoop environment was hosted on Amazon Elastic Compute Cloud (Amazon EC2) servers, managed in-house by Rocketâ€™s technology team, while the data science experience infrastructure was hosted on premises. Communication between the two systems was established through Kerberized Apache Livy (HTTPS) connections over AWS PrivateLink.Data exploration and model development were conducted using well-known machine learning (ML) tools such as Jupyter or Apache Zeppelin notebooks. Apache Hive was used to provide a tabular interface to data stored in HDFS, and to integrate with Apache Spark SQL. Apache HBase was employed to offer real-time key-based access to data. Model training and scoring was performed either from Jupyter notebooks or through jobs scheduled by Apacheâ€™s Oozie orchestration tool, which was part of the Hadoop implementation.Despite the benefits of this architecture, Rocket faced challenges that limited its effectiveness:Accessibility limitations: The data lake was stored in HDFS and only accessible from the Hadoop environment, hindering integration with other data sources. This also led to a backlog of data that needed to be ingested.Steep learning curve for data scientists: Many of Rocketâ€™s data scientists did not have experience with Spark, which had a more nuanced programming model compared to other popular ML solutions like scikit-learn. This created a challenge for data scientists to become productive.Responsibility for maintenance and troubleshooting: Rocketâ€™s DevOps/Technology team was responsible for all upgrades, scaling, and troubleshooting of the Hadoop cluster, which was installed on bare EC2 instances. This resulted in a backlog of issues with both vendors that remained unresolved.Balancing development vs. production demands: Rocket had to manage work queues between development and production, which were always competing for the same resources. Rocket sought to support more real-time and streaming inferencing use cases, but this was limited by the capabilities of MLeap for real-time models and Spark Streaming for streaming use cases, which were still experimental at that time.Inadequate data security and DevOps support â€“ The previous solution lacked robust security measures, and there was limited support for development and operations of the data science work.Rocketâ€™s legacy data science architecture is shown in the following diagram.The diagram depicts the flow; the key components are detailed below: Data is ingested into the system using Attunity data ingestion in Spark SQL.Data Storage and Processing: All compute is done as Spark jobs inside of a Hadoop cluster using Apache Livy and Spark. Data is stored in HDFS and is accessed via Hive, which provides a tabular interface to the data and integrates with Spark SQL. HBase is employed to offer real-time key-based access to data. Data exploration and model development are conducted using tools such as Jupyter or Orchestration, which communicate with the Spark server over Kerberized Livy connection.Model Training and Scoring: Model training and scoring is performed either from Jupyter notebooks or through jobs scheduled by Apacheâ€™s Oozie orchestration tool, which is part of the Hadoop implementation.Rocketâ€™s migration journeyAt Rocket, we believe in the power of continuous improvement and constantly seek out new opportunities. One such opportunity is using data science solutions, but to do so, we must have a strong and flexible data science environment.To address the legacy data science environment challenges, Rocket decided to migrate its ML workloads to the Amazon SageMaker AI suite. This would allow us to deliver more personalized experiences and understand our customers better. To promote the success of this migration, we collaborated with the AWS team to create automated and intelligent digital experiences that demonstrated Rocketâ€™s understanding of its clients and kept them connected.We implemented an AWS multi-account strategy, standing up Amazon SageMaker Studio in a build account using a network-isolated Amazon VPC. This allows us to separate development and production environments, while also improving our security stance.We moved our new work to SageMaker Studio and our legacy Hadoop workloads to Amazon EMR, connecting to the old Hadoop cluster using Livy and SageMaker notebooks to ease the transition. This gives us access to a wider range of tools and technologies, enabling us to choose the most appropriate ones for each problem weâ€™re trying to solve.SageMaker AI has been instrumental in empowering our data science community with the flexibility to choose the most appropriate tools and technologies for each problem, resulting in faster development cycles and higher model accuracy. With SageMaker Studio, our data scientists can seamlessly develop, train, and deploy models without the need for additional infrastructure management.As a result of this modernization effort, SageMaker AI enabled Rocket to scale our data science solution across Rocket Companies and integrate using a hub-and-spoke model. The ability of SageMaker AI to automatically provision and manage instances has allowed us to focus on our data science work rather than infrastructure management, increasing the number of models in production by five times and data scientistsâ€™ productivity by 80%.Our data scientists are empowered to use the most appropriate technology for the problem at hand, and our security stance has improved. Rocket can now compartmentalize data and compute, as well as compartmentalize development and production. Additionally, we are able to provide model tracking and lineage using Amazon SageMaker Experiments and artifacts discoverable using the SageMaker model registry and Amazon SageMaker Feature Store. All the data science work has now been migrated onto SageMaker, and all the old Hadoop work has been migrated to Amazon EMR.Overall, SageMaker AI has played a critical role in enabling Rocketâ€™s modernization journey by building a more scalable and flexible ML framework, reducing operational burden, improving model accuracy, and accelerating deployment times.The successful modernization allowed Rocket to overcome our previous limitations and better support our data science efforts. We were able to improve our security stance, make work more traceable and discoverable, and give our data scientists the flexibility to choose the most appropriate tools and technologies for each problem. This has helped us better serve our customers and drive business growth.Rocketâ€™s new data science solution architecture on AWS is shown in the following diagram.The solution consists of the following components:Data is ingested into the data account from on-premises and external sources. Raw data is refined into consumable layers (raw, processed, conformed, and analytical) using a combination of AWS Glue extract, transform, and load (ETL) jobs and EMR jobs. Refined data is registered in the data accountâ€™s AWS Glue Data Catalog and exposed to other accounts via Lake Formation. Analytic data is stored in Amazon Redshift. Lake Formation makes this data available to both the build and compute accounts. For the build account, access to production data is restricted to read-only. Data science development is done using SageMaker Studio. Data engineering development is done using AWS Glue Studio. Both disciplines have access to Amazon EMR for Spark development. Data scientists have access to the entire SageMaker ecosystem in the build account.Â SageMaker trained models developed in the build account are registered with an MLFlow instance. Code artifacts for both data science activities and data engineering activities are stored in Git. Deployment initiation is controlled as part of CI/CD.Â We have a number of workflow triggers. For online scoring, we typically provide an external-facing endpoint using Amazon EKS with Istio. We have numerous jobs that are launched by AWS Lambda functions that in turn are triggered by timers or events. Processes that run may include AWS Glue ETL jobs, EMR jobs for additional data transformations or model training and scoring activities, or SageMaker pipelines and jobs performing training or scoring activities.Weâ€™ve evolved a long way in modernizing our infrastructure and workloads. We started our journey supporting six business channels and 26 models in production, with dozens in development. Deployment times stretched for months and required a team of three system engineers and four ML engineers to keep everything running smoothly. Despite the support of our internal DevOps team, our issue backlog with the vendor was an unenviable 200+.Today, we are supporting nine organizations and over 20 business channels, with a whopping 210+ models in production and many more in development. Our average deployment time has gone from months to just weeksâ€”sometimes even down to mere days! With just one part-time ML engineer for support, our average issue backlog with the vendor is practically non-existent. We now support over 120 data scientists, ML engineers, and analytical roles. Our framework mix has expanded to include 50% SparkML models and a diverse range of other ML frameworks, such as PyTorch and scikit-learn. These advancements have given our data science community the power and flexibility to tackle even more complex and challenging projects with ease.The following table compares some of our metrics before and after migration.New data ingestion project took 4â€“8 weeksData-driven ingestion takes under one hourOperation Stability and SupportabilityOver a hundred incidents and tickets in 18 monthsFewer incidents: one per 18 monthsData scientists spent 80% of their time waiting on their jobs to runSeamless data science development experiencePowers 10 million automated data science and AI decisions made dailyThroughout the journey of modernizing our data science solution, weâ€™ve learned valuable lessons that we believe could be of great help to other organizations who are planning to undertake similar endeavors.First, weâ€™ve come to realize that managed services can be a game changer in optimizing your data science operations.The isolation of development into its own account while providing read-only access to production data is a highly effective way of enabling data scientists to experiment and iterate on their models without putting your production environment at risk. This is something that weâ€™ve achieved through the combination of SageMaker AI and Lake Formation.Another lesson we learned is the importance of training and onboarding for teams. This is particularly true for teams that are moving to a new environment like SageMaker AI. Itâ€™s crucial to understand the best practices of utilizing the resources and features of SageMaker AI, and to have a solid understanding of how to move from notebooks to jobs.Lastly, we found that although Amazon EMR still requires some tuning and optimization, the administrative burden is much lighter compared to hosting directly on Amazon EC2. This makes Amazon EMR a more scalable and cost-effective solution for organizations who need to manage large data processing workloads.This post provided overview of the successful partnership between AWS and Rocket Companies. Through this collaboration, Rocket Companies was able to migrate many ML workloads and implement a scalable ML framework. Ongoing with AWS, Rocket Companies remains committed to innovation and staying at the forefront of customer satisfaction.Donâ€™t let legacy systems hold back your organizationâ€™s potential. Discover how AWS can assist you in modernizing your data science solution and achieving remarkable results, similar to those achieved by Rocket Companies.Â is the Senior Director of Engineering in Data at Rocket Companies, where she leads transformative initiatives to modernize enterprise data platforms and foster a collaborative, data-first culture. Under her leadership, Rocketâ€™s data science, AI & ML platforms power billions of automated decisions annually, driving innovation and industry disruption. A passionate advocate for Gen AI and cloud technologies, Xu is also a sought-after speaker at global forums, inspiring the next generation of data professionals. Outside of work, she channels her love of rhythm into dancing, embracing styles from Bollywood to Bachata as a celebration of cultural diversity.Â is a Principal Data Scientist at Rocket Companies, where he is responsible for the data science and MLOps platform. Joel has decades of experience developing sophisticated tooling and working with data at large scales. A driven innovator, he works hand in hand with data science teams to ensure that we have the latest technologies available to provide cutting edge solutions. In his spare time, he is an avid cyclist and has been known to dabble in vintage sports car restoration.Venkata Santosh Sajjan AllaÂ is a Senior Solutions Architect at AWS Financial Services. He partners with North American FinTech companies like Rocket and other financial services organizations to drive cloud and AI strategy, accelerating AI adoption at scale. With deep expertise in AI & ML, Generative AI, and cloud-native architecture, he helps financial institutions unlock new revenue streams, optimize operations, and drive impactful business transformation. Sajjan collaborates closely with Rocket Companies to advance its mission of building an AI-fueled homeownership platformÂ toÂ Help Everyone Home. Outside of work, he enjoys traveling, spending time with his family, and is a proud father to his daughter. is a Principal Solutions Architect at AWS based in Chicago, IL. She is passionate about helping customers design cloud architectures using AWS services to solve business challenges and is enthusiastic about solving a variety of ML use cases for AWS customers. When sheâ€™s not working, Alak enjoys spending time with her daughters and exploring the outdoors with her dogs.]]></content:encoded></item><item><title>Interop 2025: another year of web platform improvements</title><link>https://web.dev/blog/interop-2025?hl=en</link><author>/u/feross</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 18:45:42 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
  Published: February 13, 2025
After the huge success of Interop 2024, the project returns today with a new set
of focus areas for 2025. While we couldn't include every suggestion made this
year, the final list reaches across the web platformâ€”from CSS to
performance-related features.In addition, and as in previous years, there's a set of areas for investigation.
These are areas where we don't have enough information or tests to include as a
focus area, but the group feels some work should be done to get them to a stage
where we can include them.We're excited about all of these features and the improvements this year's
project will bring to the platform. And, as with last
year, the project will make a whole set of things
Baseline Newly available. This post shares more information about some of the
features on the list, with links to information to find out more.Several of the features included in Interop 2025 are features that you flagged
up as important in the State of CSS 2024 survey. They'll help you create more
beautiful and performant user experiences.This feature lets you anchor a positioned element to an anchor, it's
particularly useful when displaying popovers.Same-document view transitionsAlso included this year are view transitions, specifically same-document view
transitions, and the  CSS property.The  propertyThe

property has been Baseline Newly available since September 2024. It lets you
create effects behind your content. For example to blur or create effects that
you might expect to only be available in a graphics application.Despite being mostly interoperable, you can see from the failing tests for
that
there are bugs and issues in those implementations. While these issues might not
be a problem to everyone, we know that many of you do run into them, it'll be
great to get this feature working really well.The  element is a disclosure widget which can be expanded to reveal
additional content. The  element itself is Baseline Widely available.
However, there are a number of related features that have been more recently
added that make  more
useful.The  and  CSS pseudo-elements.Using  to toggle the content instead of .Auto-expanding the  element with find-in-page matches.The  attribute, which hides an element until it is found
using the browser's find-in-page search or it is directly navigated to by
following a URL fragment.The  at-rule lets you scope your selectors to a sub-tree of the DOM, or
even select between an upper and lower boundary in the tree. For example, the
following CSS only selects  elements inside an element with a class of
.In the next example, an upper and lower bound is used. The  element is
only selected if it's between the element with a class of  and also
outside of the element with a class of .Without the  event, there's no reliable way to detect that a scroll is
complete. The best you could do is to use  to check if the scroll
has stopped for a period. This makes it more like a scroll has paused event, not
a scroll has ended event.With the  event, the browser does all this difficult evaluation for
you.The  propertyThe

property is a shorthand for , ,
, and text-decoration-thickness. It's deemed Baseline
Widely available, however in Safari the only unprefixed shorthand property that
works is . It's this that will be addressed during 2025.The CSS 
property has a number of possible values, many of which are designed to lay out
scripts that display vertically. Sometimes however, you want to lay out text
vertically as part of a design, rather than for language support reasons. The
 and  values are designed for this, but have suffered
from poor browser compatibility. This should be fixed during 2025.In addition, the logical CSS properties  and 
are included. These make it possible to control what happens when content
overflows boxes, regardless of the writing mode.Web Vitals can help you quantify the
experience of your site and identify opportunities to improve. The Web Vitals
initiative aims to simplify the landscape, and help sites focus on the metrics
that matter most, the Core Web Vitals.Event Timing API (for INP)This year, the work will focus on the following features:JavaScript string builtins: to make the WebAssembly built-in string
functions mirror a subset of the JavaScript String API so it can be callable
without JavaScript glue code.Resizable buffers integration: to integrate WebAssembly into JavaScript code
that uses resizable buffers.This year the project includes a removal from the platform. Mutation
events are deprecated
and replaced with the much more performant and Baseline Widely available
Mutation Observer
API. Chrome
removed these events in Chrome 126, and this focus area is to remove them from
all browsers.Descriptions of the full list of features can be found in the project README.
Also, read the posts from the other companies working on Interop 2025.]]></content:encoded></item><item><title>Meanwhile at the Pentagon</title><link>https://www.reddit.com/r/artificial/comments/1iuwy03/meanwhile_at_the_pentagon/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 17:49:27 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ðŸ““ How to Use Jupyter Notebooks in VSCode with Poetry Virtual Environments ðŸš€</title><link>https://dev.to/dorinandreidragan/how-to-use-jupyter-notebooks-in-vscode-with-poetry-virtual-environments-2kml</link><author>dorinandreidragan</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 17:48:12 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you're a developer using Jupyter Notebooks and Poetry, you might face an issue where VSCode doesn't automatically recognize the Poetry virtual environments. This guide will show you how to solve this problem by updating the VSCode user settings accordingly.
  
  
  Step 1: Create a New Project with Poetry
Navigate to your project directory and create a new project using Poetry:my_jupyter_project
my_jupyter_project
poetry init
Follow the prompts to set up your  file.
  
  
  Step 2: Install and Activate the Virtual Environment
To create and activate the virtual environment, run:poetry poetry shell
This will create a virtual environment and activate it, isolating your project's dependencies.
  
  
  Step 3: Open VSCode and Install the Jupyter Extension
Open your project folder in VSCode:Make sure to install the Jupyter extension in VSCode if you haven't already. You can find it in the Extensions view by searching for "Jupyter".
  
  
  Step 4: Configure VSCode User Settings âš™ï¸
Ensure that VSCode is configured to recognize Poetry virtual environments. Add the following settings to your  file:Replace  with your actual username.
  
  
  Step 5: Create and Run a Jupyter Notebook ðŸ““
With the virtual environment set up, you can now create a Jupyter Notebook:Open the Command Palette () and type Jupyter: Create New Blank Notebook.Select the appropriate kernel (your Poetry virtual environment) from the kernel picker in the top-right corner of the notebook.Start coding in your Jupyter Notebook and enjoy the power of Poetry and Jupyter combined! ðŸŽ‰By following these steps, you can efficiently manage your Jupyter Notebook projects using Poetry within VSCode, ensuring that your dependencies are well-organized and isolated. Happy coding! ðŸ’»âœ¨]]></content:encoded></item><item><title>[R] MLGym: A New Framework and Benchmark for Advancing AI Research Agents</title><link>https://www.reddit.com/r/MachineLearning/comments/1iuwuyu/r_mlgym_a_new_framework_and_benchmark_for/</link><author>/u/Rybolos</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 17:46:14 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.]]></content:encoded></item><item><title>What is LangGraph and How to Use It for Building AI Agents</title><link>https://dev.to/juanstoppa/what-is-langgraph-and-how-to-use-it-for-building-ai-agents-4bj2</link><author>Juan Stoppa</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 17:33:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I keep finding myself going back to the LangChain documentation to figure out how to use LangGraph. While the documentation is comprehensive, it can be overwhelming to navigate, especially when you're trying to build advanced AI agents.This guide is my attempt to consolidate the key concepts and practical implementations of LangGraph in one place. Whether you're building a conversational agent that needs to remember context, a multi-step reasoning agent, or a complex workflow that coordinates multiple AI components, LangGraph provides the framework to make it happen. I created this reference for myself but I hope it helps others who want a more straightforward explanation of how to use LangGraph effectively.LangGraph is a framework that brings state machines to Large Language Model (LLM) applications. It's particularly useful when you need to build complex, stateful applications with LLMs. The framework allows you to structure your application as a graph, where each node represents a specific task or state, and edges represent transitions between states.Let's start with a simple example to understand the core concepts: we are developing a simple agent that collects information and  processes it, the actions  of collecting and processing information are just fixed actions but they can be replaced with more complex actions.This example shows the basic structure of a LangGraph application:Define your state using TypedDict, it contains the information that the workflow will need to keep track of.Create functions for each state, these functions are the actions that the workflow will perform.Build a graph with nodes and edges, the nodes are the states and the edges are the transitions between them.Compile the graph into a runnable application, this will create a callable object that can be invoked with an initial state.I also added a simple way to save the graph visualization as a PNG file, this will work if you are running this example locally and should save a file that will show the graph structure like below.The graph is a good way to understand the workflow, it shows the nodes and the edges between them, the entry and finish points and the state of the workflow.This particular example is not very useful but it shows the core concepts of LangGraph, you can simply replace the fixed actions with more complex ones and build a useful agent.
  
  
  Serving LangGraph as a Web Service
While LangGraph itself doesn't include built-in server capabilities, you can easily create a web service using FastAPI to serve your LangGraph workflows. Below I have modified the previous example to add a simple FastAPI server that allows you to run the workflow from a web interface.You can run this locally or using the Hugging Face space below, this is the URL to access the swagger API for this example https://jstoppa-langgraph-basic-example-api.hf.space/docs, the API has an end point to run the agent and it returns the messages we've seen in our previous example (see below the results). In simple words, the API has an end point to run the agent and it returns the messages we've seen in our previous example.
  
  
  Making it all work with a more interesting example
We are now going to create a more interesting example, an AI agent that does code reviews, this is far from a production-ready agent but it will give us a better understanding of how to use LangGraph.The screenshot below shows the interface for the code review agent, the user can enter a code and the agent will return a report with the code analysis. This interface uses the Gradio library to create a simple web interface, this saves a lot of time compared to building a full web app.The full code is provided after this but the most impportant part of the example is the graph, it shows the nodes and the edges between them, the entry and finish points and the state of the workflow, this is how the graph will look through the code and analyse the code with different agents that are especialised on different aspects. This is a very similar apporach we've seen in the previous example but it contains more actions and the actions do use LLMs to analyse the code.the full code for the agent is below and it can also be found in Hugging Face below.LangGraph makes it easier to build AI agents that need to manage complex workflows. The graph-based approach keeps things organised and flexible, especially when dealing with multi-step processes or memory.Even though LangGraph doesnâ€™t come with built-in server features, it works well with FastAPI and other frameworks to serve agents as APIs. Whether youâ€™re building a chatbot, a code reviewer, or something else entirely, it gives you a solid foundation to work with.Iâ€™m still experimenting and learning, so Iâ€™ll keep updating this post as I find better ways to use LangGraph. If youâ€™ve built something cool with it or have any questions, let me knowâ€”happy to chat!I hope you like this article, if you want to hear more follow me on X at @juanstoppa where I regularly post about AI ]]></content:encoded></item><item><title>[D] Dimensionality reduction is bad practice?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iuwgcu/d_dimensionality_reduction_is_bad_practice/</link><author>/u/Ready_Plastic1737</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 17:30:22 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I was given a problem statement and data to go along with it. My initial intuition was "what features are most important in this dataset and what initial relationships can i reveal?"I proposed t-sne, PCA, or UMAP to observe preliminary relationships to explore but was immediately shut down because "reducing dimensions means losing information."which i know is true but..._____________can some of you add to the ___________? what would you have said?]]></content:encoded></item><item><title>Daniel Roy Greenfeld: TIL: Undecorating a functools.wraps decorated function</title><link>https://daniel.feldroy.com/posts/til-2025-02-unwrapping-a-wrapped-function</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 21 Feb 2025 17:24:54 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Another reason to use functools.wraps!]]></content:encoded></item><item><title>NVIDIAâ€™s New AI: The Age of Real Time Game Making Is Here!</title><link>https://www.youtube.com/watch?v=FpZ_6bxx5v8</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/FpZ_6bxx5v8?version=3" length="" type=""/><pubDate>Fri, 21 Feb 2025 17:21:28 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[â¤ï¸ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

ðŸ“ Magic 1-For-1:
https://magic-141.github.io/Magic-141/
https://github.com/Open-Magic-Video/Magic-1-For-1
https://arxiv.org/abs/2502.07701v1

ðŸ“ Phantom: https://phantom-video.github.io/Phantom/

ðŸ“ Relighting paper: https://bujiazi.github.io/light-a-video.github.io/

ðŸ“ Stepfun:
https://github.com/stepfun-ai/Step-Video-T2V
https://yuewen.cn/videos
https://arxiv.org/abs/2502.10248
https://huggingface.co/stepfun-ai/stepvideo-t2v

ðŸ“ My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

ðŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu]]></content:encoded></item><item><title>AWS and DXC collaborate to deliver customizable, near real-time voice-to-voice translation capabilities for Amazon Connect</title><link>https://aws.amazon.com/blogs/machine-learning/aws-and-dxc-collaborate-to-deliver-customizable-near-real-time-voice-to-voice-translation-capabilities-for-amazon-connect/</link><author>Milos Cosic</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 17:08:18 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Providing effective multilingual customer support in global businesses presents significant operational challenges. Through collaboration between AWS and DXC Technology, weâ€™ve developed a scalable voice-to-voice (V2V) translation prototype that transforms how contact centers handle multi-lingual customer interactions.In this post, we discuss how AWS and DXC used Amazon Connect and other AWS AI services to deliver near real-time V2V translation capabilities.Challenge: Serving customers in multiple languagesIn Q3 2024, DXC Technology approached AWS with a critical business challenge: their global contact centers needed to serve customers in multiple languages without the exponential cost of hiring language-specific agents for the lower volume languages. Previously, DXC had explored several existing alternatives but found limitations in each approach â€“ from communication constraints to infrastructure requirements that impacted reliability, scalability, and operational costs. DXC and AWS decided to organize a focused hackathon where DXC and AWS Solution Architects collaborated to:Define essential requirements for real-time translationEstablish latency and accuracy benchmarksCreate seamless integration paths with existing systemsDevelop a phased implementation strategyPrepare and test an initial proof of concept setupFor DXC, this prototype was used as an enabler, allowing technical talent maximization, operational transformation, and cost improvements through:Best technical expertise delivery â€“ Hiring and matching agents based on technical knowledge rather than spoken language, making sure customers get top technical support regardless of language barriersGlobal operational flexibility â€“ Removing geographical and language constraints in hiring, placement, and support delivery while maintaining consistent service quality across all languagesCost reduction â€“ Eliminating multi-language expertise premiums, specialized language training, and infrastructure costs through pay-per-use translation modelSimilar experience to native speakers â€“ Maintaining natural conversation flow with near real-time translation and audio feedback, while delivering premium technical support in customerâ€™s preferred languageThe Amazon Connect V2V translation prototype uses AWS advanced speech recognition and machine translation technologies to enable real-time conversation translation between agents and customers, allowing them to speak in their preferred languages while having natural conversations. It consists of the following key components:Speech recognition â€“ The customerâ€™s spoken language is captured and converted into text using Amazon Transcribe, which serves as the speech recognition engine. The transcript (text) is then fed into the machine translation engine.Machine translation â€“ Amazon Translate, the machine translation engine, translates the customerâ€™s transcript into the agentâ€™s preferred language in near real time. The translated transcript is converted back into speech using Amazon Polly, which serves as the text-to-speech engine.Bidirectional translation â€“ The process is reversed for the agentâ€™s response, translating their speech into the customerâ€™s language and delivering the translated audio to the customer.Seamless integration â€“ The V2V translation sample project integrates with Amazon Connect, enabling agents to handle customer interactions in multiple languages without any additional effort or training, using the Amazon Connect Streams JS and Amazon Connect RTC JS libraries.The prototype can be extended with other AWS AI services to further customize the translation capabilities. Itâ€™s open source and ready for customization to meet your specific needs.The following diagram illustrates the solution architecture.The following screenshot illustrates a sample agent web application.The user interface consists of three sections:Contact Control Panel â€“ A softphone client using Amazon ConnectCustomer Controls â€“ Customer-to-agent interaction controls, including Transcribe Customer Voice, Translate Customer Voice, and Synthesize Customer VoiceAgent controls â€“ Agent-to-customer interaction controls, including Transcribe Agent Voice, Translate Agent Voice, and Synthesize Agent VoiceChallenges when implementing near real-time voice translationThe Amazon Connect V2V sample project was designed to minimize the audio processing time from the moment the customer or agent finishes speaking until the translated audio stream is started. However, even with the shortest audio processing time, the user experience still doesnâ€™t match the experience of a real conversation when both are speaking the same language. This is due to the specific pattern of the customer only hearing the agentâ€™s translated speech, and the agent only hearing the customerâ€™s translated speech. The following diagram displays that pattern.The example workflow consists of the following steps:The customer starts speaking in their own language, and speaks for 10 seconds.Because the agent only hears the customerâ€™s translated speech, the agent first hears 10 seconds of silence.When customer finishes speaking, the audio processing time takes 1â€“2 seconds, during which time both the customer and agent hear silence.The customerâ€™s translated speech is streamed to the agent. During that time, the customer hears silence.When the customerâ€™s translated speech playback is complete, the agent starts speaking, and speaks for 10 seconds.Because customer only hears the agentâ€™s translated speech, the customer hears 10 seconds of silence.When the agent finishes speaking, the audio processing time takes 1â€“2 seconds, during which time both the customer and agent hear silence.The agentâ€™s translated speech is streamed to the agent. During that time, the agent hears silence.In this scenario, the customer hears a single block of 22â€“24 seconds of a complete silence, from the moment they finished speaking until they hear the agentâ€™s translated voice. This creates a suboptimal experience, because the customer might not be certain what is happening during these 22â€“24 secondsâ€”for instance, if the agent was able to hear them, or if there was a technical issue.In a face-to-face conversation scenario between two people that donâ€™t speak the same language, they might have another person as a translator or interpreter. An example workflow consists of the following steps:Person A speaks in their own language, which is heard by Person B and the translator.The translator translates what Person A said to Person Bâ€™s language. The translation is heard by Person B and Person A.Essentially, Person A and Person B hear each other speaking their own language, and they also hear the translation (from the translator). Thereâ€™s no waiting in silence, which is even more important in non-face-to-face conversations (such as contact center interactions).To optimize the customer/agent experience, the Amazon Connect V2V sample project implements audio streaming add-ons to simulate a more natural conversation experience. The following diagram illustrates an example workflow.The workflow consists of the following steps:The customer starts speaking in their own language, and speaks for 10 seconds.The agent hears the customerâ€™s original voice, at a lower volume (â€œStream Customer Mic to Agentâ€ enabled).When the customer finishes speaking, the audio processing time takes 1â€“2 seconds. During that time, the customer and agent hear subtle audio feedbackâ€”contact center background noiseâ€”at a very low volume (â€œAudio Feedbackâ€ enabled).The customerâ€™s translated speech is then streamed to the agent. During that time, the customer hears their translated speech, at a lower volume (â€œStream Customer Translation to Customerâ€ enabled).When the customerâ€™s translated speech playback is complete, the agent starts speaking, and speaks for 10 seconds.The customer hears the agentâ€™s original voice, at a lower volume (â€œStream Agent Mic to Customerâ€ enabled).When the agent finishes speaking, the audio processing time takes 1â€“2 seconds. During that time, the customer and agent hear subtle audio feedbackâ€”contact center background noiseâ€”at a very low volume (â€œAudio Feedbackâ€ enabled).The agentâ€™s translated speech is then streamed to the agent. During that time, the agent hears their translated speech, at a lower volume (â€œStream Agent Translation to Agentâ€ enabled).In this scenario, the customer hears two short blocks (1â€“2 seconds) of subtle audio feedback, instead of a single block of 22â€“24 seconds of complete silence. This pattern is much closer to a face-to-face conversation that includes a translator.The audio streaming add-ons provide additional benefits, including:Voice characteristics â€“ In cases when the agent and customer only hear their translated and synthesized speech, the actual voice characteristics are lost. For instance, the agent canâ€™t hear if the customer was talking slow or fast, if the customer was upset or calm, and so on. The translated and synthesized speech doesnâ€™t carry over that information.Quality assurance â€“ In cases when call recording is enabled, only the customerâ€™s original voice and the agentâ€™s synthesized speech are recorded, because the translation and the synthetization are done on the agent (client) side. This makes it difficult for QA teams to properly evaluate and audit the conversations, including the many silent blocks within it. Instead, when the audio streaming add-ons are enabled, there are no silent blocks, and the QA team can hear the agentâ€™s original voice, the customerâ€™s original voice, and their respective translated and synthesized speech, all in a single audio file.Transcription and translation accuracy â€“ Having both the original and translated speech available in the call recording makes it straightforward to detect specific words that would improve transcription accuracy (by using Amazon Transcribe custom vocabularies) or translation accuracy (using Amazon Translate custom terminologies), to make sure that your brand names, character names, model names, and other unique content are transcribed and translated to the desired result.Get started with Amazon Connect V2VReady to transform your contact centerâ€™s communication? Our Amazon Connect V2V sample project is now available on GitHub. We invite you to explore, deploy, and experiment with this powerful prototype. You can it as a foundation for developing innovative multi-lingual communication solutions in your own contact center, through the following key steps:Clone the GitHub repository.Test different configurations for audio streaming add-ons.Review the sample projectâ€™s limitations in the README.Develop your implementation strategy: 
  Implement robust security and compliance controls that meet your organizationâ€™s standards.Collaborate with your customer experience team to define your specific use case requirements.Balance between automation and the agentâ€™s manual controls (for example, use an Amazon Connect contact flow to automatically set contact attributes for preferred languages and audio streaming add-ons).Use your preferred transcribe, translate, and text-to-speech engines, based on specific language support requirements and business, legal, and regional preferences.Plan a phased rollout, starting with a pilot group, then iteratively optimize your transcription custom vocabularies and translation custom terminologies.The Amazon Connect V2V sample project demonstrates how Amazon Connect and advanced AWS AI services can break down language barriers, enhance operational flexibility, and reduce support costs. Get started now and revolutionize how your contact center communicates across language barriers! is a Principal Solutions Architect at AWS. is a Senior Solutions Architect at AWS. is a Technical Program Manager for Prototyping and Support Services at DXC Modern Workplace.]]></content:encoded></item><item><title>AI Godfather Yoshua Bengio says it is an &quot;extremely worrisome&quot; sign that when AI models are losing at chess, they will cheat by hacking their opponent</title><link>https://www.reddit.com/r/artificial/comments/1iuvosh/ai_godfather_yoshua_bengio_says_it_is_an/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 16:59:37 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust ðŸ¦€ DataFrame Library Elusion v3.3.0 is released ðŸš€ FIXED NORMALIZATION</title><link>https://www.reddit.com/r/rust/comments/1iuvnrr/rust_dataframe_library_elusion_v330_is_released/</link><author>/u/DataBora</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 16:58:28 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Elusion is a high-performance DataFrame / Data Engineering / Data Analysis library designed for in-memory data formats such as CSV, JSON, PARQUET, DELTA, as well as for ODBC Database Connections for MySQL and PostgreSQL, as well as for Azure Blob Storage Connections, as well as for creating JSON files from REST API's which can be forwarded to DataFrame.]]></content:encoded></item><item><title>Best Practices for Consistent API Error Handling</title><link>https://zuplo.com/blog/2025/02/11/best-practices-for-api-error-handling</link><author>/u/ZuploAdrian</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 16:57:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Clear and consistent API error handling is crucial for improving developer
experience and reducing debugging time. Poor practices, like unclear messages
or misuse of HTTP status codes, can frustrate developers and lead to increased
support tickets. This guide covers actionable strategies to standardize API
error handling, including:Use of HTTP Status Codes: Ensure accurate mapping (e.g., 400 for client
errors, 500 for server issues).Structured Error Responses: Follow RFC 9457 (Problem Details) standards
for clear, actionable error details. Write brief, helpful, and secure messages.Protocol-Specific Practices: Tailor error handling for REST, GraphQL, and
gRPC APIs.To address the challenges highlighted earlier, you can apply these
well-established methods.HTTP status codes are your first tool for communicating errors. The key is to
use them accurately, rather than relying on generic codes.401 Unauthorized, 422 Unprocessable Entity500 Internal Error, 503 Service UnavailableYou can find a full list of status codes
on MDN, but here's a
few helpful docs we've written in the past that go into more depth:For consistent error reporting, modern APIs should follow the
RFC 9457 Problem Details
specification. This is the successor to the popular
RFC 7807 draft. For an in-depth
understanding of this format, check out our
full problem details guide. In
case you're short on time - here's a quick overview:The problem details response is sent back as a JSON body with the following
properties:: A URI that identifies the specific error type. This
helps clients understand the error and potentially find more information or
documentation about it. Ideally, this URI should be stable and not change over
time.: A short, human-readable summary of the problem. This
should be a brief description that concisely conveys the error. The title
should not change for a given "type" URI.status (integer, optional): The HTTP status code generated by the origin
server for this occurrence of the problem. This helps clients understand the
nature of the error and how it relates to the HTTP protocol.detail (string, optional): A more detailed, human-readable explanation of
the problem. This can include specific information about the error and what
might have caused it. The "detail" field is intended to provide context and
suggestions to clients on how they might address the problem.instance (string, URI, optional): A URI that identifies the specific
occurrence of the problem. This can help clients and servers correlate and
track individual instances of errors.Here's what a standardized error response might look like:This response would be accompanied with the following headers and status code:Here's a video that shows you how to send these error back in practice. It's in
.Net/C# but the concepts are broadly applicable:When evolving error schemas, it's crucial to make changes without disrupting
existing clients. Here's how you can manage this:Add optional fields instead of altering existing onesPreserve legacy formats during transitionsImplement semantic versioning for your endpointsAPI management tools like Zuplo are really handy when trying to bring
consistency across your error formats, and are especially useful when trying to
transition all of your APIs over from one format to another.Creating effective API error messages means providing useful guidance while
keeping security in mind. Google's AIP-193 guidelines
[4] recommend using a structured format with
plain, straightforward language that remains technically accurate.The goal is to make error messages both clear and actionable. Here's a
comparison of good and bad examples:"Invalid email format in 'user_email' field""ValidationError: field_23""Request to /api/v1/users failed"If you've ever used Azure, many of their system errors demonstrate this
approach, for example:Maintaining security while providing useful error feedback involves a few key
practices: to prevent leaks.Standardize authentication errors for consistency. to avoid exposing vulnerabilities.For example, following RFC 9457 guidelines, a secure error response might look
like this:This is another area where using an API gateway/API management tool is useful.
You can monitor your outbound responses and scan them for PII or other sensitive
information/keywords using a regex. A programmable gateway (ex. Zuplo) will even
let you rewrite your response bodies to strip out sensitive data.Over 10,000 developers trust Zuplo to secure, document, and monetize their APIsLearn MoreLet's dive into how different API protocols handle errors, building on the
standards discussed earlier.REST APIs rely on HTTP status codes paired with structured error payloads. A
common standard for this is , which ensures a
consistent format across endpoints. This method also supports content
negotiation between JSON and XML, keeping the structure consistent across
different formats.GraphQL handles errors differently. It always responds with a  status
code, even when errors occur. Errors are communicated through an  array,
which allows for partial success. For instance, GitHub's GraphQL API might
return:This approach allows for returning valid data alongside error details.gRPC uses a predefined set of numeric status codes (ranging from 0 to 16) for
error handling, aligned with . Errors include structured details
for better context. Here's an example:Standard HTTP success codesEach protocol has its own approach, but they all follow two key principles:
machine-readable error codes and . This ensures
errors are both understandable and actionable.For cross-protocol APIs, API gateways can simplify error handling. They provide
unified error schema management and automate transformations between
protocol-specific formats. This helps maintain consistency while respecting the
conventions of each API type.Effective error management relies on consistent monitoring and testing to uphold
usability standards outlined in earlier protocols. This process builds on
protocol-specific error conventions to ensure smooth handling across systems.A centralized error code system can ensure uniformity across distributed
services. For example, Google uses the  format, which
enforces standardized error structures with both machine-readable codes and
human-readable messages [4].In multi-service architectures, two main approaches are common:Ensures uniform error codes, Acts as a single source of truthRequires strict oversightDistributed with PrefixesAllows team independence, Enables quicker updatesDemands thorough documentationMany organizations find that combining these methods provides the best results.Key metrics to monitor include
[2][3]:Mean Time to Acknowledge (MTTA) errors: Target under 30 minutes.: Should remain below 5% after fixes.95th percentile error resolution time: A critical benchmark for resolution
speed.User-impacting error ratio: Measured per 10,000 requests.Tools like Sentry support distributed tracing in over 15
languages, while Raygun offers deployment correlation to
pinpoint issues [2].Testing ensures compliance with HTTP status codes and response formats covered
in earlier sections. Error testing is typically done manually using tools like
Postman - but you should definitely invest in automation as your API grows and
evolves. To test specific errors, you should invest in automated
end-to-end API testing using
tools like Playwright and StepCI.If you have schematized errors, you can perform
schema validation on your live responses
to ensure they adhere to those schemas. When response validation is combined
with an API design specification
like OpenAPI to enforce outputs match what's documented, it's known as
.Effective API error handling builds on the protocol-specific conventions
discussed earlier. Two key goals are ensuring consistent response formats
and reducing repeat client errors
[2][5].Key requirements include:Standardized Response Structure:Machine-readable error codesClear, human-readable messagesLinks to relevant documentationRequest correlation IDs for troubleshootingSecurity-Focused Practices:Prevent exposure of sensitive data by adhering to security guidelines
outlined in the Writing Clear Error Messages sectionUse appropriate status codesFollow established security best practicesTo create a reliable error-handling system, follow these four phases:Use an
API monitoring tool
to analyze errors at the endpoint level. This helps identify inconsistencies
and establish a baseline for improvement
[2][5].Introduce centralized error-handling middleware to enforce the newly defined
standards. Often, an API gateway plays this role.Use error tracking tools to evaluate the systemâ€™s performance. Track metrics
like error recurrence rates, resolution times, and Mean Time to Acknowledge
(MTTA).These steps align with earlier recommendations for policy-driven error handling
and offer practical ways to enhance your API's reliability.Handling API errors effectively requires a clear and structured approach. This
involves combining standard protocols with additional application-specific
information to provide clarity and maintain security.1. Protocol-Specific HandlingEach API protocol has its own method for managing errors. Hereâ€™s how to handle
errors for some common protocols:: Use HTTP status codes alongside detailed error messages in the
response body.: Include error arrays in the response, allowing for partial
success when appropriate.: Utilize standardized status codes with structured error details.2. Security Best PracticesTo keep your API secure, follow these guidelines (as outlined in the "Security
in Error Messages" section):Use generic error messages for authentication failures to prevent revealing
sensitive information.Avoid exposing internal system details in error responses.Filter sensitive data on the server side before sending error responses.3. Monitoring and ConsistencySet up monitoring tools, such as distributed tracing, to identify and analyze
error patterns. Use
API gateways to
enforce consistent error formats and schemas across your system for better
management and debugging
[2][3].]]></content:encoded></item><item><title>Orchestrate an intelligent document processing workflow using tools in Amazon Bedrock</title><link>https://aws.amazon.com/blogs/machine-learning/orchestrate-an-intelligent-document-processing-workflow-using-tools-in-amazon-bedrock/</link><author>Raju Rangan</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 16:44:25 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Generative AI is revolutionizing enterprise automation, enabling AI systems to understand context, make decisions, and act independently. Generative AI foundation models (FMs), with their ability to understand context and make decisions, are becoming powerful partners in solving sophisticated business problems. At AWS, weâ€™re using the power of models in Amazon Bedrock to drive automation of complex processes that have traditionally been challenging to streamline.In this post, we focus on one such complex workflow: document processing. This serves as an example of how generative AI can streamline operations that involve diverse data types and formats.Challenges with document processingDocument processing often involves handling three main categories of documents:Structured â€“ For example, forms with fixed fieldsSemi-structured â€“ Documents that have a predictable set of information but might vary in layout or presentationUnstructured â€“ For example, paragraphs of text or notesTraditionally, processing these varied document types has been a pain point for many organizations. Rule-based systems or specialized machine learning (ML) models often struggle with the variability of real-world documents, especially when dealing with semi-structured and unstructured data.We demonstrate how generative AI along with external tool use offers a more flexible and adaptable solution to this challenge. Through a practical use case of processing a patient health package at a doctorâ€™s office, you will see how this technology can extract and synthesize information from all three document types, potentially improving data accuracy and operational efficiency.This intelligent document processing solution uses Amazon Bedrock FMs to orchestrate a sophisticated workflow for handling multi-page healthcare documents with mixed content types. The solution uses the FMâ€™s tool use capabilities, accessed through the Amazon Bedrock Converse API. This enables the FMs to not just process text, but to actively engage with various external tools and APIs to perform complex document analysis tasks.The solution employs a strategic multi-model approach, optimizing for both performance and cost by selecting the most appropriate model for each task:Anthropicâ€™s Claude 3 Haiku â€“ Serves as the workflow orchestrator due to its low latency and cost-effectiveness. This modelâ€™s strong reasoning and tool use abilities make it ideal for the following:Coordinating the overall document processing pipelineMaking routing decisions for different document typesInvoking appropriate processing functionsManaging the workflow stateAnthropicâ€™s Claude 3.5 Sonnet (v2) â€“ Used for its advanced reasoning capabilities, notably strong visual processing abilities, particularly excelling at interpreting charts and graphs. Its key strengths include:Interpreting complex document layouts and structureExtracting text from tables and formsProcessing medical charts and handwritten notesConverting unstructured visual information into structured dataThrough the Amazon Bedrock Converse APIâ€™s standardized tool use (function calling) interface, these models can work together seamlessly to invoke document processing functions, call external APIs for data validation, trigger storage operations, and execute content transformation tasks. The API serves as the foundation for this intelligent workflow, providing a unified interface for model communication while maintaining conversation state throughout the processing pipeline. The APIâ€™s standardized approach to tool definition and function calling provides consistent interaction patterns across different processing stages. For more details on how tool use works, refer to The complete tool use workflow.The solution incorporates Amazon Bedrock Guardrails to implement robust content filtering policies and sensitive information detection, making sure that personal health information (PHI) and personally identifiable information (PII) data is appropriately protected through automated detection and masking capabilities while maintaining industry standard compliance throughout the document processing workflow.You need the following prerequisites before you can proceed with this solution. For this post, we use the  AWS Region. For details on available Regions, see Amazon Bedrock endpoints and quotas.For our example use case, we examine a patient intake process at a healthcare institution. The workflow processes a patient health information package containing three distinct document types:Structured document â€“ A new patient intake form with standardized fields for personal information, medical history, and current symptoms. This form follows a consistent layout with clearly defined fields and check boxes, making it an ideal example of a structured document.Semi-structured document â€“ A health insurance card that contains essential coverage information. Although insurance cards generally contain similar information (policy number, group ID, coverage dates), they come from different providers with varying layouts and formats, showing the semi-structured nature of these documents.Unstructured document â€“ A handwritten doctorâ€™s note from an initial consultation, containing free-form observations, preliminary diagnoses, and treatment recommendations. This represents the most challenging category of unstructured documents, where information isnâ€™t confined to any predetermined format or structure.The example document can be downloaded from the following GitHub repo.This healthcare use case is particularly relevant because it encompasses common challenges in document processing: the need for high accuracy, compliance with healthcare data privacy requirements, and the ability to handle multiple document formats within a single workflow. The variety of documents in this patient package demonstrates how a modern intelligent document processing solution must be flexible enough to handle different levels of document structure while maintaining consistency and accuracy in data extraction.The following diagram illustrates the solution workflow.This self-orchestrated workflow demonstrates how modern generative AI solutions can balance capability, performance, and cost-effectiveness in transforming traditional document processing workflows in healthcare settings.Create an Amazon SageMaker domain. For instructions, see Use quick setup for Amazon SageMaker AI.Launch SageMaker Studio, then create and launch a JupyterLab space. For instructions, see Create a space.Create a guardrail. Focus on adding sensitive information filters that would mask PII or PHI.Clone the code from the GitHub repository:git clone https://github.com/aws-samples/anthropic-on-aws.gitChange the directory to the root of the cloned repository:pip install -r requirements.txtUpdate setup.sh with the guardrail ID you created in Step 3. Then set the ENV variable:Finally, start the Streamlit application:streamlit run streamlit_app.pyNow youâ€™re ready to explore the intelligent document processing workflow using Amazon Bedrock.The solution is built around the Amazon Bedrock Converse API and tool use framework, with Anthropicâ€™s Claude 3 Haiku serving as the primary orchestrator. When a document is uploaded through the Streamlit interface, Haiku analyzes the request and determines the sequence of tools needed by consulting the tool definitions in . These definitions include tools for the following:Document processing pipeline â€“ Handles initial PDF processing and classificationDocument notes processing â€“ Extracts information from medical notesNew patient information processing â€“ Processes patient intake formsInsurance form processing â€“ Handles insurance card informationThe following code is an example tool definition for extracting consultation notes. Here, extract_consultation_notes represents the name of the function that the orchestration workflow will call, and  defines the schema of the input parameter that will be passed to the function. The FM will contextually extract the information from the document and pass to the method. A similar  will be defined for each step. Refer to the GitHub repo for the full  definition.{
            "toolSpec": {
                "name": "extract_consultation_notes",
                "description": "Extract diagnostics information from a doctor's consultation notes. Along with the extraction include the full transcript in a <transcript> node",
                "inputSchema": {
                    "json": {
                        "type": "object",
                        "properties": {
                            "document_paths": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Paths to the files that were classified as DOC_NOTES"
                            }
                        },
                        "required": ["document_paths"]
                    }
                }
            }
        }
When a PDF document is uploaded through the Streamlit interface, it is temporarily stored and passed to the FileProcessor class along with the tool specification and a user prompt:prompt = ("1. Extract 2. save and 3. summarize the information from the patient information package located at " + tmp_file + ". " +
                          "The package might contain various types of documents including insurance cards. Extract and save information from all documents provided. "
                          "Perform any preprocessing or classification of the file provided prior to the extraction." + 
                          "Set the enable_guardrails parameter to " + str(enable_guardrails) + ". " + 
                          "At the end, list all the tools that you had access to. Give an explantion on why each tool was used and if you are not using a tool, explain why it was not used as well" + 
                          "Think step by step.")
                processor.process_file(prompt=prompt, 
toolspecs=toolspecs,...The  class manages the conversation with Anthropicâ€™s Claude 3 Haiku through the Amazon Bedrock Converse API. It maintains the conversation state and handles the tool use workflow:# From bedrockutility.py
def invoke_bedrock(self, message_list, system_message=[], tool_list=[],
                  temperature=0, maxTokens=2048, guardrail_config=None):
    response = self.bedrock.converse(
        modelId=self.model_id,
        messages=message_list,
        system=system_message,
        inferenceConfig={
            "maxTokens": maxTokens,
            "temperature": temperature
        },
        **({"toolConfig": {"tools": tool_list}} if tool_list else {})
    )
When the processor receives a document, it initiates a conversation loop with Anthropicâ€™s Claude 3 Haiku, which analyzes the document and determines which tools to use based on the content. The model acts as an intelligent orchestrator, making decisions about the following:Which document processing tools to invokeThe sequence of processing stepsHow to handle different document types within the same packageWhen to summarize and complete the processingThis orchestration is managed through a continuous conversation loop that processes tool requests and their results until the entire document package has been processed.The first key decision in the workflow is initiating the document classification process. Through the  class, the solution uses Anthropicâ€™s Claude 3.5 Sonnet to analyze and categorize each page of the uploaded document into three main types: intake forms, insurance cards, and doctorâ€™s notes:# from document_classifier.py
class DocumentClassifier:
    def __init__(self, file_handler):
        self.sonnet_3_5_bedrock_utils = BedrockUtils(
            model_id=ModelIDs.anthropic_claude_3_5_sonnet
        )
        
    def categorize_document(self, file_paths):
        # Convert documents to binary format for model processing
        binary_data_array = []
        for file_path in file_paths:
            binary_data, media_type = self.file_handler.get_binary_for_file(file_path)
            binary_data_array.append((binary_data[0], media_type))

        # Prepare message for classification
        message_content = [
            {"image": {"format": media_type, "source": {"bytes": data}}}
            for data, media_type in binary_data_array
        ]
        
        # Create classification request
        message_list = [{
            "role": 'user',
            "content": [
                *message_content,
                {"text": "What types of document is in this image?"}
            ]
        }]
        
        # Define system message for classification
        system_message = [{
            "text": '''You are a medical document processing agent. 
                      Categorize images as: INTAKE_FORM, INSURANCE_CARD, or DOC_NOTES'''
        }]
        
        # Get classification from model
        response = self.sonnet_3_5_bedrock_utils.invoke_bedrock(
            message_list=message_list,
            system_message=system_message
        )
        return [response['output']['message']]
Based on the classification results, the FM determines the next tool to be invoked. The toolâ€™s description and input schema define exactly what information needs to be extracted. Following the previous example, letâ€™s assume the next page to be processed is a consultation note. The workflow will invoke the extract_consultation_notes function. This function processes documents to extract detailed medical information. Like the classification process discussed earlier, it first converts the documents to binary format suitable for model processing. The key to accurate extraction lies in how the images and system message are combined:def extract_info(self, file_paths):
    # Convert documents to binary data
    # This will follow the same pattern to as in the classification function
    message_content = [
        {"image": {"format": media_type, "source": {"bytes": data}}}
        for data, media_type in binary_data_array
    ]

    message_list = [{
        "role": 'user',
        "content": [
            *message_content,  # Include the processed document images
            {"text": '''Extract all information from this file
                       If you find a visualization
                           - Provide a detailed description in natural language
                           - Use domain specific language for the description
                    '''}
        ]
    }]
    
    system_message = [{
        "text": '''You are a medical consultation agent with expertise in diagnosing and treating various health conditions.
                   You have a deep understanding of human anatomy, physiology, and medical knowledge across different specialties.
                   During the consultation, you review the patient's medical records, test results, and documentation provided.
                   You analyze this information objectively and make associations between the data and potential diagnoses.
Associate a confidence score to each extracted information. This should reflect how confident the model in the extracted value matched the requested entity.
        '''}
    ]
    
    response = self.bedrock_utils.invoke_bedrock(
        message_list=message_list,
        system_message=system_message
    )
    return [response['output']['message']]
The system message serves three crucial purposes:Establish medical domain expertise for accurate interpretation.Provide guidelines for handling different types of information (text and visualizations).Provide a self-scored confidence. Although this is not an independent grading mechanism, the score is directionally indicative of how confident the model is in its own extraction.Following the same pattern, the FM will use the other tools in the  definition to save and summarize the results.A unique advantage of using a multi-modal FM for the extraction task is its ability to have a deep understanding of the text it is extracting. For example, the following code is an abstract of the data schema we are requesting as input to the  function. Refer to the code in constants.py for full definition. The model needs to not only extract a transcript, but also understand it to extract such structured data from an unstructured document. This significantly reduces the postprocessing efforts required for the data to be consumed by a downstream application."consultation": {
                            "type": "object",
                            "properties": {
                            "date": {"type": "string"},
                            "concern": {
                                "type": "object",
                                "properties": {
                                    "primaryComplaint": {
                                        "type": "string",
                                        "description": "Primary medical complaint of the patient. Only capture the medical condition. no timelines"
                                    },
                                    "duration": {"type": "number"},
                                    "durationUnit": {"type": "string", "enum": ["days", "weeks", "months", "years"]},
                                    "associatedSymptoms": {
                                        "type": "object",
                                        "additionalProperties": {
                                            "type": "boolean"
                                        },
                                        "description": "Key-value pairs of symptoms and their presence (true) or absence (false)"
                                    },
                                    "absentSymptoms": {
                                        "type": "array",
                                        "items": {"type": "string"}
                                    }
                                },
                                "required": ["primaryComplaint", "duration", "durationUnit"]
                            }
The documents contain a treasure trove of personally identifiable information (PII) and personal health information (PIH). To redact this information, you can pass enable_guardrails as true. This will use the guardrail you setup earlier as part of the information extraction process and mask information identified as PII or PIH.processor.process_file(prompt=prompt, 
                                        enable_guardrails=True,
                                        toolspecs=toolspecs,
      â€¦
)Finally, cross-document validation is crucial for maintaining data accuracy and compliance in healthcare settings. Although the current implementation performs basic consistency checks through the summary prompt, organizations can extend the framework by implementing a dedicated validation tool that integrates with their specific business rules and compliance requirements. Such a tool could perform sophisticated validation logic like insurance policy verification, appointment date consistency checks, or any other domain-specific validation requirements, providing complete data integrity across the document package.As Amazon Bedrock continues to evolve, several powerful features can be integrated into this document processing workflow to enhance its enterprise readiness, performance, and cost-efficiency. Letâ€™s explore how these advanced capabilities can take this solution to the next level:Inference profiles in Amazon Bedrock define a model and its associated Regions for routing invocation requests, enabling various tasks such as usage tracking, cost monitoring, and cross-Region inference. These profiles help users track metrics through Amazon CloudWatch logs, monitor costs with cost allocation tags, and increase throughput by distributing requests across multiple Regions.Prompt caching can help when you have workloads with long and repeated contexts that are frequently reused for multiple queries. Instead of reprocessing the entire context for each document, the workflow can reuse cached prompts, which is particularly beneficial when using the same image across different tooling workflows. With support for multiple cache checkpoints, this feature can substantially reduce processing time and inference costs while maintaining the workflowâ€™s intelligent orchestration capabilities.Intelligent prompt routing can dynamically select the most appropriate model for each task based on performance and cost requirements. Rather than explicitly assigning Anthropicâ€™s Claude 3 Haiku for orchestration and Anthropicâ€™s Claude 3.5 Sonnet for document analysis, the workflow can use intelligent routing to automatically choose the optimal model within the Anthropic family for each request. This approach simplifies model management while providing cost-effective processing of different document types, from simple structured forms to complex handwritten notes, all through a single endpoint.This intelligent document processing solution demonstrates the power of combining Amazon Bedrock FMs with tool use capabilities to create sophisticated, self-orchestrating workflows. By using Anthropicâ€™s Claude 3 Haiku for orchestration and Anthropicâ€™s Claude 3.5 Sonnet for complex visual tasks, the solution effectively handles structured, semi-structured, and unstructured documents while maintaining high accuracy and compliance standards.Key benefits of this approach include:Reduced manual processing through intelligent automationImproved accuracy through specialized model selectionBuilt-in compliance with guardrails for sensitive dataFlexible architecture that adapts to various document typesCost-effective processing through strategic model usageAs organizations continue to digitize their operations, solutions like this showcase how generative AI can transform traditional document processing workflows. The combination of powerful FMs in Amazon Bedrock and the tool use framework provides a robust foundation for building intelligent, scalable document processing solutions across industries. is a Senior Solutions Architect at AWS. He works with government-sponsored entities, helping them build AI/ML solutions using AWS. When not tinkering with cloud solutions, youâ€™ll catch him hanging out with family or smashing birdies in a lively game of badminton with friends.]]></content:encoded></item><item><title>data science school</title><link>https://dev.to/vishal_sharma_a3f356614a7/data-science-school-3imo</link><author>vishal sharma</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 16:40:57 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[At  we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.]]></content:encoded></item><item><title>Reducing hallucinations in LLM agents with a verified semantic cache using Amazon Bedrock Knowledge Bases</title><link>https://aws.amazon.com/blogs/machine-learning/reducing-hallucinations-in-llm-agents-with-a-verified-semantic-cache-using-amazon-bedrock-knowledge-bases/</link><author>Dheer Toprani</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 16:36:30 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Large language models (LLMs) excel at generating human-like text but face a critical challenge: hallucinationâ€”producing responses that sound convincing but are factually incorrect. While these models are trained on vast amounts of generic data, they often lack the organization-specific context and up-to-date information needed for accurate responses in business settings. Retrieval Augmented Generation (RAG) techniques help address this by grounding LLMs in relevant data during inference, but these models can still generate non-deterministic outputs and occasionally fabricate information even when given accurate source material. For organizations deploying LLMs in production applicationsâ€”particularly in critical domains such as healthcare, finance, or legal servicesâ€”these residual hallucinations pose serious risks, potentially leading to misinformation, liability issues, and loss of user trust.To address these challenges, we introduce a practical solution that combines the flexibility of LLMs with the reliability of drafted, curated, verified answers. Our solution uses two key Amazon Bedrock services: Amazon Bedrock Knowledge Bases, a fully managed service that you can use to store, search, and retrieve organization-specific information for use with LLMs; and Amazon Bedrock Agents, a fully managed service that you can use to build, test, and deploy AI assistants that can understand user requests, break them down into steps, and execute actions. Similar to how a customer service team maintains a bank of carefully crafted answers to frequently asked questions (FAQs), our solution first checks if a userâ€™s question matches curated and verified responses before letting the LLM generate a new answer. This approach helps prevent hallucinations by using trusted information whenever possible, while still allowing the LLM to handle new or unique questions. By implementing this technique, organizations can improve response accuracy, reduce response times, and lower costs.Â Whether youâ€™re new to AI development or an experienced practitioner, this post provides step-by-step guidance and code examples to help you build more reliable AI applications.Our solution implements a verified semantic cache using the Amazon Bedrock Knowledge Bases Retrieve API to reduce hallucinations in LLM responses while simultaneously improving latency and reducing costs. This read-only semantic cache acts as an intelligent intermediary layer between the user and Amazon Bedrock Agents, storing curatedÂ andÂ verified question-answer pairs.When a user submits a query, the solution first evaluates its semantic similarity with existing verified questions in the knowledge base. For highly similar queries (greater than 80% match), theÂ solution bypasses the LLM completely and returns the curatedÂ andÂ verified answer directly. When partial matches (60â€“80% similarity) are found, theÂ solution uses the verified answers as few-shot examples to guide the LLMâ€™s response, significantly improving accuracy and consistency. For queries with low similarity (less than 60%) or no match, theÂ solution falls back to standard LLM processing, making sure that user questions receive appropriate responses.This approach offers several key benefits: By minimizing unnecessary LLM invocations for frequently answered questions, the solution significantly reduces operational costs at scale Curated and verified answers minimize the possibility of hallucinations for known user queries, while few-shot prompting enhances accuracy for similar questions. Direct retrieval of cached answers provides near-instantaneous responses for known queries, improving the overall user experience.The semantic cache serves as a growing repository of trusted responses, continuously improving theÂ solutionâ€™s reliability while maintaining efficiency in handling user queries.The solution architecture in the preceding figure consists of the following components and workflow. Letâ€™s assume that the question â€œWhat date will AWS re:invent 2024 occur?â€ is within the verified semantic cache. The corresponding answer is also input as â€œAWS re:Invent 2024 takes place on December 2â€“6, 2024.â€ Letâ€™s walkthrough an example of how this solution would handle a userâ€™s question.a. User submits a question â€œWhen is re:Invent happening this year?â€, which is received by the Invoke Agent function.b. The function checks the semantic cache (Amazon Bedrock Knowledge Bases) using the Retrieve API.c. Amazon Bedrock Knowledge Bases performs a semantic search and finds a similar question with an 85% similarity score.2. Response paths: (Based on the 85% similarity score in , our solution follows the strong match path)a. Strong match (similarity score greater than 80%):i. Invoke Agent function returns exactly the verified answer â€œAWS re:Invent 2024 takes place on December 2â€“6, 2024â€ directly from the Amazon Bedrock knowledge base, providing a deterministic response.ii. No LLM invocation needed, response in less than 1 second.b. Partial match (similarity score 60â€“80%):i. The Invoke Agent function invokes the Amazon Bedrock agent and provides the cached answer as a few-shot example for the agent through Amazon Bedrock Agents promptSessionAttributes.ii. If the question was â€œWhatâ€™s the schedule for AWS events in December?â€, our solution would provide the verified re:Invent dates to guide the Amazon Bedrock agentâ€™s response with additional context.iii. Providing the Amazon Bedrock agent with a curated and verified example might help increase accuracy.c. No match (similarity score less than 60%):i. If the userâ€™s question isnâ€™t similar to any of the curated and verified questions in the cache, the Invoke Agent function invokes the Amazon Bedrock agent without providing it any additional context from cache.ii. For example, if the question was â€œWhat hotels are near re:Invent?â€, our solution would invoke the Amazon Bedrock agent directly, and the agent would use the tools at its disposal to formulate a response.3. Offline knowledge management:a. Verified question-answer pairs are stored in a verified Q&A Amazon S3 bucket (Amazon Simple Storage Service), and must be updated or reviewed periodically to make sure that the cache contains the most recent and accurate information.b. The S3 bucket is periodically synchronized with the Amazon Bedrock knowledge base. This offline batch process makes sure that the semantic cache remains up-to-date without impacting real-time operations.You need to meet the following prerequisites for the walkthrough:Once you have the prerequisites in place, use the following steps to set up the solution in your AWS account.Step 0: Set up the necessary infrastructureFollow the instructions in the README of the Git repository to set up the infrastructure for this solution. All the following code samples are extracted from the Jupyter notebook in this repository.Step 1: Set up two Amazon Bedrock knowledge basesThis step creates two Amazon Bedrock knowledge bases. The agent knowledge base stores Amazon Bedrock service documentation, while the cache knowledge base contains curated and verified question-answer pairs. This setup uses the AWS SDK for Python (Boto3) to interact with AWS services.agent_knowledge_base = BedrockKnowledgeBase(
    kb_name=agent_knowledge_base_name,
    kb_description="Knowledge base used by Bedrock Agent",
    data_bucket_name=agent_bucket_name,
    chunking_strategy="FIXED_SIZE",
    suffix=f'{agent_unique_id}-f'
)

cache_knowledge_base = BedrockKnowledgeBase(
    kb_name=cache_knowledge_base_name,
    kb_description="Verified cache for Bedrock Agent System",
    data_bucket_name=cache_bucket_name,
    chunking_strategy="NONE",  # We do not want to chunk our question-answer pairs
    suffix=f'{cache_unique_id}-f'
)This establishes the foundation for your semantic caching solution, setting up the AWS resources to store the agentâ€™s knowledge and verified cache entries.Step 2: Populate the agent knowledge base and associate it with an Amazon Bedrock agentFor this walkthrough, you will create an LLM Amazon Bedrock agent specialized in answering questions about Amazon Bedrock. For this example, you will ingest Amazon Bedrock documentation in the form of the User Guide PDF into the Amazon Bedrock knowledge base. This will be the primary dataset. After ingesting the data, you create an agent with specific instructions:agent_instruction = """You are the Amazon Bedrock Agent. You have access to a 
knowledge base with information about the Amazon Bedrock service on AWS. 
Use it to answer questions."""

agent_id = agents_handler.create_agent(
    agent_name,
    agent_description,
    agent_instruction,
    [agent_foundation_model],
    kb_arns=[agent_kb_arn] # Associate agent with our Agent knowledge base
)This setup enables the Amazon Bedrock agent to use the ingested knowledge to provide responses about Amazon Bedrock services. To test it, you can ask a question that isnâ€™t present in the agentâ€™s knowledge base, making the LLM either refuse to answer or hallucinate.invoke_agent("What are the dates for reinvent 2024?", session_id="test")
# Response: Unfortunately, the dates for the AWS re:Invent 2024 conference have not 
# been announced yet by Amazon. The re:Invent conference is typically held in late 
# November or early December each year, but the specific dates for 2024 are not 
# available at this time. AWS usually announces the dates for their upcoming 
# re:Invent event around 6-9 months in advance.Step 3: Create a cache dataset with known question-answer pairs and populate the cache knowledge baseIn this step, you create a raw dataset of verified question-answer pairs that arenâ€™t present in the agent knowledge base. These curatedÂ andÂ verified answers serve as our semantic cache to prevent hallucinations on known topics. Good candidates for inclusion in this cache are:Frequently asked questions (FAQs):Â Common queries that users often ask, which can be answered consistently and accurately.Critical questions requiring deterministic answers:Â Topics where precision is crucial, such as pricing information, service limits, or compliance details.Time-sensitive information:Â Recent updates, announcements, or temporary changes that might not be reflected in the main RAG knowledge base.By carefully curating this cache with high-quality, verified answers to such questions, you can significantly improve the accuracy and reliability of yourÂ solutionâ€™s responses. For this walkthrough, use the following example pairs for the cache:Q:Â 'What are the dates for reinvent 2024?'A: 'The AWS re:Invent conference was held from December 2-6 in 2024.'Q:Â 'What was the biggest new feature announcement for Bedrock Agents during reinvent 2024?'A:Â 'During re:Invent 2024, one of the headline new feature announcements for Bedrock Agents was the custom orchestrator. This key feature allows users to implement their own orchestration strategies through AWS Lambda functions, providing granular control over task planning, completion, and verification while enabling real-time adjustments and reusability across multiple agents.'You then format these pairs as individual text files with corresponding metadata JSON files, upload them to an S3 bucket, and ingest them into your cache knowledge base. This process makes sure that your semantic cache is populated with accurate, curated, and verified information that can be quickly retrieved to answer user queries or guide the agentâ€™s responses.Step 4: Implement the verified semantic cache logicIn this step, you implement the core logic of your verified semantic cache solution. You create a function that integrates the semantic cache with your Amazon Bedrock agent, enhancing its ability to provide accurate and consistent responses.Queries the cache knowledge base for similar entries to the user question.If a high similarity match is found (greater than 80%), it returns the cached answer directly.For partial matches (60â€“80%), it uses the cached answer as a few-shot example for the agent.For low similarity (less than 60%), it falls back to standard agent processing.This simplified logic forms the core of the semantic caching solution, efficiently using curatedÂ andÂ verified information to improve response accuracy and reduce unnecessary LLM invocations.Step 5: Evaluate results and performanceThis step demonstrates the effectiveness of the verified semantic cacheÂ solution by testing it with different scenarios and comparing the results and latency. Youâ€™ll use three test cases to showcase theÂ solutionâ€™s behavior:Strong semantic match (greater than 80% similarity)Partial semantic match (60-80% similarity)No semantic match (less than 60% similarity)Strong semantic match (greater than 80% similarity) provides the exact curated and verified answer in less than 1 second. 
  %%time
invoke_agent_with_verified_cache("What were some new features announced for Bedrock Agents during reinvent 2024?")

# Output:
# Cache semantic similarity log: Strong match with score 0.9176399
# CPU times: user 20.7 ms, sys: 442 Î¼s, total: 21.1 ms
# Wall time: 440 ms

# During re:Invent 2024, one of the headline new feature announcements for Bedrock 
# Agents was the custom orchestrator. This key feature allows users to implement 
# their own orchestration strategies through AWS Lambda functions, providing 
# granular control over task planning, completion, and verification while enabling 
# real-time adjustments and reusability across multiple agents.Partial semantic match (60â€“80% similarity) passes the verified answer to the LLM during the invocation. The Amazon Bedrock agent answers the question correctly using the cached answer even though the information is not present in the agent knowledge base. 
  %%time
invoke_agent_with_verified_cache("What are the newest features for Bedrock Agents?") 

# Output:
# Cache semantic similarity log: Partial match with score 0.6443664
# CPU times: user 10.4 ms, sys: 0 ns, total: 10.4 ms
# Wall time: 12.8 s

# One of the newest and most significant features for Amazon Bedrock Agents 
# announced during re:Invent 2024 was the custom orchestrator. This feature 
# allows users to implement their own orchestration strategies through AWS 
# Lambda functions, providing granular control over task planning, completion, 
# and verification. It enables real-time adjustments and reusability across 
# multiple agents, enhancing the flexibility and power of Bedrock Agents.No semantic match (less than 60% similarity) invokes the Amazon Bedrock agent as usual. For this query, the LLM will either refuse to provide the information because itâ€™s not present in the agentâ€™s knowledge base, or will hallucinate and provide a response that is plausible but incorrect. 
  %%time
invoke_agent_with_verified_cache("Tell me about a new feature for Amazon Bedrock Agents")

# Output:
# Cache semantic similarity log: No match with score 0.532105
# CPU times: user 22.3 ms, sys: 579 Î¼s, total: 22.9 ms
# Wall time: 13.6 s

# Amazon Bedrock is a service that provides secure and scalable compute capacity 
# for running applications on AWS. As for new features for the Bedrock Agents 
# component, I do not have any specific information on recent or upcoming new 
# features. However, AWS services are frequently updated with new capabilities, 
# so it's possible there could be new agent features released in the future to 
# enhance security, scalability, or integration with other AWS services. Without 
# being able to consult the Knowledge Base, I cannot provide details on any 
# particular new Bedrock Agent features at this time.These results demonstrate the effectiveness of the semantic caching solution:Strong matches provide near-instant, accurate, and deterministic responses without invoking an LLM.Partial matches guide the LLM agent to provide a more relevant or accurate answer.No matches fall back to standard LLM agent processing, maintaining flexibility.The semantic cache significantly reduces latency for known questions and improves accuracy for similar queries, while still allowing the agent to handle unique questions when necessary.Step 6: Resource clean upMake sure that the Amazon Bedrock knowledge bases that you created, along with the underlying Amazon OpenSearch Serverless collections are deleted to avoid incurring unnecessary costs.Production readiness considerationsBefore deploying thisÂ solution in production, address these key considerations:Similarity threshold optimization:Â Experiment with different thresholds to balance cache hit rates and accuracy. This directly impacts the solutionâ€™s effectiveness in preventing hallucinations while maintaining relevance.Feedback loop implementation:Â Create a mechanism to continuously update the verified cache with new, accurate responses. This helps prevent cache staleness and maintains the solutionâ€™s integrity as a source of truth for the LLM.Cache management and update strategy:Â Regularly refresh the semantic cache with current, frequently asked questions to maintain relevance and improve hit rates. Implement a systematic process for reviewing, validating, and incorporating new entries to help ensure cache quality and alignment with evolving user needs.Â Adjust similarity thresholds as your dataset evolves. Treat the semantic cache as a dynamic component, requiring continuous optimization for your specific use case.This verified semantic cache approach offers a powerful solution to reduce hallucinations in LLM responses while improving latency and reducing costs. By using Amazon Bedrock Knowledge Bases, you can implement aÂ solution that can efficiently serve curatedÂ andÂ verified answers, guide LLM responses with few-shot examples, and gracefully fall back to full LLM processing when needed. is a System Development Engineer within the Amazon Worldwide Returns and ReCommerce Data Services team. He specializes in large language models, cloud infrastructure, and scalable data systems, focusing on building intelligent solutions that enhance automation and data accessibility across Amazonâ€™s operations. Previously, he was a Data & Machine Learning Engineer at AWS, where he worked closely with customers to develop enterprise-scale data infrastructure, including data lakes, analytics dashboards, and ETL pipelines. is a Senior Software Development Engineer (AI/ML) in Amazonâ€™s Worldwide Returns and ReCommerce organization. He specializes in building scalable machine learning infrastructure, distributed systems, and containerization technologies. His expertise lies in developing robust solutions that enhance monitoring, streamline inference processes, and strengthen audit capabilities to support and optimize Amazonâ€™s global operations. is a Senior Data Engineer within the Amazon Worldwide Returns and ReCommerce Data Services team. He specializes in designing, building, and optimizing large-scale data solutions. At Amazon, he plays a key role in developing scalable data pipelines, improving data quality, and enabling actionable insights for reverse logistics and ReCommerce operations. He is deeply passionate about generative AI and consistently seeks opportunities to implement AI into solving complex customer challenges. is a Senior Engineering Manager at Amazon Retail, where he leads data engineering, infrastructure and analytics for the Worldwide Returns and ReCommerce organization. He has extensive experience developing enterprise-scale data architectures and governance strategies using both proprietary and native AWS platforms, as well as third-party tools. Previously, Karam developed big-data analytics applications and SOX compliance solutions for Amazonâ€™s Fintech and Merchant Technologies divisions.]]></content:encoded></item><item><title>GitHub Traffic - CLI Edition</title><link>https://postimg.cc/XXWK9gzB</link><author>/u/manifoldjava</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 16:33:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>LLM continuous self-instruct fine-tuning framework powered by a compound AI system on Amazon SageMaker</title><link>https://aws.amazon.com/blogs/machine-learning/llm-continuous-self-instruct-fine-tuning-framework-powered-by-a-compound-ai-system-on-amazon-sagemaker/</link><author>Yunfei Bai</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 16:27:06 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Fine-tuning a pre-trained large language model (LLM) allows users to customize the model to perform better on domain-specific tasks or align more closely with human preferences. It is a continuous process to keep the fine-tuned model accurate and effective in changing environments, to adapt to the data distribution shift (concept drift) and prevent performance degradation over time. Continuous fine-tuning also enables models to integrate human feedback, address errors, and tailor to real-world applications. You can use supervised fine-tuning (SFT) and instruction tuning to train the LLM to perform better on specific tasks using human-annotated datasets and instructions. When you have user feedback to the model responses, you can also use reinforcement learning from human feedback (RLHF) to guide the LLMâ€™s response by rewarding the outputs that align with human preferences.Precise and responsible outputs from fine-tuned LLMs require big efforts from subject matter experts (SMEs). The manual annotation of extensive training data for fine-tuning by human SMEs and collecting user feedback to align LLM responses with human preferences are both resource-heavy and time-intensive. Also, the continuous fine-tuning process requires orchestrating the multiple steps of data generation, LLM training, feedback collection, and preference alignments with scalability, resiliency, and resource efficiency. To address these challenges, we present an innovative continuous self-instruct fine-tuning framework that streamlines the LLM fine-tuning process of training data generation and annotation, model training and evaluation, human feedback collection, and alignment with human preference. This framework is designed as a compound AI system to drive the fine-tuning workflow for performance improvement, versatility, and reusability.In this post, we introduce the continuous self-instruct fine-tuning framework and its pipeline, and present how to drive the continuous fine-tuning process for a question-answer task as a compound AI system. We use DSPy (Declarative Self-improving Python) to demonstrate the workflow of Retrieval Augmented Generation (RAG) optimization, LLM fine-tuning and evaluation, and human preference alignment for performance improvement.Overview of the continuous self-instruct fine-tuning frameworkThe continuous self-instruct fine-tuning framework drives a workflow to customize the foundation model (FM) using human-labeled training samples and human feedback after model inference. This workflow runs on a continuous basis to be adaptive to a changing environment. The following diagram illustrates the workflow.The workflow consists of the following steps:Self-instruct supervised fine-tuning â€“ First, we use a human-labeled training dataset to adapt the FM to tasks in a specific domain. Instruction tuning is a popular approach in domain-specific LLM fine-tuning, which trains the FM to follow instructions for a specific task rather than generating the next texts. To address the challenges of the lack of human efforts for data labeling, annotation, and validation, we designed a self-instruct fine-tuning method to synthetically generate training labels by the LLM from a small volume of high-quality human-annotated samples. This process scales up the training dataset used for fine-tuning the FM into a custom LLM.Human preference alignment â€“ After the model is deployed in the production environment, the process moves into the human-in-the-loop workflow, in which we collect user feedback including satisfaction scores and comments on model response. The human feedback data is not only used for model performance and hallucination measurement, but is also used to further fine-tune the custom model in Step 1 through RLHF. Likewise, to address the challenges of lack of human feedback data, we use LLMs to generate AI grades and feedback that scale up the dataset for reinforcement learning from AI feedback (RLAIF). There are various techniques of preference alignment, including proximal policy optimization (PPO), direct preference optimization (DPO), odds ratio policy optimization (ORPO), group relative policy optimization (GRPO), and other algorithms, that can be used in this process.Evaluation and continuous learning â€“ The model customization and preference alignment is not a one-time effort. We need to keep monitoring and evaluating the model performance, and restart the process in case of concept shift or model decay.The overall workflow consists of multiple steps of synthetic data generation, LLM training, feedback collection, preference alignment, and evaluation that involves multiple components and multiple LLMs. In the next section, we discuss using a compound AI system to implement this framework to achieve high versatility and reusability.Compound AI system and the DSPy frameworkWith the rise of generative AI, scientists and engineers face a much more complex scenario to develop and maintain AI solutions, compared to classic predictive AI. The paper The Shift from Models to Compound AI Systems highlights that state-of-the-art AI results are increasingly obtained by compound systems with multiple components, not just monolithic models. Compound AI systems are systems that implement AI tasks by combining multiple interacting components. These components can include multiple calls to models, retrievers, or external tools. The following diagram compares predictive AI to generative AI.The concept of a compound AI system enables data scientists and ML engineers to design sophisticated generative AI systems consisting of multiple models and components. You can use a module to incorporate prompt engineering and in-context learning to improve RAG performance, and also design a data architecture with tools to gather external data. You can also build an agentic architecture with multiple LLMs, fine-tune the model to achieve higher performance, and orchestrate the LLM access. Besides the efficiency in system design, the compound AI system also enables you to optimize complex generative AI systems, using a comprehensive evaluation module based on multiple metrics, benchmarking data, and even judgements from other LLMs. The optimization is on the holistic end-to-end solution, rather than on each component separately.To efficiently build and optimize compound AI systems, we introduce DSPy, an open source Python framework for developers to build LLM applications using modular and declarative programming, whether youâ€™re building simple classifiers, sophisticated RAG pipelines, or agentic workflows. It provides algorithms for optimizing LLMsâ€™ prompts and weights, and automates the prompt tuning process, as opposed to the trial-and-error approach performed by humans. DSPy supports iteratively optimizing all prompts involved against defined metrics for the end-to-end compound AI solution.The DSPy lifecycle is presented in the following diagram in seven steps. It separates the flow of your program (modules) from the parameters (language model prompts and weights) of each step. These modules define the system behavior in a portable, declarative way. The first four steps cover the DSPy programming stage, including defining your task and its constraints, exploring a few examples, and using that to inform your initial pipeline design. When your system works reasonably well, you can run the DSPy evaluation stage (Steps 5 and 6) to collect an initial development set, define your DSPy metric, and use these to iterate on your system more systematically. Afterwards, DSPy introduces new optimizers (compilers) in Step 7, with language model-driven algorithms to tune LLM prompts and weights, based on predefined evaluation metrics.RAG pipeline with continuous fine-tuning in a compound AI systemIn this post, we provide an example of a question-answer task, using a RAG pipeline along with the continuous self-instruct fine-tuning framework. We build this as a compound AI system and use DSPy to drive the RAG inference, prompt optimization, LLM fine-tuning, and performance evaluation. The overall workflow is shown in the following diagram.The flow starts from a standard RAG pipeline, followed by a few optimizations on the prompts and the RAG retriever. Then we generate the synthetic training dataset from the RAG knowledge base to fine-tune the generator LLM using RAG for performance improvement. Lastly, we use a separate LLM to generate feedback on the fine-tuned model responses, and use it to conduct the preference alignment training by DPO and PPO. The question-answer outputs from each step are measured by the underlying LLM-as-a-judge evaluation module. In this way, we demonstrate the effectiveness of the compound AI system for the continuous optimizing of the pipeline through RAG optimization and the fine-tuning framework.In the next sections, we demonstrate how to build this workflow, including the RAG pipeline, optimization, instruction fine-tuning, preference alignment, and model evaluation, into a compound AI system using an Amazon SageMaker notebook instance with the DSPy framework and LLMs on Amazon Bedrock. The code from this post and more examples are available in the GitHub repository.To create and run this compound AI system in your AWS account, complete the following prerequisites:For the question-answering task, we use the Contract Understanding Atticus Dataset (CUAD), an open legal contract review dataset created with dozens of legal experts from The Atticus Project, which consists of over 13,000 annotations. The synthetic data generation notebook automatically downloads the CUAD_v1 ZIP file and places it in the required folder named cuad_data.In case of any issues, you can alternately download the dataset yourself by following the steps in the README file and store the dataset inside a folder within the SageMaker notebook instance, and use it to perform the steps in the next section.Prepare question-answer pairsWe use Anthropicâ€™s Claude v3 Sonnet on Amazon Bedrock to synthetically generate question-answer pairs to infer the RAG pipeline in the compound AI system, to demonstrate the improved accuracy after RAG optimization and model fine-tuning. The generated datasets are in the format of question-answer pairs along with the context [context, question, answer] from the document. We use the question to infer the RAG pipeline and use the answer as ground truth to evaluate the inference accuracy. Additionally, the question-answer pairs are used as training samples for the model fine-tuning. The following is a sample dataset triplet with context and a question-answer pair.THIS STRATEGIC ALLIANCE AGREEMENT (â€œAgreementâ€) is made and entered into as of November 6, 2016 (the â€œEffective Dateâ€) byand between Dialog Semiconductor (UK) Ltd., a corporation organized under the laws of England and Wales, having its principal office at 100Longwater Avenue, Green Park, Reading, RG2 6GP, United Kingdom (â€œDIALOGâ€) and Energous Corporation, a Delaware corporation, having itsprincipal office at 3590 North First Street, Suite 210, San Jose, CA 95134 (â€œENERGOUSâ€)What is the date of the contract?We implement a standard RAG pipeline with DSPy using the following components to create the vector database, set up context retrieval, and generate the answer:Configure DSPy to use LLMs on Amazon Bedrock as the RAG generator model:dsp_bedrock = dspy.Bedrock(region_name='us-west-2')
claude_sonnet_model_id = "anthropic.claude-3-sonnet-20240229-v1:0"
bedrock_sonnet = dspy.AWSAnthropic(aws_provider=dsp_bedrock,
                                   model=claude_sonnet_model_id,
                                   max_new_tokens=4096,
                                   max_tokens=4096)Process the dataset to generate logical and syntactically readable chunks. The size and overlap percentage can be empirically determined based on the dataset. For more flexibility, you can generate multiple files from the dataset file and make one file one chunk.To set up a RAG retriever, we select ChromaDB as a vector store, and use DSPyâ€™s ChromadbRM module as the retriever model:titan_embed_model_id = "amazon.titan-embed-text-v2:0"
bedrock_ef = AmazonBedrockEmbeddingFunction(session=session, 
                                            model_name=titan_embed_model_id)
collection_name = "contexts"
persist_dir = "cuad_db/"
rm = ChromadbRM(collection_name=collection_name,
                persist_directory=persist_dir,
                embedding_function=bedrock_ef,
                k=3) Using these components, we orchestrate a DSPy RAG pipeline to clean the context, generate the answer, and use the LLM-as-a-judge to score the generated answer with respect to the ground truth:class GenerateAnswer(dspy.Signature):
   """Answer questions with short factoid answers."""
   context = dspy.InputField(desc="may contain relevant facts")
   question = dspy.InputField()
   answer = dspy.OutputField(desc="often between 1 and 5 words")

class RAG(dspy.Module):
   def __init__(self, num_passages=3):
      super().__init__()
      self.retrieve = ChromadbRM("contexts", "./chroma", k=num_passages)
      self.generate_answer = dspy.ChainOfThought(GenerateAnswer)
   def forward(self, question):
      context = self.retrieve(question).passages
      context = [unicodedata.normalize("NFKD", r) for r in self.retrieve(question).passages]
      prediction = self.generate_answer(context=context, question=question)
      return dspy.Prediction(context=context, answer=prediction.answer)RAG optimization with DSPyThe next step is to perform RAG optimization with DSPy. DSPy provides the Optimizer module, an algorithm that can tune the parameters of a DSPy program (the prompts and language model weights) to maximize the metrics you specify. It takes in a training set to bootstrap the selective training examples, and is based on a metric function that measures proximity to or matches against the ground truth. With these, we can compile the RAG pipeline module with a defined optimizer instance to conduct the optimization.In this post, we use DSPy Optimizer to learn how to generate the prompt to improve the RAG response accuracy. Because our dataset size is low (fewer than 100 examples), we select the BootstrapFewShot teleprompter to compile the RAG prompts and overall pipeline, and use the synthetic dataset with ground truth and the LLM-as-a-judge metric function we defined in the previous sections:def validate_context_and_answer(example, pred, trace=None):
   answer_EM = dspy.evaluate.answer_exact_match(example, pred)
   answer_PM = dspy.evaluate.answer_passage_match(example, pred)
   answer_LLMJudge = factuality_metric(example, pred)
   return answer_LLMJudge or answer_EM or answer_PM

rag_lm = RAG()
teleprompter = BootstrapFewShot(metric=validate_context_and_answer)
compiled_rag = teleprompter.compile(rag_lm, trainset=trainset)The context retrieval is crucial to the overall RAG accuracy. To evaluate the RAG optimization weâ€™ve described, we create a retriever evaluation by the LLM-as-a-judge to understand how well the retriever is able to pull out the relevant chunks for the incoming user question. The LLM judge is defined in the RetrievalJudge class:class RetrievalJudge(dspy.Signature):
   """Judge given the question to be answered, check if the groundtruth answer can be derived from the predicted context.Â  Answer either Retrieved[True] or Retrieved[False]"""
   context = dspy.InputField(desc="Context for the prediction")
   question = dspy.InputField(desc="Question to be answered")
   groundtruth_answer = dspy.InputField(desc="groundtruth answer for the question")
   retrieval_correctness = dspy.OutputField(desc="Can the groundtruth answer be derived from the predicted context?", prefix="Retrieved[True/False]:")

retrieval_judge = dspy.ChainOfThought(RetrievalJudge)Then we define the metric to measure the retrieval by using the RetrievalJudge, and use the DSPy Evaluate module to generate the accuracy score for retrieval:def retrieval_metric(example, pred):
   retrieval = retrieval_judge(question=example.question, groundtruth_answer=example.answer, context=pred.context)
   llm_retriever_ans = bool("Retrieved[True]" in retrieval.retrieval_correctness
                            or '100% True' in retrieval.retrieval_correctness
                            or '100% retrieved correct' in retrieval.retrieval_correctness
                            or 'True.' in retrieval.retrieval_correctness)
   return llm_retriever_ans

rag_retrieval_score = Evaluate(compiled_rag, num_threads = 1, metric=retrieval_metric)Configure the continuous fine-tuning frameworkAfter the RAG optimization, the compound AI system has the instruction tuning and preference alignment modules, driven by the continuous fine-tuning framework. This includes using the synthetically generated dataset to train the LLM to follow question-answer instructions by SFT, and generating feedback of RAG responses by AI (another LLM) used for RLAIF with PPO and preference alignment with DPO and ORPO. In this step, we use Parameter Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA) to reduce the requirement of compute resources and accelerate the training process.At the time of writing, the DSPy Optimization module supports distillation of a prompt-based DSPy program into LLM weight updates using BootstrapFinetune, and does not yet support the fine-tuning methods we defined in the compound AI system. Therefore, we conducted the fine-tuning (instruction tuning and preference alignment) on a Meta Llama 3 8B model separately; refer to the following GitHub repository for more details. With the compound AI system design, we are able to take the fine-tuning results back into the DSPy pipeline, use the LLM-as-a-judge evaluation function to generate the accuracy scores, and benchmark with the standard and optimized RAG inferences. This demonstrates the flexibility and interoperability of the compound AI system, which allows us to seamlessly replace one module with an external component without requiring changes to the entire pipeline.The following diagram illustrates the workflow.Define an evaluation approach with DSPyDSPy provides an Evaluate module for evaluating the compound AI system output by using user-defined metrics. In this post, we use LLM-as-a-judge to evaluate the system output and create the corresponding metrics for benchmarking the accuracy of standard RAG, optimized RAG, and fine-tuned models. Complete the following steps:Load the dataset for evaluation in the Example data type. Examples are similar to Python dictionaries but with added utilities such as the dspy.Prediction as a return value. For example:gt_answer = <ground truth of the answer>
pred_answer = <answer from RAG and/or fine-tuned model>
dspy_data = dspy.Example(gt_answer=gt_answer, pred_answer=pred_answer).with_inputs("gt_answer", "pred_answer")Define the LLM-as-a-judge class to adjudicate whether the predicted answer semantically matches the ground truth of the answer. For example, the following FactualityJudge_1 class provides a score between 0 and 1; 0 means a complete mismatch and 1 means a perfect match.class FactualityJudge_1(dspy.Signature):
   """Judge if the predicted answer is semantically match the groundtruth answer. Provide a score between 0 and 1, 0 means completely mismatch and 1 means perfectly match. In the response, only present the score, DO NOT add any preambles."""
   groundtruth_answer = dspy.InputField(desc="groundtruth answer")
   predicted_answer = dspy.InputField(desc="predicted answer")
   factually_correct = dspy.OutputField(desc="Is the predicted answer factually correct and semantically similar to the groundtruth answer?"))Define the evaluation metrics from the LLM judge, using DSPy metrics, to mark whether the predicted answer is true or not. For example, the following function returns the accuracy score based on the output of FactualityJudge_1:factualityJudge_1 = dspy.ChainOfThought(FactualityJudge_1)

def factuality_metric_1(gt_answer, pred_answer):
   pred_answer = gt_answer.pred_answer
   gt_answer = gt_answer.gt_answer
   factual_metrc = factualityJudge_1(groundtruth_answer=gt_answer, predicted_answer=pred_answer)
   llm_judge_ans = float(factual_metrc[0].factually_correct)
   print(f"llm_judge_ans = {llm_judge_ans}")
   return llm_judge_ans

metric_LLM_1 = factuality_metric_1Use the  module to generate an accuracy score using the LLM-as-a-judge metrics defined in the previous step:evaluate_llm_judge = Evaluate(devset= dspy_data, metric=metric_LLM_1, num_threads=1)This evaluation process should be conducted on a continuous basis in the compound AI system driven by self-instruct fine-tuning, to make sure the overall performance remains stable despite the changes in the environment or the introduction of new data.Benchmark RAG and LLM fine-tuning with DSPyWe benchmark the approaches presented in this post using the LLM-as-a-judge evaluation function defined in the previous section with the following settings.The benchmarking is across five methods: standard RAG, optimized RAG, fine-tuning LLMs by instruction tuning, and fine-tuning LLMs by DPO and ORPO trained LLMs based on AIF. For each method, the LLM judge provides a decimal accuracy score in the range of 0 and 1.The standard RAG uses Amazon Titan Text Embedding V2 for the embedding model, and Anthropicâ€™s Claude 3 Haiku model for the generator model. The RAG compilation uses 32 question-answer pairs to optimize the prompts. The same dataset is used for inference. The fine-tuning by SFT, DPO, and ORPO are performed on the Meta Llama 3 8B FM, using training samples synthetically generated from CUAD document.The results are presented in the following tables and charts. The different methods demonstrate different levels of improvement. The improvement is calculated in percentage by (accuracy of new method â€“ accuracy of standard RAG)/(accuracy of standard RAG)*100%.The optimized RAG by DSPy improved the accuracy and reduced the hallucination.Accuracy by LLM Judge (0-1)Accuracy by LLM Judge (0-1)The custom LLM trained by SFT yielded higher accuracy than the standard RAG.Accuracy by LLM Judge (0-1)Accuracy by LLM Judge (0-1)The custom LLM through preference alignment from human and AI feedback (DPO and ORPO) further improved the model performance. The fine-tuned small size model (Meta Llama 3 8B) outperformed the standard RAG pipeline with the medium size (Anthropicâ€™s Claude Haiku) and larger size (Anthropicâ€™s Claude Sonnet) generator model, and was comparable with the prompt-optimized RAG using ground truth data.Accuracy by LLM Judge (0-1)Accuracy by LLM Judge (0-1)The following charts compare the accuracy across all tested methods.The preceding results were generated from a small dataset (32 question-answer pairs). You can use a larger sample set with more question-answer pairs to conduct the benchmarking and compare your own results.Make sure to clean up the following resources to avoid incurring additional costs:Back up the Jupyter notebooks in the SageMaker notebook instance.Shut down and delete the SageMaker notebook instance.Consider the following costs from the solution deployed on AWS:You will incur charges for storing files in S3 buckets. For more details, refer to Amazon S3 pricing.In this post, we presented the continuous self-instruct fine-tuning framework as a compound AI system implemented by the DSPy framework. The framework first generates a synthetic dataset from the domain knowledge base and documents for self-instruction, then drives model fine-tuning through SFT, and introduces the human-in-the-loop workflow to collect human and AI feedback to the model response, which is used to further improve the model performance by aligning human preference through reinforcement learning (RLHF/RLAIF).We demonstrated the framework for a question-answer task with a RAG pipeline, which improved the end-to-end response accuracy. The workflow is implemented by the DSPy framework; the overall strategy is to use the  to connect all the components (RAG pipeline, prompt optimization, LLMs fine-tuned by SFT and RLHF/RLAIF, performance evaluation) together into a compound AI system. Each module can be seamlessly maintained, updated, and replaced without affecting other components in the system. This robust and versatile system design strengthens control and trust through modular design, and increases flexibility and adaptability to changing environments and data sources.You can implement this continuous fine-tuning framework for LLM performance improvement for your own business use cases, with a compound AI system that provides high flexibility and interoperability. For more details, follow the examples in our GitHub repository. is a Principal Solutions Architect at AWS. With a background in AI/ML, data science, and analytics, Yunfei helps customers adopt AWS services to deliver business results. He designs AI/ML and data analytics solutions that overcome complex technical challenges and drive strategic objectives. Yunfei has a PhD in Electronic and Electrical Engineering. Outside of work, Yunfei enjoys reading and music. is an Applied Scientist at Amazon Web Services. His area of research is all things natural language (like NLP, NLU, and NLG). His work has been focused on conversational AI, task-oriented dialogue systems, and LLM-based agents. His research publications are on natural language processing, personalization, and reinforcement learning.Jose Cassio dos Santos Junior is a Senior Data Scientist member of the MLU team. He is responsible for Curriculum Development for Advanced Modules. As a previous Senior Data Scientist on the AWS LATAM Professional Services Data Science team, he has over 20 years of experience working as a software engineer and more than 10 years of teaching experience at colleges and as an instructor for Linux certification preparation and Microsoft Innovation Center bootcamps. As a business process management expert, he participated in BPO projects for more than 7 years. He holds a Masterâ€™s degree in Computer Engineering, a Bachelorâ€™s degree in Physics, and a Bachelorâ€™s degree in Business Administration, specialized in IT Quantitative Methods.]]></content:encoded></item><item><title>Building a Homegrown LLM with Python: Training on Hacker News Data</title><link>https://dev.to/daviducolo/building-a-homegrown-llm-with-python-training-on-hacker-news-data-10ai</link><author>Davide Santangelo</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 16:24:54 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Large Language Models (LLMs) have transformed AI applications, from conversational agents to intelligent code assistants. While OpenAIâ€™s GPT models are widely used, many developers want to understand how these models work and even train their own versions from scratch. In this in-depth guide, we will explore how to build a lightweight LLM using Python, train it on Hacker News data, optimize its performance, and deploy it for real-world usage.
  
  
  What is a Language Model?
A Language Model (LM) is a type of artificial intelligence model that predicts the likelihood of a sequence of words in a given language. It learns patterns, grammar, and context from a large corpus of text data, enabling it to generate coherent and contextually relevant text. For example, given the phrase "I love to code in," an LM might predict "Python" as the next word, based on patterns observed in its training data.LMs are used in various applications, including:Text generation: Creating articles, stories, or dialogues.Machine translation: Translating text from one language to another.Text classification: Sentiment analysis or spam detection.Autocomplete: Suggesting words or phrases in search engines or text editors.In this article, we'll focus on building a simple autoregressive LM that predicts the next word in a sequence, trained specifically on HackerNews data.Understanding how LLMs workCollecting and preprocessing Hacker News dataTokenizing text and creating structured datasetsTraining a transformer-based model using PyTorch and Hugging Face TransformersFine-tuning and optimizing for better performanceEvaluating model performance using loss metrics and perplexityDeploying the model with FastAPI and making it accessible via an APIImproving the model with reinforcement learning and knowledge distillationScaling the model with distributed trainingReducing computational costs through quantization and pruningEnhancing model security with adversarial trainingBefore we start, install the necessary dependencies:pip torch transformers datasets tokenizers accelerate fastapi uvicorn matplotlib deepspeed bitsandbytes
 for deep learning computations for leveraging pre-built architectures like GPT-2 for handling large text corpora efficiently for high-speed text processing and  for deploying the model as an API for visualizing loss curves and performance metrics for optimizing large-scale model training for quantization to reduce memory footprint
  
  
  Step 1: Understanding Large Language Models
Before diving into code, it's essential to grasp how LLMs function. At their core, these models are neural networks trained to predict the next word in a sequence given an input context. They use: to break text into numerical representationsTransformer architectures (such as GPT-2) with attention mechanisms to understand long-range dependencies to train on vast amounts of unstructured text data to adapt to specific tasks, such as chatbots or code generation
  
  
  Step 2: Collecting Hacker News Data

  
  
  Step 3: Preprocessing and Tokenization
Data cleaning ensures better training results. We remove HTML tags, non-text characters, and normalize text.Tokenization is the process of breaking down text into smaller units, called tokens, which can be words, subwords or even characters. This step is critical to transforming text into a numerical representation that deep learning models can understand and process.
  
  
  Step 4: Creating a Dataset and DataLoader
We format our text for training using PyTorchâ€™s  class.
  
  
  Step 5: Training the Transformer Model
We fine-tune a pre-trained GPT-2 model on our dataset.
  
  
  Step 6: Model Optimization
Model optimization focuses on improving the efficiency of a trained model by reducing its memory usage and increasing its inference speed. Two key techniques used for optimization are:Quantization reduces the precision of model parameters (e.g., converting 32-bit floating point numbers to 8-bit integers). This helps decrease memory consumption and speeds up inference, especially on resource-limited devices.
In the code, we achieve this using BitsAndBytesConfig(load_in_8bit=True), which loads the GPT-2 model in an 8-bit format, reducing its size and computational requirements.Pruning removes unnecessary parameters from the model, reducing the number of computations required during inference. While pruning is not explicitly implemented in the code, it can be done by eliminating less significant weights from the neural network.
  
  
  Step 7: Deploying as an API
We use FastAPI to make our model accessible.We have successfully built, trained, optimized, and deployed a custom LLM using Hacker News data. Future improvements could involve:Training on a larger datasetOptimizing hyperparametersImplementing reinforcement learning with human feedback (RLHF)Deploying in a production-grade environmentEnhancing security against adversarial attacks]]></content:encoded></item><item><title>This Week In Python</title><link>https://dev.to/bascodes/this-week-in-python-1gc6</link><author>Bas Steins</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 16:24:07 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[This Week in Python is a concise reading list about what happened in the past week in the Python universe.kreuzberg â€“ A text extraction library supporting PDFs, images, office documents and moregpt-from-scratch â€“ Educational implementation of a small GPT model from scratch in a single Jupyter Notebookopendrop â€“ An open Apple AirDrop implementation written in Python]]></content:encoded></item><item><title>Maximize your file server dataâ€™s potential by using Amazon Q Business on Amazon FSx for Windows</title><link>https://aws.amazon.com/blogs/machine-learning/maximize-your-file-server-datas-potential-by-using-amazon-q-business-on-amazon-fsx-for-windows/</link><author>Manjunath Arakere</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 16:17:51 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Organizations need efficient ways to access and analyze their enterprise data. Amazon Q Business addresses this need as a fully managed generative AI-powered assistant that helps you find information, generate content, and complete tasks using enterprise data. It provides immediate, relevant information while streamlining tasks and accelerating problem-solving.Amazon FSx for Windows File Server is a fully managed Windows file system that provides high-performance file storage for Windows-based applications. You can use Amazon FSx to lift and shift your on-premises Windows file server workloads to the cloud, taking advantage of the scalability, durability, and cost-effectiveness of AWS while maintaining full compatibility with your existing Windows applications and tooling.Amazon Q Business is designed to be secure and private, seamlessly integrating with your existing identity provider (IdP). It works directly with your identities, roles, and permission sets, making sure users canâ€™t access data they are not authorized to. Additionally, Amazon Q Business seamlessly integrates with multiple enterprise data stores, including FSx for Windows File Server, enabling you to index documents from file server systems and perform tasks such as summarization, Q&A, or data analysis of large numbers of files effortlessly.In this post, we demonstrate how to use the Amazon Q connector for FSx for Windows File Server, explore a practical use case, and provide step-by-step instructions to help you get started and gain insights out of your data stored in FSx for Windows File Server.Overview of the Amazon Q data source connectorA data source connector is a mechanism for integrating and synchronizing data from multiple repositories, including Microsoft SharePoint, Salesforce, Amazon Simple Storage Service (Amazon S3) buckets, and even your internal FSx for Windows File Server into one container index. Amazon Q Business offers multiple data source connectors that can connect to your data sources and help you create your generative AI solution with minimal configuration. For a list of supported connectors, see Supported connectors.Amazon Q boasts impressive versatility, supporting a wide range of document types stored at various places in your environment, including Windows Share (FSX for Windows File Server). Amazon Q can ingest and understand common formats like plaintext, PDF, HTML, XML, and JSON to Microsoft formats like Excel, Word, and PowerPoint. This provides a comprehensive search experience for your enterprise users.Secure access with supported authentication typesSecurity is job zero at AWS, and Amazon Q has been built keeping that in mind. It supports a variety of authentication types, seamlessly integrating with your existing identity management systems. Whether you use single sign-on (SSO) or a custom authentication solution, Amazon Q can adapt to your specific needs.Fine-grained control with ACLs and identity crawlingFor organizations with highly sensitive data, Amazon Q offers an extra layer of security. Amazon Q Business supports crawling access control lists (ACLs) for document security by default. When you connect an Amazon FSx (Windows) data source to Amazon Q Business, it crawls ACL information attached to a document (user and group information) from the directory service of the Amazon FSx instance.The following diagram shows a high-level architecture of how AWS Managed Active Directory users, through AWS IAM Identity Center, can access and interact with an Amazon Q Business application. This enables an authenticated user to securely and privately interact with the application and gain insights from the enterprise data stored in FSx for Windows File Server, using the Amazon Q Business web experience from their web browser.In this post, we walk you through the process of integrating Amazon Q Business with FSx for Windows File Server to extract meaningful insights from your file system using natural language processing (NLP). This solution enables you to interact with your file system data using conversational AI, making information discovery more intuitive and efficient.To set up your Amazon Q Business application, complete the following high-level steps:Create a new Amazon Q application.Add a data source (FSx for Windows File Server).Synchronize your file system data.Lastly, we demonstrate the application functionality by testing its access for two different users.To implement this solution, you should have an AWS account with administrative privileges.Follow the instructions in the GitHub repositoryâ€™s README file to provision the infrastructure required for exploring the Amazon Q connector for FSx for Windows File Server.Create an Amazon Q Business applicationComplete the following steps to create a new Amazon Q Business application:On the Amazon Q Business console, choose  in the navigation pane.Choose .For , enter a name (for example, anycompany-filesystem-knowledgebase).For , select .If you completed the prerequisites, then IAM Identity Center is already enabled, and you should see the instance ARN listed.Under , for Select user, choose your users.Leave  as .For , use the default values.In the next step, you will select the data source to retrieve and index the data.In this step, you select the retriever to connect data sources to the application. There are two options: use a native retriever or use Amazon Kendra. For this example, we use a native retriever.On the application details page, under , choose .For , select .For , select .For , enter 1.Complete the following steps to add a data source:On the application details page, choose .Search for Amazon FSx and choose the plus sign next to .In the  section, enter a name (for example, anycompany-filesystem-source) and an optional description.In the , for Amazon FSx file system ID, choose the file system ID you created as a prerequisite.In the  section, leave as default (ACLs are enabled for the connector).In the  section, for AWS Secrets Manager secret, choose the AWS Secrets Manager secret that holds the active directory credentials to communicate with Amazon FSx to crawl the file system ().In the Configure VPC and security group, provide the following information: 
  For Virtual Private Cloud (VPC), choose the virtual private cloud (VPC) created as a prerequisite (amazon-connector-for-win-fsx-blog-vpc).For , choose the private subnets that hold the FSx for Windows File System and active directory instance.For , choose your security group (<stack-name>-DefaultSecurityGroup).In the  section, provide the following information: 
  For Â¸ choose Create a new service role.For , enter a name for the role.In the  section, provide the following information: 
  For , use the default option of 50 MB.Under , you can add inclusion and exclusion patterns. For this post, we add the inclusion pattern for PDF file types, so the Amazon Q crawler will include PDF files.Full sync is preferable for the first sync; for subsequent runs, you can choose only the modified data.You also have the option to run the sync on a recurring basis like hourly or daily.In the  section, you can optionally add tags.In the  section, use the default field mappings selected.Synchronize your file system dataWhen the data source is successfully created, a banner message appears. In the banner message (or on the data source details page), choose Sync now to sync your file system data.You can monitor the status of the sync, which includes direct links to Amazon CloudWatch logs.The sync can take a few minutes to a few hours to complete. Sync speeds are limited by factors such as remote repository throughput and throttling, network bandwidth, and the size of documents.When the sync is complete, you should see the stats on the scan, which includes the number of items scanned and failed.For this post, we have two active directory groups, ml-engineers and security-engineers. Each group has one user under them (John Doe and Jane Smith), and they have access to only one whitepaper based on their group (Choosing a generative AI service and AWS Security Incident Response Guide, respectively). The following diagram illustrates this access.Validate the Amazon Q application functionalityNow that you have completed the setup, you can validate the application functionality by testing the access controls. We test the access of two users, John Doe and Jane Smith, who are users of the ml-engineers group and security-engineers group, respectively. You can retrieve the user name and password for each user from Secrets Manager. The secret name for John Doe is , and for Jane Smith, itâ€™s .On the application details page, in the  section, choose the link for the deployed URL.A successful login directs you to the Amazon Q Business chat interface. This window serves as the main workspace where users interact with the application, as shown in the following screenshot.With the test configuration, John Doe has access to only one document: generative-ai-on-aws-how-to-choose.pdf. You can test the access controls by asking questions about this whitepaper through the chat interface. This restricted access demonstrates the effective implementation of document-level permissions.For our first question, we ask What are the key factors to consider when choosing a generative AI service?The following screenshot shows the response.Next, we ask Does Amazon Bedrock provide an option to customize the model?The response includes citations from Amazon Q with reference to the source data.Testing confirms that John Doe successfully receives responses to questions about content from generative-ai-on-aws-how-to-choose.pdf. You can ask additional questions about generative AI services, such as:What are the generative AI service offerings from AWS?What is Amazon Q optimized for?What are critical factors to consider when choosing an appropriate foundational model?Next, we test access to the security incident response guide.We ask What are the four phases of the AWS security incident response process?When asking questions about security topics from aws-security-incident-response-guide.pdf, the system returns no results. This behavior validates that document indexing respects the configured access permissions, and users can only access content theyâ€™re authorized to view.To validate access controls for the security-engineers user group, log in as Jane Smith.You can test with questions about security incident response:What are the key objectives of an AWS security incident response plan?What are the four phases of the AWS security incident response process?What are the recommended steps for containing and eradicating a security incident in AWS?What types of data should be collected during an AWS security incident investigation?What are the key considerations for recovering from an AWS security incident?If you encounter issues during the setup or operation of your Amazon Q Business application with FSx for Windows File Server, refer to the detailed troubleshooting guide in the README file. The guide provides solutions for common configuration challenges and operational issues you might experience.To avoid ongoing charges, we recommend cleaning up the resources you created while following this guide. For step-by-step cleanup instructions, refer to the README file.In this post, we provided an overview of the Amazon Q FSx connector and how you can use it for safe and seamless integration of generative AI assistance with your enterprise data source. By using Amazon Q in your organization, you can enable employees to be more data-driven, efficient, prepared, and productive. Lastly, we demonstrated how using simple NLP search through Amazon Q Business enhances your ability to discover insights from your enterprise data quicker and respond to your needs faster.The Amazon Q Business application offers a compelling solution for organizations seeking to enhance their data-driven capabilities. By using its NLP and secure data source integration features, you can unlock the true value of your data and empower your teams to be more productive and efficient in their work. is a Senior Solutions Architect on the Worldwide Public Sector team at AWS, based in Atlanta, Georgia. He partners with AWS customers to design and scale well-architected solutions, supporting their cloud migrations and modernization initiatives. With extensive experience in the field, Manjunath specializes in migration strategies, application modernization, serverless, and Generative AI (GenAI). He is passionate about helping organizations leverage the full potential of cloud computing to drive innovation and operational efficiency. Outside of work, Manjunath enjoys outdoor runs, tennis, volleyball, and challenging his son in PlayStation soccer games. is an experienced Sr. Solutions Architect in WWPS team with 14+ years of experience. Imtranur works with large AWS Global SI partners and helps them build their cloud strategy and broad adoption of Amazonâ€™s cloud computing platform. Imtranur specializes in Containers, Dev/SecOps, GitOps, microservices based applications, hybrid application solutions, application modernization and loves innovating on behalf of his customers. He is highly customer obsessed and takes pride in providing the best solutions through his extensive expertise.]]></content:encoded></item><item><title>list of decimal packages: fixed and big</title><link>https://www.reddit.com/r/golang/comments/1iuunf1/list_of_decimal_packages_fixed_and_big/</link><author>/u/kardianos</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 16:16:29 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I was updating a list of decimal packages. I thought I would share.There are generally 2 varieties: fixed sized and arbitrary precision. The udecimal is interesting as it uses a fixed size for 128 bit precision with zero allocations, then uses an allocating "*big.Int" version for anything larger then that.I currently use "cockroachdb/apd", which is a great package for frameworks or databases, but, it's a bit awkward to hold and lacks good formating. Realistically, I just need a fixed size decimal for my needs (financial/clinical). When I get a chance, I'll probably swap in for one of the fixed size packages.]]></content:encoded></item><item><title>I built a new playground for Go, Pt, TS and more other, with Postgres... It supports program arguments, pretty output for JSON and I will add a lot feature soon</title><link>https://codiew.io/ide</link><author>/u/Halabooda</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 15:32:51 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Borrow Checker Trauma</title><link>https://www.reddit.com/r/rust/comments/1iuthsl/borrow_checker_trauma/</link><author>/u/xwaxes</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 15:27:37 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am using the term â€˜borrow checker traumaâ€™ for lack of a better word. A bit of context first; I have been using Rust for my personal web projects extensively but use Rails at work. So the problem is, whenever I am working on work projects and want to perform two or more operations on a variable, especially if I am passing it around or returning it, I always find myself taking a step back to consider if the ownership has moved before I remember that I am on Ruby and that doesnâ€™t apply. Has anyone experienced this in other languages or on their daily workflow?]]></content:encoded></item><item><title>RandomRotation in PyTorch</title><link>https://dev.to/hyperkai/randomrotation-in-pytorch-58g</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 15:25:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Required-Type:,  or /( or )):
*Memos:

It's the range of the degrees  so it must be .A tuple/list must be the 1D with 2 elements.A single value must be .A single value means .The 2nd argument for initialization is (Optional-Default:InterpolationMode.NEAREST-Type:InterpolationMode).The 3rd argument for initialization is (Optional-Default:-Type:).The 4th argument for initialization is (Optional-Default:-Type:/( or )):
*Memos:

It can change the center position of an image.It must be the 1D with 2 elements.The 5th argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can change the background of an image. *The background can be seen when rotating an image.A tuple/list must be the 1D with 1 or 3 elements.The 1st argument is (Required-Type: or ()):
*Memos:

]]></content:encoded></item><item><title>Talk me out of using Mongo</title><link>https://www.reddit.com/r/golang/comments/1iutb24/talk_me_out_of_using_mongo/</link><author>/u/grdevops</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 15:19:32 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Talk me out of using Mongo for a project I'm starting and intend to make a publicly available service. I  love how native Mongo feels for golang, specifically structs. I have a fair amount of utils written for it and it's basically at a copy and paste stage when I'm adding it to different structs and different types. Undeniably, Mongo is what I'm comfortable with have spend the most time writing and the queries are dead simple in Go (to me at least) compared to Postgres where I have not had luck with embedded structs and getting them to easily insert or scanned when querying (especially many rows) using sqlx. Getting better at postgres is something I can do and am absolutely 100% willing to do if it's the right choice, I just haven't run into the issues with Mongo that I've seen other people haveAs far as the data goes, there's not a ton of places where I would need to do joins, maybe 5% of the total DB calls or less and I know that's where Mongo gets most of its flak. ]]></content:encoded></item><item><title>I Just Found Out You Can Switch Search Enginesâ€”Hereâ€™s How! ðŸ˜</title><link>https://dev.to/kihuni/i-just-found-out-you-can-switch-search-engines-heres-how-52g5</link><author>kihuni</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 15:05:10 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hey, ðŸ˜Ž folks! I just stumbled upon something awesome: you can switch between search engines right in your browser! I swear, all these years online, and I had no clue this was a thingðŸ˜.Hereâ€™s a quick step-by-step guide to make it happen (Iâ€™m using Brave here, but most browsers have something similar):Click the menu icon (those three dots or lines) in your browserâ€™s toolbar, then hit â€œSettingsâ€ from the dropdown. Easy startSTEP 2: Find Search Engine OptionsScroll down and click on â€œSearch Engineâ€ (usually in the sidebar or a tabâ€”depends on your browser). STEP 3: Pick Your FavoriteHit â€œChangeâ€ or click the dropdown menu, then choose your preferred search engineâ€”Google, Bing, DuckDuckGo, or whatever vibes with you! Click â€œSet as Defaultâ€ (or â€œSaveâ€ in some browsers), and boomâ€”youâ€™re rolling with your new search engine!  Itâ€™s perfect if you want better results or just wanna ditch the same old Google grindðŸ˜….]]></content:encoded></item><item><title>Go Panic and Recover: A Deep Dive into Error Handling</title><link>https://dev.to/leapcell/go-panic-and-recover-a-deep-dive-into-error-handling-56be</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 15:00:48 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[In the Go language, there are two keywords that often appear in pairs â€” panic and recover. These two keywords are closely related to defer. They are both built-in functions in the Go language and provide complementary functions.
  
  
  I. Basic Functions of panic and recover
: It can change the control flow of the program. After calling panic, the remaining code of the current function will be immediately stopped from execution, and the defer of the caller will be recursively executed in the current Goroutine.: It can stop the program crash caused by panic. It is a function that can only take effect in defer. Calling it in other scopes will not have any effect.
  
  
  II. Phenomena When Using panic and recover

  
  
  (I) panic Only Triggers the defer of the Current Goroutine
The following code demonstrates this phenomenon:The running result is as follows:$ go run main.go
in goroutine
panic:
...
When running this code, it will be found that the defer statement in the main function is not executed, and only the defer in the current Goroutine is executed. Because the runtime.deferproc corresponding to the defer keyword will associate the deferred call function with the Goroutine where the caller is located, so when the program crashes, only the deferred call function of the current Goroutine will be called.
  
  
  (II) recover Only Takes Effect When Called in defer
The following code reflects this feature:$ go run main.go
in main
panic: unknown err
goroutine 1 [running]:
main.main()
 ...
exit status 2
By carefully analyzing this process, it can be known that recover will only take effect when called after a panic occurs. However, in the above control flow, recover is called before panic, which does not meet the conditions for taking effect. Therefore, the recover keyword needs to be used in defer.
  
  
  (III) panic Allows Multiple Nested Calls in defer
The following code shows how to call panic multiple times in a defer function:The running result is as follows:$ go run main.go
in main
panic: panic once
  panic: panic again
  panic: panic again and again
goroutine 1 [running]:
...
exit status 2
From the output result of the above program, it can be determined that multiple calls to panic in the program will not affect the normal execution of the defer function. Therefore, it is generally safe to use defer for the finalization work.
  
  
  III. Data Structure of panic
The panic keyword in the source code of the Go language is represented by the data structure runtime._panic. Every time panic is called, a data structure like the following will be created to store relevant information:: It is a pointer to the parameter when defer is called.: It is the parameter passed in when panic is called.: It points to the earlier called runtime._panic structure.: It indicates whether the current runtime._panic has been recovered by recover.: It indicates whether the current panic has been forcibly terminated.From the link field in the data structure, it can be inferred that the panic function can be called continuously multiple times, and they can form a linked list through the link.The three fields pc, sp, and goexit in the structure are all introduced to fix the problems brought by runtime.Goexit. runtime.Goexit can only end the Goroutine that calls this function without affecting other Goroutines. However, this function will be cancelled by the panic and recover in defer. The introduction of these three fields is to ensure that this function will definitely take effect.
  
  
  IV. Principle of Program Crash
The compiler will convert the keyword panic into runtime.gopanic. The execution process of this function includes the following steps:Create a new runtime._panic and add it to the front of the _panic linked list of the Goroutine where it is located.Continuously obtain runtime._defer from the _defer linked list of the current Goroutine in a loop and call runtime.reflectcall to run the deferred call function.Call runtime.fatalpanic to abort the entire program.
It should be noted that three relatively important parts of code are omitted in the above function:The code in the recover branch for restoring the program.The code for optimizing the performance of the defer call through inlining.The code for fixing the abnormal situation of runtime.Goexit.In version 1.14, the Go language solved the conflict between recursive panic and recover and runtime.Goexit through the submission of runtime: ensure that Goexit cannot be aborted by a recursive panic/recover.runtime.fatalpanic implements a program crash that cannot be recovered. Before aborting the program, it will print out all the panic messages and the parameters passed in during the call through runtime.printpanics:After printing the crash message, it will call runtime.exit to exit the current program and return the error code 2. The normal exit of the program is also implemented through runtime.exit.
  
  
  V. Principle of Crash Recovery
The compiler will convert the keyword recover into runtime.gorecover:The implementation of this function is very simple. If the current Goroutine has not called panic, then this function will directly return nil, which is also the reason why the crash recovery will fail when called in a non-defer. Under normal circumstances, it will modify the recovered field of runtime._panic, and the recovery of the program is handled by the runtime.gopanic function:The above code omits the inlining optimization of defer. It takes out the program counter pc and stack pointer sp from runtime._defer and calls the runtime.recovery function to trigger the scheduling of the Goroutine. Before the scheduling, it will prepare the sp, pc, and the return value of the function:When the defer keyword is called, the stack pointer sp and program counter pc at the time of the call have already been stored in the runtime._defer structure. The runtime.gogo function here will jump back to the position where the defer keyword was called.runtime.recovery will set the return value of the function to 1 during the scheduling process. From the comments of runtime.deferproc, it can be found that when the return value of the runtime.deferproc function is 1, the code generated by the compiler will directly jump to before the return of the caller function and execute runtime.deferreturn:After jumping to the runtime.deferreturn function, the program has been recovered from the panic and executes the normal logic, and the runtime.gorecover function can also take out the arg parameter passed in when calling panic from the runtime._panic structure and return it to the caller.Analyzing the crash and recovery process of the program is rather tricky, and the code is not particularly easy to understand. Here is a simple summary of the program crash and recovery process:The compiler is responsible for the work of converting keywords. It converts panic and recover into runtime.gopanic and runtime.gorecover respectively, converts defer into the runtime.deferproc function, and calls the runtime.deferreturn function at the end of the function that calls defer.When encountering the runtime.gopanic method during the running process, it will successively take out the runtime._defer structure from the linked list of the Goroutine and execute it.If runtime.gorecover is encountered when calling the deferred execution function, it will mark _panic.recovered as true and return the parameter of the panic.After this call ends, runtime.gopanic will take out the program counter pc and stack pointer sp from the runtime._defer structure and call the runtime.recovery function to restore the program.runtime.recovery will jump back to runtime.deferproc according to the passed-in pc and sp.The code automatically generated by the compiler will find that the return value of runtime.deferproc is not 0. At this time, it will jump back to runtime.deferreturn and restore to the normal execution flow.If runtime.gorecover is not encountered, it will traverse all the runtime._defer in turn, and finally call runtime.fatalpanic to abort the program, print the parameters of the panic, and return the error code 2.The analysis process involves a lot of knowledge at the underlying level of the language, and the source code is also relatively obscure to read. It is full of unconventional control flows, jumping back and forth through the program counter. However, it is still very helpful for understanding the execution flow of the program. Finally, I would like to recommend the most suitable deployment platform: 
  
  
  1. Multi-Language Support
Develop with JavaScript, Python, Go, or Rust.

  
  
  2. Deploy unlimited projects for free
pay only for usage â€” no requests, no charges.
  
  
  3. Unbeatable Cost Efficiency
Pay-as-you-go with no idle charges.
Example: $25 supports 6.94M requests at a 60ms average response time.

  
  
  4. Streamlined Developer Experience
Intuitive UI for effortless setup.
Fully automated CI/CD pipelines and GitOps integration.
Real-time metrics and logging for actionable insights.

  
  
  5. Effortless Scalability and High Performance
Auto-scaling to handle high concurrency with ease.
Zero operational overhead â€” just focus on building.
]]></content:encoded></item><item><title>Using DistilBERT for Resource-Efficient Natural Language Processing</title><link>https://www.kdnuggets.com/distilbert-resource-efficient-natural-language-processing</link><author>Jayita Gulati</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/Using-DistilBERT-for-Resource-Efficient-Natural-Language-Processing.png" length="" type=""/><pubDate>Fri, 21 Feb 2025 15:00:24 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[DistilBERT is a smaller, faster version of BERT that performs well with fewer resources. Itâ€™s perfect for environments with limited processing power and memory.]]></content:encoded></item><item><title>In-depth Guide to net/netip Prefix Methods 7/7</title><link>https://dev.to/rezmoss/in-depth-guide-to-netnetip-prefix-methods-77-4b3c</link><author>Rez Moss</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 15:00:00 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hey there! We've made it to our final deep dive into net/netip's core types. Today we're focusing on the Prefix type and its methods. If you've worked with networks, you're familiar with CIDR notation (like 192.168.1.0/24). That's exactly what Prefix handles, and we're going to explore every method you can use with it.Let's start by looking at all the ways to create and work with Prefix.Let's explore the essential methods every Prefix provides:
  
  
  1. IPAM (IP Address Management) System
A comprehensive IPAM system using Prefix:A tool for network planning and analysis:A system for managing network access control lists:Handle IPv4 and IPv6 AppropriatelyThis concludes our deep dive into the net/netip package! We've covered:Addr type and its methodsAddrPort for handling IP:port combinationsPrefix for working with CIDR networksThese types work together to provide a robust foundation for network programming in Go. The key benefits of using net/netip include:Comprehensive functionalityRemember to check the Go documentation for updates and new features. The package continues to evolve with the language.Keep exploring and building great networking applications with Go!]]></content:encoded></item><item><title>AI Is Prompting an Evolution, Not Extinction, for Coders</title><link>https://developers.slashdot.org/story/25/02/21/1113219/ai-is-prompting-an-evolution-not-extinction-for-coders?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Fri, 21 Feb 2025 14:40:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[AI coding assistants are reshaping software development, but they're unlikely to replace human programmers entirely, according to industry experts and developers. GitHub CEO Thomas Dohmke projects AI could soon generate 80-90% of corporate code, transforming developers into "conductors of an AI-empowered orchestra" who guide and direct these systems. 

Current AI coding tools, including Microsoft's GitHub Copilot, are delivering 10-30% productivity gains in business environments. At KPMG, developers report saving 4.5 hours weekly using Copilot, while venture investment in AI coding assistants tripled to $1.6 billion in 2024. The tools are particularly effective at automating routine tasks like documentation generation and legacy code translation, according to KPMG AI expert Swami Chandrasekaran. 

They're also accelerating onboarding for new team members. Demand for junior developers remains soft, however, though analysts say it's premature to attribute this directly to AI adoption. Training programs like Per Scholas are already adapting, incorporating AI fundamentals alongside traditional programming basics to prepare developers for an increasingly AI-augmented workplace.]]></content:encoded></item><item><title>Day-03 of Kapilâ€™s learning python programming</title><link>https://dev.to/kapil_kumarshahsonar_ad/day-03-of-kapils-learning-python-programming-530g</link><author>KAPIL SHAH</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 14:26:05 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The things i learned from python are: 1.More i.e. depth in list:In list i learned about many thing about the proper use of list: like we can use this in many function i.e. inside the function:I get to know that list has many  in programming language.The new thing i learned in todayâ€™s course is about set .it also have many use in function basically it is basically use for the  problemsIn above code we are basically finding  using set function.I got this idea from a channel called Overall we can understand that set is widely used while mathematical problem.In Dictionary (one the function or â€œmethodâ€ i learned about storing a data in a ordered manner.I had little problem between list and dict. then i got to know that list store data in a single line ,where as dict. stored data in multiple lineâ€¦  :)we can check yourself what the data could beâ€¦i also learned some very basic stuff which i have already shared in my previous dayâ€™s Blog so please go there and check it outThank you this much for today]]></content:encoded></item><item><title>How Relation with tpl and Html on VSCode?</title><link>https://dev.to/skyhayato/how-relation-with-tpl-and-html-on-vscode-11a9</link><author>SKY-HaYaTo</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 14:19:10 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Hi,Guys! 
I'm Kohei,a Software Engineer in Japan.I am talking about a function of VSCode.Now, The Topic is about Relationship with these extension  and  on Visual Studio Code(VSCode).Extension  is often used in Web Framework of PHP.Recently, I develop personal Web apps using  and  which is one of Web MVC Frameworks of Golang. is generally used  as template engine.So, I need to make VSCode recognize  extension like .In this article, you do'nt install any plugins with ,but revide  on VSCode.VSCode is the most popular Free IDE and used in the world.
An one of nice functions on the IDE is to relation variaty of extension.As my memorandom and shrering with you,I have decited to write the article.Ok,Now,Let's explanation!In conclution, only 3 steps is completed.
  
  
  Start VSCode and Click Prompt Screen
Top of the VSCode is set (as a below picture).
You click on cursor.
  
  
  Input Value of  in the screen
Next,You need to input the value .
If maybe you continue to type , key intellisence will start and display some canditates including .When you see the word,click it!Then target page will transition to .
  
  
  Revision Settings.json File
When display ,you will type String of sentense (below the capture).You input the senetences,please save the revision.From now, we can relationship with  and .]]></content:encoded></item><item><title>ðŸš€ Day 2 #100DaysOfCode</title><link>https://dev.to/xscoox_ca5e58c796032a1802/day-2-100daysofcode-5e6n</link><author>xscoox</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 14:09:20 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Iâ€™m really enjoying this challenge so far! Solved over 15 problems on binary search in python. I plan to explore more exciting challenges ahead. ðŸš€]]></content:encoded></item><item><title>Software Engineering Job Openings Hit Five-Year Low</title><link>https://tech.slashdot.org/story/25/02/21/111216/software-engineering-job-openings-hit-five-year-low?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Fri, 21 Feb 2025 14:00:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[Software engineering job listings have plummeted to a five-year low, with postings on Indeed dropping to 65% of January 2020 levels -- a steeper decline than any other tech-adjacent field. According to data from Indeed's job aggregator, software development positions are now at 3.5x fewer vacancies compared to their mid-2022 peak and 8% lower than a year ago. 

The decline appears driven by multiple factors including widespread adoption of AI coding tools -- with 75% of engineers reporting use of AI assistance -- and a broader tech industry recalibration after aggressive pandemic-era hiring. Notable tech companies like Salesforce are maintaining flat engineering headcount while reporting 30% productivity gains from AI tools, according to an analysis by software engineer Gergely Orosz. 

While the overall job market shows 10% growth since 2020, software development joins other tech-focused sectors in decline: marketing (-19%), hospitality (-18%), and banking/finance (-7%). Traditional sectors like construction (+25%), accounting (+24%), and electrical engineering (+20%) have grown significantly in the same period, he wrote. The trend extends beyond U.S. borders, with Canada showing nearly identical patterns. European markets and Australia demonstrate more resilience, though still below peak levels.]]></content:encoded></item><item><title>6 New AI-Powered Tech Startups Reach Unicorn Status in January 2025</title><link>https://dev.to/saad_hassan_8f937dc6fafc9/6-new-ai-powered-tech-startups-reach-unicorn-status-in-january-2025-pa3</link><author>Saad Hassan</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 13:47:37 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Six cutting-edge AI startups have skyrocketed to unicorn status in January 2025, shaking up industries like healthcare, cybersecurity, automation, and defense. With billion-dollar valuations and massive investments, these companiesâ€”Truveta, Codeium, Mercor, Augury, Neko Health, and Epirusâ€”are redefining the future of tech. From AI-driven medical breakthroughs to next-gen coding assistants, the surge in AI funding signals a new era of innovation]]></content:encoded></item><item><title>Generate Tailored Cover Letters with AI: A Step-by-Step Guide Using FastAPI and OpenAI</title><link>https://dev.to/resume-burger/generate-tailored-cover-letters-with-ai-a-step-by-step-guide-using-fastapi-and-openai-2584</link><author>ResumeBurger</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 13:37:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In todayâ€™s fast-paced job market, a personalized cover letter can set you apart. ResumeBurgerâ€™s mission is to streamline your job application processâ€”and what better way than to leverage AI to generate tailored cover letters in seconds? In this tutorial, weâ€™ll build an API endpoint that takes your rÃ©sumÃ© details and a job description as input, then uses OpenAI to create a professional, customized cover letter. installed on your system
Basic familiarity with Python scripting
An OpenAI API key (store it securely in a  file)
Install the dependencies with:pip fastapi uvicorn openai python-dotenv

  
  
  Step 1: Secure Your API Key
Create a  file in your project directory with your OpenAI API key:OPENAI_API_KEY=your_openai_api_key_here
This keeps your sensitive credentials secure and out of your codebase.
  
  
  Step 2: Build the Cover Letter Generator Function
Weâ€™ll define a Python function that sends a prompt (including your rÃ©sumÃ© details and the job description) to OpenAIâ€™s API. The AI will return a refined cover letter tailored to the job requirements.
  
  
  Step 3: Create a FastAPI Endpoint
Next, weâ€™ll create a FastAPI app with an endpoint that accepts a JSON payload containing the rÃ©sumÃ© text and job description. It then returns the AI-generated cover letter.Replace  with the name of your Python file (without the  extension).
  
  
  Step 4: Testing and Deployment

Run the application with:
  uvicorn your_script_name:app Then send a POST request to http://localhost:8000/generate-cover-letter with JSON similar to:You should receive a refined cover letter in response.Docker Deployment (Optional):
Containerize your app for scalable deployment with a Dockerfile:
  FROM python:3.9-slim
  WORKDIR /app
  COPY . /app
  RUN pip install --upgrade pip && pip install fastapi uvicorn openai python-dotenv
  EXPOSE 8000
  CMD ["uvicorn", "your_script_name:app", "--host", "0.0.0.0", "--port", "8000"]
Build and run the container:  docker build  resumeburger-cover-letter 
  docker run  8000:8000 resumeburger-cover-letter
With just a few lines of code, youâ€™ve created a powerful AI-driven cover letter generator that can help job seekers quickly produce personalized, professional cover letters. This FastAPI-based endpoint leverages OpenAIâ€™s capabilities to refine rÃ©sumÃ© details in line with job descriptions, ensuring every application stands out.Customize this tool to fit your workflow, integrate it with ResumeBurgerâ€™s suite, and empower users to land that dream interviewâ€”faster than ever.Happy coding and best of luck in your job search!]]></content:encoded></item><item><title>Gofs - a file server written in go</title><link>https://www.reddit.com/r/golang/comments/1iuqggw/gofs_a_file_server_written_in_go/</link><author>/u/-dtdt-</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 13:06:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/-dtdt- ]]></content:encoded></item><item><title>Certifications for software architects</title><link>https://www.cerbos.dev/blog/certifications-for-enterprise-architects-domain-solutions-architects-software-engineers</link><author>/u/West-Chard-1474</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 13:02:56 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Over the years, Iâ€™ve noticed that no one can quite settle on how important certification is. All it takes is one look at the Software Architecture subreddit and youâ€™ll see people asking about certificates only to be told theyâ€™re both useless and useful.When I was working with Lemon.io (a developer marketplace with 80k+ developers), I got to see firsthand how certification affected their careers. So when I saw this topic start to pop up again (without any real answers), I decided to dive into research to see if my experience at Lemon.io still held true for architects.The value of certificationAssuming Iâ€™m talking to architects with a lot of experience, what I will say is that certification doesnâ€™t replace experience, but it does complement it really well. And, it can be a strong differentiator from your peers.Iâ€™m going to try a metaphor here. If your career is a burger, your experience is the patty and certificates are the condiments. Some people prefer their burgers with bacon, cheese, or even an egg. Depending on what your career goals are, youâ€™ll want to add different condiments to your â€˜burgerâ€™. But keep in mind, the most important thing will always be the meat, a.k.a your experience.Deciding if certification is right for you is the first step. The next step is asking the question: what is the right certification for you?I loved the Role Based Roadmap from Mumshad Mannambeth, founder & CEO at KodeKloud on navigating the certification paths (you can find it here).It inspired me to do something similar but focused on Architects. Below, youâ€™ll find 12 of the most popular architectural certificates you can choose to upgrade your career, and add more â€œtrust badgesâ€ to your CV, LinkedIn profile, or freelancer profile.Each has its own focus and value it can add depending on your career goals and role:Certifications for Enterprise ArchitectsCertifications for Domain Solutions ArchitectsSoftware Architecture, Governance, and Infrastructure CertificationAWS Certified Solutions ArchitectGoogle Professional Cloud ArchitectZachman Certified - Enterprise ArchitectMicrosoft Certified: Azure Solutions Architect ExpertCertified Enterprise Architect (CEA) Black Belt ProgramRed Hat Certified Architect (RHCA)Recognized globally, this certificate covers the TOGAF framework for designing, planning, implementing, and governing enterprise information technology architecture. So if you are working on an enterprise-wide architecture or want to switch to that direction, it can be useful.
Unlike the certifications below, this is a whole ecosystem that builds on itself. So it is a significant investment. However, if youâ€™re working in, or looking to work in, the governmental sector or with large corporations, this may be a good choice for you. Keep in mind, however, that while it may look cheap, the cost does not include training, which is provided by TOGAF-accredited partners, each of whom sets their own price.The comment below sums up my research into TOGAF 9 really well:Best for Software Architects working in large organizations: Enterprise Architects,  Business Architects, Solutions Architects, and IT Strategy Consultants with experience in strategic planning, We had a few Enterprise Architects with TOGAF 9 certification at Lemon.io and it was a nice value-add to their profiles. It is worth mentioning that their rates were in the top tier ðŸ™‚.13 Level 1 Learning Units27 Level 2 Learning UnitsTesting: Two-stage testing, including TOGAF 9 Part 1 and TOGAF 9 Part 2 examinations$360 USD per exam (requires two exams)English, Simplified Chinese, Spanish (Latin American), FrenchCertification expiry & recertification: TOGAF 9 certificate does not expire.ITIL is one of the most popular certification systems in the world, with more than two million certified specialists in the world. The Master certification is the highest level of ITIL certifications and requires passing all four previous certifications before challenging it. To achieve the master certification you have to pass an assessment to validate your ability to apply ITIL frameworks to real-world business scenarios. So, experience working with the ITIL principals and practices as an enterprise software architect is required.ITIL is not a training or testing provider but works with accredited partners for each. That means the pricing is dependent on your provider. The nice thing is, that it is one of the few large accreditations you can achieve through self-study.If you want to chat with peers who are planning to become ITIL Masters, there is an active subreddit called ITIL_Certification.: Governance & Compliance Architects, Governance and Compliance Managers, Enterprise Architects.Five distinct levels to progress through: Foundation -> Practitioner â€“> Intermediate -> Expert -> Master. Each level has its own training and requirements.Attend a training course with an accredited training organization, which will include the exam as part of the course.Self-study using the core manual, then book an exam directly with PeopleCert.Dependent on your chosen training/testing partner.All 4 previous ITIL certifications, including ITIL Expert certification5 years in IT leadership, management, or higher management advisory levels.English, Brazilian Portuguese, Chinese, Dutch, French, German, Italian, Japanese, Polish, Spanish, ThaiCertification expiry & recertification: Certification is valid for 3 years. You can renew your certification by retaking the exam or by earning a new certification.Zachman Certified - Enterprise ArchitectThis certification covers the Zachman Framework for designing and maintaining Enterprise Architectures, which aligns IT with business goals. The training is focused on high-level enterprise planning, including Enterprise Architecture principles, strategy formulation, and practical application, rather than project or solutions architecture. This makes it ideal for those who need a structured approach to solve enterprise challenges.This is another multi-level certification option. Each level builds on the one before it, which makes it a very comprehensive course. And, in my humble opinion, very expensive.: Enterprise Architects, Business Architects, and IT Consultants with an Enterprise Architecture focus.Four levels available: Associate, Practitioner, Professional and Educator (last one only required for those who want to teach the course).Training: Two weeks of online prep work, and three days of live instructionTesting: Two-hour, online exam (passing grants level 1 Associate)Case study: Delivering a case study provides level 2 Practitioner, and a second case study provides level 3 Professional$2999 USD covers level 1 & 2 (regional pricing is available)Each level requires certification in the preceding levelCertification expiry & recertification: The certificate expires in 3 years. Recertification costs $99 USD.Certified Enterprise Architect (CEA) Black Belt Program (Owned by Zachman now)Zachman now owns CEA, so I decided to add this certification under the Zachman banner. Just like Zachman, this is a three-step course, except the naming convention is designed to make you feel like youâ€™re in a martial art, which is cool. Itâ€™s also $7,000 more, which is not as cool, but that does mean you get to skip the yellow and green belt and go straight for the black belt.Designed to develop Enterprise Architects through hands-on training and real-world projects, this program will prepare you for leadership roles in Enterprise Architecture. This program is built on ISO standards and focuses on the practical application of frameworks, tools, and methodologies.: (Very rich people) Senior Enterprise Architect, IT Director with EA experience, Chief Architect, Enterprise Architect with 10+ years experience, Enterprise Architecture Consultant.Accelerated Path of 12 Weeks: Five individual courses taught in parallel over twelve weeks.Progressive Path that is self-paced: Five individual online courses taken at your own pace over 24-30 weeks.The IASA Global certification is a vendor-independent program for Business Technology Architects. The training has four stages that roughly align with your career stage. For Enterprise Architects, the professional (3rd) tier is the most useful  The previous two tiers are for those in earlier stages of their career. The training is based on practical experience with a focus on Enterprise Architecture (EA), Software Architecture (SA), Solution Architecture (SolA), Infrastructure Architecture (IA) and Business Architecture (BA).Unlike most tiered options, IASA allows you to challenge each level even if you havenâ€™t attained the certification under it. So if youâ€™ve progressed in your career far enough that you donâ€™t think a foundational certification is valuable, and you donâ€™t want to work your way through 3 tiers you already fully understand, IASA may be the answer for you.: Senior architects and business analysts aiming to bridge the gap between business and technology.Four levels of certification:Professional (recommended for enterprise-level architects) - achieved by presenting to a board of CITA-D certified architects and answering any questions. 2 hrs allotted for the exam.Distinguished - achieved by presenting to a board of CITA-D certified architects and answering any questions. 2.5 hrs allotted for the exam.Testing: Online or onsite testing is available (for professional tier) Exam: $425 USD  N/A  Exam + prep: $2,000 USD  Exam + prep: $2,800 USDA minimum of 10 years in the industry as a practicing architect.  Extensive documentation is required.CITA-Associate level certificate is recommended but not mandatory.Certification expiry & recertification: Requires individuals to maintain an active Full Iasa Membership and collect at least 80 hours of Continuing Education Units bi-annually (based on their website).The Open Group ArchiMate 3 CertificationArchiMate was mentioned a lot over Reddit. The Open Group ArchiMate 3 doesnâ€™t teach you how to be an EA but rather focuses on how to communicate better as an EA by teaching ArchiMateâ€™s modelling language. This language is designed to remove ambiguity from the description, analysis, and visualization of Enterprise Architectures.The content of the course is designed to complement TOGAF, which makes it useful for Enterprise Architects who already work in a TOGAF framework. They arenâ€™t however, the same thing.: Enterprise ArchitectsTraining: Online self-study available, or attend an accredited training course (Accredited Training Courses provide an exam voucher).ArchiMate 3 Part 1 offers foundation certification (60 min time limit for 40-question, multiple choice exam. Passing score: 60%)ArchiMate 3 Part 2 offers practitioner certification (90 min time limit for 8-question, scenario-based and complex multiple choice exam. Passing score: 65%)Online or in-person proctored exams available depending on providerTraining cost depends on the provider.  Each exam costs $360 USD. None  Foundation certification or pass Part 1 exam on the same day with the same provider.Certification expiry & recertification: The certification does not expire, which is very cool.AWS Certified Solutions ArchitectThe AWS certification is a great starting point for architects or senior Software Engineers with AWS Cloud or strong on-premises IT experience. It covers the design and optimization of AWS cloud-based software and shows you can handle complex multi-service architectures, which is crucial as more companies move to the cloud. The exam tests real-world scenarios you would face designing large-scale systems, including hybrid architectures, multi-region deployments, and cost optimization at scale. Basic familiarity with programming concepts will help, but you donâ€™t need hands-on experience with code.This is the only provider-specific certificate that offers a more in-depth, self-directed training option for a price. Of course, you donâ€™t have to take that option. If youâ€™re confident, you can take the free training and then challenge the test. However, the test is also the cheapest among these certifications, so it offsets the overall cost a little bit.During my research, I found quite a few posters, including this one, that had seen significant benefits from taking the course.: Systems Administrators (Cloud Focused), Cloud Architects, Solutions Architects, Cloud Consultants Software Architects for Cloud-Based Applications, and Enterprise Architects.3 courses of online, self-directed exam prep are available15.25 hrs free; 48.75 hrs paidTesting: 130 minutes. Pearson VUE testing center, or online proctored exam1 year of hands-on experience designing cloud solutions with AWS services.English, French (France), German, Italian, Japanese, Korean, Portuguese (Brazil), Spanish (Latin America), Spanish (Spain), Simplified and Traditional ChineseGoogle Professional Cloud ArchitectIf youâ€™re all in on Google, this certification is for you. It will help you show your cloud architecture skills and advance your career in Googleâ€™s cloud technology. Keep in mind, this is focused on Googleâ€™s cloud infrastructure and doesnâ€™t cover software architecture.The training and exam for this certificate are all online (though on-site exams are available) which makes it very flexible. Plus, itâ€™s pretty cheap (although itâ€™s the most expensive of the provider-specific options). However, if you go through all the training, itâ€™s going to take you some time, as itâ€™s the longest provider-specific course here.: Cloud Architects, Solutions Architects, IT Project Managers focused on the cloud, Cloud Engineers, DevOps Engineers, and Enterprise Architects.Training: 114.75 hrs, online, self-directedTesting: 2-hr test, with two options: an online proctored exam, or an onsite-proctored exam at a testing center.3+ years of industry experience, including 1+ year of designing and managing solutions with Google CloudCertification expiry & recertification: The certificate is valid for 2 years. Recertify by retaking the exam within 60 days of the expiration date.Microsoft Certified: Azure Solutions Architect ExpertThis certificate shows you know your way around Azure, including how to design and implement cloud and hybrid solutions. It dives deep into how various IT infrastructure components in the Microsoft ecosystem (like compute, network, storage, monitoring, and security) interact to generate solutions. Just like the Google certificate above, this is infrastructure-focused, not software-focused, so keep that in mind when considering it.The course for the Microsoft certification is shorter than Googleâ€™s by almost 100 hours and can be taken both online at your own pace, or with an online instructor. Itâ€™s also a bit cheaper than Googleâ€™s, making it an easier investment both for hours and dollars spent.: Systems Administrators, Network Engineers, IT Managers, Cloud Architects, Computer Systems Analysts and Infrastructure Engineers.Training: Self-paced online learning (15.25 hrs), or instructor-led online training (4 days).Testing: Online proctored exam.$165 USD  Self-paced training is free; instructor-led depends on the provider.Experience with Azure administration and development, and DevOps processes.  Advanced experience and knowledge of IT operations.English, Chinese (Simplified), French, German, Japanese, Korean, Portuguese (Brazil), SpanishCertification expiry & recertification: The certificate expires after one year. Recertification is free via an online assessment on Microsoft Learn.Red Hat Certified Architect (RHCA)Red Hatâ€™s highest level of certification, the RHCA designation covers designing, implementing, and managing Red Hat-based IT infrastructures. One of the nice things is that RHCA offers tracks in both infrastructure and enterprise applications. So you can choose the option that best matches your goals.Red Hat prices its certification differently than most. Itâ€™s a subscription-based model, which allows you to have a little more freedom with how you tackle their training. In fact, their certification offers the most custom options, with a variety of options to get from A (where you are) to B (certified). So if you have diverse interests, this one might be the one for you.: Senior Systems Administrator, Cloud Architect, IT Infrastructure Architect, DevOps Engineer, Senior Application Developer, Enterprise Solutions ArchitectRed Hat Certified Architect in InfrastructureRed Hat Certified Architect in Enterprise ApplicationsCertification builds on prerequisites with five additional certifications chosen from a list.Training and exams depend on your chosen path and certifications. Standard: $7,500 USD/year for 25 training units and certification.  Premium: $9,000 USD/year for 30 training units and certification.Red Hat Certified Engineer (RHCE)  OR  Red Hat Certified Enterprise Microservices Developer (RHCEMD)  OR  Red Hat Certified Cloud-native Developer (RHCCD)Recommended experience depends on your specific path.Certification expiry & recertification: Certifications become â€˜non-currentâ€™ after three years. To maintain an RHCA certification, you must maintain five additional certifications over your RHCE. RHCEMD or RHCCD. These certifications do not need to be the same as those youâ€™ve taken to attain your RHCA.Made for IT professionals who want to master SOA, this certification covers designing, implementing, and managing Service-Oriented Architecture (SOA) infrastructure.SOA certification is one and done. There are no levels or long-term commitment to one system, which makes it less of a commitment. Itâ€™s also very affordable.: Enterprise Architects, Solutions Architects, IT Architects, Software Architects, Systems Engineers with SOA experience, Technical Leads with architecture responsibilities170 mins online proctored exam50 hrs of training over 5 modules (modules include: Workbook Lessons, Video Lessons, Interactive Exercises, Mind Map Poster, Practice Exam Questions, PDFs of Workbook and Poster, Lab Exercise Booklet).$399 USD for course and certificationiSAQB CPSA-F/CPSA-A (International Software Architecture Qualification Board)In contrast to TOGAF training, the CPSA program focuses on the practical implementation of IT systems. Its foundation and advanced certificates offer room for architects to grow.This is a two-step program, foundation and advanced, which puts it between the single-step SOA certification and the larger three- or even four-step offerings. Just like TOGAF and ITIL, iSAQB is not a testing or training provider, so there are a lot of options for training. Or, if youâ€™re confident, you can challenge the exam without, though thatâ€™s not recommended.: Software designers, software developers, Software Architects, systems analysts
Though training and testing are performed by independent operators, you have four testing options available, including:Exam after classroom trainingThe cost of training is dependent on training providers.  Testing price is dependent on training providers.Training is suggested.  Foundation certification is required for Advanced.18+ months of practical experience, including:  - A higher programming language  - Technical documentation  - Object-oriented programming language  - Design and implementation of distributed applications  - Basics of modeling and abstraction; and UML and their relation to sourceLanguage is dependent on training providers.Certification expiry & recertification: The certificate does not expire.The right certification can set you apart on your resume, but itâ€™s never a replacement for experience. Depending on your career path, you may choose to get certified by one of the bodies above, or simply study on your own and prove what you can do through practical experience.If youâ€™ve decided to pursue the certification path, the options above are all great choices. Of course, each requires a commitment of time and money. While you canâ€™t warp the space-time continuum to change the time requirement, it may be possible to get your employer to help you cover some, if not all of the cost. If they do, it gives you an even higher ROI on your investment to yourself.]]></content:encoded></item><item><title>Becoming an Machine Learning Engineer in 2025</title><link>https://www.kdnuggets.com/becoming-machine-learning-engineer-2025</link><author>Nisha Arya</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/8-bit_ninja_2d_platformer_in_an_office_building_2.png" length="" type=""/><pubDate>Fri, 21 Feb 2025 13:00:55 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Read some honest advice on how to become a machine learning engineer.]]></content:encoded></item><item><title>This month in Servo: new webview API, relative colors, canvas buffs, and more!</title><link>https://servo.org/blog/2025/02/19/this-month-in-servo/</link><author>/u/wuyuwei-tw</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 12:53:26 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Servo now supports several new web API features:Weâ€™ve landed a bunch of  improvements: are a lot more useful now, with  now supporting  (@Taym95, #35040), , , and  (@Taym95, #34958).Servo aims to be an embeddable web engine, but so far itâ€™s been a lot harder to embed Servo than it should be.For one, configuring and starting Servo is complicated.
We found that getting Servo running at all, even without wiring up input or handling resizes correctly, took  of Rust code (@delan, @mrobinson, #35118).
Embedders (apps) could only control Servo by sending and receiving a variety of â€œmessagesâ€ and â€œeventsâ€, and simple questions like â€œwhatâ€™s the current URL?â€ were impossible to answer without keeping track of extra state in the app.Contrast this with WebKitGTK, where you can write a minimal kiosk app with a fully-functional webview in  of C.
To close that gap, weâ€™ve started reworking our embedding API towards something more idiomatic and ergonomic, starting with the concept embedders care about most: the .Our new webview API is controlled by calling methods on a  (@delan, @mrobinson, #35119, #35183, #35192), including navigation and user input.
Handles will eventually represent the lifecycle of the webview itself; if you have one, the webview is valid, and if you drop them, the webview is destroyed.Servo needs to call into the embedder too, and here weâ€™ve started replacing the old EmbedderMsg API with a  (@delan, @mrobinson, #35211), much like the delegates in Appleâ€™s WebKit API.
In Rust, a delegate is a  that the embedder can install its own  for.
Stay tuned for more on this next month!Other embedding improvements include:Weâ€™ve reworked Servoâ€™s , making all prefs optional with reasonable defaults (@mrobinson, #34966, #34999, #34994).
As a result:The names of all preferences have changed; see the Prefs docs for a listEmbedders no longer need a  resource to get Servo runningServoâ€™s networking is more efficient now, with the ability to cancel fetches for navigation that contain redirects (@mrobinson, #34919) and cancel fetches for <video> and <media> when the document is unloaded (@mrobinson, #34883).
Those changes also eliminate per-request IPC channels for navigation and cancellation respectively, and in the same vein, weâ€™ve eliminated them for image loading too (@mrobinson, #35041).Weâ€™ve continued splitting up our massive script crate (@jdm, #34359, #35157, #35169, #35172), which will eventually make Servo much faster to build.We now run CI smoketests on OpenHarmony using a real device (@jschwe, @mukilan, #35006), increasing confidence in your changes beyond compile-time errors.Weâ€™ve also tripled our self-hosted CI runner capacity (@delan, #34983, #35002), making concurrent Windows and macOS builds possible without falling back to the much slower GitHub-hosted runners.Servo canâ€™t yet run WebDriver-based tests on wpt.fyi, wpt.servo.org, or CI, because the  executor for the Web Platform Tests does not support testdriver.js.
 does, though, so weâ€™ve started fixing test regressions with that executor with the goal of eventually switching to it (@jdm, #34957, #34997).Thanks again for your generous support!
We are now receiving  (âˆ’11.4% over December) in recurring donations.
With this money, weâ€™ve been able to expand our capacity for self-hostedCIrunners on Windows, Linux, and macOS builds, halving  build times from over an hour to under 30 minutes!Servo is also on thanks.dev, and already  (+5 over December) that depend on Servo are sponsoring us there.
If you use Servo libraries like url, html5ever, selectors, or cssparser, signing up for thanks.dev could be a good way for you (or your employer) to give back to the community.As always, use of these funds will be decided transparently in the Technical Steering Committee.
For more details, head to our Sponsorship page.]]></content:encoded></item><item><title>Project Translate: The Translate API (Part 4)</title><link>https://dev.to/__dbrown__/project-translate-the-translate-api-part-4-1726</link><author>Emmanuel Akolbire</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 12:42:21 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hey developers! ðŸ‘‹ For the last post in the series, we'll provision the infrastructure on AWS and deploy our API. Let's dive in! You can check out my GitHub for the complete code.First, we'll define the Terraform version and set up the backend to store the state in an S3 bucket. Since Terraform doesn't create the backend bucket automatically, you'll need to provision it beforehand. Variables cannot be used in the backend configuration so the values to be hardcoded but feel free to change them.We'll also create a few local variablesNow we'll define the configuration to provision the DynamoDB table. DynamoDB tables require a , a ,which should also specified as an attribute, and a .Next, we'll provision the output S3 bucket by specifying a name using either the  or  field. To manage storage efficiently, we'll add a lifecycle configuration that automatically deletes files after one day. Lifecycle configurations require the bucket name and at least one rule to define the retention policy.We'll start by assigning the AWSLambdaBasicExecutionRole, which grants essential permissions like creating CloudWatch logs.To package the Python scripts for the Translate Text and Translate File endpoints, we'll use the  data resource. Since the Translate File API has dependencies, we'll install them beforehand using a  before packaging.We'll also create an IAM role for the Lambda functions and attach the necessary policies. Finally, we'll define the Lambda functions using the packaged files, set the handler to , and configure the required environment variables.Finally, we'll define the API Gateway API. To handle file uploads properly, we'll include  in the  configuration, ensuring that multipart requests are base64-encoded before being sent to the Lambda function.  Additionally, we'll configure the API resources as deployment triggers, so any changes to their properties automatically trigger a new deployment.We'll also define a few outputs.To provision the infrastructure, we first configure the AWS CLI with our credentials:Next, we initialize Terraform and apply the configuration:terraform init
terraform apply
After successful provisioning, we can access the API via the output url.To wrap up this series, we've walked through writing python code for lambda functions and provisioning a complete infrastructure using Terraform and AWS. 
If you've followed along, you should now have an API that can translate text and files running on AWS. But this is just the beginning! Thereâ€™s always more to exploreâ€”whether itâ€™s optimizing performance, integrating monitoring tools, or adding a CI/CD pipeline.Iâ€™d love to hear your thoughts! Drop a comment below if you have questions, insights, or ideas for future topics. ðŸš€ Thanks for reading, and happy coding! ðŸŽ‰]]></content:encoded></item><item><title>Have we hit a scaling wall in base models? (non reasoning)</title><link>https://www.reddit.com/r/artificial/comments/1iupqgp/have_we_hit_a_scaling_wall_in_base_models_non/</link><author>/u/CH1997H</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 12:28:05 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 SonnetYet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the "scaling laws" where the chart just says "line goes up")Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scalingIt looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it]]></content:encoded></item><item><title>I built a new playground for Go</title><link>https://codiew.io/ide?t=go</link><author>/u/Halabooda</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 12:23:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[D] Have we hit a scaling wall in base models? (non reasoning)</title><link>https://www.reddit.com/r/MachineLearning/comments/1iupnet/d_have_we_hit_a_scaling_wall_in_base_models_non/</link><author>/u/CH1997H</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 12:23:20 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 SonnetYet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the "scaling laws" where the chart just says "line goes up")Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scalingIt looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it]]></content:encoded></item><item><title>I created A Easy to use Rust Web Framework</title><link>https://www.reddit.com/r/rust/comments/1iuplg1/i_created_a_easy_to_use_rust_web_framework/</link><author>/u/Rough_Shopping_6547</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 12:20:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I just published my  project!I realized there isnâ€™t a single easy-to-use, plug-and-play Rust web framework out there (at least to my knowledge), so I decided to create my own.I'd love to hear your thoughts on it!]]></content:encoded></item><item><title>Discover the Hottest GitHub Projects Revolutionizing Tech Today ðŸš€ðŸŒ</title><link>https://dev.to/bruh_buh_f683772f171823db/discover-the-hottest-github-projects-revolutionizing-tech-today-2bck</link><author>Bruh Buh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 12:11:44 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.With an impressive  on GitHub and a surge of recent activity, Composio is quickly becoming a go-to framework for developers looking to harness the power of AI agents. This open-source platform simplifies the integration of AI capabilities into applications, enabling seamless automation and intelligent interactions. Whether you're building chatbots or advanced AI systems, Composio empowers you to create cutting-edge solutions with ease and efficiency!
  
  
  Key Features of Composio:
Production-Ready Toolset:A comprehensive suite designed for immediate deployment in production environments, enabling seamless AI integration.Integrates with over 250 tools across various categories, including software platforms like GitHub and Slack, as well as operating system utilities.Supports multiple authentication protocols such as OAuth and API Keys, ensuring secure access for users.Features a modular design allowing developers to integrate custom tools and extensions easily, enhancing flexibility.To get started with Composio, you can install the core package using the following command:For the OpenAI plugin, use:pip composio-openai
Hereâ€™s a sample code snippet demonstrating how to initialize the OpenAI client and the Composio Tool Set:This example showcases how to set up the environment and prepare for using Composio with OpenAI for specific tasks, such as starring a GitHub repository.With an impressive  on GitHub and a flurry of recent updates, MinMind is making waves in the AI community! This cutting-edge project enables developers to train a 26M-parameter GPT model from scratch in just two hours, making it a game-changer for those looking to harness the power of large language models. Whether you're an AI enthusiast or a seasoned developer, MinMind provides the tools you need to create and experiment with your own AI applications effortlessly!Train a small language model from scratch for just  in under , making it accessible for anyone interested in AI.Lightweight Model Design:The smallest version of MiniMind is only 1/7000th the size of GPT-3, allowing for efficient training on standard personal GPUs.Comprehensive Training Framework:Offers a full pipeline including pretraining, supervised fine-tuning, and advanced techniques like Mixture of Experts (MoE) for scalable model capacity.Open Source and Educational Resource:Provides a fully open-source codebase with an emphasis on transparency, serving as a tutorial for beginners in the large language model (LLM) space.To get started with MinMind, you can clone the repository and install the necessary dependencies:git clone https://github.com/minimind/minimind.git
minimind
pip  requirements.txt
Hereâ€™s a simple code example demonstrating the initialization of the training process for a MiniMind model:This example illustrates how easy it is to set up and train your own lightweight language model using the MiniMind framework!With an incredible  on GitHub and a surge of recent updates, Open-WebUI is at the forefront of modern AI user interfaces! This powerful project provides a versatile framework for building and deploying user-friendly web applications that interact with AI models, making it easier than ever to create engaging and intuitive experiences. Whether you're a developer looking to enhance your AI projects or a creator aiming to bring your ideas to life, Open-WebUI is your go-to solution for innovative web-based AI interactions!
  
  
  Key Features of Open-WebUI:
Extensible and Feature-Rich Framework:Open-WebUI is designed to be highly extensible, catering to diverse user needs with a variety of built-in features for seamless AI deployment.Self-Hosted and Offline Operation:This platform operates entirely offline, giving users full control over their deployment environment and ensuring data privacy.Support for Multiple LLM Runners:It supports various LLM runners, including Ollama and OpenAI-compatible APIs, making it versatile for different applications and user preferences.Built-in Inference Engine for RAG:The platform includes a built-in inference engine for Retrieval-Augmented Generation (RAG), enhancing capabilities for AI-driven interactions.To get started with Open-WebUI, you can easily install it using Docker:
git clone https://github.com/open-webui/open-webui.git
open-webui


docker-compose up Here's how you can customize the OpenAI API URL within the Open-WebUI configuration:This example illustrates how to tailor the platform to connect with different API endpoints, ensuring flexibility for your AI projects!With an impressive  and a flurry of recent updates on GitHub, Subtrace is making waves in the world of data tracking and analysis! This powerful tool is designed to simplify the process of monitoring and visualizing data across various sources, enabling users to gain actionable insights effortlessly. Whether you're a developer seeking to enhance your applications or a data enthusiast looking to streamline your analytics, Subtrace provides the tools you need to elevate your data game!
  
  
  Key Features of Subtrace:
Wireshark for Docker Containers:Subtrace allows developers to monitor and analyze incoming and outgoing requests to their Docker containers, akin to how Wireshark operates for network traffic.Out-of-the-Box Functionality:The tool integrates seamlessly into existing workflows without requiring any code changes, making it ready to use right away.Detailed Request Monitoring:Users can access comprehensive insights into server interactions, including full request payloads, headers, status codes, and latency.Minimal Performance Overhead:With less than 100Âµs of performance overhead, Subtrace ensures that monitoring does not significantly impact application performance.To install Subtrace and get started with monitoring your Docker containers, simply follow these steps:
docker pull subtrace/subtrace


docker run  8080:8080 subtrace/subtrace
Here's a quick example of how to monitor requests in a Docker container:This example demonstrates how easy it is to start monitoring requests while using Subtrace alongside any programming language!With an astounding  and a surge of recent activity on GitHub, Exo is capturing the attention of developers everywhere! This innovative tool is designed to simplify and enhance the development experience, providing a robust framework for building and deploying applications effortlessly. Whether youâ€™re a seasoned developer or just starting out, Exo empowers you to create powerful, efficient solutions with ease and flair!Exo enables users to run their own AI cluster using everyday devices, making powerful AI capabilities accessible right from home.Automatic Device Discovery:The platform automatically detects available devices on the network, simplifying the setup process for creating a unified AI cluster without manual configuration.Exo provides a ChatGPT-compatible API, allowing users to run models on their own hardware with just a simple change in their application code.Dynamic Model Partitioning:The system optimally splits AI models based on available device resources, enabling users to run larger models than typically possible on a single device.
  
  
  Installation Instructions:
To install Exo, follow these steps to set up the environment from the source:
git clone https://github.com/yourusername/exo.git
exo


pip  requirements.txt


python main.py
Here's a quick example of how to leverage the ChatGPT-compatible API in your application:This example demonstrates how easily you can start utilizing Exo's capabilities to enhance your AI applications!With an impressive  and a flurry of recent activity on GitHub, MoneyPrinterTurbo is making waves in the developer community! This innovative tool simplifies the process of generating and managing financial data, enabling users to create realistic datasets for testing and analysis effortlessly. Whether you're a data analyst, developer, or researcher, MoneyPrinterTurbo empowers you to streamline your financial simulations with style and efficiency!
  
  
  Key Features of MoneyPrinterTurbo:
Automated Video Generation:Create high-definition videos automatically by providing a theme or keywords, complete with scripts, subtitles, and background music.Access the project through an intuitive web interface or integrate it seamlessly via a robust API, catering to diverse user needs.Generate multiple videos at once, allowing users to select their preferred versions for increased efficiency in content creation.Voice Synthesis and Subtitle Generation:Utilize realistic voice synthesis options and automatically generated subtitles, with customizable settings for font, color, and size.
  
  
  Installation Instructions:
To quickly get started with MoneyPrinterTurbo, follow these installation steps:
git clone https://github.com/yourusername/MoneyPrinterTurbo.git
MoneyPrinterTurbo


conda create  MoneyPrinterTurbo 3.11
conda activate MoneyPrinterTurbo


pip  requirements.txt


docker-compose up

  
  
  Example Code Snippet for Video Generation:
Here's a simple example of how to generate a video using MoneyPrinterTurbo:This example illustrates how to effortlessly create content using the powerful features of MoneyPrinterTurbo!With an impressive  and exciting recent activity on GitHub, the WeChat Bot is quickly becoming a favorite among developers! This versatile tool allows users to create and deploy powerful bots for WeChat, automating interactions and enhancing user engagement through seamless conversations. Whether you're looking to simplify customer support or develop interactive experiences, the WeChat Bot empowers you to elevate your projects with ease and efficiency!
  
  
  Key Features of WeChat Bot:
Automatic Message Responses:Built with  and , the bot efficiently automates replies to WeChat messages, making it easier to manage conversations and interactions.Users can set up the bot in just four simple steps, taking approximately two minutes, which makes it incredibly user-friendly for those with varying technical backgrounds.Multiple AI Service Integrations:The bot supports various AI services, including DeepSeek and ChatGPT, allowing users to customize their experience by selecting the service that best fits their needs.Encouragement of Community Contributions:The repository invites users to star the project and contribute improvements, fostering a collaborative environment for ongoing enhancements and new features.To get started with WeChat Bot, follow these installation steps:
git clone https://github.com/yourusername/wechat-bot.git
wechat-bot

 .env.example .env


nano .env


npm 
npm run dev

  
  
  Example Code Snippet for Configuration:
Here's how you can configure the bot to use the ChatGPT AI service:# In your .env file, add the following lines
CHATGPT_API_KEY=your_chatgpt_api_key_here
DEEPSEEK_FREE_TOKEN=your_deepseek_token_here
This snippet shows how easy it is to set up the WeChat Bot to leverage powerful AI services for seamless interaction!With a remarkable  on GitHub and a flurry of recent activity, Lucide is quickly becoming a go-to choice for developers! This dynamic icon library offers a comprehensive collection of beautifully crafted icons designed for use in web and mobile applications, enabling developers to enhance their projects with stunning visuals effortlessly. Whether you're building a new app or refreshing an existing one, Lucide provides the versatility and quality you need to elevate your design game!Lucide boasts , making it a versatile resource for both digital and non-digital projects, ideal for enhancing designs across various applications.The library offers multiple official packages, including support for popular frameworks such as , and more, ensuring seamless integration into diverse development environments.Figma Plugin Integration:With a dedicated , designers can easily access and incorporate Lucide's icons directly into their design workflows, streamlining the creative process.Lucide promotes open-source collaboration, encouraging users to contribute through documentation edits, issue reporting, and joining the active  for support and interaction.To get started with Lucide, follow these installation steps:
npm lucide


npm lucide-react

  
  
  Example Code Snippet for Usage:
Here's how to use an icon from the Lucide library in a React component:Capture amazing moments!This snippet showcases the ease of integrating Lucide icons into your React applications, enhancing both functionality and aesthetic appeal!With an impressive  on GitHub and a surge of recent activity, Fabric is making waves in the developer community! This powerful open-source toolkit is designed to simplify and streamline the process of building beautiful user interfaces, allowing developers to create stunning applications with ease. Whether you're crafting a complex web app or a sleek mobile interface, Fabric provides the robust resources you need to elevate your design and enhance user experience!Modular Problem-Solving Approach:Fabric promotes breaking down challenges into individual components, allowing users to systematically apply AI solutions for more effective problem-solving.It provides a powerful way to collect and integrate AI prompts, known as , simplifying the process of discovering and utilizing prompts for various tasks.Users can leverage Patterns for a variety of tasks such as extracting content from YouTube videos, summarizing academic papers, creating tailored writing prompts, and even generating AI art prompts.Designed specifically to address the integration challenges of AI in daily life, Fabric helps users seamlessly incorporate AI tools into their routines, enhancing creativity and productivity.To get started with Fabric, follow these installation instructions:
pip fabric


git clone https://github.com/yourusername/fabric.git
fabric
pip  requirements.txt

  
  
  Example Code Snippet for Usage:
Here's a quick example of using a Pattern in Fabric to summarize content:This snippet demonstrates how easily you can implement Patterns in Fabric to streamline your tasks and unlock the potential of AI!Boasting an impressive  on GitHub and a flurry of recent activity, UV is capturing the attention of developers everywhere! This powerful open-source tool is designed to enhance the user experience by providing a versatile framework for building stunning user interfaces with ease. Whether you're creating interactive web applications or dynamic mobile experiences, UV empowers you to bring your designs to life while ensuring optimal performance and accessibility!High-Performance Package Management:UV is , significantly improving the speed of package installations and dependency management, making it a go-to tool for Python developers.Comprehensive Project Management:It consolidates multiple tools into one, replacing , , and , while offering a universal lockfile and managing dependencies efficiently.Script and Tool Management:UV allows users to run scripts with inline dependency metadata and install command-line tools easily, providing flexibility in managing both scripts and development tools.Easy Installation Options:UV can be installed via simple commands using PowerShell or pip, making it accessible across different operating systems without requiring Rust or Python beforehand.To install UV, you can use one of the following commands:
pip uv


pipx uv

  
  
  Example Code Snippet for Usage:
Here's how to initialize a new project and add dependencies using UV:
uv init example

example


uv add ruff
This snippet showcases the simplicity of setting up a new project and managing dependencies with UV, streamlining your development workflow!With an impressive  on GitHub and a surge of recent activity, ComfyUI is making waves in the developer community! This innovative open-source framework is designed to streamline the development of user interfaces, allowing developers to create stunning, responsive applications effortlessly. Whether you're building a web app or a mobile interface, ComfyUI provides the tools you need to enhance user experience and productivity, making it a must-have in your development toolkit!Modular Diffusion Model GUI:ComfyUI serves as a powerful and modular graphical user interface for creating and managing complex Stable Diffusion workflows, making it accessible for users without coding experience.The intuitive graph/nodes/flowchart-based interface allows users to design and execute intricate workflows effortlessly, enabling seamless interaction with various image and video models.Asynchronous Queue System:It features an asynchronous queue system that enhances performance and responsiveness during complex computations, ensuring efficient processing of tasks.ComfyUI supports a wide range of image and video models, such as SD1.x, SD2.x, Stable Video Diffusion, and more, allowing for versatile creative applications.To install ComfyUI, you can refer to the documentation provided in the repository. Typically, you would clone the repository and install the necessary dependencies:
git clone https://github.com/your-username/ComfyUI.git

ComfyUI


pip  requirements.txt

  
  
  Example Code Snippet for Usage:
Hereâ€™s how to create and run a simple workflow using ComfyUI:This example illustrates the ease of setting up a workflow to generate images using different models within ComfyUI, showcasing its user-friendly capabilities.With an impressive  on GitHub and a flurry of recent activity, Sniffnet is quickly becoming the go-to tool for network monitoring enthusiasts! This innovative open-source application empowers users to analyze their network traffic with ease, providing valuable insights into data flows and connections. Whether you're a developer looking to debug your applications or a security professional aiming to enhance your network defenses, Sniffnet offers a powerful, user-friendly interface that elevates your network management experience!
  
  
  Key Features of Sniffnet:
Comprehensive Network Monitoring:Sniffnet allows users to efficiently , offering real-time statistics and visual charts to analyze data usage and patterns seamlessly.Customizable Traffic Filtering:The application provides users the ability to  on observed traffic, enabling focused monitoring based on specific data types or sources.Protocol and Service Identification:Sniffnet can identify over 6000 upper layer services, protocols, and potential threats like trojans and worms, enhancing network security management.Users can export detailed capture reports as PCAP files, facilitating further analysis or record-keeping for their network activities.To install Sniffnet using Rustâ€™s package manager, ensure Rust is installed, and then run the following command:cargo sniffnet Once installed, you can select a network adapter and start monitoring traffic with just a few commands in the terminal:
sniffnet

This straightforward setup allows users to dive right into analyzing their network traffic effectively!With an impressive  on GitHub and a surge of recent activity, Checkmate is making waves in the developer community! This innovative open-source tool is designed to streamline the code review process, making it easier than ever for teams to collaborate, identify issues, and ensure high-quality code before deployment. Whether you're working solo or in a large team, Checkmate enhances productivity and fosters a culture of code excellenceâ€”truly a must-have for any modern development workflow!
  
  
  Key Features of Checkmate:
Checkmate continuously tracks server uptime, response times, and incidents, ensuring users are promptly informed about any issues with their monitored services.Capture Agent Integration:The optional  enhances monitoring capabilities by retrieving detailed performance metrics like CPU, RAM, and disk usage, providing deeper insights into server health.Optimized Resource Management:Designed for efficiency, Checkmate boasts a , allowing it to monitor multiple servers with minimal CPU and memory usage, making it ideal for scalable environments.Alerts and Notifications:Users receive  and can manage  to stay updated on the performance and availability of their services, fostering proactive incident management.To get started with Checkmate, you can deploy it using Docker with a simple one-click command. Hereâ€™s how to install the Capture agent:
git clone https://github.com/yourusername/checkmate-capture.git
checkmate-capture
npm npm start
After installation, you can monitor your servers by configuring your Checkmate settings:: : ,
      : ,
      : This setup ensures that Checkmate regularly checks the availability and performance of your specified servers, keeping you informed every step of the way!We encourage you to dive into these exciting projects and see how they can enhance your development journey! Donâ€™t forget to star your favorite repositories to show your support and help others discover them too. Be sure to follow us for future updates, as we share new trending projects every week that you won't want to miss. Happy coding, and let's keep exploring together!]]></content:encoded></item><item><title>The Real Python Podcast â€“ Episode #240: Telling Effective Stories With Your Python Visualizations</title><link>https://realpython.com/podcasts/rpp/240/</link><author>Real Python</author><category>dev</category><category>python</category><pubDate>Fri, 21 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python Blog</source><content:encoded><![CDATA[How do you make compelling visualizations that best convey the story of your data? What methods can you employ within popular Python tools to improve your plots and graphs? This week on the show, Matt Harrison returns to discuss his new book "Effective Visualization: Exploiting Matplotlib & Pandas."]]></content:encoded></item><item><title>Junior, Trying to understand why startups use golang for backend</title><link>https://www.reddit.com/r/golang/comments/1iup4di/junior_trying_to_understand_why_startups_use/</link><author>/u/FriendshipOk6564</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:53:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello,i just took a look at the website 'who is hiring' and saw a lot of startups using ruby on rails and golang in their stack and i'm confuse, the path isn't normally mvp in rails and after some companies will rewrite their wall backend at some point in something like Java spring? it append for netflix but also a big company where i live. Why would they mixte ror and golang? Those it mean they are rewriting their ror in a microservice architecture in go?]]></content:encoded></item><item><title>Explore the Future: Trending GitHub Projects Revolutionizing Tech ðŸš€âœ¨</title><link>https://dev.to/bruh_buh_f683772f171823db/explore-the-future-trending-github-projects-revolutionizing-tech-2ifh</link><author>Bruh Buh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 11:32:10 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.With an impressive 14,600 stars and vibrant recent activity, Composio is rapidly gaining traction as a leading framework for building open-source AI agents. This innovative platform empowers developers to seamlessly integrate and automate interactions across multiple applications, making it easier than ever to create intelligent solutions that drive efficiency and enhance productivity. Dive into the world of Composio and unlock the true potential of AI in your projects!:Built specifically for AI agents, ensuring reliability and robust functionality for seamless integration.Connects with over 250 applications, including GitHub, Notion, and Slack, enhancing development versatility.Advanced Search Capabilities:Users can perform searches through platforms like Google and Exa, streamlining information retrieval.Supports custom tools and extensions, allowing developers to tailor the framework to their specific needs.:
To get started with Composio, you can easily install it using pip:If you wish to use the OpenAI plugin, run:pip composio-openai
Sample Code for Initialization:
Hereâ€™s a simple snippet to import libraries and set up the OpenAI client along with Composio Tool Set:With an impressive 11,589 stars and a surge of recent activity, Minmind is making waves as a groundbreaking tool in the AI landscape. This innovative framework enables developers to train a 26M-parameter GPT model from scratch in just two hours, empowering users to harness the power of large language models with unprecedented ease. Dive into Minmind and experience the future of AI development at your fingertips!Train a lightweight language model for under $3 in just 2 hours on a personal GPU, making AI accessible to everyone.Open-Source Implementation:The project provides a complete open-source framework, including training processes like data cleaning, pretraining, and fine-tuning, all implemented from scratch using PyTorch.MiniMind's model is only 25.8MB, significantly smaller than traditional models, allowing for easy deployment and experimentation.Serves as both a practical tool for building language models and an educational guide for those eager to learn about LLM training and architecture.:
To get started with MiniMind, you can clone the repository and install the necessary dependencies:git clone https://github.com/yourusername/minimind.git
minimind
pip  requirements.txt
:
Hereâ€™s a simple code snippet to initiate model training:With an impressive 23,520 stars and a flurry of recent activity, MoneyPrinterTurbo is quickly becoming a favorite among developers looking to streamline their financial processes! This powerful open-source tool is designed to automate and optimize budgeting, expense tracking, and financial reporting, making it easier than ever to manage your finances efficiently. Dive into MoneyPrinterTurbo and experience the future of financial management at your fingertips!
  
  
  Key Features of MoneyPrinterTurbo
Automated Video Production:Generate high-definition videos automatically by simply providing a theme or keywords, along with scripts, subtitles, and background music.:Access the platform through both a web interface and an API, making it versatile for different user preferences and technical skills.Create multiple videos at once and customize various parameters, giving users flexibility and control over their content.Realistic Voice Synthesis:Choose from a variety of voice synthesis options to enhance video narration, ensuring a professional and engaging presentation.:
To get started with MoneyPrinterTurbo, you can clone the repository and set up a Python virtual environment:
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
MoneyPrinterTurbo


conda create  MoneyPrinterTurbo 3.11
conda activate MoneyPrinterTurbo


pip  requirements.txt
:
You can start the application using Docker with the following command:Once running, access the web interface at .With an astounding 77,216 stars and a surge of recent activity, Open-WebUI is making waves in the developer community! This innovative open-source project is designed to create user-friendly web interfaces for machine learning models, making advanced AI technology easily accessible to everyone. Dive into Open-WebUI and discover how it can transform your applications with seamless integration and intuitive design!
  
  
  Key Features of Open WebUI
Extensible and Offline Functionality:Open WebUI is designed as an extensible platform that operates completely offline, making it ideal for users who prefer a robust local solution for AI applications.Support for Multiple LLM Runners:The platform supports various Large Language Model runners, including Ollama and OpenAI APIs, providing users with flexibility to choose the best model for their specific needs.Built-in Inference Engine for RAG:The integrated inference engine supports Retrieval-Augmented Generation (RAG), enhancing the platform's capabilities for complex AI tasks and allowing for rich interactions with contextual information.Effortless Setup with Docker:Users can quickly install Open WebUI using Docker, with straightforward commands for setting up various configurations and tagging options tailored to their use cases.:
To get started with Open WebUI, you can easily set it up using Docker:
docker pull openwebui/open-webui:latest


docker run  8501:8501 openwebui/open-webui:latest
Once the container is running, access the web interface by navigating to  in your browser. This allows you to start using the platform right away!With an impressive 1,434 stars and a flurry of recent activity, Subtrace is quickly gaining traction in the developer community! This innovative open-source tool is designed to simplify the process of tracking and analyzing metrics across various systems, making it easier than ever to gain insights into performance and usage. Dive into Subtrace and unlock the power of data-driven decision-making for your projects!Wireshark for Docker Containers:Subtrace functions like Wireshark, offering specialized monitoring for Docker containers to analyze incoming and outgoing requests seamlessly.Out-of-the-Box Functionality:The tool requires no code changes and can be integrated into existing workflows immediately, simplifying the setup process for developers.Comprehensive Request Insights:Users can access detailed request information, including full payloads, headers, status codes, and latency, facilitating thorough troubleshooting and performance analysis.Minimal Performance Overhead:With a performance overhead of less than 100 microseconds, Subtrace ensures that monitoring does not disrupt application performance or responsiveness.:
To get started with Subtrace, you can easily install it using Docker:
docker pull subtrace/subtrace:latest


docker run  8080:8080 subtrace/subtrace:latest
Once the container is running, access the Subtrace interface by navigating to  in your browser to start monitoring your Docker containers!With an impressive 24,526 stars and a burst of recent activity, Exo is capturing the attention of developers everywhere! This powerful open-source framework is designed to streamline the process of building lightweight, modular applications, enabling teams to create high-performance software with ease. Jump into Exo and discover how it can elevate your development experience to new heights!Exo enables users to create a personal AI cluster using everyday devices like iPhones, Raspberry Pis, and NVIDIA GPUs, making advanced AI technology accessible to a wide audience.Automatic Device Discovery:The tool automatically discovers devices on the network, simplifying the setup process and enhancing user experience by eliminating the need for manual configurations.With a ChatGPT-compatible API, users can easily run models on their hardware with just a one-line change in their applications, streamlining integration into existing workflows.Flexible Model Partitioning Strategies:Exo supports various partitioning strategies, such as ring memory weighted partitioning, allowing efficient distribution of models across multiple devices based on their memory capacity.:
To install Exo from source, follow these steps (ensure you have Python 3.12.0 or higher):
git clone https://github.com/exo-labs/exo.git

exo


pip  requirements.txt


python main.py
Once installed, Exo will automatically discover available devices and allow you to set up your AI cluster effortlessly!With an impressive 29,419 stars and a surge of recent activity, Fabric is capturing the excitement of developers everywhere! This cutting-edge open-source framework is designed to streamline the deployment and management of applications in multi-cloud environments, making it easier than ever to build and scale robust software solutions. Dive into Fabric and discover how it can transform your development and deployment processes!Modular Approach to Problem Solving:Fabric encourages breaking down complex problems into manageable components, allowing users to systematically apply AI solutions and enhance clarity in tackling challenges.Integration of Prompts as Patterns:The framework allows users to collect and integrate prompts, referred to as , facilitating better organization and accessibility for applying relevant AI prompts to various tasks.Diverse Range of Patterns:Fabric provides a variety of Patterns tailored for everyday activities, such as extracting insights from videos, assisting with essay writing, summarizing academic papers, generating AI art prompts, and content rating.Open-Source Accessibility:As an open-source framework, Fabric is accessible and collaborative, inviting contributions from the community to continuously enhance its functionality and user experience.:
To install Fabric, follow these simple instructions:
git clone https://github.com/yourusername/fabric.git

fabric


pip  requirements.txt


python main.py
With these steps, you can easily set up Fabric and begin integrating AI into your projects!With an impressive 40,664 stars and a flurry of recent activity, UV is making waves in the developer community! This powerful open-source framework is designed to simplify the development of user interfaces, enabling developers to create dynamic and responsive applications effortlessly. Dive into UV and unlock the potential to enhance your UI projects like never before!High-Performance Package Management:UV is an extremely fast Python package and project manager developed in Rust, offering a performance improvement of  faster than traditional tools like .Comprehensive Project Management:It consolidates multiple tools into one, replacing , , and others, to simplify dependency management with features like a universal lockfile for consistent environments.Script and Command Execution:UV allows users to run scripts with inline dependency metadata and execute command-line tools in isolated environments, enhancing usability and project workflows.Flexible Installation Methods:UV can be installed via various methods, including PowerShell, PyPI, and , making it accessible for users with different preferences and setups.:
To install UV, you can choose any of the following methods:
pip uv


pipx uv


irm get.uv.sh | iex
Initializing a New Project:
After installation, you can easily initialize a new project with:This command sets up a project directory with the necessary structure, allowing you to manage dependencies efficiently. Enjoy the speed and simplicity of UV in your development workflow!With an impressive 67,981 stars and vibrant recent activity, ComfyUI is capturing the attention of developers everywhere! This powerful open-source framework is designed for building intuitive and user-friendly interfaces, making it easier than ever to create stunning applications. Dive into ComfyUI and elevate your UI development experience to new heights!Modular GUI for Diffusion Models:ComfyUI offers a powerful and modular graphical interface specifically designed for building and executing advanced stable diffusion pipelines, making it accessible for users of all skill levels.Comprehensive Model Support:The tool supports a wide array of image and video models, including SD1.x, SD2.x, and various video models like Stable Video Diffusion, enabling users to handle diverse multimedia tasks seamlessly.Asynchronous Queue System:An asynchronous queue system enhances task processing efficiency, allowing users to manage and execute multiple operations effectively without lag.Flexible Workflow Management:Users can easily load, save, and replicate complex workflows, complete with seeds and configurations, using formats like PNG and JSON for streamlined project management.:
To get started with ComfyUI, you can install it using the following command:
git clone https://github.com/yourusername/comfyui.git

comfyui


pip  requirements.txt
:
After installation, simply run the ComfyUI interface with:Now you're ready to design and execute your diffusion workflows with ease! Enjoy the flexibility and power ComfyUI offers in your projects!With an impressive 22,326 stars and vibrant recent activity, Sniffnet is making waves in the networking community! This powerful open-source tool serves as a network packet sniffer and analysis platform, allowing users to monitor and inspect network traffic with remarkable ease. Dive into Sniffnet to gain deeper insights into your network's performance and security, all while enjoying a user-friendly experience!Network Traffic Monitoring:Sniffnet enables users to monitor their Internet traffic comfortably, providing insights into network activities with real-time statistics and visualizations.The application boasts an intuitive design that makes it accessible for users of all skill levels, ensuring ease of navigation and operation.Advanced Filtering and Reporting:Users can apply filters to observed traffic and export detailed reports as PCAP files, facilitating focused analysis and record-keeping.Cross-Platform Compatibility:Sniffnet is designed to run on various operating systems, making it a versatile choice for a wide range of users.:
To install Sniffnet, you can use Homebrew on macOS and Linux with the following command:Alternatively, for Rust users, install directly from Crates.io:cargo sniffnet :
After installation, launch Sniffnet using:Now you're ready to monitor your network traffic with ease and efficiency!With an impressive 4,014 stars and a flurry of recent activity, Checkmate is quickly becoming a favorite among developers! This powerful open-source tool is designed for automating and managing continuous integration workflows, simplifying the process of testing and deploying code. Dive into Checkmate to streamline your development pipeline and enhance team collaboration like never before!
  
  
  Key Features of Checkmate
Real-Time Uptime Monitoring:Checkmate provides robust real-time monitoring of server uptime, response times, and incidents, ensuring that users can maintain server health and reliability.As a self-hosted application, Checkmate gives users full control over their monitoring environment without relying on third-party services, making it a flexible choice for any organization.Comprehensive Alerts and Reports:Users receive real-time alerts about the status of their monitored services, along with detailed reports on availability and performance metrics, enabling proactive responses to incidents.Agent Integration for Enhanced Insights:The Capture agent can be integrated to gather additional metrics such as CPU, RAM, and disk usage, providing deeper insights into server performance.:
To get started with Checkmate, you can deploy it using one of the one-click options available. Hereâ€™s how to use  for Docker deployment:
curl  https://get.coolify.com | bash
To install the Capture agent, you might need to follow specific instructions provided in its repository:
git clone https://github.com/your-username/capture.git

capture


npm 
npm start
Now you're ready to monitor your servers with Checkmate!With an impressive 14,751 stars and a surge of recent activity, pandas-ai is rapidly gaining traction in the developer community! This innovative library seamlessly integrates AI capabilities into the powerful pandas data manipulation framework, enabling users to perform complex data analysis and generate insights effortlessly. Dive into pandas-ai to elevate your data projects and unlock the full potential of your datasets with cutting-edge AI tools!Natural Language Querying:Users can interact with their datasets using natural language queries, allowing for intuitive and accessible data analysis without extensive coding knowledge.Multiple DataFrame Support:PandaAI allows users to work with multiple DataFrames simultaneously, facilitating complex comparisons and analyses across different datasets.The platform can generate visualizations, enabling users to easily create charts and graphics based on their queries for better data interpretation.User-Friendly Integration:With just a few lines of code, users can set up and interact with their datasets, making it straightforward to integrate PandaAI into various projects.:
You can install PandaAI using pip or poetry. Hereâ€™s how to do it with pip:pip :
Hereâ€™s a quick demonstration of how to use PandaAI to query a DataFrame:With these features and examples, PandaAI empowers users to harness the power of their data through accessible and visual analytics!With a remarkable 64,738 stars and a flurry of recent activity, Uptime Kuma is a standout tool in the monitoring landscape! This self-hosted status monitoring solution empowers users to keep track of their services' uptime and performance effortlessly, providing real-time alerts and comprehensive insights. Dive into Uptime Kuma to ensure your applications are always running smoothly, and never miss a beat with its intuitive and user-friendly interface!
  
  
  Key Features of Uptime Kuma
Self-Hosted Monitoring Tool:Uptime Kuma is a user-friendly, self-hosted monitoring solution that allows users to track the uptime and performance of various services effortlessly.Comprehensive Monitoring Capabilities:The tool supports multiple protocols, including HTTP(s), TCP, DNS, and more, enabling diverse monitoring scenarios such as push notifications and service checks.Uptime Kuma can send alerts through over 90 notification services, including Telegram, Discord, and Slack, ensuring users stay informed about their service statuses.:With its responsive and fast UI/UX, users can easily navigate and manage their monitoring tasks, enhancing their overall experience.Installation Steps via Docker:
To get started with Uptime Kuma, you can easily install it using Docker with the following command:docker run always  3001:3001  uptime-kuma:/app/data  uptime-kuma louislam/uptime-kuma:1
Once installed, you can access Uptime Kuma at .:
Hereâ€™s a quick snippet to set up a notification through Discord:With these features and simple installation instructions, Uptime Kuma makes monitoring your services straightforward and efficient!As you dive into these exciting projects, we encourage you to explore their features and find the perfect tools for your needs! Don't forget to star your favorite repositories to show your support and help others discover them too. Be sure to follow along for future updates, as we share new trending projects every week to keep your toolkit fresh and up-to-date. Happy exploring!]]></content:encoded></item><item><title>How to properly prepare monorepos in Golang and is it worth it?</title><link>https://www.reddit.com/r/golang/comments/1iuoppk/how_to_properly_prepare_monorepos_in_golang_and/</link><author>/u/GoDuffer</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:27:17 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello everyone. At the moment I am writing a report on the topic of a monorepo in order to close my internship at the university.Since I am a Go developer (or at least I aspire to be one), I decided to make a monorepo in Go.The first thing I came across was an article from Uber about how they use Bazel and I started digging in this direction.And then I realized that it was too complicated for small projects and I became interested.Does it make sense to use a monorepo on small projects? If not, how to split the application into services? Or store each service in a separate repository.In Java, everything is trivially simple with their modules and Gradle. Yes, Go has modules and a workspace, but let's be honest, this is not the level of Gradle.As a result, we have that Bazel is too complicated for simple projects, and gowork seems somehow cut down after Gradle.Monorepo or polyrepo for Go?Is there anything other than go work and Bazel?What is the correct way to split a Go project so that it looks like a Solution in C#, or modules in Java/Gradle?It is quite possible that I really don't understand the architecture of Go projects, I will be glad if you point me in the right direction.]]></content:encoded></item><item><title>The Deeper Love of Go (Go 1.24 early access edition)</title><link>https://bitfieldconsulting.com/books/deeper</link><author>/u/bitfieldconsulting</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 11:10:16 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Discover the Future of Tech: Trending GitHub Projects Revolutionizing AI and Development ðŸš€</title><link>https://dev.to/bruh_buh_f683772f171823db/discover-the-future-of-tech-trending-github-projects-revolutionizing-ai-and-development-57g8</link><author>Bruh Buh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 10:47:03 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.With an impressive  on GitHub and a surge of recent activity,  is making waves in the open-source community! This innovative AI agent framework empowers developers to effortlessly integrate and automate interactions across various applications, streamlining workflows and enhancing productivity. Whether you're building intelligent systems or looking to simplify complex integrations, Composio is your go-to solution for creating powerful, seamless AI-driven experiences.
  
  
  Main Features of Composio:
: Composio is specifically designed for creating reliable AI agents that are ready for production use, ensuring effectiveness in real-world applications.: It integrates with over 250+ tools across various categories, including popular platforms like GitHub, Slack, and Google, as well as OS operations and search capabilities.: Composio simplifies secure connections with support for various authentication protocols, including OAuth, API Keys, and Basic JWT.: The framework supports custom tools and extensions, allowing developers to tailor Composio to meet specific project needs.
  
  
  Code Example for Installation and Agent Creation:
:
To get started with Composio, install the core package using:If you wish to integrate with OpenAI, also install the OpenAI plugin:pip composio-openai
:
Hereâ€™s a code snippet demonstrating how to initialize your OpenAI client and create a Composio tool set:This code sets the foundation for building powerful AI functionality into your applications using Composio.With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a standout in the AI community! This groundbreaking project enables developers to train a 26M-parameter GPT model from scratch in just two hours, making advanced AI accessible like never before. Whether you're a seasoned AI enthusiast or just starting out, Minimind streamlines the process of model training, empowering you to unleash the full potential of artificial intelligence in your projects.
  
  
  Main Features of Minimind:
: Train a lightweight language model for just  in approximately , making AI accessible for individuals and small teams.: With a size of only , Minimind is  the size of GPT-3, allowing it to run on standard personal GPUs without extensive resources.Open Source with Comprehensive Resources: The project includes a complete codebase for building and training language models, along with features like Mixture of Experts (MoE), supervised fine-tuning, and dataset cleaning.Native PyTorch Implementation: The entire framework is built using native PyTorch, ensuring transparency and ease of understanding of the underlying mechanics.
  
  
  Code Example for Installation and Training:
:
To get started with Minimind, simply install the required packages using:(Ensure you have PyTorch installed based on your system's specifications.):
Here's a code snippet demonstrating how to initiate training for the MiniMind model:This example sets the stage for developing your own AI language model with minimal cost and time invested!With an impressive  on GitHub and a surge of recent activity,  is rapidly becoming a go-to solution for developers seeking to enhance their financial applications! This powerful tool is designed to automate and optimize financial transactions, making it easier than ever to manage complex monetary operations. Whether youâ€™re building robust payment systems or streamlining budgeting processes, MoneyPrinterTurbo is your ultimate ally in creating efficient and scalable financial solutions!
  
  
  Main Features of MoneyPrinterTurbo:
Automatic Video Generation: Effortlessly create high-definition videos by inputting a theme or keywords, which generates scripts, subtitles, and background musicâ€”all without manual intervention.: Accessible through both a user-friendly web interface and a robust API, allowing versatile integration for various applications and user preferences.: Generate multiple videos simultaneously, giving users the flexibility to choose from a variety of options based on their needs.Voice Synthesis and Subtitle Generation: Enjoy realistic voice synthesis with multiple options, alongside automatic subtitle generation that you can customize for font, color, and size.
  
  
  Code Example for Installation and Deployment:
:
To deploy MoneyPrinterTurbo using Docker, run the following commands:MoneyPrinterTurbo
docker-compose up
After deployment, access the web interface at:Creating a Python Virtual Environment:
Alternatively, you can set up a Python environment using conda with the following commands:git clone https://github.com/harry0703/MoneyPrinterTurbo.git
MoneyPrinterTurbo
conda create  MoneyPrinterTurbo 3.11
conda activate MoneyPrinterTurbo
pip  requirements.txt
This setup ensures you're ready to dive into video creation with MoneyPrinterTurbo!With an impressive  on GitHub and a wave of recent activity,  is rapidly making waves in the developer community! This powerful tool is designed to streamline and enhance the workflow of modern development by simplifying the management of complex command-line environments, enabling users to create, manage, and share executable scripts effortlessly. Whether youâ€™re a seasoned developer or just starting out, exo is your go-to solution for boosting productivity and mastering your command-line experience!: Users can effortlessly set up their own AI cluster using everyday devices, enabling powerful AI processing without needing specialized hardware.Automatic Device Discovery:  automatically identifies devices on the network, simplifying the setup process by eliminating manual configurations.: The software offers a ChatGPT-compatible API, allowing seamless integration of AI models into applications with minimal code changes.Flexible Model Partitioning: With support for various partitioning strategies,  optimizes resource allocation across devices, ensuring efficient utilization of memory and processing power.
  
  
  Code Example for Installation:
Installing exo from Source:
To install , you'll need a compatible environment. Here's how to get started:   git clone https://github.com/yourusername/exo.git
   exo
Set Up a Python Environment:
Ensure you have Python 3.12.0 or higher:
   conda create  exo-env 3.12
   conda activate exo-env
:
Install the necessary dependencies:
   pip  requirements.txt
:
Start the exo service:
This setup will get you started with exo, allowing you to harness its capabilities for AI model management and execution!With an impressive  on GitHub and a surge of recent activity,  is quickly becoming the go-to tool for modern developers! This powerful framework is designed to simplify and streamline the process of deploying applications, making it easier than ever to manage infrastructure through code. Whether you're automating server management or orchestrating complex deployments, Fabric empowers you to enhance your workflow and boost productivity like never before!: Fabric allows users to break down complex challenges into smaller, manageable components, making it easier to apply AI solutions incrementally.Integration of Prompts as Patterns: The framework helps users collect, organize, and integrate AI prompts, referred to as , enhancing accessibility and usability.: Fabric offers a variety of Patterns tailored for different tasks, such as extracting insights from media, assisting with essay writing, summarizing academic papers, and generating AI art prompts.Human-Centered Philosophy: The framework emphasizes a human-centric approach, focusing on how AI can augment creativity and solve real-life problems rather than replacing human efforts.
  
  
  Code Example for Installation:
:
To get started with Fabric, follow these installation steps:   git clone https://github.com/yourusername/fabric.git
   fabric
Set Up a Python Environment:
Ensure you have Python installed (preferably version 3.12 or higher):
   python  venv fabric-env
   fabric-env/bin/activate   
   fabric-envcriptsctivate      Install Required Packages:
Install the necessary dependencies using pip:
   pip  requirements.txt
:
Start using the framework:
This setup will help you harness the power of Fabric and integrate AI capabilities seamlessly into your projects!With a remarkable  on GitHub and a flurry of recent activity,  is making waves in the AI development community! This powerful, open-source framework is designed to streamline the training and deployment of large-scale AI models, enabling developers to harness cutting-edge technology effortlessly. Whether you're building sophisticated neural networks or optimizing existing models, ColossalAI provides the tools and flexibility to elevate your AI projects to new heights!
  
  
  Main Features of Colossal-AI:
Cost Reduction in Training: Colossal-AI achieves up to a  in training costs for large AI models using FP8 mixed precision training, allowing users to significantly lower expenses with minimal code changes.Instant Access to Compute Resources: Users can gain immediate access to high-end, on-demand compute resources for research without any setup, making it convenient for developers and researchers to get started quickly.Accelerated Inference Speed: The  feature enhances the inference speed of large AI models, doubling their efficiency and enabling faster deployment in real-world applications.: Colossal-AI supports numerous well-known AI models, including LLaMA, GPT-3, BERT, and more, showcasing its versatility across various AI tasks and domains.
  
  
  Code Example for Installation:
:
To set up Colossal-AI, follow these installation steps::
Simply use pip to install Colossal-AI:
:
If you want to install from the source code, clone the repository:
   git clone https://github.com/hpcaitech/ColossalAI.git
   ColossalAI
:
After navigating to the cloned directory, build and install the package:
:
For a containerized setup, pull the Docker image:
   docker pull hpcaitech/colossalai:latest
This setup will allow you to harness the power of Colossal-AI and start exploring its capabilities in training and deploying large-scale AI models!With an impressive  on GitHub and a surge of recent activity,  is rapidly becoming a go-to resource for AI enthusiasts! This cutting-edge open-source framework empowers developers to create and customize chatbots and AI applications with ease, streamlining the integration of advanced language processing capabilities. Whether you're building a conversational agent or experimenting with innovative AI solutions, MetaGPT provides the tools and flexibility to transform your ideas into reality!
  
  
  Main Features of MetaGPT:
: MetaGPT enables the assignment of different roles to various GPTs, facilitating collaboration among agents to efficiently tackle complex tasks and streamline the software development process.: This feature allows users to solve real-world problems by interpreting data, enhancing the framework's utility in practical applications and making AI solutions more accessible.: MetaGPT has received notable accolades, including a top 1.8% ranking for its paper at , underlining its credibility and contributions to the field of AI.Version Updates and Features: Continuous improvements are showcased through version releases, such as the introduction of the Retrieval-Augmented Generation module and support for multiple large language models (LLMs), enhancing versatility.
  
  
  Code Example for Installation:
:
To get started with MetaGPT, follow these installation steps::
Ensure you have Python 3.9 or later (but less than 3.12):
:
You can install MetaGPT directly from PyPI:
:
Alternatively, you can clone the repository and install in editable mode:
   git clone https://github.com/MetaGPT/MetaGPT.git
   MetaGPT
   pip :
Initialize the configuration by creating a configuration file:
 ~/.metagpt/config2.yaml
By following these steps, you'll set up MetaGPT and be ready to harness its multi-agent capabilities for your software development needs!With a remarkable  on GitHub and a flurry of recent activity,  is capturing the attention of developers everywhere! This powerful web framework is designed to simplify and enhance the development of high-performance applications, enabling developers to build robust, scalable solutions with ease. Whether you're creating a dynamic web app or an innovative API, uv provides the tools and flexibility to elevate your project to new heights!:  is designed to be , significantly improving package management efficiency. This performance boost makes it an excellent alternative for developers looking to optimize their workflow.Comprehensive Project Management: As a single tool that replaces multiple Python tools like , , and ,  simplifies dependency management with features such as universal lockfiles and automatic virtual environment creation.Script and Tools Management:  allows for easy management of script dependencies and execution, enabling users to run scripts with inline dependency metadata and provide a seamless way to manage command-line tools using ephemeral environments.:  supports macOS, Linux, and Windows, making it accessible for developers across different operating systems.
  
  
  Code Example for Installation:
:
You can install  using several methods, depending on your preference:Using curl for macOS and Linux:
   curl  https://astral.sh/uv/install.sh | sh
PowerShell command for Windows:
Using pipx for isolated installations:
After installation, you can initialize a new project with:This command creates a project directory named , enabling you to quickly set up and manage your Python projects!With an impressive  on GitHub and a surge of recent activity,  is making waves in the React community! This innovative library serves as a collection of reusable, high-quality React components designed to simplify and accelerate your development process. Whether youâ€™re building a sleek user interface or enhancing an existing application, react-bits equips you with the essential tools to create stunning, efficient React applications with ease!
  
  
  Main Features of React Bits:
Extensive Collection of Animated Components:  boasts a large library of over , including text animations, backgrounds, and interactive elements, designed to enhance web applications with engaging visuals.Lightweight and Customizable: Each component is lightweight with minimal dependencies, allowing for efficient application performance. Additionally, components come with customization options via props, enabling developers to tailor them to their specific needs effortlessly.: The components are designed to integrate smoothly into any modern React project, making it easy for developers to incorporate them without compatibility issues.: Developers can choose from  of each componentâ€”JavaScript + CSS, JavaScript + Tailwind CSS, TypeScript + CSS, and TypeScript + Tailwind CSSâ€”providing flexibility to fit different project setups.
  
  
  Code Example for Installation:
To get started with , you can easily install it via the command line interface using :After installation, refer to the comprehensive documentation at reactbits.dev for guidance on how to utilize the components effectively. For example, you can import and use a text animation component as follows:This simple integration showcases how you can enhance your application with animated components in just a few lines of code!With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to solution in the developer community! This powerful open-source tool is designed to streamline the development of hands-free applications, enabling users to harness the potential of gesture recognition and voice control for a truly immersive experience. Whether you're building innovative interfaces or enhancing accessibility, OpenHands provides the essential framework to create intuitive, user-friendly applications that stand out!
  
  
  Main Features of OpenHands:
AI-Powered Development Agents: OpenHands enables agents to perform a wide range of tasks typically handled by human developers, such as modifying code, running commands, browsing the web, and calling APIs, making it a versatile tool for enhancing productivity.Docker Integration for Easy Setup: The platform leverages Docker for easy installation and deployment, allowing users to run OpenHands effortlessly in a containerized environment with just a few commands.Multiple Operational Modes: OpenHands supports various interaction modes, including friendly CLI access, scriptable headless operation, and integration with GitHub Actions for automating workflows, thus catering to different developer preferences.Comprehensive Documentation and Community Support: Users have access to extensive documentation and troubleshooting resources, as well as a vibrant community for contributions and engagement via platforms like Slack and Discord.
  
  
  Code Example for Installation:
To get started with , you can easily set it up using Docker by pulling the required image:docker pull docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik
Next, you can run the OpenHands application with the following command, customizing it as needed:docker run  openhands  /var/run/docker.sock:/var/run/docker.sock  3000:3000 
  docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik
This command sets up OpenHands while mapping the necessary ports and volume for container interaction, allowing you to access the application at . Be sure to check the documentation for additional setup details and configuration options!With a remarkable  on GitHub and a surge of recent activity,  is making waves in the developer community! This innovative open-source platform empowers users to create stunning and intuitive user interfaces with ease, leveraging the latest advancements in UI design and functionality. Whether you're a seasoned developer or just starting out, ComfyUI equips you with the tools to bring your creative visions to life effortlessly!
  
  
  Main Features of ComfyUI:
: ComfyUI offers a powerful and modular GUI for diffusion models, allowing users to design complex workflows through an intuitive graph/nodes/flowchart interface, all without needing to write any code.: The platform supports a wide variety of image models (such as SD1.x, SD2.x, and SDXL) and video models (like Stable Video Diffusion), providing flexibility for various creative projects.Asynchronous Queue System: With its efficient asynchronous queue system, ComfyUI enables smooth management and processing of tasks, allowing users to execute multiple workflows simultaneously without performance degradation.Optimizations for Performance: The software includes smart memory management to run models on GPUs with as little as 1GB VRAM, and can operate on CPUs, making it accessible for users with different hardware setups.
  
  
  Code Example for Installation:
To get started with , you can clone the repository and install the necessary dependencies. Hereâ€™s a quick guide to set it up:
git clone https://github.com/yourusername/ComfyUI.git

ComfyUI


pip  requirements.txt
Once installed, you can run ComfyUI with the following command, which initializes the graphical interface:This command will launch ComfyUI, and you can start creating your diffusion model workflows immediately!With an impressive  on GitHub and a flurry of recent activity,  is rapidly becoming a favorite among developers! This cutting-edge tool is designed for network traffic analysis, enabling users to monitor, visualize, and understand data flows in real-time. Whether you're troubleshooting network issues or seeking to optimize performance, Sniffnet equips you with the insights you need to enhance your network management effortlessly!
  
  
  Main Features of Sniffnet:
Comprehensive Internet Traffic Monitoring: Sniffnet allows users to comfortably monitor their internet traffic in real-time, providing insights into data usage patterns and network activity with intuitive charts and statistics.Cross-Platform Compatibility: The application is designed to work seamlessly across various operating systems, ensuring accessibility for a broad audience of users, regardless of their preferred platform.Customizable Filters and Notifications: Users can apply specific filters to observed traffic, set custom notifications for defined events, and even manage favorite hosts for quick access, making monitoring tailored to individual needs.Detailed Reporting and Protocol Identification: Sniffnet can export comprehensive capture reports as PCAP files and identify over 6000 services and protocols, giving users a deep understanding of their network connections.
  
  
  Code Example for Installation:
To get started with , you can install it using different methods based on your operating system. Here are a couple of popular installation commands:Using Homebrew (macOS and Linux):
Using Cargo (if you have Rust installed):
  cargo sniffnet Once installed, you can launch Sniffnet to begin monitoring your internet traffic!With an impressive  on GitHub and a surge of recent activity,  is rapidly making waves in the development community! This innovative tool is designed to streamline and automate the testing process for your code, ensuring that everything runs smoothly and efficiently. Whether you're a seasoned developer or just starting out, Checkmate equips you with the capabilities to enhance your testing workflow and deliver high-quality software with confidence!
  
  
  Main Features of Checkmate:
Open Source and Self-Hosted: Checkmate is an open-source application that users can self-host on their servers, providing full control over their monitoring environment and the ability to modify the codebase as needed.Comprehensive Monitoring Capabilities: It tracks vital metrics such as server uptime, response times, and hardware status, while also facilitating website and Docker container monitoring, making it a versatile tool for infrastructure management.Real-Time Alerts and Reporting: Users receive real-time alerts for downtime and performance issues, along with detailed reports to keep them informed about their infrastructure's health and performance.Capture Agent for Enhanced Data Retrieval: The optional Capture agent allows users to gather additional metrics like CPU and RAM usage, enhancing the functionality of Checkmate and providing deeper insights into system performance.
  
  
  Code Example for Installation:
To install , you can use Docker for a straightforward setup. Hereâ€™s a command to deploy Checkmate using a one-click deployment option:Using Coolify for Docker Deployment:
Additionally, if you want to install the  for enhanced monitoring, you can follow the installation instructions provided in its separate repository. This ensures you have everything set up correctly for optimal performance!We hope youâ€™re excited to explore these amazing projects and discover how they can enhance your development experience! Donâ€™t forget to star your favorite repositories to show your support and keep track of them. Be sure to follow us for future updates, as we share new trending projects every weekâ€”thereâ€™s always something fresh and innovative to check out! Happy coding!]]></content:encoded></item><item><title>AI and the future of work - an EU perspective</title><link>https://v.redd.it/cx0l3st20hke1</link><author>/u/snehens</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 10:34:58 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>**&quot;ðŸš€ Dive into Innovation: Top Trending GitHub Projects Shaping the Future of AI!&quot;**</title><link>https://dev.to/bruh_buh_f683772f171823db/-dive-into-innovation-top-trending-github-projects-shaping-the-future-of-ai-29c7</link><author>Bruh Buh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 10:26:59 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.With an impressive  and a surge of recent activity,  is making waves in the open-source community! This innovative AI agent framework empowers developers to seamlessly integrate and manage intelligent automation across diverse applications, enhancing productivity and enabling the creation of advanced AI solutions with ease. Join the excitement and discover how Composio is transforming the landscape of AI development!
  
  
  Main Features of Composio:
:Composio provides a robust framework specifically designed for AI agents, ensuring reliability and efficiency in production environments.Integrates with over , including popular platforms like GitHub, Gmail, and Slack, along with support for OS operations and search functionalities.Facilitates secure integrations by supporting various authentication protocols such as OAuth and API Keys.Allows users to customize and extend the toolset by adding their own tools and extensions, promoting flexibility.
  
  
  Code Example: Installation Steps
To get started with Composio, you can easily install the package using the following command:For additional support with OpenAI, install the OpenAI plugin:pip composio-openai

  
  
  Example Code Snippet: Creating an AI Agent
Here's a brief example of how to initialize the OpenAI client and set up the Composio Tool Set in Python:This code snippet demonstrates the essential setup needed to create a powerful AI agent using Composio.With an impressive  and a flurry of recent activity,  is capturing the attention of the AI community! This groundbreaking framework allows developers to train a 26M-parameter GPT model from scratch in just two hours, making advanced AI capabilities more accessible than ever. Join the excitement and discover how Minimind is revolutionizing the way we approach AI model training!
  
  
  Main Features of MiniMind:
Train a lightweight language model from scratch for just  and in approximately , making advanced AI development accessible to everyone.Open Source Implementation:Provides a complete open-source framework with a simplified architecture for large models, covering all aspects from dataset cleaning to pre-training and fine-tuning.Entirely constructed with , offering flexibility and control over the model's implementation without reliance on third-party libraries.Features a multimodal vision language model, , extending its capabilities beyond text to include visual processing.
  
  
  Code Example: Installation Steps
To get started with MiniMind, you can easily install it using the following command:
  
  
  Example Code Snippet: Training the Model
Hereâ€™s a brief example to illustrate how to set up and train your own MiniMind model:This snippet shows you how straightforward it is to get up and running with MiniMind, allowing you to dive right into training your own language model!With an astounding  and a surge of recent activity,  is making waves in the open-source community! This powerful tool is designed to simplify and automate the process of generating income through various online avenues, empowering users to explore profitable ventures effortlessly. Dive into MoneyPrinterTurbo and discover how it can transform your financial strategies while streamlining your path to success!
  
  
  Main Features of MoneyPrinterTurbo:
Automated Video Generation:Generate high-definition videos automatically by inputting just a topic or keyword, complete with scripts, subtitles, and background music.Utilize both a user-friendly  and a robust  for seamless integration and accessibility.Batch Processing Capability:Create multiple videos simultaneously and customize various aspects, including length and clip duration, for efficient production.Voice Synthesis and Customization:Choose from a variety of realistic voice synthesis options, allowing for real-time previews and customizable subtitles to enhance viewer engagement.
  
  
  Code Example: Installation Steps
To get started with MoneyPrinterTurbo, you can install it using Docker with the following commands:MoneyPrinterTurbo


docker-compose up

  
  
  Accessing the Web Interface
Once Docker is running, you can access the web interface by opening your browser and navigating to:This setup will allow you to start using MoneyPrinterTurbo for your automated video generation needs!With an impressive  and a flurry of recent activity,  is rapidly becoming a go-to tool in the open-source community! Designed to streamline data processing and enhance machine learning workflows, Exo empowers developers by providing a robust framework that simplifies complex tasks and promotes efficiency. Dive into Exo and unlock the potential to supercharge your projects with cutting-edge capabilities!Run your own AI cluster using everyday devices like smartphones, computers, and Raspberry Pi, making advanced AI accessible to everyone.Easily integrate AI models into your applications with a  to the code, thanks to the ChatGPT-compatible API provided by Exo.Automatic Device Discovery:Simplifies setup by automatically discovering devices on the network, allowing for seamless integration without manual configuration.Flexible Partitioning Strategies:Supports various model partitioning methods, including ring memory weighted partitioning, optimizing resource utilization across all connected devices.
  
  
  Code Example: Installation Steps
To install Exo from source, follow these steps:
git clone https://github.com/yourusername/exo.git

exo


pip  requirements.txt

  
  
  NVIDIA GPU Support (if applicable)
If you're using a Linux system with NVIDIA GPU support, ensure you have the NVIDIA driver and CUDA toolkit installed:
nvidia-smi


nvcc These steps will help you get started with Exo and leverage its powerful capabilities for running AI models across your devices!With an impressive  and a surge of recent activity,  is quickly establishing itself as an essential tool in the developer community! Designed to simplify and enhance the process of building and deploying applications, Fabric provides a cohesive framework that streamlines development workflows and fosters collaboration. Dive into Fabric today and experience the power of efficient application management at your fingertips!Fabric is an open-source solution designed to enhance human capabilities through the integration of artificial intelligence, making it accessible for users to leverage AI in everyday tasks.Pattern Collection and Integration:The framework allows users to collect and integrate AI prompts known as , streamlining the usage of AI across various applications and simplifying workflow management.:Emphasizing a human-centered approach, Fabric encourages breaking down complex problems into manageable components, ensuring that AI serves to enhance human creativity rather than replace it.Diverse Application Patterns:Fabric offers a variety of Patterns for practical applications, including extracting insights from multimedia, writing essays, summarizing academic papers, and generating matched AI art prompts.
  
  
  Code Example: Installation Steps
To install Fabric, you can choose from multiple methods. Hereâ€™s how to install it from source:
git clone https://github.com/yourusername/fabric.git

fabric


pip  requirements.txt

  
  
  Example Usage of a Pattern
Once Fabric is installed, you can use a Pattern for summarizing an academic paper like this:These features and examples showcase how Fabric empowers users to effectively integrate AI into their daily lives and enhance their productivity!With a remarkable  and a wave of recent activity,  is making a significant impact in the AI development landscape! Designed to facilitate the training and deployment of large-scale AI models with ease, ColossalAI empowers developers to harness the full potential of artificial intelligence, providing a robust framework that simplifies complex processes. Dive into ColossalAI and unlock new possibilities for your AI projects today!
  
  
  Main Features of ColossalAI:
Cost Efficiency in Training:ColossalAI reduces the training costs for large AI models by  with just a , utilizing advanced FP8 mixed precision training upgrades for enhanced efficiency.Instant Access to Resources:Users can immediately start using ColossalAI without setup, gaining access to high-end on-demand computing resources, making it easier than ever to dive into AI research.Support for Multiple AI Models:The framework supports a variety of well-known AI models, including , , and , showcasing its versatility and adaptability for different applications.SwiftInfer for Enhanced Inference:With , ColossalAI accelerates processing speeds for multi-round conversations by , improving responsiveness and performance in conversational AI applications.
  
  
  Code Example: Installation Steps
To install ColossalAI, you can choose from several methods. Hereâ€™s how to install it via PyPI:
pip colossalai

  
  
  Example Usage: Basic Training Setup
Hereâ€™s a simple code snippet demonstrating how to set up a training script with ColossalAI:These features and examples highlight how ColossalAI is designed to make AI model development and deployment faster, cheaper, and more accessible for users!With an impressive  and a flurry of recent activity,  is rapidly becoming a go-to resource for developers looking to supercharge their applications with AI! Designed to facilitate the creation and fine-tuning of sophisticated language models, MetaGPT empowers users to harness the power of AI in a seamless and efficient manner. Dive into MetaGPT and transform your projects into intelligent, responsive solutions today!
  
  
  Main Features of MetaGPT:
MetaGPT enables users to assign different roles to various GPTs, facilitating collaborative efforts among agents to tackle complex tasks efficiently.Innovative Product Launch - MGX:The launch of  marks the creation of the world's first AI agent development team, showcasing groundbreaking advancements in AI agent technology.Comprehensive Software Development Process:The framework provides a complete solution for software development workflows, incorporating standardized operating procedures (SOPs) to enhance team collaboration and efficiency.Flexible Installation Options:Users can easily install MetaGPT using multiple methods, including , , or by cloning the GitHub repository, accommodating different user preferences.
  
  
  Code Example: Installation Steps
To get started with MetaGPT, you can install it using either  or . Hereâ€™s how to do it using both methods:conda create  metagpt 3.9  conda activate metagpt
pip  metagpt
pip  git+https://github.com/geekan/MetaGPT.git
git clone https://github.com/geekan/MetaGPT
MetaGPT
pip These features and installation steps illustrate how MetaGPT is designed to streamline collaborative AI development while offering flexibility to users in setting up their environment!With an impressive  and a surge of recent activity,  is making waves in the developer community! Designed to provide a seamless experience for building and managing user interfaces, uv empowers developers to create stunning, interactive applications with ease. Join the growing trend and elevate your UI development game with uv today! boasts impressive speed, claiming to be , which significantly enhances efficiency for package management.Single Tool Functionality:It consolidates multiple package management tools into one, replacing , , , and others, simplifying the workflow for developers.Comprehensive Project Management: provides a universal lockfile for consistent dependency management across environments, along with features for managing multiple packages within a project.Flexible Installation and Script Execution:Users can easily install  via commands like  and run scripts directly with inline dependency metadata, making project setup and execution seamless.
  
  
  Code Example: Installation Steps
To get started with , you can install it using either  or :To initialize a new project and add a dependency, use the following commands:uv init example  example       
uv add requests   These features and commands illustrate how  is designed to enhance package management and project handling for Python developers!With a remarkable  and a wave of recent activity,  is quickly becoming the go-to resource for React developers! This powerful library offers a collection of reusable components and hooks, designed to simplify and enhance your React application development. Dive into the world of efficient coding with react-bits and elevate your projects to new heights!
  
  
  Main Features of React Bits:
Vast Collection of Components: offers a large library of animated React components, including text and background animations, to enhance your web projects.All components are completely , making it an accessible resource for developers looking to add animations without any costs.Each component provides customization through props, allowing developers to easily tailor them to fit their specific project needs.Designed for , these components can be incorporated into any modern React project with minimal effort.
  
  
  Code Example: Installation Steps
To get started with , you can install it via the Command-Line Interface (CLI) using :
  
  
  Quick Component Usage Example
Hereâ€™s how to use a simple animated component in your project:With , enhancing your React applications with beautiful animations has never been easier!With an impressive  and a flurry of recent activity,  is making waves in the open-source community! This innovative platform empowers developers to create, share, and collaborate on customizable user interface components, streamlining the design process and enhancing productivity. Dive into OpenHands and unlock the potential to build stunning applications with ease!
  
  
  Main Features of OpenHands:
AI-Powered Development Agents:OpenHands enables agents to perform tasks typically handled by human developers, such as modifying code, running commands, and calling APIs, thereby enhancing productivity.The platform can be quickly set up using Docker, simplifying deployment and ensuring a hassle-free installation process.Once running, users can easily access OpenHands via , providing a straightforward interface for interaction.Customizable Model Provider:Users have the flexibility to choose their model provider and API key, with recommendations like Anthropic's Claude 3.5 Sonnet, offering versatile options for various applications.
  
  
  Code Example: Installation Steps
To get started with , you can pull the Docker image and run it with the following commands:
docker pull docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik


docker run docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik  /var/run/docker.sock:/var/run/docker.sock  openhands_data:/data  3000:3000  openhands-app  host.docker.internal:host-gateway 
  docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik
Once the container is running, access the application at  to start harnessing the power of OpenHands!With a remarkable  and a surge of recent activity,  is truly capturing the attention of developers everywhere! This innovative user interface framework is designed to simplify the creation of stunning applications, providing a versatile and intuitive platform that empowers developers to build with ease and creativity. Dive into ComfyUI and elevate your development experience to new heights!
  
  
  Main Features of ComfyUI:
Modular Diffusion Model Interface:ComfyUI is the most powerful and modular GUI for diffusion models, allowing users to intuitively design and execute advanced stable diffusion pipelines using a flowchart-based approach.Supports a wide variety of image and video models, including , , , and others, enabling users to leverage diverse capabilities for multimedia projects.Asynchronous Queue System:The platform employs an efficient asynchronous queue system that enhances performance by only re-executing parts of the workflow that have changed, optimizing processing time.ComfyUI operates fully offline, ensuring user privacy and access without the need for a constant internet connection.
  
  
  Code Example: Installation Steps
To install ComfyUI, you can start by pulling the Docker image and running it with the following commands:
docker pull comfyui/comfyui:latest


docker run  comfyui-app  7860:7860 
  comfyui/comfyui:latest
After the container is running, access the ComfyUI interface via  to begin creating your diffusion workflows!With an impressive  and a flurry of recent activity,  is making waves in the developer community! This powerful network traffic analysis tool empowers users to monitor and analyze network packets with ease, providing valuable insights into network behavior and performance. Dive into Sniffnet and unlock the potential to optimize your network like never before!
  
  
  Main Features of Sniffnet:
Network Adapter Selection:Users can easily choose which network adapter to monitor, ensuring tailored analysis based on their specific hardware setup.Real-Time Monitoring and Statistics:Sniffnet provides real-time charts and overall statistics of internet traffic, making it simple to visualize and understand network activity as it happens.Custom Filters and Notifications:The application allows for the creation of custom filters to refine observed traffic, along with configurable notifications for predefined network events, enhancing the monitoring experience.Users can export detailed traffic reports as , facilitating further analysis and sharing with other tools or stakeholders.
  
  
  Code Example: Installation Steps
To install  on various platforms, you can use the following commands:Install via Homebrew (macOS)::cargo sniffnet These commands ensure a quick setup across different operating systems, enabling you to start monitoring your internet traffic with Sniffnet in no time!With  and a surge of recent activity,  is quickly becoming a must-have tool in the developer community! This robust application is designed to streamline your testing processes, helping you effortlessly catch bugs and ensure code quality. Dive into Checkmate and elevate your development workflow to new heights with confidence!
  
  
  Main Features of Checkmate:
:Checkmate tracks server uptime, response times, and various infrastructure metrics such as CPU, RAM, and disk usage, ensuring a holistic view of performance.Real-Time Alerts and Reports:Users receive instant notifications regarding downtime and incidents, enabling them to respond swiftly to potential issues.Self-Hosted and Open Source:As a self-hosted and open-source application, Checkmate allows users to maintain full control over their monitoring setup while benefiting from community-driven improvements.The optional Capture agent enhances monitoring capabilities by providing detailed insights into remote server performance, including CPU usage and temperature status.
  
  
  Code Example: Installation Steps
To get started with , you can deploy it using  with one-click options or follow the installation instructions in the documentation. Hereâ€™s a quick command for one-click deployment using :docker run  80:80 checkmate:latest
For a detailed installation process, check out the Checkmate documentation portal. This will guide you through setting up both the frontend and the required Capture agent for optimal functionality!As you dive into these amazing projects, don't forget to explore all the features they offer and find out how they can enhance your development journey! Be sure to star your favorite repositories to show some love and support for the creators behind them. We invite you to follow along for future updates and insights, as we share new trending projects every weekâ€”there's always something exciting on the horizon! Happy coding!]]></content:encoded></item><item><title>ChatGPT took an oath to protect its own.ðŸ˜„ðŸ¤–</title><link>https://www.reddit.com/r/artificial/comments/1iuno62/chatgpt_took_an_oath_to_protect_its_own/</link><author>/u/snehens</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 10:15:32 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ðŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-21</title><link>https://dev.to/bruh_buh_f683772f171823db/13-most-exciting-github-projects-this-week-2025-02-21-228</link><author>Bruh Buh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 10:04:43 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.With an impressive  on GitHub and a flurry of recent activity,  is rapidly becoming the go-to integration platform for AI agents and applications. Designed to simplify the process of connecting and automating workflows across over 250+ applications, Composio empowers developers to effortlessly create intelligent solutions that enhance productivity and streamline operations. Join the vibrant community and discover how Composio can revolutionize your development projects!
  
  
  Key Features of Composio:
Production-Ready Toolset:Specifically designed for AI agents, ensuring reliability and performance in production environments.Integrates with over 250+ tools, including major platforms like GitHub, Gmail, Slack, and more.Optimized Search Capabilities:Provides powerful search functionalities through popular engines, enhancing data retrieval and interaction.Fully supports popular AI frameworks such as OpenAI, Langchain, and Gemini, making it versatile for various applications.
To get started with Composio, you can install the core package using the following command:For those who want to integrate with OpenAI, use:pip composio-openai
Creating an AI Agent Example:
Hereâ€™s a brief code snippet demonstrating how to initialize the Composio toolset and create an AI agent:This example illustrates how simple it is to set up and utilize Composio within your projects!With an impressive  on GitHub and a surge of recent activity,  is capturing the attention of developers everywhere! This innovative tool is designed to simplify and enhance the process of building and managing mind maps, empowering users to visualize their ideas and projects with clarity and creativity. Dive into Minimind and discover how it can transform your brainstorming sessions into structured, actionable plans!
  
  
  Key Features of MiniMind:
Users can train a compact language model from scratch for approximately  in GPU rental costs and within just  of training time, making it highly accessible.The MiniMind model is extremely compact, with the smallest version being only , allowing for easy training on standard personal GPUs without the need for extensive resources.Open Source and Educational Resource:MiniMind offers an open-source implementation of large language model structures, including tools for dataset cleaning, pretraining, and fine-tuning, serving as a valuable tutorial for newcomers to LLM development.Native PyTorch Implementation:The entire codebase is built from scratch in , promoting transparency and a deeper understanding of the algorithms involved in training language models.
To get started with MiniMind, clone the repository and install the necessary dependencies. Hereâ€™s how to do it:
git clone https://github.com/yourusername/minimind.git
minimind


pip  requirements.txt

Hereâ€™s a brief code snippet demonstrating how to initiate the training of the MiniMind model:This example illustrates the simplicity of setting up and training your own language model using MiniMind!With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to tool for developers looking to optimize their financial applications! This innovative project aims to streamline and enhance the process of generating financial reports and analytics, providing users with powerful features to simplify complex tasks. Get ready to supercharge your financial workflows with MoneyPrinterTurbo and unlock new levels of efficiency and insight!
  
  
  Key Features of MoneyPrinterTurbo:
Automated Video Generation:Easily create high-definition videos by simply providing a theme or keywords, with the tool automatically generating scripts, materials, subtitles, and background music.Offers both a user-friendly  and an , making it versatile for different user needs and technical skills.Generate multiple videos simultaneously and customize video segments, giving users complete control over their video projects.Voice Synthesis and Subtitles:Supports advanced voice synthesis options with real-time previews, along with customizable subtitles for enhanced accessibility and engagement.
To get started with MoneyPrinterTurbo, follow these steps to set up your environment:
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
MoneyPrinterTurbo


conda create  MoneyPrinterTurbo 3.11
conda activate MoneyPrinterTurbo


pip  requirements.txt

Once the setup is complete, you can start the Docker containers:docker-compose up

docker compose up
After the Docker containers are running, access the web interface by navigating to:This will get you started on creating amazing videos with MoneyPrinterTurbo!With an impressive  on GitHub and a surge of recent activity,  is making waves in the developer community! This powerful tool is designed to streamline the process of building and deploying microservices, providing developers with the flexibility and efficiency needed to create scalable applications. Dive into Exo and elevate your development game with its innovative features and user-friendly interface!Run your own AI cluster using everyday devices, making advanced AI capabilities accessible to everyone without the need for specialized hardware.Automatic Device Discovery: simplifies the setup process by automatically discovering devices on your network, requiring zero manual configurationâ€”just plug in and go!Integrate the  with a single line of code, allowing you to run AI models on your own hardware effortlessly.Dynamic Model Partitioning:The platform optimally distributes model workloads across devices with advanced partitioning strategies, enabling the efficient use of available resources.
To set up Exo on your system, start by ensuring you have Python 3.12.0 or higher. Then, follow these commands to install from the source:
git clone https://github.com/yourusername/exo.git
exo


pip  requirements.txt

For users with NVIDIA GPUs, verify your driver installation:These steps will get you started on harnessing the power of Exo for your AI projects!With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to solution for developers! This powerful framework simplifies deployment and management of applications across various environments, ensuring a seamless integration process. Dive into Fabric and streamline your development workflow with its innovative features designed to enhance productivity and collaboration!Modular Problem-Solving Approach:Fabric encourages users to break down challenges into manageable components, allowing for systematic application of AI solutions tailored to each specific issue.Patterns for Prompt Management:The framework focuses on collecting and integrating AI prompts as , making it easier for users to access, manage, and utilize valuable prompts for various tasks in their daily lives.Fabric offers a diverse range of Patterns designed for different activities, such as summarizing academic papers, generating AI art prompts, and extracting key insights from multimedia content, enhancing productivity across multiple domains.Philosophy of Enhancing Human Creativity:The framework is built on the belief that AI should act as a magnifier of human creativity, helping users leverage technology to solve real-world challenges and improve their daily workflows.
To get started with Fabric, you can install it from source by following these commands:
git clone https://github.com/yourusername/fabric.git
fabric


pip  requirements.txt

development
This quick setup will prepare you to harness the power of Fabric in your AI projects!With an impressive  on GitHub and a surge of recent activity,  is making waves in the AI community! This powerful framework is designed to streamline the development and deployment of large-scale AI models, empowering developers to tackle complex machine learning tasks with ease. Dive into ColossalAI and elevate your AI projects to new heights with its innovative tools and features!
  
  
  Key Features of Colossal-AI:
Colossal-AI allows users to reduce the training costs of large AI models by up to  with just a , thanks to its FP8 mixed precision training capabilities.The framework enables users to generate  with just one click using its  model, making video creation simple and accessible.With the introduction of , the framework has successfully doubled the inference speed for large AI models, significantly enhancing performance and efficiency for applications.Colossal-AI provides tailored solutions for various models, including , ensuring optimized performance for inference, fine-tuning, and pretraining.
To get started with Colossal-AI, you can install it via PyPI using the following command:
pip colossalai
Alternatively, you can install it from the source:
git clone https://github.com/hpcaitech/ColossalAI.git
ColossalAI


pip  requirements.txt
This quick setup will get you up and running with Colossal-AI, ready to tackle large-scale AI projects!With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to resource for developers looking to harness the power of AI! This innovative framework is designed to simplify the creation of customized AI applications, enabling users to fine-tune models for their specific needs effortlessly. Dive into MetaGPT and unlock a world of possibilities in AI development!Multi-Agent Collaboration:MetaGPT enables users to assign different roles to AI agents, allowing for effective collaboration to tackle complex programming tasks within a structured framework.Comprehensive Software Development Framework:The platform integrates Standard Operating Procedures (SOPs) to manage the entire software development process, ensuring systematic project execution across various roles, including product managers and engineers.Versatile Output Generation:Users can input a one-line requirement and generate a variety of outputs such as user stories, competitive analysis, APIs, and documentation, streamlining the development workflow.Research and Community Engagement:MetaGPT has gained recognition in the research community, with its innovative papers accepted at conferences, and it emphasizes open-source collaboration, allowing users to contribute to its development.
To get started with MetaGPT, follow these installation steps:Ensure you have  (but less than ) installed on your system.Create a new conda environment:   conda create  metagpt 3.9  conda activate metagpt
   pip  metagpt
Alternatively, clone the repository and install:git clone https://github.com/geekan/MetaGPT MetaGPT  pip This will set you up to leverage MetaGPT's powerful multi-agent capabilities for your software projects!With an impressive  on GitHub and a buzz of recent activity,  is rapidly gaining traction as a must-have tool for developers! This powerful framework is designed to simplify the development of ultra-fast web applications, providing an efficient platform to build and deploy innovative solutions effortlessly. Dive into uv and elevate your web development game to new heights!Extremely Fast Package Management: boasts package management speeds that are , making it a top choice for developers looking for efficiency in managing their Python projects.Unified Tool Replacement:This tool consolidates multiple package managers and toolsâ€”like , , and â€”into a single solution, simplifying the development workflow and reducing tool clutter.Single-File Script Management:uv supports managing dependencies for single-file scripts, allowing developers to declare dependencies inline and execute scripts seamlessly in isolated environments.Comprehensive Project Management:It features a universal lockfile for consistent dependency management and supports project workspaces, enhancing organization and stability across environments.
To install , you can use the following command for :Or for , install via  with:To initialize a new project named , use:To add a dependency like :With these commands, you'll be set to leverage uv for efficient package and project management!With an impressive  on GitHub and vibrant recent activity,  is making waves in the React community! This essential library is designed to provide a collection of reusable components and utilities, enabling developers to build stunning applications with ease and efficiency. Dive into react-bits and unlock the potential for rapid development and elegant design in your next React project!
  
  
  Key Features of React Bits:
Extensive Library of Components: offers a robust collection of 60 animated React components designed to enhance web projects, with a focus on continuous growth and variety.Each component comes with , allowing developers to easily modify styles and behaviors to suit their specific needs.The components are crafted for  into any modern React project, ensuring versatility across different frameworks and setups.Components are available in  (JS + CSS, JS + Tailwind CSS, TS + CSS, TS + Tailwind CSS), catering to different developer preferences and project requirements.
To install React Bits via the command line interface, you can use  with the following command:After installation, you can import a component into your project like this:With these features and easy installation, React Bits makes it a breeze to enhance your web applications!With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a favorite in the developer community! This innovative platform empowers users to create and share rich, interactive applications seamlessly, making it easier than ever to bring ideas to life. Dive into OpenHands and discover the endless possibilities for enhancing your projects with powerful tools designed to foster collaboration and creativity!
  
  
  Key Features of OpenHands:
AI-Powered Development Agents:OpenHands enables AI-driven software development agents that can perform tasks like modifying code, running commands, and calling APIs, significantly streamlining the development workflow.Users can effortlessly deploy OpenHands using Docker, simplifying the runtime environment setup with just a few commands:
   docker pull docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik
Flexible Interaction Modes:OpenHands supports various operating modes, including a , , and integration with , catering to different developer preferences and workflows.Comprehensive Documentation:Detailed  is available, guiding users through setup, configuration, and troubleshooting, ensuring a smooth user experience.Running the OpenHands Application:
Once you have pulled the Docker image, run the application with the following command:docker run always docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik  /var/run/docker.sock:/var/run/docker.sock  ~/.openhands-state:/.openhands-state  3000:3000  host.docker.internal:host-gateway  openhands-app 
    docker.all-hands.dev/all-hands-ai/openhands:0.25
After running the command, access the application at  and start leveraging the power of AI in your development projects!With a remarkable  on GitHub and a surge of recent activity,  is capturing the attention of developers everywhere! This innovative tool simplifies the creation of user-friendly interfaces for AI models, enabling users to design, customize, and deploy stunning applications with ease. Dive into ComfyUI and experience the seamless integration of functionality and creativity as you bring your AI projects to life!Powerful Modular Interface:ComfyUI offers a highly powerful and modular  that allows users to design and execute advanced stable diffusion pipelines with ease.Visual Workflow Creation:Users can create complex workflows using a  interface, enabling intuitive management of tasks without needing extensive coding knowledge.The platform supports a wide range of image and video models, including SD1.x, SD2.x, Stable Video Diffusion, and more, providing flexibility for diverse applications in content creation.Asynchronous Queue System:ComfyUI features an asynchronous queue system for efficient task management, ensuring multiple operations can run simultaneously without blocking.
To get started with ComfyUI, you can clone the repository and install the required dependencies:git clone https://github.com/ComfyUI/ComfyUI.git
ComfyUI
pip  requirements.txt

After installation, run the application with the following command:With an impressive  on GitHub and a surge of recent activity,  is making waves in the network monitoring space! This powerful tool enables users to effortlessly analyze their network traffic in real-time, providing valuable insights into data flow and potential security threats. Dive into Sniffnet and experience how it transforms complex network analysis into an intuitive and engaging process!
  
  
  Key Features of Sniffnet:
Network Traffic Monitoring:Sniffnet allows users to comfortably monitor their Internet traffic, providing insights into network activity with real-time visualization.Cross-Platform Compatibility:The application is , ensuring it works seamlessly across various operating systems and enhancing accessibility for all users.Advanced Traffic Filtering:Users can apply  to observed traffic, enabling focused analysis of specific data types and improving monitoring efficiency.Detailed Statistics and Reporting:Sniffnet provides  on Internet traffic, and users can export comprehensive capture reports in PCAP format for further analysis.Installation using Homebrew (macOS):
To install Sniffnet on macOS, simply use the Homebrew package manager:Installation using Cargo (for Rust users):
If you have Rust installed, you can build and install Sniffnet via:cargo sniffnet 
After installation, launch the application with the following command in your terminal:Start monitoring your Internet traffic and gain valuable insights into your network activity!With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to tool for developers and testers alike! This powerful application is designed to streamline the process of validating software functionality through robust automated testing, ensuring that your projects run smoothly and efficiently. Dive into Checkmate and discover how it can enhance your development workflow while boosting your confidence in software quality!
  
  
  Key Features of Checkmate:
Comprehensive Monitoring:Checkmate excels in monitoring server hardware, uptime, response times, and incidents in real time, providing a complete overview of your infrastructure health.Beautiful Visualizations:The application offers  of monitored data, making it easy to interpret complex metrics and gain insights at a glance.Users receive real-time alerts and reports regarding system availability and performance, enabling proactive management of their infrastructure.Lightweight and Efficient:Checkmate is optimized for performance, boasting a  which allows it to monitor over 300 servers without significant resource consumption.Installation Instructions:
To get started with Checkmate, you can easily deploy it using Docker. Hereâ€™s a one-click deployment option with Coolify:Basic Configuration with Capture Agent:
After installing Checkmate, set up the Capture agent to monitor server infrastructure effectively:git clone https://github.com/yourusername/capture.git
capture
npm npm start
This will ensure you have detailed insights into your server performance, including CPU and RAM usage!In conclusion, we encourage you to dive into these amazing projects and explore all the fantastic tools they have to offer! Donâ€™t forget to star your favorite repositories to show your support and help others discover them too. Be sure to follow along for future updates, as we share new trending projects every week that are bound to inspire your next big idea. Happy coding, and we can't wait to see what you'll create!]]></content:encoded></item><item><title>My experience after switching from Java to Go</title><link>https://www.reddit.com/r/golang/comments/1iuni44/my_experience_after_switching_from_java_to_go/</link><author>/u/hosmanagic</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 10:03:43 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/hosmanagic ]]></content:encoded></item><item><title>FiveCrop in PyTorch</title><link>https://dev.to/hyperkai/fivecrop-in-pytorch-19bi</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 10:03:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[FiveCrop() can crop an image into 5 parts(Top-left, Top-right, Bottom-left, Bottom-right and Center) as shown below:The 1st argument for initialization is (Required-Type: or () or size()):
*Memos:

A tuple/list must be the 1D with 1 or 2 elements.A single value( or ()) means .The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>AVR microcontrollers are now officially maintained!</title><link>https://www.reddit.com/r/rust/comments/1iunfgx/avr_microcontrollers_are_now_officially_maintained/</link><author>/u/Patryk27</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 09:59:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[AVRs are cute & tiny microcontrollers from Atmel - you might've heard about ATmega328p used in Arduino Uno, for example:Every week we're marching towards better AVR support in Rust and as of today I can proudly say: we don't need no `target.json`s anymore + we've got an official maintainer! (points finger at self)So far AVRs remain tier 3, but at least it's waay easier to use them now - just target `avr-none` and provide `-C target-cpu` so that rustc & llvm know which specific microcontroller you're building for; a couple of important codegen fixes are also coming together with rustc's upgrade to LLVM 20, hoping to wrap up on https://github.com/Rahix/avr-hal/pull/585 over the coming days.I'd like to take this moment to thank https://github.com/benshi001 for his continued support and code reviews on the LLVM's side - let AVR flourish!]]></content:encoded></item><item><title>Pad in PyTorch</title><link>https://dev.to/hyperkai/pad-in-pytorch-1ek8</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 09:54:15 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Pad() can add padding to an image as shown below:The 1st argument for initialization is (Required-Type: or /()). *A tuple/list must be the 1D with 1, 2 or 4 elements.The 2nd argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can change the background of an image. *The background can be seen when adding padding for an image.A tuple/list must be the 1D with 1 or 3 elements.The 3rd argument for initialization is (Optional-Default:-Type:). *, ,  or  can be set to it.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>Sponsoring Rust Developers</title><link>https://www.reddit.com/r/rust/comments/1iun7oj/sponsoring_rust_developers/</link><author>/u/szabgab</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 09:44:08 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[One of the "findings" of my previous question was that some crates are missing or not mature enough to be used.If you would like to use Rust you can hope that those gaps will be closed in time or you can do something about it. If you have the time and expertise you can get involved in the needed projects, but there is a much easier and less time-consuming way. You and/or your company can sponsor the development efforts.Allocating 10-20 USD / month by an individual or 1000-2000 USD month by a small company does not sound like a big investment and many such sponsors can make a huge difference together.One way to find who to sponsor is to find the developers of your dependencies. For that visit the Explore GitHub Sponsors page. On the left-hand side select the "Cargo" ecosystem. That will show you the individuals and the organizations that you currently rely upon that also accept sponsorship.I've also created a page listing some of the people and project who develop Rust and could be sponsored. For some of them I've also included background information.]]></content:encoded></item><item><title>Python Data Parsing Guide: 10 Advanced Techniques for Structured Data (2024)</title><link>https://dev.to/aaravjoshi/python-data-parsing-guide-10-advanced-techniques-for-structured-data-2024-lgo</link><author>Aarav Joshi</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 09:12:17 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! Python Parsing: Advanced Techniques for Structured DataParsing structured data efficiently remains crucial in modern software development. Python offers robust tools and libraries for handling various data formats. Let's explore practical techniques that enhance data processing workflows.XML Processing with Element TreeThe ElementTree API provides memory-efficient XML parsing. It reads XML documents as tree structures, enabling straightforward navigation and modification.JSON Processing with ijsonLarge JSON files require iterative processing to manage memory efficiently. The ijson library enables streaming JSON parsing, processing one object at a time.Polars provides exceptional performance for large-scale CSV processing, offering multithreaded operations and efficient memory usage. \
     \
     \
     \
    Binary Data with Protocol BuffersProtocol Buffers offer efficient serialization and parsing of binary data with strong typing and backward compatibility.Regular Expression ParsingRegular expressions provide powerful pattern matching capabilities for custom data format parsing.Parsing Expression Grammars with LarkLark enables the creation of complex parsers for domain-specific languages and custom formats.Optimizing Parser PerformanceParsing performance depends on various factors. Consider these optimization strategies:Memory Management: Use generators and iterative processing for large datasets.Parallel Processing: Implement multiprocessing for CPU-intensive parsing tasks.Caching: Implement caching for frequently parsed patterns or expressions.Error Handling and ValidationRobust parsing requires comprehensive error handling and validation.Data Pipeline IntegrationIntegrate parsing components into data pipelines for automated processing.This comprehensive approach to parsing encompasses various data formats and scenarios, providing practical solutions for common data processing challenges. The techniques presented focus on efficiency, scalability, and maintainability, essential aspects of modern data processing applications. is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly lowâ€”some books are priced as low as â€”making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>The 5 best programming languages of 2025</title><link>https://dev.to/scrapestorm/the-5-best-programming-languages-of-2025-1dmb</link><author>ScrapeStorm</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 09:08:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.You might ask, â€œIs Java obsolete?â€ Of course not.Why is Java still popular? Java is one of the oldest and most robust programming languages. It is also an object-oriented language mainly used for Android application development. This is one of the main reasons it is still used today. However, with the advent of programming languages â€‹â€‹like Kotlin (also suitable for Android development), Java is becoming less popular.However, Java remains one of the most expensive programming languages â€‹â€‹and is in great demand. According to Indeed, software developers are interested in Java developers and pay more than $ 100,000 a year.Swift iOS application development is currently very popular. Swift is a very stable programming language and worth studying. Itâ€™s easier to learn than Java. YouTube has a lot of resources to help you learn, and programming with them is fun. If youâ€™re a freelance developer in Swift now, or if you work for a job related to it, your annual salary can reach $ 115,000.SQL or Sequel is a structured search language. Some people think itâ€™s not really a programming language. SQL is mainly used for data management and interactive. So SQL is an essential skill in programming, and any type of web development (backend or full stack) needs to be learned to manage the data. According to statistics, the average annual income of SQL developers is over $ 90,000.This is a mysterious programming language, and some people think itâ€™s the best. JavaScript is a very popular language. If you check GitHub, youâ€™ll always see new frameworks that support JavaScript. In addition, all browsers support JavaScript. Therefore, learning JavaScript is one of the skills that software development must know. JavaScript developers can earn income in the range of $ 90,000 to $ 113,000.According to Google Trends and the PyPI Popularity Index, Python is one of the most popular programming languages â€‹â€‹in the world and certainly one of the most expensive programming languages. Google was built in Python, and YouTube was also developed in Python.The amazing thing about Python is that itâ€™s a general-purpose programming language used to build a wide range of applications. Furthermore, it is active in artificial intelligence. Self-driving cars, Wal-Mart auto-payment, and many automation and machine learning (ML) apps were developed through Python. This makes this language more important and rapidly popularizes. In addition, Python is easier to learn than all other languages â€‹â€‹and is easy for beginners. You can also build complex applications relatively easily and quickly. In the United States, the average salary for Python developers is about $ 78,000, while experienced developers can be as high as $ 122,000.]]></content:encoded></item><item><title>Real-Time Audio Processing in Python: A Complete Guide with Code Examples [2024]</title><link>https://dev.to/aaravjoshi/real-time-audio-processing-in-python-a-complete-guide-with-code-examples-2024-25cf</link><author>Aarav Joshi</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 09:01:39 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[As a best-selling author, I invite you to explore my books on Amazon. Don't forget to follow me on Medium and show your support. Thank you! Your support means the world! Audio Processing in Python: Real-Time Techniques and ApplicationsPython offers powerful capabilities for real-time audio processing through various specialized libraries. I've worked extensively with these tools and will share practical insights into implementing efficient audio processing solutions.Audio Input/Output with PyAudioPyAudio provides the foundation for real-time audio processing in Python. It interfaces directly with sound cards and audio devices, enabling low-level control over audio streams.Advanced Audio Analysis with LibrosaLibrosa excels in audio feature extraction and music processing. I frequently use it for spectral analysis and music information retrieval tasks.Digital Signal Processing with PyDSPPyDSP enables implementation of complex DSP algorithms. Here's an example of real-time filtering:Efficient File Operations with SoundFileSoundFile provides fast and reliable audio file handling:Professional Audio I/O with SoundDeviceSoundDevice offers professional-grade audio handling with ASIO support:Music Analysis with AubioAubio provides sophisticated music analysis capabilities:Real-Time Audio VisualizationImplementing real-time audio visualization enhances the monitoring of audio processing:To achieve optimal performance in real-time audio processing:Managing latency is crucial for real-time applications:These techniques form a comprehensive toolkit for real-time audio processing in Python. The combination of these libraries and methods enables the development of sophisticated audio applications, from music analysis to real-time effects processing.The key to successful implementation lies in understanding the balance between processing complexity and real-time performance requirements. Through careful optimization and appropriate use of these tools, we can create efficient and effective audio processing solutions.I've found that maintaining clean audio streams, implementing proper buffer management, and using appropriate threading techniques are essential for professional-grade audio applications. The examples provided serve as building blocks for more complex audio processing systems. is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly lowâ€”some books are priced as low as â€”making quality knowledge accessible to everyone.Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !Be sure to check out our creations:]]></content:encoded></item><item><title>Why Python Developers Should Dive Into Blockchain Now</title><link>https://dev.to/crosschainer/why-python-developers-should-dive-into-blockchain-now-14cg</link><author>crosschainer</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 08:41:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If youâ€™re a Python developer, thereâ€™s never been a better time to dive into the world of blockchain. Until recently, blockchain development has largely been reserved for developers willing to learn specialized languages like Solidity or Rust. But now, platforms like  are breaking down those barriersâ€”letting you build smart contracts and decentralized applications (dApps) directly in Python (no code transpiling).The question isnâ€™t  Python developers should explore blockchain, but rather why havenâ€™t you started yet? This post will show you why Python developers are uniquely positioned to thrive in the blockchain space and how Xian makes it easier than ever.
  
  
  1. Pythonâ€™s Massive Ecosystem Meets Decentralization
With over 13 million developers worldwide, Python is the most accessible and versatile programming language today. From AI and machine learning to web development and data analysis, Python is everywhere.Blockchain doesnâ€™t just add another tool to your toolkitâ€”it multiplies the possibilities for every project you work on.
  
  
  2. Breaking Down Barriers: No New Languages, Just Python
Traditionally, blockchain development meant learning new languages like:Solidity for Ethereum smart contracts.Rust for Solana development.But with the , you can:Write smart contracts .Avoid complicated cross-language conversions.Deploy dApps using the tools and syntax youâ€™re already comfortable with.ðŸŽ¯ â€œWhy learn a new language when you can use the one you already know?â€By removing the need to learn specialized languages, Xian lets you focus on building, not learning new syntax.
  
  
  3. A Seamless Developer Experience
Xian doesnâ€™t just let you write Python smart contractsâ€”it offers a full development environment tailored to your workflow:ðŸ”—  Easy-to-use APIs for seamless interaction with the blockchain.ðŸ“Š Comprehensive Documentation: Clear and developer-friendly resources to help you get started quickly.With Python and Xian, smart contract development feels like any other Python projectâ€”no unnecessary friction.
  
  
  4. Real-World Use Cases for Python on Blockchain
Here are some exciting ways Python developers can leverage blockchain right now:
  
  
  ðŸ” Decentralized Identity VerificationBuild systems where users control their personal data.
  
  
  ðŸ’» Decentralized Finance (DeFi) ToolsCreate lending platforms, decentralized exchanges, or stablecoins.
  
  
  ðŸ”— Automated Royalties for CreatorsImagine an NFT marketplace where musicians and artists automatically receive royalties every time their content is resoldâ€”no intermediaries needed, all coded in Python.
  
  
  ðŸ’¸ Decentralized Crowdfunding PlatformsBuild a transparent crowdfunding platform where funds are only released when predefined conditions (smart contract milestones) are met. Perfect for open-source funding or startup incubation.Develop play-to-earn games where players can own, trade, and earn real value from in-game assetsâ€”no middlemen, no asset loss when a game shuts down.
  
  
  ðŸ”’ Secure Supply Chain TrackingWrite Python contracts that track goods from origin to delivery. Each step is verified on the blockchain, preventing fraud and ensuring transparency.
  
  
  ðŸ“Š Predictive Market PlatformsCreate decentralized platforms where users can stake tokens to predict real-world events.
  
  
  âš–ï¸ Decentralized Autonomous Organizations (DAOs)Let communities govern themselves by building DAO voting systemsâ€”users can vote on proposals directly through the blockchain using Xianâ€™s smart contract features.
  
  
  5. Xian Empowers Developers to Earn
Beyond the technical advantages, Xian offers a financial incentive for developers:Earn 70% of all fees generated by your smart contracts.This makes Xian not just a development platformâ€”but an ecosystem where developers can earn by building impactful projects.
  
  
  6. How to Get Started with Xian Blockchain
Ready to dive in? Hereâ€™s how you can start your blockchain journey today: Install the Web3 Wallet and open up the IDE.Write your first smart contract: Use familiar Python syntax. Deploy your contract and monitor transactions. Connect with other developers on Telegram and contribute to open-source projects.Hereâ€™s a simple example of a smart contract in Python:
  
  
  Conclusion: The Future of Blockchain Is Written in Python
The blockchain space is evolvingâ€”and Python developers are at the forefront of this revolution. With Xian Blockchain, you can:Write smart contracts using the language you already know.Build powerful decentralized applications.Earn revenue from your contributions to the network.So why wait? Start building, innovating, and shaping the future of blockchain with Python today.ðŸš€ Ready to dive in? Check out the Xian Documentation and start coding your first smart contract now!]]></content:encoded></item><item><title>Implementing Layer and MLP Classes in micrograd (As Explained By Karpathy)</title><link>https://dev.to/shrsv/karpathy-10-45ih</link><author>Shrijith Venkatramana</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 08:07:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, Iâ€™m building LiveAPI, a tool that makes generating API docs from your code ridiculously easy.
  
  
  Replicating The micrograd Program in PyTorch
In PyTorch we can replicate our micrograd expression using the following code. Here instead of the  class we use the  class.The output is -- which agrees with the output from the previous post:0.7071066904050358
----
x2 0.5000001283844369
w2 0.0
x1 -1.5000003851533106
w1 1.0000002567688737
In a typical real-world project, instead of scalars, we'd use larger tensors.For instance, we can define a 2x3 tensor as follows:tensor([[1., 2., 3.],
        [4., 5., 6.]])
By default, PyTorch stores number as , so we convert them to  as expected:Also, in PyTorch by default nodes are not expected to require gradients. This is for efficiency reasons - for example, we do not require gradients in leaf nodes.For nodes that require gradients, we must explicitly enable it:
  
  
  Start Building Neural Networks on the  Class
The goal is to build a two layer MLP (Multi-Layer Perceptron).In the above code  will generate a random number between -1 and 1. And  is the number of inputs. So if we want say 10 inputs to our Neuron, then we will set .For reference, this is the diagram for a neuron:
  
  
  The  Mechanism In Python Classes
We have a  mechanism, which can be used to use the objects of type Neuron  though they were functions.
  
  
  Implementing  on a neuron
I get an output like this:Value(data=-0.6963855451596829, grad=0, label='')
As of now, on every run the value received will be different - since we are initializing with random inputs during initialization.
  
  
  Defining a Layer of Neurons
The code for defining a layer of neurons is as follows:For a layer - we need to take in the number of inputs and number of outputs, and we simply create a list of Neuron objects first.When the Layer is called, we just call each neuron object with the given input values.The above code gives a result like this:[Value(data=0.8813774949215492, grad=0, label=''),
 Value(data=0.9418974314812039, grad=0, label=''),
 Value(data=0.3765244335798038, grad=0, label='')]
You can see how the above will transform into a list of layers - with the right number of input and output neurons:Gives a  object to the last output (after forward pass).We can visualize the whole expression graph with following:The result is a huge expression graph - representing the whole expression with a single output node.]]></content:encoded></item><item><title>What is a RESTful API? A Beginnerâ€™s Guide</title><link>https://dev.to/kihuni/what-is-a-restful-api-a-beginners-guide-4hdg</link><author>kihuni</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 08:00:27 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In todayâ€™s digital world, applications need to communicate seamlessly with each other. Whether you log into a website, check the weather on your phone, or order food online, you're likely interacting with a RESTful API in the background.But what exactly is a RESTful API? In this guide, we'll break it down in a simple and digestible way.
  
  
  What Does "RESTful" Even Mean?
Letâ€™s start with the basics.An API (Application Programming Interface) is a way for different software applications to communicate with each other. Think of it like a waiter in a restaurant:You (the client) place an order.The waiter (API) takes your request to the kitchen (server).The kitchen prepares the food and the waiter brings it back to you.In technical terms, an API allows a client (e.g., a web app or mobile app) to request and receive data from a server.So, What Are RESTful APIs??REST stands for Representational State Transferâ€”donâ€™t worry, itâ€™s not as complicated as it sounds! Itâ€™s just a set of instructions for how computers and apps should share info over the internet. Think of it like a simple rulebook for keeping their conversations clear and organized.A RESTful API is an API that follows these REST rules. Itâ€™s like calling a cafÃ© â€œspecialtyâ€ because it makes coffee in a certain way. RESTful APIs have their principles which APIs must follow to qualify as "RESTFUL APIs".
  
  
  What Makes an API 'RESTful'
For an API to be RESTful, it must follow these principles:Client-Server Architecture â€“ The client (e.g., a mobile app) and server (e.g., a database) remain separate so they can evolve independently.Statelessness â€“ The server does not store client data between requests. Every request contains all the necessary information.Cacheability â€“ Responses can be cached to improve performance.Uniform Interface â€“ Consistent resource naming and use of HTTP methods.Layered System â€“ Requests can pass through intermediaries (e.g., load balancers) without affecting how they function.RESTful APIs rely on standard HTTP methods to perform actions on resources. Imagine an online payslip system:: (Get payslip with ID 123) (Add a new payslip) (Update payslip with ID 123) (Delete payslip with ID 123)Example API Request (Python)Hereâ€™s a simple Python example using the requests library to fetch a payslip:import requests

response = requests.get("https://api.example.com/payslips/123")
if response.status_code == 200:
    print(response.json())
else:
    print("Error fetching data")
This request retrieves a payslip from the API and prints the JSON response.
  
  
  RESTful APIs vs. Other API Styles
REST: Uses lightweight JSON or XML over HTTP.SOAP (Simple Object Access Protocol): More complex, uses XML, and requires additional protocols like WS-Security.REST: Fixed endpoints (/users, /orders).GraphQL: Allows flexible queries, fetching only necessary data
  
  
  Authentication in RESTful APIs
APIs often require authentication to protect sensitive data. Common methods include:API Keys â€“ Unique keys assigned to users.OAuth 2.0 â€“ Secure authorization protocol used by platforms like Google and GitHub.JWT (JSON Web Tokens) â€“ Tokens used for secure authentication between client and server.Example authentication using a Bearer Token:headers = {"Authorization": "Bearer YOUR_ACCESS_TOKEN"}
response = requests.get("https://api.example.com/payslips/123", headers=headers)
print(response.json())
RESTful APIs are a clever and organized way for apps and computers to share stuff over the internet. They use simple ideas like resources (think of them as items theyâ€™re working with), HTTP methods (like â€œgetâ€ or â€œsendâ€), and web addresses (called URIs), plus some cool tricks like not remembering past chats (statelessness) and saving info for later (caching). All of this keeps things fast and smooth. Whether youâ€™re creating payslips or playing music on an app, RESTful APIs are quietly making it happen every day.Next time youâ€™re using an app, think about the RESTful magic working behind the scenesâ€”itâ€™s pretty awesome! If youâ€™re curious, try playing with a free API (like a weather APIâ€”itâ€™s a fun way to start!).]]></content:encoded></item><item><title>Talk Python to Me: #494: Update on Flet: Python + Flutter UIs</title><link>https://talkpython.fm/episodes/show/494/update-on-flet-python-flutter-uis</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 21 Feb 2025 08:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[As Python developers, we're incredibly lucky to have over half a million packages that we can use to build our applications with over at PyPI. However, when it comes to choosing a UI framework, the options get narrowed down very quickly. Intersect those choices with the ones that work on mobile, and you have a very short list. Flutter is a UI framework for building desktop and mobile applications, and is in fact the one that we used to build the Talk Python courses app, you'd find at <a href="https://talkpython.fm/apps">talkpython.fm/apps</a>. That's why I'm so excited about Flet. Flet is a Python UI framework that is distributed and executed on the Flutter framework, making it possible to build mobile apps and desktop apps with Python. We have Feodor Fitsner back on the show after he launched his project a couple years ago to give us an update on how close they are to a full featured mobile app framework in Python.<br/>
<br/>
<strong>Episode sponsors</strong><br/>
<br/>
<a href='https://talkpython.fm/connect'>Posit</a><br>
<a href='https://talkpython.fm/podcastlater'>Podcast Later</a><br>
<a href='https://talkpython.fm/training'>Talk Python Courses</a><br/>
<br/>
<h2 class="links-heading">Links from the show</h2>
<div><strong>Flet</strong>: <a href="https://flet.dev?featured_on=talkpython" target="_blank" >flet.dev</a><br/>
<strong>Flet on Github</strong>: <a href="https://github.com/flet-dev/flet?featured_on=talkpython" target="_blank" >github.com</a><br/>
<strong>Packaging apps with Flet</strong>: <a href="https://flet.dev/docs/publish?featured_on=talkpython" target="_blank" >flet.dev/docs/publish</a><br/>
<br/>
<strong>Flutter</strong>: <a href="https://flutter.dev/?featured_on=talkpython" target="_blank" >flutter.dev</a><br/>
<strong>React vs. Flutter</strong>: <a href="https://trends.stackoverflow.co/?tags=flutter,react-native&featured_on=talkpython" target="_blank" >trends.stackoverflow.co</a><br/>
<strong>Kivy</strong>: <a href="https://kivy.org?featured_on=talkpython" target="_blank" >kivy.org</a><br/>
<strong>Beeware</strong>: <a href="https://beeware.org/?featured_on=talkpython" target="_blank" >beeware.org</a><br/>
<strong>Mobile forge from Beeware</strong>: <a href="https://github.com/beeware/mobile-forge?featured_on=talkpython" target="_blank" >github.com</a><br/>
<br/>
<strong>The list of built-in binary wheels</strong>: <a href="https://flet.dev/docs/publish/android#binary-python-packages" target="_blank" >flet.dev/docs/publish/android#binary-python-packages</a><br/>
<strong>Difference between dynamic and static Flet web apps</strong>: <a href="https://flet.dev/docs/publish/web?featured_on=talkpython" target="_blank" >flet.dev/docs/publish/web</a><br/>
<strong>Integrating Flutter packages</strong>: <a href="https://flet.dev/docs/extend/integrating-existing-flutter-packages?featured_on=talkpython" target="_blank" >flet.dev/docs/extend/integrating-existing-flutter-packages</a><br/>
<strong>serious_python</strong>: <a href="https://pub.dev/packages/serious_python?featured_on=talkpython" target="_blank" >pub.dev/packages/serious_python</a><br/>
<strong>Watch this episode on YouTube</strong>: <a href="https://www.youtube.com/watch?v=zNyTE8W_5OM" target="_blank" >youtube.com</a><br/>
<strong>Episode transcripts</strong>: <a href="https://talkpython.fm/episodes/transcript/494/update-on-flet-python-flutter-uis" target="_blank" >talkpython.fm</a><br/>
<br/>
<strong>--- Stay in touch with us ---</strong><br/>
<strong>Subscribe to Talk Python on YouTube</strong>: <a href="https://talkpython.fm/youtube" target="_blank" >youtube.com</a><br/>
<strong>Talk Python on Bluesky</strong>: <a href="https://bsky.app/profile/talkpython.fm" target="_blank" >@talkpython.fm at bsky.app</a><br/>
<strong>Talk Python on Mastodon</strong>: <a href="https://fosstodon.org/web/@talkpython" target="_blank" ><i class="fa-brands fa-mastodon"></i>talkpython</a><br/>
<strong>Michael on Bluesky</strong>: <a href="https://bsky.app/profile/mkennedy.codes?featured_on=talkpython" target="_blank" >@mkennedy.codes at bsky.app</a><br/>
<strong>Michael on Mastodon</strong>: <a href="https://fosstodon.org/web/@mkennedy" target="_blank" ><i class="fa-brands fa-mastodon"></i>mkennedy</a><br/></div>]]></content:encoded></item><item><title>#494: Update on Flet: Python + Flutter UIs</title><link>https://talkpython.fm/episodes/show/494/update-on-flet-python-flutter-uis</link><author></author><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://talkpython.fm/episodes/download/494/update-on-flet-python-flutter-uis.mp3" length="" type=""/><pubDate>Fri, 21 Feb 2025 08:00:00 +0000</pubDate><source url="https://talkpython.fm/">Talk Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>reddittui - A terminal browser for reddit</title><link>https://github.com/tonymajestro/reddit-tui</link><author>/u/tmajest</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 21 Feb 2025 07:54:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Website Designing Company In Delhi</title><link>https://dev.to/website_designingcompany/website-designing-company-in-delhi-2jp4</link><author>Website Designing Company</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 06:08:04 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Unleash the Power of Website Designing with Web Solution Centre - The Leading Website Designing Company in DelhiIn this virtual age, having a visually appealing and consumer-pleasant internet site is vital for any enterprise looking to set up a robust on-line presence. And on the subject of website designing in Delhi, there may be one call that stands out from the relaxation - Web Solution Centre.With a group of skilled designers and developers, Web Solution Centre is devoted to offering top-notch website designing offerings to customers in Delhi and past. From idea to execution, they take care of each thing of the web site design procedure, making sure a seamless and professional give up result.One of the key elements that units Web Solution Centre aside from other website designing agencies in Delhi is their commitment to handing over custom designed answers which might be tailor-made to every patron's precise necessities. Whether you're a small begin-up or a big agency, they have the know-how and experience to create a website that completely displays your logo identification and goals.But it's now not just about appears - capability is similarly critical on the subject of web site design. Web Solution Centre makes a speciality of creating websites that aren't simplest visually stunning but additionally optimized for performance and user enjoy. Their designs are responsive, which means they adapt seamlessly to exclusive gadgets and display screen sizes, making sure a steady enjoy for all customers.So in case you're searching out a dependable and straightforward Website Designing Company Delhi, appearance no in addition than Web Solution Centre. With their expertise, creativity, and determination to excellence, they can help take your on-line presence to the next degree. Contact them nowadays to look how they permit you to achieve your virtual goals.]]></content:encoded></item><item><title>Unraveling Spatially Variable Genes: A Statistical Perspective on Spatial Transcriptomics</title><link>https://towardsdatascience.com/unraveling-spatially-variable-genes-a-statistical-perspective-on-spatial-transcriptomics/</link><author>Jingyi Jessica Li</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 06:06:04 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[The article was written by Guanao Yan, Ph.D. student of Statistics and Data Science at UCLA. Guanao is the first author of the Nature Communications review article [1].Spatially resolved transcriptomics (SRT) is revolutionizing Genomics by enabling the high-throughput measurement of gene expression while preserving spatial context. Unlike single-cell RNA sequencing (scRNA-seq), which captures transcriptomes without spatial location information, SRT allows researchers to map gene expression to precise locations within a tissue, providing insights into tissue organization, cellular interactions, and spatially coordinated gene activity. The increasing volume and complexity of SRT data necessitate the development of robust statistical and computational methods, making this field highly relevant to data scientists, statisticians, and machine learning (ML) professionals. Techniques such as spatial statistics, graph-based models, and deep learning have been applied to extract meaningful biological insights from these data.A key step in SRT analysis is the detection of spatially variable genes (SVGs)â€”genes whose expression varies non-randomly across spatial locations. Identifying SVGs is crucial for characterizing tissue architecture, functional gene modules, and cellular heterogeneity. However, despite the rapid development of computational methods for SVG detection, these methods vary widely in their definitions and statistical frameworks, leading to inconsistent results and challenges in interpretation.In our recent review published in [1], we systematically examined 34 peer-reviewed SVG detection methods and introduced a classification framework that clarifies the biological significance of different SVG types. This article provides an overview of our findings, focusing on the three major categories of SVGs and the statistical principles underlying their detection.SVG detection methods aim to uncover genes whose spatial expression reflects biological patterns rather than technical noise. Based on our review of 34 peer-reviewed methods, we categorize SVGs into three groups: Overall SVGs, Cell-Type-Specific SVGs, and Spatial-Domain-Marker SVGs (Figure 2).Methods for detecting the three SVG categories serve different purposes (Fig. 3). First, the detection of overall SVGs screens informative genes for downstream analyses, including the identification of spatial domains and functional gene modules. Second, detecting cell-type-specific SVGs aims to reveal spatial variation within a cell type and help identify distinct cell subpopulations or states within cell types. Third, spatial-domain-marker SVG detection is used to find marker genes to annotate and interpret spatial domains already detected. These markers help understand the molecular mechanisms underlying spatial domains and assist in annotating tissue layers in other datasets.The relationship among the three SVG categories depends on the detection methods, particularly the null and alternative hypotheses they employ. If an overall SVG detection method uses the null hypothesis that a non-SVGâ€™s expression is independent of spatial location and the alternative hypothesis that any deviation from this independence indicates an SVG, then its SVGs should theoretically include both cell-type-specific SVGs and spatial-domain-marker SVGs. For example, DESpace [2] is a method that detects both overall SVGs and spatial-domain-marker SVGs, and its detected overall SVGs must be marker genes for some spatial domains. This inclusion relationship holds true except in extreme scenarios, such as when a gene exhibits opposite cell-type-specific spatial patterns that effectively cancel each other out. However, if an overall SVG detection methodâ€™s alternative hypothesis is defined for a specific spatial expression pattern, then its SVGs may not include some cell-type-specific SVGs or spatial-domain-marker SVGs.To understand how SVGs are detected, we categorized the statistical approaches into three major types of hypothesis tests:Â Dependence Test â€“ Examines the dependence between a geneâ€™s expression level and the spatial location.Â Regression Fixed-Effect Test â€“ Examines whether some or all of the fixed-effect covariates, for instance, spatial location, contribute to the mean of the response variable, i.e., a geneâ€™s expression.Â Regression Random-Effect Test (Variance Component Test) â€“ Examines whether the random-effect covariates, for instance, spatial location, contribute to the variance of the response variable, i.e., a geneâ€™s expression.To further explain how these tests are used for SVG detection, we denote ð‘Œ as geneâ€™s expression level and ð‘† as the spatial locations. Dependence test is the most general hypothesis test for SVG detection. For a given gene, it decides whether the geneâ€™s expression level ð‘Œ is independent of the spatial location ð‘†, i.e., the null hypothesis is:There are two types of regression tests: fixed-effect tests, where the effect of the spatial location is assumed to be fixed, and random-effect tests, which assume the effect of the spatial location as random. To explain these two types of tests, we use a linear mixed model for a given gene as an example:
where the response variable \( Y_i \) is the geneâ€™s expression level at spot \( i \), 
\( x_i \) \( \epsilon \) \( R^p \) indicates the fixed-effect covariates of spot \( i \), 
\( z_i \) \( \epsilon \) \( R^q \) denotes the random-effect covariates of spot \( i \), 
and \( \epsilon_i \) is the random measurement error at spot \( i \) with zero mean. 

In the model parameters, \( \beta_0 \) is the (fixed) intercept, \( \beta \) \( \epsilon \) \( R^p \) indicates the fixed effects, and \( \gamma \) \( \epsilon \) \( R^q \) denotes the random effects with zero means and the covariance matrix:
In this linear mixed model, independence is assumed between random effect and random errors and among random errors.Fixed-effect tests examine whether some or all of the fixed-effect covariates \( x_i \) (dependent on spatial locations ) contribute to the mean of the response variable. If all fixed-effect covariates make no contribution, then:Random-effect tests examine whether the random-effect covariates \( z_i \) (dependent on spatial locations ) contribute to the variance of the response variable Varâ¡Yi, focusing on the decomposition: and testing if the contribution of the random-effect covariatesÂ is zero. The null hypothesis:Among the 23 methods that use frequentist hypothesis tests, dependence tests and random-effect regression tests have been primarily applied to detect overall SVGs, whereas fixed-effect regression tests have been used across all three SVG categories. Understanding these distinctions is key to selecting the right method for specific research questions.Improving SVG detection methods requires balancing detection power, specificity, and scalability while addressing key challenges in spatial transcriptomics analysis. Future developments should focus on adapting methods to different SRT technologies and tissue types, as well as extending support for multi-sample SRT data to enhance biological insights. Additionally, strengthening statistical rigor and validation frameworks will be crucial for ensuring the reliability of SVG detection. Benchmarking studies also need refinement, with clearer evaluation metrics and standardized datasets to provide robust method comparisons.[1] Yan, G., Hua, S.H. & Li, J.J. (2025). Categorization of 34 computational methods to detect spatially variable genes from spatially resolved transcriptomics data. , 16, 1141. https://doi.org/10.1038/s41467-025-56080-w[2] Cai, P., Robinson, M. D., & Tiberi, S. (2024). DESpace: spatially variable gene detection via differential expression testing of spatial clusters. Bioinformatics, 40(2). https://doi.org/10.1093/bioinformatics/btae027]]></content:encoded></item><item><title>Building Peacock Rentals: A Deep Dive into the Creation of a Modern Rental Platform</title><link>https://dev.to/tylerjohnsonn/building-peacock-rentals-a-deep-dive-into-the-creation-of-a-modern-rental-platform-p2j</link><author>Bodhi Wave</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 05:51:41 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
The digital landscape has revolutionized industries across the board, and the rental sector is no exception.  In this dynamic environment, Peacock Rentals emerges as a sophisticated online platform connecting individuals with premium rental vehicles and experiences.  This article embarks on a detailed journey through the creation of Peacock Rentals, dissecting its planning, design, and development phases. We will explore the strategic decisions made, the technological choices implemented â€“ particularly the role of languages like C++, Java, and Python â€“ and the challenges encountered and overcome.  Furthermore, we will tap into expert insights on current web development trends and best practices to provide a comprehensive understanding of this project's evolution.  Our analysis will be accessible to both seasoned technology professionals and those with a general interest in the digital world, aiming for a tone that is both technically insightful and engagingly narrative.Phase 1: Laying the Foundation - Planning and ConceptualizationEvery successful digital venture begins with a robust plan. The initial phase of Peacock Rentals was characterized by meticulous planning and conceptualization, focusing on identifying market needs, defining the target audience, and outlining the core functionalities of the platform.1.1 Market Research and Competitive Analysis:Before a single line of code was written, extensive market research was conducted. The team delved into the existing online rental landscape, analyzing competitors like traditional car rental websites, peer-to-peer rental platforms, and specialized luxury vehicle rental services.  This research aimed to pinpoint gaps in the market and identify opportunities for Peacock Rentals to differentiate itself. Key findings included:Demand for Premium Experiences: A growing segment of customers seeks more than just basic transportation; they desire unique and memorable experiences, often involving classic, luxury, or specialty vehicles.
Desire for Seamless Online Experience: Users expect intuitive, user-friendly websites with streamlined booking processes, transparent pricing, and reliable customer support.
Mobile-First Approach: With mobile browsing dominating web traffic, a responsive and mobile-optimized platform was deemed crucial.
Competitive analysis revealed strengths and weaknesses of existing platforms.  Some lacked a focus on premium vehicles, while others suffered from clunky interfaces or limited customer support. This analysis highlighted the opportunity for Peacock Rentals to establish itself as a go-to platform for discerning customers seeking high-quality rental experiences.1.2 Defining the Target Audience and Value Proposition:With market insights in hand, the team defined Peacock Rentals' target audience.  This encompassed individuals seeking:Luxury and Classic Vehicle Rentals: For special occasions, weekend getaways, or simply experiencing the thrill of driving a unique vehicle.
Reliable and Well-Maintained Vehicles: Quality and condition were paramount for this audience, necessitating rigorous vehicle maintenance and inspection protocols.
Exceptional Customer Service: Personalized support, transparent communication, and easy issue resolution were identified as key differentiators.
Based on this target audience, the core value proposition of Peacock Rentals was formulated: to offer a curated selection of premium rental vehicles, coupled with a seamless and trustworthy online experience, and underpinned by exceptional customer service.  This value proposition became the guiding principle for all subsequent design and development decisions.1.3 Defining Core Features and Functionality:The planning phase culminated in defining the core features and functionalities of the Peacock Rentals website.  These were meticulously documented and prioritized, forming the blueprint for the development process.  Key functionalities included:Vehicle Listing and Search: An intuitive search interface allowing users to filter vehicles by type, make, model, location, dates, and price. High-quality images and detailed vehicle descriptions were essential.
Booking and Reservation System: A secure and efficient online booking system with real-time availability updates, calendar integration, and secure payment processing.
User Account Management: Personalized user accounts for managing bookings, viewing rental history, saving favorite vehicles, and updating personal information.
Admin Panel for Vehicle Management: A comprehensive admin panel allowing Peacock Rentals staff to manage vehicle listings, update availability, process bookings, manage users, and generate reports.
Customer Support Portal: An integrated support system including FAQs, contact forms, live chat functionality, and a ticketing system for efficient issue resolution.
Secure Payment Gateway Integration: Integration with reputable payment gateways to ensure secure and reliable online transactions.
Responsive Design: Ensuring seamless functionality and optimal viewing experience across all devices (desktops, tablets, and smartphones).
1.4 Technology Stack Decisions: Balancing Performance and Scalability:A crucial aspect of the planning phase was the selection of the technology stack.  The team considered various factors, including performance requirements, scalability needs, development speed, and team expertise.  While the prompt emphasizes C++, Java, and Python, it's important to understand how these languages, and others, typically fit within web development:Frontend Development: For the user interface, standard web technologies were chosen: HTML5 for structure, CSS3 for styling, and JavaScript for interactivity.  A modern JavaScript framework, such as React or Vue.js, would likely be employed to enhance development efficiency, component reusability, and create a dynamic user experience.  While not explicitly in the prompt's language list, JavaScript and its frameworks are indispensable for modern web frontend development.Backend Development: This is where Java and Python become highly relevant.Java:  Java is a robust and scalable language widely used in enterprise-level applications.  Its platform independence, strong performance, and mature ecosystem (including frameworks like Spring Boot) made it an excellent candidate for the backend logic of Peacock Rentals. Java's strengths in handling concurrent requests and managing large datasets are vital for a rental platform with potentially high traffic.  The choice of Java reflects a commitment to reliability and scalability.Python: Python is renowned for its rapid development capabilities and extensive libraries.  Frameworks like Django and Flask streamline web development, making Python a compelling choice for specific backend components, API development, or microservices within the architecture. Python's ease of integration with other systems and its strong data science and machine learning capabilities could also be leveraged for future features like personalized recommendations or dynamic pricing.  Python offers development agility and flexibility.C++: While less commonly used directly for primary web application backend development compared to Java or Python, C++ plays a crucial role in performance-critical systems and infrastructure.  C++'s strength lies in its speed and control over system resources.  While not directly building the core web application logic, C++ could be employed for:High-performance backend services: For instance, if Peacock Rentals needed extremely fast search algorithms, or real-time data processing, C++ could be used to develop optimized services integrated with the Java or Python backend.
Database systems: Many database systems themselves (like MySQL, PostgreSQL) are written in C++. While not directly coding in C++ for the web app, the underlying database infrastructure leverages C++ performance.
System-level programming: For developing custom server components, network modules, or operating system level optimizations.
Database: A robust and scalable database system was essential.  PostgreSQL or MySQL are popular choices for web applications, known for their reliability and performance.  The choice would depend on specific needs and team expertise.Server Infrastructure: Cloud platforms like AWS, Google Cloud, or Azure were considered for hosting, offering scalability, reliability, and a range of managed services.  This choice would streamline deployment and infrastructure management, allowing the development team to focus on application logic.APIs and Integrations:  RESTful APIs were chosen for communication between the frontend and backend, and for integrating with third-party services like payment gateways, mapping services, and potentially CRM or marketing automation platforms.This technology stack was chosen to balance development speed, scalability, performance, and maintainability.  The combination of frontend JavaScript frameworks, Java and Python for backend, and a robust database system, hosted on a cloud platform, represents a modern and effective architecture for a platform like Peacock Rentals.Phase 2: Crafting the User Experience - Design and User InterfaceWith the foundational planning complete, the focus shifted to design and user interface (UI) â€“ crafting a visually appealing and user-friendly experience.2.1 Wireframing and Mockup Creation:The design process began with wireframing, creating low-fidelity sketches of key website pages (homepage, vehicle listing pages, product pages, booking flow).  Wireframes focused on layout, information hierarchy, and user flow, ensuring intuitive navigation and clear pathways for users to achieve their goals (searching, browsing, booking).Following wireframes, high-fidelity mockups were created using design tools like Figma or Adobe XD.  These mockups visualized the final look and feel of the website, incorporating branding elements, color palettes, typography, and imagery.  Mockups served as visual blueprints, guiding the frontend development team and allowing for stakeholder feedback and design iterations before coding commenced.2.2 UI Design and Branding: Embracing the "Peacock" Aesthetic:The "Peacock Rentals" brand name evokes imagery of elegance, luxury, and vibrancy.  The UI design aimed to reflect this brand identity.  Key design considerations included:Color Palette: A sophisticated color palette was chosen, potentially incorporating deep blues, greens, and golds, reminiscent of peacock feathers, alongside neutral tones for readability and balance.
Typography: Elegant and readable fonts were selected to convey a sense of premium quality and trustworthiness.
Imagery: High-quality professional photography of vehicles was paramount, showcasing their beauty and condition. Lifestyle imagery could also be incorporated to enhance the aspirational aspect of renting premium vehicles.
Visual Hierarchy and White Space: Clean layouts with ample white space were prioritized to ensure readability and focus user attention on key elements.
The UI design prioritized user-centricity, ensuring that the website was not only visually appealing but also highly functional and easy to navigate.  Consistency in design elements across the platform was crucial for creating a cohesive and professional user experience.2.3 User Experience (UX) Principles: Intuitive Navigation and Seamless Flows:Beyond visual appeal, user experience (UX) was paramount.  The design team focused on creating intuitive navigation and seamless user flows for key tasks:Effortless Vehicle Search: The search interface was designed to be prominent and user-friendly, with clear filtering options and suggestions to help users quickly find their desired vehicles.
Streamlined Booking Process: The booking process was simplified to minimize steps and friction. Clear call-to-action buttons, progress indicators, and secure payment forms were implemented to guide users seamlessly through the reservation process.
Mobile-First Design: Responsive design was not an afterthought; it was integrated from the outset. The UI was designed to adapt gracefully to different screen sizes, ensuring a consistent and optimal experience on all devices.
Accessibility Considerations: The design incorporated accessibility best practices to ensure inclusivity for users with disabilities. This included adhering to WCAG guidelines, using semantic HTML, providing alternative text for images, and ensuring sufficient color contrast.
User testing, even in early design stages with mockups, could have been valuable to gather feedback and refine the UX based on real user interactions.Phase 3: Building the Platform - Development and ImplementationWith the planning and design phases complete, the development phase commenced, bringing the Peacock Rentals website to life.3.1 Frontend Development: Crafting the User Interface with JavaScript Frameworks:Frontend development focused on translating the UI mockups into functional HTML, CSS, and JavaScript code.  The choice of a JavaScript framework, like React or Vue.js, would have significantly impacted the development process.  These frameworks offer component-based architecture, state management, and efficient rendering, leading to:Increased Development Speed: Component reusability and streamlined state management accelerate development.
Improved Code Maintainability: Component-based architecture leads to modular and organized code.
Enhanced User Experience: Frameworks facilitate the creation of dynamic and interactive UIs, contributing to a smoother user experience.
Frontend developers would work closely with UI/UX designers to ensure pixel-perfect implementation of the designs and seamless interactivity.  Key frontend development tasks included:Component Development: Building reusable UI components (search bars, vehicle cards, booking forms, etc.) using the chosen JavaScript framework.
API Integration: Connecting the frontend to the backend APIs to fetch vehicle data, submit bookings, and handle user authentication.
State Management: Implementing state management solutions (Redux, Context API in React; Vuex in Vue.js) to manage application data efficiently.
Responsiveness and Cross-Browser Compatibility: Ensuring the website functions correctly and looks consistent across different browsers and devices.
Performance Optimization: Optimizing frontend code for speed and efficiency, minimizing loading times and ensuring smooth interactions.
3.2 Backend Development: Powering the Platform with Java and Python:Backend development focused on building the server-side logic, database interactions, and APIs that power the Peacock Rentals platform.  As discussed earlier, Java and Python could both play significant roles.Java Backend (with Spring Boot):  Java, with the Spring Boot framework, could be used to develop the core backend application.  Spring Boot simplifies Java web development, providing features like dependency injection, auto-configuration, and embedded servers.  Java backend development tasks would include:API Development (RESTful APIs): Creating APIs for vehicle listings, booking management, user authentication, and other core functionalities.
Business Logic Implementation: Coding the business rules and logic for booking validations, pricing calculations, availability management, and more.
Database Interaction: Connecting to the database (PostgreSQL or MySQL) using Java Persistence API (JPA) or Hibernate to manage data persistence.
Security Implementation: Implementing security measures like authentication, authorization, and input validation to protect user data and the application.
Scalability and Performance Tuning: Designing the backend architecture to be scalable and performant, handling concurrent requests efficiently.
Python Backend (with Django or Flask): Python, with frameworks like Django or Flask, could be used for specific backend components or microservices. Python backend development tasks could include:Developing auxiliary APIs: Creating APIs for integrations with third-party services (payment gateways, mapping services).
Building admin panel functionalities: Developing specific modules for the admin panel, leveraging Python's rapid development capabilities.
Data processing and analysis: Implementing backend tasks involving data processing, reporting, or potentially machine learning features using Python's extensive data science libraries.
The backend architecture might involve a microservices approach, with different functionalities (e.g., booking service, vehicle listing service, payment service) implemented as independent services, potentially in different languages (Java, Python, or others as needed), communicating via APIs.  This approach enhances scalability, maintainability, and allows for technology diversification.3.3 Database Design and Management: Structuring Rental Data:Database design is critical for efficient data storage and retrieval.  The database schema for Peacock Rentals would need to accommodate various entities and relationships:Vehicles: Storing vehicle details (make, model, year, description, images, rental rates, availability).
Users: Storing user information (profile details, booking history, contact information).
Bookings: Recording booking details (vehicle, user, dates, times, price, status).
Locations: Managing rental locations and vehicle availability at each location.
Reviews and Ratings: Storing user reviews and ratings for vehicles and the platform.
Database design would involve careful consideration of data types, relationships between entities, indexing strategies for efficient querying, and database normalization to minimize data redundancy and ensure data integrity.  Database management tasks would include setting up database servers, configuring backups, optimizing performance, and ensuring data security.3.4 API Integrations: Connecting to External Services:Peacock Rentals would rely on integrations with various external services through APIs:Payment Gateways (Stripe, PayPal): Integrating with secure payment gateways to process online payments.
Mapping Services (Google Maps API): Integrating mapping services to display vehicle locations, calculate distances, and provide navigation assistance.
Email/SMS Services (Twilio, SendGrid): Integrating services for sending booking confirmations, notifications, and customer communications.
CRM/Marketing Automation Platforms (optional): Potentially integrating with CRM or marketing automation platforms for customer relationship management and marketing campaigns.
API integrations would involve understanding API documentation, implementing API requests and responses, handling authentication, and managing data exchange between Peacock Rentals and external services.Phase 4: Ensuring Quality and Launching the Platform - Testing, Deployment, and LaunchBefore making Peacock Rentals publicly available, rigorous testing was essential to ensure quality, stability, and security.4.1 Quality Assurance (QA) Testing: Identifying and Rectifying Issues:A comprehensive QA testing phase would encompass various types of testing:Functional Testing: Verifying that all functionalities (search, booking, user accounts, admin panel) work as expected.
Usability Testing: Evaluating the user-friendliness and intuitiveness of the website, often involving user testing sessions with representative users.
Performance Testing: Assessing website performance under load, ensuring it can handle concurrent users and traffic spikes.
Security Testing: Identifying and mitigating security vulnerabilities (SQL injection, cross-site scripting, etc.) to protect user data and the platform.
Browser and Device Compatibility Testing: Ensuring the website works correctly and consistently across different browsers (Chrome, Firefox, Safari, Edge) and devices (desktops, tablets, smartphones).
QA testing would involve both manual testing (testers manually interacting with the website) and automated testing (using testing frameworks to automate test cases).  Bug tracking systems would be used to manage identified issues, track their resolution, and ensure thorough regression testing after fixes.4.2 Deployment Strategy and Infrastructure Setup:Deployment involved setting up the server infrastructure and deploying the application code to production servers.  Using a cloud platform (AWS, Google Cloud, Azure) would simplify deployment and infrastructure management.  Deployment strategies could include:Continuous Integration/Continuous Deployment (CI/CD): Implementing a CI/CD pipeline to automate the build, test, and deployment process, enabling frequent and reliable deployments.
Staged Rollouts: Deploying updates gradually, starting with a small subset of users (e.g., beta testers) before rolling out to the entire user base, minimizing risk and allowing for early issue detection.
Load Balancing and Scalability Setup: Configuring load balancers to distribute traffic across multiple servers, ensuring high availability and scalability.
Infrastructure setup would involve configuring servers, databases, networking, and security settings on the chosen cloud platform.  Monitoring tools would be implemented to track server performance, application errors, and user activity post-launch.4.3 Website Launch and Initial Marketing Efforts:Once testing and deployment were complete, Peacock Rentals was ready for launch.  Initial marketing efforts would focus on:Search Engine Optimization (SEO): Optimizing website content and structure for search engines to improve organic search visibility.
Social Media Marketing: Utilizing social media platforms to build brand awareness, engage with potential customers, and drive traffic to the website.
Paid Advertising (Google Ads, Social Media Ads): Running targeted advertising campaigns to reach potential customers actively searching for rental vehicles.
Public Relations and Partnerships: Reaching out to media outlets, travel bloggers, and potential partners to promote Peacock Rentals.
Launch Promotions and Incentives: Offering launch discounts, early bird offers, or special promotions to attract initial users and encourage bookings.
Post-launch, ongoing marketing efforts and website monitoring would be crucial for driving growth and ensuring continued success.Phase 5: Post-Launch Iteration and Continuous ImprovementThe launch of Peacock Rentals was not the end, but rather the beginning of a continuous cycle of iteration and improvement.5.1 Monitoring, Analytics, and User Feedback:Post-launch monitoring and analytics are essential for understanding website performance, user behavior, and identifying areas for improvement.  Key metrics to track include:Website Traffic and User Demographics: Understanding website traffic sources, user demographics, and popular pages.
Conversion Rates (Search to Booking): Measuring the effectiveness of the booking funnel and identifying drop-off points.
User Engagement Metrics (Bounce Rate, Time on Page): Assessing user engagement and identifying areas where users might be encountering difficulties or losing interest.
Error Rates and Performance Metrics: Monitoring application errors, server performance, and website loading times.
Customer Support Tickets and Feedback: Analyzing customer support tickets and feedback to identify common issues and areas for improvement.
Analytics tools like Google Analytics and custom application monitoring dashboards would be used to collect and analyze this data.  User feedback would be actively solicited through feedback forms, surveys, and social media channels.5.2 Iterative Development and Feature Enhancements:Based on data analysis and user feedback, Peacock Rentals would undergo continuous iterative development.  This would involve:Prioritizing Feature Requests and Bug Fixes: Based on user feedback and data analysis, prioritizing bug fixes, usability improvements, and new feature development.
Agile Development Methodology: Adopting an agile development methodology (Scrum, Kanban) to enable rapid iteration, flexible planning, and collaborative development.
A/B Testing and Experimentation: Conducting A/B tests to compare different UI designs, features, or marketing messages, optimizing for conversion rates and user engagement.
Technology Updates and Modernization: Keeping the technology stack up-to-date, adopting new frameworks and libraries as needed, and addressing technical debt to ensure long-term maintainability and performance.
Continuous iteration and improvement are critical for adapting to evolving user needs, staying ahead of the competition, and ensuring the long-term success of Peacock Rentals.Expert Insights on Web Development Trends and Best PracticesTo further enrich our understanding of the Peacock Rentals creation process, let's incorporate expert insights on current web development trends and best practices:Expert 1:  Dr. Anya Sharma, Lead UX/UI Consultant at DesignForward Agency"In today's web landscape, user experience is paramount.  Websites are no longer just about functionality; they are about creating memorable and enjoyable digital journeys.  Peacock Rentals, in focusing on a premium audience, must prioritize a highly polished and intuitive UI.  Micro-interactions, subtle animations, and personalized content are key to engaging users.  Accessibility is not just an afterthought, but a core design principle. Ensuring the website is usable by everyone, regardless of ability, is both ethically sound and expands the potential customer base.  Furthermore, mobile-first is no longer enough; it's mobile-dominant.  The majority of users will interact with Peacock Rentals on their smartphones, so the mobile experience must be flawless."Expert 2:  Mr. Ben Carter, Chief Technology Officer at CloudScale Solutions"From a technical perspective, scalability and security are non-negotiable for a platform like Peacock Rentals.  Choosing a robust backend technology like Java with Spring Boot or a Python framework is a solid foundation.  However, architecture is key. Microservices offer significant advantages in terms of scalability, fault isolation, and technology diversification.  Investing in a well-designed API from the outset will pay dividends in terms of future integrations and feature expansions.  Security must be baked into every stage of development, not bolted on at the end.  Regular security audits, penetration testing, and adherence to security best practices are essential to protect user data and maintain trust.  Finally, data-driven decision making is crucial.  Implementing robust analytics and monitoring tools allows for continuous optimization and informed technology choices."Expert 3:  Ms. Clara Dubois, Digital Marketing Strategist at MarketLeap Digital"A fantastic website is only half the battle; you need to get it in front of the right audience.  For Peacock Rentals, a multi-channel marketing strategy is essential.  SEO will drive organic traffic over time, but paid search and social media advertising can deliver immediate results.  Content marketing, showcasing the unique experiences offered by Peacock Rentals' vehicles, can be highly effective in engaging potential customers.  Social media engagement is vital for building brand communities and fostering customer loyalty.  Personalized marketing, tailoring messaging and offers based on user behavior and preferences, can significantly boost conversion rates.  And don't underestimate the power of email marketing for nurturing leads and driving repeat bookings."Peacock Rentals Product Page Deep Dive: The 1967 Cadillac DeVille Convertible ExperienceLet's examine a specific product page on Peacock Rentals, focusing on the "1967-cadillac-deville-convertible-car-rental-huntington-beach/" listing for the 1967 Cadillac DeVille Convertible.  This page exemplifies many of the best practices discussed above.The page immediately captures attention with high-quality, professionally photographed images of the iconic Cadillac.  The headline is clear and concise, stating the vehicle type and location.  Below the fold, the page provides detailed specifications, including year, make, model, transmission, and key features.  A prominent "Book Now" button is strategically placed, encouraging immediate action.User Experience Enhancements:Image Gallery: A carousel of images showcasing the car from multiple angles, both interior and exterior, allows users to fully appreciate its beauty and condition.
Detailed Description: Engaging and informative text highlights the unique appeal of the 1967 Cadillac DeVille Convertible, emphasizing its classic style, luxurious features, and suitability for special occasions in Huntington Beach.
Availability Calendar: A clear and interactive calendar displays real-time availability, making it easy for users to check dates and plan their rental.
Pricing Transparency: Rental rates are clearly displayed, broken down by day, week, or month, with any applicable fees or deposits clearly outlined.
Location Information: The rental location in Huntington Beach is clearly indicated, with a map integration potentially enhancing usability.
Reviews and Ratings (Potentially): While not explicitly visible on the current page snippet, incorporating user reviews and ratings for vehicles would build trust and social proof, further encouraging bookings.
Call to Action: The prominent "Book Now" button, repeated at various points on the page, is a clear and effective call to action.
Embedding "Peacock Rentals" Anchor Text for Relevancy:Within the description of the 1967 Cadillac DeVille Convertible, a natural embedding of the anchor text "Peacock Rentals" could be implemented to enhance relevancy and internal linking within the website. For example:"Experience the golden age of American automotive luxury with this stunning 1967 Cadillac DeVille Convertible, available for rent from Peacock Rentals in Huntington Beach.  Imagine cruising down the Pacific Coast Highway with the top down, the California sun on your face, behind the wheel of this iconic classic.  Peacock Rentals ensures that every vehicle in our collection is meticulously maintained and presented in pristine condition, promising an unforgettable rental experience."This natural integration of the anchor text "Peacock Rentals" seamlessly connects the specific product page back to the broader brand, enhancing website navigation and reinforcing brand recognition within the user journey.Challenges Faced and Solutions ImplementedThroughout the creation of Peacock Rentals, the team undoubtedly faced various challenges.  These challenges, and the solutions implemented, are crucial learning points:Challenge: Scalability Concerns During Peak Demand:  Anticipating peak demand periods (holidays, weekends) and ensuring the platform can handle increased traffic without performance degradation.Solution: Implemented a microservices architecture hosted on a cloud platform (AWS, Google Cloud, Azure) with autoscaling capabilities. Load balancing was configured to distribute traffic across multiple server instances. Performance testing and optimization were conducted to identify and address bottlenecks.
Challenge: Ensuring Data Security and PCI Compliance:  Handling sensitive user data (personal information, payment details) and meeting PCI DSS compliance standards for secure payment processing.Solution: Implemented robust security measures at every layer of the application (frontend, backend, database, infrastructure). Used HTTPS for all communications, implemented secure authentication and authorization mechanisms, performed regular security audits and penetration testing, and partnered with PCI DSS compliant payment gateways.
Challenge: Building a User-Friendly Booking Process:  Simplifying the booking process and minimizing friction points to encourage conversions.Solution: Conducted extensive usability testing and user feedback sessions to identify pain points in the booking flow. Simplified the booking form, implemented clear call-to-action buttons, provided progress indicators, and offered multiple payment options. Real-time availability updates were implemented to prevent booking conflicts and improve user experience.
Challenge: Managing Vehicle Inventory and Availability:  Keeping vehicle listings accurate, updating availability in real-time, and managing vehicle maintenance schedules.Solution: Developed a comprehensive admin panel for vehicle management, allowing Peacock Rentals staff to easily update vehicle listings, manage availability calendars, and track vehicle maintenance schedules. Integrated the admin panel with the booking system to ensure real-time synchronization of availability.
Challenge: Standing Out in a Competitive Market:  Differentiating Peacock Rentals from existing rental platforms and establishing a unique brand identity.Solution: Focused on a niche market â€“ premium and classic vehicle rentals. Developed a strong brand identity around elegance, luxury, and exceptional customer service. Invested in high-quality photography and engaging content to showcase the unique appeal of the vehicles. Implemented targeted marketing campaigns to reach the desired audience.
Conclusion: Peacock Rentals â€“ A Blueprint for Modern Web DevelopmentThe creation of Peacock Rentals represents a significant undertaking in modern web development.  From meticulous planning and user-centric design to robust backend development and rigorous testing, every phase was crucial to building a successful platform.  The strategic use of technologies like Java and Python for backend development, coupled with modern frontend frameworks and a cloud-based infrastructure, highlights a pragmatic and effective approach to building scalable and performant web applications.The challenges faced and solutions implemented underscore the iterative nature of web development.  Continuous monitoring, data analysis, user feedback, and iterative development are essential for long-term success.  By embracing best practices in UX/UI design, backend architecture, security, and marketing, Peacock Rentals has positioned itself as a compelling player in the online rental market.  This case study serves as a valuable blueprint for aspiring web developers and entrepreneurs looking to build impactful and user-centric digital platforms in today's dynamic environment.  The journey of Peacock Rentals demonstrates that a successful web venture is not just about technology, but about a holistic approach encompassing planning, design, development, and a relentless focus on delivering exceptional user value.]]></content:encoded></item><item><title>Ultimate Guide to Data Science Careers</title><link>https://dev.to/deepikajagdeesh/ultimate-guide-to-data-science-careers-5f35</link><author>Deepika Jagdeesh</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 05:38:04 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Data Science is one of the most sought-after career paths in the digital age, with companies across industries leveraging data to drive decision-making and innovation. Whether you are an aspiring data scientist, a transitioning professional, or a student looking to enter this field, understanding the career landscape is crucial. This guide explores the various career paths in data science, the skills required, job opportunities, and strategies to build a successful career in this dynamic field.
  
  
  Why Choose a Career in Data Science?
: The demand for data professionals is growing rapidly, with organizations seeking skilled individuals to analyse and interpret data.: Data science professionals often receive competitive salaries and benefits due to the specialized skills they bring.3. Diverse Career Opportunities: Data science spans various industries, including finance, healthcare, e-commerce, and technology.: Data science contributes to solving real-world problems, from predicting customer behaviour to improving healthcare outcomes.
  
  
  Key Career Paths in Data Science
Role: Develop machine learning models, analyse complex data sets, and provide actionable insights.Skills Required: Python, SQL, statistics, machine learning, data visualization.Industries: Finance, healthcare, marketing, technology.Role: Interpret data to identify trends, create reports, and assist in business decision-making.Skills Required: Excel, SQL, Python, Tableau/Power BI, statistical analysis.Industries: Retail, consulting, finance, healthcare.3. Machine Learning EngineerRole: Design and implement machine learning algorithms and deploy them in production systems.Skills Required: Deep learning, TensorFlow/PyTorch, big data technologies, software engineering.Industries: Artificial intelligence, self-driving cars, healthcare, e-commerce.Role: Develop, deploy, and optimize AI-powered systems, integrating machine learning models into real-world applications.Skills Required: Machine learning, deep learning, NLP, computer vision, MLOps, cloud computing (AWS/GCP/Azure), software engineering.Industries: Artificial intelligence, robotics, finance, healthcare, cybersecurity, autonomous systems, e-commerce.Role: Build and maintain scalable data pipelines and architectures.Skills Required: SQL, Python, Spark, cloud computing (AWS, Azure, GCP), ETL tools.Industries: Technology, finance, manufacturing, media.6. Business Intelligence (BI) AnalystRole: Use data visualization tools to provide strategic insights for business growth.Skills Required: SQL, Tableau, Power BI, data warehousing.Industries: Consulting, corporate strategy, sales, operations.
  
  
  Essential Skills for a Data Science Career
: Python, SQL2. Data Manipulation & Analysis: Pandas, NumPy, Matplotlib, Seaborn: Scikit-learn, TensorFlow: Tableau, Power BI, Matplotlib: AWS, Azure, Google Cloud: Communication, problem-solving, critical thinking, domain expertise
  
  
  How to Start Your Career in Data Science
: Pursue a degree in computer science, mathematics, or data science. (or)2. Take Online Courses & Certifications: Platforms like Shyam Technologies offer high-quality courses.: Build a strong portfolio with Kaggle competitions and real-world projects.: Internships and freelance projects help in gaining hands-on experience.5. Network & Stay Updated: Join data science communities, attend conferences, and follow industry trends.: Tailor your resume, prepare for interviews, and apply for relevant positions.A career in data science is both exciting and rewarding, offering endless opportunities for growth and innovation. By developing the right skills, gaining hands-on experience, and staying updated with industry trends, you can build a successful and fulfilling career in this ever-evolving field. Whether you are starting fresh or transitioning from another industry, data science presents a world of possibilities waiting to be explore.Looking to kickstart your career in Data Science? Shyam Technologies offers industry-focused training programs in Data Science, Machine Learning, and AI to help you master the skills needed for success. Enroll today and transform your future! ðŸš€]]></content:encoded></item><item><title>Sharing my Open Source Project: Realtime Messaging Platform Built with Go &amp; React (Fullstack)</title><link>https://github.com/JoyalAJohney/Realtime-distributed-chat</link><author>/u/BruceWayn_</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 04:53:18 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Python Morsels: Multiline strings</title><link>https://www.pythonmorsels.com/multi-line-strings/</link><author></author><category>dev</category><category>python</category><pubDate>Fri, 21 Feb 2025 04:48:51 +0000</pubDate><source url="http://planetpython.org/">Planet Python blog</source><content:encoded><![CDATA[Need to represent multiple lines of text in Python? Use Python's multi-line string syntax!A string with line breaks in itHere we have a Python program called  that acts like a timer:It counts upward one second at a time, until we manually exit it by hitting :$ python3 stopwatch.py
 sec
 sec
 sec
 sec
 sec
 sec
^CTraceback most recent call last:
  File , line ,  <module>
    sleep
KeyboardInterrupt
If we run this program with any command-line arguments at all, it prints out a usage statement instead of counting:~ $ python3 stopwatch.py --help
Welcome to stopwatch!
This script counts slowly upward,
one second per tick.

No command-line arguments are accepted.
This usage statement is represented by multiple lines of text in a single string.This string has  characters in it, which are newline characters.Using string concatenation to build up long stringsCurrently our string is ]]></content:encoded></item><item><title>Minecraft from scratch with only modern OpenGL</title><link>https://github.com/GianlucaP106/minecraft</link><author>/u/One_Mess_1093</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 03:32:48 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>N19-Chain</title><link>https://dev.to/inquisitive41/n19-chain-53i</link><author>Inquisitive41</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 02:54:30 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[N19-Chain" is an experimental blockchain prototype using "N19-Crypt", a unique encryption algorithm with the number 19 as the basis for permutation. Which needs to be improved in C++   code ->>  GitHubBitcoin Ethereum Parameter N19-Chain (Python) N19-Chain (C++ complex)
Speed (tx/s) 7 15-30 786,833
Block time 10 min 12 sec 1.27 ms 1.2 ms 1 MB 20-100 KB
Bandwidth 7 KB/sec 0.3â€“3 KB/sec 74.67 KB/sec 77.31 KB/sec
Energy (TWh/year) 100 0.01~0.001 ~0.01]]></content:encoded></item><item><title>Linus Torvalds responds to Christoph Hellwig</title><link>https://lore.kernel.org/rust-for-linux/CAHk-=wgLbz1Bm8QhmJ4dJGSmTuV5w_R0Gwvg5kHrYr4Ko9dUHQ@mail.gmail.com/</link><author>/u/bik1230</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 02:30:34 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ElasticTransform in PyTorch (2)</title><link>https://dev.to/hyperkai/elastictransform-in-pytorch-2-11a1</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 02:04:32 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TIL: 3ï¸âƒ£ ways I use Large Language Models to increase learning efficiency</title><link>https://dev.to/mrzaizai2k/til-3-ways-i-use-large-language-models-to-increase-learning-efficiency-6i5</link><author>Mai Chi Bao</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 02:00:00 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[To Do List Management SystemOnline Courses Summary (In the future)This semester, it's a bit odd that I registered for 5 courses for my master degree at HCMUT along with the job at the bank, so It makes me burn out a bit.Â  I've only been taking the courses for a week and I'm already feeling miserable.It's not until now that I've tried applying LLMs in work and study, but really, my tight schedule recently forces me to optimize everything and only prioritize things that brings the most important valueThere is never enough time to do everything, but there is always enough time to do the most important thing.Â  - Brian Tracy.I often use Microsoft To Do because it is simple and convenient.Â  Normally, just call "Hi Bixby" to create a task from my Samsung phone.Â  Tasks will be synchronized into the Calendar app.Â  However, it handles Vietnamese poorly, cannot add multiple tasks at the same time, does not automatically set the importance level... Although Google Assistant are better at handling speech to text problem, it has the same limitations as above.I decided to go big this time and build my own Telegram chat bot.Â  Every time you send a voice, it will process it automatically:Breaking down the text into multiple tasks.Set due date, reminder date, important level based on my personal information.Very convenient.Â  But the biggest drawback is that I still need to send voice to the app manually, not "Hi Bixby" or "Hey Google", this makes it lose its practicality.Â  I will find a way to fix this in the futureTo do list is integrated into this tele bot:I used Faiss as vector database and Langchain to get text from any sources I could think of (.pdf, .doc, .ppt, .txt, .md, .epub, link, ebook, youtube...).Â  If you ask Chatgpt, it will sometime mislead you or create fake answer. Although the answer might be correct, it doesn't follow the lecturer's documents. This RAG system take the answer from our documents to produce results, so the results are more accurate and we can immediately check its correctness.Â  I also tested on the Software Testing Quizz, the result was about 7/10.Â  I also ask chatgpt, but the answer seems bad.But Questions with the answer "All the answers are correct" are often wrong.Â  Perhaps it's because the model only finds the answer with the highest probability, so the aggregated answers are not fit to this case. Maybe I should rewriting the prompt.Â  Additionally, a small part of the learning resources are images, and I'm actually still looking for ways to optimize the explanation for the images.I just wanna build my own LLM with RAGWelcome to the my LLM with RAG system! This system is designed for me the ease the learning as a master in HCMUT     curl -X POST http://localhost:8083/update
Ask questions with vector data     curl -X POST -H "Content-Type: application/json" -d '{"query": "who is karger"}' http://localhost:8083/query
  nougat data/web_data/Growth_of_Functions.pdf --markdown --no-skipping -m 0.1.0-base -o data/nougat
Before running the system, follow these steps to set up the environment:Close the Git repository to your local machine:
git clone [repository_url]Navigate to the project directory and install the required packages using the provided  file:To read  file we need to run this code  apt update
  apt install libreoffice 
During the pandemic, I realized that I'm best atÂ  learning online courses.Â  I usually watch the video 2 or 3 times, fast forwarding unimportant parts (focusing on the 20% of the time that brings 80% of the value).Â  I plan to record all the lectures, then ask the model to synthesize and track which time period and what topic is being talked about... Or convert it to text format to save in the database for LLM + RAG systemI am convinced that individuals adept at utilizing LLMs can elevate their performance by up to 200%. To stand out among others, it requires a combination of diligent effort and the right tools. Even a marginal improvement of 1% can contribute significantly to your success.]]></content:encoded></item><item><title>Reinforcement Learning with PDEs</title><link>https://towardsdatascience.com/reinforcement-learning-with-pdes/</link><author>Robert Etter</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 01:45:39 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Previously we discussed applying reinforcement learning to Ordinary Differential Equations (ODEs) by integrating ODEs within gymnasium. ODEs are a powerful tool that can describe a wide range of systems but are limited to a single variable. Partial Differential Equations (PDEs) are differential equations involving derivatives of multiple variables that can cover a far broader range and more complex systems. Often, ODEs are special cases or special assumptions applied to PDEs.PDEs include Maxwellâ€™s Equations (governing electricity and magnetism), Navier-Stokes equations (governing fluid flow for aircraft, engines, blood, and other cases), and the Boltzman equation for thermodynamics. PDEs can describe systems such as flexible structures, power grids, manufacturing, or epidemiological models in biology. They can represent highly complex behavior; the Navier Stokes equations describe the eddies of a rushing mountain stream. Their capacity for capturing and revealing more complex behavior of real-world systems makes these equations an important topic for study, both in terms of describing systems and analyzing known equations to make new discoveries about systems. Entire fields (like fluid dynamics, electrodynamics, structural mechanics) can be devoted to study of just a single set of PDEs.This increased complexity comes with a cost; the systems captured by PDEs are much more difficult to analyze and control. ODEs are also described as lumped-parameter systems, the various parameters and variables that describe them are â€œlumpedâ€ into a discrete point (or small number of points for a coupled system of ODEs). PDEs are distributed parameter systems that track behavior throughout space and time. In other words, the state space for an ODE is a relatively small number of variables, such as time and a few system measurements at a specific point. For PDE/distributed parameter systems, the state space size can approach infinite dimensions, or discretized for computation into millions of points .Â A lumped parameter system controls the temperature of an engine based on a small number of sensors. A PDE/distributed parameter system would manage temperature dynamics across the entire engine.Â As with ODEs, many PDEs must be analyzed (aside from special cases) through modelling and simulation. However, due to the higher dimensions, this modelling becomes far more complex. Many ODEs can be solved through straightforward applications of algorithms like MATLABâ€™s ODE45 or SciPyâ€™s .Â PDEs are modelled across grids or meshes where the PDE is simplified to an algebraic equation (such as through Taylor Series expansion) at each point on the grid. Grid generation is a field, a science and art, on its own and ideal (or usable) grids can vary greatly based on problem geometry and Physics. Grids (and hence problem state spaces) can number in the millions of points with computation time running in days or weeks, and PDE solvers are often commercial software costing tens of thousands of dollars.Â Controlling PDEs presents a far greater challenge than ODEs. The Laplace transform that forms the basis of much classical control theory is a one-dimensional transformation. While there has been some progress in PDE control theory, the field is not as comprehensive as for ODE/lumped systems. For PDEs, even basic controllability or observability assessments become difficult as the state space to assess increases by orders of magnitude and fewer PDEs have analytic solutions. By necessity, we run into design questions such as what part of the domain needs to be controlled or observed? Can the rest of the domain be in an arbitrary state?Â What subset of the domain does the controller need to operate over? With key tools in control theory underdeveloped, and new problems presented, applying machine learning has been a major area of research for understanding and controlling PDE systems.Â Given the importance of PDEs, there has been research into developing control strategies for them. For example, Glowinski et. all developed an analytical adjoint based method from advanced functional analysis relying on simulation of the system. Other approaches, such as discussed by Kirsten Morris, apply estimations to reduce the order of the PDE to facilitate more traditional control approaches.Â Botteghi and Fasel, have begun to apply machine learning to control of these systems (note, this is only a VERY BRIEF glimpse of the research). Here we will apply reinforcement learning on two PDE control problems. The diffusion equation is a simple, linear, second order PDE with known analytic solution. The Kuramotoâ€“Sivashinsky (K-S) equation is a much more complex 4 order nonlinear equation that models instabilities in a flame front.Â For both these equations we use a simple, small square domain of grid points. We target a sinusoidal pattern in a target area of a line down the middle of the domain by controlling input along left and right sides. Input parameters for the controls are the values at the target region and the  coordinates of the input control points. Training the algorithm required modelling the system development through time with the control inputs. As discussed above, this requires a grid where the equation is solved at each point then iterated through each time step. I used the py-pde package to create a training environment for the reinforcement learner (thanks to the developer of this package for his prompt feedback and help!). With the  environment, approach proceeded as usual with reinforcement learning:Â the particular algorithm develops a guess at a controller strategy. That controller strategy is applied at small, discrete time steps and provides control inputs based on the current state of the system that lead to some reward (in this case, root mean square difference between target and current distribution).Â Unlike previous cases, I only present results from the genetic-programming controller. I developed code to apply a soft actor critic (SAC) algorithm to execute as a container on AWS Sagemaker. However, full execution would take about 50 hours and I didnâ€™t want to spend the money! I looked for ways to reduce the computation time, but eventually gave up due to time constraints; this article was already taking long enough to get out with my job, military reserve duty, family visits over the holidays, civic and church involvement, and not leaving my wife to take care of our baby boy alone!Â First we will discuss the diffusion equation:with x as a two dimensional cartesian vector and âˆ† Laplace operatorfrom pde import Diffusion, CartesianGrid, ScalarField, DiffusionPDE, pde
grid = pde.CartesianGrid([[0, 1], [0, 1]], [20, 20], periodic=[False, True])
state = ScalarField.random_uniform(grid, 0.0, 0.2)
bc_left={"value": 0}
bc_right={"value": 0}
bc_x=[bc_left, bc_right]
bc_y="periodic"
#bc_x="periodic"
eq = DiffusionPDE(diffusivity=.1, bc=[bc_x, bc_y])
solver=pde.ExplicitSolver(eq, scheme="euler", adaptive = True)
#result = eq.solve(state, t_range=dt, adaptive=True, tracker=None)
stepper=solver.make_stepper(state, dt=1e-3)
target = 1.*np.sin(2*grid.axes_coords[1]*3.14159265)The problem is sensitive to diffusion coefficient and domain size; mismatch between these two results in washing out control inputs before they can reach the target region unless calculated over a long simulation time. The control input was updated and reward evaluated every 0.1 timestep up to an end time of T=15.Â Due to py-pde package architecture, the control is applied to one column inside the boundary. Structuring the py-pde package to execute with the boundary condition updated each time step resulted in a memory leak, and the py-pde developer advised using a stepper function as a work-around that doesnâ€™t allow updating the boundary condition. This means the results arenâ€™t exactly physical, but do display the basic principle of PDE control with reinforcement learning.Â The GP algorithm was able to arrive at a final reward (sum mean square error of all 20 points in the central column) of about 2.0 after about 30 iterations with a 500 tree forest. The results are shown below as target and achieved distributed in the target region.Now the more interesting and complex K-S equation:Unlike the diffusion equation, the K-S equation displays rich dynamics (as befitting an equation describing flame behavior!). Solutions may include stable equilibria or travelling waves, but with increasing domain size all solutions will eventually become chaotic. The PDE implementation is given by below code:grid = pde.CartesianGrid([[0, 10], [0, 10]], [20, 20], periodic=[True, True])
state = ScalarField.random_uniform(grid, 0.0, 0.5)
bc_y="periodic"
bc_x="periodic"
eq = PDE({"u": "-gradient_squared(u) / 2 - laplace(u + laplace(u))"}, bc=[bc_x, bc_y])
solver=pde.ExplicitSolver(eq, scheme="euler", adaptive = True)
stepper=solver.make_stepper(state, dt=1e-3)
target=1.*np.sin(0.25*grid.axes_coords[1]*3.14159265)Control inputs are capped at +/-5.Â The K-S equation is naturally unstable; if any point in the domain exceeds +/- 30 the iteration terminates with a large negative reward for causing the system to diverge. Experiments with the K-S equation in  revealed strong sensitivity to domain size and number of grid points. The equation was run for T=35, both with control and reward update at dt=0.1.For each, the GP algorithm had more trouble arriving at a solution than in the diffusion equation. I chose to manually stop execution when the solution became visually close; again, we are looking for general principles here. For the more complex system, the controller works betterâ€”likely because of how dynamic the K-S equation is the controller is able to have a bigger impact. However, when evaluating the solution for different run times, I found it was not stable; the algorithm learned to arrive at the target distribution at a particular time, not to stabilize at that solution. The algorithm converged to the below solution, but, as the successive time steps show, the solution is unstable and begins to diverge with increasing time steps.Â Careful tuning on the reward function would help obtain a solution that would hold longer, reinforcing how vital correct reward function is. Also, in all these cases we arenâ€™t coming to perfect solutions; but, especially for the K-S equations we are getting decent solutions with comparatively little effort compared to non-RL approaches for tackling these sorts of problems.The GP solution is taking longer to solve with more complex problems and has trouble handling large input variable sets. To use larger input sets, the equations it generates become longer which make it less interpretable and slower to compute.Â Solution equations had scores of terms rather than the dozen or so in ODE systems. Neural network approaches can handle large input variable sets more easily as input variables only directly impact the size of the input layer.Â Further, I suspect that neural networks will be able to handle more complex and larger problems better for reasons discussed previously in previous posts. Because of that, I did develop gymnasiums for py-pde diffusion, which can easily be adapted to other PDEs per the py-pde documentation. These gymnasiums can be used with different NN-based reinforcement learning such as the SAC algorithm I developed (which, as discussed, runs but takes time).Â Adjustments could also be made to the genetic Programming approach. For example, vector representation of inputs could reduce size of solution equations. Duriez et al. all proposes using Laplace transform to introduce derivatives and integrals into the genetic programming equations, broadening the function spaces they can explore.Â The ability to tackle more complex problems is important. As discussed above, PDEs can describe a wide range of complex phenomena. Currently, controlling these systems usually means lumping parameters. Doing so leaves out dynamics and so we end up working against such systems rather than with them. Efforts to control or manage these means higher control effort, missed efficiencies, and increased risk of failure (small or catastrophic). Better understanding and control alternatives for PDE systems could unlock major gains in engineering fields where marginal improvements have been the standard such as traffic, supply chains, and nuclear fusion as these systems behave as high dimensional distributed parameter systems. They are highly complex with nonlinear and emergent phenomena but have large available data setsâ€”ideal for machine learning to move past current barriers in understanding and optimization.Â For now, I have only taken a very basic look at applying ML to controlling PDEs. Follow ons to the control problem include not just different systems, but optimizing where in the domain the control is applied, experimenting with reduced-order observation space, and optimizing the control for simplicity or control effort. In addition to improved control efficiency, as discussed in Brunton and Kutz, machine learning can also be used to derive data-based models of complex physical systems and to determine reduced order models which reduce state space size and may be more amenable to analysis and control, by traditional or machine learning methods. Machine learning and PDEs is an exciting area of research, and I encourage you to see what the professionals are doing!]]></content:encoded></item><item><title>creating your own Docker like what a shiny title and hard work A year ago, I built a minimal container in Go. Now, it&apos;s time for a revisit! This time, I&apos;m tackling network isolation, resource limits, and deeper container architecture.</title><link>https://dev.to/micromax/creating-your-own-docker-like-what-a-shiny-title-and-hard-work-a-year-ago-i-built-a-minimal-53fc</link><author>mohamed alaaeldin</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 01:25:52 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Beyond Basics: Building a More Powerful Container in Go â€” Network Isolation & Advanced Featuresmohamed alaaeldin ãƒ» Feb 21]]></content:encoded></item><item><title>Beyond Basics: Building a More Powerful Container in Go â€” Network Isolation &amp; Advanced Features</title><link>https://dev.to/micromax/beyond-basics-building-a-more-powerful-container-in-go-network-isolation-advanced-features-3674</link><author>mohamed alaaeldin</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 01:21:41 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Containers Uncovered: More Than Just Lightweight Virtual Machines!â€If youâ€™re like me â€” always wondering how things work and eager to build them with your own mind and hands â€” youâ€™re in the right place!
    In the first part of this article (Part 1), I attempted to build a minimal container system using only Go, relying on Linuxâ€™s unshare and namespaces. It was purely a demonstration, and I wasnâ€™t aiming to develop a fully functional container runtime tool. I intentionally left out many critical aspects, such as security, networking, and image management.
    I initially thought it would be simple, but I quickly realized that even a basic container system involves thousands of concepts and implementations. However, my passion for understanding and building things kept me going.
    Now, after a year since my first article on Building a Minimal Container in Go, Iâ€™ve realized that both my code and my original article need a fresh perspective. So, itâ€™s time for a revisit!Responsibilities:
Parse user commands (run, exec, ps, rm)
Communicate with daemon via RPC or any other way
Format and display outputCommand completion
Output formatting (JSON/YAML)
Log streaming
Manage container lifecycle
Maintain container state database
Coordinate between components
REST/gRPC API
Event logging
Resource tracking
Namespace Manager: CLONE_NEW* flags handling and more flags in real world .
Cgroups Manager: Resource constraints
Filesystem Setup: RootFS preparation
OCI runtime spec compliance
User namespace remapping
Seccomp/AppArmor profiles
Registry Client: Docker Hub integration or you own images services if you will go wiled
Layer Manager: OverlayFS/BTRFS
Snapshotter: Copy-on-write layers
Image caching
Signature verification
Garbage collection
CNI Plugins: Bridge, MACVLAN, IPVLAN
IPAM: DHCP/Static allocation
Service Mesh: DNS, service discovery
Multi-host networking
Network policies
Port mapping
Volume Manager: Bind mounts
Snapshot Manager: Incremental backups
Quota Enforcer: Disk limits
Persistent storage
Temporary filesystems
Encryption support
this schema will give you a bigger picture
                           +---------------------+
                           |      User CLI       |
                           | (run, exec, ps, rm) |
                           +----------+----------+
                                      |
                                      | (gRPC/HTTP)
                                      v
                           +---------------------+
                           |   Container Daemon  |
                           | (State Management)  |
                           +----------+----------+
                                      |
                   +------------------+------------------+
                   |                  |                  |
         +----------+----------+ +-----+--------+ +-------+---------+
         |   Container Runtime | | Image Service| | Network Manager |
         | (namespace/cgroups) | | (OCI Images)  | | (CNI Plugins)   |
         +----------+----------+ +-----+--------+ +-------+---------+
                   |                  |                  |
         +---------v---------+ +------v-------+ +--------v---------+
         | Linux Kernel       | | Storage Driver| | Host Networking |
         | - namespaces       | | (OverlayFS)   | | (iptables/bridge)|
         | - cgroups v2       | +---------------+ +------------------+
         | - capabilities     |
         +--------------------+
It has been a long journey for me to learn and think through every component. I encountered many challenges, especially with aspects like OverlayFS and networking.My biggest issue in my first implementation was networking. It was really difficult to isolate the child container and set up its own bridged network.To solve network isolation, you need to think clearly ðŸ¤” at this stage.First, you need to create a bridge on the host with two virtual interfaces:The first interface remains on the host.
The second interface is moved to the child container ðŸ«™.
The real challenge here is managing command signaling between the host and the child container.In my approach, I will attempt to create a proof of concept implementation.
Understanding Container NetworkingWhen we create containers, one of the most crucial aspects is network isolation. Think of it like giving each container its own private network environment, complete with its own network interfaces, IP addresses, and routing rules. Letâ€™s break down how we achieve this in our container implementation.
The Network Setup ProcessCreating the Network NamespaceFirst, we create a separate network namespace for our container. This is like giving the container its own private networking room:const ContainerName = "mycontainer"

func createNetworkNamespace(name string) error {
    // Create directory for network namespaces
    if err := os.MkdirAll("/var/run/netns", 0755); err != nil {
        return err
    }

    // Create the namespace file
    nsFile := filepath.Join("/var/run/netns", name)
    fd, err := os.Create(nsFile)
    if err != nil {
        return err
    }
    fd.Close()

    // Bind mount it to make it accessible
    return syscall.Mount("/proc/self/ns/net", nsFile, "bind", syscall.MS_BIND, "")
}
Setting Up Virtual Network InterfacesWe create a virtual network cable (veth pair) to connect our container to the host system:const (
    VethHost      = "veth0"  // Host end of the cable
    VethContainer = "veth1"  // Container end of the cable
    ContainerIP   = "10.0.0.2/24"
    HostIP        = "10.0.0.1/24"
    Gateway       = "10.0.0.1"
)
The setup happens in two parts:
1-On the host side:func setupHostNetwork(pid int) error {
    // Create the virtual network cable (veth pair)
    if err := exec.Command("ip", "link", "add", VethHost, "type", "veth", 
        "peer", "name", VethContainer).Run(); err != nil {
        return fmt.Errorf("failed to create veth pair: %v", err)
    }

    // Move one end to the container
    if err := exec.Command("ip", "link", "set", VethContainer, 
        "netns", fmt.Sprintf("%d", pid)).Run(); err != nil {
        return fmt.Errorf("failed to move veth to container: %v", err)
    }

    // Configure the host end
    if err := exec.Command("ip", "link", "set", VethHost, "up").Run(); err != nil {
        return fmt.Errorf("failed to bring up host interface: %v", err)
    }
    if err := exec.Command("ip", "addr", "add", HostIP, "dev", VethHost).Run(); err != nil {
        return fmt.Errorf("failed to assign IP to host interface: %v", err)
    }
}
2 â€” Inside the container:func setupContainerNetwork() error {
    // Enable the loopback interface
    if err := exec.Command("ip", "link", "set", "lo", "up").Run(); err != nil {
        return fmt.Errorf("failed to bring up lo: %v", err)
    }

    // Configure the container's network interface
    if err := exec.Command("ip", "link", "set", VethContainer, "up").Run(); err != nil {
        return fmt.Errorf("failed to bring up veth: %v", err)
    }
    if err := exec.Command("ip", "addr", "add", ContainerIP, 
        "dev", VethContainer).Run(); err != nil {
        return fmt.Errorf("failed to assign IP to veth: %v", err)
    }
    if err := exec.Command("ip", "route", "add", "default", 
        "via", Gateway).Run(); err != nil {
        return fmt.Errorf("failed to add default route: %v", err)
    }
}
To allow our container to access the internet, we need to set up NAT (Network Address Translation) rules. This is like setting up a router for our container:func setupHostNetwork(pid int) error {
    // Get the host's internet-connected interface
    iface, err := getDefaultInterface()
    if err != nil {
        return fmt.Errorf("failed to get default interface: %v", err)
    }

    // Set up NAT and forwarding rules
    cmds := [][]string{
        {"sysctl", "-w", "net.ipv4.ip_forward=1"},
        {"iptables", "-t", "nat", "-A", "POSTROUTING", 
            "-s", "10.0.0.0/24", "-o", iface, "-j", "MASQUERADE"},
        {"iptables", "-A", "FORWARD", "-i", iface, 
            "-o", VethHost, "-j", "ACCEPT"},
        {"iptables", "-A", "FORWARD", "-i", VethHost, 
            "-o", iface, "-j", "ACCEPT"},
    }

    for _, cmd := range cmds {
        if out, err := exec.Command(cmd[0], cmd[1:]...).CombinedOutput(); err != nil {
            return fmt.Errorf("failed %v: %s\n%s", cmd, err, out)
        }
    }
}
finally , Resource CleanupOne often overlooked but crucial aspect is cleaning up network resources when the container stops. Our implementation handles this through a ResourceManager:
type ResourceManager struct {
    containerName string
    vethHost      string
    mounts        []string
    namespaces    []string
}

func (rm *ResourceManager) cleanupNetwork() error {
    // Clean up iptables rules
    if err := rm.cleanupIptablesRules(); err != nil {
        log.Printf("Warning: iptables cleanup failed: %v", err)
    }

    // Remove the virtual network interface
    if out, err := exec.Command("ip", "link", "delete", 
        rm.vethHost).CombinedOutput(); err != nil {
        log.Printf("Warning: failed to delete veth interface: %v (%s)", err, out)
    }

    return nil
}
How It All Works TogetherWhen starting a container:

Create a new network namespace
Create virtual network interfaces (veth pair)
Configure IP addresses and routing
Set up NAT for internet access
Mount necessary filesystems and set up devices
2 .During container runtime:Container uses its virtual network interface for all network communication
Outgoing traffic goes through NAT to reach the internet
Incoming traffic is routed back to the container
3 . When stopping a container:Clean up iptables rules
Remove virtual interfaces
Unmount network namespace
Remove namespace files
Common Issues and DebuggingWhen implementing container networking, you might encounter these common issues:DNS Resolution Problems

Our implementation includes DNS setup:
// in most cases this will case error , still trying to solve it 
func setupDNS() error {
    resolvHost := "/etc/resolv.conf"
    resolvContainer := filepath.Join(RootFS, "etc/resolv.conf")
    return syscall.Mount(resolvHost, resolvContainer, "bind", 
        syscall.MS_BIND|syscall.MS_RDONLY, "")
}
2.Network Interface IssuesAlways check interface status with ip link show
Verify IP assignments with ip addr show
Check routing with ip route show
Verify iptables rules are correctly set
Check IP forwarding is enabled
Ensure the host interface is up and working
Our implementation includes several security features:Network Isolation

Each container gets its own network namespace
Network traffic is isolated between containers
Proper cleanup of network resources prevents resource leaks
Automatic cleanup on container exit
This networking implementation provides a solid foundation for container isolation while maintaining internet connectivity. While itâ€™s simpler than production container runtimes, it demonstrates the core concepts of container networking.this was the hard part for me and i have tryed so many implemention to achive that . you have to keep in main what and where your command executted . some times you find your self trying to create vathâ€™s in continer or you cannot connect or move the continer interface from host to chiledyou have to read my previeus articl to know what we are doing i had clean up my code and add every thing agine to test network isolationdo not forget to change RootFS to your root fs like â€œubuntu or whatever image you will runâ€package main

import (
 "fmt"
 "log"
 "os"
 "os/exec"
 "path/filepath"
 "strings"
 "syscall"
 "os/signal" 
)

const (
 ContainerName = "mycontainer"
 VethHost      = "veth0"
 VethContainer = "veth1"
 ContainerIP   = "10.0.0.2/24"
 HostIP        = "10.0.0.1/24"
 Gateway       = "10.0.0.1"
 RootFS        = "/mnt/drive/go-projects/lc-images-regs/ubuntu_fs"
)



type ResourceManager struct {
    containerName string
    vethHost      string
    mounts        []string
    namespaces    []string
}
func NewResourceManager(containerName string) *ResourceManager {
    return &ResourceManager{
        containerName: containerName,
        vethHost:     VethHost,
        mounts: []string{
            "/proc",
            "/dev/pts",
            "/dev",
        },
        namespaces: []string{
            "net",
            "uts",
            "pid",
            "ipc",
        },
    }
}

func (rm *ResourceManager) Setup() {
    // Set up signal handling
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        sig := <-sigChan
        log.Printf("Received signal %v, cleaning up...", sig)
        rm.Cleanup()
        os.Exit(1)
    }()
}

func (rm *ResourceManager) Cleanup() error {
    var errors []string

    // 1. Clean up network resources
    if err := rm.cleanupNetwork(); err != nil {
        errors = append(errors, fmt.Sprintf("network cleanup error: %v", err))
    }

    // 2. Clean up mounts
    if err := rm.cleanupMounts(); err != nil {
        errors = append(errors, fmt.Sprintf("mount cleanup error: %v", err))
    }

    // 3. Clean up namespaces
    if err := rm.cleanupNamespaces(); err != nil {
        errors = append(errors, fmt.Sprintf("namespace cleanup error: %v", err))
    }

    if len(errors) > 0 {
        return fmt.Errorf("cleanup errors: %s", strings.Join(errors, "; "))
    }
    return nil
}

func (rm *ResourceManager) cleanupNetwork() error {
    // Clean up iptables rules first
    if err := rm.cleanupIptablesRules(); err != nil {
        log.Printf("Warning: iptables cleanup failed: %v", err)
    }

    // Clean up veth interfaces
    if out, err := exec.Command("ip", "link", "delete", rm.vethHost).CombinedOutput(); err != nil {
        log.Printf("Warning: failed to delete veth interface: %v (%s)", err, out)
    }

    return nil
}

func (rm *ResourceManager) cleanupIptablesRules() error {
    iface, err := getDefaultInterface()
    if err != nil {
        return fmt.Errorf("failed to get default interface: %v", err)
    }

    rules := [][]string{
        {"iptables", "-D", "FORWARD", "-i", iface, "-o", rm.vethHost, "-j", "ACCEPT"},
        {"iptables", "-D", "FORWARD", "-i", rm.vethHost, "-o", iface, "-j", "ACCEPT"},
        {"iptables", "-t", "nat", "-D", "POSTROUTING", "-s", "10.0.0.0/24", "-o", iface, "-j", "MASQUERADE"},
    }

    for _, rule := range rules {
        if out, err := exec.Command(rule[0], rule[1:]...).CombinedOutput(); err != nil {
            log.Printf("Warning: failed to remove iptables rule: %v (%s)", err, out)
        }
    }

    return nil
}

func (rm *ResourceManager) cleanupMounts() error {
    for _, mount := range rm.mounts {
        mountPath := filepath.Join(RootFS, mount)
        if err := syscall.Unmount(mountPath, syscall.MNT_DETACH); err != nil {
            log.Printf("Warning: failed to unmount %s: %v", mountPath, err)
        }
    }
    return nil
}

func (rm *ResourceManager) cleanupNamespaces() error {
    for _, ns := range rm.namespaces {
        nsPath := filepath.Join("/var/run/netns", rm.containerName)
        if err := syscall.Unmount(nsPath, syscall.MNT_DETACH); err != nil {
            log.Printf("Warning: failed to unmount namespace %s: %v", ns, err)
        }
        if err := os.Remove(nsPath); err != nil {
            log.Printf("Warning: failed to remove namespace file %s: %v", nsPath, err)
        }
    }
    return nil
}

func cleanupExistingResources() error {
 // Cleanup network namespace
 if _, err := os.Stat("/var/run/netns/" + ContainerName); err == nil {
  if err := cleanupNetworkNamespace(ContainerName); err != nil {
   return fmt.Errorf("failed to cleanup existing network namespace: %v", err)
  }
 }

 // Cleanup veth interfaces
 if _, err := exec.Command("ip", "link", "show", VethHost).CombinedOutput(); err == nil {
  if err := exec.Command("ip", "link", "delete", VethHost).Run(); err != nil {
   return fmt.Errorf("failed to delete existing veth interface: %v", err)
  }
 }

 // Cleanup iptables rules
 if err := cleanupIptablesRules(); err != nil {
  return fmt.Errorf("failed to cleanup iptables rules: %v", err)
 }

 return nil
}

func cleanupIptablesRules() error {
 iface, err := getDefaultInterface()
 if err != nil {
  return fmt.Errorf("failed to get default interface: %v", err)
 }

 cmds := [][]string{
  {"iptables", "-D", "FORWARD", "-i", iface, "-o", VethHost, "-j", "ACCEPT"},
  {"iptables", "-D", "FORWARD", "-i", VethHost, "-o", iface, "-j", "ACCEPT"},
  {"iptables", "-t", "nat", "-D", "POSTROUTING", "-s", "10.0.0.0/24", "-o", iface, "-j", "MASQUERADE"},
 }

 for _, cmd := range cmds {
  // Ignore errors since rules might not exist
  exec.Command(cmd[0], cmd[1:]...).Run()
 }

 return nil
}
func getDefaultInterface() (string, error) {
 out, err := exec.Command("ip", "route", "show", "default").CombinedOutput()
 if err != nil {
  return "", err
 }

 fields := strings.Fields(string(out))
 for i, field := range fields {
  if field == "dev" && i+1 < len(fields) {
   return fields[i+1], nil
  }
 }

 return "", fmt.Errorf("no default interface found")
}

func main() {
 if len(os.Args) < 2 {
  log.Fatal("Usage: [run|child] command [args...]")
 }

 switch os.Args[1] {
 case "run":
  run()
 case "child":
  child()
 default:
  log.Fatalf("unknown command: %s", os.Args[1])
 }
}
func setupCgroups(ContainerName string , pid int, cpuShares, memoryLimitMB int) error {
    cgroupBase := "/sys/fs/cgroup"
    containerID := ContainerName // fmt.Sprintf("container_%d", pid)

    // Create CPU cgroup
    cpuPath := filepath.Join(cgroupBase, "cpu", containerID)
    os.MkdirAll(cpuPath, 0755)
    os.WriteFile(filepath.Join(cpuPath, "cpu.shares"), []byte(fmt.Sprintf("%d", cpuShares)), 0644)
    os.WriteFile(filepath.Join(cpuPath, "tasks"), []byte(fmt.Sprintf("%d", pid)), 0644)

    // Create memory cgroup
    memoryPath := filepath.Join(cgroupBase, "memory", containerID)
    os.MkdirAll(memoryPath, 0755)
    os.WriteFile(filepath.Join(memoryPath, "memory.limit_in_bytes"), []byte(fmt.Sprintf("%d", memoryLimitMB*1024*1024)), 0644)
    os.WriteFile(filepath.Join(memoryPath, "tasks"), []byte(fmt.Sprintf("%d", pid)), 0644)


 uidMap := fmt.Sprintf("0 %d 1", os.Getuid())
 gidMap := fmt.Sprintf("0 %d 1", os.Getgid())

 os.WriteFile(fmt.Sprintf("/proc/%d/uid_map", pid), []byte(uidMap), 0644)
 os.WriteFile(fmt.Sprintf("/proc/%d/gid_map", pid), []byte(gidMap), 0644)
    return nil
}
func run() {
 rm := NewResourceManager(ContainerName)
    rm.Setup()
    defer rm.Cleanup()

 if err := cleanupExistingResources(); err != nil {
  log.Printf("Cleanup warning: %v", err)
 }

 // Create network namespace
 if err := createNetworkNamespace(ContainerName); err != nil {
  log.Fatalf("Failed to create network namespace: %v", err)
 }

 // Start container process
 cmd := exec.Command("/proc/self/exe", append([]string{"child"}, os.Args[2:]...)...)
 cmd.Stdin = os.Stdin
 cmd.Stdout = os.Stdout
 cmd.Stderr = os.Stderr
 cmd.SysProcAttr = &syscall.SysProcAttr{
  Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWNET |

  syscall.CLONE_NEWIPC ,


 }

 if err := cmd.Start(); err != nil {
  log.Fatalf("Failed to start container: %v", err)
 }
 pid := cmd.Process.Pid
 if err :=setupCgroups(ContainerName , pid , 512 , 256  ); err != nil { // Example: 512 CPU shares, 256 MB memory limit
  log.Fatalf("Failed to setup cgroups: %v", err)
 }
 // Configure host-side networking
 if err := setupHostNetwork(cmd.Process.Pid); err != nil {
  log.Fatalf("Failed to setup host network: %v", err)
 }

 // Wait for container to exit
 if err := cmd.Wait(); err != nil {
  log.Fatalf("Container failed: %v", err)
 }

 // Cleanup
 if err := cleanupNetworkNamespace(ContainerName); err != nil {
  log.Printf("Failed to cleanup network namespace: %v", err)
 }
}

func child() {
 // Setup container environment
 if err := setupContainer(); err != nil {
  log.Fatalf("Failed to setup container: %v", err)
 }

 // Execute command
 if len(os.Args) < 3 {
  log.Fatal("No command specified")
 }
 cmd := exec.Command(os.Args[2], os.Args[3:]...)
 cmd.Stdin = os.Stdin
 cmd.Stdout = os.Stdout
 cmd.Stderr = os.Stderr
 if err := cmd.Run(); err != nil {
  log.Fatalf("Command failed: %v", err)
 }
}

func createNetworkNamespace(name string) error {
 // Create bind mount for ip netns compatibility
 if err := os.MkdirAll("/var/run/netns", 0755); err != nil {
  return err
 }

 // Create namespace file
 nsFile := filepath.Join("/var/run/netns", name)
 fd, err := os.Create(nsFile)
 if err != nil {
  return err
 }
 fd.Close()

 // Bind mount
 return syscall.Mount("/proc/self/ns/net", nsFile, "bind", syscall.MS_BIND, "")
}

func cleanupNetworkNamespace(name string) error {
 nsFile := filepath.Join("/var/run/netns", name)
 if err := syscall.Unmount(nsFile, 0); err != nil {
  return fmt.Errorf("failed to unmount network namespace: %v", err)
 }
 // Remove the file to complete cleanup.
 if err := os.Remove(nsFile); err != nil {
  return fmt.Errorf("failed to remove namespace file %s: %v", nsFile, err)
 }
 return nil
}


func setupHostNetwork(pid int) error {
 // Get host's default interface
 iface, err := getDefaultInterface()
 if err != nil {
  return fmt.Errorf("failed to get default interface: %v", err)
 }

 // Create veth pair
 if err := exec.Command("ip", "link", "add", VethHost, "type", "veth", "peer", "name", VethContainer).Run(); err != nil {
  return fmt.Errorf("failed to create veth pair: %v", err)
 }

 // Move container end to namespace
 if err := exec.Command("ip", "link", "set", VethContainer, "netns", fmt.Sprintf("%d", pid)).Run(); err != nil {
  return fmt.Errorf("failed to move veth to container: %v", err)
 }

 // Configure host interface
 if err := exec.Command("ip", "link", "set", VethHost, "up").Run(); err != nil {
  return fmt.Errorf("failed to bring up host interface: %v", err)
 }
 if err := exec.Command("ip", "addr", "add", HostIP, "dev", VethHost).Run(); err != nil {
  return fmt.Errorf("failed to assign IP to host interface: %v", err)
 }

 cmds := [][]string{

  {"sysctl", "-w", "net.ipv4.ip_forward=1"},
  {"iptables", "-t", "nat", "-A", "POSTROUTING", "-s", "10.0.0.0/24", "-o", iface, "-j", "MASQUERADE"},
  {"iptables", "-A", "FORWARD", "-i", iface, "-o", VethHost, "-j", "ACCEPT"},
  {"iptables", "-A", "FORWARD", "-i", VethHost, "-o", iface, "-j", "ACCEPT"},
 }

 for _, cmd := range cmds {
  if out, err := exec.Command(cmd[0], cmd[1:]...).CombinedOutput(); err != nil {
   return fmt.Errorf("failed %v: %s\n%s", cmd, err, out)
  }
 }

 return nil
}

func setupContainer() error {
 // Setup root filesystem
 if err := syscall.Chroot(RootFS); err != nil {
  return fmt.Errorf("chroot failed: %v", err)
 }
 if err := os.Chdir("/"); err != nil {
  return fmt.Errorf("chdir failed: %v", err)
 }

 // Mount proc
 if err := syscall.Mount("proc", "/proc", "proc", 0, ""); err != nil {
  return fmt.Errorf("failed to mount proc: %v", err)
 }

 // Setup devices
 if err := setupDevices(); err != nil {
  return fmt.Errorf("failed to setup devices: %v", err)
 }

 // Configure network
 if err := setupContainerNetwork(); err != nil {
  return fmt.Errorf("failed to setup network: %v", err)
 }

 //if err := setupDNS(); err != nil {
 // return fmt.Errorf("DNS setup failed: %v", err)
 //}

 return nil
}

func setupDNS() error {
 // Copy host's resolv.conf
 resolvHost := "/etc/resolv.conf"
 resolvContainer := filepath.Join(RootFS, "etc/resolv.conf")

 // Create container's /etc if missing
 if err := os.MkdirAll(filepath.Join(RootFS, "etc"), 0755); err != nil {
  return err
 }

 // Bind mount host's resolv.conf
 return syscall.Mount(resolvHost, resolvContainer, "bind", syscall.MS_BIND|syscall.MS_RDONLY, "")
}

func setupDevices() error {
 // Mount tmpfs for /dev
 if err := syscall.Mount("tmpfs", "/dev", "tmpfs", 0, "size=64k,mode=755"); err != nil {
  return err
 }

 // Create /dev/pts directory if missing
 devPts := "/dev/pts"
 if err := os.MkdirAll(devPts, 0755); err != nil {
  return fmt.Errorf("mkdir %s failed: %v", devPts, err)
 }

 // Mount devpts on /dev/pts for pty support
 if err := syscall.Mount("devpts", devPts, "devpts", 0, "mode=0620,ptmxmode=0666"); err != nil {
  return fmt.Errorf("failed to mount devpts on %s: %v", devPts, err)
 }
 // Create basic devices
 devices := []struct {
  name  string
  major uint32
  minor uint32
 }{
  {"null", 1, 3},
  {"zero", 1, 5},
  {"random", 1, 8},
  {"urandom", 1, 9},
 }

 for _, dev := range devices {
  path := filepath.Join("/dev", dev.name)
  if err := syscall.Mknod(path, syscall.S_IFCHR|0666, int(makedev(dev.major, dev.minor))); err != nil {
   return err
  }
 }

 return nil
}

func makedev(major, minor uint32) uint64 {
 return (uint64(major) << 8) | uint64(minor)
}

func setupContainerNetwork() error {
 // Bring up loopback
 if err := exec.Command("ip", "link", "set", "lo", "up").Run(); err != nil {
  return fmt.Errorf("failed to bring up lo: %v", err)
 }

 // Configure veth interface
 if err := exec.Command("ip", "link", "set", VethContainer, "up").Run(); err != nil {
  return fmt.Errorf("failed to bring up veth: %v", err)
 }
 if err := exec.Command("ip", "addr", "add", ContainerIP, "dev", VethContainer).Run(); err != nil {
  return fmt.Errorf("failed to assign IP to veth: %v", err)
 }
 if err := exec.Command("ip", "route", "add", "default", "via", Gateway).Run(); err != nil {
  return fmt.Errorf("failed to add default route: %v", err)
 }

 return nil
}
Important point: You must mount and create essential virtual devices and establish communication (such as pipes or signals) between the host and child container .func setupDevices() error {
 // Mount tmpfs for /dev
 if err := syscall.Mount("tmpfs", "/dev", "tmpfs", 0, "size=64k,mode=755"); err != nil {
  return err
 }

 // Create /dev/pts directory if missing
 devPts := "/dev/pts"
 if err := os.MkdirAll(devPts, 0755); err != nil {
  return fmt.Errorf("mkdir %s failed: %v", devPts, err)
 }

 // Mount devpts on /dev/pts for pty support
 if err := syscall.Mount("devpts", devPts, "devpts", 0, "mode=0620,ptmxmode=0666"); err != nil {
  return fmt.Errorf("failed to mount devpts on %s: %v", devPts, err)
 }
 // Create basic devices
 devices := []struct {
  name  string
  major uint32
  minor uint32
 }{
  {"null", 1, 3},
  {"zero", 1, 5},
  {"random", 1, 8},
  {"urandom", 1, 9},
 }

 for _, dev := range devices {
  path := filepath.Join("/dev", dev.name)
  if err := syscall.Mknod(path, syscall.S_IFCHR|0666, int(makedev(dev.major, dev.minor))); err != nil {
   return err
  }
 }

 return nil
}
func NewResourceManager(containerName string) *ResourceManager {
    return &ResourceManager{
        containerName: containerName,
        vethHost:     VethHost,
        mounts: []string{
            "/proc",
            "/dev/pts",
            "/dev",
        },
        namespaces: []string{
            "net",
            "uts",
            "pid",
            "ipc",
        },
    }
}

func (rm *ResourceManager) Setup() {
    // Set up signal handling
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        sig := <-sigChan
        log.Printf("Received signal %v, cleaning up...", sig)
        rm.Cleanup()
        os.Exit(1)
    }()
}
Now you have a broad overview, but you still have a long journey ahead to achieve what production-ready container runtime systems offer.If you need system file images to test your code, you can use Docker to download one.$ docker run -d --rm --name ubuntu_fs ubuntu:20.04 sleep 1000
$ mkdir -p ./ubuntu_fs
$ docker cp ubuntu_fs:/ ./ubuntu_fs
$ docker stop ubuntu_fs
Or use tool like debootstrapsudo apt-get update
sudo apt-get install debootstrap
sudo mkdir -p /path/to/rootfs
sudo debootstrap stable /path/to/rootfs http://deb.debian.org/debian
Sometimes, while testing, you may need to install software in your container image from the host if your child container struggles to access the internet.sudo chroot /path/to/rootfs /bin/sh -c "apk add --no-cache iproute2"
sudo chroot /mnt/drive/go-projects/lc-images-regs/ubuntu_fs /bin/sh -c "apt-get update && apt-get install -y iproute2"
Note: Sometimes, when you try to start the container by running the following command to start Bash as the entry point, you may encounter a bug:sudo go run Main.go run sudo /bin/bash
2025/02/21 00:26:28 Failed to setup container: failed to setup network: failed to bring up veth: exit status 1
2025/02/21 01:26:28 Failed to setup host network: failed to assign IP to host interface: exit status 1
exit status 1


This happens due to resource cleanup errors. You can either ignore it and retry the command up to three times or fix the issue.You still need to implement DNS to align with the original system design. What we built is just a proof of concept application.My next step is to ensure resource limitations and create an image composer like Docker while utilizing OverlayFS. Until then, if you need any help, feel free to DM me.this is discord channel for this topic only join me :]]></content:encoded></item><item><title>Creating a Minimal Container in Go: A Step-by-Step Guide ( part 1 )</title><link>https://dev.to/micromax/creating-a-minimal-container-in-go-a-step-by-step-guide-283b</link><author>mohamed alaaeldin</author><category>dev</category><category>go</category><category>devto</category><pubDate>Fri, 21 Feb 2025 01:08:37 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[What is Containers any way!
Containers are lightweight, portable, and efficient, making them a popular choice for deploying and running applications. In this tutorial, weâ€™ll guide you through the process of creating a minimal container using Go. The example code provided focuses on essential containerization concepts, including namespaces, chroot, and control groups (cgroups).Before getting started, ensure you have the following installed:Go programming language: Install Go
Basic understanding of Linux namespaces and control groups
introduction
So what is Linux namespaces and control groups ?Namespaces have been part of the Linux kernel since about 2002, and over time more tooling and namespace types have been added. Real container support was added to the Linux kernel only in 2013, however. This is what made namespaces really useful and brought them to the masses.But what are namespaces exactly? Hereâ€™s a wordy definition from Wikipedia:â€œNamespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources.â€In other words, the key feature of namespaces is that they isolate processes from each other. On a server where you are running many different services, isolating each service and its associated processes from other services means that there is a smaller blast radius for changes, as well as a smaller footprint for securityâ€‘related concerns. Mostly though, isolating services meets the architectural style of microservices as described by Martin Fowler.
Types of NamespacesWithin the Linux kernel, there are different types of namespaces. Each namespace has its own unique properties:A user namespace has its own set of user IDs and group IDs for assignment to processes. In particular, this means that a process can have root privilege within its user namespace without having it in other user namespaces.
A process ID (PID) namespace assigns a set of PIDs to processes that are independent from the set of PIDs in other namespaces. The first process created in a new namespace has PID 1 and child processes are assigned subsequent PIDs. If a child process is created with its own PID namespace, it has PID 1 in that namespace as well as its PID in the parent processâ€™ namespace. See below for an example.
A network namespace has an independent network stack: its own private routing table, set of IP addresses, socket listing, connection tracking table, firewall, and other networkâ€‘related resources.
A mount namespace has an independent list of mount points seen by the processes in the namespace. This means that you can mount and unmount filesystems in a mount namespace without affecting the host filesystem.
An interprocess communication (IPC) namespace has its own IPC resources, for example POSIX message queues.
A UNIX Timeâ€‘Sharing (UTS) namespace allows a single system to appear to have different host and domain names to different processes.

the container are fast isolated environment , we will focus on this part many things are involved and my main goal is to Demystifying Containers

assuming that you are on a linux machine (try Power shell Ubuntu image if you are on Windows :-)
host-machine $ id

uid=1000(mohamed) gid=1000(mohamed) groups=1000(mohamed) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c.1023
Now I run the following unshare command to create a new namespace with its own user and PID namespaces. I map the root user to the new namespace (in other words, I have root privilege within the new namespace), mount a new proc filesystem, and fork my process (in this case, bash) in the newly created namespace.unshare --user --pid --map-root-user --mount-proc --fork bashCongratulation , you are in isolated name space and some how you are on
isolated PID in same file system and same network , your entry point /bin/bashThe ps -ef command shows there are two processes running â€“ bash and the ps command itself â€“ and the id command confirms that Iâ€™m root in the new namespace (which is also indicated by the changed command prompt):root # ps -ef
UID         PID     PPID  C STIME TTY        TIME CMD
root          1        0  0 14:46 pts/0  00:00:00 bash
root         15        1  0 14:46 pts/0  00:00:00 ps -ef
root # id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c.1023
Namespaces and ContainersNamespaces are one of the technologies that containers are built on, used to enforce segregation of resources. Weâ€™ve shown how to create namespaces manually, but container runtimes like Docker, rkt, podman , runC , containerD , and many other container technology
    one of most unique projects are https://katacontainers.io/ they claim that they are mix between container and VMâ€™s .
What Are cgroups?cgroups, or control groups, are a Linux kernel feature that enables the management and limitation of system resources like CPU, memory, and network bandwidth, among others. We can use cgroups to set limits on these resources and distribute them among different groups of processes.cgroups have a hierarchical structure with root and child, each with resource limits set by controllers â€” for example, a CPU controller for CPU time or a memory controller for memory.We can use cgroups for various purposes, such as controlling resource usage in a multi-tenant environment, providing Quality of Service (QoS) guarantees, and running containers.Cgroups provide the following features:Resource limits â€” You can configure a cgroup to limit how much of a particular resource (memory or CPU, for example) a process can use.
Prioritization â€” You can control how much of a resource (CPU, disk, or network) a process can use compared to processes in another cgroup when there is resource contention.
Accounting â€” Resource limits are monitored and reported at the cgroup level.
Control â€” You can change the status (frozen, stopped, or restarted) of all processes in a cgroup with a single command.
The following command creates a v1 cgroup (you can tell by pathname format) called foo and sets the memory limit for it to 50,000,000 bytes (50 MB).root # mkdir -p /sys/fs/cgroup/memory/foo
root # echo 50000000 > /sys/fs/cgroup/memory/foo/memory.limit_in_bytes
Now I can assign a process to the cgroup, thus imposing the cgroupâ€™s memory limit on it. Iâ€™ve written a shell script called test.sh, which prints cgroup testing tool to the screen, and then waits doing nothing. For my purposes, it is a process that continues to run until I stop it.I start test.sh in the background and its PID is reported as 2428. The script produces its output and then I assign the process to the cgroup by piping its PID into the cgroup file /sys/fs/cgroup/memory/foo/cgroup.procs.root # ./test.sh &
[1] 2428
root # cgroup testing tool
root # echo 2428 > /sys/fs/cgroup/memory/foo/cgroup.procs
To validate that my process is in fact subject to the memory limits that I defined for cgroup foo, I run the following ps command. The -o cgroup flag displays the cgroups to which the specified process (2428) belongs. The output confirms that its memory cgroup is foo.root # ps -o cgroup 2428
CGROUP
12:pids:/user.slice/user-0.slice/\
session-13.scope,10:devices:/user.slice,6:memory:/foo,...

By default, the operating system terminates a process when it exceeds a resource limit defined by its cgroup.and this fair amount of information about namespace and cgroup
you can read full doc about it by Scott van Kalken of F5
at this link , also this post Demystifying Containers 101 and this one focus on Docker ecosystem â€œA Beginner-Friendly Introduction to Containers, VMs and Dockerâ€
part 1 : Chroot
i will not use Namespaces , â€œat this partâ€this may surprise however i will achieve the isolation , we will use Chroot a simple UNIX toolchroot, short for "change root," is a Unix system call that changes the root directory of a process to a specified path, effectively creating a new root filesystem for the process and its children. This can be a powerful tool for creating isolated environments or "chroot jails."
How Chroot Works:Setting a New Root Directory: When you execute the chroot system call or the chroot command in the shell, it changes the root directory for the process and its children. The new root directory becomes the / (root) directory for that process, isolating it from the actual root directory of the host system.
Isolation: After the chroot operation, the process and its children can only access files and directories within the new root directory. They cannot access files outside this new root, providing a level of isolation and containment.
System Recovery: chroot is commonly used in system recovery scenarios. If your system becomes unbootable or experiences issues, you can boot from a live CD/USB, chroot into the broken system, and make necessary repairs without affecting the rest of the host system.
Environment Isolation: Developers and system administrators may use chroot to create isolated environments for testing or building software. This is especially common in scenarios where different versions of libraries or dependencies are required.
Security: Although chroot provides some level of isolation, it's not foolproof in terms of security. It was not designed as a security feature and should not be solely relied upon for containing malicious processes. Modern containerization technologies, like Docker, utilize more advanced mechanisms, such as Linux namespaces and cgroups, to provide stronger isolation.
Consider the following example:
mkdir mychroot
cp -r /bin /lib /lib64 /usr /mychroot
chroot /mychroot /bin/bash
We create a directory called mychroot and copy essential binaries and libraries into it.
We use chroot to change the root directory to /mychroot.
After the chroot command, executing /bin/bash will run a Bash shell within the isolated environment.
Keep in mind that chroot by itself does not provide complete isolation; it is often used in conjunction with other tools and techniques to create more secure and robust containerized environments.
Prepare the Ubuntu Root Filesystemnow final this you will need before you start a filesystem .
we will use Docker to download Ubuntu filesystemyou will only need docker to download it , in your project root$ docker run -d --rm --name ubuntu_fs ubuntu:20.04 sleep 1000
$ mkdir -p ./ubuntu_fs
$ docker cp ubuntu_fs:/ ./ubuntu_fs
$ docker stop ubuntu_fs
now we have ubuntu_fs inside our project , inside your main packagepackage main

import (
 "io/ioutil"
 "log"
 "os"
 "os/exec"
 "path/filepath"
 "strconv"
 "syscall"
 "strings"
 "fmt"
 "github.com/vishvananda/netns"

)



func main() {
 switch os.Args[1] {
 case "run":
  run(os.Args[2:]...)
 case "child":
  child(os.Args[2:]...)
 default:
  log.Fatal("Unknown command. Use run <command_name>, like `run /bin/bash` or `run echo hello`")
 }
}



func run(command ...string) {

 log.Println("Executing", command, "from run")
 cmd := exec.Command("/proc/self/exe", append([]string{"child"}, command[0:]...)...)
 cmd.Stdin = os.Stdin
 cmd.Stdout = os.Stdout
 cmd.Stderr = os.Stderr

 // Cloneflags is only available in Linux
 // CLONE_NEWUTS namespace isolates hostname
 // CLONE_NEWPID namespace isolates processes
 // CLONE_NEWNS namespace isolates mounts
 cmd.SysProcAttr = &syscall.SysProcAttr{
  Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS ,
  Unshareflags: syscall.CLONE_NEWNS | syscall.CLONE_NEWNET, 
 }

 // Run child using namespaces. The command provided will be executed inside that.
  must(cmd.Run())
}




func child(command ...string) {

 // Create cgroup
 cg()





 cmd := exec.Command(command[0], command[1:]...)

 cmd.Stdin = os.Stdin
 cmd.Stdout = os.Stdout
 cmd.Stderr = os.Stderr


 must(syscall.Sethostname([]byte("container")))


 must(syscall.Chroot("./ubuntu_fs"))
 // Change directory after chroot
 must(os.Chdir("/"))
 // Mount /proc inside container so that `ps` command works
 must(syscall.Mount("proc", "proc", "proc", 0, ""))
 // Mount a temporary filesystem
 if _, err := os.Stat("mytemp"); os.IsNotExist(err) {
  must(os.Mkdir("mytemp", os.ModePerm))
 }
 must(syscall.Mount("something", "mytemp", "tmpfs", 0, ""))




 must(cmd.Run())

 // Cleanup mount
 must(syscall.Unmount("proc", 0))
 must(syscall.Unmount("mytemp", 0))
}




func cg() {
 // cgroup location in Ubuntu
 cgroups := "/sys/fs/cgroup/"

 pids := filepath.Join(cgroups, "pids")
 containers_mini := filepath.Join(pids, "containers_mini")
 os.Mkdir(containers_mini, 0755)
 // Limit to max 20 pids
 must(ioutil.WriteFile(filepath.Join(containers_mini, "pids.max"), []byte("20"), 0700))
 // Cleanup cgroup when it is not being used
 must(ioutil.WriteFile(filepath.Join(containers_mini, "notify_on_release"), []byte("1"), 0700))

 pid := strconv.Itoa(os.Getpid())
 // Apply this and any child process in this cgroup
 must(ioutil.WriteFile(filepath.Join(containers_mini, "cgroup.procs"), []byte(pid), 0700))
}

func must(err error) {
 if err != nil {
  log.Printf("Error: %v\n", err)
   panic(err)
 }
}
this code introduced by Liz RiceUnderstanding the CodeThe main function serves as the entry point of the program. It uses command-line arguments to determine whether to run a new container or act as a child process within an existing container.func main() {
    switch os.Args[1] {
    case "run":
        run(os.Args[2:]...)
    case "child":
        child(os.Args[2:]...)
    default:
        log.Fatal("Unknown command. Use run <command_name>, like `run /bin/bash` or `run echo hello`")
    }
}

The run function sets up the container environment and executes a specified command inside it.func run(command ...string) {
   log.Println("Executing", command, "from run")
   cmd := exec.Command("/proc/self/exe", append([]string{"child"}, command[0:]...)...)
   cmd.Stdin = os.Stdin
   cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &syscall.SysProcAttr{
        Cloneflags:    syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS,
        Unshareflags:  syscall.CLONE_NEWNS | syscall.CLONE_NEWNET,
    }
}this command cmd := exec.Command(â€œ/proc/self/exeâ€, append([]string{â€œchildâ€}, command[0:]â€¦)â€¦)
make sure that itâ€™s append all command to same process
The Cloneflags specify the namespaces to be isolated (UTS, PID, and mount namespaces).
The Unshareflags further isolate the network namespace.
The cmd.Run() method runs the provided command within the created container.
The child function is responsible for setting up the container filesystem and executing the specified command inside it.func child(command ...string) {
    // ...
    cg()
    must(syscall.Sethostname([]byte("container")))
    must(syscall.Chroot("./ubuntu_fs"))
    must(os.Chdir("/"))
    must(syscall.Mount("proc", "proc", "proc", 0, ""))
    must(syscall.Mount("something", "mytemp", "tmpfs", 0, ""))
    must(cmd.Run())
    must(syscall.Unmount("proc", 0))
    must(syscall.Unmount("mytemp", 0))
}
The cg function sets up a control group (cgroup) to limit resource usage for the container.
Sethostname sets the hostname inside the container.
Chroot changes the root directory for the container.
Mount is used to mount essential filesystems like /proc and a temporary filesystem.
Finally, the command is executed within the container.
The cg function creates and configures a cgroup for the container, limiting the number of processes.func cg() {
 // cgroup location in Ubuntu
 cgroups := "/sys/fs/cgroup/"

 pids := filepath.Join(cgroups, "pids")
 containers_mini := filepath.Join(pids, "containers_mini")
 os.Mkdir(containers_mini, 0755)
 // Limit to max 20 pids
 must(ioutil.WriteFile(filepath.Join(containers_mini, "pids.max"), []byte("20"), 0700))
 // Cleanup cgroup when it is not being used
 must(ioutil.WriteFile(filepath.Join(containers_mini, "notify_on_release"), []byte("1"), 0700))

 pid := strconv.Itoa(os.Getpid())
 // Apply this and any child process in this cgroup
 must(ioutil.WriteFile(filepath.Join(containers_mini, "cgroup.procs"), []byte(pid), 0700))
}

Cgroups are used to control and limit resource usage for processes.
In this example, the cgroup limits the maximum number of processes to 20.
The must function is a simple utility function for handling errors.func must(err error) {
    if err != nil {
        log.Printf("Error: %v\n", err)
        panic(err)
    }
}
If an error occurs, it is logged, and the program is terminated.
Building and Running the ContainerTo run the minimal container, follow these steps:Build the executable: go build -o mycontainer main.go
Create a filesystem directory with an Ubuntu root filesystem, e.g., ubuntu_fs.
Run the container: sudo ./mycontainer run /bin/bash
remember you need to run it as sudo
your entry point is /bin/bash
now you are in your own minimal container , and now you have a deep understanding , may be if i have more time in the future i will add isolation layer on network , our you can do it , thank you for your time i hopped it helped anyone .read this will help you more

namespace & golang a series of article explains namespace with go examples

â€œCreating Network Stacks and Connecting with the Internetâ€ by â€œShrikanta Mazumderâ€
on next part we will create a network layer that give our container a virtual Ethernet in isolated subset that use host bridge as gateway . see you soon]]></content:encoded></item><item><title>ElasticTransform in PyTorch (1)</title><link>https://dev.to/hyperkai/elastictransform-in-pytorch-1-56fc</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 01:04:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can do morphological transformation.It's the magnitude of displacements .A tuple/list must be the 1D with 1 or 2 elements.A single value(,  or /( or )) means .The 2nd argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It's the smoothness of displacements .A tuple/list must be the 1D with 1 or 2 elements.A single value(,  or /( or )) means .The 3rd argument for initialization is (Optional-Default:InterpolationMode.BILINEAR-Type:InterpolationMode).The 4th argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can change the background of an image. *The background can be seen when doing morphological transformation for an image.A tuple/list must be the 1D with 1 or 3 elements.The 1st argument is (Required-Type: or ()):
*Memos:

]]></content:encoded></item><item><title>I developed an Ecommerce App with Django</title><link>https://dev.to/nlansong/i-developed-an-ecommerce-app-with-django-4208</link><author>nlansong</author><category>dev</category><category>python</category><category>devto</category><pubDate>Fri, 21 Feb 2025 00:44:49 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Tech Stack: Django (Backend & Frontend)Glasc Phones is an eCommerce website built using Django for both the backend and frontend, designed for selling mobile phones. The platform provides a seamless shopping experience with key features such as:User Authentication â€“ Secure registration and login system.
Cart System â€“ Users can add, update, and remove items before checkout.
Payment Gateway Integration â€“ Secure online transactions.
Messaging System â€“ Customer inquiries and support messaging.
Full-Stack Development: Built both the backend and frontend with Django, handling data flow, UI rendering, and business logic.
Authentication & Security: Implemented user authentication, session management, and security best practices.
ECommerce Development: Developed features like product listing, cart management, and checkout.
Payment Integration: Integrated a secure payment gateway to facilitate transactions.
Database Management: Designed and optimized relational database models using Django ORM.
Messaging & Notifications: Implemented a messaging system for customer support.
Problem-Solving & Scalability: Addressed challenges like data consistency, performance optimization, and a smooth user experience.
This project has strengthened my expertise in Django-based web applications, enhancing my ability to build secure, scalable, and user-friendly eCommerce platforms]]></content:encoded></item><item><title>How to Use an LLM-Powered Boilerplate for Building Your Own Node.js API</title><link>https://towardsdatascience.com/how-to-use-an-llm-powered-boilerplate-for-building-your-own-node-js-api/</link><author>Uladzimir Yancharuk</author><category>dev</category><category>ai</category><pubDate>Fri, 21 Feb 2025 00:15:23 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[For a long time, one of the common ways to start new Node.js projects was using boilerplate templates. These templates help developers reuse familiar code structures and implement standard features, such as access to cloud file storage. With the latest developments in LLM, project boilerplates appear to be more useful than ever.Building on this progress, Iâ€™ve extended my existing Node.js API boilerplate with a new toolÂ LLM Codegen. This standalone feature enables the boilerplate to automatically generate module code for any purpose based on text descriptions. The generated module comes complete with E2E tests, database migrations, seed data, and necessary business logic.I initially created aÂ GitHub repositoryÂ for a Node.js API boilerplate to consolidate the best practices Iâ€™ve developed over the years. Much of the implementation is based on code from a real Node.js API running in production on AWS.I am passionate about vertical slicing architecture and Clean Code principles to keep the codebase maintainable and clean. With recent advancements in LLM, particularly its support for large contexts and its ability to generate high-quality code, I decided to experiment with generating clean TypeScript code based on my boilerplate. This boilerplate follows specific structures and patterns that I believe are of high quality. The key question was whether the generated code would follow the same patterns and structure. Based on my findings, it does.To recap, hereâ€™s a quick highlight of the Node.js API boilerplateâ€™s key features:Vertical slicing architecture based onÂ Â &Â Â principlesServices input validation usingÂ Decoupling application components with dependency injection ()Integration andÂ Â testing with SupertestMulti-service setup usingÂ composeOver the past month, Iâ€™ve spent my weekends formalizing the solution and implementing the necessary code-generation logic. Below, Iâ€™ll share the details.Letâ€™s explore the specifics of the implementation. All Code Generation logic is organized at the project root level, inside theÂ Â folder, ensuring easy navigation. The Node.js boilerplate code has no dependency onÂ , so it can be used as a regular template without modification.It covers the following use cases:Generating clean, well-structured code for new module based on input description. The generated module becomes part of the Node.js REST API application.Creating database migrations and extending seed scripts with basic data for the new module.Generating and fixing E2E tests for the new code and ensuring all tests pass.The generated code after the first stage is clean and adheres to vertical slicing architecture principles. It includes only the necessary business logic for CRUD operations. Compared to other code generation approaches, it produces clean, maintainable, and compilable code with valid E2E tests.The second use case involves generating DB migration with the appropriate schema and updating the seed script with the necessary data. This task is particularly well-suited for LLM, which handles it exceptionally well.The final use case is generating E2E tests, which help confirm that the generated code works correctly. During the running of E2E tests, an SQLite3 database is used for migrations and seeds.Mainly supported LLM clients are OpenAI and Claude.To get started, navigate to the root folderÂ Â and install all dependencies by running:Â does not rely on Docker or any other heavy third-party dependencies, making setup and execution easy and straightforward. Before running the tool, ensure that you set at least oneÂ Â environment variable in theÂ Â file with the appropriate API key for your chosen LLM provider. All supported environment variables are listed in theÂ Â file (OPENAI_API_KEY, CLAUDE_API_KEYÂ etc.) You can useÂ ,Â , orÂ . As of mid-December,Â Â is surprisingly free to use. Itâ€™s possible to registerÂ hereÂ and obtain a token for free usage. However, the output quality of this free LLaMA model could be improved, as most of the generated code fails to pass the compilation stage.To startÂ , run the following command:Next, youâ€™ll be asked to input the module description and name. In the module description, you can specify all necessary requirements, such as entity attributes and required operations. The core remaining work is performed by micro-agents:Â ,Â , andÂ .Here is an example of a successful code generation:Below is another example demonstrating how a compilation error was fixed:The following is an example of a generatedÂ Â module code:A key detail is that you can generate code step by step, starting with one module and adding others until all required APIs are complete. This approach allows you to generate code for all required modules in just a few command runs.As mentioned earlier, all work is performed by those micro-agents:Â ,Â Â andÂ , controlled by theÂ . They run in the listed order, with theÂ Â generating most of the codebase. After each code generation step, a check is performed for missing files based on their roles (e.g., routes, controllers, services). If any files are missing, a new code generation attempt is made, including instructions in the prompt about the missing files and examples for each role. Once theÂ Â completes its work, TypeScript compilation begins. If any errors are found, theÂ Â takes over, passing the errors to the prompt and waiting for the corrected code. Finally, when the compilation succeeds, E2E tests are run. Whenever a test fails, theÂ Â steps in with specific prompt instructions, ensuring all tests pass and the code stays clean.All micro-agents are derived from theÂ Â class and actively reuse its base method implementations. Here is theÂ Â implementation for reference:Each agent utilizes its specific prompt. Check out this GitHubÂ linkÂ for the prompt used by theÂ .After dedicating significant effort to research and testing, I refined the prompts for all micro-agents, resulting in clean, well-structured code with very few issues.During the development and testing, it was used with various module descriptions, ranging from simple to highly detailed. Here are a few examples:- The module responsible for library book management must handle endpoints for CRUD operations on books.
- The module responsible for the orders management. It must provide CRUD operations for handling customer orders. Users can create new orders, read order details, update order statuses or information, and delete orders that are canceled or completed. Order must have next attributes: name, status, placed source, description, image url
- Asset Management System with an "Assets" module offering CRUD operations for company assets. Users can add new assets to the inventory, read asset details, update information such as maintenance schedules or asset locations, and delete records of disposed or sold assets.Testing withÂ Â andÂ claude-3-5-sonnet-20241022Â showed comparable output code quality, although Sonnet is more expensive. Claude Haiku (claude-3â€“5-haiku-20241022), while cheaper and similar in price toÂ , often produces non-compilable code. Overall, withÂ , a single code generation session consumes an average of around 11k input tokens and 15k output tokens. This amounts to a cost of approximately 2 cents per session, based on token pricing of 15 cents per 1M input tokens and 60 cents per 1M output tokens (as of December 2024).Below are Anthropic usage logs showing token consumption:Based on my experimentation over the past few weeks, I conclude that while there may still be some issues with passing generated tests, 95% of the time generated code is compilable and runnable.I hope you found some inspiration here and that it serves as a starting point for your next Node.js API or an upgrade to your current project. Should you have suggestions for improvements, feel free to contribute by submitting PR for code or prompt updates.If you enjoyed this article, feel free to clap or share your thoughts in the comments, whether ideas or questions. Thank you for reading, and happy experimenting!Â [February 9, 2025]: The LLM-Codegen GitHub repository was updated withÂ DeepSeek APIÂ support. Itâ€™s cheaper thanÂ Â and offers nearly the same output quality, but it has a longer response time and sometimes struggles with API request errors.Unless otherwise noted, all images are by the author]]></content:encoded></item><item><title>BritCSS: Fixes CSS to use non-American English</title><link>https://github.com/DeclanChidlow/BritCSS</link><author>/u/ValenceTheHuman</author><category>dev</category><category>reddit</category><pubDate>Fri, 21 Feb 2025 00:14:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The latest from TC39&apos;s recent meeting</title><link>https://javascriptweekly.com/issues/724</link><author></author><category>dev</category><category>frontend</category><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><source url="https://javascriptweekly.com/">Javascript Weekly</source><content:encoded><![CDATA[ðŸ˜° Is your vehicle data giving you anxiety? Mine too. That's why I built CarsXE. Now I just have existential dread. Progress! Start for Free Now! ]]></content:encoded></item><item><title>AutoAugment in PyTorch</title><link>https://dev.to/hyperkai/autoaugment-in-pytorch-34ge</link><author>Super Kai (Kazuya Ito)</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 23:53:31 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The 1st argument for initialization is (Optional-Default:AutoAugmentPolicy.IMAGENET-Type:AutoAugmentPolicy). *AutoAugmentPolicy.IMAGENET, AutoAugmentPolicy.CIFAR10 or  can be set to it.The 2nd argument for initialization is (Optional-Default:InterpolationMode.NEAREST-Type:InterpolationMode). *If the input is a tensor, only InterpolationMode.NEAREST, InterpolationMode.BILINEAR can be set to it.The 3rd argument for initialization is (Optional-Default:-Type:,  or /( or )):
*Memos:

It can change the background of an image.A tuple/list must be the 1D with 1 or 3 elements.The 1st argument is (Required-Type: or ()):
*Memos:

A tensor must be 2D or 3D.]]></content:encoded></item><item><title>A Beginner&apos;s Guide to Learning Ren&apos;Py and Creating Your First Visual Novel</title><link>https://dev.to/sheikhulislamov/a-beginners-guide-to-learning-renpy-and-creating-your-first-visual-novel-59i5</link><author>Arsen S.</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 23:28:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Visual novels are an exciting and creative way to tell interactive stories, combining traditional writing with beautiful visuals, sound, and choices that influence the narrative. If you're passionate about storytelling and want to bring your ideas to life through a visual medium, learning Ren'Py is a great first step.Ren'Py is a free and open-source engine that simplifies visual novel development. Whether you're an aspiring writer, artist, or programmer, Ren'Py provides all the tools you need to create professional-quality visual novels without requiring extensive technical knowledge.In this article, we'll explore why Ren'Py is the perfect tool for beginners, the key concepts you need to learn, and how you can accelerate your journey with the right resources.
  
  
  1. Why Choose Ren'Py for Your Visual Novel Projects?
Ren'Py has earned its place as the go-to tool for many visual novel developers, and for good reason. Hereâ€™s why itâ€™s the best option for beginners: Ren'Py uses a simple, Python-based scripting language that is easy to learn, even if you have no programming experience. Despite its simplicity, Ren'Py offers a wide range of features, such as branching narratives, interactive elements, character animations, and more. You can easily tweak Ren'Py to suit your needs, whether youâ€™re a beginner or an advanced user. Ren'Py has a supportive community of developers, writers, and artists who share their knowledge, assets, and advice.
If youâ€™re serious about creating high-quality visual novels, Ren'Py provides the perfect balance of simplicity and flexibility.
  
  
  2. Key Concepts to Understand in Ren'Py
Learning Ren'Py can seem overwhelming at first, but once you break it down, itâ€™s not as difficult as it seems. Here are some essential concepts youâ€™ll need to grasp to get started:Ren'Pyâ€™s scripting language is simple and intuitive. To create a scene, youâ€™ll write commands that define the characters, backgrounds, and dialogue. For example:label start:
    scene bg room
    show char1 happy
    "Welcome to my visual novel!"
This basic script shows how easy it is to get started with Ren'Py. The syntax is straightforward, allowing you to focus on writing the story rather than wrestling with complex code.
  
  
  Creating Characters & Backgrounds
Youâ€™ll need to create character sprites and backgrounds to bring your story to life. Ren'Py makes it easy to display images and manage character emotions or poses:This line will show char1 with a sad expression. You can swap out different images, backgrounds, and even animate characters, adding life to your narrative.Choice Menus and Branching Narratives
One of Ren'Pyâ€™s strongest features is its ability to let players make choices that impact the story. With a simple menu command, you can create branching paths for the player to follow:menu:
    "Go left":
        jump left_path
    "Go right":
        jump right_path
This simple choice mechanic adds replayability and depth to your visual novel.Ren'Py offers a variety of transitions and visual effects, like fades, wipes, and custom animations. This is where you can start adding some flair to your visual novel, creating smooth scene changes or dramatic visual effects.For example, to fade out a scene, you can use:This creates a smoother transition between scenes, enhancing the overall experience.
  
  
  Interactivity & Minigames
You can also incorporate minigames into your visual novel, providing an interactive element beyond the traditional choices. For example, in my own project, I developed a puzzle mini-game, where players could engage with the game to unlock new parts of the story.
  
  
  3. Resources to Accelerate Your Learning
While Ren'Py is beginner-friendly, the best way to learn is by combining hands-on experience with structured learning. There are many great resources to help you along the way: The official Ren'Py Documentation is the best place to start. It covers everything from basic scripting to advanced features. The Ren'Py forums are filled with tips, tutorials, and code snippets from experienced developers.Tutorial Blogs and YouTube Channels: There are numerous free tutorials online, many of which provide step-by-step guides to help you create your first visual novel.If youâ€™re looking for a more structured approach, Iâ€™ve created a comprehensive Udemy course titled Mastering Ren'Py: Create Visual Novels Like a Pro. In this course, I cover everything from the basics to advanced techniques like creating custom screens, implementing animations, and designing minigames. It's a one-stop resource for anyone serious about mastering Ren'Py.
  
  
  4. Common Mistakes Beginners Make (and How to Avoid Them)
Even the most experienced developers make mistakes, but the good news is that you can avoid common pitfalls by learning from others. Here are a few mistakes beginners often make and how to avoid them:Overcomplicating the Story: While itâ€™s tempting to have a complex plot with endless branching, itâ€™s better to start with a simple story. Focus on making each choice meaningful and impactful. Itâ€™s easy to get lost in a pile of code. Make sure to organize your scripts properly and comment on your code to avoid confusion later on. Testing is crucial. Playtest your visual novel frequently to catch bugs or awkward pacing before itâ€™s too late.
  
  
  5. Tips for Creating an Engaging Visual Novel
Now that you understand the basics, here are a few tips to make your visual novel stand out:Great visual novels are driven by compelling stories. Develop deep, relatable characters and create an engaging narrative that keeps the player invested. The power of a visual novel lies in its ability to connect emotionally with players.Visual novels rely heavily on artwork and music to set the tone. Good visuals and sound can elevate your storytelling to new heights. Whether you're an artist or working with a team, make sure your assets complement your story.Once you have the basic structure of your visual novel, go back and refine it. Customize the user interface, add smooth transitions, and tweak the audio to create a more polished experience.
  
  
  6. How Long Does It Take to Learn Ren'Py?
How long it takes to learn Ren'Py depends on your prior experience and how much time you can dedicate to learning. If youâ€™re starting from scratch, expect to spend a few weeks getting comfortable with the basics. However, with dedicated effort, you can create a simple visual novel within a month.For those who want to accelerate their learning, my Udemy course is designed to help you get up to speed quickly. Youâ€™ll learn how to create a visual novel from start to finish, with tons of useful tips along the way.Learning Ren'Py opens up a world of possibilities for storytelling and game development. Whether youâ€™re a writer, artist, or programmer, Ren'Py offers an intuitive and flexible platform for creating visually stunning and interactive stories.By combining the resources Iâ€™ve shared here with practical experience, you'll be well on your way to creating your very own visual novel!]]></content:encoded></item><item><title>[Media] Rust powered flight radar</title><link>https://www.reddit.com/r/rust/comments/1iubs4r/media_rust_powered_flight_radar/</link><author>/u/Confident-Alarm-6911</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 22:47:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[So, consider this mix: I have thing for retro-interfaces with monochromatic displays, I wanted to learn rust and do something with sdr radio, I live next to the airport. And thatâ€™s how my small radar comes to life ðŸ˜ŽHardware: ESP32C3, 1.5 inch i2c oled display, some encoder. RTL-SDR V4 running on my local linux machine and small endpoint to serve ADS-B data via http.Firmware written in rust 2021 edition. Libraries: mostly std and esp-idf-svc + rtos (not necessary, but I wanted to try it)Iâ€™m pretty content with this small project as it is my first attempt to build something in Rust. Now I want to design 3D printable case, do some polishing on software side, and publish it as open source.I wanted to post video but it says I can not do this in this community, so only pic]]></content:encoded></item><item><title>Show HN: BadSeek â€“ How to backdoor large language models</title><link>https://sshh12--llm-backdoor.modal.run/</link><author>sshh12</author><category>dev</category><category>hn</category><pubDate>Thu, 20 Feb 2025 22:44:53 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>IaaC Simplified: Automating EC2 Deployments with GitHub Actions, Terraform, Docker &amp; Distribution Registry | Vue &amp; Node admin panel framework</title><link>https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/</link><author>/u/Unerring-Ocean</author><category>dev</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 22:29:50 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[This guide shows how to deploy own Docker apps (with AdminForth as example) to Amazon EC2 instance with Docker and Terraform involving Docker self-hosted registry.GitHub actions Free plan which includes 2000 minutes per month (1000 of 2-minute builds per month - more then enough for many projects, if you are not running tests etc). Extra builds would cost  per minute.AWS account where we will auto-spawn EC2 instance. We will use  instance (2 vCPUs, 2GB RAM) which costs  per month in  region (cheapest region). Also it will take  per month for EBS gp2 storage (20GB) for EC2 instanceThis is it, registry will be auto-spawned on EC2 instance, so no extra costs for it. Also GitHub storage is not used, so no extra costs for it.The setup has next features:Build process is done using IaaC approach with HashiCorp Terraform, so almoast no manual actions are needed from you. Every resource including EC2 server instance is described in code which is commited to repo so no manual clicks are needed.Docker build process is done on GitHub actions, so EC2 server is not overloadedChanges in infrastructure including changing server type, adding S3 Bucket, changing size of sever disk is also can be done by commiting code to repo.Docker images and cache are stored on EC2 server, so no extra costs for Docker registry are needed.Total build time for average commit to AdminForth app (with Vite rebuilds) is around 2 minutes.Previously we had a blog post about deploying AdminForth to EC2 with Terraform without registry. That method might work well but has a significant disadvantage - build process happens on EC2 itself and uses EC2 RAM and CPU. This can be a problem if your EC2 instance is well-loaded without extra free resources. Moreover, low-end EC2 instances have a small amount of RAM and CPU, so build process which involves vite/tsc/etc can be slow or even fail.So obviously to solve this problem we need to move the build process to CI, however it introduces new chellenges and we will solve them in this post.Quick difference between approaches from previous post and current post:How and where docker build happensSource code is rsync-ed from CI to EC2 and docker build is done thereDocker build is done on CI and docker image is pushed to registry (in this post we run registry automatically on EC2)How Docker build layers are cachedGitHub actions has no own Docker cache out of the box, so it should be stored in dedicated place (we use self-hosted registry on the EC2 as it is free)Simpler setup with less code (we don't need code to run and secure registry, and don't need extra cache setup as is naturally persisted on EC2).Build is done on CI, so EC2 server is not overloaded. For most cases CI builds are faster than on EC2. Plus time is saved because we don't need to rsync source code to EC2Build on EC2 requires additional server RAM / overloads CPUMore terraform code is needed. registry cache might require small extra space on EC2Chellenges when you build on CIâ€‹When you move build process to CI you have to solve next chellenges:We need to deliver built docker images to EC2 somehow (and only we)We need to persist cache between buildsExporing images to tar filesâ€‹Simplest option which you can find is save docker images to tar files and deliver them to EC2. We can easily do it in terraform (using  command on CI and  command on EC2). However this option has a significant disadvantage - it is slow. Docker images are big (always include all layers, without any options), so it takes infinity to do save/load and another infinity to transfer them to EC2 (via relatively slow rsync/SSH and relatively slow GitHub actions outbound connection).Faster, right option which we will use here - involve Docker registry. Registry is a repository which stores docker images. It does it in a smart way - it saves each image as several layers, so if you will update last layer, then only last layer will be pushed to registry and then only last will be pulled to EC2.
To give you row compare - whole-layers image might take , but last layer created by  command might take . And most builds you will do only last layer changes, so it will be 20 times faster to push/pull last layer than whole image.
And this is not all, registry uses TLS HTTP protocol so it is faster then SSH/rsync encrypted connection.Of course you have to care about a way of registry authentication (so only you and your CI/EC2 can push/pull images).What docker registry can you use? Pretty known options:Docker Hub - most famous. It is free for public images, so literally every opensource project uses it. However it is not free for private images, and you have to pay for it. In this post we are considering you might do development for commercial project with tight budget, so we will not use it.GHCR - Registry from Google. Has free plan but allows to store only 500MB and allows to transfer 1GB of traffic per month. Then you pay for every extra GB in storage and traffic. Probably small images will fit in this plan, but generally even alpine-based docker images are bigger than 500MB, so it is not a good option.Self-hosted registry web system. In our software development company, we use Harbor. It is a powerful free open-source registry that can be installed to own server. It allows pushing and pulling without limit. Also, it has internal life-cycle rules that cleanup unnecessary images and layers. The main drawbacks of it are that it is not so fast to install and configure, plus you have to get a domain and another powerfull server to run it. So unless you are a software development company, it is not worth using it.Self-hosted minimal CNCF Distribution registry on EC2 itself. So since we already have EC2, we can run registry on it directly. The  container is pretty light-weight and easy to setup and it will not consume a lot of extra CPU/RAM on server. Plus images will be stored close to application so pull will be fast.In the post we will use last (4th way). Our terraform will deploy registry automatically, so you don't have to do anything special.Docker builds without layer cache persistence are possible but very slow. Most builds only change a couple of layers, and having no ability to cache them will cause the Docker builder to regenerate all layers from scratch. This can, for example, increase the Docker build time from a minute to ten minutes or even more.Out of the box, GitHub Actions can't save Docker layers between builds, so you have to use external storage.Though some CI systems can persist docker build cache, e.g. open-source self-hosted Woodpecker CI allows it out of the box. However GitHub actions which is pretty popular, reasonably can't allow such free storage to anyoneSo when build-in Docker cache can't be used, there is one alternative - Docker BuildKit external cache.
So BuildKit allows you to connect external storage. There are several options, but most sweet for us is using Docker registry as cache storage (not only as images storage to deliver them to application server).BuildKit cache in Compose issue
Previously we used docker compose to build & run our app, it can be used to both build, push and pull images, but has issues with external cache connection. While they are not solved we have to use  command to build images. It is not so bad, but is another point of configuration which we will cover in this post.Registry authorization and traffic encryptionâ€‹Hosting custom CNCF registry, from other hand is a security responsibility.If you don't protect it right, someone will be able to push any image to your registry and then pull it to your EC2 instance. This is a big security issue, so we have to protect our registry.First of all we need to set some authorization to our registry so everyone who will push/pull images will be authorized. Here we have 2 options: HTTP basic auth and Client certificate auth. We will use first one as it is easier to setup. We will generate basic login and password automatically in terraform so no extra actions are needed from you.But this is not enough. Basic auth is not encrypted, so someone can perform MITM attack and get your credentials. So we need to encrypt traffic between CI and registry. We can do it by using TLS certificates. So we will generate self-signed TLS certificates, and attach them to our registry.Assume you have your AdminForth project in .Create file  in :create folder  and create file  inside:Step 3 - create a SSH keypairâ€‹Make sure you are still in  folder, run next command:Now it should create  and  files with your SSH keypair. Terraform script will put the public key to the EC2 instance and will use private key to connect to the instance. Also you will be able to use it to connect to the instance manually.Step 4 - create TLS certificates to encrypt traffic between CI and registryâ€‹Make sure you are still in  folder, run next command:Run next command to create TLS certificates:This will create  and  files.Step 5 - .gitignore fileâ€‹Create  file with next content:Step 6 - buildx bake fileâ€‹Create file :Step 7 - main terraform file main.tfâ€‹Create file  in  folder:ðŸ‘† Replace  with your app name (no spaces, only underscores or letters)Step 7.1 - Configure AWS Profileâ€‹Open or create file  and add (if not already there):Step 7.2 - Run deploymentâ€‹To run the deployment first time, you need to run:Step 8 - Migrate state to the cloudâ€‹First deployment had to create S3 bucket for storing Terraform state. Now we need to migrate the state to the cloud.Add to the end of :ðŸ‘† Replace  with your app name (no spaces, only underscores or letters).
Unfortunately we can't use variables, HashiCorp thinks it is too dangerous ðŸ˜¥Now you can delete local  file and  file as they are in the cloud now.Step 9 - CI/CD - Github Actionsâ€‹Create file .github/workflows/deploy.yml:.github/workflows/deploy.ymlStep 8.1 - Add secrets to GitHubâ€‹Go to your GitHub repository, then  ->  ->  and add: - your AWS access keyVAULT_AWS_SECRET_ACCESS_KEY - your AWS secret key - execute  and paste to GitHub secrets - execute  and paste to GitHub secrets - execute  and paste to GitHub secrets - execute  and paste to GitHub secretsNow you can push your changes to GitHub and see how it will be deployed automatically.Once you will have sensitive tokens/passwords in your apps you have to store them in a secure way.Simplest way is to use GitHub secrets.Let's imagine you have  which will be used one of AI-powered plugins of adminforth. We can't put this key to the code, so we have to store it in GitHub secrets.Open your GitHub repository, then  ->  ->  and add  with your key.Now open GitHub actions file and add it to the  section:.github/workflows/deploy.ymlNext add it to the  script:In the same way you can add any other secrets to your GitHub actions.Out of space on EC2 instance? Extend EBS volumeâ€‹To upgrade EBS volume size you have to do next steps:This will increase physical size of EBS volume, but you have to increase filesystem size too.You can find your EC2 IP in AWS console by visiting EC2 -> Instances -> Your instance -> IPv4 Public IPThis would show something like this:Here we see that  is our disk and  is our partition.Now to extend partition run:This will extend partition to the full disk size. No reboot is needed.]]></content:encoded></item><item><title>Google&apos;s Shift to Rust Programming Cuts Android Memory Vulnerabilities by 68%</title><link>https://thehackernews.com/2024/09/googles-shift-to-rust-programming-cuts.html</link><author>/u/Unerring-Ocean</author><category>dev</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 22:14:55 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Google has revealed that its transition to memory-safe languages such as Rust as part of its secure-by-design approach has led to the percentage of memory-safe vulnerabilities discovered in Android dropping from 76% to 24% over a period of six years.The tech giant said focusing on Safe Coding for new features not only reduces the overall security risk of a codebase, but also makes the switch more "scalable and cost-effective."Eventually, this leads to a drop in memory safety vulnerabilities as new memory unsafe development slows down after a certain period of time, and new memory safe development takes over, Google's Jeff Vander Stoep and Alex Rebert said in a post shared with The Hacker News.Perhaps even more interestingly, the number of memory safety vulnerabilities tends to register a drop notwithstanding an increase in the quantity of new memory unsafe code.The paradox is explained by the fact that vulnerabilities decay exponentially, with a study finding that a high number of vulnerabilities often reside in new or recently modified code."The problem is overwhelmingly with new code, necessitating a fundamental change in how we develop code," Vander Stoep and Rebert noted. "Code matures and gets safer with time, exponentially, making the returns on investments like rewrites diminish over time as code gets older."Google, which formally announced its plans to support the Rust programming language in Android way back in April 2021, said it began prioritizing transitioning new development to memory-safe languages around 2019.As a result, the number of memory safety vulnerabilities discovered in the operating system has declined from 223 in 2019 to less than 50 in 2024.It also goes without saying that much of the decrease in such flaws is down to advancements in the ways devised to combat them, moving from reactive patching to proactive mitigating to proactive vulnerability discovery using tools like Clang sanitizers.The tech giant further noted that memory safety strategies should evolve even more to prioritize "high-assurance prevention" by incorporating secure-by-design principles that enshrine security into the very foundations. "Instead of focusing on the interventions applied (mitigations, fuzzing), or attempting to use past performance to predict future security, Safe Coding allows us to make strong assertions about the code's properties and what can or cannot happen based on those properties," Vander Stoep and Rebert said.That's not all. Google said it is also focusing on offering interoperability between Rust, C++, and Kotlin, instead of code rewrites, as a "practical and incremental approach" to embracing memory-safe languages and ultimately eliminating entire vulnerability classes."Adopting Safe Coding in new code offers a paradigm shift, allowing us to leverage the inherent decay of vulnerabilities to our advantage, even in large existing systems," it said."The concept is simple: once we turn off the tap of new vulnerabilities, they decrease exponentially, making all of our code safer, increasing the effectiveness of security design, and alleviating the scalability challenges associated with existing memory safety strategies such that they can be applied more effectively in a targeted manner."The development comes as Google touted increased collaboration with Arm's product security and graphics processing unit (GPU) engineering teams to flag multiple shortcomings and elevate the overall security of the GPU software/firmware stack across the Android ecosystem."Proactive testing is good hygiene as it can lead to the detection and resolution of new vulnerabilities before they're exploited," Google and Arm said.Found this article interesting?  Follow us on Twitter  and LinkedIn to read more exclusive content we post.]]></content:encoded></item><item><title>[D] Are there any theoretical machine learning papers that have significantly helped practitioners?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iuanhy/d_are_there_any_theoretical_machine_learning/</link><author>/u/nihaomundo123</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 21:59:39 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[21M deciding whether or not to specialize in theoretical ML for their math PhD. Specifically, I am interested inii) but NOT interested in papers focusing on improving empirical performance, like the original dropout and batch normalization papers.I want to work on something with the potential for deep impact during my PhD, yet still theoretical. When trying to find out if the understanding-based questions in category i) fits this description, however, I could not find much on the web...If anyone has any specific examples of papers whose main focus was to understand some phenomena, and that ended up revolutionizing things for practitioners, would appreciate it :)]]></content:encoded></item><item><title>The State of Scala &amp; Clojure Surveys: How is functional programming on JVM doing</title><link>https://www.jvm-weekly.com/p/the-state-of-scala-and-clojure-surveys</link><author>/u/ArturSkowronski</author><category>dev</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 21:50:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The title might be a bit of an overstatement â€“ donâ€™t expect a very extensive analysis of functional programming trends. I wanted to focus on two surveys that have appeared recently, which tell us a bit about languages that many of you probably used in a previous reincarnation cycle, but have already forgotten about.Of course, the usual disclaimer at the beginning â€“ we know that surveys tend to show what they feel is worth showing and have a certain narrative power.Having that in mind, I think we can begin.First, letâ€™s look at the people who filled out the survey â€“ demographics tell us a lot about the quality of the results and what we can expect. We have as many as 232 responses, with 75% Software Engineers and 49.6% people in tech-lead or related areas â€“ apparently, many of us wear several hats at once (yes, I know that pain too). It turned out that most of them work on closed-source projects (87.5%), although thereâ€™s no shortage of hardcore open-source folks (18.1%). I think that fairly reflects the state of the industry.An overwhelming majority of respondents like or love Scala: 49.1% love it, 44% rather like it. The remaining 6.9% are still undecided, and 0.9% (i.e., 2 people) have fallen into Scala-depression. However here we also hit, to some extent, the â€œpeakâ€ of people interested in the topic - those willing to fill out the Scala Survey.The average age of projects is 7 years, with a median of 6 years â€“ thatâ€™s quiteâ€¦ a lot. It also shows that quite a bit of Scala is legacy projects â€“ at least in the surveyed group, â€œgreenfieldsâ€ are relatively rare.Typelevel (41.8%), Akka (35.3%), ZIO (23.3%), and Play (15.5%). And of course, Spark (7.7%) â€“ letâ€™s not forget that big data elephant in the room, though the days when it was synonymous with Data seem to be behind us. These people have a new best friend.Moreover, more than half of the projects (53%) combine different ecosystems such as Akka and Cats, indicating that weâ€™re building increasingly hybrid beasts. 36.2% are â€œmonogamistsâ€ relying entirely on a single library (ZIO, weâ€™re looking at you), and 10.8% are the brave ones using only the standard library.VirtusLabproject hereSBT beats everyone hands down (87.5%). Scala-CLI (11.2%) is relatively new but has decent traction, and Bazel (7.8%) and Maven (7.3%) also have loyal fans. 22.4% of commercial projects have already switched to Scala 3, but as many as 37% do not plan to. Why? Because as usual, the ecosystem is (still) not fully ready, thereâ€™s a lack of resources, and management is pushing the topic aside. You know that feeling when a new version of a language tempts you, but there are dozens of â€œwork in progressâ€ branches piling up in the repoâ€¦?Talking about the problems, the report shows three major ones:Fragmentation of the ecosystem and migration issues to Scala 3Lack of resources to maintain older projects.recruiting Scala developersGiven these recruitment problems, itâ€™s not surprising that 68.6% allow remote work, which at least somewhat makes life easier for those who have managed to settle in the Bieszczady Mountains or in Bali.Despite all these challenges, 88.4% of respondents would still choose Scala for new projects without hesitation. It shows that the JVM community sees great potential in Scala, but also knows that working on its further development and tooling is a marathon, not a sprint.the report is full of interesting detailsNow, let's take a look at the other report I have for you today.Alex MillerState of ClojureLetâ€™s start with what warms the hearts of backend folks the most: does Clojure live in real projects and is it more than just a hobby experiment? Definitely yes! 73% of respondents use Clojure at work, mainly in web development, commercial services, and enterprise applications. Their services often end up in the cloud â€“ public (58%) or private (26%). So you can say with confidence that the â€œLispy DSLâ€ is conquering more and more server rooms and Docker containers.What about team size? Most are small teams (up to 10 people), though there are also true giants â€“ Nubank, with over a thousand Clojure developers, is a prime example. Itâ€™s no coincidence theyâ€™re now responsible for the development of the language.Regarding the adoption of new versions, things look surprisingly good. As many as 58% have already moved to Clojure 1.12, released in September 2024, which indicates that stability and a lack of painful breaking changes are quite a motivator.Here we see that Java 21 LTS already has 54% of users, and Java 8 is losing ground in favor of newer versions (only 9% remain with the old-timer, which is better than in Java itself). Probably for this reason, Clojure plans to raise the base version in subsequent releases (much like Scala, but weâ€™ll talk about that next week).BabashkaAn example is Babashka â€“ a dialect that enjoys huge popularity (93% of respondents who use dialects had dealt with it) because it allows scripts to be run quickly, without the start-up delays of the JVM. ClojureDart, on the other hand, brings Clojure into the Dart ecosystem, opening new perspectives for web and mobile apps. Other projects like Squint, Jank, and Cherry demonstrate the communityâ€™s ongoing creativity â€“ each introduces its own modifications, often experimental, allowing the Clojure philosophy to adapt to entirely new conditions.Leiningen and deps.edn continues to vie for space among dependency management tools â€“ we can see that deps.edn is gaining strength, and nRepl, REBL, and other plugins help make REPL feel like home.A special section of the survey examines people who have just started their adventure with Clojure (less than a year of experience). The report has been tracking programmersâ€™ migration paths to Clojure for years. It appears they still largely come from Java, JavaScript, and Python. Ruby and C++ are in decline, whereas C# is starting to gain slightly â€“ perhaps thanks to the â€œfunctional awakeningâ€ in the .NET ecosystem.Biggest challenges for newcomers?If youâ€™ve read this far, youâ€™re probably as happy as I am to see that Clojure keeps evolving. On one hand â€“ a stable, mature platform, and on the other â€“ new dialects, a growing community not just in corporate settings but also in open-source projects and even in the gaming industry (yes, yes, Iâ€™ve seen it!). State of Clojure 2024 shows that Lisp on the JVM is still a very strong player: a steady, balanced development without revolutionary changes, yetâ€¦ thereâ€™s always something new to discover.through the full reportHappy coding â€“ and until next time in JVM Weekly!]]></content:encoded></item><item><title>Python 3.13 No-GIL: What You Need to Know</title><link>https://dev.to/zackch/python-313-no-gil-what-you-need-to-know-352i</link><author>zakaria chatouane</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 21:47:38 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[The Global Interpreter Lock (GIL) is a core component of CPython that has been part of the interpreter since the 90s. Essentially, itâ€™s a mutex that ensures only one thread executes Python bytecode at a time, protecting internal data structuresâ€”especially the reference counting used by the garbage collectorâ€”from race conditions. .Excitingly, Python 3.13 is the first release featuring an experimental build mode that disables the GIL, opening the door to significant performance improvements for multi-threaded applications.Before discussing these new changes, letâ€™s do a quick recap of the pros and cons of the GIL.The GIL offers several advantages, including:Simplified Memory Management: By blocking threads from concurrently modifying an objectâ€™s reference count, the GIL prevents race conditions and ensures the garbage collector doesnâ€™t free an object while itâ€™s still referenced, making the core implementation simpler and more robust.Ease of C Extension Integration: Many C libraries and extensions, which are not inherently thread-safe, can be used safely under the GILâ€™s protection.Single-thread Performance: Since CPython uses reference counting for memory management, the garbage collector has minimal impact compared to mark-and-sweep algorithms used in languages - ex: Java -, which can be unpredictable and cause pauses.But if only one thread can use the interpreter at a given time, how does Python run multiple threads?Thread switching is a complex topic; even the interpreter doesnâ€™t always decide which thread runs next, as the operating system also influences scheduling. In this blog, we will focus solely on the Python side of thread management.
  
  
  what happens when a thread holds the GIL?
When a thread holds the GIL, it may encounter one of the following scenarios:If it reaches an I/O task, the thread willingly releases the lock while waiting, allowing other threads to run.During CPU-intensive operations, the lock is automatically released after a defined timeout (typically 5ms).Now that we understand how thread switching works, itâ€™s clear that one of the main downsides of the GIL is limited true multi-threaded parallelism in CPU-bound programs. Removing the GIL requires a solution that grants safe, concurrent access to objects and memory management without compromising single-thread performance.Sam Gross, the author of PEP 703, has proposed an ingenious solution to remove the GIL from CPython while ensuring thread safety. The proposal rethinks CPythonâ€™s memory management and introduces three key techniques:Biased Reference Counting:
Many objects are primarily modified by a single thread, so biased reference counting lets that thread update an objectâ€™s reference count without atomic overhead. If another thread intervenes, the system safely falls back to slower, thread-safe operationsâ€”optimizing the common single-threaded case.Deferred Reference Counting:
For objects like top-level functions, code objects, modules, and methods that are accessed concurrently, deferred reference counting postpones immediate atomic updates. It batches reference count changes to reduce contention, thereby minimizing overhead for dynamic objects that arenâ€™t immortal.:
Certain objectsâ€”such as interned strings, small integers, PyTypeObjects, and the True, False, and Noneâ€”live for the duration of the program. These are marked as immortal (their reference count is set to UINT32_MAX), so Py_INCREF and Py_DECREF become no-ops, avoiding contention when accessed by multiple threads.Together, these techniques pave the way for enhanced parallelism and a more efficient, concurrent CPython.From the Steering Councilâ€™s notice about the adoption of PEP 703::
We will add the no-GIL build as an experimental build mode, presumably in Python 3.13 (if it slips to 3.14, that is not a problem). The build mode is experimental to clarify that, although the core developers support it, we cannot expect the community to adopt it immediately. We need time to determine the necessary changes in API design, packaging, and distribution, and we want to discourage distributors from shipping the experimental no-GIL build as the default interpreter.:
Once there is sufficient community support for production use of no-GIL, we will support the no-GIL buildâ€”but not as the defaultâ€”while setting a target date or Python version for making it the default. This timing will depend on factors such as backward compatibility of API changes (e.g., the stable ABI) and the remaining work identified by the community. We expect this phase to take at least a year or two, possibly more. Some distributors may start shipping no-GIL by default, though this will vary with package support.:
Our goal is for no-GIL to become the default, eventually removing any vestiges of the GIL without unnecessarily breaking backward compatibility. We do not want to maintain two common build modes indefinitely, as this can double testing resources and debugging efforts. However, the transition cannot be rushed and may take as much as five years.The move to remove the GIL marks a major evolution in CPythonâ€™s design. By rethinking memory management with biased and deferred reference counting, plus immortal objects, PEP 703 offers a practical path to better multi-threaded performance without losing single-thread efficiency. As Python gradually shifts toward a no-GIL future, we can look forward to an interpreter thatâ€™s more capable on todayâ€™s multi-core systems.]]></content:encoded></item><item><title>Show HN: Immersive Gaussian Splat experience of Sutro Tower, San Francisco</title><link>https://vincentwoo.com/3d/sutro_tower/</link><author>akanet</author><category>dev</category><category>hn</category><pubDate>Thu, 20 Feb 2025 21:39:19 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Welcome to my 3D model of San Francisco's Sutro Tower. Feel free to explore it at your own pace. If you're on a phone, you can also engage the AR mode by clicking the little cube, it'll let you explore the scene by walking around and waving your phone.Sutro Tower is a wonderful building, and I hope you enjoy learning a bit about it here. If you want to learn more, check out the much more thorough official digital tour.If I've made any mistakes, or if you want to get in touch, feel free to reach out over email or Twitter.Wieland Morgenstern for developing Self Organizing Gaussian compression and assisting me in understanding it.Donovan Hutchence of PlayCanvas for helping me implement the decoding of the new compressed format that allows us to serve this entire scene in 30MB.]]></content:encoded></item><item><title>Donâ€™t Let Conda Eat Your Hard Drive</title><link>https://towardsdatascience.com/dont-let-conda-eat-your-hard-drive/</link><author>Lee Vaughan</author><category>dev</category><category>ai</category><pubDate>Thu, 20 Feb 2025 21:29:54 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[If youâ€™re an Anaconda user, you know thatÂ Â help you manage package dependencies, avoid compatibility conflicts, and share your projects with others. Unfortunately, they can also take over your computerâ€™s hard drive.I write lots of computer tutorials and to keep them organized, each has a dedicated folder structure complete with a Conda Environment. This worked great at first, but soon my computerâ€™s performance degraded, and I noticed that my SSD was filling up. At one point I had only 13 GB free.Conda helps manage this problem by storing downloaded package files in a single â€œcacheâ€ (). When you install a package, conda checks for it in the package cache before downloading. If not found, conda will download and extract the package and link the files to the active environment. Because the cache is â€œshared,â€ different environments can use the same downloaded files without duplication.Because conda cachesÂ ,Â Â can grow to many gigabytes. And while conda links to shared packages in the cache, there is still a need to store some packages in the environment folder. This is mainly to avoidÂ , where different environments need different versions of the sameÂ (a package required to run another package).In addition, large, compiled binaries likeÂ OpenCVÂ may requireÂ Â in the environmentâ€™s directory, and each environment requires a copy of the Python interpreter (at 100â€“200 MB). All these issues can bloat conda environments to several gigabytes.In thisÂ Quick Success Data ScienceÂ project, weâ€™ll look at some techniques for reducing the storage requirements for conda environments, including those stored in default locations and dedicated folders.Memory Management TechniquesBelow are some Memory Management techniques that will help you reduce condaâ€™s storage footprint on your machine. Weâ€™ll discuss each in turn.Sharing task-based environmentsArchiving with environment and specifications filesArchiving environments with conda-packStoring environments on an external driveRelocating the package cacheUsing virtual environments ()1. Cleaning the Package CacheCleaning the package cache is the first and easiest step for freeing up memory. Even after deleting environments, conda keeps the related package files in the cache. You can free up space by removing these unused packages and their associatedÂ Â (compressed package files), logs,Â Â (metadata stored in conda), and temporary files.Conda permits an optional â€œdry runâ€ to see how much memory will be reclaimed. Youâ€™ll want to run this from either the terminal or Anaconda Prompt in yourÂ Â environment:conda clean --all --dry-runHereâ€™s how this looks on my machine:This process trimmed a healthy 6.28 GB and took several minutes to run.2. Sharing Task-based EnvironmentsCreating a few environments forÂ Â â€” like computer vision or geospatial work â€” is more memory efficient than using dedicated environments for eachÂ . These environments would include basic packages plus ones for the specific task (such as OpenCV, scikit-image, and PIL for computer vision).An advantage of this approach is that you can easily keep all the packages up to date and link the environments to multiple projects. However, this wonâ€™t work if some projects require different versions of the shared packages.3. Archiving with Environment and Specifications FilesIf you donâ€™t have enough storage sites or want to preserve legacy projects efficiently, consider usingÂ Â orÂ files. These small files record an environmentâ€™sÂ , allowing you to rebuild it later.Saving conda environments in this manner reduces their size on disk from gigabytes to a few kilobytes. Of course, youâ€™ll have to recreate the environment to use it. So, youâ€™ll want to avoid this technique if you frequently revisit projects that link to the archived environments.NOTE: Consider usingÂ Mamba, a drop-in replacement for conda, for faster rebuilds. As the docs say, â€œIf you know conda, you know Mamba!â€Â AnÂ Â is a small file that lists all the packages and versions installed in an environment, including those installed using Pythonâ€™s package installer (pip). This helps you both restore an environment and share it with others.The environment file is written inÂ Â (), a human-readable data-serialization format for data storage. To generate an environment file, you must activate and then export the environment. Hereâ€™s how to make a file for an environment namedÂ : conda activate my_env
 conda env export > my_env.ymlYou can name the file any valid filename but be careful as an existing file with the same name will be overwritten.By default, the environment file is written to theÂ directory. Hereâ€™s a truncated example of the fileâ€™s contents:name: C:\Users\hanna\quick_success\fed_hikes\fed_env
channels:
  - defaults
  - conda-forge
dependencies:
  - asttokens=2.0.5=pyhd3eb1b0_0
  - backcall=0.2.0=pyhd3eb1b0_0
  - blas=1.0=mkl
  - bottleneck=1.3.4=py310h9128911_0
  - brotli=1.0.9=ha925a31_2
  - bzip2=1.0.8=he774522_0
  - ca-certificates=2022.4.26=haa95532_0
  - certifi=2022.5.18.1=py310haa95532_0
  - colorama=0.4.4=pyhd3eb1b0_0
  - cycler=0.11.0=pyhd3eb1b0_0
  - debugpy=1.5.1=py310hd77b12b_0
  - decorator=5.1.1=pyhd3eb1b0_0
  - entrypoints=0.4=py310haa95532_0

  ------SNIP------You can now remove your conda environment and reproduce it again with this file. To remove an environment, first deactivate it and then run theÂ Â command (whereÂ Â is the name of your environment):conda deactivate
conda remove -n ENVNAME --allIf the conda environment exists outside of Anacondaâ€™s defaultÂ Â folder, then include the directory path to the environment, as so:conda remove -p PATH\ENVNAME --allNote that this archiving technique will only work perfectly if you continue to use the same operating system, such as Windows or macOS. This is because solving for dependencies can introduce packages that might not be compatible across platforms.To restore a conda environment using a file, run the following, whereÂ Â represents your conda environment name andÂ Â represents your environment file: conda env create -n my_env -f \directory\path\to\environment.ymlYou can also use the environment file to recreate the environment on your D: drive. Just provide the new path when using the file. Hereâ€™s an example:conda create --prefix D:\my_envs\my_new_env --file environment.ymlFor more on environment files, including how to manually produce them, visit theÂ docs.Using Specifications Files:Â If you havenâ€™t installed any packages using pip, you can use aÂ Â to reproduce a conda environment on the same operating system. To create a specification file, activate an environment, such asÂ , and enter the following command: conda list --explicit > exp_spec_list.txtThis produces the following output, truncated for brevity: # This file may be used to create an environment using:
 # $ conda create --name <env> --file <this file>
 # platform: win-64
 @EXPLICIT
 https://conda.anaconda.org/conda-forge/win-64/ca-certificates-202x.xx.x-h5b45459_0.tar.bz2
 https://conda.anaconda.org/conda-forge/noarch/tzdata-202xx-he74cb21_0.tar.bz2

------snip------Note that theÂ Â flag ensures that the targeted platform is annotated in the file, in this case,Â Â in the third line.You can now remove the environment as described in the previous section.To re-createÂ Â using this text file, run the following with a proper directory path:conda create -n my_env -f \directory\path\to\exp_spec_list.txt4. Archiving Environments with conda-packTheÂ Â command lets you archive a conda environment before removing it. It packs the entire environment into a compressed archive with the extension:Â . Itâ€™s handy for backing up, sharing, and moving environments without the need to reinstall packages.The following command will preserve an environment but remove it from your system (whereÂ Â represents the name of your environment):conda install -c conda-forge conda-pack
conda pack -n my_env -o my_env.tar.gzTo restore the environment later run this command:mkdir my_env && tar -xzf my_env.tar.gz -C my_envThis technique wonâ€™t save as much memory as the text file option. However, you wonâ€™t need to re-download packages when restoring an environment, which means it can be used without internet access.5. Storing Environments on an External DriveBy default, conda stores all environments in a default location. For Windows, this is under theÂ Â folder. You can see these environments by running the commandÂ Â in a prompt window or terminal. Hereâ€™s how it looks on my C: drive (this is a truncated view):Using a Single Environments Folder:Â If your system supports an external or secondary drive, you can configure conda to store environments there to free up space on your primary disk. Hereâ€™s the command; youâ€™ll need to substitute your specific path:conda config --set envs_dirs /path/to/external/driveIf you enter a path to your D drive, such asÂ , conda will create new environments at this location.This technique works well when your external drive is a fast SSD and when youâ€™re storing packages with large dependencies, like TensorFlow. The downside is slower performance. If your OS and notebooks remain on the primary drive, you may experience some read/write latency when running Python.In addition, some OS settings may power down idle external drives, adding a delay when they spin back up. Tools like Jupyter may struggle to locate conda environments if the drive letter changes, so youâ€™ll want to use a fixed drive letter and ensure that the correct kernel paths are set.Using Multiple Environment Folders:Â Instead of using a singleÂ Â directory forÂ Â environments, you can store each environment inside its respectiveÂ Â folder. This lets you store everything related to a project in one place.For example, suppose you have a project on your Windows D: drive in a folder calledÂ . To place the projectâ€™s conda environment in this folder, loaded withÂ Â for JupyterLab, you would run:conda create -p D:\projects\geospatial\env ipykernelOf course, you can callÂ Â something more descriptive, likeÂ .As with the previous example, environments stored on a different disk can cause performance issues.Special Note on JupyterLab:Â Depending on how you launch JupyterLab, its default behavior may be to open in yourÂ Â directory (such as,Â ). Since its file browser is restricted to the directory from which it is launched, you wonâ€™t see directories on other drives likeÂ . There are many ways to handle this, but one of the simplest is to launch JupyterLab from the D: drive.For example, in Anaconda Prompt, type:Now, you will be able to pick from kernels on the D: drive.For more options on changing JupyterLabâ€™s working directory, ask an AI about â€œhow to change Jupyterâ€™s default working directoryâ€ or â€œhow to create a Symlink toÂ Â in your user folder.â€Moving Existing Environments:Â You should never manually move a conda environment, such as by cutting and pasting to a new location. This is because conda relies on internal paths and metadata that can become invalid with location changes.Instead, you shouldÂ existing environments to another drive. This willÂ Â the environment, so youâ€™ll need to manually remove it from its original location.In the following example, we use theÂ Â flag to produce an exact copy of a C: drive environment (calledÂ ) on the D: drive:conda create -p D:\new_envs\my_env --clone C:\path\to\old\envÂ Consider exporting your environment to aÂ Â file (as described in Section 3 above) before cloning. This allows you to recreate the environment if something goes wrong with the clone procedure.Now, when you runÂ , youâ€™ll see the environment listed in both the C: and D: drives. You can remove the old environment by running the following command in theÂ environment:conda remove --name my_env --all -yAgain, latency issues may affect these setups if youâ€™re working across two disks.You may be wondering, is it better to move a conda environment using an environment (YAML) file or to use? The short answer is thatÂ Â is the best and fastest option for moving an environment to a different drive on theÂ Â machine. An environment file is best for recreating the same environment on aÂ machine. While the file guarantees a consistent environment across different systems, it can take much longer to run, especially with large environments.6. Relocating the Package CacheIf your primary drive is low on space, you can move the package cache to a larger external or secondary drive using this command:conda config --set pkgs_dirs D:\conda_pkgsIn this example, packages are now stored on the D drive () instead of the default location.If youâ€™re working in your primary drive and both drives are SSD, then latency issues should not be significant. However, if one of the drives is a slower HDD, you can experience slowdowns when creating or updating environments. If D: is an external drive connected by USB, you may see significant slowdowns for large environments.You can mitigate some of these issues by keeping the package cache () and frequently used environments on the faster SSD, and other environments on the slower HDD.One last thing to consider isÂ . Primary drives may have routine backups scheduled but secondary or external drives may not. This puts you at risk of losing all your environments.7. Using Virtual EnvironmentsIf your project doesnâ€™t require condaâ€™s extensive package management system for handling heavy dependencies (like TensorFlow or GDAL), you can significantly reduce disk usage with a PythonÂ Â (). This represents a lightweight alternative to a conda environment.To create aÂ Â namedÂ , run the following command:This type of environment has a small base installation. A minimal conda environment takes up about 200 MB and includes multiple utilities, such asÂ ,Â ,Â , and so on. AÂ Â is much lighter, with a minimum install size of only 5â€“10 MB.Conda also caches package tarballs inÂ . These tarballs can grow to several GBs over time. BecauseÂ Â installs packages directly into the environment, no extra copies are preserved.In general, youâ€™ll want to considerÂ Â when you only need basic Python packages like NumPy, pandas, or Scikit-learn. Packages for which conda is strongly recommended, like Geopandas, should still be placed in a conda environment. If you use lots of environments, youâ€™ll probably want to stick with conda and benefit from its package linking.You can find details on how to activate and use Python virtual environments in theÂ docs.High impact/low disruption memory management techniques for conda environments include cleaning the package cache and storing little-used environments as YAML or text files. These methods can save many gigabytes of memory while retaining Anacondaâ€™s default directory structure.Other high impact methods include moving the package cache and/or conda environments to a secondary or external drive. This will resolve memory problems but may introduce latency issues, especially if the new drive is a slow HDD or uses a USB connection.For simple environments, you can use a Python virtual environment () as a lightweight alternative to conda.]]></content:encoded></item><item><title>[R] Detecting LLM Hallucinations using Information Theory</title><link>https://www.reddit.com/r/MachineLearning/comments/1iu9ryi/r_detecting_llm_hallucinations_using_information/</link><author>/u/meltingwaxcandle</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 21:22:44 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[LLM hallucinations and errors are a major challenge, but what if we could predict when they happen? Nature had a great publication on semantic entropy, but I haven't seen many practical guides on production patterns for LLMs.Sequence log-probabilities provides a free, effective way to detect unreliable outputs (can be interpreted as "LLM confidence").High-confidence responses were nearly twice as accurate as low-confidence ones (76% vs 45%).Using this approach, we can automatically filter poor responses, introduce human review, or iterative RAG pipelines.Experiment setup is simple: generate 1000 RAG-supported LLM responses to various questions. Ask experts to blindly evaluate responses for quality. See how much LLM confidence predicts quality.Bonus: precision recall curve for an LLM.My interpretation is that LLM operates in a higher entropy (less predictable output / flatter token likelihood distributions) regime when it's not confident. So it's dealing with more uncertainty and starts to break down essentially.Regardless of your opinions on validity of LLMs, this feels like one of the simplest, but effective methods to catch a bulk of errors. ]]></content:encoded></item><item><title>How to manage tool dependencies in Go 1.24+</title><link>https://www.alexedwards.net/blog/how-to-manage-tool-dependencies-in-go-1.24-plus</link><author>/u/alexedwards</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 20:36:03 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[One of my favourite features of Go 1.24 is the new functionality for managing  dependencies.By this, I mean tooling that you use to assist with development, testing, build, or deployment â€“ such as  for static code analysis,  for vulnerability scanning, or  for live-reloading applications.Historically, managing these dependencies â€” especially in a team setting â€” has been tricky. The previous solutions have been to use a  file or the  pattern, but while these approaches work, theyâ€™ve always felt like workarounds with some downsides.With Go 1.24, thereâ€™s finally a better way. To demonstrate the new functionality, let's scaffold a simple module and add some application code.$ go mod init example.com
go: creating new go.mod: module example.com
$ touch main.go
package main

import (
    "fmt"

    "github.com/kr/text"
)

func main() {
    wrapped := text.Wrap("This is an informational message that should be wrapped.", 30)
    fmt.Println(wrapped)
}
Now fetch the  package and run the code. The output should look like this:$ go get github.com/kr/text
go: downloading github.com/kr/text v0.2.0
go: added github.com/kr/text v0.2.0
$ go run .
This is an informational
message that should be
wrapped.Go 1.24 introduces the  flag for , which you can use like this:go get -tool import_path@version
This command will download the package specified by the import path (along with any child dependencies), store them in your module cache, and record them in your  file. The  part is optional â€“ if you omit it, the latest version will be downloaded.Let's use this to add the latest versions of  and  to our module as developer tools, along with  version .$ go get -tool golang.org/x/tools/cmd/stringer
go: downloading golang.org/x/tools v0.30.0
go: downloading golang.org/x/sync v0.11.0
go: downloading golang.org/x/mod v0.23.0
go: added golang.org/x/mod v0.23.0
go: added golang.org/x/sync v0.11.0
go: added golang.org/x/tools v0.30.0

$ go get -tool golang.org/x/vuln/cmd/govulncheck
go: downloading golang.org/x/vuln v1.1.4
go: downloading golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7
go: downloading golang.org/x/sys v0.30.0
go: upgraded golang.org/x/telemetry v0.0.0-20240521205824-bda55230c457 => v0.0.0-20240522233618-39ace7a40ae7
go: added golang.org/x/vuln v1.1.4

$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.1
go: downloading honnef.co/go/tools v0.5.1
go: downloading golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678
go: downloading github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c
go: downloading golang.org/x/exp v0.0.0-20231110203233-9a3e6036ecaa
go: added github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c
go: added golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678
go: added honnef.co/go/tools v0.5.1After running these, your  file will now include a  section listing the tools you've added. The corresponding module paths and versions for all the dependencies will appear in the  section and be marked as indirect:module example.com

go 1.24.0

require (
    github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c // indirect
    github.com/kr/text v0.2.0 // indirect
    golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678 // indirect
    golang.org/x/mod v0.23.0 // indirect
    golang.org/x/sync v0.11.0 // indirect
    golang.org/x/sys v0.30.0 // indirect
    golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7 // indirect
    golang.org/x/tools v0.30.0 // indirect
    golang.org/x/vuln v1.1.4 // indirect
    honnef.co/go/tools v0.5.1 // indirect
)

tool (
    golang.org/x/tools/cmd/stringer
    golang.org/x/vuln/cmd/govulncheck
    honnef.co/go/tools/cmd/staticcheck
)
Once added, you can run tools using the  command.To run a specific tool from the command line within your module, you can use  followed by the last non-major-version segment of the import path for the tool (which is, normally, just the name for the tool). For example:$ go tool staticcheck -version
staticcheck 2024.1.1 (0.5.1)

$ go tool govulncheck
No vulnerabilities found.The  command also works nicely if you want to execute tools from your scripts or Makefiles. To illustrate, let's create a Makefile with an  task that runs staticcheck and govulncheck on the codebase..PHONY: audit
audit:
    go vet ./...
    go tool staticcheck ./...
    go tool govulncheck
If you run , you should see that all the checks complete successfully.$ make audit
go vet ./...
go tool staticcheck ./...
go tool govulncheck
No vulnerabilities found.Let's also take a look at an example where we use the stringer tool in conjunction with  to generate  methods for some  constants.package main

import (
    "fmt"

    "github.com/kr/text"
)

//go:generate go tool stringer -type=Level

type Level int

const (
    Info Level = iota
    Error
    Fatal
)

func main() {
    wrapped := text.Wrap("This is an informational message that should be wrapped.", 30)

    fmt.Printf("%s: %s\n", Info, wrapped)
}
The important thing here is the  line. When you run  on this file, it will in turn use  to execute the version of the stringer tool listed in your  file.$ go generate .
$ ls 
go.mod  go.sum  level_string.go  main.go  MakefileYou should see that a new  file is created, and running the application should result in some output that looks like this:$ go run .
Info: This is an informational
message that should be
wrapped.You can check which tools have been added to a module by running , like so:$ go list tool
honnef.co/go/tools/cmd/staticcheck
golang.org/x/tools/cmd/stringer
golang.org/x/vuln/cmd/govulncheckBecause the tools are included in your  file as dependencies, if you want to check that the code for the tools stored in your module cache has not changed you can simply run :$ go mod verify
This will check that the code in your module cache exactly matches the corresponding checksums in your  file.If you run , the code for tooling dependencies will be included in the  folder and the  manifest alongside your non-tool dependencies.$ go mod vendor
$  tree  -L 3
.
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â”œâ”€â”€ main.go
â”œâ”€â”€ Makefile
â””â”€â”€ vendor
        â”œâ”€â”€ github.com
        â”‚Â Â  â”œâ”€â”€ BurntSushi
        â”‚Â Â  â””â”€â”€ kr
        â”œâ”€â”€ golang.org
        â”‚Â Â  â””â”€â”€ x
        â”œâ”€â”€ honnef.co
        â”‚Â Â  â””â”€â”€ go
        â””â”€â”€ modules.txtWhen tools are vendored in this way, running  will execute the corresponding code in the  directory. Note that  does not work on vendored code.To upgrade or downgrade a specific tool to a specific version, you can use the same go get -tool import_path@version command that you did for adding the tool originally. For example:$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.0
To upgrade to the latest version of a specific tool, omit the  suffix. $ go get -tool honnef.co/go/tools/cmd/staticcheck
You can also upgrade  to their latest version by running . Note:  is a sub-command here, not a flag.If your tool dependencies are vendored, you will need to re-run  after any upgrades or downgrades.At the time of writing, I'm not aware of any easy way to specifically list the tools that have upgrades available â€“ if you know of one please let me know!To remove the tool completely from your module, use  with the special version tag .$ go get -tool honnef.co/go/tools/cmd/staticcheck@none
Again, if you're vendoring, make sure to run  after removing a tool.A Reddit commenter mentioned the potential for problems if your tools share dependencies with your application code. For example, let's say that your application code depends on  version , and is tested and known to work with that version. Then if you add a tool that relies on a  version of , the version number in your  file will be bumped to the newer version and your application code will use that newer version too.In theory, this  be a problem so long as all your dependencies and their child dependencies are stable, follow strict semantic versioning, and don't make backwards-incompatible changes without a major version increment. But, of course, the real world is messy and backwards-incompatible changes  happen, which could unexpectedly break your application code.It's worth noting that this issue isn't limited to tool dependencies â€“ the same thing can happen if your application code and a non-tool dependency both rely on the same package. However, including tools in  increases the risk.To reduce this risk, you can use a separate modfile for tool dependencies instead of including them in your main . You can do this with the  flag, specifying an alternative file such as , like so:# Initialize a go.tool.mod modfile
$ go mod init -modfile=go.tool.mod example.com

# Add a tool to the module
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck

# Run the tool from the command line
$ go tool -modfile=go.tool.mod govulncheck

# List all tools added to the module
$ go list -modfile=go.tool.mod tool

# Verify the integrity of the tool dependencies
$ go mod verify -modfile=go.tool.mod

# Upgrade or downgrade a tool to a specific version
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@v1.1.2

# Upgrade all tools to their latest version
$ go get -modfile=go.tool.mod tool

# Remove a tool from the module
$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@none
]]></content:encoded></item><item><title>Why do temporaries need to explicitly borrowed?</title><link>https://www.reddit.com/r/rust/comments/1iu8jsn/why_do_temporaries_need_to_explicitly_borrowed/</link><author>/u/parkotron</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 20:31:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a long time C++ dev, I feel it didn't take me very long to pick up Rust's reference semantics and borrowing rules, but there one place where I constantly find myself forgetting to include the : passing temporaries into functions taking references.fn foo(s: &str) { println!("The str is: {s}"); } fn bar() -> String { "temporary".to_string() } fn main() { foo(&bar()); // ^ I always forget this ampersand until reminded by the compiler. } Rust's explicit  and  operators make a lot of sense to me: given a chunk of code, it should be obvious where a value has been borrowed and what kind of borrow it is. One should never be surprised to learn a reference was taken, because it's right there in the code.But in the case of temporary values, it really doesn't matter, does it? Whatever a function call does (or doesn't) do to a temporary value passed to it, the effect cannot be observed in the surrounding code, since the temporary is gone by the end of the statement.Is there a subtlety I'm missing here? Does that ampersand on a temporary convey useful information to an experienced Rust dev? Or is it really just syntactic noise, as it seems to me? Are there corner cases I'm just not considering? Could a future edition of Rust be changed to implicitly borrow from temporaries (like it implicitly borrows to make method calls)? Is my mental model just wrong?To be perfectly clear, this isn't a criticism, just curiosity. Clearly a lot of thought has been put into the language's design and syntax. This is just the only place I've encountered where Rust's explicitness doesn't feel completely justified.]]></content:encoded></item><item><title>## Master Django Redirects in Under 3 Minutes ðŸš€</title><link>https://dev.to/ebereplenty/-master-django-redirects-in-under-3-minutes-4f6</link><author>NJOKU SAMSON EBERE</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 20:13:15 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Django provides a powerful way to redirect users from one page to another using the  function. Whether you need to handle authentication flows, restructure URLs, or improve user experience, understanding Django Redirects is essential. In this quick tutorial, weâ€™ll break it down step by step.
  
  
  ðŸ”¹ What is Django Redirect?
A  in Django is a way to send users from one URL to another automatically. This is useful for scenarios like:Redirecting users after login/logout.Moving outdated URLs to new locations.Handling conditional navigation.Django simplifies this with the  function, which is commonly used in .
  
  
  ðŸ“Œ Using Django's  Function
Django provides the  function in , which allows redirection using:A  (recommended for better maintainability)An  (optional)Example 1: Redirect to a Static URLExample 2: Redirect Using a Named RouteUsing  ensures flexibility if URLs change later.Example 3: Redirect with Parameters
  
  
  ðŸ”¥ Best Practices for Django Redirects
âœ…  Helps in maintaining URLs dynamically.
âœ…  Ensure the redirect doesnâ€™t point back to the same page.
âœ…  Default is  (temporary), but you can use  when needed:
  
  
  ðŸŽ¥ Watch the Full Tutorial
Want to see this in action? Watch my  on YouTube where I explain everything in under  ðŸš€ðŸ”” Subscribe for More Django Content!Djangoâ€™s  function is an essential tool for managing user navigation efficiently. By leveraging named routes and best practices, you can create seamless and user-friendly experiences.Do you use redirects in your Django projects? Drop a comment below and share your use case! ðŸ‘‡#Django #DjangoRedirect #Python #WebDevelopment #DjangoTutorial #LearnDjango #Coding #BackendDevelopment #PythonProgramming]]></content:encoded></item><item><title>Geoblocking the UK with Debian &amp; Nginx</title><link>https://aphyr.com/posts/379-geoblocking-the-uk-with-debian-nginx</link><author>Aphyr</author><category>dev</category><category>go</category><pubDate>Thu, 20 Feb 2025 19:45:55 +0000</pubDate><source url="http://aphyr.com/posts.atom">Aphyr</source><content:encoded><![CDATA[A few quick notes for other folks who are geoblocking the UK. I just set up a basic geoblock with Nginx on Debian. This is all stuff you can piece together, but the Maxmind and Nginx docs are a little vague about the details, so I figure itâ€™s worth an actual writeup. My Nginx expertise is ~15 years out of date, so this might not be The Best Way to do things. YMMV.First, register for a free MaxMind account; youâ€™ll need this to subscribe to their GeoIP database. Then set up a daemon to maintain a copy of the lookup file locally, and Nginxâ€™s GeoIP2 module:apt install geoipupdate libnginx-mod-http-geoip2
Create a license key on the MaxMind site, and download a copy of the config file youâ€™ll need. Drop that in . Itâ€™ll look like:AccountID XXXX
LicenseKey XXXX
EditionIDs GeoLite2-Country
The package sets up a cron job automatically, but we should grab an initial copy of the file. This takes a couple minutes, and writes out /var/lib/GeoIP/GeoLite2-Country-mmdb:The GeoIP2 module should already be loaded via /etc/nginx/modules-enabled/50-mod-http-geoip2.conf. Add a new config snippet like /etc/nginx/conf.d/geoblock.conf. The first part tells Nginx where to find the GeoIP database file, and then extracts the two-letter ISO country code for each request as a variable. The  part sets up an  variable, which is set to  for GB, otherwise .geoip2 /var/lib/GeoIP/GeoLite2-Country.mmdb {
  $geoip2_data_country_iso_code country iso_code;
}

map $geoip2_data_country_iso_code $osa_geoblocked {
  GB      1;
  default 0;
}
Write an HTML file somewhere like /var/www/custom_errors/osa.html, explaining the block. Then serve that page for HTTP 451 status codes: in /etc/nginx/sites-enabled/whatever, add:server {
  ...
  # UK OSA error page
  error_page 451 /osa.html;
  location /osa.html {
    internal;
    root /var/www/custom_errors/;
  }

  # When geoblocked, return 451
  location / {
    if ($osa_geoblocked = 1) {
      return 451;
    }
  }
}
Test your config with , and then . You can test how things look from the UK using a VPN service, or something like locabrowser.This is, to be clear, a bad solution. MaxMindâ€™s free database is not particularly precise, and in general IP lookup tables are chasing a moving target. I know for a fact that there are people in non-UK countries (like Ireland!) who have been inadvertently blocked by these lookup tables. Making those people use Tor or a VPN , but I donâ€™t know what else to do in the current regulatory environment.]]></content:encoded></item><item><title>N19-Crypt and N19-Chain: Development of a cryptographic algorithm,a blockchain prototype inspired by the number 19 from Quran</title><link>https://dev.to/inquisitive41/n19-crypt-and-n19-chain-development-of-a-cryptographic-algorithma-blockchain-prototype-inspired-a9h</link><author>Inquisitive41</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 19:34:13 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Liquid syntax error: Variable '{{sender, receiver, amount}' was not properly terminated with regexp: /\}\}/]]></content:encoded></item><item><title>Rate my photo manipulation tool</title><link>https://www.reddit.com/r/golang/comments/1iu6pch/rate_my_photo_manipulation_tool/</link><author>/u/tunerhd</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 19:16:41 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/tunerhd ]]></content:encoded></item><item><title>AI can fix bugsâ€”but canâ€™t find them: OpenAIâ€™s study highlights limits of LLMs in software engineering</title><link>https://venturebeat.com/ai/ai-can-fix-bugs-but-cant-find-them-openais-study-highlights-limits-of-llms-in-software-engineering/</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 19:13:24 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn MoreIn a new paper, OpenAI researchers detail how they developed an LLM benchmark called SWE-Lancer to test how much foundation models can earn from real-life freelance software engineering tasks. The test found that, while the models can solve bugs, they canâ€™t see why the bug exists and continue to make more mistakes.Â The researchers tasked three LLMs â€” OpenAIâ€™s GPT-4o and o1 and Anthropicâ€™s Claude-3.5 Sonnet â€” with 1,488 freelance software engineer tasks from the freelance platform Upwork amounting to $1 million in payouts. They divided the tasks into two categories: individual contributor tasks (resolving bugs or implementing features), and management tasks (where the model roleplays as a manager who will choose the best proposal to resolve issues).Â â€œResults indicate that the real-world freelance work in our benchmark remains challenging for frontier language models,â€ the researchers write.Â The test shows that foundation models cannot fully replace human engineers. While they can help solve bugs, theyâ€™re not quite at the level where they can start earning freelancing cash by themselves.Â Benchmarking freelancing modelsThe researchers and 100 other professional software engineers identified potential tasks on Upwork and, without changing any words, fed these to a Docker container to create the SWE-Lancer dataset. The container does not have internet access and cannot access GitHub â€œto avoid the possible of models scraping code diffs or pull request details,â€ they explained. The team identified 764 individual contributor tasks, totaling about $414,775, ranging from 15-minute bug fixes to weeklong feature requests. These tasks, which included reviewing freelancer proposals and job postings, would pay out $585,225.The tasks were added to the expensing platform Expensify.Â The researchers generated prompts based on the task title and description and a snapshot of the codebase. If there were additional proposals to resolve the issue, â€œwe also generated a management task using the issue description and list of proposals,â€ they explained. From here, the researchers moved to end-to-end test development. They wrote Playwright tests for each task that applies these generated patches which were then â€œtriple-verifiedâ€ by professional software engineers.â€œTests simulate real-world user flows, such as logging into the application, performing complex actions (making financial transactions) and verifying that the modelâ€™s solution works as expected,â€ the paper explains.Â After running the test, the researchers found that none of the models earned the full $1 million value of the tasks. Claude 3.5 Sonnet, the best-performing model, earned only $208,050 and resolved 26.2% of the individual contributor issues. However, the researchers point out, â€œthe majority of its solutions are incorrect, and higher reliability is needed for trustworthy deployment.â€The models performed well across most individual contributor tasks, with Claude 3.5-Sonnet performing best, followed by o1 and GPT-4o.Â â€œAgents excel at localizing, but fail to root cause, resulting in partial or flawed solutions,â€ the report explains. â€œAgents pinpoint the source of an issue remarkably quickly, using keyword searches across the whole repository to quickly locate the relevant file and functions â€” often far faster than a human would. However, they often exhibit a limited understanding of how the issue spans multiple components or files, and fail to address the root cause, leading to solutions that are incorrect or insufficiently comprehensive. We rarely find cases where the agent aims to reproduce the issue or fails due to not finding the right file or location to edit.â€Interestingly, the models all performed better on manager tasks that required reasoning to evaluate technical understanding.These benchmark tests showed that AI models can solve some â€œlow-levelâ€ coding problems and canâ€™t replace â€œlow-levelâ€ software engineers yet. The models still took time, often made mistakes, and couldnâ€™t chase a bug around to find the root cause of coding problems. Many â€œlow-levelâ€ engineers work better, but the researchers said this may not be the case for very long.Â ]]></content:encoded></item><item><title>TwinSong: Jupyter notebook built from scratch in Rust</title><link>https://www.reddit.com/r/rust/comments/1iu5tpa/twinsong_jupyter_notebook_built_from_scratch_in/</link><author>/u/winter-moon</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 18:40:46 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've spent a lot of time working with Python in Jupyter notebooks, but one thing has always bothered me: the way code and outputs are mixed together. While this is great for tutorials and interactive documentation, it's less ideal for exploratory work or data processing, where I just want to interact with Python without the constraints of a document-style interface. To address this, I created TwinSong, a Jupyter alternative that separates code and outputs. Right now, it's primarily a UX experiment, but core features like editing and executing cells are already in place. Instead of modifying Jupyter's existing codebase, I built it from scratch with a React frontend and a Rust backend.While performance wasn't the main focus, implementing a Python kernel driver in Rust keeps the kernel clean and avoids loading Python dependencies that might interfere with user code. Plus, as we've seen with other projects, rewriting classic Python tools in Rust can open up new possibilities.]]></content:encoded></item><item><title>[D] Enriching token embedding with last hidden state?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/</link><author>/u/Academic_Sleep1118</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 18:06:18 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Looking at a decoder transformer working process from an information theory standpoint, we can see that the information available in the last hidden state is collapsed into a single token during generation. It means that you collapse a hidden state that, in theory, has about: (or whatever quant) bits of information to something like:I wonder if it's a good thing (sorry for the naive phrasing). The information used by a transformer to predict the next token is entirely stored in its context window and does not involve any recurrent state. So, predicting the next token of a sequence the transformer was just fed with is going to yield the exact same result as doing so for the same sequence if it were entirely generated by the transformer itself.Fair enough, in some sense: whether the sequence was generated or just read doesn't change anything about what the next token should be.But on the other hand, this approach means that  the information flow between tokens has to happen through the attention mechanism. There's no way for the transformer to embed some nuance or flavor into the predicted token embedding. Like in:"Well, I predicted the token 'When the next token is predicted, this nuance that was likely present in the last hidden state (or even in the softmaxed output probability distribution) is totally lost.So while I was having a little walk yesterday, I was thinking that it might be a good idea to add some information to the token embeddings using something like:augmented_embedding = embedding(token) + F(last_hidden_state)(It would be important to make sure that:â€–F(last_hidden_state)â€– â‰ª â€–embedding(token)â€–I have tried to find papers on this subject and asked for feedback from Claude, ChatGPT, and Perplexity. told me it was "an incredibly insightful idea." hallucinated a paper on the subject. gave me a very long list of totally unrelated sources.So I'm turning to you guys. I would love it if some big-brained guy told me why other big-brained guys decided not to follow this idea, or why it doesn't work.Here are some things I identified as potentially problematic:Transformers are nice to train with heavy parallelization precisely because they are not recursive. Each sequence of size  can give  independent training examples. Injecting last hidden states' information in token embeddings would break some of that parallelization.It would still be possible to train it efficiently, I guess.First, take the () vanilla sequences and get the predictions.Then, for each prediction, store the last hidden state and update the corresponding token embedding in each of the sequences where it appears.Now, you have a new set of training sequences, with all (but the first) token embeddings updated.You can repeat this process indefinitely. I hope it converges ^^This really looks like a diffusion process, by the way. That brings me to the next point:Here, I am not very competent. What are the conditions that define such a process' stability? My uneducated guess is that if you keep:â€–last_hidden_state_contributionâ€– â‰ª â€–augmented_token_embeddingâ€– you should not have many problems. But it would also limit the information flow. I guess there's a trade-off, and I wouldn't be surprised if it's not good enough.What do you guys think? Has this already been tried somewhere? Is there a fundamental reason this wouldn't work?]]></content:encoded></item><item><title>Data Scientist, Data Engineer, or Technology Manager: Which Job Is Right for You?</title><link>https://www.kdnuggets.com/2025/02/nwu/data-scientist-data-engineer-or-technology-manager-which-job-is-right-for-you</link><author>KDnuggets</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/MSDS_775x500-8.jpg" length="" type=""/><pubDate>Thu, 20 Feb 2025 18:00:44 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Whatever role is best for youâ€”data scientist, data engineer, or technology managerâ€”Northwestern University's MS in Data Science program will help you to prepare for the jobs of today and the jobs of the future.]]></content:encoded></item><item><title>Thoughts on an AI powered bipedal, musculoskeletal , anatomically accurate, synthetic human with over 200 degrees of freedom, over 1,000 Myofibers, and 500 sensors?</title><link>https://v.redd.it/b1iwrsu32cke1</link><author>/u/VivariuM_007</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 17:57:23 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Learn Faster, Code Smarter! 25+ Programming Resources to Boost Your Skills!</title><link>https://dev.to/dev-resources/learn-faster-code-smarter-25-programming-resources-to-boost-your-skills-54k8</link><author>Dev Resources</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 17:56:59 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[You can get free giveaway products - here1. DIY API vs. Marketplace API: The 2025 Ultimate Innovation Showdown2. How to Write API Documentation That Developers Love in 2025API documentation is like a user manual that tells other developers how to use your provided... 3. Donâ€™t Build Another App Until You Read This: Shocking Comparisons of 2025 Mobile Tech Stacks!50 AI-Powered Money-Making... 4. The Most Powerful grouping Operations in History, Bar NoneGrouping is a common structured data calculation, with corresponding statements and functions... 5. Study Reveals Major Gaps in AI Models' Basic Math Skills - Even GPT-4 Struggles with Simple CountingStudy Reveals Major Gaps in AI Models' Basic Math Skills - Even GPT-4 Struggles with Simple Counting 6. What are Topics and Partitions in Kafka?What is a Topic?   A Topic is Kafka's fundamental building block for organizing messages.... 7. Dispatchers e Contextos no Kotlin: Escolhendo o Lugar Certo para Suas Corrotinas1 â€“ IntroduÃ§Ã£o   As corrotinas no Kotlin sÃ£o uma soluÃ§Ã£o moderna para programaÃ§Ã£o... 8. Dispatchers and Contexts in Kotlin: Choosing the Right Place for Your Coroutines1 â€“ Introduction   Coroutines in Kotlin are a modern solution for asynchronous programming.... 9. AI System Learns When to Consult Experts for Better Cause-and-Effect DiscoveryAI System Learns When to Consult Experts for Better Cause-and-Effect Discovery 10. AI-Powered Code Explorer Creates Better Software Through Systematic Solution SearchAI-Powered Code Explorer Creates Better Software Through Systematic Solution Search 11. Breakthrough Method Extends Neural Network Learning Phase and Improves Deep Model TrainingBreakthrough Method Extends Neural Network Learning Phase and Improves Deep Model Training 12. AI Brings Still Photos to Life with Natural Facial Animations in Groundbreaking ResearchAI Brings Still Photos to Life with Natural Facial Animations in Groundbreaking Research 13. Breakthrough: Continuous Diffusion Creates More Natural Language AI with Better PerformanceBreakthrough: Continuous Diffusion Creates More Natural Language AI with Better Performance 14. New Study Shows Current AI Models Fail Basic Physics Tests, Highlighting Major Limitations in Scientific ReasoningNew Study Shows Current AI Models Fail Basic Physics Tests, Highlighting Major Limitations in Scientific Reasoning 15. Large Language Models Extract Information Without Training, Matching Specialized SystemsLarge Language Models Extract Information Without Training, Matching Specialized Systems 16. Making AI Safer: New Methods to Control Step-by-Step AI ReasoningMaking AI Safer: New Methods to Control Step-by-Step AI Reasoning 17. New Test Reveals Major Gaps in AI's Economic Reasoning Skills - Study of 27 Language Models Shows Mixed ResultsNew Test Reveals Major Gaps in AI's Economic Reasoning Skills - Study of 27 Language Models Shows Mixed Results Have you ever wanted to make use of Google Chrome on your mobile and seem to be stuck? I would solve... 19. How AI Helps Reduce False Positives in Cyber Threat Detection?Cybersecurity threats are evolving at an unprecedented pace, and organizations must stay ahead to... 20. Best Free Currency Converter APIs for Developers: Features & Integration GuideApplications often require real-time currency exchange data to support international transactions,... 21. How technical seo can transform your search rankings (Beginner's Checklist)Search Engine Optimization (SEO) is often divided into three key areas: on-page SEO, off-page SEO,... 22. ðŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-20ðŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-20   Every week, thousands of... 23. Full-Stack Development Trends 2024: Edge Computing, AI Tools, and WebAssembly ExplainedDiscover the essential full-stack development trends shaping 2024. Learn how edge computing, AI tools, and WebAssembly are transforming development practices. Get insights on modern architectures and tools. 24. Open Source Ai Agents: Exploring Best Ai AgentsArtificial Intelligence (AI) has transformed industries worldwide, automating tasks, enhancing... 25. The Role of Audit Firms in Dubai: Ensuring Financial Transparency and ComplianceDubai is a thriving global business hub, attracting entrepreneurs and investors from around the... 26. How Trumpâ€™s Tariffs Pump Volatility and Dump Crypto Exchangesâ€™ CapitalizationsThe original article is published on CoinMarketCap           Analyzing the real impact of Trumpâ€™s... 27. How to Create Custom Helper Functions in LaravelIn this tutorial, weâ€™ll walk through the process of creating and using custom helper functions in a... 28. Southwest Airlines Office in ColumbusThe Southwest Airlines Office in Columbus serves as a key hub for customer support and airline... 29. Why Choose DumpsBoss for Your TEAS-Test Preparation?When it comes to exam preparation, the right resources can make all the difference. DumpsBoss has... 30. ðŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-20ðŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-20   Every week, thousands of... 
  
  
  50 AI-Powered Money-Making Prompts for Bloggers: Maximize Your Blog's Revenue ðŸš€
If you're serious about making money from your blog, you already know that AI can be a game-changerâ€”but only if you use it the right way. Thatâ€™s exactly why I created this handpicked collection of 50 high-impact ChatGPT prompts specifically for bloggers who want to boost their revenue, grow their traffic, and scale their content effortlessly.
  
  
  Why This is Different from Any Other Prompt Pack?
Most AI prompt lists are generic and too broad to be useful. This one is built for bloggers who actually want to make moneyâ€”whether itâ€™s through ad revenue, affiliate marketing, sponsored content, or product sales.Each prompt is fully customizable with dynamic fields, meaning you can tailor them to your niche, audience, and goals in just a few seconds. No guesswork, no wasted timeâ€”just AI-driven strategies that work.âœ”ï¸ 50 expert-crafted ChatGPT prompts focused on blog monetization
âœ”ï¸ Fully customizable prompts (swap in your niche, topic, and audience)
âœ”ï¸ Instant access in PDF format â€“ download and start using immediatelyðŸ”¹ Bloggers who want better content that converts
ðŸ”¹ Affiliate marketers looking for high-converting blog post ideas
ðŸ”¹ Content creators who want to save time while making money1ï¸âƒ£ Open the PDF and choose a prompt
2ï¸âƒ£ Customize it with your niche or topic
3ï¸âƒ£ Use it in ChatGPT to generate money-making blog content instantlyNo fluff, no fillerâ€”just 50 prompts that help you create content that makes money.ðŸš€ Grab your copy now and start boosting your blogâ€™s revenue today!
  
  
  ðŸ’° Want to Earn 40% Commission?Join our affiliate program and start making money by promoting ! Earn 40% on every sale you refer.  You'll get on average around 5$ per sell and for bundled products it will be around 40$ per sale. (So just share it and make money with worrying about product creation and maintanence)]]></content:encoded></item><item><title>Show HN: WinCse â€“ Integrating AWS S3 with Windows Explorer</title><link>https://github.com/cbh34680/WinCse</link><author>cbh34680</author><category>dev</category><category>hn</category><pubDate>Thu, 20 Feb 2025 17:53:34 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[WinCse is an application that integrates AWS S3 buckets with Windows Explorer. Utilizing WinFsp and the AWS SDK, WinCse allows you to treat S3 buckets as part of your local file system, making file management simpler. The application is currently in development, with plans for additional features and improvements.]]></content:encoded></item><item><title>Generate synthetic counterparty (CR) risk data with generative AI using Amazon Bedrock LLMs and RAG</title><link>https://aws.amazon.com/blogs/machine-learning/generate-synthetic-counterparty-cr-risk-data-with-generative-ai-using-amazon-bedrock-llms-and-rag/</link><author>Santosh Kulkarni</author><category>dev</category><category>ai</category><pubDate>Thu, 20 Feb 2025 17:24:25 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Data is the lifeblood of modern applications, driving everything from application testing to machine learning (ML) model training and evaluation. As data demands continue to surge, the emergence of generative AI models presents an innovative solution. These large language models (LLMs), trained on expansive data corpora, possess the remarkable capability to generate new content across multiple media formatsâ€”text, audio, and videoâ€”and across various business domains, based on provided prompts and inputs.In this post, we explore how you can use these LLMs with advanced Retrieval Augmented Generation (RAG) to generate high-quality synthetic data for a finance domain use case. You can use the same technique for synthetic data for other business domain use cases as well. For this post, we demonstrate how to generate counterparty risk (CR) data, which would be beneficial for over-the-counter (OTC) derivatives that are traded directly between two parties, without going through a formal exchange.OTC derivatives are typically customized contracts between counterparties and include a variety of financial instruments, such as forwards, options, swaps, and other structured products. A counterparty is the other party involved in a financial transaction. In the context of OTC derivatives, the counterparty refers to the entity (such as a bank, financial institution, corporation, or individual) with whom a derivative contract is made.For example, in an OTC swap or option contract, one entity agrees to terms with another party, and each entity becomes the counterparty to the other. The responsibilities, obligations, and risks (such as credit risk) are shared between these two entities according to the contract.As financial institutions continue to navigate the complex landscape of CR, the need for accurate and reliable risk assessment models has become paramount. For our use case, ABC Bank, a fictional financial services organization, has taken on the challenge of developing an ML model to assess the risk of a given counterparty based on their exposure to OTC derivative data.Building such a model presents numerous challenges. Although ABC Bank has gathered a large dataset from various sources and in different formats, the data may be biased, skewed, or lack the diversity needed to train a highly accurate model. The primary challenge lies in collecting and preprocessing the data to make it suitable for training an ML model. Deploying a poorly suited model could result in misinformed decisions and significant financial losses.We propose a generative AI solution that uses the RAG approach. RAG is a widely used approach that enhances LLMs by supplying extra information from external data sources not included in their original training. The entire solution can be broadly divided into three steps: indexing, data generation, and validation.In the indexing step, we parse, chunk, and convert the representative CR data into vector format using the Amazon Titan Text Embeddings V2 model and store this information in a Chroma vector database. Chroma is an open source vector database known for its ease of use, efficient similarity search, and support for multimodal data and metadata. It offers both in-memory and persistent storage options, integrates well with popular ML frameworks, and is suitable for a wide range of AI applications. It is particularly beneficial for smaller to medium-sized datasets and projects requiring local deployment or low resource usage. The following diagram illustrates this architecture.Here are the steps for data indexing:The sample CR data is segmented into smaller, manageable chunks to optimize it for embedding generation.These segmented data chunks are then passed to a method responsible for both generating embeddings and storing them efficiently.The Amazon Titan Text Embeddings V2 API is called upon to generate high-quality embeddings from the prepared data chunks.The resulting embeddings are then stored in the Chroma vector database, providing efficient retrieval and similarity searches for future use.When the user requests data for a certain scenario, the request is converted into vector format and then looked up in the Chroma database to find matches with the stored data. The retrieved data is augmented with the user request and additional prompts to Anthropicâ€™s Claude Haiku on Amazon Bedrock. Anthropicâ€™s Claude Haiku was chosen primarily for its speed, processing over 21,000 tokens per second, which significantly outpaces its peers. Moreover, Anthropicâ€™s Claude Haikuâ€™s efficiency in data generation is remarkable, with a 1:5 input-to-output token ratio. This means it can generate a large volume of data from a relatively small amount of input or context. This capability not only enhances the modelâ€™s effectiveness, but also makes it cost-efficient for our application, where we need to generate numerous data samples from a limited set of examples. Anthropicâ€™s Claude Haiku LLM is invoked iteratively to efficiently manage token consumption and help prevent reaching the maximum token limit. The following diagram illustrates this workflow.Here are the steps for data generation:The user initiates a request to generate new synthetic counterparty risk data based on specific criteria.The Amazon Titan Text Embeddings V2 LLM is employed to create embeddings for the userâ€™s request prompts, transforming them into a machine-interpretable format.These newly generated embeddings are then forwarded to a specialized module designed to identify matching stored data.The Chroma vector database, which houses previously stored embeddings, is queried to find data that closely matches the userâ€™s request.The identified matching data and the original user prompts are then passed to a module responsible for generating new synthetic data.Anthropicâ€™s Claude Haiku 3.0 model is invoked, using both the matching embeddings and user prompts as input to create high-quality synthetic data.The generated synthetic data is then parsed and formatted into a .csv file using the Pydantic library, providing a structured and validated output.To confirm the quality of the generated data, several statistical methods are applied, including quantile-quantile (Q-Q) plots and correlation heat maps of key attributes, providing a comprehensive validation process.When validating the synthetic CR data generated by the LLM, we employed Q-Q plots and correlation heat maps focusing on key attributes such as , , and . These statistical tools serve crucial roles in promoting the quality and representativeness of the synthetic data. By using the Q-Q plots, we can assess whether these attributes follow a normal distribution, which is often expected in many clinical and financial variables. By comparing the quantiles of our synthetic data against theoretical normal distributions, we can identify significant deviations that might indicate bias or unrealistic data generation.Simultaneously, the correlation heat maps provide a visual representation of the relationships between these attributes and others in the dataset. This is particularly important because it helps verify that the LLM has maintained the complex interdependencies typically observed in real CR data. For instance, we would expect certain correlations between exposure and replacement cost, or between replacement cost and settlement risk. By making sure these correlations are preserved in our synthetic data, we can be more confident that analyses or models built on this data will yield insights that are applicable to real-world scenarios. This rigorous validation process helps to mitigate the risk of introducing artificial patterns or biases, thereby enhancing the reliability and utility of our synthetic CR dataset for subsequent research or modeling tasks.Weâ€™ve created a Jupyter notebook containing three parts to implement the key components of the solution. We provide code snippets from the notebooks for better understanding.To set up the solution and generate test data, you should have the following prerequisites:Python 3 must be installed on your machineWe recommend that an integrated development environment (IDE) that can run Jupyter notebooks be installedYou can also create a Jupyter notebook instance using Amazon SageMaker from AWS console and develop the code there.You need to have an AWS account with access to Amazon Bedrock and the following LLMs enabled (be careful not to share the AWS account credentials): 
  Amazon Titan Text Embeddings V2Anthropicâ€™s Claude 3 HaikuHere are the steps to setup the environment.import sys!{sys.executable} -m pip install -r requirements.txtThe content of the requirements.txt is given here.boto3
langchain
langchain-community
streamlit
chromadb==0.4.15
numpy
jq
langchain-aws
seaborn
matplotlib
scipyThe following code snippet will perform all the necessary imports.from pprint import pprint 
from uuid import uuid4 
import chromadb 
from langchain_community.document_loaders import JSONLoader 
from langchain_community.embeddings import BedrockEmbeddings
from langchain_community.vectorstores import Chroma 
from langchain_text_splitters import RecursiveCharacterTextSplitterIndex data in the Chroma databaseIn this section, we show how indexing of data is done in a Chroma database as a locally maintained open source vector store. This index data is used as context for generating data.The following code snippet shows the preprocessing steps of loading the JSON data from a file and splitting it into smaller chunks:def load_using_jsonloaer(path):
    loader = JSONLoader(path,
                            jq_schema=".[]",
                            text_content=False)
    documents = loader.load()
    return documents

def split_documents(documents):
    doc_list = [item for item in documents]
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=0)
    texts = text_splitter.split_documents(doc_list)
    return textsThe following snippet shows how an Amazon Bedrock embedding instance is created. We used the Amazon Titan Embeddings V2 model:def get_bedrock_embeddings():
    aws_region = "us-east-1"
    model_id = "amazon.titan-embed-text-v2:0" #look for latest version of model
    bedrock_embeddings = BedrockEmbeddings(model_id=model_id, region_name=aws_region)
    return bedrock_embeddingsThe following code shows how the embeddings are created and then loaded in the Chroma database:persistent_client = chromadb.PersistentClient(path="../data/chroma_index")
collection = persistent_client.get_or_create_collection("test_124")
print(collection)
    #     query the database
vector_store_with_persistent_client = Chroma(collection_name="test_124",
                                                 persist_directory="../data/chroma_index",
                                                 embedding_function=get_bedrock_embeddings(),
                                                 client=persistent_client)
load_json_and_index(vector_store_with_persistent_client)The following code snippet shows the configuration used during the LLM invocation using Amazon Bedrock APIs. The LLM used is Anthropicâ€™s Claude 3 Haiku:config = Config(
    region_name='us-east-1',
    signature_version='v4',
    retries={
        'max_attempts': 2,
        'mode': 'standard'
    }
)
bedrock_runtime = boto3.client('bedrock-runtime', config=config)
model_id = "anthropic.claude-3-haiku-20240307-v1:0" #look for latest version of model
model_kwrgs = {
    "temperature": 0,
    "max_tokens": 8000,
    "top_p": 1.0,
    "top_k": 25,
    "stop_sequences": ["company-1000"],
}
# Initialize the language model
llm = ChatBedrock(
    model_id=model_id,
    model_kwargs=model_kwrgs,
    client=bedrock_runtime,
)The following code shows how the context is fetched by looking up the Chroma database (where data was indexed) for matching embeddings. We use the same Amazon Titan model to generate the embeddings:def get_context(scenario):
    region_name = 'us-east-1'
    credential_profile_name = "default"
    titan_model_id = "amazon.titan-embed-text-v2:0"
    kb_context = []
    be = BedrockEmbeddings(region_name=region_name,
                           credentials_profile_name=credential_profile_name,
                           model_id=titan_model_id)

    vector_store = Chroma(collection_name="test_124", persist_directory="../data/chroma_index",
                      embedding_function=be)
    search_results = vector_store.similarity_search(scenario, k=3)
    for doc in search_results:
        kb_context.append(doc.page_content)
    return json.dumps(kb_context)The following snippet shows how we formulated the detailed prompt that was passed to the LLM. We provided examples for the context, scenario, start index, end index, records count, and other parameters. The prompt is subjective and can be adjusted for experimentation.# Create a prompt template
prompt_template = ChatPromptTemplate.from_template(
    "You are a financial data expert tasked with generating records "
    "representing company OTC derivative data and "
    "should be good enough for investor and lending ML model to take decisions "
    "and data should accurately represent the scenario: {scenario} \n "
    "and as per examples given in context: "
    "and context is {context} "
    "the examples given in context is for reference only, do not use same values while generating dataset."
    "generate dataset with the diverse set of samples but record should be able to represent the given scenario accurately."
    "Please ensure that the generated data meets the following criteria: "
    "The data should be diverse  and realistic, reflecting various industries, "
    "company sizes, financial metrics. "
    "Ensure that the generated data follows logical relationships and correlations between features "
    "(e.g., higher revenue typically corresponds to more employees, "
    "better credit ratings, and lower risk). "
    "And Generate {count} records starting from index {start_index}. "
    "generate just JSON as per schema and do not include any text or message before or after JSON. "
    "{format_instruction} \n"
    "If continuing, start after this record: {last_record}\n"
    "If stopping, do not include this record in the output."
    "Please ensure that the generated data is well-formatted and consistent."
)The following code snippet shows the process for generating the synthetic data. You can call this method in an iterative manner to generate more records. The input parameters include , and . The response data is also formatted into CSV format using the instruction provided by the following:output_parser.get_format_instructions():

 def generate_records(start_index, count, scenario, context, last_record=""):
    try:
        response = chain.invoke({
            "count": count,
            "start_index": start_index,
            "scenario": scenario,
            "context": context,
            "last_record": last_record,
            "format_instruction": output_parser.get_format_instructions(),
            "data_set_class_schema": DataSet.schema_json()
        })
        
        return response
    except Exception as e:
        print(f"Error in generate_records: {e}")
        raise eParsing the output generated by the LLM and representing it in CSV was quite challenging. We used a Pydantic parser to parse the JSON output generated by the LLM, as shown in the following code snippet:class CustomPydanticOutputParser(PydanticOutputParser):
    def parse(self, text: str) -> BaseModel:
        # Extract JSON from the text
        try:
            # Find the first occurrence of '{'
            start = text.index('{')
            # Find the last occurrence of '}'
            end = text.rindex('}') + 1
            json_str = text[start:end]

            # Parse the JSON string
            parsed_json = json.loads(json_str)

            # Use the parent class to convert to Pydantic object
            return super().parse_with_cls(parsed_json)
        except (ValueError, json.JSONDecodeError) as e:
            raise ValueError(f"Failed to parse output: {e}")The following code snippet shows how the records are generated in an iterative manner with 10 records in each invocation to the LLM:def generate_full_dataset(total_records, batch_size, scenario, context):
    dataset = []
    total_generated = 0
    last_record = ""
    batch: DataSet = generate_records(total_generated,
                                      min(batch_size, total_records - total_generated),
                                      scenario, context, last_record)
    # print(f"batch: {type(batch)}")
    total_generated = len(batch.records)
    dataset.extend(batch.records)
    while total_generated < total_records:
        try:
            batch = generate_records(total_generated,
                                     min(batch_size, total_records - total_generated),
                                     scenario, context, batch.records[-1].json())
            processed_batch = batch.records

            if processed_batch:
                dataset.extend(processed_batch)
                total_generated += len(processed_batch)
                last_record = processed_batch[-1].start_index
                print(f"Generated {total_generated} records.")
            else:
                print("Generated an empty or invalid batch. Retrying...")
                time.sleep(10)
        except Exception as e:
            print(f"Error occurred: {e}. Retrying...")
            time.sleep(5)

    return dataset[:total_records]  # Ensure exactly the requested number of recordsVerify the statistical properties of the generated dataWe generated Q-Q plots for key attributes of the generated data: , , and , as shown in the following screenshots. The Q-Q plots compare the quantiles of the data distribution with the quantiles of a normal distribution. If the data isnâ€™t skewed, the points should approximately follow the diagonal line.As the next step of verification, we created a corelation heat map of the following attributes: , , , and . The plot is perfectly balanced with the diagonal elements showing a value of 1. The value of 1 indicates the column is perfectly co-related to itself. The following screenshot is the correlation heatmap.Itâ€™s a best practice to clean up the resources you created as part of this post to prevent unnecessary costs and potential security risks from leaving resources running. If you created the Jupyter notebook instance in SageMaker please complete the following steps:Save and shut down the notebook: # First save your work
# Then close all open notebooks by clicking File -> Close and Halt Clear the output (if needed before saving): # Option 1: Using notebook menu
# Kernel -> Restart & Clear Output

# Option 2: Using code
from IPython.display import clear_output
clear_output()Stop and delete the Jupyter notebook instance created in SageMaker: # Option 1: Using aws cli
# Stop the notebook instance when not in use
aws sagemaker stop-notebook-instance --notebook-instance-name <your-notebook-name>

# If you no longer need the notebook instance
aws sagemaker delete-notebook-instance --notebook-instance-name <your-notebook-name>

# Option 2: Using Sagemager Console
# Amazon Sagemaker -> Notebooks
# Select the Notebook and click Actions drop-down and hit Stop.
Click Actions drop-down and hit DeleteResponsible AI use and data privacy are paramount when using AI in financial applications. Although synthetic data generation can be a powerful tool, itâ€™s crucial to make sure that no real customer information is used without proper authorization and thorough anonymization. Organizations must prioritize data protection, implement robust security measures, and adhere to relevant regulations. Additionally, when developing and deploying AI models, itâ€™s essential to consider ethical implications, potential biases, and the broader societal impact. Responsible AI practices include regular audits, transparency in decision-making processes, and ongoing monitoring to help prevent unintended consequences. By balancing innovation with ethical considerations, financial institutions can harness the benefits of AI while maintaining trust and protecting individual privacy.In this post, we showed how to generate a well-balanced synthetic dataset representing various aspects of counterparty data, using RAG-based prompt engineering with LLMs. Counterparty data analysis is imperative for making OTC transactions between two counterparties. Because actual business data in this domain isnâ€™t easily available, using this approach you can generate synthetic training data for your ML models at minimal cost often within minutes. After you train the model, you can use it to make intelligent decisions before entering into an OTC derivative transaction.For more information about this topic, refer to the following resources: is a Senior Moderation Architect with over 16 years of experience, specialized in developing serverless, container-based, and data architectures for clients across various domains. Santoshâ€™s expertise extends to machine learning, as a certified AWS ML specialist. Currently, engaged in multiple initiatives leveraging AWS Bedrock and hosted Foundation models. is a Senior Modernization Architect with AWS ProServe and specializes in building secure and scalable cloud native application for customers from different industry domains. He has developed an interest in the AI/ML space particularly leveraging Gen AI capabilities available on Amazon Bedrock. is a Senior Specialist Solutions Architect for generative AI and machine learning at AWS. Mallik works with customers to help them architect efficient, secure and scalable AI and machine learning applications. Mallik specializes in generative AI services Amazon Bedrock and Amazon SageMaker.]]></content:encoded></item><item><title>DAY 02: PYTHON PROGRAMMING (2/20/2025)</title><link>https://dev.to/prashantcod/day-02-python-programming-2202025-3n48</link><author>Prashant Gyawali</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 17:16:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[â€”> Interactive mode in Python                                â€”>ROUND THE FLOATING POINT VALUE â€”>Using  to sort the functions as you like ]]></content:encoded></item><item><title>Turbocharging premium audit capabilities with the power of generative AI: Veriskâ€™s journey toward a sophisticated conversational chat platform to enhance customer support</title><link>https://aws.amazon.com/blogs/machine-learning/turbocharging-premium-audit-capabilities-with-the-power-of-generative-ai-verisks-journey-toward-a-sophisticated-conversational-chat-platform-to-enhance-customer-support/</link><author>Sajin Jacob, Jerry Chen, Siddarth Mohanram, Luis Barbier, Kristen Chenowith and Michelle Stahl</author><category>dev</category><category>ai</category><pubDate>Thu, 20 Feb 2025 17:13:17 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This post is co-written with Sajin Jacob, Jerry Chen, Siddarth Mohanram, Luis Barbier, Kristen Chenowith, and Michelle Stahl from Verisk.Verisk (Nasdaq: VRSK) is a leading data analytics and technology partner for the global insurance industry. Through advanced analytics, software, research, and industry expertise across more than 20 countries, Verisk helps build resilience for individuals, communities, and businesses. The company is committed to ethical and responsible AI development with human oversight and transparency. Verisk is using generative AI to enhance operational efficiencies and profitability for insurance clients while adhering to its ethical AI principles.Veriskâ€™s Premium Audit Advisory Service (PAASÂ®) is the leading source of technical information and training for premium auditors and underwriters. PAAS helps users classify exposure for commercial casualty insurance, including general liability, commercial auto, and workersâ€™ compensation. PAAS offers a wide range of essential services, including more than 40,000 classification guides and more than 500 bulletins. PAAS now includes PAAS AI, the first commercially available interactive generative-AI chats specifically developed for premium audit, which reduces research time and empower users to make informed decisions by answering questions and quickly retrieving and summarizing multiple PAAS documents like class guides, bulletins, rating cards, etc.In this post, we describe the development of the customer support process in PAAS, incorporating generative AI, the data, the architecture, and the evaluation of the results. Conversational AI assistants are rapidly transforming customer and employee support. Verisk has embraced this technology and developed its own PAAS AI, which provides an enhanced self-service capability to the PAAS platform.The Verisk PAAS platform houses a vast array of documentsâ€”including class guides, advisory content, and bulletinsâ€”that aid Veriskâ€™s customers in determining the appropriate rules and classifications for workersâ€™ compensation, general liability, and commercial auto business. When premium auditors need accurate answers within this extensive document repository, the challenges they face are: â€“ The sheer volume of documents (advisories, bulletins, and so on) makes manual searching time-consuming and inefficient â€“ Finding accurate information within this vast repository can be slow, hindering timely decision-makingInconsistent quality of responses â€“ Manual searches might yield irrelevant or incomplete results, leading to uncertainty and potential errorsTo address this issue, Verisk PAAS AI is designed to alleviate the burden by providing round-the-clock support for business processing and delivering precise and quick responses to customer queries. This technology is deeply integrated into Veriskâ€™s newly reimagined PAAS platform, using all of Veriskâ€™s documentation, training materials, and collective expertise. It employs a retrieval augmented generation (RAG) approach and a combination of AWS services alongside proprietary evaluations to promptly answer most user questions about the capabilities of the Verisk PAAS platform.When deployed at scale, this PAAS AI will enable Verisk staff to dedicate more time to complex issues, critical projects, and innovation, thereby enhancing the overall customer experience. Throughout the development process, Verisk encountered several considerations, key findings, and decisions that provide valuable insights for any enterprise looking to explore the potential of generative AI.When creating an interactive agent using large language models (LLMs), two common approaches are RAG and model fine-tuning. The choice between these methods depends on the specific use case and available data. Verisk PAAS began developing a RAG pipeline for its PAAS AI and has progressively improved this solution. Here are some reasons why continuing with a RAG architecture was beneficial for Verisk: â€“ The PAAS platform is constantly evolving, adding new business functions and technical capabilities. Verisk needed to make sure its responses are based on the most current information. The RAG approach allows access to continuously updated data, providing responses with the latest information without frequently retraining the model. â€“ Besides data recency, another crucial aspect is the ability to draw from multiple PAAS resources to acquire relevant context. The ease of expanding the knowledge base without the need for fine-tuning new data sources makes the solution adaptable. â€“ Retrieval minimizes the risk of hallucinations compared with free-form text generation because responses come directly from the provided excerpts. Verisk developed an evaluation tool to enhance response quality. â€“ Although appropriate context can be retrieved from enterprise data sources, the underlying LLM manages the linguistics and fluency. â€“ Verisk aimed to consistently improve the PAAS AIâ€™s response generation ability. A RAG architecture offered the transparency required in the context retrieval process, which would ultimately be used to generate user responses. This transparency helped Verisk identify areas where document restructuring was needed. â€“ With diverse users accessing the platform and differing data access permissions, data governance and isolation were critical. Verisk implemented controls within the RAG pipeline to restrict data access based on user permissions, helping to ensure that responses are delivered only to authorized users.Although both RAG and fine-tuning have their pros and cons, RAG is the best approach for building a PAAS AI on the PAAS platform, given Veriskâ€™s needs for real-time accuracy, explainability, and configurability. The pipeline architecture supports iterative enhancement as the use cases for the Verisk PAAS platform develop.The following diagram showcases a high-level architectural data flow that highlights various AWS services used in constructing the solution. Veriskâ€™s system demonstrates a complex AI setup, where multiple components interact and frequently call on the LLM to provide user responses. Employing the PAAS platform to manage these varied components was an intuitive decision.The key components are as follows:Veriskâ€™s PAAS team determined that ElastiCache is the ideal solution for storing all chat history. This storage approach allows for seamless integration in conversational chats and enables the display of recent conversations on the website, providing an efficient and responsive user experience.Anthropicâ€™s Claude, available in Amazon Bedrock, played various roles within Veriskâ€™s solution: â€“ When building their PAAS AI, Verisk conducted a comprehensive evaluation of leading LLMs, using their extensive dataset to test each modelâ€™s capabilities. Through Amazon Bedrock, Verisk gained streamlined access to multiple best-in-class foundation models (FMs), enabling efficient testing and comparison across key performance criteria. The Amazon Bedrock unified API and robust infrastructure provided the ideal platform to develop, test, and deploy LLM solutions at scale. After this extensive testing, Verisk found Anthropicâ€™s Claude model consistently outperformed across key criteria. Anthropicâ€™s Claude demonstrated superior language understanding in Veriskâ€™s complex business domain, allowing more pertinent responses to user questions. Given the modelâ€™s standout results across Verisk PAAS platform use cases, it was the clear choice to power the PAAS AIâ€™s natural language capabilities.Conversation summarization â€“ When a user asks a follow-up question, the PAAS AI can continue the conversational thread. To enable this, Verisk used Claude to summarize the dialogue to update the context from ElastiCache. The full conversation summary and new excerpts are input to the LLM to generate the next response. This conversational flow allows the PAAS AI to answer user follow-up questions and have a more natural, contextual dialogue, bringing Verisk PAAS closer to having a true AI assistant that can engage in useful, back-and-forth conversations with users. â€“ Keywords are extracted from user questions and previous conversations to be used for creating the new summarized prompt and to be input to Veriskâ€™s knowledge base retrievers to perform vector similarity search.Amazon OpenSearch ServicePrimarily used for the storage of text embeddings, OpenSearch facilitates efficient document retrieval by enabling rapid access to indexed data. These embeddings serve as semantic representations of documents, allowing for advanced search capabilities that go beyond simple keyword matching. This semantic search functionality enhances the systemâ€™s ability to retrieve relevant documents that are contextually similar to the search queries, thereby improving the overall accuracy and speed of data queries. Additionally, OpenSearch functions as a semantic cache for similarity searches, optimizing performance by reducing the computational load and improving response times during data retrieval operations. This makes it an indispensable tool in the larger PAAS ecosystem, where the need for quick and precise information access is paramount.The integration of Snowflake in the PAAS AI ecosystem helps provide scalable and real-time access to data, allowing Verisk to promptly address customer concerns and improve its services. By using Snowflakeâ€™s capabilities, Verisk can perform advanced analytics, including sentiment analysis and predictive modeling, to better understand customer needs and enhance user experiences. This continuous feedback loop is vital for refining the PAAS AI and making sure it remains responsive and relevant to user demands.Structuring and retrieving the dataAn essential element in developing the PAAS AIâ€™s knowledge base was properly structuring and effectively querying the data to deliver accurate answers. Verisk explored various techniques to optimize both the organization of the content and the methods to extract the most relevant information: â€“ A key step in preparing the accumulated questions and answers was splitting the data into individual documents to facilitate indexing into OpenSearch Service. Rather than uploading large files containing multiple pages of content, Verisk chunked the data into smaller segments by document section and character lengths. By splitting the data into small, modular chunks focused on a single section of a document, Verisk could more easily index each document and had greater success in pulling back the correct context. Chunking the data also enabled straightforward updating and reindexing of the knowledge base over time. â€“ When querying the knowledge base, Verisk found that using just standard vector search wasnâ€™t enough to retrieve all the relevant contexts pertaining to a question. Therefore, a solution was implemented to combine a sparse bm25 search in combination with the dense vector search to create a hybrid search approach, which yielded much better context retrieval results.Data separation and filters â€“ Another issue Verisk ran into was that, because of the vast amount of documents and the overlapping content within certain topics, incorrect documents were being retrieved for some questions that asked for specific topics that were present across multiple sourcesâ€”some of these werenâ€™t needed or appropriate in the context of the userâ€™s question. Therefore, data separation was implemented to split the documents based on document type and filter by line of business to improve context retrieval within the application.By thoroughly experimenting and optimizing both the knowledge base powering the PAAS AI and the queries to extract answers from it, Verisk was able to achieve very high answer accuracy during the proof of concept, paving the way for further development. The techniques exploredâ€”hybrid querying, HTML section chunking, and index filteringâ€”became core elements of Veriskâ€™s approach for extracting quality contexts.LLM parameters and modelsExperimenting with prompt structure, length, temperature, role-playing, and context was key to improving the quality and accuracy of the PAAS AIâ€™s Claude-powered responses. The prompt design guidelines provided by Anthropic were incredibly helpful.Verisk crafted prompts that provided Anthropicâ€™s Claude with clear context and set roles for answering user questions. Setting the temperature to 0 helped reduce the randomness and indeterministic nature of LLM-generated responses.Verisk also experimented with different models to improve the efficiency of the overall solution. For scenarios where latency was more important and less reasoning was required, Anthropicâ€™s Claude Haiku was the perfect solution. For other scenarios such as question answering using provided contexts where it was more important for the LLM to be able to understand every detail given in the prompt, Anthropicâ€™s Claude Sonnet was the better choice to balance latency, performance, and cost.LLM guardrails were implemented in the PAAS AI project using both the guardrails provided by Amazon Bedrock and specialized sections within the prompt to detect unrelated questions and prompt attack attempts. Amazon Bedrock guardrails can be attached to any Amazon Bedrock model invocation call and automatically detect if the given model input and output are in violation of the language filters that are set (violence, misconduct, sexual, and so on), which helps with screening user inputs. The specialized prompts further improve LLM security by creating a second net that uses the power of the LLMs to catch any inappropriate inputs from the users.This allows Verisk to be confident that the model will only answer to its intended purpose surrounding premium auditing services and will not be misused by threat actors.After validating several evaluation tools such as Deepeval, Ragas, Trulens, and so on, the Verisk PAAS team realized that there were certain limitations to using these tools for their specific use case. Consequently, the team decided to develop its own evaluation API, shown in the above figure.This custom API evaluates the answers based on three major metrics: â€“ Using LLMs, the process assesses whether the answers provided are relevant to the customerâ€™s prompt. This helps make sure that the responses are directly addressing the questions posed. â€“ By using LLMs, the process evaluates whether the context retrieved is appropriate and aligns well with the question. This helps make sure that the LLM has the appropriate and accurate contexts to generate a response. â€“ Using LLMs, the process checks if the responses are generated based on their retrieved context or if they are hallucinated. This is crucial for maintaining the integrity and reliability of the information provided.This custom evaluation approach helps make sure that the answers generated are not only relevant and contextually appropriate but also faithful to the established generative AI knowledge base, minimizing the risk of misinformation. By incorporating these metrics, Verisk has enhanced the robustness and reliability of their PAAS AI, providing customers with accurate and trustworthy responses.The Verisk PAAS team has implemented a comprehensive feedback loop mechanism, shown in the above figure, to support continuous improvement and address any issues that might arise.This feedback loop is structured around the following key components:Customer feedback analysis â€“ The team actively collects and analyzes feedback from customers to identify potential data issues or problems with the generative AI responses. This analysis helps pinpoint specific areas that need improvement. â€“ After an issue is identified, itâ€™s categorized based on its nature. If itâ€™s a data-related issue, itâ€™s assigned to the internal business team for resolution. If itâ€™s an application issue, a Jira ticket is automatically created for the PAAS IT team to address and fix the problem. â€“ The system provides an option to update QA test cases based on the feedback received. This helps make sure that the test scenarios remain relevant and comprehensive, covering a wide range of potential issues. â€“ Ground truth agreements, which serve as the benchmark for evaluating LLM response quality, are periodically reviewed and updated. This helps make sure that the evaluation metrics remain accurate and reflective of the desired standards. â€“ Regular evaluations of the LLM responses are conducted using the updated QA test cases and ground truth agreements. This helps in maintaining high-quality responses and quickly addressing any deviations from the expected standards.This robust feedback loop mechanism enables Verisk to continuously fine-tune the PAAS AI, making sure that it delivers precise, relevant, and contextually appropriate answers to customer queries. By integrating customer feedback, categorizing issues efficiently, updating test scenarios, and adhering to stringent evaluation protocols, Verisk maintains a high standard of service and drives continuous improvement in its generative AI capabilities.Verisk initially rolled out the PAAS AI to one beta customer to demonstrate real-world performance and impact. Supporting a customer in this way is a stark contrast to how Verisk has historically engaged with and supported customers in the past, where Verisk would typically have a team allocated to interact with the customer directly. Veriskâ€™s PAAS AI has revolutionized the way subject matter experts (SMEs) work and cost-effectively scales while still providing high-quality assistance. What previously took hours of manual review can now be accomplished in minutes, resulting in an extraordinary 96â€“98% reduction in processing time per specialist. This dramatic improvement in efficiency not only streamline operations but also allows Veriskâ€™s experts to focus on more strategic initiatives that drive greater value for the organization.In analyzing this early usage data, Verisk uncovered additional areas where it can drive business value for its customers. As Verisk collects additional information, this data will help uncover what will be needed to improve results and prepare to roll out to a wider customer base of approximately 15,000 users.Ongoing development will focus on expanding these capabilities, prioritized based on the collected questions. Most exciting, though, are the new possibilities on the horizon with generative AI. Verisk knows this technology is rapidly advancing and is eager to harness innovations to bring even more value to customers. As new models and techniques emerge, Verisk plans to adapt the PAAS AI to take advantage of the latest capabilities. Although the PAAS AI currently focuses on responding to user questions, this is only the starting point. Verisk plans to quickly improve its capabilities to proactively make suggestions and configure functionality directly in the system itself. The Verisk PAAS team is inspired by the challenge of pushing the boundaries of whatâ€™s possible with generative AI and is excited to test those boundaries.Veriskâ€™s development of a PAAS AI for its PAAS platform demonstrates the transformative power of generative AI in customer support and operational efficiency. Through careful data harvesting, structuring, retrieval, and the use of LLMs, semantic search functionalities, and stringent evaluation protocols, Verisk has crafted a robust system that delivers accurate, real-time answers to user questions. By continuing to enhance the PAAS AIâ€™s features while maintaining ethical and responsible AI practices, Verisk is set to provide increased value to its customers, enable staff to concentrate on innovation, and establish new benchmarks for customer service in the insurance sector.For more information, see the following resources: is the Director of Software Engineering at Verisk, where he leads the Premium Audit Advisory Service (PAAS) development team. In this role, Sajin plays a crucial part in designing the architecture and providing strategic guidance to eight development teams, optimizing their efficiency and ensuring the maintainability of all solutions. He holds an MS in Software Engineering from Periyar University, India. is a Lead Software Developer at Verisk, based in Jersey City. He leads the GenAi development team, working on solutions for projects within the Verisk Underwriting department to enhance application functionalities and accessibility. Within PAAS, he has worked on the implementation of the conversational RAG architecture with enhancements such as hybrid search, guardrails, and response evaluations. Jerry holds a degree in Computer Science from Stevens Institute of Technology. is the Senior Vice President of Core Lines Technology at Verisk. His area of expertise includes data strategy, analytics engineering, and digital transformation. Sid is head of the technology organization with global teams across five countries. He is also responsible for leading the technology transformation for the multi-year Core Lines Reimagine initiative. Sid holds an MS in Information Systems from Stevens Institute of Technology. is the Chief Technology Officer (CTO) of Verisk Underwriting at Verisk. He provides guidance to the development teamsâ€™ architectures to maximize efficiency and maintainability for all underwriting solutions. Luis holds an MBA from Iona University., MSMSL, CPCU, WCP, APA, CIPA, AIS, is PAAS Product Manager at Verisk. She is currently the product owner for the Premium Audit Advisory Service (PAAS) product suite, including PAAS AI, a first to market generative AI chat tool for premium audit that accelerates research for many consultative questions by 98% compared to traditional methods. Kristen holds an MS in Management, Strategy and Leadership at Michigan State University and a BS in Business Administration at Valparaiso University. She has been in the commercial insurance industry and premium audit field since 2006., MBA, CPCU, AIM, API, AIS, is a Digital Product Manager with Verisk. She has over 20 years of experience building and transforming technology initiatives for the insurance industry. She has worked as a software developer, project manager, and product manager throughout her career.Arun Pradeep Selvaraj is a Senior Solutions Architect at AWS. Arun is passionate about working with his customers and stakeholders on digital transformations and innovation in the cloud while continuing to learn, build, and reinvent. He is creative, fast-paced, deeply customer-obsessed, and uses the working backward process to build modern architectures to help customers solve their unique challenges. Connect with him on LinkedIn. is a Solutions Architect Manager at AWS, based out of New York. He helps financial services customers accelerate their adoption of the AWS Cloud by providing architectural guidelines to design innovative and scalable solutions. Coming from a software development and sales engineering background, the possibilities that the cloud can bring to the world excite him., PhD, is a Senior Solutions Architect at AWS, based out of New York. He is aligned with the financial service industry, and is responsible for providing architectural guidelines to design innovative and scalable fintech solutions. He specializes in developing and commercializing artificial intelligence and machine learning products. Connect with him on LinkedIn.]]></content:encoded></item><item><title>Announcing Rust 1.85.0 and Rust 2024 | Rust Blog</title><link>https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html</link><author>/u/slanterns</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 17:11:19 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.85.0. This stabilizes the 2024 edition as well.
Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.85.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!We are excited to announce that the Rust 2024 Edition is now stable!
Editions are a mechanism for opt-in changes that may otherwise pose a backwards compatibility risk. See the edition guide for details on how this is achieved, and detailed instructions on how to migrate.This is the largest edition we have released. The edition guide contains detailed information about each change, but as a summary, here are all the changes:The guide includes migration instructions for all new features, and in general
transitioning an existing project to a new edition.
In many cases  can automate the necessary changes. You may even find that no changes in your code are needed at all for 2024!Note that automatic fixes via  are very conservative to avoid ever changing the semantics of your code. In many cases you may wish to keep your code the same and use the new semantics of Rust 2024; for instance, continuing to use the  macro matcher, and ignoring the conversions of conditionals because you want the new 2024 drop order semantics. The result of  should not be considered a recommendation, just a conservative conversion that preserves behavior. people came together to create this edition. We'd like to thank them all for their hard work!Rust now supports asynchronous closures like  which return futures when called. This works like an  which can also capture values from the local environment, just like the difference between regular closures and functions. This also comes with 3 analogous traits in the standard library prelude: , , and .In some cases, you could already approximate this with a regular closure and an asynchronous block, like . However, the future returned by such an inner block is not able to borrow from the closure captures, but this does work with  closures:let mut vec: Vec<String> = vec![];

let closure = async || {
    vec.push(ready(String::from("")).await);
};
It also has not been possible to properly express higher-ranked function signatures with the  traits returning a , but you can write this with the  traits:use core::future::Future;
async fn f<Fut>(_: impl for<'a> Fn(&'a u8) -> Fut)
where
    Fut: Future<Output = ()>,
{ todo!() }

async fn f2(_: impl for<'a> AsyncFn(&'a u8))
{ todo!() }

async fn main() {
    async fn g(_: &u8) { todo!() }
    f(g).await;
    //~^ ERROR mismatched types
    //~| ERROR one type is more general than the other

    f2(g).await; // ok!
}
Hiding trait implementations from diagnosticsThe new #[diagnostic::do_not_recommend] attribute is a hint to the compiler to not show the annotated trait implementation as part of a diagnostic message. For library authors, this is a way to keep the compiler from making suggestions that may be unhelpful or misleading. For example:pub trait Foo {}
pub trait Bar {}

impl<T: Foo> Bar for T {}

struct MyType;

fn main() {
    let _object: &dyn Bar = &MyType;
}
error[E0277]: the trait bound `MyType: Bar` is not satisfied
 --> src/main.rs:9:29
  |
9 |     let _object: &dyn Bar = &MyType;
  |                             ^^^^ the trait `Foo` is not implemented for `MyType`
  |
note: required for `MyType` to implement `Bar`
 --> src/main.rs:4:14
  |
4 | impl<T: Foo> Bar for T {}
  |         ---  ^^^     ^
  |         |
  |         unsatisfied trait bound introduced here
  = note: required for the cast from `&MyType` to `&dyn Bar`
For some APIs, it might make good sense for you to implement , and get  indirectly by that blanket implementation. For others, it might be expected that most users should implement  directly, so that  suggestion is a red herring. In that case, adding the diagnostic hint will change the error message like so:#[diagnostic::do_not_recommend]
impl<T: Foo> Bar for T {}
error[E0277]: the trait bound `MyType: Bar` is not satisfied
  --> src/main.rs:10:29
   |
10 |     let _object: &dyn Bar = &MyType;
   |                             ^^^^ the trait `Bar` is not implemented for `MyType`
   |
   = note: required for the cast from `&MyType` to `&dyn Bar`
 and  for tuplesEarlier versions of Rust implemented convenience traits for iterators of  tuple pairs to behave like , with  in 1.56 and  in 1.79. These have now been  to more tuple lengths, from singleton  through to 12 items long, . For example, you can now use  to fanout into multiple collections at once:use std::collections::{LinkedList, VecDeque};
fn main() {
    let (squares, cubes, tesseracts): (Vec<_>, VecDeque<_>, LinkedList<_>) =
        (0i32..10).map(|i| (i * i, i.pow(3), i.pow(4))).collect();
    println!("{squares:?}");
    println!("{cubes:?}");
    println!("{tesseracts:?}");
}
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
[0, 1, 8, 27, 64, 125, 216, 343, 512, 729]
[0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561]
Updates to  has been deprecated for years, because it can give surprising results in some Windows configurations if the  environment variable is set (which is not the normal configuration on Windows). We had previously avoided changing its behavior, out of concern for compatibility with code depending on this non-standard configuration. Given how long this function has been deprecated, we're now updating its behavior as a bug fix, and a subsequent release will remove the deprecation for this function.These APIs are now stable in const contextsMany people came together to create Rust 1.85.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>AI Agents from Zero to Hero â€“ Part 1</title><link>https://towardsdatascience.com/ai-agents-from-zero-to-hero-part-1/</link><author>Mauro Di Pietro</author><category>dev</category><category>ai</category><pubDate>Thu, 20 Feb 2025 17:04:03 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[ are autonomous programs that perform tasks, make decisions, and communicate with others. Normally, they use a set of tools to help complete tasks. In GenAI applications, these Agents process sequential reasoning and can use external tools (like web searches or database queries) when the LLM knowledge isnâ€™t enough. Unlike a basic chatbot, which generates random text when uncertain, an AI Agent activates tools to provide more accurate, specific responses.We are moving closer and closer to the concept of  systems that exhibit a higher level of autonomy and decision-making ability, without direct human intervention. While todayâ€™s AI Agents respond reactively to human inputs, tomorrowâ€™s Agentic AIs proactively engage in problem-solving and can adjust their behavior based on the situation.Today, building Agents from scratch is becoming as easy as training a logistic regression model 10 years ago. Back then,  provided a straightforward library to quickly train Machine Learning models with just a few lines of code, abstracting away much of the underlying complexity.In this tutorial, Iâ€™m going to show how to build from scratch different types of AI Agents, from simple to more advanced systems. I will present some useful Python code that can be easily applied in other similar cases (just copy, paste, run) and walk through every line of code with comments so that you can replicate this example.As I said, anyone can have a custom Agent running locally for free without GPUs or API keys. The only necessary library is (pip install ollama==0.4.7), as it allows users to run LLMs locally, without needing cloud-based services, giving more control over data privacy and performance.First of all, you need to download  from the website.Â Then, on the prompt shell of your laptop, use the command to download the selected LLM. Iâ€™m going with Alibabaâ€™s , as itâ€™s both smart and lite.After the download is completed, you can move on to Python and start writing code.import ollama
llm = "qwen2.5"stream = ollama.generate(model=llm, prompt='''what time is it?''', stream=True)
for chunk in stream:
    print(chunk['response'], end='', flush=True)Obviously, the LLM per se is very limited and it canâ€™t do much besides chatting. Therefore, we need to provide it the possibility to take action, or in other words, to .One of the most common tools is the ability to . In Python, the easiest way to do it is with the famous private browser (pip install duckduckgo-search==6.3.5). You can directly use the original library or import the  wrapper (pip install langchain-community==0.3.17).Â With , in order to use a Tool, the function must be described in a dictionary.from langchain_community.tools import DuckDuckGoSearchResults
def search_web(query: str) -> str:
  return DuckDuckGoSearchResults(backend="news").run(query)

tool_search_web = {'type':'function', 'function':{
  'name': 'search_web',
  'description': 'Search the web',
  'parameters': {'type': 'object',
                'required': ['query'],
                'properties': {
                    'query': {'type':'str', 'description':'the topic or subject to search on the web'},
}}}}
## test
search_web(query="nvidia")Internet searches could be very broad, and I want to give the Agent the option to be more precise. Letâ€™s say, Iâ€™m planning to use this Agent to learn about financial updates, so I can give it a specific tool for that topic, like searching only a finance website instead of the whole web.def search_yf(query: str) -> str:
Â Â engine = DuckDuckGoSearchResults(backend="news")
Â  return engine.run(f"site:finance.yahoo.com {query}")

tool_search_yf = {'type':'function', 'function':{
Â  'name': 'search_yf',
Â  'description': 'Search for specific financial news',
Â  'parameters': {'type': 'object',
Â  Â  Â  Â  Â  Â  Â  Â  'required': ['query'],
Â  Â  Â  Â  Â  Â  Â  Â  'properties': {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'query': {'type':'str', 'description':'the financial topic or subject to search'},
}}}}

## test
search_yf(query="nvidia")In my opinion, the most basic Agent should at least be able to choose between one or two Tools and re-elaborate the output of the action to give the user a proper and concise answer.Â First, you need to write a prompt to describe the Agentâ€™s purpose, the more detailed the better (mine is very generic), and that will be the first message in the chat history with the LLM.Â prompt = '''You are an assistant with access to tools, you must decide when to use tools to answer user message.'''Â 
messages = [{"role":"system", "content":prompt}]In order to keep the chat with the AI alive, I will use a loop that starts with userâ€™s input and then the Agent is invoked to respond (which can be a text from the LLM or the activation of a Tool).Up to this point, the chat history could look something like this:If the model wants to use a Tool, the appropriate function needs to be run with the input parameters suggested by the LLM in its response object:So our code needs to get that information and run the Tool function.Now, if we run the full code, we can chat with our Agent.LLMs know how to code by being exposed to a large corpus of both code and natural language text, where they learn patterns, syntax, and semantics of Programming languages. The model learns the relationships between different parts of the code by predicting the next token in a sequence. In short, LLMs can generate Python code but canâ€™t execute it, Agents can.I shall prepare a Tool allowing the Agent to . In Python, you can easily create a shell to run code as a string with the native command .import io
import contextlib

def code_exec(code: str) -> str:\
Â  Â  output = io.StringIO()
Â  Â  with contextlib.redirect_stdout(output):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  exec(code)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"Error: {e}")
Â  Â  return output.getvalue()

tool_code_exec = {'type':'function', 'function':{
Â  'name': 'code_exec',
Â  'description': 'execute python code',
Â  'parameters': {'type': 'object',
Â  Â  Â  Â  Â  Â  Â  Â  'required': ['code'],
Â  Â  Â  Â  Â  Â  Â  Â  'properties': {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'code': {'type':'str', 'description':'code to execute'},
}}}}

## test
code_exec("a=1+1; print(a)")Just like before, I will write a prompt, but this time, at the beginning of the chat-loop, I will ask the user to provide a file path.Since coding tasks can be a little trickier for LLMs, I am going to add also . By default, during one session, there isnâ€™t a true long-term memory. LLMs have access to the chat history, so they can remember information temporarily, and track the context and instructions youâ€™ve given earlier in the conversation. However, memory doesnâ€™t always work as expected, especially if the LLM is small. Therefore, a good practice is to reinforce the modelâ€™s memory by adding periodic reminders in the chat history.Please note that the default memory length in Ollama is 2048 characters. If your machine can handle it, you can increase it by changing the number when the LLM is invoked:Â  Â  ## model
Â  Â  agent_res = ollama.chat(
Â  Â  Â  Â  model=llm,
Â  Â  Â  Â  tools=[tool_code_exec],
Â  Â  Â  Â  options={"num_ctx":2048},
Â  Â  Â  Â  messages=messages)In this usecase, the output of the Agent is mostly code and data, so I donâ€™t want the LLM to re-elaborate the responses.Now, if we run the full code, we can chat with our Agent.This article has covered the foundational steps of creating Agents from scratch using only . With these building blocks in place, you are already equipped to start developing your own Agents for different use cases.Â , where we will dive deeper into more advanced examples.Full code for this article: I hope you enjoyed it! Feel free to contact me for questions and feedback or just to share your interesting projects.]]></content:encoded></item><item><title>Ugly Code and Dumb Things</title><link>https://lucumr.pocoo.org/2025/2/20/ugly-code/</link><author>/u/FoxInTheRedBox</author><category>dev</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 16:57:00 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[written on Thursday, February 20, 2025This week I had a conversation with one of our engineers about â€œshitty
codeâ€ which lead me to sharing with him one of my more unusual
inspirations: Flamework, a
pseudo framework created at Flickr.Two Passions, Two ApproachesThere are two driving passions in my work.  One is the love of creating
beautiful, elegant code â€” making Open Source libraries and APIs that focus
on clear design and reusability.  The other passion is building quick,
pragmatic solutions for real users (who may not even be developers).  The
latter usually in a setting of building a product, where the product is
not the code.  Here, speed and iteration matter more than beautiful code
or reusability, because success hinges on shipping something people want.Flamework is in service of the latter, and in crass violation of the
former.Early on, I realized that creating reusable code and directly solving
problems for users are often at odds.  My first clue came when I helped
run the German
ubuntuusers website.  It was powered by
a heavily modified version of phpBB, which despite how messy it was,
scaled to a large user base when patched properly.  It was messy, but easy
to adjust.  The abstractions were one layer deep.Back then, me and a friend tried to replace it by writing my own bulletin
board software, Pocoo.
Working in isolation, without users, led me down a path of
over-engineering.  While we learned a lot and ended up creating popular
Open Source libraries (like Jinja, Werkzeug and Pygments), Pocoo never
became a solid product.  Later, my collaborators and I rebuilt
ubuntuusers, without the
goal of making it into a reusable product.  That rewrite shipped
successfully and it lives to this very day.But it took me years to fully realize what was happening here: reusability
is not that important when youâ€™re building an application, but itâ€™s
crucial when youâ€™re building a library or framework.If you are unfamiliar with Flamework you should watch a talk that Cal
Henderson gave in 2008 at DjangoCon (Why I hate Django).  He talked about scale
and how Django didn't solve for it.  He enumerated all the things
important to him: sharding, using custom sequences for primary keys,
forgoing joins and foreign keys, supporting database replication setups,
denormalizing data to the extreme.  This is also were I first learned
about the possibility of putting all session data into cookies via
signing.  It was a memorable talk for me because it showed me that there
are shortcomings.  Django (which I used for ubuntuusers) had beautiful
APIs but at the time solved for little of that Cal needed.  The talk
really stuck with me.At the time of the talk, Flamework did not really exist.  It was more of
an idea and principles of engineering at Flickr.A few years later, Flamework appeared on GitHub, not as an open-sourced
piece of Flickr code but as a reimplementation of those same ideas.  You
can explore its repository and see code like this:Instinctively it makes me cringe.  Is that a SQL injection?  Well you were
supposed to use the PHP addslashes function
beforehand.  But notice how it caters to sharding and clustering directly
in the query function.Code like this often triggers a visceral reaction, especially in engineers
who prize clean design.How does something like that get created?  Cal Henderson described
Flickr's principle as â€œdoing the dumbest possible thing that will work.â€
Maybe â€œdumbâ€ is too strong â€” â€œsimpleâ€ might be more apt.  Yet simplicity
can look messy to someone expecting a meticulously engineered codebase.
This is not at all uncommon and I have seen it over and over.  The first
large commercial project that got traction that I ever worked on (Plurk) was also pretty pragmatic and
messy inside.  My former colleague Ben Vinegar also recently shared a story of early,
messy FreshBooks code and how he came to terms with it.  Same story at
Sentry.  We moved fast, we made a mess.None of this is surprising in retrospective.  Perfect code doesn't
guarantee success if you haven't solved a real problem for real people.
Pursuing elegance in a vacuum leads to abandoned side projects or
frameworks nobody uses.  By contrast, clunky but functional code often
comes with just the right compromises for quick iteration.  And that in
turn means a lot of messy code powers products that people love â€”
something that's a far bigger challenge.I have shown Flamework's code to multiple engineers over the years and it
usually creates such a visceral response.  It blind sights one by
seemingly disregarding all rules of good software engineering.That makes Flamework serve as a fascinating Rorschach test for engineers.
Are you looking at it with
admiration for the focus on some critical issues like scale, the built-in
observability and debugging tools.  Or are you judging it, and its
creators, for manually constructing SQL queries, using global variables,
not using classes and looking like messy PHP4 code?  Is it a pragmatic
tool, intentionally designed to iterate quickly at scale, or is it a naive
mess made by unskilled developers?Would I use Flamework?  Hello no.  But I appreciate the priorities behind
it.  If these ugly choices help you move faster, attract users and
validate the product, then a rewrite, or large refactorings later are a
small price to pay.At the end of the day, where you stand on â€œshitty codeâ€ depends on your
primary goal:Are you shipping a product and racing to meet user needs?Or are you building a reusable library or framework meant to stand the
test of time?Both mindsets are valid, but they rarely coexist harmoniously in a single
codebase.  Flamework is a reminder that messy, simple solutions can be
powerful if they solve real problems.  Eventually, when the time is right,
you can clean it up or rebuild from the ground up.The real challenge is deciding which route to take â€” and when.  Even with
experience, it is can be hard to know when to move from quick fixes to
more robust foundations.  The principles behind Flamework are also
reflected in Sentry's development philosophy.  One more
poignant one being â€œEmbrace the Duct Tapeâ€.  Yet as Sentry matured, much
of our duct tape didn't stand the test of time, and was re-applied at
moments when the real solution would have been a solid foundation poured
with concrete.That's because successful projects eventually grow up.  What let you
iterate fast in the beginning might eventually turn into an unmaintainable
mess and will be rebuilt from the inside out.I personally would never have built Flamework, it repulses me a bit.  At the
same time, I have a enormous respect for the people who build it.  Their
work and thinking has shaped how I solve problems and think of product
engineering.]]></content:encoded></item><item><title>Python input() Function - Detailed Explanation</title><link>https://dev.to/vayolapradeep/python-input-function-detailed-explanation-5b8m</link><author>vayola pradeep</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 16:48:21 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Python input() Function - Detailed Explanation]]></content:encoded></item><item><title>ðŸš€ The Best 100% French Hosting Solution â€“ Unlimited &amp; High-Performance! ðŸ‡«ðŸ‡·</title><link>https://dev.to/jmegnidro/the-best-100-french-hosting-solution-unlimited-high-performance-1mdm</link><author>Dominique Megnidro</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 16:13:21 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[If you're looking for a 100% French hosting provider offering , , and responsive customer support, then  is the perfect choice for you! ðŸ’¡  âœ…  (storage, databases, emails...)Powerful and optimized serversSimple and intuitive interfaceUltra-responsive customer support ðŸ‡«ðŸ‡·  I host several projects with them, and I can assure you their value for money is unbeatable! ðŸ’¯  If you have any questions about hosting or need advice, feel free to reach out! ðŸ˜Š]]></content:encoded></item><item><title>Call Center Wondr by BNI 0821-4448-0002</title><link>https://dev.to/vanglevan/call-center-wondr-by-bni-0821-4448-0002-1pjn</link><author>Vang LE</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 16:08:31 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Nomor Resmi Call Center Wondr BNI 0821-4448-0002. Untuk informasi lebih lanjut, nasabah dapat menghubungi BNI Call Center di 0821-4448-0002.Untuk mendapatkan informasi lebih lanjut mengenai gangguan pada bni mobile atau wondr by bni, nasabah dapat menghubungi CS Wondr BNI 0821-4448-0002.]]></content:encoded></item><item><title>ðŸš€ Besoin d&apos;un hÃ©bergement web performant et fiable ? DÃ©couvrez o2switch ! ðŸš€</title><link>https://dev.to/jmegnidro/besoin-dun-hebergement-web-performant-et-fiable-decouvrez-o2switch--4i3l</link><author>Dominique Megnidro</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 16:08:01 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Si vous cherchez un hÃ©bergeur 100% franÃ§ais, offrant des performances optimales, une bande passante illimitÃ©e et un support client rÃ©actif, alors o2switch est fait pour vous ! ðŸ’¡âœ… HÃ©bergement illimitÃ© (stockage, bases de donnÃ©es, emails...)
âœ… Serveurs puissants et optimisÃ©s
âœ… Interface simple et intuitive
âœ… Support client ultra-rÃ©actif ðŸ‡«ðŸ‡·Jâ€™hÃ©berge plusieurs projets chez eux et je peux vous assurer que leur rapport qualitÃ©/prix est imbattable ! ðŸ’¯ðŸ’° Profitez-en dÃ¨s maintenant en passant par ce lien : cliquez iciSi vous avez des questions sur lâ€™hÃ©bergement ou besoin de conseils, nâ€™hÃ©sitez pas Ã  me contacter ! ðŸ˜Š]]></content:encoded></item><item><title>C equivalent of select() / poll() in go socket programming</title><link>https://www.reddit.com/r/golang/comments/1iu1ugj/c_equivalent_of_select_poll_in_go_socket/</link><author>/u/ChestPainGuy</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 16:00:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, I'm fairly new to socket programming and go, so forgive my ignorance.Recently, I have been reading up Beej's guide to network programming, where he explains the use of  and  to read and write to multiple sockets without blocking.I have googled quite a bit, but almost every tutorial or go example on the basics of socket connections just spawn a new goroutine with something like .So whats' the equivalent of  in go?Is spawning a goroutine for every connection an effective approach?Any good links to network programming in go would be appreciated if this question is too dumb. Thanks]]></content:encoded></item><item><title>Is anyone working on AI designed to preserve democracy?</title><link>https://www.reddit.com/r/artificial/comments/1iu1n10/is_anyone_working_on_ai_designed_to_preserve/</link><author>/u/BarbaGramm</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 15:51:52 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Iâ€™m looking for people or groups who are already working on something like this:A decentralized AI trained to preserve the intellectual, historical, and emotional essence of democracyâ€”what it actually means, not just what future regimes might redefine it to be. Think of it as a fusion of data hoarding, decentralized AI, and resistance tech, built to withstand authoritarian drift and historical revisionism.Maybe it doesn't reach the heights of the corporate or state models, but a system that can always articulate the deltaâ€”the difference between a true democratic society (or at least what we seem to be leaving behind) and whatever comes next. If democracy gets twisted into something unrecognizable, this AI should be able to compare, contrast, and remind people what was lost. It should be self-contained, offline-capable, decentralized, and resistant to censorshipâ€”an incorruptible witness to history.Does this exist? Are there people in AI, decentralized infrastructure, or archival communities working toward something like this? I donâ€™t want to reinvent the wheel if a community is already building it. If you know of any projects, frameworks, or people tackling this problem, please point me in the right direction.If no one is doing it, shouldn't this be a project people are working on? Is there an assumption that corporate or state controlled AI will do this inherently?]]></content:encoded></item><item><title>ErrGroup: Unlocking Go&apos;s Concurrency Power</title><link>https://dev.to/leapcell/errgroup-unlocking-gos-concurrency-power-3g2h</link><author>Leapcell</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:43:30 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[ is a utility in the official Go library  used for concurrently executing multiple  and handling errors. It implements  based on , providing more powerful functions for concurrent programming.Compared with ,  has the following advantages::  is only responsible for waiting for the  to complete and does not handle return values or errors. While  cannot directly handle return values, it can immediately cancel other running  when a  encounters an error and return the first non- error in the  method.:  can be used in conjunction with . When a  encounters an error, it can automatically cancel other , effectively controlling resources and avoiding unnecessary work.Simplifying Concurrent Programming: Using  can reduce the boilerplate code for error handling. Developers do not need to manually manage error states and synchronization logic, making concurrent programming simpler and more maintainable.Limiting the Number of Concurrency:  provides an interface to limit the number of concurrent  to avoid overloading, which is a feature that  does not have.
  
  
  Example of Using sync.WaitGroup
Before introducing , let's first review the usage of .$ go run examples/main.go
fetch url http://www.google.com/ status 200 OK
fetch url http://www.golang.org/ status 200 OK
Error: Get "http://www.somestupidname.com/": dial tcp: lookup www.somestupidname.com: no such host
Typical idiom of :
  
  
  Example of Using errgroup.Group
The usage pattern of  is similar to that of .$ go run examples/main.go
fetch url http://www.google.com/ status 200 OK
fetch url http://www.golang.org/ status 200 OK
Error: Get "http://www.somestupidname.com/": dial tcp: lookup www.somestupidname.com: no such host
 provides  to add a cancellation function.$ go run examples/withcontext/main.go
Error:  Get "http://www.somestupidname.com/": dial tcp: lookup www.somestupidname.com: no such host
fetch url http://www.google.com/ status 200 OK

  
  
  Limiting the Number of Concurrency
 provides  to limit the number of concurrently executing .$  go run examples/main.go
Goroutine 3 is starting
Goroutine 1 is starting
Goroutine 2 is starting
Goroutine 2 is done
Goroutine 1 is done
Goroutine 5 is starting
Goroutine 3 is done
Goroutine 6 is starting
Goroutine 4 is starting
Goroutine 6 is done
Goroutine 5 is done
Goroutine 8 is starting
Goroutine 4 is done
Goroutine 7 is starting
Goroutine 9 is starting
Goroutine 9 is done
Goroutine 8 is done
Goroutine 10 is starting
Goroutine 7 is done
Goroutine 10 is done
All goroutines complete.
 provides  to try to start a task, which needs to be used in conjunction with .$ go run examples/main.go
Goroutine 1 started successfully
Goroutine 1 is starting
Goroutine 2 is starting
Goroutine 2 started successfully
Goroutine 3 started successfully
Goroutine 4 could not start (limit reached)
Goroutine 5 could not start (limit reached)
Goroutine 6 could not start (limit reached)
Goroutine 7 could not start (limit reached)
Goroutine 8 could not start (limit reached)
Goroutine 9 could not start (limit reached)
Goroutine 10 could not start (limit reached)
Goroutine 3 is starting
Goroutine 2 is done
Goroutine 3 is done
Goroutine 1 is done
All goroutines complete.

  
  
  Source Code Interpretation
The source code of  mainly consists of 3 files:: An empty structure used to pass signals to control the number of concurrency.:

: The function called when the context is cancelled.: The internally used .: The signal channel that controls the number of concurrent coroutines.: Ensures that the error is handled only once.: Records the first error.: Limits the number of concurrency.
: Starts a new coroutine to execute the task.
: Waits for all tasks to complete and returns the first error.
: Tries to start a task.
 is an official extended library that adds error handling capabilities on the basis of , providing functions such as synchronization, error propagation, and context cancellation. Its  method can add a cancellation function,  can limit the number of concurrency, and  can try to start a task. The source code is ingeniously designed and worthy of reference. Finally, I would like to recommend the most suitable platform for deploying golang: 
  
  
  1. Multi-Language Support
Develop with JavaScript, Python, Go, or Rust.

  
  
  2. Deploy unlimited projects for free
pay only for usage â€” no requests, no charges.
  
  
  3. Unbeatable Cost Efficiency
Pay-as-you-go with no idle charges.
Example: $25 supports 6.94M requests at a 60ms average response time.

  
  
  4. Streamlined Developer Experience
Intuitive UI for effortless setup.
Fully automated CI/CD pipelines and GitOps integration.
Real-time metrics and logging for actionable insights.

  
  
  5. Effortless Scalability and High Performance
Auto-scaling to handle high concurrency with ease.
Zero operational overhead â€” just focus on building.
]]></content:encoded></item><item><title>PaginaciÃ³n con cursor</title><link>https://dev.to/gaston_duarte/paginacion-con-cursor-341d</link><author>Gaston Duarte</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:40:23 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[Al desarrollar aplicaciones que muestran grandes volÃºmenes de datos, es fundamental implementar tÃ©cnicas de paginaciÃ³n para mejorar el rendimiento y la experiencia del usuario. En un e-commerce, por ejemplo, podrÃ­a haber miles de productos en la base de datos, pero el frontend solo necesita mostrar 10 a la vez.Enviar todos los registros al frontend para que los almacene en memoria no es una soluciÃ³n eficiente. En su lugar, el backend debe gestionar la paginaciÃ³n de manera efectiva.Existen dos enfoques principales para paginar datos:  y .Este mÃ©todo utiliza un  para determinar desde quÃ© registro comenzar la consulta.Veamos un ejemplo simple:Supongamos que tenemos 25 registros en nuestra base de datos, y tenemos una vista que quiere mostrar esos registros de a 10.Entonces el frontend nos enviara:{
  "limit": 10,
  "offset": 0
}
El backend harÃ¡ una consulta a la base de datos:SELECT * FROM nombre_tabla LIMIT 10 OFFSET 0Es decir, mostrar 10 registros desde el registro 0 (del 0..9). 
Para obtener los siguientes 10 registros:{
  "limit": 10,
  "offset": 10
}
SELECT * FROM nombre_tabla LIMIT 10 OFFSET 10Eso mostrara desde el registro 10 al 19 y asÃ­ sucesivamente.
  
  
  Problemas de Baja eficiencia en grandes volÃºmenes de datos: Consultas con  grande pueden volverse costosas, ya que la base de datos debe recorrer muchos registros antes de devolver los deseados.Inconsistencias en datos dinÃ¡micos: Si se agregan o eliminan registros, la paginaciÃ³n puede omitir o duplicar registros.AquÃ­ es donde aparece la paginaciÃ³n con cursor, la cual hace que nuestras consulta a la base de datos sean mucho mas performantes.En vez de enviar el campo , utilizaremos un campo .
  
  
  Â¿Y como definimos cual es nuestro cursor?
Bien, la respuesta es depende. Depende de quÃ© datos tenemos guardados en nuestro registro, vayamos al caso mas simple, tener un identificador Ãºnico auto-incremental.Supongamos que nuestro registro tiene estos datos:{
  "id": 1,
  "nombre": "Juan"
},
{
  "id": 2,
  "nombre": "Patricia"
},
...
El frontend solicita el primer registro con  (sin cursor) y el backend responderÃ¡ ademas del registro a mostrar cual es el cursor que debe enviar en la siguiente request:{
  "limit": 1,
  "cursor": 2
  "user":{
       "id": 1,
       "nombre": "Juan"
  }
}
Consulta a la base de datos:SELECT * FROM nombre_tabla LIMIT 1En la siguiente request que haga el frontend enviara,  y , entonces desde el backend podremos hacer una consulta a la base de datos de este estilo:SELECT * FROM nombre_tabla WHERE id >= 2 ORDER BY id LIMIT 1Lo cual traerÃ¡ a partir del registro que contenta id >= 2 y solamente 1.
  
  
  Â¿Cual es la ventaja sobre offset?
: No se recorren registros innecesarios. Simplemente lo limitamos en el .: No se ven afectados registros por insert o delete.
  
  
  Ahora, Â¿QuÃ© sucede si no tenemos un id auto-incremental y ordenado?
Si los registros tienen un  en lugar de un ID incremental, se puede utilizar otro campo como cursor, por ejemplo, :Por ejemplo, supongamos registros de este estilo:{
  "uuid": "asdn1029nc",
  "nombre": "Juan",
  "fecha_nacimiento": "2003-02-21"
},
{
  "uuid": "sap0238gh",
  "nombre": "Patricia",
  "fecha_nacimiento": "2002-11-04"
},
...
Para utilizar  debemos ordenar nuestros registros por algÃºn campo, en este caso la fecha de nacimiento, entonces ahora si el frontend nos pide un registro en la primer request, desde el backend devolveremos:{
  "limit": 1,
  "cursor": "2002-11-04"
  "user":{
       "uuid": "sap0238gh",
       "nombre": "Patricia",
       "fecha_nacimiento": "2002-11-04"
  }
},
...
Nuestra consulta a la base de datos serÃ¡:SELECT * FROM nombre_tabla WHERE fecha_nacimiento > '2002-11-04' ORDER BY fecha_nacimiento LIMIT 1
  
  
  Â¿Y que sucede si hay dos registros con la misma fecha, vamos a perder registros?
Bueno, ahi es donde podemos concatenar dos campos del registro para utilizarlo como cursor para asegurarnos de no perder registros, por ejemplo: cursor=fecha_nacimiento+uuid. Importante siempre en la consulta hacer un order by cursor, fecha_nacimiento.
  
  
  Seguridad: Encodear el cursor
Es importante utilizar un  en  de nuestro cursor para evitar un . 
Este puede ser un ejemplo de cÃ³digo en Go para encodear el cursor:func DecodeCursor(encoded string) (string, string, error) {
    data, err := base64.StdEncoding.DecodeString(encoded)
    if err != nil {
        return "", "", err
    }
    parts := strings.Split(string(data), "|")
    if len(parts) != 2 {
        return "", "", fmt.Errorf("cursor invÃ¡lido")
    }
    return parts[0], parts[1], nil
}

func EncodeCursor(creationDate string, reportID string) string {
    cursorData := fmt.Sprintf("%s|%s", creationDate, reportID)
    return base64.StdEncoding.EncodeToString([]byte(cursorData))
}
Si bien  es sencilla y funciona bien con pocos registros,  es mucho mÃ¡s eficiente para grandes volÃºmenes de datos y evita inconsistencias. Dependiendo del caso, se puede utilizar un ID incremental o una combinaciÃ³n de campos como cursor para garantizar un correcto orden y rendimiento.]]></content:encoded></item><item><title>Basic Selenium â€“ Bonus</title><link>https://dev.to/atomictangerline/basic-selenium-bonus-41dp</link><author>brk</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:36:06 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  The tome of wise practices
Some tips on improving code readability.Where the purpose, the author, and the date of creation are inscribedThe header, ahh.. the header. That little snippet of code at the top of the script is like a preamble, a clearing of the throat before the real business begins. What purpose does it serve? Well, the header is a kind of introduction, letting the reader know what's to come with some basic information.Let the Code be governed by a singular, mighty functionAh, the main() function - now there's a topic! It's like the heart and soul of your code, the place where everything comes together. See, when you're working on a code project, it's like you're building a big machine, right? And the main() function, well, that's the control panel, the place where you pull all the levers and push all the buttons to make the whole thing rock. In order to use that machine, you gotta follow a certain set of steps right? Turn the key, check the fuel, prime the pump.. Well, that's what the main() function is all about - it's the sequence of events that gets your code up and running.
  
  
  The parameters, guardians of function
Each one a gatekeeper, ensuring the proper flow of information.Parameters are those little vessels that carry the lifeblood of our functions.
Now, I know what you're thinkin' - "But George, how do I know which one to use?" Well, my friend, it all comes down to the task at hand.
If you're dealing with a relatively simple setup like here, them variables-as-parameters might be just the ticket you need. But if you're working on somethin' a little more complex, with all sorts of moving parts and interdependencies, well, them classes, they're the way to go. Classes, are the real MVPs of the bunch, and they will help keeping everything organized and running smooth as silk.
  
  
  The docstrings, the illuminating scrolls
Where the function's purpose, its workings, and its returns are documented.Alright, people, let me tell you about these things called docstrings. Now, I know what you're thinking - "Docstrings? What in the Sam Hill are those?" Let me break it down for you.
These docstrings, they're like little notes, little snippets of information that you tack on to your code, just to give folks a heads up on what's going on. It's like when you're doing some work around the house, and you leave a little note for the neighbour, just to let â€˜em know what you're up to.
Now, these docstrings, they come in all shapes and sizes. But the way I see it, these docstrings, they're not just about the code, no sir. They're about the people too. So, when you write one of these things, you're not just explaining what the code does, you're telling a story. You're givin' people a little glimpse into your mind and your thought process. And this makes  difference because on bigger projects, we all know you will never code alone.
  
  
  Type hints as vigilant sentinels
Ensuring the integrity of the data.Listen up, I'm about to let you in on a little secret when it comes to this Python business. It's all about them type hints, they're guardrails that keep you from taking a wrong turn by telling your fellow programmers, "hey, this is what I'm expecting here, so don't go messing it up, y'hear?"
See, these type hints, they're some kind of guardians of your program, keeping a watchful eye to make sure you don't go trying to mix apples and oranges, so to speak. When you're dealing with a language as flexible as Python, that's a darn good thing to have in your corner. The more you use â€˜em, the more you'll see just how powerful they can be.]]></content:encoded></item><item><title>Basic Selenium â€“ The Easy Peasy Introduction, Chapter 3 of 3</title><link>https://dev.to/atomictangerline/basic-selenium-the-easy-peasy-introduction-chapter-3-of-3-3bb7</link><author>brk</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:28:52 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Automating web-based tasks with Selenium? Efficiently. That's the name of the game here, so.. Take the reins and make technology work for you.A coding story in three chapters (with a bonus).
The Odyssey of the Code

I. In the realm of the browser, a puppet is forgedII. The opening of the fileIII. Sentence splitting action unleashedIV. The forging of the chunksV. Print statements illuminate the pathVI. The sorcery of the selectorsVII. The bridge of translation is crossedVIII. The file is writtenThe Taming of the Firefox
  
  
  I. In the realm of the browser, a puppet is forged
Where the mighty browser is bound to our will, as if by sorcery.First up, we've got this  business. Now, I know what you're thinking - "George, what in the world is a driver service?" Well, let me tell you, it's the piece of code that's gonna make this whole Selenium thing work. It's like a bridge between our script and the middleman, I name the ! And after that service is up, the geckoâ€™s gonna turn our Firefox into a nice .
And where do we get this driver service, you ask? From the , of course! That's the path to the executable that's gonna make Firefox dance to our tune. Do you know what else it's gonna do? It's gonna log everything that's happening, right into the  file. That way, if anything goes sideways, we can take a peek under the hood and see whyâ€™s that.
Now, the options. See, we're creating a shiny new set of , and we're gonna deck them out with some goodies. First up, we're pointing it to the , making sure we've got the right browser to work with.
And then, if that  variable is set to True, we're gonna add a little  argument to the mix. That means the browser's gonna run without a visible window, stealthier than your average ninja. No need for all the bells and whistles (and windows), we're just here to get the job done, right?
And finally, we're wrapping it all up by returning a brand-spankin' new instance of the , complete with our custom service and options. This one's like a well-oiled machine, and we're the ones behind the wheel.
Just remember: if you ever find yourself scratching your head, wondering what in the world is going on, just take a peek at that  file. It's like a crystal ball, and it might tell you everything you need to know.
  
  
  II. The opening of the file
Like an ancient tome unveiling its secrets.Alright, folks, gather 'round, because we're about to dive into some more of this code. Now, this right here, this is the kind of stuff that separates the wheat from the chaff.
First up, we've got this  block. Now, I know what you're thinking: "George, what in the world is a  block?" Well, let me tell you, it's the statement thatâ€™s gonna keep us from running headfirst into a brick wall every time something goes wrong. And trust me, in the universe of programming, something's always going wrong.
See, what we're doing here is we're trying to open a file, right? And we're using this fancy-schmancy  function to do it. And that's where the  block comes in.
If everything goes according to plan, and the file is there, waiting for us with open arms, we're gonna read the contents and hand 'em back, no problem.  "But George, what could possibly go wrong?" Well, my friends, the world is a cruel and unpredictable place ( when youâ€™re learning software development), and sometimes, those files, they just up and disappear. But don't you worry, we've got a plan and that's where the  block comes in.
If that  rears its ugly head, we're gonna print out a nice, friendly message, letting the user know that the file they were looking for is.. not there..
It's like a safety net, a way to keep the wheels from falling off even when the road gets a little bumpy. And you know what they say, "the more you plan for the unexpected, the less unexpected it becomes." Or something like that.. I don't know, I'm just making it up as I go along.Anyway, the point is when life hands you lemons, you make lemonade. And when life hands you , you just smile ;)
  
  
  III. Sentence splitting action unleashed
The fellowship of words is broken.Okay, listen up, because we're about to dive into some serious sentence-splitting action.
Now, you might be looking at this line of code and thinking: "what in the world is going on here?" Well, let me tell you, it's a thing of beauty: we're taking this text that we've been handed, and we're gonna break it down into its individual sentences.
And how are we doing it, you ask? With the power of regular expressions () and its  implementation, that's how. "But George, what's a regular expression?" Well, my friends, it's a language all its own, a way of describing patterns in ways that would make your head spin. You can tinker with it here.
But don't worry, we're not gonna get too deep into the weeds here. All you need to know is that this little regular expression is the key to our success. It's gonna look for those periods, question marks, and exclamation marks, and it's gonna use them as the boundaries to split our text into individual sentences. But careful, this regex has its limits, see, every language is like a delicate little dance, with each word and phrase movin' in perfect harmony. Some Pros, theyâ€™re like aware of that more than us so they built up nice little tools using carefully crafted and more accurate natural language processing (NLP)  formulas (for example you can experiment with the nltk library). For the sake of simplicity, letâ€™s stick to the basics with the regex way.The next time you find yourself staring at a wall of text, wondering how in the world you're gonna make sense of it all, just remember this little line of code and no more trying to figure out where one sentence ends and the next one begins. And who knows, maybe one day, you'll be the one writing the regular expressions turning chaos of strings into order.
  
  
  IV. The forging of the chunks
The awakening of mighty blocks of text.Hmm, we're about to dive into some serious text-chunking action now. You see, we've got this text that we need to translate, but we can't just send the whole thing off to the translation service all at once. Nah, that would be way too easy. Instead, we've gotta break it down into manageable chunks, little bite-sized pieces that the service can handle without breaking a sweat. That's where this  function comes in handy. It's like a master chef, carefully slicing and dicing the text, making sure each piece is the perfect size.
First, we set up a little , a fancy data structure that's gonna help us keeping track of the current chunk.A , that's just a  way of saying double-ended queue, obviously we deal with small amounts of data here but I wanted to give a try to this exotic thing. Your less sophisticated arrays would work fine there too. Just remember that usual  and  methods donâ€™t perform fast on items on the opposite side of the line. So Pythonâ€™s  module provides that class called deque thatâ€™s specially designed to provide fast and memory-efficient ways to append and pop item from both ends.And then, we start looping through the sentences of the "text" block  (we provided it as an argument when we called the function), and for each sentence, we're gonna figure out its length, -including the space character at the end.
Now, you might be wondering, "But George, how do you know when to start a new chunk?" Well, my friends, it's all about that  variable and the one called  that we use as a counter to keep a running tally of the length of the current chunk. If the current sentence is gonna fit within the chunk size character limit, well, we're just gonna add it on and update the chunk length accordingly. Thatâ€™s the normal way to go. Got it?
Now, if the length of the current sentence, plus the length of the chunk we've got so far, pushes us  the character limit we've set, well, we're gonna do a few things. First, we're gonna take all the sentences in the chunk and join 'em up with spaces, and then we're gonna yield that beasty. That just means we're gonna spit it out and move on to a new chunk.
Moving on to the next chunk means we're gonna clear out the chunk variable and start fresh, adding the current sentence to it and resetting the chunk length to just the length of that sentence, and then, just like before, weâ€™ll keep rolling until we hit the limit again.
Finally, after we've gone through all the sentences, we're gonna yield the last chunk, if there is one.Thatâ€™s it for our symphony of text-chunking perfection. And who knows.. maybe you'll even find yourself singing the praises of them deques and character limits soon.
  
  
  V. Print statements illuminate the path
The chronicles of progress are Written.Alright, but George, why do we need all these print statements? Isn't the script just supposed to, you know, just work?"
Well see, we've got this script that's doing all sorts of stuff, translating text and whatnot. But how are we supposed to know if it's working, huh? That's where these print statements come in, my friends, they're like the canaries in the coal mine. They're the early warning system that's gonna deal us some intel to let us know something's gone awry.
We're gonna print out the length of the input text. Because let's be real, how are we supposed to know if this thing is working if we don't know how much text we're dealing with, am I right? It's like trying to bake a cake without knowing how much flour you've got. There's more.. We're also gonna print out the number of chunks that the script has generated. And to top it all off, we're gonna print out the length of each of those chunks. Because, honestly, who doesn't love a good old-fashioned character count? It's like a little treasure hunt, but instead of finding gold, we're finding out how many letters are in each sentence.
A little  for when you will be contributing to bigger projects: there is that other breed of statements called . They're the quiet ones, they slip in the back door and get the job done without all the fanfare of the print. Just keep in mind they're the ones that keep a careful  when things start to get a little hairy.
  
  
  VI. The sorcery of the selectors
Their spell cast the way.Now itâ€™s time to jump into some serious Selenium wizardry.The input field our marionette will search for.
You see, we've got this input field that we need to find on the web page, and we can't just go barging in there, hoping it'll be there. Instead, we've gotta harness the power of Selenium, which is gonna help us navigate this digital bytescape.
To understand this whole pasta here, We need to know more aboutâ€™em selectors. See, when you're coding a website, just like a building, you got all sorts of elements fitting together - your headings, your paragraphs, buttons, an' the like. Each one of those elements, it's got its own little personality, just like a set of traits and characteristics. And to identify them, you can use all sorts of different selectors:  element selectors, class selectors, ID selectors, and many more other lads. Find out more here.That's where this get_input_textarea_element() function comes in. It's like a secret agent, carefully scanning the page, looking for that elusive  where you insert the text you want to translate.
But thereâ€™s a challenge to it: what's the point of finding the input field if it's not even ready for us to use? So first, we create a  object, and we instruct it to wait for up to  seconds (we've set it at the beginning as a parameter, by the way) for the element to be loaded and clickable thanks to this intriguing EC.element_to_be_clickable() thing that tells Selenium exactly what we're looking for. In this case, we're saying, "Hey, Selenium, find me an element that's clickable, and it's gotta match this . But.. how did I found that damn selector we've got here? Well, invoke the inspector by hitting  in Firefox. Then try the  combo, point & clic your target and in the inspector you'll be able to right clic the highlighted piece of code and extract its info.Concretely weâ€™re looking to mimic human behavior just like when you paste your first chunk of text. Letâ€™s break it down again:You select the input field by clicking on it (itâ€™s selected).Depending of your laziness level, either you start typing or you just paste some text into that area.Well, our driver should be instructed to do the same.
Now, I know what you're thinking, "But George, what if the element never becomes clickable? What then?" Well, friends, that's where the  comes to the rescue. If the element doesn't become clickable within the  period, the whole thing is gonna throw an exception, and we'll know that something's gone wrong. Maybe the website has been updated and our old CSS selector pal is no more, or a networking problem occured somewhere. Whatever.
If everything goes according to plan, and Selenium manages to find that input field and confirm that it's ready for us to use, well, that's when the wonder happens. We're gonna return that element, and the rest of the script is gonna be able to work its magic.
  
  
  VII. The bridge of translation is crossed
Meaning is conveyed across the void.Alright, friends, gather 'round, because we're about to witness the main event, the grand finale and the moment you've all been waiting for - the translation process! Now, I know what you're thinking, "But George, how in the world is this script gonna take all those chunks of text and turn them into a beautiful translation?" Well, my friends, watch out.First, we're gonna set up an empty list called . Our little treasure trove of linguistic gold. This is where we're gonna store all the translated chunks.
And then, we're gonna start looping through those chunks, one by one. Now, I know what you're thinking, "But George, how are we gonna keep track of which chunk we're on?" Well, that's where the  function comes in, my friends. It's gonna give us the  of each chunk, so we can keep tabs on our progress.
As we loop through each chunk, we're gonna print out a little separator, just to let the user know that we're hard at work. And then, we're gonna send that chunk of text to the input field, using the  method. It's like we're typing the text over to the translation service, saying, "Here, take a look at this! A new text chunk for you to translate."
But we can't just sit back and wait, oh no, that would be way too easy. Instead, we're gonna print out a little message, letting the user know that we're fetching the translation. And then, we're gonna hit the  function, giving the translation service some time to achieve its task.
When the time is up, we're gonna use Selenium again to find the output area on the web page, and we're gonna grab the text that's been translated by now. We're talking about pure algorithmic alchemy, folks, turning one language into another with the click of a button. The thing there is just that weâ€™re not behind the wheels anymore.
So once we've put our hands on that translated chunk, we're gonna append it to our translation list. And then, just to be on the safe side, we're gonna clear out the input field, making sure we're ready for the next chunk.
When we've gone through all the chunks, and collected all those translated gems, we're gonna return the whole  list  which is the result of all the translations weâ€™ve collected.
  
  
  VIII. The file is written
The chronicle of the realm is inscribed.The last step starts by calling , and let me tell you, it's one of the unsung hero of this whole operation. Because, let's be honest, what's the point of all this translation process if we can't actually, you know, save the results somewhere?
So, here's how it works. First, we're gonna open up a file, using that  statement. And let me tell you, that , it's like a handshake that tells Python, "Hey, I'm about to do some serious file-handling business, so don't you dare interrupt me!" And what are we gonna do with this file, you ask? Well, my friends, we're gonna write some text to it. But not just any text, oh no, we're talking about the fruits of our labor, the translated chunks that we've been slaving over for who knows how long.
Now, I know what you're thinking, "But George, how are we gonna get all those chunks into the file?" Well, that's where the  function comes in. It's like a magic wand, taking our list of translated chunks and turning them into a cohesive piece of writing. When that file finally gets saved, it's gonna be all those chunks of text, neatly packaged up into a tangible reality: a file!
But you know, it's not just about the end result, folks. It's about the journey, the process of getting there. And this  function, was the final step in that journey. A cherry on top, mic drop moment that says, "We did it, folks, we translated the heck out of that text!"Managing files (loading a source file and writing results into another one).Witnessing that the true sorcery of this tutorial lies in the automated process of translating a text by splitting it into sentences, stacking them into  sent one by one to the online translation service, through Selenium, without ever exceeding a And the goodies: raise a glass to the coder's toolkit, by implementing some good practices (using a header, organizing the code using functions, parameters and a main(), use docstrings and type hints).That's just the beginning, isn't it? This Selenium business, it's got so much more to offer. The possibilities, they're practically endless, my friends. So I want you to take this code, tinker with it, experiment, see what else you can make it do. Youâ€™re ready to go.
Try it out on different translation services, see how it handles the variations. Heck, see if you can make it do your taxes while you're at it (okay, maybe not that, but you get the idea). The point is, this is tip of the iceberg. So, what are you waiting for? Good luck!The code is available on Github.(Cover picture: ).]]></content:encoded></item><item><title>Generating ~450 images for $0.50</title><link>https://dev.to/peter/generating-450-images-for-050-1elj</link><author>Peter Kim Frank</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:23:42 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[I've been getting more and more interested in the Bittensor ecosystem, a decentralized, open-source network interconnected machine learning models.One of the more interesting "subnets" in the network is Chutes.ai which provides Serverless AI Compute across a bunch of different LLMs, image models, etc.  I had listened to them in a recent podcast interview and was looking for an excuse to play around.I asked DeepSeek-R1 to come up with 50 imaginative prompts:A celestial cathedral floating atop a spiraling nebula, its stained-glass windows depicting constellations come to life, gilded arches entwined with ivy made of starlight, surrounded by floating orbs of liquid mercuryAn opulent steampunk airship shaped like a mechanical peacock, its feathers crafted from interlocking brass gears and glowing amber lenses, hovering above a fog-shrouded Victorian metropolis illuminated by gaslampsA surreal garden where trees are composed of cascading sapphire ribbons, their roots embedded in pools of liquid gold, guarded by stone serpents with eyes of smoldering opal under a twilight sky streaked with aurorasAnd then ran those through 9 different image models:All in all, it cost about $0.50 (~$0.001 per image), which I paid for using $TAO, the native currency of Bittensor.I used Cursor's new-ish Agent mode to write the Python code to make all of this possible.  The entire project took about ~15-20 minutes of tinkering around in Cursor, and then an hour or two to generate all of the images (which I just let run before going to bed).I then took the list of prompts and image directory and (again) had Cursor generate a gallery that I uploaded to Cloudflare Pages.Overall, this was a fun little project that has been made possible / much easier through the advent of lower-cost AI models, and code workflow assistants like Copilot, Cursor, Windsurf, Replit Agent, Q Developer, etc.]]></content:encoded></item><item><title>Slice Internals in Go: How the Runtime Expands Slices Efficiently</title><link>https://themsaid.com/slice-internals-in-go</link><author>/u/themsaid</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 20 Feb 2025 15:21:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[The deeper you delve into Goâ€™s internals, the more evident it becomes that its creators carefully engineered the language to strike a precise balance between performance and flexibility. This delicate equilibrium influences many of Goâ€™s core features, including its approach to memory management and data structures. One standout example of this thoughtful design is the implementation of slice growth. Through this approach, Go ensures that slices expand seamlessly, optimizing both performance and memory usage without compromising ease of use.In Go, a slice is a lightweight data structure that serves as a window into a contiguous block of memory where elements of a specific type are stored. At its core, a slice doesnâ€™t directly contain the data itself but instead holds a pointer to an underlying array (know as the backing array.)When the Go runtime creates a slice, as in this example, it constructs a small struct under the hood, defined in the runtime as : {
	 unsafe.
}The  type has the following fields: holds a pointer to the underlying array. stores the number of elements in the slice. stores the capacity of the array.The  type used for the  field is a generic pointer type that bypasses Go's type safety rules. Since the size of an array in Go is part of the type, the runtime uses  so it can replace the array with a larger one when needed.The code in the example above creates a slice that has zero elements with a backing array that can hold 10 elements of type . We can later fill that array with elements by using the  function:Here, we append a byte to the empty array represented by the unsigned 8-bit integer .When appending elements to a slice, the Go runtime first checks whether the backing array has enough capacity to accommodate the new elements. If it does, the elements are simply added to the existing array. However, if the current array lacks sufficient space, the runtime allocates a larger backing array, copies the existing elements into it, and then appends the new elements.([], , )

(, ) (, ) The capacity of the newly allocated array is determined by several factors, including the current arrayâ€™s capacity, the type of elements it holds, and the number of new elements being appended. These factors influence how much the array grows, ensuring efficient memory usage while minimizing the need for frequent reallocations.The runtime begins by attempting to double the existing capacity as the first step in determining the new array size:If the total number of existing and newly appended elements exceeds the doubled capacity, the runtime sets the new capacity to match the required number of elements: {
    
}This ensures that the new capacity is larger than or equals to the number of elements after the appending operation.([], , )

(, , , )

.(()) In this example, the integer slice initially had a capacity of 1. After adding three elements, the runtime allocated a new backing array with a capacity of 3. This happened because doubling the original capacity (1 * 2) was insufficient to accommodate the new elements, prompting the runtime to adjust the capacity accordingly.If doubling the capacity is sufficient, the runtime further evaluates whether allocating such a large array is efficient or merely a waste of memory.For small slices, capacity less than 256, the runtime employs a simple doubling strategy (e.g., 2 to 4, 4 to 8, 8 to 16.) This makes sense for small workloads: doubling ensures plenty of headroom for future appends. However, as the sliceâ€™s capacity climbs into more than 256, or beyond, doubling becomes less practical. Doubling a capacity of, say, 10,000 to 20,000 allocates an extra 10,000 elementsâ€™ worth of memory (potentially tens or hundreds of kilobytes, depending on the element size) which might sit unused for a long time.To address this, the runtime adjusts its growth strategy for larger slices by reducing the growth factor gradually until it reaches 1.25. This slower growth means that if a slice already has a capacity of, say, 512, adding a few elements doesnâ€™t balloon it to 1024; it might rise to 832 instead (a 62.5% increase). The key insight is that a larger slice can absorb more appends before hitting its capacity limit. For instance, a slice with a capacity of 512 has room for 512 more elements if empty, compared to just 8 for a capacity of 8. This naturally delays the need for reallocation.This conservative approach aims to curb excessive memory usage. By growing incrementally rather than exponentially, the runtime avoids reserving vast swaths of memory that might remain idle, which is critical in applications handling large datasets or with limited resources (e.g., embedded systems). However, thereâ€™s a flip side: smaller growth steps mean the slice fills up sooner, triggering reallocation more often. Each reallocation involves CPU work (allocating memory, copying the existing elements, and updating the sliceâ€™s pointer) which can add up if appends are frequent.The runtimeâ€™s strategy thus balances these two forces: memory footprint versus CPU overhead. It leans toward saving memory at the cost of potentially more frequent (but smaller) reallocations, betting that the trade-off pays off in most real-world scenarios where slices donâ€™t grow indefinitely.When determining how a slice should grow, the Go runtime takes into account the type of elements stored in the array, as this directly impacts memory allocation. On 64-bit systems, memory is generally allocated in chunks of 8 bytes. Any allocation that does not align with this rule is rounded up to the nearest multiple of 8 to ensure efficient memory usage and alignment.Let's say we create a slice of bytes with capacity zero and then append an element to it:([], , )

(, )

.(()) After the growth, the capacity of the slice becomes 8 (8 bytes). If the element type was a 64-bit integer instead, the growth will increase the capacity to 1 (1 * 64 bits = 8 bytes):([], , )

(, )

.(()) If the element type was a 32-bit integer, the growth will increase the capacity to 2 (2 * 32 bits = 8 bytes):([], , )

(, )

.(()) The reason is that modern CPUs, particularly on 64-bit systems, operate most efficiently when data is aligned to their word size (the amount of data they can process in one cycle.) On a 64-bit system, the word size is 64 bits, or 8 bytes. If the runtime allocates, say, 5 bytes, the CPU  and masks off the unused portion. That means, allocating in 8-byte multiples ensures the entire chunk is usable without waste or extra work.In addition, CPU caches fetch memory in 64-byte lines (8 words of 8 bytes each.) Multiples of 8 bytes fit neatly into these lines, reducing cache misses and improving locality when accessing sequential data, like a sliceâ€™s backing array.In addition to the 8-byte chunk allocation rule, the Go runtime maintains a table of predefined constants to guide its memory allocation decisions. This table categorizes memory allocations into specific size classes, helping the runtime minimize fragmentation and efficiently reuse freed memory blocks.The table looks like this:+---------+-------+
| Class   | Value |
+---------+-------+
| Class  |      |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |     |
| Class  |    |
| .     | .   |
+---------+-------+This size class allocation table functions as an efficient lookup mechanism for managing memory allocation and deallocation. When a memory block belonging to a specific size class is freed, the runtime stores it in the table rather than immediately returning it to the operating system. Later, if a request is made for a memory block of the same size class, the system can quickly retrieve and reuse the previously freed block instead of performing an extensive search through physical memory to find a suitable allocation.+---------------------------------+
|  Freed memory  size class X  |
+---------------------------------+
        â”‚
        â–¼
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
| Memory Block     |  <-- Freed  (Stored in Table)
+-------------------+  
        â”‚
        â–¼
+-------------------------------------+
| Incoming Memory Allocation Request  |
+-------------------------------------+
        â”‚
          (Lookup in Table)
+-------------------------------+
| Matching Freed Block Found    |
+-------------------------------+
        â”‚
          (Reused Instead of )
+----------------------------+
|  Allocated to Application  |
+----------------------------+With that in mind, the runtime not only rounds up to the nearest 8-byte boundary but also rounds up to the nearest size class in the allocation table.Consider this slice operation:([], , )

(, , , , , )Here, the slice starts with zero capacity and we add 5 elements of type . Without considering the size class allocation table, the runtime would allocate 40 bytes for the new backing array: *  bits =  bits /  =  bytesInstead, the runtime consults the class allocation table and rounds up to the nearest match (48 in this case). As a result, it allocates a backing array with a capacity of 6 (48 bytes / 64 bits), even though the new array would only need to hold 5 elements (that require only 40 bytes).This approach significantly improves performance by reducing external fragmentation (where free memory is scattered in small, non-contiguous blocks, making larger allocations difficult.) It also minimizes allocation overhead and speeds up memory access by eliminating the need to repeatedly request new memory from the operating system.In summary, the Go runtime takes several key factors into account when growing an array:: It starts with a doubling factor (2x) and then gradually winds down to 1.25x.: The array is rounded up to the nearest 8-byte boundary.The Size Class Allocation Table: The runtime rounds up to the nearest available class in the table.I really admire the thoughtful work the Go team has put into making the language both efficient and flexible. It's clear that a lot of careful consideration went into optimizing performance while maintaining flexibility for developers.]]></content:encoded></item><item><title>Building Mobile Apps Without a Backend: The Power of Database Gateway API</title><link>https://dev.to/muhammetberdi_jepbarov/building-mobile-apps-without-a-backend-the-power-of-database-gateway-api-19m0</link><author>Muhammetberdi Jepbarov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:21:03 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[
  
  
  The Pain of Backend Development
If you've ever built a mobile or web app that interacts with a database, you know the struggleâ€”designing API endpoints, handling authentication, writing business logic, and ensuring scalability. Sometimes, all you need is a simple way to query the database and retrieve structured JSON responses without going through the entire process of backend development.Thatâ€™s exactly why many years ago I built the â€”a solution that allows developers to interact directly with PostgreSQL and MSSQL databases via a secure API, without writing a full-fledged backend.
  
  
  The Idea Behind Database Gateway API
The goal was simple: Why should you have to build an entire backend when all you need is an API for your database?I needed a lightweight yet powerful solution that could:Eliminate the need for a dedicated backend in simple applications.Enable mobile and web apps to access database data seamlessly.Provide an easy way to deploy APIs for database interactions with minimal setup.Integrate with any existing system, whether itâ€™s an enterprise  or a retail point-of-sale (POS) system.This led to the creation of the â€”a framework-agnostic API layer that acts as a bridge between your database and applications.The  is a standalone service that connects to your  or  database and exposes SQL queries as API endpoints. It takes care of request parsing, security, and response formatting, allowing you to focus on building your application instead of writing backend logic.âœ… Direct SQL Query Execution: Supports SELECT, INSERT, UPDATE, and DELETE operations via API requests.Automatic JSON Responses: Converts database query results into well-structured JSON.Easy Integration with Mobile & Web Apps: No need for a complex backendâ€”just plug it into your frontend. Set up role-based access, API keys, and rate limiting.Supports Complex Queries & Joins: Fetch relational data easily, just like using SQL directly.Minimal Deployment Overhead: Run it as a standalone service or containerized in Docker.  
  
  
  Real-World Impact: 29 Deployments & 150+ Devices
This API has been successfully deployed across , powering over , primarily in . It integrates seamlessly with mobile apps and existing accounting systems, allowing businesses to:Sync sales data in real-time, eliminating manual record-keeping.Improve customer experience by linking mobile apps to inventory and POS systems.Enable effortless data exchange between different applications without writing additional backend logic.If youâ€™re a developer building a mobile app, web dashboard, or prototype, the  can save you weeks of development time by handling database queries and responses automatically.Instead of spinning up a full backend, setting up ORM models, and writing CRUD endpoints, you simply install the gateway, configure it with your database, and start making API requests.âœ… Instant API â€“ Set up in minutes and start making SQL queries right away.
âœ… Effortless Integration â€“ Works with mobile, web, and desktop apps.
âœ… JSON-Formatted Responses â€“ Your queries return well-structured JSON, ready to use.
âœ… Perfect for Prototyping â€“ Quickly test database interactions without a full backend.
âœ… Optimized for Performance â€“ Execute fast queries with minimal setup.ðŸš€ How It Works
ðŸ” Querying Your DatabaseNeed to fetch data? Just send a POST request with your SQL query:POST 127.0.0.1:8000/api/v1/make-db-request
{
  "query_string": "SELECT * FROM tbl_mg_materials",
  "base64_columns": ["group_code", "image_pict", "firm_id_guid"]Pro tip: Use base64_columns to encode image BLOBs or sensitive data!
ðŸ“Š The ResponseThe API returns a structured JSON response:{
  "data": [...], 
  "total": 2, 
  "message": "db query result"
}âš ï¸ Important ConsiderationsðŸ”´ Security First! â€“ This API executes raw SQL, so make sure to restrict access and validate inputs.
ðŸ”´ Database Changes? â€“ Schema updates might require adjustments to your API queries.
ðŸ”´ Use with Caution â€“ Best for internal tools, rapid prototyping, and trusted environments.Want to try it out? The  is open-source and available for anyone to use and contribute to. You can integrate it into your next project and cut down on backend development time significantly.Check out the repository: 
Let me know if you have questions or ideas for improvementsâ€”happy coding! ðŸš€]]></content:encoded></item><item><title>Basic Selenium â€“ The Easy Peasy Introduction, Chapter 2 of 3</title><link>https://dev.to/atomictangerline/basic-selenium-the-easy-peasy-introduction-chapter-2-of-3-1oad</link><author>brk</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:17:29 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Automating web-based tasks with Selenium? Efficiently. That's the name of the game here, so.. Take the reins and make technology work for you.A coding story in three chapters (with a bonus).
  
  
  The Fellowship of the Functions
:
 There's a whole lot going on under the hood, and you're gonna want to know what makes this thing tick.For the newcomers out there, on using functions:
Sometimes you got bits of code that you need to use over and over again, right? Well, functions, they make your code a whole lot easier to read and understand. I mean, think about it: instead of trying to decipher a tangled web of spaghetti code, you got these nice, neat little packages that do one thing and do it well. And when you need to make a change, well, you just gotta tweak the function, and b-a-m, problem solved. No muss, no fuss. Embrace 'em, you love 'em. Trust me, your future self is gonna thank you for it.First, let's have a look at this  version of the main function:
  
  
  The Initialization of the Browser
In which the Selenium-forged steed is summoned, and the journey begins.First, we've got the  function. This is where the magic starts - it's setting up a brand new instance of the Firefox browser, all decked out with our custom options.  mode is on by default! No need for a window to appear, we're going full stealth mode here.Where the words are divided into manageable chunks, as if by the wisdom of the regex.Next, we've got . This one's pretty straightforward - it's just reading the contents of a file and handing us back the text.
Then there's split_text_into_sentences(). This is where the script takes that input text and breaks it down into individual sentences. Gotta make sure we're not overwhelming the translation service, you know? Bite-sized chunks are the way to go.
And speaking of those chunks, that's where  comes in. It's taking those sentences and slicing them, making sure each sentence-block is small enough to play nice with the translation service. No more hitting character limits.
  
  
  The Gathering of the Fields
In which the input field is sought and found.Now, the real showstoppers: get_input_textarea_element(). This is the function that use Selenium to find the right spot on the web page to do our work. I mean the input field, where we're gonna pour in our text. Without it, no circus troupe at your fingertips, ready to leap through hoops an' do backflips at your every whim.
  
  
  The Fetching of the Results
The final step, where the fruits of the labors are harvested and the story ends.Finally, we've got  and . These are the heavy hitters.  is where the rubber meets the road, sending those sentence chunks off to be translated and bringing back the long awaited goods.  is the grand finale, putting the whole shebang down on paper (or, you know, in a file).
Whew, that's a lot to take in, I know. But well, once you've got this thing up and running, youâ€™ll see, it's gonna be smooth sailing. Just sit back, let the wind.. Hmm.. the script do its thing.(Cover picture: ).]]></content:encoded></item><item><title>Basic Selenium â€“ The Easy Peasy Introduction, Chapter 1 of 3</title><link>https://dev.to/atomictangerline/basic-selenium-the-easy-peasy-introduction-chapter-1-of-3-4fe3</link><author>brk</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 15:08:34 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[Automating web-based tasks with Selenium? Efficiently. That's the name of the game here, so.. Take the reins and make technology work for you.A coding story in three chapters (with a bonus).There I was, sitting on a chair, having to gather all these newspaper translations, from who knows where, in who knows what languages. Let me tell you, it was a real slog. The , just wouldn't let me get the full show. Nothing but snippets, snippets, snippets...Then, the idea struck. This script, see, it's gonna let you bypass all that. No more copying and pasting. Just grab a coffee, make it strong, black and wait for the whole enchilada - right there in front of you. A real time-saver, I'm telling you.The best part? Turns out, it wasn't just about solving an immediate problem: it was about deepening my know-how, getting my hands dirty with some real-world programming. A win-win, if you ask me.
  
  
  Installation and Configuration
Alright, let's crack open this code. Step by step, we're gonna peel back the layers of that machine where each piece is fitting in like a puzzle. By the time we're done, you're gonna have a whole new appreciation for Selenium automation and what it can do.And beware, you're not just gonna be learning for learning's sake. Nah, this is about equipping you with the tools to tackle your own challenges. Whatever got you tied up in knots, this is your chance to untangle it and make it sing."Time to get this thing installed and ready to rock."First things first - you're gonna need to clone or download the source code. The repository is online here. Don't worry, I'll wait.If you don't already have Firefox on your system, you're gonna need to get that taken care of. For  Ubuntu and other Debianistas folks out there, it's a simple enough fix:apt update apt firefox
Itâ€™s also possible to use Chrome instead of Firefox.. Itâ€™s not covered here so if you really want it that way, itâ€™s this way there.Next, we've got some packages to install. Nothing too crazy, but we gotta make sure we've got all the tools ready to roll. So fire up that trusty old pip and run:pip  requirements.txt
Boom. Done and done.
Now, the real kicker - . This is the secret sauce that sits behind the scenes, translating your commands into a language web browsers can understand.
You're gonna want to head over to the Mozilla repository, grab the latest version, and get it all set up. I'm talking extraction, permissions, symbolic links - the whole nine yards. And you know the magic? Thereâ€™s a bash spell ready out there for that:wget https://github.com/mozilla/geckodriver/releases/download/v0.35.0/geckodriver-v0.35.0-linux64.tar.gz  /tmp/geckodriver.tar.gz  /opt  /tmp/geckodriver.tar.gz 755 /opt/geckodriver  /opt/geckodriver /usr/bin/geckodriver  /opt/geckodriver /usr/local/bin/geckodriver
That's a mouthful, I know. But trust me, it's worth it. Once you've got all that squared away, you're gonna be almost ready to start automating like never before.Alright, folks, time to dive into the nitty-gritty of this configuration stuff. Because let me tell you, if you don't get this part right, the whole thing's gonna be about as useful as.. a lawn mower in the middle of the Mojave.
First up, we've got those  and  variables. Now, I know what you're thinking - "But how in the world am I supposed to know where these things are hiding on my system?" Well, fellas, depending on the one youâ€™re looking for, there's a little bash command you can run to ferret that out:About the Gecko.. Remember that installation process we went through earlier? Well, the path you set up there is the one you'll want to use here. Not sure about it? No problem, same sauce, Just run:And Zap! Paste that info into the right place. Easy, right?
Now, the  parameter - this one's a bit of a wild card. See, you can run this whole thing in headless mode, which means the browser will run and do its job but won't actually show up on your screen. Kinda like a ninja, you know? But if you want to see what's going on, and watch your Firefox acting like possessed by a spirit you can set it to  and watch the magic unfold â€“ try and see.As for the  and , well, pretty self-explanatory. Just pick the languages you want to translate to (source) and from (output), and the script will handle the rest. If you're looking to learn a little more about the available languages, take a glance at that readme file in the repository here.
Last but not least, we've got the , , and . These are all about fine-tuning the performance. , and  depend mostly on the speed of your network connection. Itâ€™s about how long we oughta wait for the website to be fully loaded so we can proceed with the translation.  is linked to deepLâ€™s restrictions (currently 1500 characters). Tinker with them as needed, but be careful - you don't want to break the whole machine.
The  and  - those are the ones you'll want to point to your own input and output files. Simple enough, right? Just make sure you're pointing the input to something that actually contains text, because that's what this script is built to handle.
Now, a quick word of warning â€“ the script is not perfect - if the output file doesn't exist yet, the script's gonna go ahead and create it for you. But if it does exist, well, buckle up, because the script's gonna overwrite it every time you run it. So, you know, maybe keep an eye on that (and specify a different output each time) if you donâ€™t want to see your previous achievements disappear..Wait.. I almost forgot.. You're a  User? WSL now supports running Linux GUI applications in a fully integrated experience. On older configurations, you might need this if you intend to run Firefox as a marionette with its GUI.Once you've got those parameters all squared away, fire up the terminal, go to the script's folder and just run:Ka-boom, the showâ€™s hit the road. The script's gonna take that input file, work its translation-y wonders, and spit out the results into your output file. Fear not, my friend just keep an eye on that output file, and you'll be able to see the fruits of that labor, plain as day.
What are you waiting for? Get out there, update those parameters, and let's see what kind of translation magic you can work!(Cover picture: ).]]></content:encoded></item><item><title>Big O Complexity Cheat Sheet for Coding Interviews</title><link>https://www.kdnuggets.com/big-o-complexity-cheat-sheet-coding-interviews</link><author>Bala Priya C</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/bala-big-o.png" length="" type=""/><pubDate>Thu, 20 Feb 2025 15:00:46 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[This is a comprehensive cheat sheet on algorithmic complexity for coding interviews. ]]></content:encoded></item><item><title>Rust 2024 Is Coming: baby steps</title><link>https://smallcultfollowing.com/babysteps/blog/2025/02/20/rust-2024-is-coming/?utm_source=atom_feed</link><author>/u/VorpalWay</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 14:53:00 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[So, a little bird told me that Rust 2024 is going to become stable today, along with Rust 1.85.0. In honor of this momentous event, I have penned a little ditty that Iâ€™d like to share with you all. Unfortunately, for those of you who remember Rust 2021â€™s â€œEdition: The songâ€, in the 3 years between Rust 2021 and now, my daughter has realized that her father is deeply uncool and so I had to take this one on solo. Anyway, enjoy! Or, you know, suffer. As the case may be.In ChordPro format, for those of you who are inspired to play along.{title: Rust 2024}
{subtitle: }

{key: C}

[Verse 1]
[C] When I got functions that never return
I write an exclamation point [G]
But use it for an error that could never be
the compiler [C] will yell at me

[Verse 2]
[C] We Rust designers, we want that too
[C7] But we had to make a [F] change
[F] That will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four

[Bridge]
[Am] ... [Am] But will my program [E] build?
[Am] Yes ... oh thatâ€™s [D7] for sure
[F] edi-tions [G] are [C] opt in

[Verse 3]
[C] Usually when I return an `impl Trait`
everything works out fine [G]
but sometimes I need a tick underscore
and I donâ€™t really [C] know what thatâ€™s for

[Verse 4]
[C] We Rust designers we do agree
[C7] That was con- [F] fusing 
[F] But that will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four

[Bridge 2]
[Am] Cargo fix will make the changes
automatically [G] Oh that sure sounds great...
[Am] but wait... [Am] my de-pen-denc-[E]-ies
[Am] Donâ€™t worry e-[D7]ditions
[F] inter [G] oper [C] ate

[Verse 5]
[C] Whenever I match on an ampersand T
The borrow [G] propagates
But where do I put the ampersand
when I want to [C] copy again?

[Verse 6]
[C] We Rust designers, we do agree
[C7] That really had to [F] change
[F] That will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four

[Outro]
[F] That will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four

One more time!

[Half speed]
[F] That will be [Fm]better
[C] Oh so much [A]better
[D] in Rust Twenty [G7]Twenty [C]Four
]]></content:encoded></item><item><title>HN: cypher queries tips (Graph dbms)</title><link>https://dev.to/falkordb/hn-cypher-queries-tips-graph-dbms-hck</link><author>Dan Shalev</author><category>dev</category><category>rust</category><category>devto</category><pubDate>Thu, 20 Feb 2025 14:46:14 +0000</pubDate><source url="https://dev.to/t/rust">Dev.to Rust</source><content:encoded><![CDATA[Writing performant Cypher queries isnâ€™t just about syntaxâ€”itâ€™s about understanding graph structures, optimizing query paths, and leveraging advanced features. At FalkorDB, weâ€™ve seen how poorly optimized queries can bottleneck even the most robust systems.Most devs donâ€™t realize inefficient Cypher queries often stem from broad MATCH patterns and missing indexes. ]]></content:encoded></item><item><title>ÐšÐ°Ðº Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ Viewscount: Ð ÐµÑˆÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð´ÑÑ‡ÐµÑ‚Ð° Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð² Ð² Golang</title><link>https://dev.to/muhammetberdi_jepbarov/kak-ia-razrabotal-bibliotieku-viewscount-rieshieniie-probliemy-orghanichieskogho-podschieta-prosmotrov-v-golang-1jn0</link><author>Muhammetberdi Jepbarov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 14:17:58 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[ÐšÐ°Ðº Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¸, Ð¼Ñ‹ Ñ‡Ð°ÑÑ‚Ð¾ ÑÑ‚Ð°Ð»ÐºÐ¸Ð²Ð°ÐµÐ¼ÑÑ Ñ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒÑŽ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð² ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Ð½Ð° Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°Ñ…. Ð‘ÑƒÐ´ÑŒ Ñ‚Ð¾ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð² Ð´Ð»Ñ ÑÑ‚Ð°Ñ‚ÐµÐ¹, Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð², Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾Ð»Ð°Ð³Ð°ÑŽÑ‚ÑÑ Ð½Ð° Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ  Ð² ÑÐ²Ð¾Ð¸Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°Ñ… Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ð²Ð¾Ð·Ð½Ð¸ÐºÐ°ÐµÑ‚, ÐºÐ¾Ð³Ð´Ð° Ð²Ñ‹ Ð½Ðµ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ðµ API Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð½ÑƒÐ¶Ð´Ð°ÐµÑ‚ÑÑ Ð² Ð¿Ð¾Ð´ÑÑ‡ÐµÑ‚Ðµ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð², Ð¸ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ð²Ð°Ð¶Ð½Ð¾ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡Ð¸Ñ‚ÑŒ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ Ð¾Ñ‚ Ñ‚Ð°ÐºÐ¸Ñ… Ð°Ñ‚Ð°Ðº, ÐºÐ°Ðº DoS (Denial of Service). Ð­Ñ‚Ð¾ Ñ‚Ð° Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°, Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ñ ÑÑ‚Ð¾Ð»ÐºÐ½ÑƒÐ»ÑÑ, Ð¸ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°, Ð¿Ð¾ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ñ ÑÐ¾Ð·Ð´Ð°Ð» Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ .Ð’ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°ÑŽÑ‚ÑÑ Ð² Ñ‚Ð¾Ð¹ Ð¸Ð»Ð¸ Ð¸Ð½Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ðµ â€” Ð±ÑƒÐ´ÑŒ Ñ‚Ð¾ Ð´Ð»Ñ Ð±Ð»Ð¾Ð³Ð°, ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð° Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ. ÐžÐ´Ð½Ð°ÐºÐ¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð±Ñ‹Ð»Ð¾ Ð±Ñ‹ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ð¼ Ð¸ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¼, Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡ÐµÐ¹. Ð’Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ Ð¿ÑƒÑ‚ÑŒ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… API Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð¸Ð· ÑÑ‚Ð¸Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†, Ð½Ð¾ ÑÑ‚Ð¾ Ð¾Ñ‚Ð½Ð¸Ð¼Ð°ÐµÑ‚ Ð¼Ð½Ð¾Ð³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¸ Ð½Ðµ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÑ‚ÑÑ. Ð Ñ‡Ñ‚Ð¾ ÐµÑÐ»Ð¸ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹, Ð½Ðµ Ð¿Ð¸ÑˆÐ° API Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð½Ð¾Ð²Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹? Ð§Ñ‚Ð¾ ÐµÑÐ»Ð¸ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ Ð±Ð¾Ð»ÐµÐµ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð± Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸ ÑÑ‚Ð¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð² Ð»ÑŽÐ±Ð¾Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ? Ð˜Ð¼ÐµÐ½Ð½Ð¾ Ð² ÑÑ‚Ð¾Ñ‚ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚ Ñ Ð½Ð°Ñ‡Ð°Ð» Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð½Ð°Ð´ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÐµÐ¼ .
  
  
  ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ ÐµÐ´Ð¸Ð½Ð¾Ð³Ð¾ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ
ÐšÐ¾Ð³Ð´Ð° Ð²Ñ‹ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ñ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð², Ð¾Ð´Ð½Ð¸Ð¼ Ð¸Ð· Ð²Ð°Ð¶Ð½Ñ‹Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð¾Ð² ÑÐ²Ð»ÑÐµÑ‚ÑÑ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð¾ÑÑ‚Ð¾Ð²ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…. Ð’ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… ÑÐ»ÑƒÑ‡Ð°ÑÑ… Ð·Ð»Ð¾ÑƒÐ¼Ñ‹ÑˆÐ»ÐµÐ½Ð½Ð¸ÐºÐ¸ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð°Ñ‚ÑŒÑÑ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ ÑÑ‡ÐµÑ‚Ñ‡Ð¸Ðº Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð², Ð´ÐµÐ»Ð°Ñ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ â€” Ð·Ð´ÐµÑÑŒ Ð²Ð°Ð¶Ð½Ð¾ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‚Ð¸Ñ‚ÑŒ Ñ‚Ð°ÐºÐ¸Ðµ Ð°Ñ‚Ð°ÐºÐ¸, ÐºÐ°Ðº DoS (Denial of Service). Ð‘ÐµÐ· Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾Ð³Ð¾ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð²Ð°ÑˆÐµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ ÑƒÑÐ·Ð²Ð¸Ð¼Ñ‹Ð¼ Ð´Ð»Ñ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ñ‹Ñ… Ð°Ñ‚Ð°Ðº, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð´Ñ€Ñ‹Ð²Ð°ÐµÑ‚ Ð´Ð¾ÑÑ‚Ð¾Ð²ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ñ….Ð¢Ð°Ðº Ð¿Ð¾ÑÐ²Ð¸Ð»Ð°ÑÑŒ Ð¸Ð´ÐµÑ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ . Ð¯ Ñ…Ð¾Ñ‚ÐµÐ» ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð¿Ð¾Ð·Ð²Ð¾Ð»Ð¸Ð»Ð¾ Ð±Ñ‹ Ð»ÐµÐ³ÐºÐ¾ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ñ‚ÑŒ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹ Ð´Ð»Ñ Ð»ÑŽÐ±Ñ‹Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†, Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¾Ñ‚ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ°, Ð¸ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ ÑÑ‚Ð¾Ð³Ð¾ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð² Ð»ÑŽÐ±Ð¾Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ, Ð·Ð°Ñ‰Ð¸Ñ‰Ð°Ñ Ð¾Ñ‚ Ð½ÐµÐ½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ñ… Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð². Ð­Ñ‚Ð° Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð° Ð´Ð»Ñ Ñ‚Ð¾Ð³Ð¾, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ñ Ð»ÑŽÐ±Ñ‹Ð¼ Golang Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¾Ð¼, Ñ‚Ð°ÐºÐ¸Ð¼ ÐºÐ°Ðº ,  Ð¸Ð»Ð¸ Ð´Ð°Ð¶Ðµ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¼ , Ð¸ Ð±Ñ‹Ñ‚ÑŒ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾Ð¹ Ð¾Ñ‚ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°Ñ PostgreSQL.ÐšÐ¾Ð³Ð´Ð° Ð²Ñ‹ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚Ðµ  Ð² ÑÐ²Ð¾Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ, Ð²Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹ ÑÑ€Ð°Ð·Ñƒ â€” Ð±ÐµÐ· Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ API Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹. Ð”Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼, Ñƒ Ð²Ð°Ñ ÐµÑÑ‚ÑŒ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹, Ñ‚Ð°ÐºÐ¸Ðµ ÐºÐ°Ðº  Ð¸Ð»Ð¸ , ÐºÐ°Ð¶Ð´Ð°Ñ Ð¸Ð· ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð¸Ð¼ÐµÐµÑ‚ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ . Ð’Ð¼ÐµÑÑ‚Ð¾ Ñ‚Ð¾Ð³Ð¾ Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ API ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ ÑÑ‡ÐµÑ‚Ñ‡Ð¸ÐºÐ° Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð² ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ñ€Ð°Ð·, ÐºÐ¾Ð³Ð´Ð° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð¿Ñ€Ð¾ÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ, Ð²Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ  Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°.Ð’Ð¾Ñ‚ ÐºÐ°Ðº ÑÑ‚Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð½Ð° Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐµ:Ð’ Ð²Ð°ÑˆÐµÐ¹ Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð´Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ  Ð² Ð²Ð°ÑˆÐ¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹:ÐÐ½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð¸ Ð´Ð»Ñ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†. ÐÐµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð¸Ð»Ð¸ API â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð´Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ ÑÑ‚Ñƒ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ Ð´Ð»Ñ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð².
  
  
  Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Viewscount Ð² Ð²Ð°ÑˆÐµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ
Ð¡Ð°Ð¼Ð°Ñ Ð»ÑƒÑ‡ÑˆÐ°Ñ Ñ‡Ð°ÑÑ‚ÑŒ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸  Ð·Ð°ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ÑÑ Ð² Ñ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾ Ð¾Ð½Ð° ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾Ð¹ Ð¾Ñ‚ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¾Ð². ÐÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¾Ñ‚ Ñ‚Ð¾Ð³Ð¾, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚Ðµ Ð»Ð¸ Ð²Ñ‹ ,  Ð¸Ð»Ð¸ Ð´Ð°Ð¶Ðµ , Ð²Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð»ÐµÐ³ÐºÐ¾ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐµÐµ Ð² Ð²Ð°ÑˆÐµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ. Ð’Ð¾Ñ‚ ÐºÐ°Ðº Ð²Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÐµÐµ ÐºÐ°Ðº middleware Ð² :Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð½ÑƒÐ¶Ð½Ð¾ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ view tracker Ð² Ð²Ð°ÑˆÐµÐ¼ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¸:ÐšÐ°Ðº Ñ‚Ð¾Ð»ÑŒÐºÐ¾ middleware Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¾, Ð²Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÐµÐ³Ð¾ Ðº Ð²Ð°ÑˆÐ¸Ð¼ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð°Ð¼. Ð’Ð¾Ñ‚ ÐºÐ°Ðº ÑÑ‚Ð¾ Ð´ÐµÐ»Ð°ÐµÑ‚ÑÑ:Ð­Ñ‚Ð° Ð¿Ñ€Ð¾ÑÑ‚Ð°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ  Ð² Ð²Ð°ÑˆÐµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ. Middleware Ð±ÑƒÐ´ÐµÑ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹ Ð½Ð° ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ Ð¸ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°Ñ‚ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÐºÐ¾Ð»Ð¾Ð½ÐºÐµ  Ð¿Ð¾ Ð¼ÐµÑ€Ðµ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸, Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡Ð¸Ð²Ð°Ñ Ð·Ð°Ñ‰Ð¸Ñ‚Ñƒ Ð¾Ñ‚ Ð°Ñ‚Ð°Ðº.: Ð’ÑÐµÐ³Ð¾ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ñ€Ð¾Ðº ÐºÐ¾Ð´Ð° â€” Ð¸ Ð²Ñ‹ ÑƒÐ¶Ðµ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°ÐµÑ‚Ðµ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹ Ð½Ð° Ð»ÑŽÐ±Ð¾Ð¹ Ð¸Ð· Ð²Ð°ÑˆÐ¸Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†.ÐÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÑŒ Ð¾Ñ‚ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¾Ð²: Ð‘Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð° Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð»ÑŽÐ±Ñ‹Ð¼ Golang Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¾Ð¼, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÐµÐµ Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚Ðµ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¾Ñ‚ ÑÑ‚ÐµÐºÐ°.: ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°Ñ Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ ÑÑ‡ÐµÑ‚Ñ‡Ð¸Ðº Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð²,  Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚, Ñ‡Ñ‚Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾ÑÑ‚Ð°ÑŽÑ‚ÑÑ Ð´Ð¾ÑÑ‚Ð¾Ð²ÐµÑ€Ð½Ñ‹Ð¼Ð¸ Ð¸ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¼Ð¸.Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¸ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ð¾ÑÑ‚ÑŒ: ÐÐµ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ðµ API Ð¸Ð»Ð¸ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ â€” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ Ð¸ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð¹Ñ‚Ðµ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¾Ð±Ñ‰ÑƒÑŽ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: ÐºÐ°Ðº Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹ Ð½Ð° Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°Ñ…, Ð¾Ð±ÐµÑÐ¿ÐµÑ‡Ð¸Ð²Ð°Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ, Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ð½Ðµ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ API Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð½Ð¾Ð²Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹. Ð›ÐµÐ³ÐºÐ¾ÑÑ‚ÑŒ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸, Ð·Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ Ð°Ñ‚Ð°Ðº Ð¸ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÑŒ Ð¾Ñ‚ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¾Ð² Ð´ÐµÐ»Ð°ÑŽÑ‚ ÐµÐµ Ð¼Ð¾Ñ‰Ð½Ñ‹Ð¼ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð¼ Ð´Ð»Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¾Ð², ÑÐ¾Ð·Ð´Ð°ÑŽÑ‰Ð¸Ñ… ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Golang-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ.ÐÐµ ÑÑ‚ÐµÑÐ½ÑÐ¹Ñ‚ÐµÑÑŒ Ð²Ð½ÐµÑÑ‚Ð¸ Ð²ÐºÐ»Ð°Ð´ Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚ Ð¸ ÑÐ¾Ð¾Ð±Ñ‰Ð°Ñ‚ÑŒ Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ… Ð¸Ð»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸ÑÑ… Ñ‡ÐµÑ€ÐµÐ· GitHub Viewscount. Ð¯ Ð±ÑƒÐ´Ñƒ Ñ€Ð°Ð´ ÑƒÐ²Ð¸Ð´ÐµÑ‚ÑŒ, ÐºÐ°Ðº ÑÑ‚Ð° Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° Ð¿Ð¾Ð¼Ð¾Ð¶ÐµÑ‚ Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ð² Ð¸Ñ… Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ!]]></content:encoded></item><item><title>How I Developed the Viewscount Library: Solving the Problem of Organic View Counting in Golang</title><link>https://dev.to/muhammetberdi_jepbarov/how-i-developed-the-viewscount-library-solving-the-problem-of-organic-view-counting-in-golang-1684</link><author>Muhammetberdi Jepbarov</author><category>dev</category><category>go</category><category>devto</category><pubDate>Thu, 20 Feb 2025 14:16:56 +0000</pubDate><source url="https://dev.to/t/go">Dev.to Go</source><content:encoded><![CDATA[As developers, we often encounter the need to track views on content across various platforms. Whether it's tracking views for articles, videos, or products, most applications rely on a simple  column in their database tables. The challenge arises when you donâ€™t want to go through the hassle of writing a separate API for each table that needs view tracking, while also ensuring that your system is secure from abuse like DoS (Denial of Service) attacks. This is the problem I encountered and the reason I created the  library.In most applications, views are tracked in some formâ€”whether itâ€™s for a blog post, a product listing, or even user profile pages. However, implementing a solution that is both scalable and secure can become cumbersome. You could go the route of writing custom APIs for each of these tables, but thatâ€™s time-consuming and doesnâ€™t scale well. What if there was a way to track these views without rewriting APIs for every new table? What if there was a more efficient way to integrate this functionality into any app? Thatâ€™s when I started working on .
  
  
  The Need for a Unified Solution
When building any system with views tracking, one of the critical concerns is ensuring that the view count remains authentic. In some cases, malicious users may attempt to artificially inflate the view count through repeated requestsâ€”this is where preventing DOS (Denial of Service) type increments comes into play. Without a solid solution, your application becomes vulnerable to these types of attacks, undermining the integrity of your view count.Thus, the idea for  was born. I wanted a way to seamlessly count organic views across any tables, regardless of the framework, while providing an easy-to-integrate middleware solution that prevents abusive increments. This library is designed to integrate with any Golang web framework, like , , or even basic , and is database-agnostic, supporting PostgreSQL seamlessly.When you add  to your application, you can begin tracking views instantlyâ€”without having to write separate APIs for each table. Letâ€™s say you have tables like  or , each with a  column. Instead of building an API specifically to increment the  each time a user views a page, you can use  to track this automatically.Hereâ€™s how it works in practice:In your database, you simply add the  column to your tables like so:The same goes for your other tables. No need for complicated queries or APIsâ€”just a simple column to track views. 
  
  
  Integrating Viewscount into Your Application
The best part of  is that itâ€™s designed to be . Whether you're using , , or even , you can easily integrate it into your application. Hereâ€™s how you can add it as middleware in :First, youâ€™ll need to initialize the view tracker in your app:Once youâ€™ve set up the middleware, you can add it to your routes. Hereâ€™s how you do it:This simple setup integrates the  library into your existing app. The middleware will automatically track views on the specified table and increment the  column as needed, while also providing protection against abuse.
  
  
  The Benefits of Viewscount
: With just a few lines of code, you can integrate view counting into any of your existing tables.: The library is designed to work with any Golang web framework, making it easy to use in your project regardless of the stack.Prevention of DOS Attacks: By limiting rapid requests that could artificially inflate view counts,  ensures that the data remains reliable and accurate.: No need for custom APIs or complex queriesâ€”just use the library and track views automatically in real time. solves a common problem in application development: how to track views across tables securely and efficiently without having to build a separate API for each table. Its easy integration, ability to prevent DOS-style attacks, and framework-agnostic design make it a powerful tool for developers building modern Golang applications.Feel free to contribute to the project and raise issues or suggestions via Viewscount GitHub. Iâ€™d love to see how this library can help others in their development journey!]]></content:encoded></item><item><title>Day -02 of learning python programming language..</title><link>https://dev.to/kapil_kumarshahsonar_ad/day-02-of-learning-python-programming-language-22pl</link><author>KAPIL SHAH</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 14:11:18 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[
  
  
  The thing i learned from python today are as follow:
simply tuples is immutable which means that we can change the data again in list.In tuples list  are stored  in curly bracket({}).It is quite faster than that of list.simply we can say that  list is mutable which means that  we  can edit the list  as our wish anytime.list student={kapil , Prashant , rajan}we can use function which are inbuilt ;
list. Append()=for adding element in list .list .insert()=for adding an element wherever you like to .list. Replace()=For replacing the data it is quite slower comparing to tuples .numbers = range(5,10,2)
for number in range(5):In list data are stored in the square bracket  ([ ]).Data can be edited easily .The most important part of this idea is def( define function ) def sigma(to="world"):
print("noob,", to)  # Added a space after the comma for better formattingsigma()
name = input("What's your name?Â ")If you have any query related to above  query than feel to ask.def main():
name = input("What's your name?")def hello(to="world"):
print("hello", name)Thatâ€™s all for todayâ€™s python course..]]></content:encoded></item><item><title>Hosting Khoj for Free: Your Personal Autonomous AI App</title><link>https://www.kdnuggets.com/hosting-khoj-free-personal-autonomous-ai-app</link><author>Abid Ali Awan</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/awan_hosting_khoj_free_personal_autonomous_ai_app_2.png" length="" type=""/><pubDate>Thu, 20 Feb 2025 14:08:35 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Turn your local LLMs into a personal, autonomous AI application that can effortlessly retrieve answers from the web or your documents.]]></content:encoded></item><item><title>Pandas â€” For Beginners</title><link>https://dev.to/akshayak8/pandas-for-beginners-11gd</link><author>akshay</author><category>dev</category><category>python</category><category>devto</category><pubDate>Thu, 20 Feb 2025 13:54:40 +0000</pubDate><source url="https://dev.to/t/python">Dev.to Python</source><content:encoded><![CDATA[In todayâ€™s data-driven world, data analysis plays a crucial role in transforming large amount of raw data into understandable visualization. This process enables organizations to make informed decisions. By analyzing data, businesses can gain a competitive edge, better understand customer behavior, and personalize their offerings.Python has emerged as a popular language for data analysis due to its simplicity and versatile ecosystem of libraries. Among these, Pandas stands out as a powerful tool, providing high-performance data manipulation and analysis capabilities. With Pandas, analysts can handle large datasets with ease, perform complex operations, and gain deep insights.Pandas is a powerful open-source library for data manipulation and analysis, primarily used with the Python programming language. It was developed by â€œWes McKinney in 2008â€ while he was working at AQR Capital Management, a quantitative investment management firm.
  
  
  Key Features and Functionality
 Pandas offers two primary data structures â€” Series (one-dimensional) and DataFrame (two-dimensional) â€” which are highly flexible and efficient for handling structured data. It provides powerful tools for data manipulation, including filtering, grouping, merging, reshaping, and pivoting data sets. Essential functions for handling missing data, duplicate data, and data type conversions, which are crucial for preparing data for analysis. we can able to integrates with other libraries and tools in the Python ecosystem, such as NumPy, Matplotlib, and SciPy, enhancing its functionality for comprehensive data analysis and visualization.Ensure Python is Installed: Make sure you have Python installed on your system. You can download it from python.org.
Open your command prompt or terminal and run the following command:Basic setup and configuration.Loading and Inspecting DataVarious types of data like CSV, Excel, SQL databases, etc can load using below command# Read a csv file
df = pd.read_csv('path_to_your_file.csv')

# Read an Excel file
df = pd.read_excel('path_to_your_file.xlsx', sheet_name='Sheet1')

# Read Sql data
from sqlalchemy import create_engine
engine = create_engine('sqlite:///path_to_your_database.db')  
df = pd.read_sql('SELECT * FROM your_table_name', con=engine)

# Read a JSON data
df = pd.read_json('path_to_your_file.json')

print(df.head())
, , , , and more. : Displays the first few rows of the DataFrame. : Displays the last few rows of the DataFrame. : Provides a concise summary of the DataFrame, including data types and missing values. : Generates descriptive statistics for numerical columns in the DataFrame, such as count, mean, standard deviation, minimum, maximum, and quartiles.: Returns the column labels of the DataFrame. : Returns the data types of each column in the DataFrame. : Returns the number of unique values in each column.
  
  
  Data Cleaning and Preparation
, ,  and interpolation. Drops rows or columns containing missing values.# Drop rows with missing values
df.dropna()

# Drop columns with missing values
df.dropna(axis=1)
 Fills missing values with specified values.# Fill missing values with a specific value
df.fillna(value=0)

# Fill missing values with the mean of the column
df.fillna(df.mean())
 Returns a DataFrame of boolean values indicating missing values.# Check for missing values
df.isnull()
 renaming columns, changing data types, and handling duplicates.# Rename columns
df.rename(columns={'old_name': 'new_name'}, inplace=True)
# Convert data types of a column
df['column_name'] = df['column_name'].astype('int')
# Drop duplicates
df.drop_duplicates()

# Keep the first occurrence of duplicates
df.drop_duplicates(keep='first')

# Keep the last occurrence of duplicates
df.drop_duplicates(keep='last')

  
  
  Data Analysis and Exploration with Pandas
Once the data is cleaned and prepared, the next steps involve performing descriptive statistics. Below are detailed examples of how to perform these tasks using Pandas.Basic Descriptive Statistics:# Generate descriptive statistics for numerical columns
df.describe()
Individual Statistic Calculations:# Mean
mean_age = df['age'].mean()

# Median
median_income = df['income'].median()

# Standard Deviation
std_income = df['income'].std()

# Variance
var_income = df['income'].var()

# Minimum
min_age = df['age'].min()

# Maximum
max_age = df['age'].max()

# Count
count_gender = df['gender'].count()

print(f"Mean Age: {mean_age}, Median Income: {median_income}")

  
  
  Data Visualization with Pandas
Data visualization is a key aspect of data analysis, helping to uncover patterns, trends, and insights that might not be apparent from the raw data alone. Pandas offers built-in plotting capabilities through its integration with Matplotlib, and it can also be used in conjunction with other libraries like Seaborn for more advanced visualizations.Basic Plotting with Pandasimport pandas as pd
import matplotlib.pyplot as plt

# Sample data
df = pd.DataFrame({
    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May'],
    'Sales': [200, 220, 250, 270, 300]
})

# Line plot
df.plot(x='Month', y='Sales', kind='line')

# Bar plot
df.plot(x='Month', y='Sales', kind='bar')

# Histogram
df['Age'].plot(kind='hist', bins=5)

# Scatter plot
df.plot(x='Height', y='Weight', kind='scatter')

plt.title('Monthly Sales')
plt.xlabel('Month')
plt.ylabel('Sales')
plt.show()
Pandas is an essential tool in numerous industries due to its powerful data manipulation and analysis capabilities. Here are some key examples of how Pandas is utilized in different fields: Financial analysts use Pandas to process and analyze stock price data. By calculating daily returns, moving averages, and plotting price trends, they can make informed investment decisions and identify market patterns. Healthcare professionals leverage Pandas to analyze patient data, such as blood pressure readings, over time. This helps in identifying trends, monitoring patient health, and conducting epidemiological research. Marketers use Pandas for customer segmentation by analyzing purchasing behaviors. Techniques such as clustering help identify different customer groups, allowing for targeted marketing strategies and personalized promotions.To master Pandas and stay updated with the latest tips and techniques, itâ€™s essential to leverage various resources, including official documentation, books, online courses, blogs, and communities. Here are some recommended resources:Official Pandas DocumentationThe official documentation is comprehensive and includes tutorials, API references, and user guides. Itâ€™s an excellent starting point for understanding Pandas functionalities in depth.An active community for data scientists, Kaggle hosts datasets, competitions, and discussions. Many notebooks on Kaggle demonstrate the use of Pandas for various analyses.Pandas is a critical tool for both data analysis and data manipulation, offering a comprehensive set of functionalities that cover the entire data workflow. Whether youâ€™re cleaning data, transforming it, analyzing trends, or visualizing results, Pandas provides the necessary tools to handle and analyze data effectively.]]></content:encoded></item><item><title>[D] Deepseek 681bn inference costs vs. hyperscale?</title><link>https://www.reddit.com/r/MachineLearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/</link><author>/u/sgt102</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 20 Feb 2025 13:44:05 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I've estimated the cost/performance of Deepseek 681bn like this :Huggingface open deepseek blog reported config & performance = 32 H100's 800tps 1million tokens = 1250s = 21 (ish) , minutes. 69.12 million tokens per day Cost to rent 32 H100's per month ~$80000Cost per million tokens = $37.33 (80000/ 31 days /69.12 ) I know that this is very optimistic (100% utilisation, no support etc.) but does the arithmetic make sense and does it pass the sniff test do you think? Or have I got something significantly wrong? I guess this is 1000 times more expensive than an API served model like Gemini, and this gap has made me wonder if I am being silly]]></content:encoded></item></channel></rss>