<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Programming</title><link>https://www.awesome-dev.news</link><description></description><item><title>Proj Ideas üí° - Willing to lock in for Go (2025)</title><link>https://www.reddit.com/r/golang/comments/1iqp4re/proj_ideas_willing_to_lock_in_for_go_2025/</link><author>/u/ComfortableAcadia839</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 16 Feb 2025 10:04:12 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm a full stack JS/TS developer but just recently tried Go, built an in memory key-value Redis clone.. I've realised the language makes me enjoy coding ---> Can y'all recommend some project ideas (intermediate to advanced difficulty)I want to build some solid projects ;)]]></content:encoded></item><item><title>NASA has a list of 10 rules for software development</title><link>https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 09:07:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[NASA has a list of 10 rules for software developmentThose rules were written from the point of view of people writing
embedded software for extremely expensive spacecraft, where tolerating
a lot of programming pain is a good tradeoff for not losing a mission.
I do not know why someone in that situation does not use the SPARK
subset of Ada, which subset was explicitly designed for verification,
and is simply a better starting point for embedded programming than C.
I am criticising them from the point of view of people writing
programming language processors (compilers, interpreters, editors)
and application software.
We are supposed to teach critical thinking.  This is an example.
How have Gerard J. Holzmann's and my different contexts affected
our judgement?
Can you blindly follow his advice without considering 
context?
Can you blindly follow  advice without considering
your context?
Would these rules necessarily apply to a different/better
programming language?  What if function pointers
were tamed?  What if the language provided opaque abstract
data types as Ada does?
1. Restrict all code to very simple control flow constructs ‚Äî
do not use  statements,
 or  constructs,
and direct or indirect .Note that  and 
are how C does exception handling, so this rule bans any use
of exception handling.

It is true that banning recursion and jumps and loops without
explicit bounds means that you  your program is
going to terminate.  It is also true that recursive functions
can be proven to terminate about as often as loops can, with
reasonably well-understood methods.  What's more important here is
that ‚Äúsure to terminate‚Äù does not imply
‚Äúsure to terminate in my lifetime‚Äù:
    int const N = 1000000000;
    for (x0 = 0; x0 != N; x0++)
    for (x1 = 0; x1 != N; x1++)
    for (x2 = 0; x2 != N; x2++)
    for (x3 = 0; x3 != N; x3++)
    for (x4 = 0; x4 != N; x4++)
    for (x5 = 0; x5 != N; x5++)
    for (x6 = 0; x6 != N; x6++)
    for (x7 = 0; x7 != N; x7++)
    for (x8 = 0; x8 != N; x8++)
    for (x9 = 0; x9 != N; x9++)
        -- do something --;
This does a bounded number of iterations.  The bound is N.
In this case, that's 10.  If each iteration of the loop body
takes 1 nsec, that's 10 seconds, or about 7.9√ó10
years.  What is the  difference between ‚Äúwill stop
in 7,900,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000
years‚Äù and ‚Äúwill never stop‚Äù?

Worse still, taking a problem that is  expressed
using recursion and contorting it into something that manipulates an
explicit stack, while possible, turns clear maintainable code into
buggy spaghetti.  (I've done it, several times.  There's an example
on this web site.  It is  a good idea.)

2. All loops must have a fixed upper-bound.  It must be trivially
possible for a checking tool to prove statically that a preset
upper-bound on the number of iterations of a loop cannot be exceeded.
If the loop-bound cannot be proven statically, the rule is considered
violated.This is an old idea.  As the example above shows, it is not enough
by itself to be of any practical use.  You have to try to make the
bounds reasonably , and you have to regard hitting an
artificial bound as a run-time error.

By the way, note that putting depth bounds on recursive procedures
makes them every bit as safe as loops with fixed bounds.

3. Do not use dynamic memory allocation after initialization.This is also a very old idea.  Some languages designed for embedded
work don't even  dynamic memory allocation.  The big
thing, of course, is that embedded applications have a fixed amount of
memory to work with, are never going to get any more, and should not
crash because they couldn't handle another record.

Note that the rationale actually supports a much stronger rule:
don't even  dynamic memory allocation.  You can of
course manage your own storage pool:
    typedef struct Foo_Record *foo;
    struct Foo_Record {
	foo next;
	...
    };
    #define MAX_FOOS ...
    static struct Foo_Record foo_zone[MAX_FOOS];
    foo foo_free_list = 0;

    void init_foo_free_list() {
	for (int i = MAX_FOOS - 1; i >= 0; i--) {
	    foo_zone[i].next = foo_free_list;
	    foo_free_list = &foo_zone[i];
	}
    }

    foo malloc_foo() {
	foo r = foo_free_list;
	if (r == 0) report_error();
	foo_free_list = r->next;
	return r;
    }

    void free_foo(foo x) {
	x->next = foo_free_list;
	foo_free_list = x;
    }
This  satisfies the rule, but it
violates the  of the rule.  Simulating malloc()
and free() this way is  than using the real
thing, because the memory in foo_zone is permanently tied up
for Foo_Records, even if we don't need any of those at the
moment but do desperately need the memory for something else.

What you really need to do is to use a memory allocator
with known behaviour, and to prove that the amount of memory
in use at any given time (data bytes + headers) is bounded
by a known value.

Note also that SPlint can verify at compile time that
the errors NASA speak of do not occur.

One of the reasons given for the ban is that the performance
of malloc() and free() is unpredictable.  Are these the only
functions we use with unpredictable performance?  Is there
anything about malloc() and free() which makes them
 unpredictable?  The existence of
hard-real-time garbage collectors suggests not.

The rationale for this rule says that

Note that the only way
to dynamically claim memory in the absence of memory allocation from the
heap is to use stack memory.  In the absence of recursion (Rule 1), an
upper bound on the use of stack memory can derived statically, thus
making it possible to prove that an application will always live within
its pre-allocated memory means.
Unfortunately, the sunny optimism shown here is unjustified.  Given
the ISO C standard (any version, C89, C99, or C11) it is 
to determine an upper bound on the use of stack memory.  There is not even
any standard way to determine how much memory a compiler will use for the
stack frame of a given function.  (There could have been.  There just isn't.)
There isn't even any requirement that two invocations of the same function
with the same arguments will use the same amount of memory.
Such a bound can only be calculated for a  version of a
specific compiler with specific options.  Here's a trivial example:
void f() {
    char a[100000];
}
How much memory will that take on the stack?  Compiled for debugging,
it might take a full stack frame (however big that is) plus traceback
information plus a million bytes for a[].  Compiled with optimisation,
the compiler might notice that a[] isn't used, and might even compile
calls to f() inline so that they generate no code and take no space.
That's an extreme example, but not really unfair.  If you want bounds
you can rely on, you had better  what your compiler does,
and recheck every time anything about the compiler changes.

4.  No function should be longer than what can be printed on
a single sheet of paper in a standard reference format with one line per
statement and one line per declaration.  Typically, this means no more
than about 60 lines of code per function.Since programmers these days typically read their code on-screen,
not on paper, it's not clear why the size of a sheet of paper is
relevant any longer.

The rule is arguably stated about the wrong thing.  The thing that
needs to be bounded is not the size of a function, but the size of a
chunk that a programmer needs to read and comprehend.

There are also question marks about how to interpret this if you
are using a sensible language (like Algol 60, Simula 67, Algol 68,
Pascal, Modula2, Ada, Lisp, functional languages like ML, O'CAML,
F#, Clean, Haskell, or Fortran) that allows nested procedures.
Suppose you have a folding editor that presents a procedure to
you like this:
function Text_To_Floating(S: string, E: integer): Double;
   ÔøΩ variables ÔøΩ
   ÔøΩ procedure Mul(Carry: integer) ÔøΩ
   ÔøΩ function Evaluate: Double ÔøΩ

   Base, Sign, Max, Min, Point, Power := 10, 0, 0, 1, 0, 0;
   for N := 1 to S.length do begin
       C := S[N];
       if C = '.' then begin
          Point := -1
       end else
       if C = '_' then begin
          Base := Round(Evaluate);
          Max, Min, Power := 0, 1, 0
       end else
       if Char ‚â† ' ' then begin
          Q := ord(C) - ord('0');
          if Q > 9 then Q := ord(C) - ord('A') + 10
          Power := Point + Point
          Mul(Q)
       end
    end;
    Power := Power + Exp;
    Value := Evaluate;
    if Sign < 0 then Value := -Value;
end;
which would be much bigger if the declarations
were expanded out instead of being hidden behind ÔøΩfoldsÔøΩ.
Which size do we count?  The folded size or the unfolded size?
I was using a folding editor called Apprentice on the Classic Mac
back in the 1980s.  It was written by Peter McInerny and was lightning
fast.

5.  The  of the code should average to a minimum of
two assertions per function.Assertions are wonderful documentation and the very best debugging tool
I know of.  I have never seen any real code that had too many assertions.

The example here is one of the ugliest pieces of code I've seen in a while.
if (!c_assert(p >= 0) == true) {
    return ERROR;
}
It should, of course, just be
if (!c_assert(p >= 0)) {
    return ERROR;
}
Better still, it should be something like
#ifdef NDEBUG
#define check(e, c) (void)0
#else
#define check(e, c) if (!(c)) return bugout(c), (e)
#ifdef NDEBUG_LOG
#define bugout(c) (void)0
#else
#define bugout(c) \
    fprintf(stderr, "%s:%d: assertion '%s' failed.\n", \
    __FILE__, __LINE__, #s)
#endif
#endif
Ahem.  The more interesting part is the required density.
I just checked an open source project from a large telecoms
company, and 23 out of 704 files (not functions) contained
at least one assertion.  I just checked my own Smalltalk
system and one SLOC out of every 43 was an assertion, but
the average Smalltalk ‚Äúfunction‚Äù is only a few
lines.  If the biggest function allowed is 60 lines, then
let's suppose the average function is about 36 lines, so
this rule requires 1 assertion per 18 lines.
Assertions are good, but what they are especially good
for is expressing the requirements on data that come
from outside the function.  I suggest then that
Every argument whose validity is not guaranteed by
its typed should have an assertion to check it.
Every datum that is obtained from an external
source (file, data base, message) whose validity is
not guaranteed by its type should have an assertion
to check it.
The NASA 10 rules are written for embedded systems, where
reading stuff from sensors is fairly common.

6.  Data objects must be declared at the smallest possible level of
scope.This is excellent advice, but why limit it to data objects?
Oh yeah, the rules were written for crippled languages where you
 declare functions in the right place.

People using Ada, Pascal (Delphi), JavaScript, or functional
languages should also declare types and functions as locally as
possible.

7.  The return value of non-void functions must be checked by each
calling function, and the validity of parameters must be checked inside
each function.This again is mainly about C, or any other language that indicates
failure by returning special values.  ‚ÄúStandard libraries
famously violate this rule‚Äù?  No, the  library does.

You have to be reasonable about this: it simply isn't practical
to check  aspect of validity for 
argument.  Take the C function
void *bsearch(
    void const *key  /* what we are looking for */,
    void const *base /* points to an array of things like that */,
    size_t      n    /* how many elements base has */,
    size_t      size /* the common size of key and base's elements */
    int (*      cmp)(void const *, void const *)
);
This does a binary search in an array.  We must have key‚â†0,
base‚â†0, size‚â†0, cmp‚â†0, cmp(key,key)=0, and for all
1<i<n,
cmp((char*)base+size*(i-1), (char*)base+size*i) <= 0
Checking the validity in full would mean checking
that [key..key+size) is a range of readable addresses,
[base..base+size*n) is a range of readable addresses,
and doing n calls to cmp.  But the whole point of binary
search is to do O(log(n)) calls to cmp.

The fundamental rules here are
Don't let run-time errors go un-noticed, and
any check is safer than no check.
8. The use of the preprocessor must be limited to the inclusion of
header files and simple macro definitions.  Token pasting, variable
argument lists (ellipses), and recursive macro calls are not allowed.Recursive macro calls don't really work in C, so no quarrel there.
Variable argument lists were introduced into macros in
C99 so that you could write code like
#define err_printf(level, ...) \
    if (debug_level >= level) fprintf(stderr, __VA_ARGS__)
...
    err_printf(HIGH, "About to frob %d\n", control_index);
This is a  thing; conditional tracing like this is a
powerful debugging aid.  It should be , not banned.

The rule goes on to ban macros that expand into things that are
not complete syntactic units.  This would, for example, prohibit
simulating try-catch blocks with macros.  (Fair enough, an earlier rule
banned exception handling anyway.)  Consider this code fragment, from
an actual program.
    row_flag = border;     
    if (row_flag) printf("\\hline");
    for_each_element_child(e0, i, j, e1)
        printf(row_flag ? "\\\\\n" : "\n");
        row_flag = true;  
        col_flag = false;
        for_each_element_child(e1, k, l, e2)
            if (col_flag) printf(" & ");
            col_flag = true;
            walk_paragraph("", e2, "");
        end_each_element_child
    end_each_element_child
    if (border) printf("\\\\\\hline");
    printf("\n\\end{tabular}\n");
It's part of a program converting slides written in something like HTML
into another notation for formatting.  The 
‚Ä¶  loops walk over a tree.  Using
these macros means that the programmer has no need to know and no reason to
care how the tree is represented and how the loop actually works.
You can easily see that  must have at
least one unmatched { and  must have at least one
unmatched }.  That's the kind of macro that's banned by requiring
complete syntactic units.  Yet the readability and maintainability of
the code is  improved by these macros.

One thing the rule covers, but does not at the beginning stress, is
‚Äúno  macro processing‚Äù.  That is,
no #if.  The argument against it is, I'm afraid, questionable.  If there
are 10 conditions, there are 2 combinations to test,
whether they are expressed as compile-time conditionals or run-time
conditionals.

In particular, the rule against conditional macro processing
would prevent you defining your own assertion macros.
It is not obvious that that's a good idea.

9.  The use of pointers should be restricted.  Specifically, no more
than one level of dereferencing is allowed.  Pointer dereference
operations may not be hidden in macro definitions or inside typedef
declarations.  Function pointers are not permitted.Let's look at the last point first.

double integral(double (*f)(double), double lower, double upper, int n) {
    // Compute the integral of f from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += f((lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += f(lower + h*i);
    return (f(lower) + f(upper) + s*4.0 + t*2.0) * (h/6.0);
}
This kind of code has been important in numerical calculations since
the very earliest days.  Pascal could do it.  Algol 60 could do it.
In the 1950s, Fortran could do it.  And NASA would ban it, because in
C,  is a function pointer.

Now it's important to write functions like this once and only once.
For example, the code has at least one error.  The comment says n+1
points, but the function is actually evaluated at 2n+1 points.  If we
need to bound the number of calls to f in order to meet a deadline,
having that number off by a factor of two will not help.
It's nice to have just one place to fix.
Perhaps I should not have copied that code from a well-known source (:-).
Certainly I should not have more than one copy!

What can we do if we're not allowed to use function pointers?
Suppose there are four functions foo, bar, ugh, and zoo that we need
to integrate.  Now we can write
enum Fun {FOO, BAR, UGH, ZOO};

double call(enum Fun which, double what) {
    switch (which) {
        case FOO: return foo(what);
        case BAR: return bar(what);
        case UGH: return ugh(what);
        case ZOO: return zoo(what);
    }
}

double integral(enum Fun which, double lower, double upper, int n) {
    // Compute the integral of a function from lower to upper 
    // using Simpson's rule with n+1 points.
    double const h = (upper - lower) / n;
    double       s;
    double       t;
    int          i;
    
    s = 0.0;
    for (i = 0; i < n; i++) s += call(which, (lower + h/2.0) + h*i);
    t = 0.0;
    for (i = 1; i < n; i++) t += call(which, lower + h*i);
    return (call(which, lower) + call(which, upper) + s*4.0 + t*2.0) * (h/6.0);
}
Has obeying NASA's rule made the code more reliable?  No, it has made
the code  to understand,  maintainable, and
 that it wasn't before.  Here's a call
illustrating the mistake:
x = integral(4, 0.0, 1.0, 10);I have checked this with two C compilers and a static checker at their
highest settings, and they are completely silent about this.

So there are legitimate uses for function pointers, and simulating
them makes programs , not better.

Now  in Fortran,
Algol 60, or Pascal.  Those languages had procedure 
but not procedure . You could pass a subprogram name as
a parameter, and such a parameter could be passed on, but you could not
store them in variables.  You could have a  of C which
allowed function pointer parameters, but made all function pointer
variables read-only.  That would give you a statically checkable subset
of C that allowed integral().

The other use of function pointers is simulating object-orientation.
Imagine for example
struct Channel {
    void (*send)(struct Channel *, Message const *);
    bool (*recv)(struct Channel *, Message *);
    ...
};
inline void send(struct Channel *c, Message const *m) {
    c->send(c, m);
}
inline bool recv(struct Channel *c, Message *m) {
    return c->recv(c, m);
}
This lets us use a common interface for sending and receiving
messages on different kinds of channels.  This approach has been
used extensively in operating systems (at least as far back as
the Burroughs MCP in the 1960s) to decouple the code that uses
a device from the actual device driver.     I would expect any
program that controls more than one hardware device to do something
like this.  It's one of our key tools for controlling complexity.
Again, we can simulate this, but it makes adding a new kind of
channel harder than it should be, and the code is 
when we do it, not better.

The rule against more than one level of dereferencing is also
an assault on good programming.  One of the key ideas that was
developed in the 1960s is the idea of ;
the idea that it should be possible for one module to define a
data type and operations on it and another module to use instances
of that data type and its operations without having to know
anything about what the data type is.
One of the things I detest about Java is that it spits in the
face of the people who worked out that idea.  Yes, Java (now) has
generic type parameters, and that's good, but you cannot use a
 type without knowing what that type is.

Suppose I have a module that offers operations
And suppose that I have two interfaces in mind.  One of them
uses integers as tokens.
// stasher.h, version 1.
typedef int token;
extern token stash(item);
extern item  recall(token);
extern void  delete(token);
Another uses pointers as tokens.
// stasher.h, version 2.
typedef struct Hidden *token;
extern  token stash(item);
extern  item  recall(token);
extern  void  delete(token);
void snoo(token *ans, item x, item y) {
    if (better(x, y)) {
	*ans = stash(x);
    } else {
	*ans = stash(y);
    }
}
By the NASA rule, the function snoo() would not be accepted or rejected on
its own merits.  With stasher.h, version 1, it would be accepted.
With stasher.h, version 2, it would be rejected.

One reason to prefer version 2 to version 1 is that version 2 gets
more use out of type checking.  There are ever so many ways to get an
int in C.  Ask yourself if it ever makes sense to do
token t1 = stash(x);
token t2 = stash(y);
delete(t1*t2);
I really do not like the idea of banning abstract data types.

10.  All code must be compiled, from the first day of development,
with all compiler warnings enabled at the compiler‚Äôs
most pedantic setting.  All code must compile with these setting without
any warnings.  All code must be checked daily with at least one, but
preferably more than one, state-of-the-art static source code analyzer
and should pass the analyses with zero warnings.This one is good advice.  Rule 9 is really about making your code
worse in order to get more benefit from limited static checkers.  (Since
C has no standard way to construct new functions at run time, the set of
functions that a particular function pointer  point to can
be determined by a fixed-point data flow analysis, at least for most
programs.)  So is rule 1.  



]]></content:encoded></item><item><title>Resigning as Asahi Linux project lead</title><link>https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 09:01:13 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them ‚Äújailbreaks‚Äù these days) to allow users to run their own unofficial apps on the Nintendo Wii.I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other people‚Äôs work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didn‚Äôt attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which I‚Äôve already spoken at length about and I won‚Äôt repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience. (Clarification: This has nothing to do with Rust at this point, it‚Äôs well before R4L was even merged. Upstreaming to Linux is a terrible experience in C too.)But then also came the entitled users. This time, it wasn‚Äôt about stealing games, it was about features. ‚ÄúWhen is Thunderbolt coming?‚Äù ‚ÄúAsahi is useless to me until I can use monitors over USB-C‚Äù ‚ÄúThe battery life sucks compared to macOS‚Äù (nobody ever complained when compared to x86 laptops‚Ä¶) ‚ÄúI can‚Äôt even check my CPU temperature‚Äù (yes, I seriously got that one). (Edit: This wasn‚Äôt just a few instances; I‚Äôve seen variations on the first three posted hundreds of times by now, including takes like ‚ÄúThunderbolt/DP Alt are never going to happen‚Äù. A few times is fine, but the same thing repeated over and over again every day while we‚Äôre trying to make these things happen will get to anyone.)And, of course, ‚ÄúWhen is M3/M4 support coming?‚ÄùFor a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were ‚Äúalpha‚Äù and ‚Äúunstable‚Äù and ‚Äúnot suitable for a daily driver‚Äù (despite thousands of users, myself included, daily driving it and even using it for servers).No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I won‚Äôt go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).I did make some progress in 2024, but this left me in a very vulnerable position. I hadn‚Äôt gotten nearly as much Asahi work done as I‚Äôd liked, and the users weren‚Äôt getting any quieter about demanding more features and machine support.We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. ‚ÄúFull‚Äù DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasn‚Äôt the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did try to bring the subject up on the mailing lists, but it didn‚Äôt get much response).The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I won‚Äôt repeat them in detail here. Suffice it to say, I consider Linus‚Äô handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.As you know, this is deeply personal to me, as we‚Äôve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.Some might be wondering why we can‚Äôt just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But there‚Äôs more.In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (that‚Äôs how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a partial workaround for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.I‚Äôm not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated ‚ÄúWe
are the ‚Äòthin blue line‚Äô‚Äù, and nobody cared, which just further confirmed to me that I don‚Äôt want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to quit.But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than bunny.net giving us free CDN credits for the hosting).I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say I‚Äôm the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they won‚Äôt touch the Linux kernel with a 10-foot pole.When Apple released the M1, Linus Torvalds wished it could run Linux, but didn‚Äôt have much hope it would ever happen. We made it happen, and Linux 5.19 was released from an M2 MacBook Air running Asahi Linux. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him an invitation to discuss the challenges of kernel contributions and maintenance and see how we could help. He never replied.Back in 2011, Con Kolivas left the Linux kernel community. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if you‚Äôre not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but it‚Äôs not fun any more. I have an M3 Pro in a box and I haven‚Äôt even turned it on yet. I dread doing the bring-up work. It doesn‚Äôt feel worth the trouble.I miss having free time where I can relax and not worry about the features we haven‚Äôt shipped yet. I miss making music. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we haven‚Äôt upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.I‚Äôm resigning as lead of the Asahi Linux project, effective immediately. The project will continue on without me, and I‚Äôm working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the Asahi Linux OpenCollective (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).I want to thank the entire Asahi Linux team, without whom I would‚Äôve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: marcan@marcan.st.: A lot of the discussion around this post and the interactions that led to it brings up the term ‚Äúbrigading‚Äù. Please read this excellent Fedi post for a discussion of what is and isn‚Äôt brigading.]]></content:encoded></item><item><title>Are Fast Programming Languages Gaining in Popularity?</title><link>https://developers.slashdot.org/story/25/02/16/0332258/are-fast-programming-languages-gaining-in-popularity?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 16 Feb 2025 08:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[In January the TIOBE Index (estimating programming language popularity) declared Python their language of the year. (Though it was already #1 in their rankings, it had showed a 9.3% increase in their ranking system, notes InfoWorld.) TIOBE CEO Paul Jansen says this reflects how easy Python is to learn, adding that "The demand for new programmers is still very high" (and that "developing applications completely in AI is not possible yet.") 

In fact on February's version of the index, the top ten looks mostly static. The only languages dropping appear to be very old languages. Over the last 12 months C and PHP have both fallen on the index ‚Äî C from the #2 to the #4 spot, and PHP from #10 all the way to #14. (Also dropping is Visual Basic, which fell from #9 to #10.) 

But TechRepublican cites another factor that seems to be affecting the rankings: language speed.


Fast programming languages are gaining popularity, TIOBE CEO Paul Jansen said in the TIOBE Programming Community Index in February. Fast programming languages he called out include C++ [#2], Go [#8], and Rust [#13 ‚Äî up from #18 a year ago]. 

Also, according to the updated TIOBE rankings... 
- C++ held onto its place at second from the top of the leaderboard.
- Mojo and Zig are following trajectories likely to bring them into the top 50, and reached #51 and #56 respectively in February. 

"Now that the world needs to crunch more and more numbers per second, and hardware is not evolving fast enough, speed of programs is getting important. Having said this, it is not surprising that the fast programming languages are gaining ground in the TIOBE index," Jansen wrote. The need for speed helped Mojo [#51] and Zig [#56] rise... 

Rust reached its all-time high in the proprietary points system (1.47%.), and Jansen expects Go to be a common sight in the top 10 going forward.
]]></content:encoded></item><item><title>Which approach to rust is more idiomatic (Helix vs Zed)?</title><link>https://www.reddit.com/r/rust/comments/1iqnats/which_approach_to_rust_is_more_idiomatic_helix_vs/</link><author>/u/No_Penalty2781</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 07:50:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi! I am curious what is the current "meta" (by "meta" I mean the current rust's community  and  way of doing things) of rust programming. I am studying source code of 2 editors I am using: Helix and Zed. And I can see that while they are doing a lot of similar things (like using LSP and parsing it outputs for example) the code is kinda different.It starts from the file structure: in Helix there are not that many folders to look at (like you have helix-core which contains features like "diagnostic", "diff", "history", etc but in Zed every single one of them is a different crate , which approach is more "idiomatic"? To divide every feature as a separate crate or to use more "packed" crates like "core".Then the code itself is kinda different, for example I am currently looking at LSP implementation in both of them and in Helix's case I can follow along and understand the code much more easily (here is the file I am referring to. But in Zed's case it is kinda hard to understand the code because of "type level programming" stuff like this one for example. It also doesn't help that files have a lot of SLOC in them (over 1500 in normal in Zed's repository, is it also how you do rust?) Maybe I am just used to lean functions from other languages (I mainly did TypeScript and Elixir in my career).Other thing I see is that Helix has more comments about "why the thing is doing that in the first place" which I find very helpful (on the other hand in seems that Zed's is abusing a lot of "type level" programming to have a self-documented code but it is harder to reason about at least for me) which approach here you prefer?]]></content:encoded></item><item><title>[R] A Survey of Logical Reasoning Capabilities in Large Language Models: Frameworks, Methods, and Evaluation</title><link>https://www.reddit.com/r/MachineLearning/comments/1iqmjal/r_a_survey_of_logical_reasoning_capabilities_in/</link><author>/u/Successful-Western27</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 06:55:36 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[This new survey provides a comprehensive analysis of logical reasoning capabilities in LLMs, examining different reasoning types, evaluation methods, and current limitations.Key technical aspects: - Categorizes logical reasoning into deductive, inductive, and abductive frameworks - Evaluates performance across multiple benchmarks and testing methodologies - Analyzes the relationship between model size and reasoning capability - Reviews techniques for improving logical reasoning, including prompt engineering and chain-of-thought methodsMain findings: - LLMs show strong performance on basic logical tasks but struggle with complex multi-step reasoning - Model size alone doesn't determine reasoning ability - training methods and problem-solving strategies play crucial roles - Current evaluation methods may not effectively distinguish between true reasoning and pattern matching - Performance degrades significantly when problems require combining multiple reasoning typesI think the most important contribution here is the systematic breakdown of where current models succeed and fail at logical reasoning. This helps identify specific areas where we need to focus research efforts, rather than treating reasoning as a monolithic capability.I think this work highlights the need for better benchmarks - many current tests don't effectively measure true reasoning ability. The field needs more robust evaluation methods that can differentiate between memorization and actual logical inference.TLDR: Comprehensive survey of logical reasoning in LLMs showing strong basic capabilities but significant limitations in complex reasoning. Highlights need for better evaluation methods and targeted improvements in specific reasoning types.]]></content:encoded></item><item><title>Speedrunning Guide: Junior to Staff Engineer in 3 years</title><link>https://blog.algomaster.io/p/speedrunning-guide-junior-to-staff</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0173844b-0e38-4ca8-b779-b34f7f778872_1600x413.png" length="" type=""/><pubDate>Sun, 16 Feb 2025 04:30:58 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[Today‚Äôs newsletter features a special guest, , who was promoted from Junior to Staff Engineer at Meta in just 3 years.In this article, Ryan will share his insights on how to fast track your career growth and get promoted faster.Once you land that first software engineering job, the next big question becomes: how do you get promoted? Many engineers fall into the day-to-day routine of writing code without a clear idea of how to grow their careers.This happened to me. At my first job at Amazon, I landed code without knowing what I could do to grow my skills. I left that job within eight months because I felt I wasn‚Äôt growing as an engineer. Three years later, I made it to Staff Software Engineer at Instagram after tons of mentorship. Early on, I learned that being good at coding wasn‚Äôt enough to get promoted; you have to think strategically about your career and often need to develop new behaviors to move up.In this article, I‚Äôll share everything that helped me fast-track my way up the ladder, from developing the right mindset to making key moves that many overlook. Even if rapid growth isn‚Äôt your goal, this guide has learnings for all tech career paths.Software Engineering LevelsAn Algorithm for PromotionJunior (IC3) ‚Üí Mid-level (IC4)Mid-level (IC4) ‚Üí Senior (IC5)Senior (IC5) ‚Üí Staff (IC6)Software Engineering LevelsNote: ‚ÄúIC‚Äù = ‚ÄúIndividual Contributor‚ÄùIn software engineering, companies measure career progression by levels that measure both behaviors and impact within the company. While the exact titles and structure can vary between companies, most tech companies follow a similar system: - Early in your career, working on smaller, well-defined tasks with guidance from more experienced engineers. - More autonomous, handling moderately complex projects, and beginning to take initiative in improving the codebase and what they build. - Leading larger projects with team-level influence. You‚Äôll mentor and guide the team while having a broad impact on the codebase.: Focusing on cross-team collaboration and solving org-wide challenges. Staff engineers are strategic thinkers who influence the technical direction of their organization.Senior Staff Engineer and Beyond (IC7+): Senior staff engineers and up operate with top technical expertise, driving large-scale initiatives that have a broad impact on the company. Senior staff engineers mentor staff engineers and work closely with executive leadership to meet business objectives.Your impact and compensation increase as you progress, which is a lot more satisfying in my experience. Not to mention that the skills that get you promoted also let you control what you and the company work on.Also, many companies consider only senior engineers (IC5) and higher to be ‚Äúterminal levels.‚Äù You must eventually get promoted to IC5, or you‚Äôll be managed out. Most engineers are promoted in time, so it‚Äôs not meant to scare you but to encourage you to grow.An Algorithm For PromotionThere‚Äôs a common set of steps across all promotions that will get you to Staff:1) Exceed expectations at your current level - Your manager will be hesitant to find you opportunities at the next level if they have concerns about your performance at the current level. Also, when your manager puts together a promotion packet, it‚Äôll contain a history of your past ratings. The promotion committee will have concerns about your packet if you have a history of only meeting expectations for your level. Work with your manager to understand the expectations for your level and how to exceed them.2) Be direct with your manager about promotion - Once you know you‚Äôre exceeding expectations for your level, ask your manager what next-level performance looks like. Your manager plays a huge role in your promotion. They build your case and advocate for it, so they have a lot of influence on this process. Also, the lower the level, the more control your manager has. IC3 -> IC4 promotions are straightforward, so your manager‚Äôs perspective is usually what happens. For IC5 -> IC6, there is a lot more ambiguity, so your manager serves more as a middleman between you and the promotion committee. Your manager still plays a significant role in writing your packet and delivering feedback.3) Find next-level scope - If you only work on projects that fit your level‚Äôs behaviors, you won‚Äôt get any closer to promotion, no matter how good your work is. One simple pattern for finding next-level scope is brainstorming projects with engineers who are 1-2 levels higher than you are. Often, they will have a lot of projects sitting in their backlog that are big enough to help you get promoted. If you take on one of their projects, they‚Äôll often help mentor you, review your designs and code, and give you strong peer feedback for your future promotion packet. I wrote more on this here. Make sure to confirm with your manager that they agree that what you‚Äôre working on fits the behaviors of the next level.4) Maintain next-level behaviors and impact - The duration you need to perform at the next level varies depending on your level and your company. At minimum though, you need to maintain that performance for 6-12 months. This is because promotions are ‚Äúlagging‚Äù in tech. You must prove that you‚Äôre already operating at the next level before getting promoted. This reduces the risk of failing to meet expectations at the new level.Getting promoted faster is a matter of doing steps (1), (2) and (3) as fast as possible. The best you can do is immediately start exceeding expectations in your first half and working with your manager on the next level.Almost every team has scope for more Senior Engineers (IC5). You can get promoted up to that level if you have the skills and behaviors. Past that, situation and business scope play a much larger role. Many teams don‚Äôt need someone who has Staff-level leadership and technical skills. If you find yourself stuck at any point due to your situation, you‚Äôll likely have to switch teams to continue growing your career.Now that you have the algorithm that applies at any of these levels let‚Äôs get into the level-specific strategies. I‚Äôll share what got me promoted and what I would change if I did it again.Junior (IC3) ‚Üí Mid-level (IC4)The main difference between these levels is in the size of the scope that you can handle independently. Here‚Äôs a rule of thumb:IC3 - Can handle individual tasks (<2 weeks of work) with minimal guidanceIC4 - Can handle medium-to-large features (<2 months of work) with minimal guidance‚ÄúMinimal guidance‚Äù doesn‚Äôt mean that you can‚Äôt ask for help‚Äîit simply means that you can unblock yourself and make consistent progress. Asking good questions is one of the most effective ways to unblock yourself.You should drive full features and do the project management for them. You should break your project into tasks, set reasonable timelines, and keep stakeholders updated.You will not be expected to come up with the projects yet at this level‚ÄîSenior Engineers will often outline them. However, at the IC4 level, you‚Äôre expected to take more initiative:Initiate refactoring and code cleanups, and give thoughtful code reviews. Leave the code in a better state than you found it.Contributing to production excellence - Participate in the team‚Äôs oncall, and help debug production breakages.Own the health of what you build - Add test coverage, logging, and build dashboards to monitor correctness.Optimize your dev velocity to grow faster at this level. Shipping code faster creates a shorter feedback loop, accelerating your learning process. This core skill will help you ship IC4-scope projects and improve as an engineer.Here‚Äôs an example of several promotion timelines for what you can expect:Promotion in 6 months (exceptional) - Rare since this means meeting IC4 expectations while onboarding. This is easier for high-performing return interns since they skip onboarding and may have some past track record already.Promotion in 12 months (great) - I‚Äôd shoot for this timeline. It‚Äôs challenging yet reasonable since it gives you six months to onboard and then start meeting IC4 expectations.Here‚Äôs my promotion timeline as an example:H1 (L3 Exceeds Expectations) - First, I took on any task that came my way. These were nice-to-have features that others didn‚Äôt have time for. I completed them quickly and started on a larger pipeline rewrite (L4 scope) that my tech lead offered me. Outside of my main project work, I made many contributions to removing dead code and speeding up existing code because I enjoyed it.I started to hit L4 expectations in the last few months of the half. But, since I didn‚Äôt have six months track record, I didn‚Äôt meet the promotion criteria.H2 (L3‚ÜíL4 Promotion, Greatly Exceeds Expectations) - I continued driving my L4-scope project independently with high engineering quality. I came up with the idea to build a test harness to validate this rewrite that was ‚Äúcomparable to L5 quality‚Äù execution. I continued my passion for improving the codebase and led the company in adding static type annotations that MonkeyType couldn‚Äôt.At this point, I had delivered on L4 scope for over six months, so the promotion made sense.What I Would Have Changed:Looking back, I would have discussed what IC4 growth looked like with my manager. I wasted our one-on-one time on project updates instead of career growth. This led to two problems:Spent time on work that wasn‚Äôt impactful - I took on any work that was passed my way, even though not all of it was impactful. I probably could‚Äôve gotten more out of my time.Didn‚Äôt have accurate expectations - I had another engineer tell me my work was IC4 level and that I should get promoted in my first half. I knew nothing then, so I took their word for it. I was surprised when I didn‚Äôt get promoted, which could have been avoided if I had been in sync with my manager.Although I could have been more calculated, writing as much code as I did opened doors. My tech lead trusted me with an IC4 project because I showed I could handle it. Similarly, some of the engineering craft work I did for my own personal pleasure ended up being part of what got me promoted too. The more work you do, the luckier you get.Mid-level (IC4) ‚Üí Senior (IC5)The IC4 to IC5 gap is larger than the IC3 to IC4 one. This is because IC5 promotion requires significant behavior changes. Raw code output is no longer the top priority. You need to lead and have a larger influence within your team too. Here are a few examples of those differences:Example 1 - Improving the codebaseIC4 - Initiates refactoring and code cleanups.IC5 - Identifies areas of improvement, influences the team to take goals on improving it together, then leads the charge on those goals.Example 2 - Production excellenceIC4 - Participates in team‚Äôs oncall and mitigating outages.IC5 - Creates an ‚Äúoncall improvement‚Äù workstream and builds a process for everyone to improve the team‚Äôs oncall.Example 3 - Project directionIC4 - Owns the project management of a medium-to-large feature.IC5 - Drives team planning and builds a roadmap of several medium-to-large features.I wouldn‚Äôt say the IC5 examples are harder, but they require a mindset shift to own things at the team level.Also, you‚Äôll need to work on projects of sufficient scope for an IC5. There are a few ways that tech companies measure scope. Here‚Äôs a comparison of the criteria for IC4 and IC5 levels:These criteria aren‚Äôt a checklist. Your work can be IC5 scope by meeting only some of these criteria.IC5 is also the first time engineers begin to focus on growing others. At this level, you should mentor others and build up the team‚Äôs culture, which includes driving meetings, knowledge sharing, recruiting activities, and organizing team activities. Starting mentorship relationships early is a good idea since you can‚Äôt rush mentorship.If you can learn the above behaviors quickly, you can expect promotion on these timelines:Promotion in 6 months (exceptional) - This is rare since you need to exert team-level influence as soon as you join the team. I could see this happening for someone who was under-leveled and just got promoted to IC4.Promotion in 12 months (great) - If you‚Äôre ambitious, I‚Äôd aim for this goal. It is possible to do this if you find IC5 scope in your first half. If not, one more half should secure your promotion.Here‚Äôs my promotion timeline as an example:H1 (IC4 Exceeds Expectations) - This half I wrapped up the workstream that got me promoted to IC4 and picked up another IC4 project. I spent a ton of time on engineering craft this half because I enjoyed it. I deprecated a few legacy systems that no one else would because they were dangerous and not that impactful. I didn‚Äôt exhibit any IC5 behaviors this half.My manager handed me an IC5 workstream (~6 eng) to cut video messaging latency in half, which I led successfully. I also began a side project, which became a multiple-half collaboration with another team. Lastly, I took on an intern who did a phenomenal job helping me execute these two roadmaps I led. Although I started exhibiting IC5 behaviors, the company canceled performance reviews this half because of the pandemic.H3 (IC4 ‚Üí IC5 Promotion, Greatly Exceeds Expectations) - My impact this half could‚Äôve met expectations at the IC6 level. I doubled down on the cross-org scope I created in H2 and developed a multi-half roadmap. I influenced and led another team to invest several engineers to revamp the IG video ads pipeline with great results. I built out a second workstream and mentored another engineer to deliver it. This half, I had massive impact, team-level influence, and mentorship, which is what got me promoted.The Skill of Tech Leading - If you grew from L3 ‚Üí L4 right, you should be exceptional at landing code. The L5 behavior of team-level influence is just helping others do the same. In my first half of leading an initiative, I remember feeling unsure about it since I only had two years of experience. Leaning on my strong execution skills helped me become comfortable leading others.Working Hard Led To More Opportunities - I worked a lot and had a ton of workstreams in flight at the same time. This approach increased my chances of having one that had a ton of impact. At the time, I didn‚Äôt know it and was just throwing myself at any problems that came my way. Looking back, it was a great way to derisk my promotion. - In my first half as an L4, I took on projects that were time-intensive and not impactful. I did these migrations because I loved cleaning up tech debt. I would‚Äôve had more impact if I had influenced someone else to do them while I found IC5 scope instead.Senior (IC5) ‚Üí Staff (IC6)Staff Engineers (IC6) are at the same level as engineering managers. They solve problems that few others can and play a critical role in setting team direction. They lead major initiatives and influence the engineering culture of teams around them.Some say that promotion from IC5 ‚Üí IC6 is harder than IC6 ‚Üí IC7 due to the significant behavior changes needed. There are a few major differences between IC5 and IC6.1) Influence Across Teams - Staff Engineer‚Äôs projects often extend beyond their team. They take on larger problems by influencing other teams without authority.Once IC6s establish these workstreams, they tackle the hardest problems and work through others. They focus on outcomes and don‚Äôt always do the work themselves. Working through delegation and influence across teams is the biggest mindset shift from IC5 ‚Üí IC6.This style of working isn‚Äôt limited to their main project impact. IC6s should also use their influence to inspire a culture of higher engineering quality and reliability across teams. - Senior Engineers (IC5) build roadmaps of several medium-to-large features that help achieve their team‚Äôs goals. In this case, the problem and its business impact are clear; we just need an engineer to create a plan to solve it.Staff Engineers (L6) handle more ambiguity. They don‚Äôt just solve known problems; they create scope by finding impactful opportunities and problems. Managers work with their L6s to expand the scope of the team.- Big tech companies determine what level projects are in a few ways. Here‚Äôs a comparison of the criteria for L5 and L6 levels:Project complexity also distinguishes IC6 scope. Problems that IC5s can‚Äôt solve are considered IC6 scope. This is why specialists often have IC6+ scope; others often can‚Äôt do their projects.These criteria aren‚Äôt a checklist. Work can be IC6 scope by meeting only some of these. Your manager will use these criteria to argue that your work is IC6 scope. This is one of the reasons why it‚Äôs important to align with your manager on your work‚Äôs scope. - Staff engineers uplift others around them. They should have the ability to help IC5 engineers grow. There are a few ways they uplift others:Mentorship - Dedicated mentorship, preferably with senior engineersKnowledge sharing - Writing wikis, giving presentations, contributing to Q&A groupsCollaborations - Growing others while working with them (e.g. code reviews, design reviews, discussions)IC6 engineers should also contribute to growing the organization. This means that they help with recruiting and partner with their manager to improve team health.Getting to the Staff Engineering level can take a long time. Since IC5 is considered ‚Äúterminal,‚Äù there is no external pressure to achieve IC6 fast. However, if you are eager to grow as fast as possible, here‚Äôs how fas you can expect promotion:Promotion in 1 half (Ridiculous) - You‚Äôd need to start influencing outside your team as soon as you join. Even then, it‚Äôs unlikely you‚Äôd get promoted this fast unless you create something company-changing.Promotion in 2 halves (Exceptional) - Finding IC6 opportunities on your team is not always possible this fast. It‚Äôs a combination of situation and skill to get promoted in two halves, even if you execute well.Promotion in 3 halves (Great) - If you‚Äôre ambitious I‚Äôd aim for this goal. It gives you a year to find IC6 scope, which is a reasonable amount of time to pivot if needed. Also, your track record of successes in the first year will help build the narrative for promotion.H1 (IC5 Exceeds Expectations) - I led two workstreams that were partnerships with other teams to hit our goals. I also landed a large win in an unplanned ads workstream, which is what brought my rating above expectations. I was also one of the top contributors to code review and interviewing in my 70-person eng org. The hidden success here was that I bootstrapped a new workstream towards the end of the half that was certainly IC6 scope.H2 (IC5 ‚Üí IC6 Promotion, Greatly Exceeds Expectations) - The IC6 workstream I created turned out to be a massive opportunity. This work was a huge success, resulting in a company-wide award and public recognition from Mark Zuckerberg. I also created a cross-org collaboration between 3 large orgs (70+ eng each), which received positive feedback from each director. Lastly, I ran infrastructure preparations for my org resulting in no major incidents during the most critical time of the year. The repeated influence and impact of these large initiatives is what got me promoted to IC6. - My past context and relationships at Instagram helped me move a lot faster. I could lead several workstreams at once because I knew so much about the codebase. Also, it was easier to get work done in collaboration because I knew partner engineers from past work. Staying at one company for a longer time does have its benefits. - When I was an IC4, I stumbled upon some IC6 scope without realizing it. I had strong initiative so I started solving problems without thinking through why it was impactful. I got lucky that the work had IC6 impact. I‚Äôve since learned the importance of understanding the ‚Äúwhy‚Äù before diving in. It helps you have consistent IC6 impact and makes it easier to get buy-in for your work.The Tech Lead Skillset Scales Well - In my promotion to Senior (IC5), I learned how to lead initiatives within my team. This skillset turned out to work well at higher levels too. The difference was just that more people were involved. This skill is a great way to continue your IC growth to the highest levels if you fit the ‚Äútech lead‚Äù archetype.Growth to the Staff level can take a long time, and luck plays a role. As you move up the ladder, each promotion depends more and more on your situation in addition to your skill.There are ways to increase your luck. For instance, you can go to growing companies and teams. You can pick business-critical projects. You can go where the most talented people are. None of these are foolproof, but they increase your chances.Aside from picking your situation, one way to manufacture luck is to do as much good work as you can. Many growth opportunities came to me because of some past work I did. People would reach out to me to do more of it or because they wanted to ask me questions about something I had launched.Although luck plays a role, there are aspects of getting promoted that rely less on luck. Here are four high-level areas:Impact is any measurable and objective outcome that benefits your company. Promotions are a byproduct of your elevated, sustained impact. If you can learn what your organization considers impactful and you deliver that, you will be rewarded.2) Leverage is how you have more impact. Software engineers increase their leverage through people, writing, and code. Leverage is what differentiates higher-level ICs from lower-level ones. What I mean by each type of leverage:People - People leverage comes from technical leadership. This means setting direction, reviewing designs/code, and growing others.Writing - Writing gives us leverage by influencing and helping others without your active involvement.Code - Not all code is created equal. High-leverage code solves problems that few others can or helps engineers move faster at scale. When people hear ‚Äúpersonal brand,‚Äù their minds often go to social media. But the brand that matters most is your ‚Äúinternal brand.‚Äù What do people within your company think about your work and its value? This is the brand that you should care most about.Most of the top ICs I know are not well-known outside of Meta. They are legendary within the company, though because people see their impressive work. Build your internal brand by doing great work and letting others know about it (further reading here).4) Build your soft skills. Working with others is a necessity to do anything of consequence. Also, being someone others want to work with makes it easier to find mentors who will uplift you along the way.Soft skills are underrated among software engineers. It‚Äôs important to be an excellent IC, but you can go so much further if you also communicate well. Also, engineers don‚Äôt often prioritize soft skills, so having them will help you stand out and lead.One last thing I‚Äôll leave you with is something that I didn‚Äôt realize until looking back. When I first joined the industry, I was an absolute machine. I would get in early and stay until the last shuttle left at 9:27 PM. Although this might sound like hell to some people, I loved it. No one made me do that; I put in those hours because I enjoyed the work and thought it was interesting.Looking back years later, I realize that was an unfair advantage I had. It let me put in a ton of work without getting burned out. Also, I got much more out of what I did because I was intrinsically motivated.If there‚Äôs one thing I wish for you, it is that you find work at the intersection of what you enjoy and what will get you promoted. That is the best recipe for hyper-career growth.Thanks for reading,Ryan PetermanIf you found it valuable, hit a like ‚ù§Ô∏è and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment. If you‚Äôre enjoying this newsletter and want to get even more value, consider becoming a .]]></content:encoded></item><item><title>Fluvio: A Rust-powered streaming platform using WebAssembly for programmable data processing</title><link>https://www.reddit.com/r/rust/comments/1iqgg02/fluvio_a_rustpowered_streaming_platform_using/</link><author>/u/drc1728</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sun, 16 Feb 2025 01:00:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am in the process of writing an essay on composable streaming first architecture for data intensive applications. I am thinking of it as a follow up on this article.Quick question for the Rust community:What information would help the Rust community know and experience Fluvio?What would you like to see covered in the essay?   submitted by    /u/drc1728 ]]></content:encoded></item><item><title>Show HN: Blunderchess.net ‚Äì blunder for your opponent every five moves</title><link>https://blunderchess.net/</link><author>eviledamame</author><category>dev</category><category>hn</category><pubDate>Sun, 16 Feb 2025 00:22:01 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Safe elimination of unnecessary bound checks.</title><link>https://www.reddit.com/r/rust/comments/1iqev5s/safe_elimination_of_unnecessary_bound_checks/</link><author>/u/tjientavara</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 23:43:26 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi, I am working on a Unicode database that is pretty fast, it is a 2 step associated lookup.Here is the code for getting the east-asian-width value of a Unicode code-point. Pay specific attention to the function. This function is a  function and the byte tables that it references are  as well. This will allow you to eventually run the unicode algorithms at both compile and run-time.Since the tables are fixed at compile time, I can proof that all values from the table will result in values that will never break any bounds, so technically the bound checks are unnecessary.There are two bound checks in the assembly output for this function.The check before accessing the EAST_ASIAN_WIDTH_COLUMN table (I use an assert! to do this, otherwise there will be double bound check).And the check on the conversion to the enum.The two bound checks are the two compare + conditional-jump instructions in this code.I could increase the size of the column table to remove one of the bound checks, but I want to keep the table small if possible.Is there a way to safely (I don't want to use the unsafe code) proof to the compiler that those two checks are unnecessary?P.S. technically there is a bound check before the index table a CMOV instruction, but it doubles as a way to also decompress the index table (last entry is repeated), so I feel this is not really a bound check.I was able to concat the two tables, and use a byte offset. So now there is no way to get an out of bound access, and the bound checks are no longer emitted by the compiler.I also added a manual check for out of bound on the enum and return zero instead, this becomes a CMOV and it eliminated all the panic code from the function.]]></content:encoded></item><item><title>Amazon AWS &quot;whoAMI&quot; Attack Exploits AMI Name Confusion to Take Over Cloud Instances</title><link>https://www.reddit.com/r/programming/comments/1iqav3c/amazon_aws_whoami_attack_exploits_ami_name/</link><author>/u/Dark-Marc</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 20:43:12 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Incoming Rust intern need advice?</title><link>https://www.reddit.com/r/rust/comments/1iq9oph/incoming_rust_intern_need_advice/</link><author>/u/Helpful_Ad_9930</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 19:52:13 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hey everyone, I'm a 19-year-old college student who just landed a SWE internship at NVIDIA! My manager has me learning Rust and exploring one of its libraries, and I‚Äôm also reading up on operating systems and computer networking. I'm almost done with the OS book and plan to start the networking one next week.I do have a bit of experience with embedded systems I completed two internships during my freshman year. However, so far I‚Äôm really enjoying Rust. I am quite a rookie compared to you experienced folks haha! But so far I love how Rust's compiler enforces safety, how Cargo makes dependency management a breeze compared to CMake, and the whole concept of ownership and borrowing is just super cool.At the moment, I‚Äôm nearly finished with the Rust book. I am on the concurrency chapter. Guess I am just wondering what next? I really want this return offer and I just want to blow this opportunity out the park. I go too a state school and my manager told me he has high expectations for me after my interviews. I just do not want to let him down you know also plus kind of getting impostor syndrome a bit seeing all the other interns coming from schools such as MIT, Harvard, Standford, etc. Sorry for the vent I guess I just want to prove my worth? and show my manager they made the right choice?What fun, Rust projects have helped you learn a lot?Are there any books you‚Äôd recommend that could help me out for the summer?Books I want to read before I start summer:Operating Systems (Three easy pieces)Beej's Guide to Network ProgrammingC++ Concurrency in Action]]></content:encoded></item><item><title>[D] Is my company missing out by avoiding deep learning?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iq9gtk/d_is_my_company_missing_out_by_avoiding_deep/</link><author>/u/DatAndre</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 19:42:42 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Disclaimer: obviously it does not make sense to use a neural network if a linear regression is enough. I work at a company that strictly adheres to mathematical, explainable models. Their stance is that methods like Neural Networks or even Gradient Boosting Machines are too "black-box" and thus unreliable for decision-making. While I understand the importance of interpretability (especially in mission critical scenarios) I can't help but feel that this approach is overly restrictive. I see a lot of research and industry adoption of these methods, which makes me wonder: are they really just black boxes, or is this an outdated view? Surely, with so many people working in this field, there must be ways to gain insights into these models and make them more trustworthy. Am I also missing out on them, since I do not have work experience with such models?EDIT: Context is formula one! However, races are a thing and support tools another. I too would avoid such models in anything strictly related to a race, unless completely necessary. I just feels that there's a bias that is context-independent here. ]]></content:encoded></item><item><title>Zed for golang</title><link>https://www.reddit.com/r/golang/comments/1iq8jsm/zed_for_golang/</link><author>/u/MrBricole</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 19:02:36 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am considering using zed for writting go. Is it working out of the box with full syntax high light for noob like me such fmt.Println() ? I mean, I need to have it displaying functions under an import library.Should I give it a try or is it only for advanced users ? ]]></content:encoded></item><item><title>Pushing autovectorization to the limit: utf-8 validator</title><link>https://www.reddit.com/r/rust/comments/1iq7yn2/pushing_autovectorization_to_the_limit_utf8/</link><author>/u/Laiho3</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 18:36:40 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/Laiho3 ]]></content:encoded></item><item><title>Introducing encode: Encoders/serializers made easy.</title><link>https://www.reddit.com/r/rust/comments/1iq6pz7/introducing_encode_encodersserializers_made_easy/</link><author>/u/Compux72</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:42:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[ is a toolbox for building encoders and serializers in Rust. It is heavily inspired by the  and  crates, which are used for building parsers. It is meant to be a companion to these crates, providing a similar level of flexibility and ease of use for reversing the parsing process.The main idea behind  is to provide a set of combinators for building serializers. These combinators can be used to build complex encoders from simple building blocks. This makes it easy to build encoders for different types of data, without having to write a lot of boilerplate code.Another key feature of  is its support for  environments. This makes it suitable for use in embedded systems, where the standard library (and particularly the [] module) is not available.See the  folder for some examples of how to use . Also, check the  module for a list of all the combinators provided by the crate.Ready to use combinators for minimizing boilerplate.: Enables the  feature.: Enables the use of the standard library.: Enables the use of the  crate.: Implements [] for [].Why the  trait instead of ?A buffer stores bytes in memory such that write operations are . The underlying storage may or may not be in contiguous memory. A BufMut value is a cursor into the buffer. Writing to BufMut advances the cursor position.The bytes crate was never designed with falible writes nor  targets in mind. This means that targets with little memory are forced to crash when memory is low, instead of gracefully handling errors.Why the  trait instead of ?Because there is no alternative, at least that i know of, that supports  properlyBecause it's easier to work with than  and Because using  with binary data often leads to a lot of boilerplate]]></content:encoded></item><item><title>Lil guy is trying his best</title><link>https://www.reddit.com/r/artificial/comments/1iq6dyy/lil_guy_is_trying_his_best/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:27:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Transition from C++ to Rust</title><link>https://www.reddit.com/r/rust/comments/1iq67vq/transition_from_c_to_rust/</link><author>/u/Dvorakovsky</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:20:14 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Guys, are here any people who were learning/coding in C++ and switched to Rust. How do you feel? I mean I could easily implement linked lists: singly, doubly in c++, but when I saw how it is implemented in Rust I'd say I got lost completely. I'm only learning rust... So yeah, I really like ownership model even tho it puts some difficulties into learning, but I think it's a benefit rather than a downside. Even tho compared to C++ syntax is a bit messy for me]]></content:encoded></item><item><title>No, your GenAI model isn&apos;t going to replace me</title><link>https://marioarias.hashnode.dev/no-your-genai-model-isnt-going-to-replace-me</link><author>/u/dh44t</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 17:06:40 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Type safe Go money library beta2!</title><link>https://www.reddit.com/r/golang/comments/1iq5stk/type_safe_go_money_library_beta2/</link><author>/u/HawkSecure4957</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 17:02:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello, after I released beta1, I received many constructive feedback! mainly lacking of locale support.This update brings locale formatting support and an improved interface for better usability. With Fulus, you can perform monetary operations safely and type-soundly. Plus, you can format money for any locale supported by CLDR. You can even define custom money types tailored specifically to your application's needs! I still need to battle test it against production projects, I have none at the moment. I am aiming next for performance benchmarking and more improvement, and parsing from string!I am open for more feedback. Thank you! ]]></content:encoded></item><item><title>Golang Mastery Exercises</title><link>https://www.reddit.com/r/golang/comments/1iq5k7w/golang_mastery_exercises/</link><author>/u/Temporary-Buy-7562</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 16:51:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I made a repository which has a prompt for you to write many exercises, if you complete this, and then drill the exercises, I would be sure you would reach mastery with the core of the language.I initially wanted to make some exercises for drilling syntax since I use copilot and lsps a lot, but ended up with quite a damn comprehensive list of things you would want to do with the language, and I find this more useful than working on leetcode to really adopt the language.]]></content:encoded></item><item><title>[D] Have any LLM papers predicted a token in the middle rather than the next token?</title><link>https://www.reddit.com/r/MachineLearning/comments/1iq4f0r/d_have_any_llm_papers_predicted_a_token_in_the/</link><author>/u/TheWittyScreenName</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 15:59:49 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[I‚Äôm working on a project (unrelated to NLP) where we use essentially the same architecture and training as GPT-3, but we‚Äôre more interested in finding a series of tokens to connect a starting and ending ‚Äúword‚Äù than the next ‚Äúword‚Äù. Since we‚Äôre drawing a lot from LLMs in our setup, I‚Äôm wondering if there‚Äôs been any research into how models perform when the loss function isn‚Äôt based on the next token, but instead predicting a masked token somewhere in the input sequence. Eventually we would like to expand this (maybe through fine tuning) to predict a longer series of missing tokens than just one but this seems like a good place to start. I couldn‚Äôt find much about alternate unsupervised training schemes in the literature but it seems like someone must have tried this already. Any suggestions, or reasons that this is a bad idea?]]></content:encoded></item><item><title>Alexandre Mutel a.k.a. xoofx is leaving Unity</title><link>https://mastodon.social/@xoofx/113997304444307991</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:53:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Don&apos;t &quot;optimize&quot; conditional moves in shaders with mix()+step()</title><link>https://iquilezles.org/articles/gpuconditionals/</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:52:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
In this article I want to correct a popular misconception that's been making the rounds in computer graphics aficionado circles for a long time now. It has to do with conditionals when selecting between two results in the GPUs. Unfortunately there are a couple of educational websites out there that are spreading some misinformation, and it would be nice correcting that. I tried contacting the authors without success, so without further ado, here goes my attempt to fix things up a little:
So, say I have this code, which I actually published the other day: snap45(  v )
{
     s = (v);
     x = (v.x);
     x>?(s.x,):
           x>?s*():
                      (,s.y);
}
The exact details of what it does don't matter for this discussion. All we care about is the two ternary operations deciding what's the final value this function should return. Indeed, depending on the value of the variable , the function will return one of three results, which are simple to compute. I could also have implemented this function with regular  statements, and all that I'm going to say in this article stays true.
Now, here's the problem - when seeing code like this, somebody somewhere will step up and invariably propose the following "optimization", which replaces what they believe (erroneously) are "conditional branches" in the code, by arithmetic operations. They will suggest something like this: snap45(  v )
{
     s = (v);
     x = (v.x);

     w0 = (,x);
     w1 = (,x)*(-w0);
     w2 = -w0-w1;

     res0 = (s.x,);
     res1 = (s.x,s.y)*();
     res2 = (,s.y);

     w0*res0 + w1*res1 + w2*res2;
}
There are two things wrong with this practice. The first one shows an incorrect understanding of how the GPU works. In particular, the original shader code had no conditional branching in it. Selecting between a few registers with a ternary operator or with a plain  statement does not lead to conditional branching; all it involves is a conditional move (a.k.a. "select"), which is a simple instruction to route the correct bits to the destination register. You can think of it as a bitwise AND+NAND+OR on the source registers, which is a simple combinational circuit. I'll repeat it again - there is no branching, the instruction pointer isn't manipulated, there's no prediction involved, no pipe to flush, no instruction cache to invalidation, no nothing.
For the record, of course GPUs can do real branching, and those are fine and fast and totally worth it when big chunks of code and computation are to be skipped given a condition. As with all things computing, always check the generated machine code to know what is happening exactly and when. But one thing you can safely assume without having to check any generated code - when moving simple values or computations like in my original example, you are guaranteed to not branch. This has been true for decades at this point, with GPUs. And while I'm not an expert in CPUs, I am pretty sure this is true for them as well.
The second wrong thing with the supposedly optimized version is that it actually runs much slower than the original version. You can measure it in a variety of hardware. I can only assume that's because the  function is probably implemented with some sort of conditional move or subtract + bit propagation + AND. step(  x,  y )
{
     x < y ?  : ;
}
Either way, using the step() "optimization" are either using the ternary operation anyways, which produces the  or  which they will use to mask in and out the different potential outputs with a series of arithmetic multiplications and additions. Which is wasteful, the values could have been conditionally moved directly, which is what the original shader code did.
But don't take my word for it, let's look at the generated machine code for the original code I published:
GLSL x>?(s.x,):
       x>?s*():
                  (,s.y);
AMD Compiler     s0,      v3, , v1
     v4, , v0
     s1,   vcc, (v2), s0
 v3, 0, v3, vcc
 v0, v0, v4, vcc
 vcc, (v2), s1
 v1, v1, v3, vcc
 v0, 0, v0, vcc
Microsoft Compiler   r0.xy, l(, ), v0.xy
   r0.zw, v0.xy, l(, )
 r0.xy, -r0.xyxx, r0.zwzz
 r0.xy, r0.xyxx
  r1.xyzw, r0.xyxy, l4()
   r2.xy, l(,), v0.xx  r0.z, l()
 r1.xyzw, r2.yyyy, r1.xyzw, r0.zyzy
 o0.xyzw, r2.xxxx, r0.xzxz, r1.xyzw
Here we can confirm that the GPU is not branching, as I explained. Instead, according to the AMD compiler, it's performing the required comparisons ( and  - cmp=compare, gt=greater than, ngt=not greated than), and then using the result to mask the results with the bitwise operations mentioned earlier ( - cnd=conditional).
The Microsoft compiler has expressed the same idea/implementation in a different format, but you can still see the comparison ( - "lt"=less than) and the masking or conditional move ( - mov=move, c=conditionally).
There are no jump/branch instructions in these listings.
Something not related to the discussion but interesting, is that some of the  GLSL calls I had in my shader before the ternary operator we are discussing, didn't become GPU instructions but rather instruction modifiers, which is the reason you see them in the listing. This means you can think of abs() calls as being free.
So, if you ever see somebody proposing this a = ( b, c, ( y, x ) );
as an optimization to
then please correct them for me.]]></content:encoded></item><item><title>Altman: OpenAI not for sale, especially to competitor who is not able to beat us</title><link>https://www.axios.com/2025/02/11/openai-altman-musk-offer</link><author>/u/namanyayg</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:17:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitHub - yaitoo/xun: Xun is an HTTP web framework built on Go&apos;s built-in html/template and net/http package‚Äôs router (1.22).</title><link>https://github.com/yaitoo/xun</link><author>/u/imlangzi</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 14:15:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What is Event Sourcing?</title><link>https://newsletter.scalablethread.com/p/what-is-event-sourcing</link><author>/u/scalablethread</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:05:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Traditional data storage typically focuses on the current state of an entity. For example, in an e-commerce system, you might store the current state of a customer's order: items, quantities, shipping address, etc. Event sourcing takes a different approach. Instead of storing the current state directly, it stores the events that led to that state. Each event represents a fact that happened in the past. Think of it as a detailed log of transactions on your bank statement. These events are immutable and stored in an append-only event store. The core idea is that an application's state can be derived by replaying events in the order they occurred, just like you can get your current bank balance by replaying all the transactions from the beginning. This makes Event Sourcing particularly useful for applications that require a high degree of audibility and traceability.Every change to the application state is captured as an event object in an Event Sourcing system. These events are then stored in an event store, a database optimized for handling event data. Here's a step-by-step breakdown of how Event Sourcing works:Reconstructing the state from events involves reading all the events related to an entity from the event store and applying them in sequence to reconstruct the current state. It's like simulating all the changes that have occurred to construct the current state. For example, consider an e-commerce application where an order goes through various states like "Created," "Paid," and "Shipped." To determine the current state of an order, you would:Retrieve all events related to the order from the event store.Initialize an empty order object.Apply each event to the order object in the order in which they were stored.By the end of this process, the order object will reflect the current state of the order.As the number of events grows, replaying the entire event stream to reconstruct the state can become slow and inefficient. This is where snapshots come in. A snapshot is a saved state of an entity at a specific point in time. Instead of replaying all events from the beginning, the application can load the latest snapshot and then replay only the events that occurred after the snapshot was taken. If you enjoyed this article, please hit the ‚ù§Ô∏è like button.If you think someone else will benefit from this, then please üîÅ share this post.]]></content:encoded></item><item><title>Built a cli tool for generating .gitignore files</title><link>https://www.reddit.com/r/golang/comments/1iq1ivv/built_a_cli_tool_for_generating_gitignore_files/</link><author>/u/SoaringSignificant</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:38:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built this mostly as an excuse to play around with Charmbracelet‚Äôs libraries like Bubble Tea and make a nice TUI, but it also solves the annoying problem of constantly looking up .gitignore templates. It‚Äôs a simple CLI tool that lets you grab templates straight from GitHub, TopTal, or even your own custom repository, all from the terminal. You can search through templates using a TUI interface, combine multiple ones like mixing Go and CLion, and even save your own locally so you don‚Äôt have to redo them every time. If you‚Äôre always setting up new projects and find yourself dealing with .gitignore files over and over, this just makes life a bit easier, hopefully. If that sounds useful, check it out here and give it a try. And if you‚Äôve got ideas to make the TUI better or want to add something cool, feel free to open a PR. Always happy to get feedback or contributions!]]></content:encoded></item><item><title>ED25519 Digital Signatures In Go</title><link>https://www.reddit.com/r/golang/comments/1iq1i84/ed25519_digital_signatures_in_go/</link><author>/u/mejaz-01</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:37:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/mejaz-01 ]]></content:encoded></item><item><title>Will AI Lead to the Disintermediation of Knowledge?</title><link>https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/</link><author>Bill Schmarzo</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 13:28:49 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Key Blog Points: For decades, organizations have operated under the central assumption that knowledge flows downward. Senior leaders, industry veterans, and domain experts have traditionally been the primary gatekeepers of critical information. Their insights, honed over years of experience, have been the cornerstone of strategic decision-making. Enter artificial intelligence (AI). Many folks are concerned that‚Ä¶¬†Read More ¬ª]]></content:encoded></item><item><title>Show HN: I Built a Reddit-style Bluesky client ‚Äì still rough, but open to ideas</title><link>https://threadsky.app/</link><author>lakshikag</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 13:19:17 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chinese Vice Minister says China and the US must work together to control rogue AI: &quot;If not... I am afraid that the probability of the machine winning will be high.&quot;</title><link>https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:27:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in  (AI).But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.‚ÄúRealistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,‚Äù Fu said.‚ÄúAs long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.‚ÄùThe panel discussion is part of a two-day global  that started in Paris on Monday.Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden‚Äôs administration and the United Nations.]]></content:encoded></item><item><title>Building the MagicMirror in Rust with iced GUI Library ü¶Ä</title><link>https://www.reddit.com/r/rust/comments/1ipzubj/building_the_magicmirror_in_rust_with_iced_gui/</link><author>/u/amindiro</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:56:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I recently embarked on a journey to build a custom MagicMirror using the Rust programming language, and I‚Äôd like to share my experiences. I wrost a blog post titled "software you can love: miroir √î mon beau miroir" this project was my attempt to create a stable, resource-efficient application for the Raspberry Pi 3A.Here's what I loved about using Rust and the iced GUI library:Elm Architecture + Rust is a match made in heaven: iced was perfect for my needs with its Model, View, and Update paradigms. It helped keep my state management concise and leverage Rust type system Opting for this lightweight rendering library reduced the size of the binary significantly, ending with a 9MB binary. Although troublesome at first, I used ‚Äòcross‚Äô to cross compile Rust for armv7.If anyone is keen, I‚Äôm thinking of open-sourcing this project and sharing it with the community. Insights on enhancing the project's functionality or any feedback would be much appreciated!Feel free to reach out if you're interested in the technical nitty-gritty or my experience with Rust GUI libraries in general.]]></content:encoded></item><item><title>[P] Daily ArXiv filtering powered by LLM judge</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/</link><author>/u/MadEyeXZ</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:14:16 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go Nullable with Generics v2.0.0 - now supports omitzero</title><link>https://github.com/LukaGiorgadze/gonull</link><author>/u/Money-Relative-1184</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 11:00:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>async-arp: library for probing hosts and sending advanced ARP (Address Resolution Protocol) requests.</title><link>https://www.reddit.com/r/rust/comments/1ipywbp/asyncarp_library_for_probing_hosts_and_sending/</link><author>/u/arcycar</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 10:48:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[After a few months of exploring and working with Rust, I am happy to share my first small Rust crate,  and I‚Äôd love to hear your thoughts! üöÄThis library provides an  way to send and receive , making it useful for network discovery, debugging, and custom networking applications.üèé  Built on Tokio for non-blocking network operationsüîç  Easily detect active devices in a subnet‚öôÔ∏è  Craft and send ARP packets dynamicallyYou can find usage examples and API documentation here: üìñ Since this is my first crate, I‚Äôd really appreciate any feedback on:üìå  ‚Äì Is the interface intuitive and ergonomic?üöÄ  ‚Äì Does it fit well into async Rust workflows?üîç  ‚Äì Any improvements or best practices I may have missed?ü¶Ä  ‚Äì Suggestions to make it more "Rustacean"?If you have further ideas, issues, or want to contribute, check it out on GitHub:Thanks for checking it out‚Äîlet me know what you think! ü¶Ä]]></content:encoded></item><item><title>what do you use golang for?</title><link>https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/</link><author>/u/Notalabel_4566</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 10:24:28 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is there any other major use than web development?]]></content:encoded></item><item><title>Lessons from David Lynch: A Software Developer&apos;s Perspective</title><link>https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/</link><author>/u/aijan1</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 09:40:30 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He‚Äôs perhaps best known for the groundbreaking TV series Twin Peaks, which inspired countless shows, including The X-Files, The Sopranos, and Lost.Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down ‚Äì even those who truly deserved it.Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that Mulholland Drive remained compulsively watchable while refusing to yield to interpretation.While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I‚Äôd like to share my perspective on his life lessons from a software developer‚Äôs viewpoint.Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you‚Äôve got to go deeper.We‚Äôve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one ‚Äìbecause they‚Äôre so rare‚Äì write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it‚Äôs a film, a painting, or software.The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.Software development is part art, part engineering. We don‚Äôt build the same software over and over again ‚Äì virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it‚Äôs very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.It‚Äôs a good habit to listen to what users have to say, but they often can only describe their problems ‚Äì they rarely come up with good ideas to solve them. And that‚Äôs OK. It‚Äôs our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  ‚Äì that magical state of mind where we lose track of time and produce code effortlessly. That‚Äôs why many developers hate meetings ‚Äì they are toxic to our productivity.I believe you need technical knowledge. And also, it‚Äôs really, really great to learn by doing. So, you should make a film.Software development is one of those rare fields where a college degree isn‚Äôt required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It‚Äôs crucial to never stop learning, experimenting, and iterating on our craft.Happy accidents are real gifts, and they can open the door to a future that didn‚Äôt even exist.Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.Be kind to your teammates, don‚Äôt embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety ‚Äìthat is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by Google‚Äôs research on the subject.It‚Äôs OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.Most of Hollywood is about making money - and I love money, but I don‚Äôt make the films thinking about money.Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.What makes these projects remarkable is that they didn‚Äôt emerge from corporate boardrooms ‚Äì they were built by communities of passionate developers, collaborating across the world.Money is just a means to an end. Unfortunately, many get this confused.David, thank you for making the world a better place!]]></content:encoded></item><item><title>[D] What&apos;s the most promising successor to the Transformer?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/</link><author>/u/jsonathan</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 06:17:01 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also xLSTM and Aaren.What do y'all think is the most promising alternative architecture to the transformer?]]></content:encoded></item><item><title>Kafka Delay Queue: When Messages Need a Nap Before They Work</title><link>https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need</link><author>/u/Sushant098123</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 05:08:28 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Webassembly and go 2025</title><link>https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/</link><author>/u/KosekiBoto</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 05:00:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[so I found this video and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you   submitted by    /u/KosekiBoto ]]></content:encoded></item><item><title>Bringing Nest.js to Rust: Meet Toni.rs, the Framework You‚Äôve Been Waiting For! üöÄ</title><link>https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/</link><author>/u/Mysterious-Rust</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 02:42:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a Rust developer coming from TypeScript, I‚Äôve been missing a Nest.js-like framework ‚Äî its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesn‚Äôt have a direct counterpart (yet!), I decided to build one myself! üõ†Ô∏èIntroducing‚Ä¶ Toni.rs ‚Äî a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And it‚Äôs live in beta! üéâHere‚Äôs what makes this project interesting:Scalable maintainability üß©:A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code ‚Äî each module lives in its own context, clean and focused.Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?Automatic Dependency Injection ü§ñ:Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.Leave your thoughts below ‚Äî suggestions, questions, or even just enthusiasm! üöÄ ]]></content:encoded></item><item><title>How I Became A Machine Learning Engineer (No CS Degree, No Bootcamp)</title><link>https://towardsdatascience.com/how-i-became-a-machine-learning-engineer-no-cs-degree-no-bootcamp/</link><author>Egor Howell</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 02:33:01 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Machine learning and AI are among the most popular topics nowadays, especially within the tech space. I am fortunate enough to work and develop with these technologies every day as a machine learning engineer!In this article, I will walk you through my journey to becoming a machine learning engineer, shedding some light and advice on how you can become one yourself!In one of my previous articles, I extensively wrote about my journey from school to securing my first Data Science job. I recommend you check out that article, but I will summarise the key timeline here.Pretty much everyone in my family studied some sort of STEM subject. My great-grandad was an engineer, both my grandparents studied physics, and my mum is a maths teacher.So, my path was always paved for me.I chose to study physics at university after watching The Big Bang Theory at age 12; it‚Äôs fair to say everyone was very proud!At school, I wasn‚Äôt dumb by any means. I was actually relatively bright, but I didn‚Äôt fully apply myself. I got decent grades, but definitely not what I was fully capable of.I was very arrogant and thought I would do well with zero work.I applied to top universities like Oxford and Imperial College, but given my work ethic, I was delusional thinking I had a chance. On results day, I ended up in clearing as I missed my offers. This was probably one of the saddest days of my life.Clearing in the UK is where universities offer places to students on certain courses where they have space. It‚Äôs mainly for students who don‚Äôt have a university offer.I was lucky enough to be offered a chance to study physics at the University of Surrey, and I went on to earn a first-class master‚Äôs degree in physics!There is genuinely no substitute for hard work. It is a cringy cliche, but it is true!My original plan was to do a PhD and be a full-time researcher or professor, but during my degree, I did a research year, and I just felt a career in research was not for me. Everything moved so slowly, and it didn‚Äôt seem there was much opportunity in the space.During this time, DeepMind released theirdocumentary on YouTube, which popped up on my home feed.From the video, I started to understand how AI worked and learn about neural networks, reinforcement learning, and deep learning. To be honest, to this day I am still not an expert in these areas.Naturally, I dug deeper and found that a data scientist uses AI and machine learning algorithms to solve problems. I immediately wanted in and started applying for data science graduate roles.I spent countless hours coding, taking courses, and working on projects. I applied to and eventually landed my first data science graduate scheme in September 2021.You can hear more about my journey from a podcast.I started my career in an insurance company, where I built various supervised learning models, mainly using gradient boosted tree packages like CatBoost, XGBoost, and generalised linear models (GLMs).I built models to predict:‚Ää‚Äî‚ÄäDid someone fraudulently make a claim to profit.‚Äî‚ÄäWhat‚Äôs the premium we should give someone.‚Äî‚ÄäHow many claims will someone have.‚Ää‚Äî‚ÄäWhat‚Äôs the average claim value someone will have.I made around six models spanning the regression and classification space. I learned so much here, especially in statistics, as I worked very closely with Actuaries, so my maths knowledge was excellent.However, due to the company‚Äôs structure and setup, it was difficult for my models to advance past the PoC stage, so I felt I lacked the ‚Äútech‚Äù side of my toolkit and understanding of how companies use machine learning in production.After a year, my previous employer reached out to me asking if I wanted to apply to a junior data scientist role that specialises in time series forecasting and optimisation problems. I really liked the company, and after a few interviews, I was offered the job!I worked at this company for about 2.5 years, where I became an expert in forecasting and combinatorial optimisation problems.I developed many algorithms and deployed my models to production through AWS using software engineering best practices, such as unit testing, lower environment, shadow system, CI/CD pipelines, and much more.Fair to say I learned a lot.¬†I worked very closely with software engineers, so I picked up a lot of engineering knowledge and continued self-studying machine learning and statistics on the side.Over time, I realised the actual value of data science is using it to make live decisions. There is a good quote by Pau Labarta BajoML models inside Jupyter notebooks have a business value of $0There is no point in building a really complex and sophisticated model if it will not produce results. Seeking out that extra 0.1% accuracy by staking multiple models is often not worth it.You are better off building something simple that you can deploy, and that will bring real financial benefit to the company.With this in mind, I started thinking about the future of data science. In my head, there are two avenues: -> You work primarily to gain insight into what the business should be doing and what it should be looking into to boost its performance. -> You ship solutions (models, decision algorithms, etc.) that bring business value.I feel the data scientist who analyses and builds PoC models will become extinct in the next few years because, as we said above, they don‚Äôt provide tangible value to a business.That‚Äôs not to say they are entirely useless; you have to think of it from the business perspective of their return on investment. Ideally, the value you bring in should be more than your salary.You want to say that you did ‚ÄúX that produced Y‚Äù, which the above two avenues allow you to do.The engineering side was the most interesting and enjoyable for me. I genuinely enjoy coding and building stuff that benefits people, and that they can use, so naturally, that‚Äôs where I gravitated towards.To move to the ML engineering side, I asked my line manager if I could deploy the algorithms and ML models I was building myself. I would get help from software engineers, but I would write all the production code, do my own system design, and set up the deployment process independently.And that‚Äôs exactly what I did.Coincidentally, my current employer contacted me around this time and asked if I wanted to apply for a machine learning engineer role that specialises in general ML and optimisation at their company!Call it luck, but clearly, the universe was telling me something. After several interview rounds, I was offered the role, and I am now a fully fledged machine learning engineer!Fortunately, a role kind of ‚Äúfell to me,‚Äù but I created my own luck through up-skilling and documenting my learning. That is why I always tell people to show their work‚Ää‚Äî‚Ääyou don‚Äôt know what may come from it.I want to share the main bits of advice that helped me transition from a machine learning engineer to a data scientist.‚Ää‚Äî‚ÄäA machine learning engineer is  an entry-level position in my opinion. You need to be well-versed in data science, machine learning, software engineering, etc. You don‚Äôt need to be an expert in all of them, but have good fundamentals across the board. That‚Äôs why I recommend having a couple of years of experience as either a software engineer or data scientist and self-study other areas.‚Ää‚Äî‚ÄäIf you are from data science, you must learn to write good, well-tested production code. You must know things like typing, linting, unit tests, formatting, mocking and CI/CD. It‚Äôs not too difficult, but it just requires some practice. I recommend asking your current company to work with software engineers to gain this knowledge, it worked for me!‚Ää‚Äî‚ÄäMost companies nowadays deploy many of their architecture and systems on the cloud, and machine learning models are no exception. So, it‚Äôs best to get practice with these tools and understand how they enable models to go live. I learned most of this on the job, to be honest, but there are courses you can take.‚Ää‚Äî‚ÄäI am sure most of you know this already, but every tech professional should be proficient in the command line. You will use it extensively when deploying and writing production code. I have a basic guide you can checkout here.Data Structures & Algorithms‚Ää‚Äî‚ÄäUnderstanding the fundamental algorithms in computer science are very useful for MLE roles. Mainly because you will likely be asked about it in interviews. It‚Äôs not too hard to learn compared to machine learning; it just takes time. Any course will do the trick.‚Ää‚Äî‚ÄäAgain, most tech professionals should know Git, but as an MLE, it is essential. How to squash commits, do code reviews, and write outstanding pull requests are musts.‚Ää‚Äî‚ÄäMany MLE roles I saw required you to have some specialisation in a particular area. I specialise in time series forecasting, optimisation, and general ML based on my previous experience. This helps you stand out in the market, and most companies are looking for specialists nowadays.The main theme here is that I basically up-skilled my software engineering abilities. This makes sense as I already had all the math, stats, and machine learning knowledge from being a data scientist.If I were a software engineer, the transition would likely be the reverse. This is why securing a machine learning engineer role can be quite challenging, as it requires proficiency across a wide range of skills.Summary & Further ThoughtsI have a free newsletter, , where I share weekly tips and advice as a practising data scientist. Plus, when you subscribe, you will get my and short PDF version of my AI roadmap!]]></content:encoded></item><item><title>Introducing Impressions at Netflix</title><link>https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb?source=rss----2615bd06b42e---4</link><author>Netflix Technology Blog</author><category>dev</category><category>official</category><pubDate>Sat, 15 Feb 2025 01:13:20 +0000</pubDate><source url="https://netflixtechblog.com/?source=rss----2615bd06b42e---4">Netflix Tech Blog</source><content:encoded><![CDATA[Part 1: Creating the Source of Truth for ImpressionsImagine scrolling through Netflix, where each movie poster or promotional banner competes for your attention. Every image you hover over isn‚Äôt just a visual placeholder; it‚Äôs a critical data point that fuels our sophisticated personalization engine. At Netflix, we call these images ‚Äòimpressions,‚Äô and they play a pivotal role in transforming your interaction from simple browsing into an immersive binge-watching experience, all tailored to your unique¬†tastes.Capturing these moments and turning them into a personalized journey is no simple feat. It requires a state-of-the-art system that can track and process these impressions while maintaining a detailed history of each profile‚Äôs exposure. This nuanced integration of data and technology empowers us to offer bespoke content recommendations.In this multi-part blog series, we take you behind the scenes of our system that processes billions of impressions daily. We will explore the challenges we encounter and unveil how we are building a resilient solution that transforms these client-side impressions into a personalized content discovery experience for every Netflix¬†viewer.Why do we need impression history?To tailor recommendations more effectively, it‚Äôs crucial to track what content a user has already encountered. Having impression history helps us achieve this by allowing us to identify content that has been displayed on the homepage but not engaged with, helping us deliver fresh, engaging recommendations.By maintaining a history of impressions, we can implement frequency capping to prevent over-exposure to the same content. This ensures users aren‚Äôt repeatedly shown identical options, keeping the viewing experience vibrant and reducing the risk of frustration or disengagement.Highlighting New¬†ReleasesFor new content, impression history helps us monitor initial user interactions and adjust our merchandising efforts accordingly. We can experiment with different content placements or promotional strategies to boost visibility and engagement.Additionally, impression history offers insightful information for addressing a number of platform-related analytics queries. Analyzing impression history, for example, might help determine how well a specific row on the home page is functioning or assess the effectiveness of a merchandising strategy.The first pivotal step in managing impressions begins with the creation of a Source-of-Truth (SOT) dataset. This foundational dataset is essential, as it supports various downstream workflows and enables a multitude of use¬†cases.Collecting Raw Impression EventsAs Netflix members explore our platform, their interactions with the user interface spark a vast array of raw events. These events are promptly relayed from the client side to our servers, entering a centralized event processing queue. This queue ensures we are consistently capturing raw events from our global user¬†base.After raw events are collected into a centralized queue, a custom event extractor processes this data to identify and extract all impression events. These extracted events are then routed to an Apache Kafka topic for immediate processing needs and simultaneously stored in an Apache Iceberg table for long-term retention and historical analysis. This dual-path approach leverages Kafka‚Äôs capability for low-latency streaming and Iceberg‚Äôs efficient management of large-scale, immutable datasets, ensuring both real-time responsiveness and comprehensive historical data availability.Filtering & Enriching Raw ImpressionsOnce the raw impression events are queued, a stateless Apache Flink job takes charge, meticulously processing this data. It filters out any invalid entries and enriches the valid ones with additional metadata, such as show or movie title details, and the specific page and row location where each impression was presented to users. This refined output is then structured using an Avro schema, establishing a definitive source of truth for Netflix‚Äôs impression data. The enriched data is seamlessly accessible for both real-time applications via Kafka and historical analysis through storage in an Apache Iceberg table. This dual availability ensures immediate processing capabilities alongside comprehensive long-term data retention.Ensuring High Quality ImpressionsMaintaining the highest quality of impressions is a top priority. We accomplish this by gathering detailed column-level metrics that offer insights into the state and quality of each impression. These metrics include everything from validating identifiers to checking that essential columns are properly filled. The data collected feeds into a comprehensive quality dashboard and supports a tiered threshold-based alerting system. These alerts promptly notify us of any potential issues, enabling us to swiftly address regressions. Additionally, while enriching the data, we ensure that all columns are in agreement with each other, offering in-place corrections wherever possible to deliver accurate¬†data.We handle a staggering volume of 1 to 1.5 million impression events globally every second, with each event approximately 1.2KB in size. To efficiently process this massive influx in real-time, we employ Apache Flink for its low-latency stream processing capabilities, which seamlessly integrates both batch and stream processing to facilitate efficient backfilling of historical data and ensure consistency across real-time and historical analyses. Our Flink configuration includes 8 task managers per region, each equipped with 8 CPU cores and 32GB of memory, operating at a parallelism of 48, allowing us to handle the necessary scale and speed for seamless performance delivery. The Flink job‚Äôs sink is equipped with a data mesh connector, as detailed in our Data Mesh platform which has two outputs: Kafka and Iceberg. This setup allows for efficient streaming of real-time data through Kafka and the preservation of historical data in Iceberg, providing a comprehensive and flexible data processing and storage solution.We utilize the ‚Äòisland model‚Äô for deploying our Flink jobs, where all dependencies for a given application reside within a single region. This approach ensures high availability by isolating regions, so if one becomes degraded, others remain unaffected, allowing traffic to be shifted between regions to maintain service continuity. Thus, all data in one region is processed by the Flink job deployed within that¬†region.Addressing the Challenge of Unschematized EventsAllowing raw events to land on our centralized processing queue unschematized offers significant flexibility, but it also introduces challenges. Without a defined schema, it can be difficult to determine whether missing data was intentional or due to a logging error. We are investigating solutions to introduce schema management that maintains flexibility while providing clarity.Automating Performance Tuning with AutoscalersTuning the performance of our Apache Flink jobs is currently a manual process. The next step is to integrate with autoscalers, which can dynamically adjust resources based on workload demands. This integration will not only optimize performance but also ensure more efficient resource utilization.Improving Data Quality¬†AlertsRight now, there‚Äôs a lot of business rules dictating when a data quality alert needs to be fired. This leads to a lot of false positives that require manual judgement. A lot of times it is difficult to track changes leading to regression due to inadequate data lineage information. We are investing in building a comprehensive data quality platform that more intelligently identifies anomalies in our impression stream, keeps track of data lineage and data governance, and also, generates alerts notifying producers of any regressions. This approach will enhance efficiency, reduce manual oversight, and ensure a higher standard of data integrity.Creating a reliable source of truth for impressions is a complex but essential task that enhances personalization and discovery experience. Stay tuned for the next part of this series, where we‚Äôll delve into how we use this SOT dataset to create a microservice that provides impression histories. We invite you to share your thoughts in the comments and continue with us on this journey of discovering impressions.We are genuinely grateful to our amazing colleagues whose contributions were essential to the success of Impressions: Julian Jaffe, Bryan Keller, Yun Wang, Brandon Bremen, Kyle Alford, Ron Brown and Shriya¬†Arora.]]></content:encoded></item><item><title>Tabiew 0.8.4 Released</title><link>https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/</link><author>/u/shshemi</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 00:21:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...üìä Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and SqliteüóÇÔ∏è Multi-table functionalityUI is updated to be more modern and responsiveHorizontally scrollable tablesVisible data frame can be referenced with name "_"Compatibility with older versions of glibcTwo new themes (Tokyo Night and Catppuccin)]]></content:encoded></item><item><title>Conditional types in TypeScript</title><link>https://2ality.com/2025/02/conditional-types-typescript.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In TypeScript, conditional types let us make decisions (think if-then-else expressions) ‚Äì which is especially useful in generic types. They are also an essential tool for working with union types because they let use ‚Äúloop‚Äù over them. Read on if you want to know how all of that works.]]></content:encoded></item><item><title>Show HN: VimLM ‚Äì A Local, Offline Coding Assistant for Vim</title><link>https://github.com/JosefAlbers/VimLM</link><author>JosefAlbers</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 23:34:41 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[VimLM is a local, offline coding assistant for Vim. It‚Äôs like Copilot but runs entirely on your machine‚Äîno APIs, no tracking, no cloud.- Deep Context: Understands your codebase (current file, selections, references).  
- Conversational: Iterate with follow-ups like "Add error handling".  
- Vim-Native: Keybindings like `Ctrl-l` for prompts, `Ctrl-p` to replace code.  
- Inline Commands: `!include` files, `!deploy` code, `!continue` long responses.Perfect for privacy-conscious devs or air-gapped environments.Try it:  
```
pip install vimlm
vimlm
```]]></content:encoded></item><item><title>Kay Hayen: Nuitka this week #16</title><link>https://nuitka.net/posts/nuitka-this-week-16.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Hey Nuitka users! This started out as an idea of a weekly update, but
that hasn‚Äôt happened, and so we will switch it over to just writing up
when something interesting happens and then push it out relatively
immediately when it happens.Nuitka Onefile Gets More Flexible:  and We‚Äôve got a couple of exciting updates to Nuitka‚Äôs onefile mode that
give you more control and flexibility in how you deploy your
applications. These enhancements stem from real-world needs and
demonstrate Nuitka‚Äôs commitment to providing powerful and adaptable
solutions.Taking Control of Onefile Unpacking: Onefile mode is fantastic for creating single-file executables, but the
management of the unpacking directory where the application expands has
sometimes been a bit‚Ä¶ opaque. Previously, Nuitka would decide whether
to clean up this directory based on whether the path used
runtime-dependent variables. This made sense in theory, but in practice,
it could lead to unexpected behavior and made debugging onefile issues
harder.Now, you have complete control! The new  option
lets you explicitly specify what happens to the unpacking directory:: This is the default behavior. Nuitka
will remove the unpacking directory unless runtime-dependent values
were used in the path specification. This is the same behavior as
previous versions.: The unpacking directory is 
removed and becomes a persistent, cached directory. This is useful
for debugging, inspecting the unpacked files, or if you have a use
case that benefits from persistent caching of the unpacked data. The
files will remain available for subsequent runs.: The unpacking directory 
removed after the program exits.This gives you the power to choose the behavior that best suits your
needs. No more guessing!Relative Paths with Another common request, particularly from users deploying applications
in more restricted environments, was the ability to specify the onefile
unpacking directory  to the executable itself. Previously, you
were limited to absolute paths or paths relative to the user‚Äôs temporary
directory space.We‚Äôve introduced a new variable, , that you can use in
the  option. This variable is dynamically
replaced at runtime with the full path to the directory containing the
onefile executable.This would create a directory named  the same
directory as the  (or  on Linux/macOS)
and unpack the application there. This is perfect for creating truly
self-contained applications where all data and temporary files reside
alongside the executable.Nuitka Commercial and Open SourceThese features, like many enhancements to Nuitka, originated from a
request by a Nuitka commercial customer. This highlights the close
relationship between the commercial offerings and the open-source core.
While commercial support helps drive development and ensures the
long-term sustainability of Nuitka, the vast majority of features are
made freely available to all users.This change will be in 2.7 and is currentlyWe encourage you to try out these new features and let us know what you
think! As always, bug reports, feature requests, and contributions are
welcome on GitHub.]]></content:encoded></item><item><title>Django Weblog: DjangoCongress JP 2025 Announcement and Live Streaming!</title><link>https://www.djangoproject.com/weblog/2025/feb/14/djangocongress-jp-2025-announcement-and-livestream/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 22:12:10 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[It will be streamed on the following YouTube Live channels:This year there will be talks not only about Django, but also about FastAPI and other asynchronous web topics. There will also be talks on Django core development, Django Software Foundation (DSF) governance, and other topics from around the world. Simultaneous translation will be provided in both English and Japanese.The Async Django ORM: Where Is it?Speed at Scale for Django Web ApplicationsImplementing Agentic AI Solutions in Django from scratchDiving into DSF governance: past, present and futureGetting Knowledge from Django Hits: Using Grafana and PrometheusCulture Eats Strategy for Breakfast: Why Psychological Safety Matters in Open Source¬µDjango. The next step in the evolution of asynchronous microservices technology.A public viewing of the event will also be held in Tokyo. A reception will also be held, so please check the following connpass page if you plan to attend.]]></content:encoded></item><item><title>Eli Bendersky: Decorator JITs - Python as a DSL</title><link>https://eli.thegreenplace.net/2025/decorator-jits-python-as-a-dsl/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 21:49:31 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Spend enough time looking at Python programs and packages for machine learning,
and you'll notice that the "JIT decorator" pattern is pretty popular. For
example, this JAX snippet:In both cases, the function decorated with  doesn't get executed by the
Python interpreter in the normal sense. Instead, the code inside is more like
a DSL (Domain Specific Language) processed by a special purpose compiler built
into the library (JAX or Triton). Another way to think about it is that Python
is used as a  to describe computations.In this post I will describe some implementation strategies used by libraries to
make this possible.Preface - where we're goingThe goal is to explain how different kinds of  decorators work by using
a simplified, educational example that implements several approaches from
scratch. All the approaches featured in this post will be using this flow: Expr IR --> LLVM IR --> Execution" /> Expr IR --> LLVM IR --> Execution" class="align-center" src="https://eli.thegreenplace.net/images/2025/decjit-python.png" />
These are the steps that happen when a Python function wrapped with
our educational  decorator is called:The function is translated to an "expression IR" - .This expression IR is converted to LLVM IR.Finally, the LLVM IR is JIT-executed.First, let's look at the  IR. Here we'll make a big simplification -
only supporting functions that define a single expression, e.g.:Naturally, this can be easily generalized - after all, LLVM IR can be used to
express fully general computations.Here are the  data structures:To convert an  into LLVM IR and JIT-execute it, we'll use this function:It uses the  class to actually generate LLVM IR from .
This process is straightforward and covered extensively in the resources I
linked to earlier; take a look at the full code here.My goal with this architecture is to make things simple, but .
On one hand - there are several simplifications: only single expressions are
supported, very limited set of operators, etc. It's very easy to extend this!
On the other hand, we could have just trivially evaluated the 
without resorting to LLVM IR; I do want to show a more complete compilation
pipeline, though, to demonstrate that an arbitrary amount of complexity can
be hidden behind these simple interfaces.With these building blocks in hand, we can review the strategies used by
 decorators to convert Python functions into s.Python comes with powerful code reflection and introspection capabilities out
of the box. Here's the  decorator:This is a standard Python decorator. It takes a function and returns another
function that will be used in its place ( ensures that
function attributes like the name and docstring of the wrapper match the
wrapped function).After  is applied to , what  holds is the
wrapper. When  is called, the wrapper is invoked with
.The wrapper obtains the AST of the wrapped function, and then uses
 to convert this AST into an :When  finishes visiting the AST it's given, its
 field will contain the  representing the function's
return value. The wrapper then invokes  with this .Note how our decorator interjects into the regular Python execution process.
When  is called, instead of the standard Python compilation and
execution process (code is compiled into bytecode, which is then executed
by the VM), we translate its code to our own representation and emit LLVM from
it, and then JIT execute the LLVM IR. While it seems kinda pointless in this
artificial example, in reality this means we can execute the function's code
in any way we like.AST JIT case study: TritonThis approach is almost exactly how the Triton language works. The body of a
function decorated with  gets parsed to a Python AST, which then
- through a series of internal IRs - ends up in LLVM IR; this in turn is lowered
to PTX by the
NVPTX LLVM backend.
Then, the code runs on a GPU using a standard CUDA pipeline.Naturally, the subset of Python that can be compiled down to a GPU is limited;
but it's sufficient to run performant kernels, in a language that's much
friendlier than CUDA and - more importantly - lives in the same file with the
"host" part written in regular Python. For example, if you want testing and
debugging, you can run Triton in "interpreter mode" which will just run the
same kernels locally on a CPU.Note that Triton lets us import names from the  package
and use them inside kernels; these serve as the  for the language
- special calls the compiler handles directly.Python is a fairly complicated language with  of features. Therefore,
if our JIT has to support some large portion of Python semantics, it may make
sense to leverage more of Python's own compiler. Concretely, we can have it
compile the wrapped function all the way to bytecode,
and start our translation from there.Here's the  decorator that does just this :The Python VM is a stack machine; so we emulate a stack to convert the
function's bytecode to  IR (a bit like an RPN evaluator).
As before, we then use our  utility function to lower
 to LLVM IR and JIT execute it.Using this JIT is as simple as the previous one - just swap 
for :Bytecode JIT case study: NumbaNumba is a compiler for Python itself. The idea
is that you can speed up specific functions in your code by slapping a
 decorator on them. What happens next is similar in spirit to
our simple , but of course much more complicated because it
supports a very large portion of Python semantics.Numba uses the Python compiler to emit bytecode, just as we did; it then
converts it into its own IR, and then to LLVM using .By starting with the bytecode, Numba makes its life easier (no need to rewrite
the entire Python compiler). On the other hand, it also makes some analyses
, because by the time we're in bytecode, a lot of semantic information
existing in higher-level representations is lost. For example, Numba has to
sweat a bit to recover control flow information from the bytecode (by
running it through a special interpreter first).The two approaches we've seen so far are similar in many ways - both rely on
Python's introspection capabilities to compile the source code of the JIT-ed
function to some extent (one to AST, the other all the way to bytecode), and
then work on this lowered representation.The tracing strategy is very different. It doesn't analyze the source code of
the wrapped function at all - instead, it  its execution by means of
specially-boxed arguments, leveraging overloaded operators and functions, and
then works on the generated trace.The code implementing this for our smile demo is surprisingly compact:Each runtime argument of the wrapped function is assigned a , and
that is placed in a , a placeholder class which lets us
do operator overloading:The remaining key function is :To understand how this works, consider this trivial example:After the decorated function is defined,  holds the wrapper function
defined inside . When  is called, the wrapper runs:For each argument of  itself (that is  and ), it creates
a new  holding a . This denotes a named variable in
the  IR.It then calls the wrapped function, passing it the boxes as runtime
parameters.When (the wrapped)  runs, it invokes . This is caught by the overloaded
 operator of , and it creates a new  with
the s representing  and  as children. This
 is then returned .The wrapper unboxes the returned  and passes it to
 to emit LLVM IR from it and JIT execute it with the
actual runtime arguments of the call: .This might be a little mind-bending at first, because there are two different
executions that happen:The first is calling the wrapped  function itself, letting the Python
interpreter run it as usual, but with special arguments that build up the IR
instead of doing any computations. This is the .The second is lowering this IR our tracing step built into LLVM IR and then
JIT executing it with the actual runtime argument values ; this is
the .This tracing approach has some interesting characteristics. Since we don't
have to analyze the source of the wrapped functions but only trace through
the execution, we can "magically" support a much richer set of programs, e.g.:This  with our basic . Since Python variables are
placeholders (references) for values, our tracing step is oblivious to them - it
follows the flow of values. Another example:This also just works! The created  will be a long chain of 
additions of 's runtime values through the loop, added to the 
for .This last example also leads us to a limitation of the tracing approach; the
loop cannot be  - it cannot depend on the function's arguments,
because the tracing step has no concept of runtime values and wouldn't know
how many iterations to run through; or at least, it doesn't know this unless
we want to perform the tracing run for every runtime execution .Tracing JIT case study: JAXThe JAX ML framework uses a tracing
approach very similar to the one described here. The first code sample in this
post shows the JAX notation. JAX cleverly wraps Numpy with its own version which
is traced (similar to our , but JAX calls these boxes "tracers"),
letting you write regular-feeling Numpy code that can be JIT optimized and
executed on accelerators like GPUs and TPUs via XLA. JAX's tracer builds up an underlying IR (called
jaxpr) which can then be
emitted to XLA ops and passed to XLA for further lowering and execution.For a fairly deep overview of how JAX works, I recommend reading the
autodidax doc.As mentioned earlier, JAX has some limitations
with things like data-dependent control flow in native Python. This won't work,
because there's control flow
that depends on a runtime value ():When  is executed, JAX will throw an exception, saying something
like:
This concrete value was not available in Python because it depends on the
value of the argument count.As a remedy, JAX has its
own built-in intrinsics from the jax.lax package.
Here's the example rewritten in a way that actually works: (and many other built-ins in the  package) is something JAX
can trace through, generating a corresponding XLA operation (XLA has support for
While loops, to which this
 can be lowered).The tracing approach has clear benefits for JAX as well; because it only cares
about the flow of values, it can handle arbitrarily complicated Python code,
as long as the flow of values can be traced. Just like the local variables and
data-independent loops shown earlier, but also things like closures. This makes
meta-programming and templating easy .The full code for this post is available on GitHub.]]></content:encoded></item><item><title>An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.</title><link>https://v.redd.it/sglstazd96je1</link><author>/u/eternviking</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 21:24:03 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenAI: The Age of AI Is Here!</title><link>https://www.youtube.com/watch?v=97kQRYwL3P0</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/97kQRYwL3P0?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 18:18:07 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[‚ù§Ô∏è Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

üìù The paper "Competitive Programming with Large Reasoning Models" is available here:
https://arxiv.org/abs/2502.06807

üìù My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

üôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu]]></content:encoded></item><item><title>Roadmap to Becoming a Data Scientist, Part 4: Advanced Machine Learning</title><link>https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-4-advanced-machine-learning/</link><author>Vyacheslav Efimov</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 17:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Data science is undoubtedly one of the most fascinating fields today.¬†Following significant breakthroughs in machine learning about a decade ago, data science has surged in popularity within the tech community.¬†Each year, we witness increasingly powerful tools that once seemed unimaginable.¬†Innovations such as the¬†,¬†, the¬†Retrieval-Augmented Generation (RAG) framework, and state-of-the-art¬†¬†‚Äî including¬†¬†‚Äî have had a profound impact on our world.However, with the abundance of tools and the ongoing hype surrounding AI, it can be overwhelming ‚Äî especially for beginners ‚Äî to determine which skills to prioritize when aiming for a career in data science.¬†Moreover, this field is highly demanding, requiring substantial dedication and perseverance.The first three parts of this series outlined the necessary skills to become a data scientist in three key areas: math, software engineering, and machine learning.¬†While knowledge of classical Machine Learning and neural network algorithms is an excellent starting point for aspiring data specialists, there are still many¬†important topics in machine learning that must be mastered to work on more advanced projects.This article will focus solely on the math skills necessary to start a career in Data Science.¬†Whether pursuing this path is a worthwhile choice based on your background and other factors will be discussed in a separate article.The importance of learning evolution of methods in machine learningThe section below provides information about the evolution of methods in natural language processing (NLP).In contrast to previous articles in this series, I have decided to change the format in which I present the necessary skills for aspiring data scientists. Instead of directly listing specific competencies to develop and the motivation behind mastering them, I will briefly outline the most important approaches, presenting them in chronological order as they have been developed and used over the past decades in machine learning.The reason is that I believe it is crucial to study these algorithms from the very beginning. In machine learning, many new methods are built upon older approaches, which is especially true for NLP and computer vision.For example, jumping directly into the implementation details of modern¬†large language models (LLMs)¬†without any preliminary knowledge may make it very difficult for beginners to grasp the motivation and underlying ideas of specific mechanisms.Given this, in the next two sections, I will highlight in¬†¬†the key concepts that should be studied.Natural language processing (NLP)¬†is a broad field that focuses on processing textual information. Machine learning algorithms cannot work directly with raw text, which is why text is usually preprocessed and converted into numerical vectors that are then fed into neural networks.Before being converted into vectors, words undergo¬†, which includes simple techniques such as¬†,¬†stemming, lemmatization, normalization, or removing¬†. After preprocessing, the resulting text is encoded into¬†. Tokens represent the smallest textual elements in a collection of documents. Generally, a token can be a part of a word, a sequence of symbols, or an individual symbol. Ultimately, tokens are converted into numerical vectors.The¬†¬†method is the most basic way to encode tokens, focusing on counting the frequency of tokens in each document. However, in practice, this is usually not sufficient, as it is also necessary to account for token importance ‚Äî a concept introduced in the¬†¬†and¬†¬†methods. While TF-IDF improves upon the naive counting approach of bag of words, researchers have developed a completely new approach called embeddings.¬†are numerical vectors whose components preserve the semantic meanings of words. Because of this, embeddings play a crucial role in NLP, enabling input data to be trained or used for model inference. Additionally, embeddings can be used to compare text similarity, allowing for the retrieval of the most relevant documents from a collection.Embeddings can also be used to encode other unstructured data, including images, audio, and videos.As a field, NLP has been evolving rapidly over the last 10‚Äì20 years to efficiently solve various text-related problems. Complex tasks like text translation and text generation were initially addressed using¬†recurrent neural networks (RNNs), which introduced the concept of memory, allowing neural networks to capture and retain key contextual information in long documents.Although RNN performance gradually improved, it remained suboptimal for certain tasks. Moreover, RNNs are relatively slow, and their sequential prediction process does not allow for parallelization during training and inference, making them less efficient.Additionally, the original Transformer architecture can be decomposed into two separate modules:¬†¬†and¬†. Both of these form the foundation of the most state-of-the-art models used today to solve various NLP problems. Understanding their principles is valuable knowledge that will help learners advance further when studying or working with other¬†large language models (LLMs).When it comes to LLMs, I strongly recommend studying the evolution of at least the first three GPT models, as they have had a significant impact on the AI world we know today. In particular, I would like to highlight the concepts of¬†¬†and¬†, introduced in¬†GPT-2, which enable LLMs to solve text generation tasks without explicitly receiving any training examples for them.Another important technique developed in recent years is¬†retrieval-augmented generation (RAG).¬†The main limitation of LLMs is that they are only aware of the context used during their training.¬†As a result, they lack knowledge of any information beyond their training data.The retriever converts the input prompt into an embedding, which is then used to query a vector database. The database returns the most relevant context based on the similarity to the embedding. This retrieved context is then combined with the original prompt and passed to a generative model. The model processes both the initial prompt and the additional context to generate a more informed and contextually accurate response.A good example of this limitation is the first version of the ChatGPT model, which was trained on data up to the year 2022 and had no knowledge of events that occurred from 2023 onward.To address this limitation, OpenAI researchers developed a RAG pipeline, which includes a constantly updated database containing new information from external sources. When ChatGPT is given a task that requires external knowledge, it queries the database to retrieve the most relevant context and integrates it into the final prompt sent to the machine learning model.The goal of distillation is to create a smaller model that can imitate a larger one. In practice, this means that if a large model makes a prediction, the smaller model is expected to produce a similar result.In the modern era, LLM development has led to models with millions or even billions of parameters. As a consequence, the overall size of these models may exceed the hardware limitations of standard computers or small portable devices, which come with many constraints.Quantization is the process of reducing the memory required to store numerical values representing a model‚Äôs weights.This is where optimization techniques become particularly useful, allowing LLMs to be compressed without significantly compromising their performance. The most commonly used techniques today include¬†,, and¬†.Pruning refers to discarding the least important weights of a model.Regardless of the area in which you wish to specialize, knowledge of¬†¬†is a must-have skill! Fine-tuning is a powerful concept that allows you to efficiently adapt a pre-trained model to a new task.Fine-tuning is especially useful when working with very large models. For example, imagine you want to use BERT to perform semantic analysis on a specific dataset. While BERT is trained on general data, it might not fully understand the context of your dataset. At the same time, training BERT from scratch for your specific task would require a massive amount of resources.Here is where fine-tuning comes in: it involves taking a pre-trained BERT (or another model) and freezing some of its layers (usually those at the beginning). As a result, BERT is retrained, but this time only on the new dataset provided. Since BERT updates only a subset of its weights and the new dataset is likely much smaller than the original one BERT was trained on, fine-tuning becomes a very efficient technique for adapting BERT‚Äôs rich knowledge to a specific domain.Fine-tuning is widely used not only in NLP but also across many other domains.As the name suggests,¬†¬†involves analyzing images and videos using machine learning. The most common tasks include image classification, object detection, image segmentation, and generation.Most CV algorithms are based on neural networks, so it is essential to understand how they work in detail. In particular, CV uses a special type of network called¬†convolutional neural networks (CNNs). These are similar to fully connected networks, except that they typically begin with a set of specialized mathematical operations called¬†.In simple terms, convolutions act as filters, enabling the model to extract the most important features from an image, which are then passed to fully connected layers for further analysis.The next step is to study the most popular CNN architectures for classification tasks, such as¬†AlexNet, VGG, Inception, ImageNet, and¬†.Speaking of the object detection task, the¬†¬†algorithm is a clear winner. It is not necessary to study all of the dozens of versions of YOLO. In reality, going through the original paper of the first YOLO should be sufficient to understand how a relatively difficult problem like object detection is elegantly transformed into both classification and regression problems. This approach in YOLO also provides a nice intuition on how more complex CV tasks can be reformulated in simpler terms.While there are many architectures for performing image segmentation, I would strongly recommend learning about¬†, which introduces an encoder-decoder architecture.Finally, image generation is probably one of the most challenging tasks in CV. Personally, I consider it an optional topic for learners, as it involves many advanced concepts. Nevertheless, gaining a high-level intuition of how¬†generative adversial networks (GAN)¬†function to generate images is a good way to broaden one‚Äôs horizons.In some problems, the training data might not be enough to build a performant model. In such cases, the data augmentation technique is commonly used. It involves the artificial generation of training data from already existing data (images). By feeding the model more diverse data, it becomes capable of learning and recognizing more patterns.It would be very hard to present in detail the Roadmaps for all existing machine learning domains in a single article. That is why, in this section, I would like to briefly list and explain some of the other most popular areas in data science worth exploring.First of all,¬†recommender systems (RecSys)¬†have gained a lot of popularity in recent years. They are increasingly implemented in online shops, social networks, and streaming services. The key idea of most algorithms is to take a large initial matrix of all users and items and decompose it into a product of several matrices in a way that associates every user and every item with a high-dimensional embedding. This approach is very flexible, as it then allows different types of comparison operations on embeddings to find the most relevant items for a given user. Moreover, it is much more rapid to perform analysis on small matrices rather than the original, which usually tends to have huge dimensions. often goes hand in hand with RecSys. When a RecSys has identified a set of the most relevant items for the user, ranking algorithms are used to sort them to determine the order in which they will be shown or proposed to the user. A good example of their usage is search engines, which filter query results from top to bottom on a web page.Closely related to ranking, there is also a¬†¬†problem that aims to optimally map objects from two sets, A and B, in a way that, on average, every object pair¬†is mapped ‚Äúwell‚Äù according to a matching criterion. A use case example might include distributing a group of students to different university disciplines, where the number of spots in each class is limited.¬†is an unsupervised machine learning task whose objective is to split a dataset into several regions (clusters), with each dataset object belonging to one of these clusters. The splitting criteria can vary depending on the task. Clustering is useful because it allows for grouping similar objects together. Moreover, further analysis can be applied to treat objects in each cluster separately.The goal of clustering is to group dataset objects (on the left) into several categories (on the right) based on their similarity.¬†is another unsupervised problem, where the goal is to compress an input dataset. When the dimensionality of the dataset is large, it takes more time and resources for machine learning algorithms to analyze it. By identifying and removing noisy dataset features or those that do not provide much valuable information, the data analysis process becomes considerably easier.¬†is an area that focuses on designing algorithms and data structures (indexes) to optimize searches in a large database of embeddings (vector database). More precisely, given an input embedding and a vector database, the goal is to¬†¬†find the most similar embedding in the database relative to the input embedding.The goal of similarity search is to approximately find the most similar embedding in a vector database relative to a query embedding.The word ‚Äúapproximately‚Äù means that the search is not guaranteed to be 100% precise. Nevertheless, this is the main idea behind similarity search algorithms ‚Äî sacrificing a bit of accuracy in exchange for significant gains in prediction speed or data compression.¬†involves studying the behavior of a target variable over time. This problem can be solved using classical tabular algorithms. However, the presence of time introduces new factors that cannot be captured by standard algorithms. For instance:the target variable can have an overall¬†, where in the long term its values increase or decrease¬†(e.g., the average yearly temperature rising due to global warming).the target variable can have a¬†¬†which makes its values change based on the currently given period¬†(e.g. temperature is lower in winter and higher in summer).Most of the time series models take both of these factors into account. In general, time series models are mainly used a lot in financial, stock or demographic analysis.Another advanced area I would recommend exploring is¬†, which fundamentally changes the algorithm design compared to classical machine learning.¬†In simple terms, its goal is to train an agent in an environment to make optimal decisions based on a reward system (also known as the¬†‚Äútrial and error approach‚Äù).¬†By taking an action, the agent receives a reward, which helps it understand whether the chosen action had a positive or negative effect.¬†After that, the agent slightly adjusts its strategy, and the entire cycle repeats.Reinforcement learning is particularly popular in complex environments where classical algorithms are not capable of solving a problem.¬†Given the complexity of reinforcement learning algorithms and the computational resources they require, this area is not yet fully mature, but it has high potential to gain even more popularity in the future.Currently the most popular applications are:.¬†Existing approaches can design optimal game strategies and outperform humans.¬†The most well-known examples are chess and Go..¬†Advanced algorithms can be incorporated into robots to help them move, carry objects or complete routine tasks at home..¬†Reinforcement learning methods can be developed to automatically drive cars, control helicopters or drones.This article was a logical continuation of the previous part and expanded the skill set needed to become a data scientist. While most of the mentioned topics require time to master, they can add significant value to your portfolio. This is especially true for the NLP and CV domains, which are in high demand today.After reaching a high level of expertise in data science, it is still crucial to stay motivated and consistently push yourself to learn new topics and explore emerging algorithms.Data science is a constantly evolving field, and in the coming years, we might witness the development of new state-of-the-art approaches that we could not have imagined in the past.All images are by the author unless noted otherwise.]]></content:encoded></item><item><title>Unlocking global AI potential with next-generation subsea infrastructure</title><link>https://engineering.fb.com/2025/02/14/connectivity/project-waterworth-ai-subsea-infrastructure/</link><author></author><category>dev</category><category>official</category><pubDate>Fri, 14 Feb 2025 16:28:06 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Publish Interactive Data Visualizations for Free with Python and Marimo</title><link>https://towardsdatascience.com/publish-interactive-data-visualizations-for-free-with-python-and-marimo/</link><author>Sam Minot</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 16:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Working in Data Science, it can be hard to share insights from complex datasets using only static figures. All the facets that describe the shape and meaning of interesting data are not always captured in a handful of pre-generated figures. While we have powerful technologies available for presenting interactive figures‚Ää‚Äî‚Ääwhere a viewer can rotate, filter, zoom, and generally explore complex data‚Ää ‚Äî‚Ää they always come with tradeoffs.Here I present my experience using a recently released Python library‚Ää‚Äî‚Äämarimo‚Ää‚Äî‚Ääwhich opens up exciting new opportunities for publishing interactive visualizations across the entire field of data science.Interactive Data VisualizationThe tradeoffs to consider when selecting an approach for presenting data visualizations can be broken into three categories:‚Ää‚Äî‚Ääwhat visualizations and interactivity am I able to present to the user?‚Ää‚Äî‚Ääwhat are the resources needed for displaying this visualization to users (e.g. running servers, hosting websites)? ‚Äì how much of a new skillset / codebase do I need to learn upfront? is the foundation of portable interactivity. Every user has a web browser installed on their computer and there are many different frameworks available for displaying any degree of interactivity or visualization you might imagine (for example, this gallery of amazing things people have made with three.js). Since the application is running on the user‚Äôs computer, no costly servers are needed. However, a significant drawback for the data science community is ease of use, as JS does not have many of the high-level (i.e. easy-to-use) libraries that data scientists use for data manipulation, plotting, and interactivity. provides a useful point of comparison. Because of its continually growing popularity, some have called this the ‚ÄúEra of Python‚Äù. For data scientists in particular, Python stands alongside R as one of the foundational languages for quickly and effectively wielding complex data. While Python may be easier to use than Javascript, there are fewer options for presenting interactive visualizations. Some popular projects providing interactivity and visualization have been Flask, Dash, and Streamlit (also worth mentioning‚Ää‚Äî‚Ääbokeh, HoloViews, altair, and plotly). The biggest tradeoff for using Python has been the cost for publishing ‚Äì delivering the tool to users. In the same way that shinyapps require a running computer to serve up the visualization, these Python-based frameworks have exclusively been server-based. This is by no means prohibitive for authors with a budget to spend, but it does limit the number of users who can take advantage of a particular project. is an intriguing middle ground‚Ää‚Äî‚ÄäPython code running directly in the web browser using WebAssembly (WASM). There are resource limitations (only 1 thread and 2GB memory) that make this impractical for doing the heavy lifting of data science. , this can be more than sufficient for building visualizations and updating based on user input. Because it runs in the browser, no servers are required for hosting. Tools that use Pyodide as a foundation are interesting to explore because they give data scientists an opportunity to write Python code which runs directly on users‚Äô computers without their having to install or run anything outside of the web browser.As an aside, I‚Äôve been interested previously in one project that has tried this approach: stlite, an in-browser implementation of Streamlit that lets you deploy these flexible and powerful apps to a broad range of users. However, a core limitation is that Streamlit itself is distinct from stlite (the port of Streamlit to WASM), which means that not all features are supported and that advancement of the project is dependent on two separate groups working along compatible lines.The interface resembles a Jupyter , which will be familiar to users.Execution of cells is , so that updating one cell will rerun all cells which depend on its output. can be captured with a flexible set of UI components.Notebooks can be quickly converted into , hiding the code and showing only the input/output elements.Apps can be run locally or converted into using WASM/Pyodide.marimo balances the tradeoffs of technology in a way that is well suited to the skill set of the typical data scientists:‚Ää‚Äî‚Ääuser input and visual display features are rather extensive, supporting user input via Altair and Plotly plots.‚Ää‚Äî‚Äädeploying as static webpages is basically free‚Ää‚Äî‚Ääno servers required‚Ää‚Äî‚Ääfor users familiar with Python notebooks, marimo will feel very familiar and be easy to pick up.Publishing Marimo Apps on the WebAs a simple example of the type of display that can be useful in data science, consisting of explanatory text interspersed with interactive displays, I have created a barebones GitHub repository. Try it out yourself here.Using just a little bit of code, users can:Generate visualizations with flexible interactivityWrite narrative text describing their findingsPublish to the web for free (i.e. using GitHub Pages)Public App / Private DataThis new technology offers an exciting new opportunity for collaboration‚Ää‚Äî‚Ääpublish the app publicly to the world, but users can only see specific datasets that they have permission to access.Rather than building a dedicated data backend for every app, user data can be stored in a generic backend which can be securely authenticated and accessed using a Python client library‚Ää‚Äî‚Ääall contained within the user‚Äôs web browser. For example, the user is given an OAuth login link that will authenticate them with the backend and allow the app to temporarily access input data.As a proof of concept, I built a simple visualization app which connects to the Cirro data platform, which is used at my institution to manage scientific data. Full disclosure: I was part of the team that built this platform before it spun out as an independent company. In this manner users can:Load the public visualization app‚Ää‚Äî‚Äähosted on GitHub PagesConnect securely to their private data storeLoad the appropriate dataset for displayShare a link which will direct authorized collaborators to the same dataAs a data scientist, this approach of publishing free and open-source visualization apps which can be used to interact with private datasets is extremely exciting. Building and publishing a new app can take hours and days instead of weeks and years, letting researchers quickly share their insights with collaborators and then publish them to the wider world.]]></content:encoded></item><item><title>Hugo van Kemenade: Improving licence metadata</title><link>https://hugovk.dev/blog/2025/improving-licence-metadata/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 15:11:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[PEP 639 defines a spec on how to document licences
used in Python projects.Change  as follows.I usually use Hatchling as a build backend, and support was added in 1.27:Replace the freeform  field with a valid SPDX license expression, and add
 which points to the licence files in the repo. There‚Äôs often only one,
but if you have more than one, list them all:Optionally delete the deprecated licence classifier:Then make sure to use a PyPI uploader that supports this.pip can also show you the metadata:A lot of work went into this. Thank you to PEP authors
Philippe Ombredanne for creating the first draft in
2019, to C.A.M. Gerlach for the second draft in 2021,
and especially to Karolina Surma for getting the third
draft finish line and helping with the implementation.And many projects were updated to support this, thanks to the maintainers and
contributors of at least:]]></content:encoded></item><item><title>5 Tips for Building a Data Science Portfolio</title><link>https://www.kdnuggets.com/5-tips-building-data-science-portfolio</link><author>Nate Rosidi</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/Rosidi_5_Tips_for_Building_a_DS_Portfolio_4.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 15:00:25 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Not every data science portfolio is worth showcasing. Follow these five tips to build a portfolio that impresses employers and gets you a job.]]></content:encoded></item><item><title>Show HN: Transform your codebase into a single Markdown doc for feeding into AI</title><link>https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html</link><author>tesserato</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 13:23:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.
The output for the current repository can be found here.Comprehensive Codebase Documentation: Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format. Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types). Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.Simple Command-Line Interface:  Offers an intuitive command-line interface with straightforward options for customization.If you have Go installed, run go install github.com/tesserato/CodeWeaver@latestto install the latest version of CodeWeaver or go install github.com/tesserato/CodeWeaver@vX.Y.Z to install a specific version.Alternatively, download the appropriate pre built executable from the releases page.If necessary, make the  executable by using the  command:The root directory to scan and document.The name of the output Markdown file.-ignore "<regex patterns>"Comma-separated list of regular expression patterns for paths to exclude.-included-paths-file <filename>File to save the list of paths that were included in the documentation.-excluded-paths-file <filename>File to save the list of paths that were excluded from the documentation.Display this help message and exit.Generate documentation for the current directory:This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).Specify a different input directory and output file:./codeweaver -dir=my_project -output=project_docs.md
This command will process the  directory and save the documentation to .Ignore specific file types and directories:./codeweaver -ignore="\.log,temp,build" -output=detailed_docs.md
This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.Save lists of included and excluded paths:./codeweaver -ignore="node_modules" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md
This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.CodeWeaver is released under the MIT License. See the  file for complete license details.]]></content:encoded></item><item><title>Show HN: A New Way to Learn Languages</title><link>https://www.langturbo.com/</link><author>sebnun</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 12:08:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Real Python Podcast ‚Äì Episode #239: Behavior-Driven vs Test-Driven Development &amp; Using Regex in Python</title><link>https://realpython.com/podcasts/rpp/239/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[What is behavior-driven development, and how does it work alongside test-driven development? How do you communicate requirements between teams in an organization? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.]]></content:encoded></item><item><title>Daniel Roy Greenfeld: Building a playing card deck</title><link>https://daniel.feldroy.com/posts/2025-02-deck-of-cards</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 09:50:04 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Today is Valentine's Day. That makes it the perfect day to write a blog post about showing how to not just build a deck of cards, but show off cards from the heart suite.]]></content:encoded></item><item><title>Building a Data Engineering Center of Excellence</title><link>https://towardsdatascience.com/building-a-data-engineering-center-of-excellence/</link><author>Richie Bachala</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 02:35:48 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[As data continues to grow in importance and become more complex, the need for skilled data engineers has never been greater. But what is data engineering, and why is it so important? In this blog post, we will discuss the essential components of a functioning data engineering practice and why data engineering is becoming increasingly critical for businesses today, and how you can build your very own Data Engineering Center of Excellence!I‚Äôve had the privilege to build, manage, lead, and foster a sizeable high-performing team of data warehouse & ELT engineers for many years. With the help of my team, I have spent a considerable amount of time every year consciously planning and preparing to manage the growth of our data month-over-month and address the changing reporting and analytics needs for our¬†20000+ global data consumers. We built many data warehouses to store and centralize massive amounts of data generated from many OLTP sources. We‚Äôve implemented Kimball methodology by creating star schemas both within our on-premise data warehouses and in the ones in the cloud.The objective is to enable our user-base to perform fast analytics and reporting on the data; so our analysts‚Äô community and business users can make accurate data-driven decisions.It took me about three years to transform¬†¬†() of data warehouse and ETL programmers into one cohesive Data Engineering team.I have compiled some of my learnings building a global data engineering team in this post in hopes that Data professionals and leaders of all levels of technical proficiency can benefit.Evolution of the Data EngineerIt has never been a better time to be a data engineer. Over the last decade, we have seen a massive awakening of enterprises now recognizing their data as the company‚Äôs heartbeat, making data engineering the job function that ensures accurate, current, and quality data flow to the solutions that depend on it.Historically, the role of Data Engineers has evolved from that of¬†data warehouse developers¬†and the¬†¬†(extract, transform and load).The data warehouse developers are responsible for designing, building, developing, administering, and maintaining data warehouses to meet an enterprise‚Äôs reporting needs. This is done primarily via extracting data from operational and transactional systems and piping it using extract transform load methodology (ETL/ ELT) to a storage layer like a data warehouse or a data lake. The data warehouse or the data lake is where data analysts, data scientists, and business users consume data. The developers also perform transformations to conform the ingested data to a data model with aggregated data for easy analysis.A data engineer‚Äôs prime responsibility is to produce and make data securely available for multiple consumers.Data engineers oversee the ingestion, transformation, modeling, delivery, and movement of data through every part of an organization. Data extraction happens from many different data sources & applications. Data Engineers load the data into data warehouses and data lakes, which are transformed not just for the Data Science & predictive analytics initiatives (as everyone likes to talk about) but primarily for data analysts. Data analysts & data scientists perform operational reporting, exploratory analytics, service-level agreement (SLA) based business intelligence reports and dashboards on the catered data. In this book, we will address all of these job functions.The role of a data engineer is to acquire, store, and aggregate data from both cloud and on-premise, new, and existing systems, with data modeling and feasible data architecture. Without the data engineers, analysts and data scientists won‚Äôt have valuable data to work with, and hence, data engineers are the first to be hired at the inception of every new data team. Based on the data and analytics tools available within an enterprise, data engineering teams‚Äô role profiles, constructs, and approaches have several options for what should be included in their responsibilities which we will discuss in this chapter.Software is increasingly automating the historically manual and tedious tasks of data engineers. Data processing tools and technologies have evolved massively over several years and will continue to grow. For example, cloud-based data warehouses (Snowflake, for instance) have made data storage and processing affordable and fast. Data pipeline services (like¬†Informatica IICS,¬†Apache Airflow,¬†Matillion,¬†Fivetran) have turned data extraction into work that can be completed quickly and efficiently. The data engineering team should be leveraging such technologies as force multipliers, taking a consistent and cohesive approach to integration and management of enterprise data, not just relying on legacy siloed approaches to building custom data pipelines with fragile, non-performant, hard to maintain code. Continuing with the latter approach will stifle the pace of innovation within the said enterprise and force the future focus to be around managing data infrastructure issues rather than how to help generate value for your business.The primary role of an enterprise Data Engineering team should be to¬†¬†into a shape that‚Äôs ready for analysis ‚Äî laying the foundation for real-world analytics and data science application.The Data Engineering team should serve as the¬†¬†for enterprise-level data with the responsibility to curate the organization‚Äôs data and act as a resource for those who want to make use of it, such as Reporting & Analytics teams, Data Science teams, and other groups that are doing more self-service or business group driven analytics leveraging the enterprise data platform. This team should serve as the¬†¬†of organizational knowledge, managing and refining the catalog so that analysis can be done more effectively. Let‚Äôs look at the essential responsibilities of a well-functioning Data Engineering team.Responsibilities of a Data Engineering TeamThe Data Engineering team should provide a¬†¬†within the enterprise that cuts across to support both the Reporting/Analytics and Data Science capabilities to provide access to clean, transformed, formatted, scalable, and secure data ready for analysis. The Data Engineering teams‚Äô core responsibilities should include:¬∑ Build, manage, and optimize the core data platform infrastructure¬∑ Build and maintain custom and off-the-shelf data integrations and ingestion pipelines from a variety of structured and unstructured sources¬∑ Manage overall data pipeline orchestration¬∑ Manage transformation of data either before or after load of raw data through both technical processes and business logic¬∑ Support analytics teams with design and performance optimizations of data warehousesData is an Enterprise Asset.Data as an Asset should be shared and protected.Data should be valued as an Enterprise asset, leveraged across all Business Units to enhance the company‚Äôs value to its respective customer base by accelerating decision making, and improving competitive advantage with the help of data. Good data stewardship, legal and regulatory requirements dictate that we protect the data owned from unauthorized access and disclosure.In other words,¬†managing Security is a crucial responsibility.Why Create a Centralized Data Engineering Team?Treating Data Engineering as a standard and core capability that underpins both the Analytics and Data Science capabilities will help an enterprise evolve how to approach Data and Analytics. The enterprise needs to stop vertically treating data based on the technology stack involved as we tend to see often and move to more of a horizontal approach of managing a¬†¬†or¬†¬†that cuts across the organization and can connect to various technologies as needed drive analytic initiatives. This is a new way of thinking and working, but it can drive efficiency as the various data organizations look to scale. Additionally ‚Äî there is value in creating a dedicated structure and career path for Data Engineering resources. Data engineering skill sets are in high demand in the market; therefore, hiring outside the company can be costly. Companies must enable programmers, database administrators, and software developers with a career path to gain the needed experience with the above-defined skillsets by working across technologies. Usually, forming a data engineering center of excellence or a capability center would be the first step for making such progression possible.Challenges for creating a centralized Data Engineering TeamThe centralization of the Data Engineering team as a service approach is different from how Reporting & Analytics and Data Science teams operate. It does, in principle, mean¬†giving up some level of control of resources¬†and establishing new processes for how these teams will collaborate and work together to deliver initiatives.The Data Engineering team will need to demonstrate that it can effectively support the needs of both Reporting & Analytics and Data Science teams, no matter how large these teams are. Data Engineering teams must¬†effectively prioritize workloads¬†while ensuring they can bring the right skillsets and experience to assigned projects.Data engineering is essential because it serves as the backbone of data-driven companies. It enables analysts to work with clean and well-organized data, necessary for deriving insights and making sound decisions. To build a functioning data engineering practice, you need the following critical components:The Data Engineering team should be a core capability within the enterprise, but it should effectively serve as a support function involved in almost everything data-related. It should interact with the Reporting and Analytics and Data Science teams in a collaborative support role to make the entire team successful.The¬†Data Engineering team doesn‚Äôt create direct business value¬†‚Äî but the value should come in making the Reporting and Analytics, and Data Science teams more productive and efficient to ensure delivery of maximum value to business stakeholders through Data & Analytics initiatives. To make that possible, the six key responsibilities within the data engineering capability center would be as follow ‚ÄìLet‚Äôs review the¬†6 pillars of responsibilities:1. Determine Central Data Location for Collation and WranglingUnderstanding and having a strategy for a¬†(a centralized data repository or data warehouse for the mass consumption of data for analysis). Defining requisite data tables and where they will be joined in the context of data engineering and subsequently converting raw data into digestible and valuable formats.2. Data Ingestion and TransformationMoving data from one or more sources to a new destination (your data lake or cloud data warehouse)¬†where it can be stored and further analyzed and then converting data from the format of the source system to that of the destinationExtracting, transforming, and loading data from one or more sources into a destination system to represent the data in a new context or style.Data modeling is an essential function of a data engineering team, granted not all data engineers excel with this capability. Formalizing relationships between data objects and business rules into a conceptual representation through understanding information system workflows, modeling required queries, designing tables, determining primary keys, and effectively utilizing data to create informed output.I‚Äôve seen engineers in interviews mess up more with this than coding in technical discussions. It‚Äôs essential to understand the differences between Dimensions, Facts, Aggregate tables.Ensuring that sensitive data is protected and implementing proper authentication and authorization to reduce the risk of a data breach6. Architecture and AdministrationDefining the models, policies, and standards that administer what data is collected, where and how it is stored, and how it such data is integrated into various analytical systems.The six pillars of responsibilities for data engineering capabilities center on the ability to determine a central data location for collation and wrangling, ingest and transform data, execute ETL/ELT operations, model data, secure access and administer an architecture. While all companies have their own specific needs with regards to these functions, it is important to ensure that your team has the necessary skillset in order to build a foundation for big data success.Besides the Data Engineering following are the other capability centers that need to be considered within an enterprise:Analytics Capability CenterThe analytics capability center enables consistent, effective, and efficient BI, analytics, and advanced analytics capabilities across the company. Assist business functions in triaging, prioritizing, and achieving their objectives and goals through reporting, analytics, and dashboard solutions, while providing operational reports and visualizations, self-service analytics, and required tools to automate the generation of such insights.Data Science Capability CenterThe data science capability center is for exploring cutting-edge technologies and concepts to unlock new insights and opportunities, better inform employees and create a culture of prescriptive information usage using Automated AI and Automated ML solutions such as¬†H2O.ai,¬†Dataiku,¬†Aible, DataRobot,¬†C3.aiThe data governance office empowers users with trusted, understood, and timely data to drive effectiveness while keeping the integrity and sanctity of data in the right hands for mass consumption.As your company grows, you will want to make sure that the data engineering capabilities are in place to support the six pillars of responsibilities. By doing this, you will be able to ensure that all aspects of data management and analysis are covered and that your data is safe and accessible by those who need it. Have you started thinking about how your company will grow? What steps have you taken to put a centralized data engineering team in place?]]></content:encoded></item><item><title>It&apos;s time to go ESM-only</title><link>https://javascriptweekly.com/issues/723</link><author></author><category>dev</category><category>frontend</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://javascriptweekly.com/">Javascript Weekly</source><content:encoded><![CDATA[ü§Ø Pfft, vehicle data is a joke without CarsXE. API goes brrrrr. VIN decoding, plate lookup, market value reports. Get¬†Serious!]]></content:encoded></item><item><title>Mapped types in TypeScript</title><link>https://2ality.com/2025/02/mapped-types-typescript.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[A mapped type is a loop over keys that produces an object or tuple type and looks as follows:{[]: }
In this blog post, we examine how mapped types work and see examples of using them. Their most importing use cases are transforming objects and mapping tuples.]]></content:encoded></item><item><title>We Were Wrong About GPUs</title><link>https://fly.io/blog/wrong-about-gpu/</link><author>Fly</author><category>dev</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://fly.io/blog/feed.xml">Fly.io blog</source><content:encoded><![CDATA[We‚Äôre building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren‚Äôt going anywhere, but: GPUs aren‚Äôt going anywhere.A Fly Machine is a Docker/OCI container running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It‚Äôs a Fly Machine that can do fast CUDA.Like everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn‚Äôt fit the moment. It‚Äôs a bet that doesn‚Äôt feel like it‚Äôs paying off.If you‚Äôre using Fly GPU Machines, don‚Äôt freak out; we‚Äôre not getting rid of them. But if you‚Äôre waiting for us to do something bigger with them, a v2 of the product, you‚Äôll probably be waiting awhile.GPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines Intel‚Äôs Cloud Hypervisor, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors.GPUs terrified our security team. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers(not even bidirectional: in common configurations, GPUs talk to each other)with arbitrary, end-user controlled computation, all operating outside our normal security boundary.We did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren‚Äôt mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there‚Äôs a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers.We funded two very large security assessments, from Atredis and Tetrel, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time.Security wasn‚Äôt directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason.We could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we‚Äôd have been on Nvidia‚Äôs driver happy-path.Alternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path.Instead, we burned months trying (and ultimately failing) to get Nvidia‚Äôs host drivers working to map virtualized GPUs into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU.I‚Äôm not sure any of this really mattered in the end. There‚Äôs a segment of the market we weren‚Äôt ever really able to explore because Nvidia‚Äôs driver support kept us from thin-slicing GPUs. We‚Äôd have been able to put together a really cheap offering for developers if we hadn‚Äôt run up against that, and developers love ‚Äúcheap‚Äù, but I can‚Äôt prove that those customers are real.On the other hand, we‚Äôre committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer‚Äôs OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our  orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying!And, of course, we bought GPUs. A lot of GPUs. Expensive GPUs.The biggest problem: developers don‚Äôt want GPUs. They don‚Äôt even want AI/ML models. They want LLMs.  may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But  don‚Äôt care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can‚Äôt just give them a GPU.For those developers, who probably make up most of the market, it doesn‚Äôt seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of ‚Äútokens per second‚Äù aren‚Äôt counting milliseconds.(you should all feel sympathy for us)This makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they‚Äôll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn‚Äôt seem to matter yet, so the market doesn‚Äôt care.Past that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough.People doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s.Near as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device.We think there‚Äôs probably a market for users doing lightweight ML work getting tiny GPUs. This is what Nvidia MIG does, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it‚Äôs not baked; we can‚Äôt use it. And I‚Äôm not sure how many of those customers there are, or whether we‚Äôd get the density of customers per server that we need.That leaves the L40S customers. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they‚Äôre the one part we have in our inventory people seem to get a lot of use out of. We‚Äôre happy with them. But they‚Äôre just another kind of compute that some apps need; they‚Äôre not a driver of our core business. They‚Äôre not the GPU bet paying off.Really, all of this is just a long way of saying that for most software developers, ‚ÄúAI-enabling‚Äù their app is best done with API calls to things like Claude and GPT, Replicate and RunPod.A very useful way to look at a startup is that it‚Äôs a race to learn stuff. So, what‚Äôs our report card?First off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of  models, the world Elixir Bumblebee looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems.But Cursor happened, and, as they say, how are you going to keep ‚Äòem down on the farm once they‚Äôve seen Karl Hungus? It seems much clearer where things are heading.GPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing.Another way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn‚Äôt a winning strategy. I‚Äôd rather we‚Äôd flopped the nut straight, but I think going in on this hand was the right call.A really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our costs here aren‚Äôt recoverable. But the hardware parts that aren‚Äôt generating revenue will ultimately get liquidated; like with our portfolio of IPv4 addresses, I‚Äôm even more comfortable making bets backed by tradable assets with durable value.In the end, I don‚Äôt think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I‚Äôm very happy about is that we didn‚Äôt compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we‚Äôre scaling back our GPU ambitions without having sacrificed any of our isolation story, and, ironically, GPUs  are making that story a lot more important. The same thing goes for our Fly Machine developer experience.We started this company building a Javascript runtime for edge computing. We learned that our customers didn‚Äôt want a new Javascript runtime; they just wanted their native code to work. We shipped containers, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That‚Äôs usually how we figure out the right answers:  by being wrong about a lot of stuff.]]></content:encoded></item><item><title>Show HN: SQL Noir ‚Äì Learn SQL by solving crimes</title><link>https://www.sqlnoir.com/</link><author>chrisBHappy</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 21:49:16 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Bojan Mihelac: Prefixed Parameters for Django querystring tag</title><link>http://code.informatikamihelac.com/en/query-string-with-prefixed-parameters/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 21:37:18 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[An overview of Django 5.1's new querystring tag and how to add support for prefixed parameters.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer ‚Äî Part 5: The Training</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-5-the-training/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 21:04:32 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[In this fifth part of my series, I will outline the steps for creating a Docker container for training your image classification model, evaluating performance, and preparing for deployment.AI/ML engineers would prefer to focus on model training and data engineering, but the reality is that we also need to understand the infrastructure and mechanics behind the scenes.I hope to share some tips, not only to get your training run running, but how to streamline the process in a cost efficient manner on cloud resources such as Kubernetes.I will reference elements from my previous articles for getting the best model performance, so be sure to check out¬†Part 1¬†and¬†Part 2¬†on the data sets, as well as¬†Part 3¬†and¬†Part 4¬†on model evaluation.Here are the learnings that I will share with you, once we lay the groundwork on the infrastructure:Building your Docker containerExecuting your training runFirst, let me provide a brief description of the setup that I created, specifically around Kubernetes. Your setup may be entirely different, and that is just fine. I simply want to set the stage on the infrastructure so that the rest of the discussion makes sense.This is a server you deploy that provides a user interface to for your subject matter experts to label and evaluate images for the image classification application. The server can run as a pod on your Kubernetes cluster, but you may find that running a dedicated server with faster disk may be better.Image files are stored in a directory structure like the following, which is self-documenting and easily modified.Image_Library/
  - cats/
    - image1001.png
  - dogs/
    - image2001.pngIdeally, these files would reside on local server storage (instead of cloud or cluster storage) for better performance. The reason for this will become clear as we see what happens as the image library grows.Cloud Storage allows for a virtually limitless and convenient way to share files between systems. In this case, the image library on your management system could access the same files as your Kubernetes cluster or Docker engine.However, the downside of cloud storage is the latency to open a file. Your image library will have¬†¬†of images, and the latency to read each file will have a significant impact on your training run time. Longer training runs means more cost for using the expensive GPU processors!The way that I found to speed things up is to create a¬†¬†file of your image library on your management system and copy them to cloud storage. Even better would be to create multiple tar files¬†, each containing 10,000 to 20,000 images.This way you only have network latency on a handful of files (which contain thousands, once extracted) and you start your training run much sooner.Kubernetes or Docker engineA Kubernetes cluster, with proper configuration, will allow you to dynamically scale up/down nodes, so you can perform your model training on GPU hardware as needed. Kubernetes is a rather heavy setup, and there are other container engines that will work.The technology options change constantly!The main idea is that you want to spin up the resources you need ‚Äî for only as long as you need them ‚Äî then scale down to reduce your time (and therefore cost) of running expensive GPU resources.Once your GPU node is started and your Docker container is running, you can extract the¬†¬†files above to¬†¬†storage, such as an¬†, on your node. The node typically has high-speed SSD disk, ideal for this type of workload. There is one caveat ‚Äî the storage capacity on your node must be able to handle your image library.Assuming we are good, let‚Äôs talk about building your Docker container so that you can train your model on your image library.Building your Docker containerBeing able to execute a training run in a consistent manner lends itself perfectly to building a Docker container. You can ‚Äúpin‚Äù the version of libraries so you know exactly how your scripts will run every time. You can version control your containers as well, and revert to a known good image in a pinch. What is really nice about Docker is you can run the container pretty much anywhere.The tradeoff when running in a container, especially with an Image Classification model, is the speed of file storage. You can attach any number of volumes to your container, but they are usually¬†¬†attached, so there is latency on each file read. This may not be a problem if you have a small number of files. But when dealing with hundreds of thousands of files like image data, that latency adds up!This is why using the¬†¬†file method outlined above can be beneficial.Also, keep in mind that Docker containers could be terminated unexpectedly, so you should make sure to store important information outside the container, on cloud storage or a database. I‚Äôll show you how below.Knowing that you will need to run on GPU hardware (here I will assume Nvidia), be sure to select the right base image for your Dockerfile, such as¬†¬†with the ‚Äúdevel¬†flavor that will contain the right drivers.Next, you will add the script files to your container, along with a ‚Äúbatch‚Äù script to coordinate the execution. Here is an example Dockerfile, and then I‚Äôll describe what each of the scripts will be doing.#####   Dockerfile   #####
FROM nvidia/cuda:12.8.0-devel-ubuntu24.04

# Install system software
RUN apt-get -y update && apg-get -y upgrade
RUN apt-get install -y python3-pip python3-dev

# Setup python
WORKDIR /app
COPY requirements.txt
RUN python3 -m pip install --upgrade pip
RUN python3 -m pip install -r requirements.txt

# Pythong and batch scripts
COPY ExtractImageLibrary.py .
COPY Training.py .
COPY Evaluation.py .
COPY ScorePerformance.py .
COPY ExportModel.py .
COPY BulkIdentification.py .
COPY BatchControl.sh .

# Allow for interactive shell
CMD tail -f /dev/nullDockerfiles are declarative, almost like a cookbook for building a small server ‚Äî you know what you‚Äôll get every time. Python libraries benefit, too, from this declarative approach. Here is a sample¬†¬†file that loads the TensorFlow libraries with CUDA support for GPU acceleration.#####   requirements.txt   #####
numpy==1.26.3
pandas==2.1.4
scipy==1.11.4
keras==2.15.0
tensorflow[and-cuda]Extract Image Library scriptIn Kubernetes, the Docker container can access local, high speed storage on the physical node. This can be achieved via the¬†¬†volume type. As mentioned before, this will only work if the local storage on your node can handle the size of your library.#####   sample 25GB emptyDir volume in Kubernetes   #####
containers:
  - name: training-container
    volumeMounts:
      - name: image-library
        mountPath: /mnt/image-library
volumes:
  - name: image-library
    emptyDir:
      sizeLimit: 25GiYou would want to have another¬†¬†to your cloud storage where you have the¬†¬†files. What this looks like will depend on your provider, or if you are using a persistent volume claim, so I won‚Äôt go into detail here.Now you can extract the¬†¬†files ‚Äî ideally in parallel for an added performance boost ‚Äî to the local mount point.As AI/ML engineers, the model training is where we want to spend most of our time.This is where the magic happens!With your image library now extracted, we can create our train-validation-test sets, load a pre-trained model or build a new one, fit the model, and save the results.One key technique that has served me well is to load the most recently trained model as my base. I discuss this in more detail in¬†Part 4¬†under ‚ÄúFine tuning‚Äù, this results in faster training time and significantly improved model performance.Be sure to take advantage of the local storage to checkpoint your model during training since the models are quite large and you are paying for the GPU even while it sits idle writing to disk.This of course raises a concern about what happens if the Docker container dies part-way though the training. The risk is (hopefully) low from a cloud provider, and you may not want an incomplete training anyway. But if that does happen, you will at least want to understand¬†, and this is where saving the main log file to cloud storage (described below) or to a package like MLflow comes in handy.After your training run has completed and you have taken proper precaution on saving your work, it is time to see how well it performed.Normally this evaluation script will pick up on the model that just finished. But you may decide to point it at a previous model version through an interactive session. This is why have the script as stand-alone.With it being a separate script, that means it will need to read the completed model from disk ‚Äî ideally local disk for speed. I like having two separate scripts (training and evaluation), but you might find it better to combine these to avoid reloading the model.Now that the model is loaded, the evaluation script should generate predictions on¬†¬†image in the training, validation, test, and benchmark sets. I save the results as a¬†¬†matrix with the softmax confidence score for each class label. So, if there are 1,000 classes and 100,000 images, that‚Äôs a table with 100 million scores!I save these results in¬†¬†files that are then used in the score generation next.Taking the matrix of scores produced by the evaluation script above, we can now create various metrics of model performance. Again, this process could be combined with the evaluation script above, but my preference is for independent scripts. For example, I might want to regenerate scores on previous training runs. See what works for you.Here are some of the¬†¬†functions that produce useful insights like F1, log loss, AUC-ROC, Matthews correlation coefficient.from sklearn.metrics import average_precision_score, classification_report
from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_scoreAside from these basic statistical analyses for each dataset (train, validation, test, and benchmark), it is also useful to identify:Which¬†¬†labels get the most number of errors?Which¬†¬†labels get the most number of incorrect guesses?How many¬†ground-truth-to-predicted¬†label pairs are there? In other words, which classes are easily confused?What is the¬†¬†when applying a minimum softmax confidence score threshold?What is the¬†¬†above that softmax threshold?For the ‚Äúdifficult‚Äù benchmark sets, do you get a sufficiently¬†¬†score?For the ‚Äúout-of-scope‚Äù benchmark sets, do you get a sufficiently¬†¬†score?As you can see, there are multiple calculations and it‚Äôs not easy to come up with a single evaluation to decide if the trained model is good enough to be moved to production.In fact, for an image classification model, it is helpful to manually review the images that the model got wrong, as well as the ones that got a low softmax confidence score. Use the scores from this script to create a list of images to manually review, and then get a¬†¬†for how well the model performs.Check out¬†Part 3¬†for more in-depth discussion on evaluation and scoring.All of the heavy lifting is done by this point. Since your Docker container will be shutdown soon, now is the time to copy the model artifacts to cloud storage and prepare them for being put to use.The example Python code snippet below is more geared to Keras and TensorFlow. This will take the trained model and export it as a¬†. Later, I will show how this is used by TensorFlow Serving in the¬†¬†section below.# Increment current version of model and create new directory
next_version_dir, version_number = create_new_version_folder()

# Copy model artifacts to the new directory
copy_model_artifacts(next_version_dir)

# Create the directory to save the model export
saved_model_dir = os.path.join(next_version_dir, str(version_number))

# Save the model export for use with TensorFlow Serving
tf.keras.backend.set_learning_phase(0)
model = tf.keras.models.load_model(keras_model_file)
tf.saved_model.save(model, export_dir=saved_model_dir)This script also copies the other training run artifacts such as the model evaluation results, score summaries, and log files generated from model training. Don‚Äôt forget about your label map so you can give human readable names to your classes!Bulk identification scriptYour training run is complete, your model has been scored, and a new version is exported and ready to be served. Now is the time to use this latest model to assist you on trying to identify unlabeled images.As I described in¬†Part 4, you may have a collection of ‚Äúunknowns‚Äù ‚Äî really good pictures, but no idea what they are. Let your new model provide a best guess on these and record the results to a file or a database. Now you can create filters based on closest match and by high/low scores. This allows your subject matter experts to leverage these filters to find new image classes, add to existing classes, or to remove images that have very low scores and are no good.By the way, I put this step inside the GPU container since you may have thousands of ‚Äúunknown‚Äù images to process and the accelerated hardware will make light work of it. However, if you are not in a hurry, you could perform this step on a separate CPU node, and shutdown your GPU node sooner to save cost. This would especially make sense if your ‚Äúunknowns‚Äù folder is on slower cloud storage.All of the scripts described above perform a specific task ‚Äî from extracting your image library, executing model training, performing evaluation and scoring, exporting the model artifacts for deployment, and perhaps even bulk identification.One script to rule them allTo coordinate the entire show, this batch script gives you the entry point for your container and an easy way to trigger everything. Be sure to produce a log file in case you need to analyze any failures along the way. Also, be sure to write the log to your cloud storage in case the container dies unexpectedly.#!/bin/bash
# Main batch control script

# Redirect standard output and standard error to a log file
exec > /cloud_storage/batch-logfile.txt 2>&1

/app/ExtractImageLibrary.py
/app/Training.py
/app/Evaluation.py
/app/ScorePerformance.py
/app/ExportModel.py
/app/BulkIdentification.pyExecuting your training runSo, now it‚Äôs time to put everything in motion‚Ä¶Let‚Äôs go through the steps to prepare your image library, fire up your Docker container to train your model, and then examine the results.Image library ‚Äòtar‚Äô filesYour image management system should now create a¬†¬†file backup of your data. Since¬†¬†is a single-threaded function, you will get significant speed improvement by creating multiple tar files in parallel, each with a portion of you data.Now these files can be copied to your shared cloud storage for the next step.All the hard work you put into creating your container (described above) will be put to the test. If you are running Kubernetes, you can create a Job that will execute the¬†¬†script.Inside the Kubernetes Job definition, you can pass environment variables to adjust the execution of your script. For example, the batch size and number of epochs are set here and then pulled into your Python scripts, so you can alter the behavior without changing your code.#####   sample Job in Kubernetes   #####
containers:
  - name: training-job
    env:
      - name: BATCH_SIZE
        value: 50
      - name: NUM_EPOCHS
        value: 30
    command: ["/app/BatchControl.sh"]Once the Job is completed, be sure to verify that the GPU node properly scales back down to zero according to your scaling configuration in Kubernetes ‚Äî you don‚Äôt want to be saddled with a huge bill over a simple configuration error.With the training run complete, you should now have model artifacts saved and can examine the performance. Look through the metrics, such as F1 and log loss, and benchmark accuracy for high softmax confidence scores.As mentioned earlier, the reports only tell part of the story. It is worth the time and effort to manually review the images that the model got wrong or where it produced a low confidence score.Don‚Äôt forget about the bulk identification. Be sure to leverage these to locate new images to fill out your data set, or to find new classes.Once you have reviewed your model performance and are satisfied with the results, it is time to modify your TensorFlow Serving container to put the new model into production.TensorFlow Serving is available as a Docker container and provides a very quick and convenient way to serve your model. This container can listen and respond to API calls for your model.Let‚Äôs say your new model is version 7, and your¬†¬†script (see above) has saved the model in your cloud share as¬†/image_application/models/007. You can start the TensorFlow Serving container with that volume mount. In this example, the¬†¬†points to folder for version 007.#####   sample TensorFlow pod in Kubernetes   #####
containers:
  - name: tensorflow-serving
    image: bitnami/tensorflow-serving:2.18.0
    ports:
      - containerPort: 8501
    env:
      - name: TENSORFLOW_SERVING_MODEL_NAME
        value: "image_application"
    volumeMounts:
      - name: models-subfolder
        mountPath: "/bitnami/model-data"

volumes:
  - name: models-subfolder
    azureFile:
      shareName: "image_application/models/007"A subtle note here ‚Äî the export script should create a sub-folder, named 007 (same as the base folder), with the saved model export. This may seem a little confusing, but TensorFlow Serving will mount this share folder as¬†¬†and detect the numbered sub-folder inside it for the version to serve. This will allow you to query the API for the model version as well as the identification.As I mentioned at the start of this article, this setup has worked for my situation. This is certainly not the only way to approach this challenge, and I invite you to customize your own solution.I wanted to share my hard-fought learnings as I embraced cloud services in Kubernetes, with the desire to keep costs under control. Of course, doing all this while maintaining a high level of model performance is an added challenge, but one that you can achieve.I hope I have provided enough information here to help you with your own endeavors. Happy learnings!]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer ‚Äî Part 3: The Evaluation</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 21:00:06 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[In this third part of my series, I will explore the evaluation process which is a critical piece that will lead to a cleaner data set and elevate your model performance. We will see the difference between evaluation of a¬†¬†model (one not yet in production), and evaluation of a¬†¬†model (one making real-world predictions).In¬†Part 1, I discussed the process of labelling your image data that you use in your Image Classification project. I showed how to define ‚Äúgood‚Äù images and create sub-classes. In¬†Part 2, I went over various data sets, beyond the usual train-validation-test sets, such as benchmark sets, plus how to handle synthetic data and duplicate images.Evaluation of the trained modelAs machine learning engineers we look at accuracy, F1, log loss, and other metrics to decide if a model is ready to move to production. These are all important measures, but from my experience, these scores can be deceiving especially as the number of classes grows.Although it can be time consuming, I find it very important to manually review the images that the model gets¬†, as well as the images that the model gives a¬†¬†softmax ‚Äúconfidence‚Äù score to. This means adding a step immediately after your training run completes to calculate scores for¬†¬†images ‚Äî training, validation, test, and the benchmark sets. You only need to bring up for manual review the ones that the model had problems with. This should only be a small percentage of the total number of images. See the Double-check process belowWhat you do during the manual evaluation is to put yourself in a ‚Äú‚Äù to ensure that the labelling standards are being followed that you setup in Part 1. Ask yourself:‚ÄúIs this a good image?‚Äù Is the subject front and center, and can you clearly see all the features?‚ÄúIs this the correct label?‚Äù Don‚Äôt be surprised if you find wrong labels.You can either remove the bad images or fix the labels if they are wrong. Otherwise you can keep them in the data set and force the model to do better next time. Other questions I ask are:‚ÄúWhy did the model get this wrong?‚Äù‚ÄúWhy did this image get a low score?‚Äù‚ÄúWhat is it about the image that caused confusion?‚ÄùSometimes the answer has nothing to do with¬†¬†specific image. Frequently, it has to do with the¬†¬†images, either in the ground truth class or in the predicted class. It is worth the effort to Double-check all images in both sets if you see a consistently bad guess. Again, don‚Äôt be surprised if you find poor images or wrong labels.When doing the evaluation of the trained model (above), we apply a lot of subjective analysis ‚Äî ‚ÄúWhy did the model get this wrong?‚Äù and ‚ÄúIs this a good image?‚Äù From these, you may only get a¬†.Frequently, I will decide to hold off moving a model forward to production based on that gut feel. But how can you justify to your manager that you want to hit the brakes? This is where putting a more¬†¬†analysis comes in by creating a weighted average of the softmax ‚Äúconfidence‚Äù scores.In order to apply a weighted evaluation, we need to identify sets of classes that deserve adjustments to the score. Here is where I create a list of ‚Äúcommonly confused‚Äù classes.Commonly confused classesCertain animals at our zoo can easily be mistaken. For example, African elephants and Asian elephants have different ear shapes. If your model gets these two mixed up, that is not as bad as guessing a giraffe! So perhaps you give partial credit here. You and your subject matter experts (SMEs) can come up with a list of these pairs and a weighted adjustment for each.This weight can be factored into a modified cross-entropy loss function in the equation below. The back half of this equation will reduce the impact of being wrong for specific pairs of ground truth and prediction by using the ‚Äúweight‚Äù function as a lookup. By default, the weighted adjustment would be 1 for all pairings, and the commonly confused classes would get something like 0.5.In other words, it‚Äôs better to be unsure (have a¬†¬†confidence score) when you are wrong, compared to being super confident and wrong.Once this weighted log loss is calculated, I can compare to previous training runs to see if the new model is ready for production.Confidence threshold reportAnother valuable measure that incorporates the confidence threshold (in my example, 95) is to report on accuracy and false positive rates. Recall that when we apply the confidence threshold before presenting results, we help reduce false positives from being shown to the end user.In this table, we look at the breakdown of ‚Äútrue positive above 95‚Äù for each data set. We get a sense that when a ‚Äúgood‚Äù picture comes through (like the ones from our train-validation-test set) it is very likely to surpass the threshold, thus the user is ‚Äúhappy‚Äù with the outcome. Conversely, the ‚Äúfalse positive above 95‚Äù is extremely low for good pictures, thus only a small number of our users will be ‚Äúsad‚Äù about the results.We expect the train-validation-test set results to be exceptional since our data is curated. So, as long as people take ‚Äúgood‚Äù pictures, the model should do very well. But to get a sense of how it does on extreme situations, let‚Äôs take a look at our benchmarks.The ‚Äúdifficult‚Äù benchmark has more modest true positive and false positive rates, which reflects the fact that the images are more challenging. These values are much easier to compare across training runs, so that lets me set a min/max target. So for example, if I target a minimum of 80% for true positive, and maximum of 5% for false positive on this benchmark, then I can feel confident moving this to production.The ‚Äúout-of-scope‚Äù benchmark has no true positive rate because¬†¬†of the images belong to any class the model can identify. Remember, we picked things like a bag of popcorn, etc., that are not zoo animals, so there cannot be any true positives. But we do get a false positive rate, which means the model gave a confident score to that bag of popcorn as some animal. And if we set a target maximum of 10% for this benchmark, then we may not want to move it to production.Right now, you may be thinking, ‚ÄúWell, what animal did it pick for the bag of popcorn?‚Äù Excellent question! Now you understand the importance of doing a manual review of the images that get bad results.Evaluation of the deployed modelThe evaluation that I described above applies to a model immediately after¬†. Now, you want to evaluate how your model is doing in the¬†. The process is similar, but requires you to shift to a ‚Äú‚Äù and asking yourself, ‚ÄúDid the model get this correct?‚Äù and ‚ÄúShould it have gotten this correct?‚Äù and ‚ÄúDid we tell the user the right thing?‚ÄùSo, imagine that you are logging in for the morning ‚Äî after sipping on your¬†cold brew coffee, of course ‚Äî and are presented with 500 images that your zoo guests took yesterday of different animals. Your job is to determine how satisfied the guests were using your model to identify the zoo animals.Using the softmax ‚Äúconfidence‚Äù score for each image, we have a threshold before presenting results. Above the threshold, we tell the guest what the model predicted. I‚Äôll call this the ‚Äúhappy path‚Äù. And below the threshold is the ‚Äúsad path‚Äù where we ask them to try again.Your review interface will first show you all the ‚Äúhappy path‚Äù images one at a time. This is where you ask yourself, ‚ÄúDid we get this right?‚Äù Hopefully, yes!But if not, this is where things get tricky. So now you have to ask, ‚ÄúWhy not?‚Äù Here are some things that it could be:‚ÄúBad‚Äù picture ‚Äî Poor lighting, bad angle, zoomed out, etc ‚Äî refer to your labelling standards.Out-of-scope ‚Äî It‚Äôs a zoo animal, but unfortunately one that isn‚Äôt found in¬†¬†zoo. Maybe it belongs to another zoo (your guest likes to travel and try out your app). Consider adding these to your data set.Out-of-scope ‚Äî It‚Äôs not a zoo animal. It could be an animal in your zoo, but not one typically¬†¬†there, like a neighborhood sparrow or mallard duck. This might be a candidate to add.Out-of-scope ‚Äî It‚Äôs something found in the zoo. A zoo usually has interesting trees and shrubs, so people might try to identify those. Another candidate to add.Prankster ‚Äî Completely out-of-scope. Because people like to play with technology, there‚Äôs the possibility you have a prankster that took a picture of a bag of popcorn, or a soft drink cup, or even a selfie. These are hard to prevent, but hopefully get a low enough score (below the threshold) so the model did not identify it as a zoo animal. If you see enough pattern in these, consider creating a class with special handling on the front-end.After reviewing the ‚Äúhappy path‚Äù images, you move on to the ‚Äúsad path‚Äù images ‚Äî the ones that got a low confidence score and the app gave a ‚Äúsorry, try again‚Äù message. This time you ask yourself, ‚Äú¬†the model have given this image a higher score?‚Äù which would have put it in the ‚Äúhappy path‚Äù. If so, then you want to ensure these images are added to the training set so next time it will do better. But most of time, the low score reflects many of the ‚Äúbad‚Äù or out-of-scope situations mentioned above.Perhaps your model performance is suffering and it has nothing to do with your model. Maybe it is the ways you users interacting with the app. Keep an eye out of non-technical problems and share your observations with the rest of your team. For example:Are your users using the application in the ways you expected?Are they not following the instructions?Do the instructions need to be stated more clearly?Is there anything you can do to improve the experience?Collect statistics and new imagesBoth of the manual evaluations above open a gold mine of data. So, be sure to collect these statistics and feed them into a dashboard ‚Äî your manager and your future self will thank you!Keep track of these stats and generate reports that you and your can reference:How often the model is being called?What times of the day, what days of the week is it used?Are your system resources able to handle the peak load?What classes are the most common?After evaluation, what is the accuracy for each class?What is the breakdown for confidence scores?How many scores are above and below the confidence threshold?The single best thing you get from a deployed model is the additional real-world images! You can add these now images to improve coverage of your existing zoo animals. But more importantly, they provide you insight on¬†¬†classes to add. For example, let‚Äôs say people enjoy taking a picture of the large walrus statue at the gate. Some of these may make sense to incorporate into your data set to provide a better user experience.Creating a new class, like the walrus statue, is not a huge effort, and it avoids the false positive responses. It would be more embarrassing to identify a walrus statue as an elephant! As for the prankster and the bag of popcorn, you can configure your front-end to quietly handle these. You might even get creative and have fun with it like, ‚ÄúThank you for visiting the food court.‚ÄùIt is a good idea to double-check your image set when you suspect there may be problems with your data. I‚Äôm not suggesting a top-to-bottom check, because that would a monumental effort! Rather specific classes that you suspect could contain bad data that is degrading your model performance.Immediately after my training run completes, I have a script that will use this new model to generate predictions for my¬†¬†data set. When this is complete, it will take the list of incorrect identifications, as well as the low scoring predictions, and automatically feed that list into the Double-check interface.This interface will show, one at a time, the image in question, alongside an example image of the ground truth and an example image of what the model predicted. I can visually compare the three, side-by-side. The first thing I do is ensure the original image is a ‚Äúgood‚Äù picture, following my labelling standards. Then I check if the ground-truth label is indeed correct, or if there is something that made the model think it was the predicted label.Remove the original image if the image quality is poor.Relabel the image if it belongs in a different class.During this manual evaluation, you might notice dozens of the same wrong prediction. Ask yourself why the model made this mistake when the images seem perfectly fine. The answer may be some incorrect labels on images in the ground truth, or even in the predicted class!Don‚Äôt hesitate to add those classes and sub-classes back into the Double-check interface and step through them all. You may have 100‚Äì200 pictures to review, but there is a good chance that one or two of the images will stand out as being the culprit.With a different mindset for a trained model versus a deployed model, we can now evaluate performances to decide which models are ready for production, and how well a production model is going to serve the public. This relies on a solid Double-check process and a critical eye on your data. And beyond the ‚Äúgut feel‚Äù of your model, we can rely on the benchmark scores to support us.In¬†Part 4, we kick off the training run, but there are some subtle techniques to get the most out of the process and even ways to leverage throw-away models to expand your library image data.]]></content:encoded></item><item><title>Build a dynamic, role-based AI agent using Amazon Bedrock inline agents</title><link>https://aws.amazon.com/blogs/machine-learning/build-a-dynamic-role-based-ai-agent-using-amazon-bedrock-inline-agents/</link><author>Ishan Singh</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:56:28 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[AI agents continue to gain momentum, as businesses use the power of generative AI to reinvent customer experiences and automate complex workflows. We are seeing Amazon Bedrock Agents applied in investment research, insurance claims processing, root cause analysis, advertising campaigns, and much more. Agents use the reasoning capability of foundation models (FMs) to break down user-requested tasks into multiple steps. They use developer-provided instructions to create an orchestration plan and carry out that plan by securely invoking company APIs and accessing knowledge bases using Retrieval Augmented Generation (RAG) to accurately handle the user‚Äôs request.Although organizations see the benefit of agents that are defined, configured, and tested as managed resources, we have increasingly seen the need for an additional, more dynamic way to invoke agents. Organizations need solutions that adjust on the fly‚Äîwhether to test new approaches, respond to changing business rules, or customize solutions for different clients. This is where the new inline agents capability in Amazon Bedrock Agents becomes transformative. It allows you to dynamically adjust your agent‚Äôs behavior at runtime by changing its instructions, tools, guardrails, knowledge bases, prompts, and even the FMs it uses‚Äîall without redeploying your application.In this post, we explore how to build an application using Amazon Bedrock inline agents, demonstrating how a single AI assistant can adapt its capabilities dynamically based on user roles.Inline agents in Amazon Bedrock AgentsThis runtime flexibility enabled by inline agents opens powerful new possibilities, such as: ‚Äì Inline agents minimize the time-consuming create/update/prepare cycles traditionally required for agent configuration changes. Developers can instantly test different combinations of models, tools, and knowledge bases, dramatically accelerating the development process.A/B testing and experimentation ‚Äì Data science teams can systematically evaluate different model-tool combinations, measure performance metrics, and analyze response patterns in controlled environments. This empirical approach enables quantitative comparison of configurations before production deployment.Subscription-based personalization ‚Äì Software companies can adapt features based on each customer‚Äôs subscription level, providing more advanced tools for premium users.Persona-based data source integration ‚Äì Institutions can adjust content complexity and tone based on the user‚Äôs profile, providing persona-appropriate explanations and resources by changing the knowledge bases associated to the agent on the fly. ‚Äì Developers can create applications with hundreds of APIs, and quickly and accurately carry out tasks by dynamically choosing a small subset of APIs for the agent to consider for a given request. This is particularly helpful for large software as a service (SaaS) platforms needing multi-tenant scaling.Inline agents expand your options for building and deploying agentic solutions with Amazon Bedrock Agents. For workloads needing managed and versioned agent resources with a pre-determined and tested configuration (specific model, instructions, tools, and so on), developers can continue to use InvokeAgent on resources created with CreateAgent. For workloads that need dynamic runtime behavior changes for each agent invocation, you can use the new InvokeInlineAgent API. With either approach, your agents will be secure and scalable, with configurable guardrails, a flexible set of model inference options, native access to knowledge bases, code interpretation, session memory, and more.Our HR assistant example shows how to build a single AI assistant that adapts to different user roles using the new inline agent capabilities in Amazon Bedrock Agents. When users interact with the assistant, the assistant dynamically configures agent capabilities (such as model, instructions, knowledge bases, action groups, and guardrails) based on the user‚Äôs role and their specific selections. This approach creates a flexible system that adjusts its functionality in real time, making it more efficient than creating separate agents for each user role or tool combination. The complete code for this HR assistant example is available on our GitHub repo.This dynamic tool selection enables a personalized experience. When an employee logs in without direct reports, they see a set of tools that they have access to based on their role. They can select from options like requesting vacation time, checking company policies using the knowledge base, using a code interpreter for data analysis, or submitting expense reports. The inline agent assistant is then configured with only these selected tools, allowing it to assist the employee with their chosen tasks. In a real-world example, the user would not need to make the selection, because the application would make that decision and automatically configure the agent invocation at runtime. We make it explicit in this application so that you can demonstrate the impact.Similarly, when a manager logs in to the same system, they see an extended set of tools reflecting their additional permissions. In addition to the employee-level tools, managers have access to capabilities like running performance reviews. They can select which tools they want to use for their current session, instantly configuring the inline agent with their choices.The inclusion of knowledge bases is also adjusted based on the user‚Äôs role. Employees and managers see different levels of company policy information, with managers getting additional access to confidential data like performance review and compensation details. For this demo, we‚Äôve implemented metadata filtering to retrieve only the appropriate level of documents based on the user‚Äôs access level, further enhancing efficiency and security.Let‚Äôs look at how the interface adapts to different user roles.The employee view provides access to essential HR functions like vacation requests, expense submissions, and company policy lookups. Users can select which of these tools they want to use for their current session.The manager view extends these options to include supervisory functions like compensation management, demonstrating how the inline agent can be configured with a broader set of tools based on user permissions.The manager view extends these capabilities to include supervisory functions like compensation management, demonstrating how the inline agent dynamically adjusts its available tools based on user permissions. Without inline agents, we would need to build and maintain two separate agents.As shown in the preceding screenshots, the same HR assistant offers different tool selections based on the user‚Äôs role. An employee sees options like Knowledge Base, Apply Vacation Tool, and Submit Expense, whereas a manager has additional options like Performance Evaluation. Users can select which tools they want to add to the agent for their current interaction.This flexibility allows for quick adaptation to user needs and preferences. For instance, if the company introduces a new policy for creating business travel requests, the tool catalog can be quickly updated to include a Create Business Travel Reservation tool. Employees can then choose to add this new tool to their agent configuration when they need to plan a business trip, or the application could automatically do so based on their role.With Amazon Bedrock inline agents, you can create a catalog of actions that is dynamically selected by the application or by users of the application. This increases the level of flexibility and adaptability of your solutions, making them a perfect fit for navigating the complex, ever-changing landscape of modern business operations. Users have more control over their AI assistant‚Äôs capabilities, and the system remains efficient by only loading the necessary tools for each interaction.Technical foundation: Dynamic configuration and action selectionInline agents allow dynamic configuration at runtime, enabling a single agent to effectively perform the work of many. By specifying action groups and modifying instructions on the fly, even within the same session, you can create versatile AI applications that adapt to various scenarios without multiple agent deployments.The following are key points about inline agents: ‚Äì Change the agent‚Äôs configuration, including its FM, at runtime. This enables rapid experimentation and adaptation without redeploying the application, reducing development cycles. ‚Äì Apply governance and access control at the tool level. With agents changing dynamically at runtime, tool-level governance helps maintain security and compliance regardless of the agent‚Äôs configuration. ‚Äì Provide only necessary tools and instructions at runtime to reduce token usage and improve the agent accuracy. With fewer tools to choose from, it‚Äôs less complicated for the agent to select the right one, reducing hallucinations in the tool selection process. This approach can also lead to lower costs and improved latency compared to static agents because removing unnecessary tools, knowledge bases, and instructions reduces the number of input and output tokens being processed by the agent‚Äôs large language model (LLM). ‚Äì Create reusable actions for dynamic selection based on specific needs. This modular approach simplifies maintenance, updates, and scalability of your AI applications.The following are examples of reusable actions:Enterprise system integration ‚Äì Connect with systems like Salesforce, GitHub, or databases ‚Äì Perform common tasks such as sending emails or managing calendars ‚Äì Interact with specialized internal tools and services ‚Äì Analyze text, structured data, or other information ‚Äì Fetch weather updates, stock prices, or perform web searches ‚Äì Use specific machine learning (ML) models for targeted tasksWhen using inline agents, you configure parameters for the following:Contextual tool selection based on user intent or conversation flowAdaptation to different user roles and permissionsSwitching between communication styles or personasModel selection based on task complexityThe inline agent uses the configuration you provide at runtime, allowing for highly flexible AI assistants that efficiently handle various tasks across different business contexts.Building an HR assistant using inline agentsLet‚Äôs look at how we built our HR Assistant using Amazon Bedrock inline agents: ‚Äì We developed a demo catalog of HR-related tools, including: 
   ‚Äì Using Amazon Bedrock Knowledge Bases for accessing company policies and guidelines based on the role of the application user. In order to filter the knowledge base content based on the user‚Äôs role, you also need to provide a metadata file specifying the type of employee‚Äôs roles that can access each file‚Äì For requesting and tracking time off.‚Äì For submitting and managing expense reports. ‚Äì For performing calculations and data analysis.‚Äì for conducting and reviewing employee compensation assessments (manager only access). ‚Äì We defined multiple conversation tones to suit different interaction styles: 
   ‚Äì For formal, business-like interactions. ‚Äì For friendly, everyday support. ‚Äì For upbeat, encouraging assistance. ‚Äì We implemented role-based access control. The application backend checks the user‚Äôs role (employee or manager) and provides access to appropriate tools and information and passes this information to the inline agent. The role information is also used to configure metadata filtering in the knowledge bases to generate relevant responses. The system allows for dynamic tool use at runtime. Users can switch personas or add and remove tools during their session, allowing the agent to adapt to different conversation needs in real time.Integrate the agent with other services and tools ‚Äì We connected the inline agent to: 
  Amazon Bedrock Knowledge Bases for company policies, with metadata filtering for role-based access.AWS Lambda functions for executing specific actions (such as submitting vacation requests or expense reports).A code interpreter tool for performing calculations and data analysis. ‚Äì We created a Flask-based UI that performs the following actions: 
  Displays available tools based on the user‚Äôs role.Allows users to select different personas.Provides a chat window for interacting with the HR assistant.To understand how this dynamic role-based functionality works under the hood, let‚Äôs examine the following system architecture diagram.As shown in preceding architecture diagram, the system works as follows:The end-user logs in and is identified as either a manager or an employee.The user selects the tools that they have access to and makes a request to the HR assistant.The agent breaks down the problems and uses the available tools to solve for the query in steps, which may include: 
  Amazon Bedrock Knowledge Bases (with metadata filtering for role-based access).Lambda functions for specific actions.Code interpreter tool for calculations.Compensation tool (accessible only to managers to submit base pay raise requests).The application uses the Amazon Bedrock inline agent to dynamically pass in the appropriate tools based on the user‚Äôs role and request.The agent uses the selected tools to process the request and provide a response to the user.This approach provides a flexible, scalable solution that can quickly adapt to different user roles and changing business needs.In this post, we introduced the Amazon Bedrock inline agent functionality and highlighted its application to an HR use case. We dynamically selected tools based on the user‚Äôs roles and permissions, adapted instructions to set a conversation tone, and selected different models at runtime. With inline agents, you can transform how you build and deploy AI assistants. By dynamically adapting tools, instructions, and models at runtime, you can:Create personalized experiences for different user rolesOptimize costs by matching model capabilities to task complexityStreamline development and maintenanceScale efficiently without managing multiple agent configurationsFor organizations demanding highly dynamic behavior‚Äîwhether you‚Äôre an AI startup, SaaS provider, or enterprise solution team‚Äîinline agents offer a scalable approach to building intelligent assistants that grow with your needs. To get started, explore our GitHub repo and HR assistant demo application, which demonstrate key implementation patterns and best practices.To learn more about how to be most successful in your agent journey, read our two-part blog series:To get started with Amazon Bedrock Agents, check out the following GitHub repository with example code.¬†is a Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building Generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau.¬†is a Senior Generative AI Data Scientist at AWS. With a background in machine learning, she has over 10 years of experience architecting and building AI applications with customers across industries. As a technical lead, she helps customers accelerate their achievement of business value through generative AI solutions on Amazon Bedrock. In her free time, Maira enjoys traveling, playing with her cat, and spending time with her family someplace warm. is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Mark‚Äôs work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS certifications, including the ML Specialty Certification. is a Sr. Enterprise Solutions Architect at AWS, experienced in Software Engineering, Enterprise Architecture, and AI/ML. He is deeply passionate about exploring the possibilities of generative AI. He collaborates with customers to help them build well-architected applications on the AWS platform, and is dedicated to solving technology challenges and assisting with their cloud journey. is a Software Development Engineer at Amazon Web Services (AWS). He specializes in backend system design, distributed architectures, and scalable solutions, contributing to the development and launch of high-impact systems at Amazon. Outside of work, he spends his time playing ping pong and hiking through Cascade trails, enjoying the outdoors as much as he enjoys building systems. is a Software Development Engineer at Amazon Web Services (AWS), working in Agents for Amazon Bedrock. He focuses on developing scalable systems on the cloud that enable AI applications frameworks and orchestrations. Shubham also has a background in building distributed, scalable, high-volume-high-throughput systems in IoT architectures. is a Principal Engineer for Amazon Bedrock.¬†He focuses on building deep learning-based AI and computer vision solutions for AWS customers. Oustide of work, Vivek enjoys trekking and following cricket.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer ‚Äî Part 1: The Data</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:55:53 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[It is said that in order for a machine learning model to be successful, you need to have good data. While this is true (and pretty much obvious), it is extremely difficult to define, build, and sustain good data. Let me share with you the unique processes that I have learned over several years building an ever-growing image classification system and how you can apply these techniques to your own application.With persistence and diligence, you can avoid the classic ‚Äúgarbage in, garbage out‚Äù, maximize your model accuracy, and demonstrate real business value.In this series of articles, I will dive into the care and feeding of a multi-class, single-label image classification app and what it takes to reach the highest level of performance. I won‚Äôt get into any coding or specific user interfaces, just the main concepts that you can incorporate to suit your needs with the tools at your disposal.Here is a brief description of the articles. You will notice that the model is last on the list since we need to focus on curating the data first and foremost:Over the past six years, I have been primarily focused on building and maintaining an image classification application for a manufacturing company. Back when I started, most of the software did not exist or was too expensive, so I created these from scratch. In this time, I have deployed two identifier applications, the largest handles 1,500 classes and achieves 97‚Äì98% accuracy.It was about eight years ago that I started online studies for Data Science and machine learning. So, when the exciting opportunity to create an AI application presented itself, I was prepared to build the tools I needed to leverage the latest advancements. I jumped in with both feet!I quickly found that building and deploying a model is probably the easiest part of the job. Feeding high quality data into the model is the best way to improve performance, and that requires focus and patience. Attention to detail is what I do best, so this was a perfect fit.It all starts with the dataI feel that so much attention is given to the model selection (deciding which neural network is best) and that the data is just an afterthought. I have found the hard way that even one or two pieces of bad data can significantly impact model performance, so that is where we need to focus.For example, let‚Äôs say you train the classic cat versus dog image classifier. You have 50 pictures of cats and 50 pictures of dogs, however one of the ‚Äúcats‚Äù is clearly (objectively) a picture of a dog. The computer doesn‚Äôt have the luxury of ignoring the mislabelled image, and instead adjusts the model weights to make it fit. Square peg meets round hole.Another example would be a picture of a cat that climbed up into a tree. But when you take a wholistic view of it, you would describe it as a picture of a tree (first) with a cat (second). Again, the computer doesn‚Äôt know to ignore the big tree and focus on the cat ‚Äî it will start to identify trees as cats, even if there is a dog. You can think of these pictures as outliers and should be removed.It doesn‚Äôt matter if you have the best neural network in the world, you can count on the model making poor predictions when it is trained on ‚Äúbad‚Äù data. I‚Äôve learned that any time I see the model make mistakes, it‚Äôs time to review the data.Example Application ‚Äî Zoo animalsFor the rest of this write-up, I will use an example of identifying zoo animals. Let‚Äôs assume your goal is to create a mobile app where guests at the zoo can take pictures of the animals they see and have the app identify them. Specifically, this is a multi-class, single-label application.¬†‚Äî There are a lot of different animals at the zoo and many of them look very similar.¬†‚Äî Guests using the app don‚Äôt always take good pictures (zoomed out, blurry, too dark), so we don‚Äôt want to provide an answer if the image is poor.¬†‚Äî The zoo keeps expanding and adding new species all the time.¬†‚Äî Occasionally you might find that people take pictures of the sparrows near the food court grabbing some dropped popcorn.¬†‚Äî Just for fun, guests may take a picture of the bag of popcorn just to see what it comes back with.These are all real challenges ‚Äî being able to tell the subtle differences between animals, handling out-of-scope cases, and just plain poor images.Before we get there, let‚Äôs start from the beginning.There are a lot of tools these days to help you with this part of the process, but the challenge remains the same ‚Äî collecting, labelling, and curating the data.Having data to collect is challenge #1. Without images, you have nothing to train. You may need to get creative on sourcing the data, or even creating synthetic data. More on that later.A quick note about image pre-processing. I convert all my images to the input size of my neural network and save them as PNG. Inside this square PNG, I preserve the aspect ratio of the original picture and fill the background black. I don‚Äôt stretch the image nor crop any features out. This also helps center the subject.Challenge #2 is to establish standards for data quality‚Ä¶and ensure that these standards are followed! These standards will guide you toward that ‚Äúgood‚Äù data. And this assumes, of course, correct labels. Having both is much easier said than done!I hope to show how ‚Äúgood‚Äù and ‚Äúcorrect‚Äù actually go hand-in-hand, and how important it is to apply these standards to every image.First, I want to point out that the image data discussed here is for the training set. What qualifies as a good image for¬†¬†is a bit different than what qualifies as a good image for¬†. More on that in Part 3.So, what is ‚Äúgood‚Äù data when talking about images? ‚ÄúA picture is worth a thousand words‚Äù, and if the¬†¬†you use to describe the picture do not include the subject you are trying to label, then it is not good and you need remove it from your training set.For example, let‚Äôs say you are shown a picture of a zebra and (removing bias toward your application) you describe it as an ‚Äúopen field with a zebra in the distance‚Äù. In other words, if ‚Äúopen field‚Äù is the first thing you notice, then you likely do¬†¬†want to use that image. The opposite is also true ‚Äî if the picture is way too close, you would described it as ‚Äúzebra pattern‚Äù.What you want is a description like, ‚Äúa zebra, front and center‚Äù. This would have your subject taking up about 80‚Äì90% of the total frame. Sometimes I will take the time to crop the original image so the subject is framed properly.Keep in mind the use of image augmentation at the time of training. Having that buffer around the edges will allow ‚Äúzoom in‚Äù augmentation. And ‚Äúzoom out‚Äù augmentation will simulate smaller subjects, so don‚Äôt start out less than 50% of the total frame for your subject since you lose detail.Another aspect of a ‚Äúgood‚Äù image relates to the label. If you can only see the back side of your zoo animal, can you really tell, for example, that it is a cheetah versus a leopard? The key identifying features need to be visible. If a human struggles to identify it, you can‚Äôt expect the computer to learn anything.What does a ‚Äúbad‚Äù image look like? Here is what I frequently watch out for:Wide angle lens stretchingHigh contrast or dark shadows‚ÄúDoctored‚Äù images, drawn lines and arrows‚ÄúUnusual‚Äù angles or situationsPicture of a mobile device that has a picture of your subjectIf you have a team of subject matter experts (SMEs) on hand to label the images, you are in a good starting position. Animal trainers at the zoo know the various species, and can spot the differences between, for example, a chimpanzee and a bonobo.To a Machine Learning Engineer, it is easy for you to assume all labels from your SMEs are correct and move right on to training the model. However, even experts make mistakes, so if you can get a second opinion on the labels, your error rate should go down.In reality, it can be prohibitively expensive to get one, let alone two, subject matter experts to review image labels. The SME usually has years of experience that make them more valuable to the business in other areas of work. My experience is that the machine learning engineer (that‚Äôs you and me) becomes the second opinion, and often the first opinion as well.Over time, you can become pretty adept at labelling, but certainly not an SME. If you do have the luxury of access to an expert, explain to them the labelling standards and how these are required for the application to be successful. Emphasize ‚Äúquality over quantity‚Äù.It goes without saying that having a¬†¬†label is so important. However, all it takes is one or two mislabelled images to degrade performance. These can easily slip into your data set with careless or hasty labelling. So, take the time to get it right.Ultimately, we as the ML engineer are responsible for model performance. So, if we take the approach of only working on model training and deployment, we will find ourselves wondering why performance is falling short.A lot of times, you will come across a really good picture of a very interesting subject, but have no idea what it is! It would be a shame to simply dispose of it. What you can do is assign it a generic label, like ‚ÄúUnknown Bird‚Äù or ‚ÄúRandom Plant‚Äù that are¬†¬†included in your training set. Later in Part 4, you‚Äôll see how to come back to these images at a later date when you have a better idea what they are, and you‚Äôll be glad you saved them.If you have done any image labelling, then you know how time consuming and difficult it can be. But this is where having a model, even a less-than-perfect model, can help you.Typically, you have a large collection of unlabelled image and you need to go through them one at a time to assign labels. Simply having the model offer a best guess and display the top 3 results lets you step through each image in a matter of seconds!Even if the top 3 results are wrong, this can help you narrow down your search. Over time, newer models will get better, and the labelling process can even be somewhat fun!In Part 4, I will show how you can bulk identify images and take this to the next level for faster labelling.I mentioned the example above of two species that look very similar, the chimpanzee and the bonobo. When you start out building your data set, you may have very sparse coverage of one or both of these species. In machine learning terms, we these ‚Äúclasses‚Äù. One option is to roll with what you have and hope that the model picks up on the differences with only a handful of example images.The option that I have used is to merge two or more classes into one, at least temporarily. So, in this case I would create a class called ‚Äúchimp-bonobo‚Äù, which is composed of the limited example pictures of chimpanzee and bonobo species classes. Combined, these may give me enough to train the model on ‚Äúchimp-bonobo‚Äù, with the trade-off that it‚Äôs a more generic identification.Sub-classes can even be normal variations. For example,¬†¬†pink flamingos are grey instead of pink. Or, male and female orangutans have distinct facial features. You wan to have a fairly balanced number of images for these normal variations, and keeping sub-classes will allow you to accomplish this.Don‚Äôt be concerned that you are merging completely different looking classes ‚Äî the neural network does a nice job of applying the ‚ÄúOR‚Äù operator. This works both ways ‚Äî it can help you identify male or female variations as one species, but it can hurt you when ‚Äúbad‚Äù outlier images sneak in like the example ‚Äúopen field with a zebra in the distance.‚ÄùOver time, you will (hopefully) be able to collect more images of the sub-classes and then be able to successfully split them apart (if necessary) and train the model to identify them separately. This process has worked very well for me. Just be sure to double-check all the images when you split them to ensure the labels didn‚Äôt get accidentally mixed up ‚Äî it will be time well spent.All of this certainly depends on your user requirements, and you can handle this in different ways either by creating a unique class label like ‚Äúchimp-bonobo‚Äù, or at the front-end presentation layer where you notify the user that you have intentionally merged these classes and provide guidance on further refining the results. Even after you decide to split the two classes, you may want to caution the user that the model could be wrong since the two classes are so similar.I realize this was a long write-up for something that on the surface seems intuitive, but these are all areas that I have tripped me up in the past because I didn‚Äôt give them enough attention. Once you have a solid understanding of these principles, you can go on to build a successful application.In¬†Part 2, we will take the curated data we collected here to create the classic data sets, with a custom benchmark set that will further enhance your data. Then we will see how best to evaluate our trained model using a specific ‚Äútraining mindset‚Äù, and switch to a ‚Äúproduction mindset‚Äù when evaluating a deployed model.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer ‚Äî Part 4: The Model</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:53:42 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[In this latest part of my series, I will share what I have learned on selecting a model for Image Classification and how to fine tune that model. I will also show how you can leverage the model to accelerate your labelling process, and finally how to justify your efforts by generating usage and performance statistics.In¬†Part 1, I discussed the process of labelling your image data that you use in your image classification project. I showed how define ‚Äúgood‚Äù images and create sub-classes. In¬†Part 2, I went over various data sets, beyond the usual train-validation-test sets, with benchmark sets, plus how to handle synthetic data and duplicate images. In Part 3, I explained how to apply different evaluation criteria to a trained model versus a deployed model, and using benchmarks to determine when to deploy a model.So far I have focused a lot of time on labelling and curating the set of images, and also evaluating model performance, which is like putting the cart before the horse. I‚Äôm not trying to minimize what it takes to design a massive neural network ‚Äî this is a very important part of the application you are building. In my case, I spent a few weeks experimenting with different available models before settling on one that fit the bill.Once you pick a model structure, you usually don‚Äôt make any major changes to it. For me, six years into deployment, I‚Äôm still using the same one. Specifically, I chose Inception V4 because it has a large input image size and an adequate number of layers to pick up on subtle image features. It also performs inference fast enough on CPU, so I don‚Äôt need to run expensive hardware to serve the model.Your mileage may vary. But again, the main takeaway is that focusing on your data will pay dividends versus searching for the best model.I will share a process that I found to work extremely well. Once I decided on the model to use, I randomly initialized the weights and let the model train for about 120 epoch before improvements plateau at a fairly modest accuracy, like 93%. At this point, I performed the evaluation of the trained model (see Part 3) to clean up the data set. I also incorporated new images as part of the data pipeline (see Part 1) and prepared the data sets for the next training run.Before starting the next training run, I simply take the last trained model, pop the output layer, and add it back in with random weights. Since the number of output classes are constantly increasing in my case, I have to pop that layer anyway to account for the new number of classes. Importantly, I leave the rest of the trained weights as they were and allow them to continue updating for the new classes.This allows the model to train much faster before improvements stall. After repeating this process dozens of times, the training reaches plateau after about 20 epochs, and the test accuracy can reach 99%! The model is building upon the low-level features that it established from the previous runs while re-learning the output weights to prevent overfitting.It took me a while to trust this process, and for a few years I would train from scratch every time. But after I attempted this and saw the training time (not to mention the cost of cloud GPU) go down while the accuracy continued to go up, I started to embrace the process. More importantly, I continue to see the evaluation metrics of the deployed model return solid performances.During training, you can apply transformations on your images (called ‚Äúaugmentation‚Äù) to give you more diversity from you data set. With our zoo animals, it is fairly safe to apply left-right flop, slight rotations clockwise and counterclockwise, and slight resize that will zoom in and out.With these transformations in mind, make sure your images are still able to act as good training images. In other words, an image where the subject is already small will be even smaller with a zoom out, so you probably want to discard the original. Also, some of your original pictures may need to be re-oriented by 90 degrees to be upright since a further rotation would make them look unusual.As I mentioned in Part 1, you can use the trained model to assist you in labelling images one at a time. But the way to take this even further is to have your newly trained model identify hundreds at a time while building a list of the results that you can then filter.Typically, we have large collections of¬†¬†images that have come in either through regular usage of the application or some other means. Recall from Part 1 assigning ‚Äúunknown‚Äù labels to interesting pictures but you have no clue what it is. By using the bulk identification method, we can sift through the collections quickly to target the labelling once we know what they are.By combining your current image counts with the bulk identification results, you can target classes that need expanded coverage. Here are a few ways you can leverage bulk identification:Increase low image counts¬†‚Äî Some of your classes may have just barely made the cutoff to be included in the training set, which means you need more examples to improve coverage. Filter for images that have low counts.Replace staged or synthetic images¬†‚Äî Some classes may be built entirely using non-real-world images. These pictures may be good enough to get started with, but may cause performance issues down the road because they look different than what typically comes through. Filter for classes that depend on staged images.¬†‚Äî A class in your data set may look like another one. For example, let‚Äôs say your model can identify an antelope, and that looks like a gazelle which your model cannot identify yet. Setting a filter for antelope and a lower confidence score may reveal gazelle images that you can label.¬†‚Äî You may not have known how to identify the dozens of cute wallaby pictures, so you saved them under ‚ÄúUnknown‚Äù because it was a good image. Now that you know what it is, you can filter for its look-alike kangaroo and quickly add a new class.Mass removal of low scores¬†‚Äî As a way to clean out your large collection of unlabelled images that have nothing worth labelling, set a filter for lowest scores.Recall the decision I made to have image cutoffs from Part 2, which allows us to ensure an adequate number of example images of a class before we train and server a model to the public. The problem is that you may have a number of classes that are¬†¬†below your cutoff (in my case, 40) and don‚Äôt make it into the model.The way I approach this is with a ‚Äúthrow-away‚Äù training run that I do not intend to move to production. I will decrease the lower cutoff from 40 to perhaps 35, build my train-validation-test sets, then train and evaluate like I normally do. The most important part of this is the bulk identification at the end!There is a chance that somewhere in the large collection of unlabelled images I will find the few that I need. Doing the bulk identification with this throw-away model helps find them.One very important aspect of any machine learning application is being able to show usage and performance reports. Your manager will likely want to see how many times the application is being used to justify the expense, and you as the ML engineer will want to see how the latest model is performing compared to the previous one.You should build logging into your model serving to record every transaction going through the system. Also, the manual evaluations from Part 3 should be recorded so you can report on performance for such things as accuracy over time, by model version, by confidence scores, by class, etc. You will be able to detect trends and make adjustments to improve the overall solution.There are a lot of reporting tools, so I won‚Äôt recommend one over the other. Just make sure you are collecting as much information as you can to build these dashboards. This will justify the time, effort, and cost associated with maintaining the application.We covered a lot of ground across this four-part series on building an image classification project and deploying it in the real world. It all starts with the data, and by investing the time and effort into maintaining the highest quality image library, you can reach impressive levels of model performance that will gain the trust and confidence of your business partners.As a Machine Learning Engineer, you are primarily responsible for building and deploying your model. But it doesn‚Äôt stop there ‚Äî dive into the data. The more familiar you are with the data, the better you will understand the strengths and weaknesses of your model. Take a close look at the evaluations and use them as an opportunity to adjust the data set.I hope these articles have helped you find new ways to improve your own machine learning project. And by the way, don‚Äôt let the machine do all the learning ‚Äî as humans, our job is to continue our own learning, so don‚Äôt ever stop!Thank you for taking this deep dive with me into a data-driven approach to model optimization. I look forward to your feedback and how you can apply this to your own application.]]></content:encoded></item><item><title>Use language embeddings for zero-shot classification and semantic search with Amazon Bedrock</title><link>https://aws.amazon.com/blogs/machine-learning/use-language-embeddings-for-zero-shot-classification-and-semantic-search-with-amazon-bedrock/</link><author>Tom Rogers</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:53:32 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[In this post, we discuss what embeddings are, show how to practically use language embeddings, and explore how to use them to add functionality such as zero-shot classification and semantic search. We then use Amazon Bedrock and language embeddings to add these features to a really simple syndication (RSS) aggregator application.Amazon Bedrock is a fully managed service that makes foundation models (FMs) from leading AI startups and Amazon available through an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case. Amazon Bedrock offers a serverless experience, so you can get started quickly, privately customize FMs with your own data, and integrate and deploy them into your applications using Amazon Web Services (AWS) services without having to manage infrastructure. For this post, we use the Cohere v3 Embed model on Amazon Bedrock to create our language embeddings.To demonstrate some of the possible uses of these language embeddings, we developed an RSS aggregator website. RSS is a web feed that allows publications to publish updates in a standardized, computer-readable way. On our website, users can subscribe to an RSS feed and have an aggregated, categorized list of the new articles. We use embeddings to add the following functionalities:This post uses this application as a reference point to discuss the technical implementation of the semantic search and zero-shot classification features.This solution uses the following services:The following diagram illustrates the solution architecture.This section offers a quick primer on what embeddings are and how they can be used.Embeddings are numerical representations of concepts or objects, such as language or images. In this post, we discuss language embeddings. By reducing these concepts to numerical representations, we can then use them in a way that a computer can understand and operate on.Let‚Äôs take Berlin and Paris as an example. As humans, we understand the conceptual links between these two words. Berlin and Paris are both cities, they‚Äôre capitals of their respective countries, and they‚Äôre both in Europe. We understand their conceptual similarities almost instinctively, because we can create a model of the world in our head. However, computers have no built-in way of representing these concepts.To represent these concepts in a way a computer can understand, we convert them into language embeddings. Language embeddings are high dimensional vectors that learn their relationships with each other through the training of a neural network. During training, the neural network is exposed to enormous amounts of text and learns patterns based on how words are colocated and relate to each other in different contexts.Embedding vectors allow computers to model the world from language. For instance, if we embed ‚ÄúBerlin‚Äù and ‚ÄúParis,‚Äù we can now perform mathematical operations on these embeddings. We can then observe some fairly interesting relationships. For instance, we could do the following: Paris ‚Äì France + Germany ~= Berlin. This is because the embeddings capture the relationships between the words ‚ÄúParis‚Äù and ‚ÄúFrance‚Äù and between ‚ÄúGermany‚Äù and ‚ÄúBerlin‚Äù‚Äîspecifically, that Paris and Berlin are both capital cities of their respective countries.The following graph shows the word vector distance between countries and their respective capitals.Subtracting ‚ÄúFrance‚Äù from ‚ÄúParis‚Äù removes the country semantics, leaving a vector representing the concept of a capital city. Adding ‚ÄúGermany‚Äù to this vector, we are left with something closely resembling ‚ÄúBerlin,‚Äù the capital of Germany. The vectors for this relationship are shown in the following graph.For our use case, we use the pre-trained Cohere Embeddings model in Amazon Bedrock, which embeds entire texts rather than a single word. The embeddings represent the meaning of the text and can be operated on using mathematical operations. This property can be useful to map relationships such as similarity between texts.One way in which we use language embeddings is by using their properties to calculate how similar an article is to one of the topics.To do this, we break down a topic into a series of different and related embeddings. For instance, for culture, we have a set of embeddings for sports, TV programs, music, books, and so on. We then embed the incoming title and description of the RSS articles, and calculate the similarity against the topic embeddings. From this, we can assign topic labels to an article.The following figure illustrates how this works. The embeddings that Cohere generates are highly dimensional, containing 1,024 values (or dimensions). However, to demonstrate how this system works, we use an algorithm designed to reduce the dimensionality of the embeddings, t-distributed Stochastic Neighbor Embedding (t-SNE), so that we can view them in two dimensions. The following image uses these embeddings to visualize how topics are clustered based on similarity and meaning.You can use the embedding of an article and check the similarity of the article against the preceding embeddings. You can then say that if an article is clustered closely to one of these embeddings, it can be classified with the associated topic.This is the k-nearest neighbor (k-NN) algorithm. This algorithm is used to perform classification and regression tasks. In k-NN, you can make assumptions around a data point based on its proximity to other data points. For instance, you can say that an article that has proximity to the music topic shown in the preceding diagram can be tagged with the culture topic.The following figure demonstrates this with an ArsTechnica article. We plot against the embedding of an article‚Äôs title and description: (The climate is changing so fast that we haven‚Äôt seen how bad extreme weather could get: Decades-old statistics no longer represent what is possible in the present day).The advantage of this approach is that you can add custom, user-generated topics. You can create a topic by first creating a series of embeddings of conceptually related items. For instance, an AI topic would be similar to the embeddings for AI, Generative AI, LLM, and Anthropic, as shown in the following screenshot.In a traditional classification system, we‚Äôd be required to train a classifier‚Äîa supervised learning task where we‚Äôd need to provide a series of examples to establish whether an article belongs to its respective topic. Doing so can be quite an intensive task, requiring labeled data and training the model. For our use case, we can provide examples, create a cluster, and tag articles without having to provide labeled examples or train additional models. This is shown in the following screenshot of results page of our website.In our application, we ingest new articles on a schedule. We use EventBridge schedules to periodically call a Lambda function, which checks if there are new articles. If there are, it creates an embedding from them using Amazon Bedrock and Cohere.We calculate the article‚Äôs distance to the different topic embeddings, and can then determine whether the article belongs to that category. This is done with Aurora PostgreSQL with pgvector. We store the embeddings of the topics and then calculate their distance using the following SQL query:const topics = await sqlClient.then(it=> it.query(
    `SELECT name, embedding_description, similarity
     FROM (SELECT topic_id as name, embedding_description, (1- ABS( 1 ‚Äì(embed.embedding <-> $1))) AS "similarity" FROM topic_embedding_link embed)  topics
     ORDER BY similarity desc`,
    [toSql(articleEmbedding)]
  ))
The <-> operator in the preceding code calculates the Euclidean distance between the article and the topic embedding. This number allows us to understand how close an article is to one of the topics. We can then determine the appropriateness of a topic based on this ranking.We then tag the article with the topic. We do this so that the subsequent request for a topic is as computationally as light as possible; we do a simple join rather than calculating the Euclidean distance.const formattedTopicInsert = pgformat(
    `INSERT INTO feed_article_topic_link(topic_id, feed_article_id) VALUES %L ON CONFLICT DO NOTHING`,
    topicLinks
  )We also cache a specific topic/feed combination because these are calculated hourly and aren‚Äôt expected to change in the interim.As previously discussed, the embeddings produced by Cohere contain a multitude of features; they embed the meanings and semantics of a word of phrase. We‚Äôve also found that we can perform mathematical operations on these embeddings to do things such as calculate the similarity between two phrases or words.We can use these embeddings and calculate the similarity between a search term and an embedding of an article with the k-NN algorithm to find articles that have similar semantics and meanings to the search term we‚Äôve provided.For example, in one of our RSS feeds, we have a lot of different articles that rate products. In a traditional search system, we‚Äôd rely on keyword matches to provide relevant results. Although it might be simple to find a specific article (for example, by searching ‚Äúbest digital notebooks‚Äù), we would need a different method to capture multiple product list articles.In a semantic search system, we first transform the term ‚ÄúProduct list‚Äù in an embedding. We can then use the properties of this embedding to perform a search within our embedding space. Using the k-NN algorithm, we can find articles that are semantically similar. As shown in the following screenshot, despite not containing the text ‚ÄúProduct list‚Äù in either the title or description, we‚Äôve been able to find articles that contain a product list. This is because we were able to capture the semantics of the query and match it to the existing embeddings we have for each article.In our application, we store these embeddings using pgvector on Aurora PostgreSQL. pgvector is an open source extension that enables vector similarity search in PostgreSQL. We transform our search term into an embedding using Amazon Bedrock and Cohere v3 Embed.After we‚Äôve converted the search term to an embedding, we can compare it with the embeddings on the article that have been saved during the ingestion process. We can then use pgvector to find articles that are clustered together. The SQL code for that is as follows:SELECT *
FROM (
    SELECT feed_articles.id as id, title, feed_articles.feed_id as feed, feedName, slug, description, url, author, image, published_at as published, 1 - ABS(1 - (embedding <-> $2)) AS "similarity"
    FROM feed_articles
    INNER JOIN (select feed_id, name as feedName from feed_user_subscription fus where fus.user_id=$1) sub on feed_articles.feed_id=sub.feed_id
    ${feedId != undefined ? `WHERE feed_articles.feed_id = $4` : ""}
)
WHERE similarity > 0.95
ORDER BY similarity desc
LIMIT $3;
This code calculates the distance between the topics, and the embedding of this article as ‚Äúsimilarity.‚Äù If this distance is close, then we can assume that the topic of the article is related, and we therefore attach the topic to the article.To deploy this application in your own account, you need the following prerequisites:Model access for Cohere Embed English. On the Amazon Bedrock console, choose  in the navigation pane, then choose . Select the FMs of your choice and request access.When the prerequisite steps are complete, you‚Äôre ready to set up the solution:Navigate to the solution directory:In your terminal, export your AWS credentials for a role or user in ACCOUNT_ID. The role needs to have all necessary permissions for AWS CDK deployment: 
  export AWS_REGION=‚Äù<region>‚Äù ‚Äì The AWS Region you want to deploy the application toexport AWS_ACCESS_KEY_ID=‚Äù<access-key>‚Äù ‚Äì The access key of your role or userexport AWS_SECRET_ACCESS_KEY=‚Äù<secret-key>‚Äù ‚Äì The secret key of your role or userIf you‚Äôre deploying the AWS CDK for the first time, run the following command:To synthesize the AWS CloudFormation template, run the following command:cdk synth -c vpc_id=<ID Of your VPC>To deploy, use the following command:cdk deploy -c vpc_id=<ID Of your VPC>When deployment is finished, you can check these deployed stacks by visiting the AWS CloudFormation console, as shown in the following screenshot.Run the following command in the terminal to delete the CloudFormation stack provisioned using the AWS CDK:In this post, we explored what language embeddings are and how they can be used to enhance your application. We‚Äôve learned how, by using the properties of embeddings, we can implement a real-time zero-shot classifier and can add powerful features such as semantic search.The code for this application can be found on the accompanying GitHub repo. We encourage you to experiment with language embeddings and find out what powerful features they can enable for your applications!is a Solutions Architect based in Amsterdam, the Netherlands. He has a background in software engineering. At AWS, Thomas helps customers build cloud solutions, focusing on modernization, data, and integrations.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer ‚Äî Part 2: The Data Sets</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:29:39 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[In¬†Part 1, we discussed the importance of collecting good image data and assigning proper labels for your Image Classification project to be successful. Also, we talked about classes and sub-classes of your data. These may seem pretty straight forward concepts, but it‚Äôs important to have a solid understanding going forward. So, if you haven‚Äôt, please check it out.Now we will discuss how to build the various data sets and the techniques that have worked well for my application. Then in the next part, we will dive into the evaluation of your models, beyond simple accuracy.I will again use the example zoo animals image classification app.As machine learning engineers, we are all familiar with the train-validation-test sets, but when we include the concept of sub-classes discussed in Part 1, and incorporate to concepts discussed below to set a minimum and maximum image count per class, as well as staged and synthetic data to the mix, the process gets a bit more complicated. I had to create a custom script to handle these options.I will walk you through these concepts before we split the data for training:¬†‚Äî Too few images and your model performance will suffer. Too many and you spend more time training than it‚Äôs worth.¬†‚Äî Your model indicates how confident it is in the predictions. Let‚Äôs use that to decide when to present results to the user.¬†‚Äî Real-world data is messy and the benchmark sets should reflect that. These need to stretch the model to the limit and help us decide when it is ready for production.Staged and synthetic data¬†‚Äî Real-world data is king, but sometimes you need to produce the your own or even generate data to get off the ground. Be careful it doesn‚Äôt hurt performance.¬†‚Äî Repeat data can skew your results and give you a false sense of performance. Make sure your data is diverse.¬†‚Äî Combine sub-classes, apply cutoffs, and create your train-validation-test sets. Now we are ready to get the show started.In my experience, using a minimum of 40 images per class provides descent performance. Since I like to use 10% each for the test set and validation set, that means at least 4 images will be used to check the training set, which feels just barely adequate. Using fewer than 40 images per class, I notice my model evaluation tends to suffer.On the other end, I set a maximum of about 125 images per class. I have found that the performance gains tend to plateau beyond this, so having more data will slow down the training run with little to show for it. Having more than the maximum is fine, and these ‚Äúoverflow‚Äù can be added to the test set, so they don‚Äôt go to waste.There are times when I will drop the minimum cutoff to, say 35, with no intention of moving the trained model to production. Instead, the purpose is to leverage this throw-away model to find more images from my unlabelled set. This is a technique that I will go into more detail in Part 3.You are likely familiar with the softmax score. As a reminder, softmax is the probability assigned to each label. I like to think of it as a confidence score, and we are interested in the class that receives the highest confidence. Softmax is a value between zero and one, but I find it easier to interpret confidence scores between zero and 100, like a percentage.In order to decide if the model is confident enough with its prediction, I have chosen a threshold of 95. I use this threshold when determining if I want to present results to the user.Scores above the threshold have a better changes of being right, so I can confidently provide the results. Scores below the threshold may not be right ‚Äî in fact it could be ‚Äúout-of-scope‚Äù, meaning it‚Äôs something the model doesn‚Äôt know how to identify. So, instead of taking the risk of presenting incorrect results, I instead prompt the user to try again and offer suggestions on how to take a ‚Äúgood‚Äù picture.Admittedly this is somewhat arbitrary cutoff, and you should decide for your use-case what is appropriate. In fact, this score could probably be adjusted for each trained model, but this would make it harder to compare performance across models.I will refer to this confidence score frequently in the evaluations section in Part 3.Let me introduce what I call the benchmark sets, which you can think of as extended test sets. These are hand-picked images designed to stretch the limits of your model, and provide a measure for specific classes of your data. Use these benchmarks to justify moving your model to production, and for an objective measure to show to your manager.¬†‚Äî These are the ‚Äúextra credit‚Äù images, like the bonus questions a professor would add to the quiz to see which students are paying attention. You need a keen eye to spot the difference between the ground truth and a similar looking class. For example, a cheetah sleeping in the shade that could pass as a leopard if you don‚Äôt look closely.¬†‚Äî These are the ‚Äútrick question‚Äù images. Our model is trained on zoo animals, but people are known for not following the rules. For example, a zoo guest takes a picture of their child wearing cheetah face paint.¬†‚Äî These are your ‚Äúbread and butter‚Äù classes that need to get near perfect scores and zero errors. This would be a make-or-break benchmark for moving to production.¬†‚Äî These are your ‚Äúrare but exceptional‚Äù classes that again need to be correct, but reach a minimum score like the confidence threshold.When looking for images to add to the benchmarks, you can likely find them in real-world images from your deployed model. See the evaluation in Part 3.For each benchmark, calculate the min, max, median, and mean scores, and also how many images get scores above and below the confidence threshold. Now you can compare these measures against what is currently in production, and against your minimum requirements, to help decide if the new model is production worthy.Perhaps the biggest hurdle to any supervised machine learning application is having data to train the model. Clearly, ‚Äúreal-world‚Äù data that comes from actual users of the application is ideal. However you can‚Äôt really collect these until the model is deployed. Chicken and egg problem.One way to get started to is to have volunteers collect ‚Äústaged‚Äù images for you, trying to act like real users. So, let‚Äôs have our zoo staff go around taking pictures of the animals. This is a good start, but there will be a certain level of bias introduced in these images. For example, the staff may take the photos over a few days, so you may not get the year-round weather conditions.Another way to get pictures is use computer-generated ‚Äúsynthetic‚Äù images. I would avoid these at all costs, to be honest. Based on my experience, the model struggles with these because they look‚Ä¶different. The lighting is not natural, the subject may superimposed on a background and so the edges look too sharp, etc. Granted, some of the AI generated images look very realistic, but if you look closely you may spot something unusual. The neural network in your model will notice these, so be careful.The way that I handle these staged or synthetic images is as a sub-class that gets merged into the training set, but only¬†¬†giving preference to the real-world images. I cap the number of staged images to 60, so if I have 10 real-world, I now only need 50 staged. Eventually, these staged and synthetic images are phased out completely, and I rely entirely on real-world.One problem that can creep into your image set are duplicate images. These can be exact copies of pictures, or they can be extremely similar. You may think that this is harmless, but imagine having 100 pictures of an elephant that are exactly the same ‚Äî your model will not know what to do with a different angle of the elephant.Now, let‚Äôs say you have only¬†¬†pictures that are nearly the same. Not so bad, right? Well, here is what can happen to them:Both pictures go in the training set ‚Äî The model doesn‚Äôt learn anything from the repeated image and it wastes time processing them.One goes into the training set, the other goes into the test set ‚Äî Your test score will be higher, but it is not an accurate evaluation.Both are in the test set ‚Äî Your test score will be compounded either higher or lower than it should be.None of these will help your model.There are a few ways to find duplicates. The approach I have taken is to calculate a hamming distance on all the pictures and identify the ones that are very close. I have an interface that displays the duplicates and I decide which one I like best, and remove the other.Another way (I haven‚Äôt tried this yet) is to create a vector representation of your images. Store these a vector database, and you can do a similarity search to find nearly identical images.Whatever method you use, it is important to clean up the duplicates.Now we are ready to build the traditional training, validation, and test sets. This is no longer a straight forward task since I want to:Merge sub-classes into a main class.Prioritize real-world images over staged or synthetic images.Apply a minimum number of images per class.Apply a maximum number of images per class, sending the ‚Äúoverflow‚Äù to the test set.This process is somewhat complicated and depends on how you manage your image library. First, I would recommend keeping your images in a folder structure that has sub-class folders. You can get image counts by using a script to simply read the folders. Second is to keep a configuration of how the sub-classes are merged. To really set yourself up for success, put these image counts and merge rules in a database for faster lookups.My train-validation-test set splits are usually 90‚Äì10‚Äì0. I originally started out using 80‚Äì10‚Äì10, but with diligence on keeping the entire data set clean, I noticed validation and test scores became pretty even. This allowed me to increase the training set size, and use ‚Äúoverflow‚Äù to become the test set, as well as using the benchmark sets.In this part, we‚Äôve built our data sets by merging sub-classes and using the image count cutoffs. Plus we handle staged and synthetic data as well as cleaning up duplicate images. We also created benchmark sets and defined confidence thresholds, which help us decide when to move a model to production.In¬†Part 3, we will discuss how we are going to evaluate the different model performances. And then finally we will get to the actual model training and the techniques to enhance accuracy.]]></content:encoded></item><item><title>Coding Interviews were HARD Until I Learned These 20 Tips</title><link>https://blog.algomaster.io/p/20-coding-interviews-tips</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/61c3f6c0-4027-4d37-b4a7-a30fc183fa12_1602x1032.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:30:27 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[I gave my first  in 2016‚Äîand failed. I failed the next five interviews as well before finally landing my first job at .Since then, I‚Äôve interviewed with many companies and faced my fair share of rejections. However, over the years, my failure rate in coding interviews dropped significantly.By 2022, with just 1.5 months of focused preparation, I successfully cleared interviews at  and .Surprisingly, my success wasn‚Äôt due to a dramatic improvement in problem-solving skills. The real game-changer was my approach‚Äî and  during the interview.In this article, I‚Äôll share  that made coding interviews significantly easier for me.These tips cover everything you need to know, including:How to systematically approach coding interview problemsKey concepts and patterns you should knowThe type of problems you should practiceHow to choose the right algorithm for a given problemTechniques to optimize your solutionHow to communicate your thought process effectivelyBy applying these strategies, you‚Äôll be able to tackle coding interviews with confidence and massively increase your chances of success.In a coding interview, interviewers want to see how well you , , and  under pressure.Here's a breakdown of what they look for:Understanding the problem: Do you ask clarifying questions instead of making assumptions to ensure you fully understand the problem?: Can you decompose the problem into smaller, manageable parts?: Can you design an optimal solution in terms of time and space complexity?: Do you handle edge cases like empty inputs, duplicates, large values, or special conditions?: Can you explain why one approach is better than another?: Do you have a strong grasp of data structures and algorithms, and can you choose the right one for the problem?Can you quickly compute the time and space complexity of your solution?Explaining your thought process: Can you clearly articulate your approach and why it works?: Are you receptive to hints and able to adjust your approach accordingly?: Do you follow good coding practices (meaningful variable names, proper indentation, modular functions etc..)?Improving the initial solution: Can you optimize and refine your first solution when prompted?Are you able to tackle variations of the original problem?Can you manually walk through your code with sample inputs to verify correctness?Most coding interviews last Depending on the company and interviewer, you may be asked to solve 2-3easy/medium problems or 1 hard problem with follow-ups.Lets assume you are given one problem, with a follow up in a 45-minute interview. Here‚Äôs how you can optimally allocate your time:The interviewer may ask you to introduce yourself. Prepare a concise 1-2 minute introduction that highlights your background, experience, and key strengths. Practice it beforehand so that you can deliver it smoothly.Understand the Problem (5-10 mins):  Carefully read the problem statement, ask clarifying questions, and walk through sample inputs and expected outputs.Plan the Approach (10-20 mins): Brainstorm possible solutions, evaluate trade-offs, and discuss time and space complexity.Implement the Code (20-30 mins): Write a clean, modular and readable code.Dry-run your code with sample inputs, debug any issues, and ensure edge cases are handled.Follow-ups and Wrap Up (35-45 mins): Answer follow up questions, and ask thoughtful questions to the interviewer about the company, role, or team.One of the biggest mistakes candidates make in coding interviews is jumping into coding too soon.If you don't fully understand the question, you might end up solving the Here‚Äôs how to ensure you grasp the problem before coding:Read the Problem CarefullyTake a moment to absorb the problem statement. Rephrase it in your own words to confirm your understanding. Identify the expected input/output format and any hidden constraints.If anything is unclear, ask questions before diving into the solution. Interviewers appreciate when you seek clarity. Never assume details that aren‚Äôt explicitly mentioned in the problem statement.Common clarifications include:Are there duplicate values?Can the input be empty? If so, what should the output be?Should the solution handle negative numbers?Should the output maintain the original order of elements?Is the graph directed or undirected?Does the input contain only lowercase English letters, or can it have uppercase, digits, or special characters?What should happen if multiple solutions exist? Should I return any valid solution, or does the problem have specific requirements?Walk Through Input/Output ExamplesOnce you understand the problem statement and constraints, go over a few input and output examples to make sure you get it.Draw them out if it helps, especially for visual data structures like trees or graphs.Try to take examples that cover different scenarios of the problem. Think about any  that might come up.]]></content:encoded></item><item><title>Looking back at our Bug Bounty program in 2024</title><link>https://engineering.fb.com/2025/02/13/security/looking-back-at-our-bug-bounty-program-in-2024/</link><author></author><category>dev</category><category>official</category><pubDate>Thu, 13 Feb 2025 17:00:46 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[Ads audience tools designed to help people choose a target audience for their ads: Mixed reality hardware products:Organizing community events and presenting joint research:Providing resources and timely updates for the research community:]]></content:encoded></item><item><title>Python vs R for data science: Which should you choose?</title><link>https://www.datasciencecentral.com/python-vs-r-for-data-science-which-should-you-choose/</link><author>Mike Steven</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 15:37:52 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Welcome to another comparison article where you will understand the features, intricacies, pros, and cons of two different stacks of the information technology industry. Today‚Äôs comparison blog is especially for data scientists who spend their day and night with datasets, insights, trends, and analysis of many other factors. From a long list of skills that‚Ä¶¬†Read More ¬ª]]></content:encoded></item><item><title>A new tool for visualizing Rust lifetimes</title><link>https://www.youtube.com/watch?v=NV6Xo_el_2o</link><author>Let&apos;s Get Rusty</author><category>dev</category><category>rust</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/NV6Xo_el_2o?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 15:00:34 +0000</pubDate><source url="https://www.youtube.com/channel/UCSp-OaMpsO8K0KkOqyBl7_w">Let&apos;s get Rusty</source><content:encoded><![CDATA[See how RustOwl can help you understand lifetimes in a real Rust codebase. A brand-new tool designed to visualize Rust lifetimes and make learning Rust easier. Check it out and see how it can change the way you write Rust!

Free Rust training: https://letsgetrusty.com/bootcamp

RustOwl: https://github.com/cordx56/rustowl

Corrections:
- Bacon is a CLI tool, not a library. Check it out here: https://github.com/Canop/bacon]]></content:encoded></item><item><title>Bridging the Gap: Democratizing AI for All</title><link>https://www.kdnuggets.com/bridging-gap-democratizing-ai</link><author>Vidhi Chugh</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/chugh_Bridging-the-Gap-Democratizing-AI-for-All_1.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 15:00:03 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Let‚Äôs explore how democratizing AI can level the playing field and create opportunities for all, no matter the background or resources.]]></content:encoded></item><item><title>Rust vs C++ Performance</title><link>https://www.youtube.com/watch?v=WnMin9cf78g</link><author>Anton Putra</author><category>dev</category><category>video</category><enclosure url="https://www.youtube.com/v/WnMin9cf78g?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 14:30:41 +0000</pubDate><source url="https://www.youtube.com/channel/UCeLvlbC754U6FyFQbKc0UnQ">Anton Putra</source><content:encoded><![CDATA[C++ vs Rust Speed.

üî¥ To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)

üçø Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&si=p-UOaVM_6_SFx52H

üëã AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra

‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Experience & Location üíº ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨
‚ñ∫  I‚Äôm a Senior Software Engineer at Juniper Networks (13+ years of experience)
‚ñ∫  Located in San Francisco Bay Area, CA (US citizen)

‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Connect with me üëã ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨
‚ñ∫  LinkedIn: https://www.linkedin.com/in/anton-putra
‚ñ∫  Twitter/X: https://twitter.com/antonvputra
‚ñ∫  GitHub: https://github.com/antonputra
‚ñ∫  Email: me@antonputra.com

‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Source Code üìö ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨
‚ñ∫ Original Source Code: https://github.com/antonputra/tutorials/tree/245/lessons/245

PR to improve Rust - https://github.com/antonputra/tutorials/pull/429
PR to improve Rust - https://github.com/antonputra/tutorials/pull/431
PR to improve Rust - https://github.com/antonputra/tutorials/pull/433

#rust #golang #devops]]></content:encoded></item><item><title>How to Scale Sklearn with Dask</title><link>https://www.kdnuggets.com/how-to-scale-sklearn-dask</link><author>Iv√°n Palomares Carrascosa</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/crVYYGyvTE2Jilsmvukhpw.jpeg" length="" type=""/><pubDate>Thu, 13 Feb 2025 13:00:49 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Here's how Dask applies the building blocks of sklearn to bring ML modeling workflows to the next level of scalability via high-performance parallel computing]]></content:encoded></item><item><title>Peter Bengtsson: get in JavaScript is the same as property in Python</title><link>http://www.peterbe.com/plog/get-in-javascript-is-the-same-as-property-in-python</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 12:41:56 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Prefix a function, in an object or class, with `get` and then that acts as a function call without brackets. Just like Python's `property` decorator.]]></content:encoded></item></channel></rss>