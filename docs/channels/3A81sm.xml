<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech</title><link>https://www.awesome-dev.news</link><description></description><item><title>China Successfully Tests Hypersonic Aircraft, Maybe At Mach 12</title><link>https://tech.slashdot.org/story/25/07/01/2222223/china-successfully-tests-hypersonic-aircraft-maybe-at-mach-12?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China's Northwestern Polytechnical University successfully tested a hypersonic aircraft called Feitian-2, claiming it reached Mach 12 and achieved a world-first by autonomously switching between rocket and ramjet propulsion mid-flight. The Register reports: The University named the craft "Feitian-2" and according to Chinese media the test flight saw it reach Mach 12 (14,800 km/h or 9,200 mph) -- handily faster than the Mach 5 speeds considered to represent hypersonic flight. Chinese media have not detailed the size of Feitian-2, or its capabilities other than to repeat the University's claim that it combined a rocket and a ramjet into a single unit. [...] The University and Chinese media claim the Feitian-2 flew autonomously while changing from rocket to ramjet while handling the hellish stresses that come with high speed flight.
 
This test matters because, as the US Congressional Budget Office found in 2023, hypothetical hypersonic missiles "have the potential to create uncertainty about what their ultimate target is. Their low flight profile puts them below the horizon for long-range radar and makes them difficult to track, and their ability to maneuver while gliding makes their path unpredictable." "Hypersonic weapons can also maneuver unpredictably at high speeds to counter short-range defenses near a target, making it harder to track and intercept them," the Office found.
 
Washington is so worried about Beijing developing hypersonic weapons that the Trump administration cited the possibility as one reason for banning another 27 Chinese organizations from doing business with US suppliers of AI and advanced computing tech. The flight of Feitian-2 was therefore a further demonstration of China's ability to develop advanced technologies despite US bans.]]></content:encoded></item><item><title>Podcast Episode: Cryptography Makes a Post-Quantum Leap</title><link>https://www.eff.org/deeplinks/2025/06/podcast-episode-cryptography-makes-post-quantum-leap</link><author>Josh Richman</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/2025-htfi-deirdre-blog.png" length="" type=""/><pubDate>Wed, 2 Jul 2025 07:05:17 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[ I only got into cryptography and especially post quantum quickly after that. further into my professional life. I was a software engineer for a whil,e and the Snowden leaks happened, and phone records get leaked. All of Verizon's phone records get leaked. and then Prism and more leaks and more leaks. And as an engineer first, I felt like everything that I was building and we were building and telling people to use was vulnerable. I wanted to learn more about how to do things securely. I went further and further and further down the rabbit hole of cryptography. And then, I think I saw a talk which was basically like, oh, elliptic curves are vulnerable to a quantum attack. And I was like, well, I, I really like these things. They're very elegant mathematical objects, it's very beautiful. I was sad that they were fundamentally broken, and, I think it was, Dan Bernstein who was like, well, there's this new thing that uses elliptic curves, but is supposed to be post quantum secure. But the math is very difficult and no one understands it. I was like, well, I want to understand it if it preserves my beautiful elliptic curves. That's how I just went, just running, screaming downhill into post quantum cryptography. That's Deirdre Connolly talking about how her love of beautiful math and her anger at the Snowden revelations about how the government was undermining security, led her to the world of post-quantum cryptography.I'm Cindy Cohn, the executive director of the Electronic Frontier Foundation. And I'm Jason Kelley, EFF's activism director. You're listening to How to Fix the Internet. On this show we talk to tech leaders, policy-makers, thinkers, artists and engineers about what the future could look like if we get things right online. Our guest today is at the forefront of the future of digital security. And just a heads up that this is one of the more technical episodes that we've recorded -- you'll hear quite a bit of cryptography jargon, so we've written up some of the terms that come up in the show notes, so take a look there if you hear a term you don't recognize. Deidre Connolly is a research engineer and applied cryptographer at Sandbox AQ, with a particular expertise in post-quantum encryption. She also co-hosts the Security, Cryptography, Whatever podcast, so she's something of a cryptography influencer too. When we asked our tech team here at EFF who we should be speaking with on this episode about quantum cryptography and quantum computers more generally, everyone agreed that Deirdre was the person. So we're very glad to have you here. Welcome, Deirdre. Thank you very much for having me. Hi. Now we obviously work with a lot of technologists here and, and certainly personally cryptography is near and dear to my heart, but we are not technologists, neither Jason nor I. So can you just give us a baseline of what post-quantum cryptography is and why people are talking about it? Sure. So a lot of the cryptography that we have deployed in the real world relies on a lot of math and security assumptions on that math based on things like abstract groups, Diffie-Hellman, elliptic curves, finite fields, and factoring prime numbers such as, uh, systems like RSA. All of these, constructions and problems, mathematical problems, have served us very well in the last 40-ish years of cryptography. They've let us build very useful, efficient, small cryptography that we've deployed in the real world. It turns out that they are all also vulnerable in the same way to advanced cryptographic attacks that are only possible and only efficient when run on a quantum computer, and this is a class of computation, a whole new class of computation versus digital computers, which is the main computing paradigm that we've been used to for the last 75 years plus. Quantum computers allow these new classes of attacks, especially, variants of Shore's algorithm – named Dr. Peter Shore – that basically when run on a sufficiently large, cryptographically relevant quantum computer, makes all of the asymmetric cryptography based on these problems that we've deployed very, very vulnerable. So post-quantum cryptography is trying to take that class of attack into consideration and building cryptography to both replace what we've already deployed and make it resilient to this kind of attack, and trying to see what else we can do with these fundamentally different mathematical and cryptographic assumptions when building cryptography. So we've kind of, we've secured our stuff behind a whole lot of walls, and we're slowly building a bulldozer. This is a particular piece of the world where the speed at which computers can do things has been part of our protection, and so we have to rethink that. Yeah, quantum computing is a fundamentally new paradigm of how we process data that promises to have very interesting, uh, and like, applications beyond what we can envision right now. Like things like protein folding, chemical analysis, nuclear simulation, and cryptanalysts, or very strong attacks against cryptography.But it is a field where it's such a fundamentally new computational paradigm that we don't even know what its applications fully would be yet, because like we didn't fully know what we were doing with digital computers in the forties and fifties. Like they were big calculators at one time. When it was suggested that we talk to you about this. I admit that I have not heard much about this field, and I realized quickly when looking into it that there's sort of a ton of hype around quantum computing and post-quantum cryptography and that kind of hype can make it hard to know whether or not something is like actually going to be a big thing or, whether this is something that's becoming like an investment cycle, like a lot of things do. And one of the things that quickly came up as an actual, like real danger is what's called sort of “save now decrypt later.” Oh yeah. Right? We have all these messages, for example, that have been encrypted with current encryption methods. And if someone holds onto those, they can decrypt them using quantum computers in the future. How serious is that danger? It’s definitely a concern and it's the number one driver I would say to post-quantum crypto adoption in broad industry right now is mitigating the threat of a Store Now/Decrypt Later attack, also known as Harvest Now/Decrypt Later, a bunch of names that all mean the same thing.And fundamentally, it's, uh, especially if you're doing any kind of key agreement over a public channel, and doing key agreement over a public channel is part of the whole purpose of like, you want to be able to talk to someone who you've never really, touched base with before, and you all kind of know, some public parameters that even your adversary knows and based on just the fact that you can send messages to each other and some public parameters, and some secret values that only you know, and only the other party knows you can establish a shared secret, and then you can start encrypting traffic between you to communicate. And this is what you do in your web browser when you have an HTTPS connection, that's over TLS.This is what you do with Signal or WhatsApp or any, or, you know, Facebook Messenger with the encrypted communications. They're using Diffie-Helman as part of the protocol to set up a shared secret, and then you use that to encrypt their message bodies that you're sending back and forth between you.But if you can just store all those communications over that public channel, and the adversary knows the public parameters 'cause they're freely published, that's part of Kerckhoff’s Principle about good cryptography - the only thing that the adversary shouldn't know about your crypto system is the secret key values that you're actually using. It should be secure against an adversary that knows everything that you know, except the secret key material. And you can just record all those public messages and all the public key exchange messages, and you just store them in a big database somewhere. And then when you have your large cryptographically relevant quantum computer, you can rifle through your files and say, hmm, let's point it at this.And that's the threat that's live now to the stuff that we have already deployed and the stuff that we're continuing to do communications on now that is protected by elliptic curve Diffie Hellman, or Finite Field Diffie Hellman, or RSA. They can just record that and just theoretically point an attack at it at a later date when that attack comes online. So like in TLS, there's a lot of browsers and servers and infrastructure providers that have updated to post-quantum resilient solutions for TLS. So they're using a combination of the classic elliptic curve, Diffie Hellman and a post-quantum KEM, uh, called ML Kem that was standardized by the United States based on a public design that's been, you know, a multi international collaboration to help do this design. I think that's been deployed in Chrome, and I think it's deployed by CloudFlare and it's getting deployed – I think it's now become the default option in the latest version of Open SSL. And a lot of other open source projects, so that's TLS similar, approaches are being adopted in open SSH, the most popular SSH implementation in the world. Signal, the service has updated their key exchange to also include a post quantum KEM and their updated key establishments. So when you start a new conversation with someone or reset a conversation with someone that is the latest version of Signal is now protected against that sort of attack. That is definitely happening and it's happening the most rapidly because of that Store now/Decrypt later attack, which is considered live. Everything that we're doing now can just be recorded and then later when the attack comes online, they can attack us retroactively. So that's definitely a big driver of things changing in the wild right now. Okay. I'm going to throw out two parallels for my very limited knowledge to make sure I understand. This reminds me a little bit of sort of the work that had to be done before Y2K in, in the sense of like, now people think nothing went wrong and nothing was ever gonna go wrong, but all of us working anywhere near the field know actually it took a ton of work to make sure that nothing blew up or stopped working. And the other is that in, I think it was 1998, EFF was involved in something we called Deep Crack, where we made, that's a, I'm realizing now that's a terrible name. But anyway, the DES cracker, um, we basically wanted to show that DES was capable of being cracked, right? And that this was a - correct me if I'm wrong - it was some sort of cryptographic standard that the government was using and people wanted to show that it wasn't sufficient. Yes - I think it was the first digital encryption standard. And then after its vulnerability was shown, they, they tripled it up to, to make it useful. And that's why Triple DES is still used in a lot of places and is actually considered okay. And then later came the advanced encryption standard, AES, which we prefer today. Okay, so we've learned the lesson, or we are learning the lesson, it sounds like. Yeah, I think that that's, that's right. I mean, EFF built the DES cracker because in the nineties the government was insisting that something that everybody knew was really, really insecure and was going to only get worse as computers got stronger and, and strong computers got in more people's hands, um, to basically show that the emperor had no clothes, um, that this wasn't very good. And I think with the NIST standards and what's happening with post-quantum is really, you know, the hopeful version is we learned that lesson and we're not seeing government trying to pretend like there isn't a risk in order to preserve old standards, but instead leading the way with new ones. Is that fair? That is very fair. NIST ran this post-quantum competition almost over 10 years, and it had over 80 submissions in the first round from all over the world, from industry, academia, and a mix of everything in between, and then it narrowed it down to. the three that are, they're not all out yet, but there's the key agreement, one called ML Kem, and three signatures. And there's a mix of cryptographic problems that they're based on, but there were multiple rounds, lots of feedback, lots of things got broken. This competition has absolutely led the way for the world of getting ready for post-quantum cryptography. There are some competitions that have happened in Korea, and I think there's some work happening in China for their, you know, for their area.There are other open standards and there are standards happening in other standards bodies, but the NIST competition has led the way, and it, because it's all open and all these standards are open and all of the work and the cryptanalysis that has gone in for the whole stretch. It's all been public and all these standards and drafts and analysis and attacks have been public. It's able to benefit everyone in the world. I got started in the crypto wars in the nineties where the government was kind of the problem and they still are. And I do wanna ask you about whether you're seeing any role of the kinda national social security, FBI infrastructure, which has traditionally tried to put a thumb on the scales and make things less secure so that they could have access, if you're seeing any of that there. But on the NIST side, I think this provides a nice counter example of how government can help facilitate building a better world sometimes, as opposed to being the thing we have to drag kicking and screaming into it.But let me circle around to the question I embedded in that, which is, you know, one of the problems that that, that we know happened in the nineties around DES, and then of course some of the Snowden revelations indicated some mucking about in security as well behind the scenes by the NSA. Are you seeing anything like that and, and what should we be on the lookout for? Not in the PQC stuff. Uh, there, like there have been a lot of people that were paying very close attention to what these independent teams were proposing and then what was getting turned into a standard or a proposed standard and every little change, because I, I was closely following the key establishment stuff.Um, every little change people were trying to be like, did you tweak? Why did you tweak that? Did, like, is there a good reason? And like, running down basically all of those things. And like including trying to get into the nitty gritty of like. Okay. We think this is approximately these many bits of security using these parameter and like talking about, I dunno, 123 versus 128 bits and like really paying attention to all of that stuff.And I don't think there was any evidence of anything like that. And, and for, for plus or minus, because there were. I don't remember which crypto scheme it was, but it, there was definitely an improvement from, I think some of the folks at NSA very quietly back in the day to, I think it was the S boxes, and I don't remember if it was DES or AES or whatever it was.But people didn't understand at the time because it was related to advanced, uh, I think it was a differential crypto analysis attacks that folks inside there knew about, and people in outside academia didn't quite know about yet. And then after the fact they were like, oh, they've made this better. Um, we're not, we're not even seeing any evidence of anything of that character either.It's just sort of like, it's very open letting, like if everything's proceeding well and the products are going well of these post-quantum standards, like, you know, leave it alone. And so everything looks good. And like, especially for NSA, uh, national Security Systems in the, in the United States, they have updated their own targets to migrate to post-quantum, and they are relying fully on the highest security level of these new standards.So like they are eating their own dog food. They're protecting the highest classified systems and saying these need to be fully migrated to fully post quantum key agreement. Uh, and I think signatures at different times, but there has to be by like 2035. So if they were doing anything to kind of twiddle with those standards, they'd be, you know, hurting themselves and shooting themselves in the foot. Well fingers crossed. Because I wanna build a better internet and a better. Internet means that they aren't secretly messing around with our security. And so this is, you know, cautiously good news. Let's take a quick moment to thank our sponsor.“How to Fix the Internet” is supported by The Alfred P. Sloan Foundation’s Program in Public Understanding of Science and Technology. Enriching people’s lives through a keener appreciation of our increasingly technological world and portraying the complex humanity of scientists, engineers, and mathematicians.We also want to thank EFF members and donors. EFF has been fighting for digital rights for 35 years, and that fight is bigger than ever, so please, if you like what we do, go to eff.org/pod to donate. Also, we’d love for you to join us at this year’s EFF awards, where we celebrate the people working towards the better digital future that we all care so much about. Those are coming up on September 12th in San Francisco. You can find more information about that at eff.org/awards.We also wanted to share that our friend Cory Doctorow has a new podcast. Listen to this.  [Who Broke the Internet trailer] And now, back to our conversation with Deirdre Connolly. I think the thing that's fascinating about this is kind of seeing this cat and mouse game about the ability to break codes, and the ability to build codes and systems that are resistant to the breaking, kind of playing out here in the context of building better computers for everyone.And I think it's really fascinating and I think it also for people I. You know, this is a pretty technical conversation, um, even, you know, uh, for our audience. But this is the stuff that goes on under the hood of how we keep journalists safe, how we keep activists safe, how we keep us all safe, whether it's our bank accounts or our, you know, people are talking about mobile IDs now and other, you know, all sorts of sensitive documents that are going to not be in physical form anymore, but are gonna be in digital form. And unless we get this lock part right, we're really creating problems for people. And you know, what I really appreciate about you and the other people kind of in the midst of this fight is it's very unsung, right? It's kind of under the radar for the rest of us, but yet it's the, it's the ground that we need to stand on to, to be safe moving forward. Yeah, and there's a lot of assumptions, uh, that even the low level theoretical cryptographers and the people implementing their, their stuff into software and the stuff, the people trying to deploy, that there's a, a lot of assumptions that have been baked into what we've built that to a degree don't quite fit in some of the, the things we've been able to build in a post-quantum secure way, or the way we think it's a post-quantum secure way.Um, we're gonna need to change some stuff and we think we know how to change some stuff to make it work. but we are hoping that we don't accidentally introduce any vulnerabilities or gaps. We're trying, but also we're not a hundred percent sure that we're not missing something, 'cause these things are new. And so we're trying, and we're also trying to make sure we don't break things as we change them because we're trying to change them to be post quantum resilient. But you know, once you change something, if there's a possibility, you, you just didn't understand it completely. And you don't wanna break something that was working well in one direction because you wanna improve it in another direction. And that's why I think it's important to continue to have a robust community of people who are the breakers, right? Who are, are hackers, who are, who are attacking. And that is a, you know, that's a mindset, right? That's a way of thinking about stuff that is important to protect and nurture, um, because, you know, there's an old quote from Bruce Schneider: Anyone can build a crypto system that they themselves cannot break. Right? It takes a community of people trying to really pound away at something to figure out where the holes are. And you know, a lot of the work that EFF does around coders rights and other kinds of things is to make sure that there's space for that. and I think it's gonna be as needed in a quantum world as it was in a kind of classical computer world. Absolutely. I'm confident that we will learn a lot more from the breakers about this new cryptography because, like, we've tried to be robust through this, you know, NIST competition, and a lot of those, the things that we learn apply to other constructions as they come out. but like there's a whole area of people who are going to be encountering this kind of newish cryptography for the first time, and they kind of look at it and they're like. Oh, uh, I, I think I might be able to do something interesting with this, and we're, we'll all learn more and we'll try to patch and update as quickly as possible And this is why we have competitions to figure out what the best options are and why some people might favor one algorithm over another for different, different processes and things like that. And that's why we're probably gonna have a lot of different flavors of post-quantum cryptography getting deployed in the world because it's not just, ah, you know, I don't love NIST. I'm gonna do my own thing in my own country over here. Or, or have different requirements. There is that at play, but also you're trying to not put all your eggs in one basket as well. Yeah, so we want a menu of things so that people can really pick, from, you know, vetted, but different strategies. So I wanna ask the kind of core question for the podcast, which is, um, what does it look like if we get this right, if we get quantum computing and, you know, post-quantum crypto, right?How does the world look different? Or does it just look the same? How, what, what does it look like if we do this well? Hopefully to a person just using their phone or using their computer to talk to somebody on the other side of the world, hopefully they don't notice. Hopefully to them, if they're, you know, deploying a website and they're like, ah, I need to get a Let’s Encrypt certificate or whatever.Hopefully Let's Encrypt just, you know, insert bot just kind of does everything right by default and they don't have to worry about it. Um, for the builders, it should be, we have a good recommended menu of cryptography that you can use when you're deploying TLS, when you're deploying SSH, uh, when you're building cryptographic applications, especially. So like if you are building something in Go or Java or you know, whatever it might be, the crypto library in your language will have the updated recommended signature algorithm or key agreement algorithm and be, like, this is how we, you know, they have code snippets to say like, this is how you should use it, and they will deprecate the older stuff. And, like, unfortunately there's gonna be a long time where there's gonna be a mix of the new post-quantum stuff that we know how to use and know how to deploy and protect. The most important, you know, stuff like to mitigate Store now/Decrypt later and, you know, get those signatures with the most important, uh, protected stuff.Uh, get those done. But there's a lot of stuff that we're not really clear about. How we wanna do it yet, and kind of going back to one of the things you mentioned earlier, uh, comparing this to Y2K, there was a lot of work that went into mitigating Y2K before, during, immediately after.Unfortunately, the comparison to the post quantum migration kind of falls down because after Y2K, if you hadn't fixed something, it would break. And you would notice in usually an obvious way, and then you could go find it. You, you fix the most important stuff that, you know, if it broke, like you would lose billions of dollars or, you know, whatever. You'd have an outage. For cryptography, especially the stuff that's a little bit fancier. Um, you might not know it's broken because the adversary is not gonna, it's not gonna blow up.And you have to, you know, reboot a server or patch something and then, you know, redeploy. If it's gonna fail, it's gonna fail quietly. And so we're trying to kind of find these things, or at least make the kind of longer tail of stuff, uh, find fixes for that upfront, you know, so that at least the option is available. But for a regular person, hopefully they shouldn't notice. So everyone's trying really hard to make it so that the best security, in terms of the cryptography is deployed with, without downgrading your experience. We're gonna keep trying to do that.I don't wanna build crap and say “Go use it.” I want you to be able to just go about your life and use a tool that's supposed to be useful and helpful. And it's not accidentally leaking all your data to some third party service or just leaving a hole on your network for any, any actor who notices to walk through and you know, all that sort of stuff.So whether it's like implementing things securely in software, or it's cryptography or you know, post-quantum weirdness, like for me, I just wanna build good stuff for people, that's not crap. Everyone listening to this agrees with you. We don't want to build crap. We want to build some beautiful things. Let's go out there and do it. Thank you so much, Deirdre. Thank you! Thank you Deirdre. We really appreciate you coming and explaining all of this to, you know, uh, the lawyer and activist at EFF. Well, I think that was probably the most technical conversation we've had, but I followed along pretty well and I feel like at first I was very nervous based on the, save and decrypt concerns. But after we talked to Deirdre, I feel like the people working on this. Just like for Y2K are pretty much gonna keep us out of hot water. And I learned a lot more than I did know before we started the conversation. What about you, Cindy? I learned a lot as well. I mean, cryptography and, attacks on security is always, you know, it's a process, and it's a process by which we do the best we can, and then, then we also do the best we can to rip it apart and find all the holes, and then we, we iterate forward. And it's nice to hear that that model is still the model, even as we get into something like quantum computers, which, um, frankly are still hard to conceptualize. But I agree. I think that what the good news outta this interview is I feel like there's a lot of pieces in place to try to do this right, to have this tremendous shift in computing that we don't know when it's coming, but I think that the research indicates that it SI coming, be something that we can handle, um, rather than something that overwhelms us.And I think that's really,it's good to hear that good people are trying to do the right thing here since it's not inevitable. Yeah, and it is nice when someone's sort of best vision for what the future looks like is hopefully your life. You will have no impacts from this because everything will be taken care of. That's always good. I mean, it sounds like, you know, the main thing for EFF is, as you said, we have to make sure that security engineers, hackers have the resources that they need to protect us from these kinds of threats and, and other kinds of threats obviously.But, you know, that's part of EFF's job, like you mentioned. Our job is to make sure that there are people able to do this work and be protected while doing it so that when the. Solutions do come about. You know, they work and they're implemented and the average person doesn't have to know anything and isn't vulnerable. Yeah, I also think that, um, I appreciated her vision that this is a, you know, the future's gonna be not just one. Size fits all solution, but a menu of things that take into account, you know, both what works better in terms of, you know, bandwidth and compute time, but also what you know, what people actually need.And I think that's a piece that's kind of built into the way that this is happening that's also really hopeful. In the past and, and I was around when EFF built the DES cracker, um, you know, we had a government that was saying, you know, you know, everything's fine, everything's fine when everybody knew that things weren't fine. So it's also really hopeful that that's not the position that NIST is taking now, and that's not the position that people who may not even pick the NIST standards but pick other standards are really thinking through. Yeah, it's very helpful and positive and nice to hear when something has improved for the better. Right? And that's what happened here. We had this, this different attitude from, you know, government at large in the past and it's changed and that's partly thanks to EFF, which is amazing. Yeah, I think that's right. And, um, you know, we'll see going forward, you know, the governments change and they go through different things, but this is, this is a hopeful moment and we're gonna push on through to this future. I think there's a lot of, you know, there's a lot of worry about quantum computers and what they're gonna do in the world, and it's nice to have a little vision of, not only can we get it right, but there are forces in place that are getting it right. And of course it does my heart so, so good that, you know, someone like Deirdre was inspired by Snowden and dove deep and figured out how to be one of the people who was building the better world. We've talked to so many people like that, and this is a particular, you know, little geeky corner of the world. But, you know, those are our people and that makes me really happy. Thanks for joining us for this episode of How to Fix the Internet.If you have feedback or suggestions, we'd love to hear from you. Visit EFF dot org slash podcast and click on listener feedback. While you're there, you can become a member, donate, maybe even pick up some merch and just see what's happening in digital rights this week and every week.Our theme music is by Nat Keefe of BeatMower with Reed MathisHow to Fix the Internet is supported by the Alfred P. Sloan Foundation's program in public understanding of science and technology.I’m Jason Kelley… And I’m Cindy Cohn. This podcast is licensed creative commons attribution 4.0 international, and includes the following music licensed creative commons attribution 3.0 unported by its creators: Drops of H2O, The Filtered Water Treatment by Jay Lang. Sound design, additional music and theme remixes by Gaetan Harris.]]></content:encoded></item><item><title>Bezos-Backed Methane Tracking Satellite Is Lost In Space</title><link>https://tech.slashdot.org/story/25/07/01/2211218/bezos-backed-methane-tracking-satellite-is-lost-in-space?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[MethaneSAT, an $88 million satellite backed by Jeff Bezos and led by the Environmental Defense Fund to track global methane emissions, has been lost in space after going off course and losing power over Norway. "We're seeing this as a setback, not a failure," Amy Middleton, senior vice president at EDF, told Reuters. "We've made so much progress and so much has been learned that if we hadn't taken this risk, we wouldn't have any of these learnings." Reuters reports: The launch of MethaneSAT in March 2024 was a milestone in a years-long campaign by EDF to hold accountable the more than 120 countries that in 2021 pledged to curb their methane emissions. It also sought to help enforce a further promise from 50 oil and gas companies made at the Dubai COP28 climate summit in December 2023 to eliminate methane and routine gas flaring. [...] While MethaneSAT was not the only project to publish satellite data on methane emissions, its backers said it provided more detail on emissions sources and it partnered with Google to create a publicly-available global map of emissions.
 
EDF reported the lost satellite to federal agencies including the National Oceanic and Atmospheric Administration, Federal Communications Commission and the U.S. Space Force on Tuesday, it said. Building and launching the satellite cost $88 million, according to the EDF. The organization had received a $100 million grant from the Bezos Earth Fund in 2020 and got other major financial support from Arnold Ventures, the Robertson Foundation and the TED Audacious Project and EDF donors. The project was also partnered with the New Zealand Space Agency. EDF said it had insurance to cover the loss and its engineers were investigating what had happened.
 
The organization said it would continue to use its resources, including aircraft with methane-detecting spectrometers, to look for methane leaks. It also said it was too early to say whether it would seek to launch another satellite but believed MethaneSAT proved that a highly sensitive instrument "could see total methane emissions, even at low levels, over wide areas."]]></content:encoded></item><item><title>Google&apos;s Data Center Energy Use Doubled In 4 Years</title><link>https://hardware.slashdot.org/story/25/07/01/221237/googles-data-center-energy-use-doubled-in-4-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: No wonder Google is desperate for more power: The company's data centers more than doubled their electricity use in just four years. The eye-popping stat comes from Google's most recent sustainability report, which it released late last week. In 2024, Google data centers used 30.8 million megawatt-hours of electricity. That's up from 14.4 million megawatt-hours in 2020, the earliest year Google broke out data center consumption. Google has pledged to use only carbon-free sources of electricity to power its operations, a task made more challenging by its breakneck pace of data center growth. And the company's electricity woes are almost entirely a data center problem. In 2024, data centers accounted for 95.8% of the entire company's electron budget.
 
The company's ratio of data-center-to-everything-else has been remarkably consistent over the last four years. Though 2020 is the earliest year Google has made data center electricity consumption figures available, it's possible to use that ratio to extrapolate back in time. Some quick math reveals that Google's data centers likely used just over 4 million megawatt-hours of electricity in 2014. That's sevenfold growth in just a decade. The tech company has already picked most of the low-hanging fruit by improving the efficiency of its data centers. Those efforts have paid off, and the company is frequently lauded for being at the leading edge. But as the company's power usage effectiveness (PUE) has approached the theoretical ideal of 1.0, progress has slowed. Last year, Google's company-wide PUE dropped to 1.09, a 0.01 improvement over 2023 but only 0.02 better than a decade ago. Yesterday, Google announced a deal to purchase 200 megawatts of future fusion energy from Commonwealth Fusion Systems, despite the energy source not yet existing. "It's a sign of how hungry big tech companies are for a virtually unlimited source of clean power that is still years away," reports CNN.]]></content:encoded></item><item><title>Nintendo’s Anti-Consumer Anti-Piracy Measures Also Reduce The Value Of The Switch 2</title><link>https://www.techdirt.com/2025/07/01/nintendos-anti-consumer-anti-piracy-measures-also-reduce-the-value-of-the-switch-2/</link><author>Dark Helmet</author><category>tech</category><pubDate>Wed, 2 Jul 2025 03:15:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[When it comes to the anti-piracy efforts taken by some of the more aggressive companies out there, such as Nintendo, the most frustrating part of the whole thing for me is just how completely short-sighted those efforts tend to be. Take Nintendo’s updated EULA for its Switch consoles, for example. The updated agreement makes several changes from its previous iteration, but the most notable is that Nintendo says that if it thinks you’re doing the piracy for any reason, it can suspend all kinds of services on your console, up to and including bricking it completely. And, while the company has yet to go the bricking route so far,  it has already begun suspending all online services on consoles for the use of MIG Switches, cards for Switch devices on which you can load legitimately extracted ROMs from purchased games, or pirated versions of the same.Now, the first layer of how this is short-sighted is easy enough to see. In order to engage in copyright protectionism, Nintendo is risking long-term reputational damage by functionally ruining the consoles of customers for actions that aren’t illegal, or even immoral. Short term protection, longer term risk of everyone thinking you don’t care about your own customers. But there’s another layer to this, as a result of these service suspensions being tied directly to the device rather than the person. And that is what this protectionism means for the secondary market for Nintendo Switches.“I was driving between work sites and stopped at two different Walmarts,” says user Bimmytung. “At the second one I find a Mario Kart edition sitting in the case and couldn’t believe my luck.” They were informed by the Walmart staff that it was an “open box return,” so it was removed from the box to be checked over, and all looked well. The code for the packaged Mario Kart World had been scratched off already, so Walmart knocked another $50 off the price, and it all seemed like a good deal. Until they got home.Finally after work I get a chance to set it up. Quickly realize I need the super special micro SD card and none of the ~half dozen in the house would work. Drive ten minutes to Target and get one there and pick up a few other accessories as well. Get home and go to finish the setup—quickly get Error Code 2124-4508. A quick Google search shows me I’m screwed. FML.”Now, there are several layers of shame here to go around. Shame on Walmart for selling a device without ensuring it would work for the buyer the way it is intended to work. And shame on Nintendo for creating an anti-piracy program such that the punishments meted out are linked to hardware rather than the supposed bad-actor it seeks to punish. But all of that aside, it should also be true that this sort of thing drives the value of a Nintendo Switch console lower than it would be otherwise. Part of the value you gain when you buy a physical thing is the ability to eventually put it on the secondary market at some point. Because of the actions that Nintendo is taking in disabling and/or bricking its own consoles, that injects a great deal of risk into the prospect of buying one on the secondary market. The value of the hardware is, by at least some measure, diminished.But because Nintendo seems to only think about these things in the short term, the company probably doesn’t much care.However, the more immediate issue is for those looking to pick up a Switch 2 from a reseller or previous owner, given their current scarcity at first-party sellers. There’s really no way of knowing at all if a console has been bricked when buying the device online, and this could make the resale market a complete shambles for the whole life cycle of the console. And, grimly, that’s not exactly a priority for Nintendo, given that reselling, either in store or online, gains the company nothing, and some would argue actually costs the company a sale—it’s not like it’ll be in a rush to address the problem.Which is why I won’t be in a rush to buy a Switch 2 anytime soon. And I’m certainly in their target market, having two young children who desperately want one. Instead of the console, however, they will be getting a lesson in making smart buying decisions as a consumer.]]></content:encoded></item><item><title>Laptop Mag Is Shutting Down</title><link>https://hardware.slashdot.org/story/25/07/01/2133224/laptop-mag-is-shutting-down?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Laptop Mag, a tech publication that began in 1991 as a print magazine, is shutting down after nearly 35 years. The Verge reports: Laptop Mag has evolved many times over the years. It started as a print publication in 1991, when Bedford Communications launched the Laptop Buyers Guide and Handbook. Laptop Mag was later acquired by TechMedia Network (which is now called Purch) in 2011 and transitioned to digital-only content in 2013. Future PLC, the publisher that owns brands like PC Gamer, Tom's Guide, and TechRadar, acquired Purch -- and Laptop Mag along with it.
 
"We are incredibly grateful for your dedication, talent, and contributions to Laptop Mag, and we are committed to supporting you throughout this transition," [Faisal Alani, the global brand director at Laptop Mag owner Future PLC] said. Laptop Mag's shutdown follows the closure of long-running tech site AnandTech, which was also owned by Future PLC. It's not clear whether Laptop Mag's archives will be available following the shutdown.]]></content:encoded></item><item><title>Apple Accuses Former Engineer of Taking Vision Pro Secrets To Snap</title><link>https://yro.slashdot.org/story/25/07/01/2128235/apple-accuses-former-engineer-of-taking-vision-pro-secrets-to-snap?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple has filed (PDF) a lawsuit against former Vision Pro engineer Di Liu, accusing him of stealing thousands of confidential files related to his work on Apple's augmented reality headset for the benefit of his new employer Snap. The company alleges Liu misled colleagues about his departure, secretly accepted a job offer from Snap, and attempted to cover his tracks by deleting files -- actions Apple claims violated his confidentiality agreement. The Register reports: Liu secretly received a job offer from Snap on October 18, 2024, a role the complaint describes as "substantially similar" to his Apple position, meaning Liu waited nearly two weeks to resign from Apple, per the lawsuit. "Even then, he did not disclose he was leaving for Snap," the suit said. "Apple would not have allowed Mr. Liu continued access had he told the truth." Liu allegedly copied "more than a dozen folders containing thousands of files" from Apple's filesystem to a personal cloud storage account, dropping the stolen bits in a pair of nested folders with the amazingly nondescript names "Personal" and "Knowledge."
 
Apple said that data Liu copied includes "filenames containing confidential Apple product code names" and files "marked as Apple confidential." Company research, product design, and supply chain management documents were among the content Liu is accused of stealing. The complaint also alleges that Liu deleted files to conceal his activities, a move that may hinder Apple's ability to determine the full scope of the data he exfiltrated. "Mr. Liu additionally took actions to conceal his theft, including deceiving Apple about his job at Snap, and deleting files from his Apple-issued computer that might have let Apple determine what data Mr. Liu stole," the complaint noted.
 
Whatever he has, Apple wants it back. The company demands a jury trial on a single count of breach of contract under a confidentiality and intellectual property agreement Liu was bound to. It also asks the court to compel Liu to return all misappropriated data, award damages to be determined at trial, and reimburse Apple's costs and attorneys' fees.]]></content:encoded></item><item><title>Tinder To Require Facial Recognition Check For New Users In California</title><link>https://yro.slashdot.org/story/25/07/01/2112208/tinder-to-require-facial-recognition-check-for-new-users-in-california?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Axios: Tinder is mandating new users in California verify their profiles using facial recognition technology starting Monday, executives exclusively tell Axios. The move aims to reduce impersonation and is part of Tinder parent Match Group's broader effort to improve trust and safety amid ongoing user frustration. The Face Check feature prompts users to take a short video selfie during onboarding. The biometric face scan, powered by FaceTec, then confirms the person is real and present and whether their face matches their profile photos. It also checks if the face is used across multiple accounts. If the criteria are met, the user receives a photo verified badge on their profile. The selfie video is then deleted. Tinder stores a non-reversible, encrypted face map to detect duplicate profiles in the future.
 
Face Check is separate from Tinder's ID Check, which uses a government-issued ID to verify age and identity. "We see this as one part of a set of identity assurance options that are available to users," Match Group's head of trust and safety Yoel Roth says. "Face Check ... is really meant to be about confirming that this person is a real, live person and not a bot or a spoofed account." "Even if in the short term, it has the effect of potentially reducing some top-line user metrics, we think it's the right thing to do for the business," Rascoff said.]]></content:encoded></item><item><title>Figma Files For IPO</title><link>https://slashdot.org/story/25/07/01/2058244/figma-files-for-ipo?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Figma has filed to go public on the NYSE under the ticker "FIG," marking one of the most anticipated IPOs in recent years following its scrapped $20 billion acquisition by Adobe. CNBC reports: Revenue in the first quarter increased 46% to $228.2 million from $156.2 million in the same period a year ago, according to Figma's prospectus. The company recorded a net income of $44.9 million, compared to $13.5 million a year earlier. As of March 31, Figma had 1,031 customers contributing at least $100,000 a year to annual revenue, up 47% from a year earlier. Clients include Amazon Web Services, Google, Microsoft and Netflix. More than half of revenue comes from outside the U.S. Figma didn't say how many shares it plans to sell in the IPO. The company was valued at $12.5 billion in a tender offer last year, and in April it announced that it had confidentially filed for an IPO with the SEC. [...]
 
Figma was founded in 2012 by CEO Dylan Field, 33, and Evan Wallace, and is based in San Francisco. The company had 1,646 employees as of March 31. Before establishing Figma, Field spent over two years at Brown University, where he met Wallace. Field then took a Thiel Fellowship "to pursue entrepreneurial projects," according to the filing. The two-year program that Founders Fund partner Peter Thiel established in 2011 gives young entrepreneurs a $200,000 grant along with support from founders and investors, according to an online description. Field is the biggest individual owner of Figma, with 56.6 million Class B shares and 51.1% of voting power ahead of the IPO. He said in a letter to investors that it was time for Figma to buck the "trend of many amazing companies staying privately indefinitely." "Some of the obvious benefits such as good corporate hygiene, brand awareness, liquidity, stronger currency and access to capital markets apply," wrote Field. "More importantly, I like the idea of our community sharing in the ownership of Figma -- and the best way to accomplish this is through public markets."
 
As a public company, Field said investors should "expect us to take big swings," including through acquisitions.
 
In April, Figma bought the assets and team of an unnamed technology company for $14 million, according to the filing. They also registered over 13 million users per month, one-third of which are designers.]]></content:encoded></item><item><title>Xerox Buys Lexmark For $1.5 Billion As Print Industry Clings To Relevance</title><link>https://slashdot.org/story/25/07/01/2247221/xerox-buys-lexmark-for-15-billion-as-print-industry-clings-to-relevance?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 23:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli shares a report from NERDS.xyz: In a move that feels straight out of a different era, Xerox has officially acquired Lexmark for $1.5 billion. The deal includes net debt and assumed liabilities, and it pulls Lexmark out of the hands of Chinese ownership and into a freshly restructured Xerox. That's a lot of money for a company best known for making machines that spit out paper.
 
According to Xerox, this is all part of a "Reinvention" strategy. The company now claims it will be one of the top five players in every major print category and the leader in managed print services. [...] Xerox says the new leadership team will include executives from both sides, and the combined business will now support over 200,000 clients in more than 170 countries. They'll also be running 125 manufacturing and distribution centers in 16 countries.]]></content:encoded></item><item><title>AMC Warns Moviegoers To Expect &apos;25-30 Minutes&apos; of Ads and Trailers</title><link>https://entertainment.slashdot.org/story/25/07/01/2052226/amc-warns-moviegoers-to-expect-25-30-minutes-of-ads-and-trailers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AMC Theatres now warns customers that movies start 25-30 minutes after the listed showtime to account for ads and trailers, "making it easier for moviegoers to know the actual start time of their film screening," reports The Verge. From the report: Starting today, AMC will also show more ads than before, meaning its preshow lineup may have to be reconfigured to avoid exceeding the 30-minute mark. The company made an agreement with the National CineMedia ad network that includes as much as five minutes of commercials shown "after a movie's official start time," according to The Hollywood Reporter, and an additional 30-to-60-second "Platinum Spot" that plays before the last one or two trailers.
 
AMC was the only major theater chain to reject the National CineMedia ad spot when it was pitched in 2019, telling Bloomberg at the time that it believed "US moviegoers would react quite negatively." Now struggling financially amid an overall decline in movie theater attendance and box-office grosses, AMC has reversed course, telling The Hollywood Reporter that its competitors "have fully participated for more than five years without any direct impact to their attendance."]]></content:encoded></item><item><title>Congress just greenlit a NASA moon plan opposed by Musk and Isaacman</title><link>https://techcrunch.com/2025/07/01/congress-just-greenlit-a-nasa-moon-plan-opposed-by-musk-and-isaacman/</link><author>Aria Alamalhodaei</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:30:59 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The $10 billion addition to the Artemis architecture, which includes funding for additional Space Launch System rockets and an orbiting station around the moon called Gateway, is a rebuke to critics who wished to see alternative technologies used instead.]]></content:encoded></item><item><title>The Moral Imperative Of Clear Language</title><link>https://www.techdirt.com/2025/07/01/the-moral-imperative-of-clear-language/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:17:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[I need to say something that will make many of you deeply uncomfortable: your refusal to call fascism “fascism” is not sophistication—it’s .When Donald Trump posts explicit orders for “REMIGRATION” and “Mass Deportation Operations” targeting American cities because they are “the core of the Democrat Power Center,” that’s not “controversial immigration policy.” That’s mass deportation directed against political opponents. When federal troops deploy against American civilians exercising constitutional rights, that’s not “enhanced law enforcement.” That’s military occupation. When the systematic dismantling of democratic institutions gets described as “political polarization,” that’s not nuanced analysis—it’s linguistic evasion that enables the very thing it refuses to name.The sophisticates hate this clarity. They prefer the safety of euphemism, the comfort of complexity that never quite arrives at moral judgment. They speak of “concerning developments” and “troubling trends” while democracy burns around them. They perform nuanced understanding while fascism consolidates power through their very refusal to name it.But here’s what they don’t understand: authoritarianism thrives in ambiguity. It requires linguistic fog to operate. It depends on our unwillingness to call things by their proper names. Every euphemism is a small surrender. Every hedge is a tiny collaboration. Every refusal to speak plainly is a gift to those who profit from confusion.Language shapes consciousness. When we refuse to name what we see clearly, we don’t just fail to communicate—we erode our collective capacity to think clearly, to feel appropriately, to respond effectively. We make ourselves complicit in our own moral disorientation.George Orwell understood this when he wrote that “political language is designed to make lies sound truthful and murder respectable, and to give an appearance of solidity to pure wind.” But he was describing propaganda techniques used by totalitarian regimes. What we face now is worse: the voluntary adoption of euphemistic language by people who should know better, who pride themselves on seeing clearly, who claim to defend democratic values.We are doing the propagandists’ work for them.Consider how this linguistic distortion operates in practice. When mass deportation operations targeting millions of people get called “immigration enforcement,” we’re not being diplomatic—we’re making state violence psychologically easier to accept. When systematic attacks on democratic institutions get labeled “political disagreements,” we’re not showing balance—we’re normalizing authoritarianism. When obvious lies get treated as “alternative perspectives,” we’re not being fair—we’re weaponizing false equivalence against truth itself.The euphemism isn’t just descriptive failure—it’s moral failure. It changes how people process information, how they make decisions, how they understand their own moral obligations. When you call fascism “populism,” you’re not just using imprecise language. You’re making it easier for people to support fascism without confronting what they’re supporting.Hannah Arendt spent her life studying how ordinary people enable extraordinary evil, and she identified linguistic evasion as one of the primary mechanisms. In , she showed how bureaucratic language—“evacuation,” “resettlement,” “special treatment”—allowed participants in genocide to avoid confronting the reality of what they were doing. They weren’t murdering children; they were “processing population transfers.” They weren’t operating death camps; they were managing “facilities for the final solution.”The language didn’t just hide the reality from others—it hid it from themselves. It allowed them to participate in evil while maintaining their self-image as decent, law-abiding citizens following proper procedures.Arendt’s insight was that evil becomes possible not primarily through active malice but through the refusal of ordinary people to see and name what’s in front of them. The “banality of evil” is fundamentally about linguistic evasion enabling moral evasion. When we stop calling violence violence, we make violence easier to commit.This is what we’re witnessing now. The systematic training of a population to see clearly but speak obliquely, to understand precisely but describe vaguely, to recognize authoritarianism but call it something else. We have become a society of people who know exactly what’s happening but lack the linguistic courage to say so.The Practice of Plain NamingConsider how this evasion plays out in our current discourse:We don’t say “Trump is implementing fascist policies.” We say “Trump’s approach raises concerns about democratic norms.”We don’t say “Republicans are supporting mass deportation operations.” We say “There are disagreements about immigration enforcement strategies.”We don’t say “Conservative media spreads lies designed to enable authoritarianism.” We say “Different sources present different perspectives on complex issues.”We don’t say “MAGA supporters have chosen to enable fascism.” We say “There are legitimate grievances driving political polarization.”Each euphemism makes the reality a little less clear, a little less urgent, a little less morally demanding. Each hedge creates space for people to avoid confronting what they’re witnessing or participating in. Each refusal to name plainly is a small act of collaboration with the forces that depend on confusion to operate.When Trump orders ICE to conduct “Mass Deportation Operations” in cities he identifies as “the core of the Democrat Power Center,” that’s not immigration policy—it’s the use of state violence against political opponents. When he calls for “REMIGRATION” of millions of people, that’s not border security—it’s forced population transfer. When federal agents separate families and detain children, that’s not law enforcement—it’s state-sanctioned cruelty.The defenders will say “the law is the law”—as if legality were equivalent to morality. But slavery was legal. Segregation was legal. Japanese internment was legal. Every authoritarian regime in history has operated through law, not despite it. “The law is the law” is not a moral position—it’s moral abdication disguised as principled governance.Law without moral foundation is just organized violence. Rules without ethical grounding are just systematized cruelty. When your only defense of a policy is that it’s technically legal, you’ve already admitted it’s morally indefensible.The Sophisticates’ ResistanceThe sophisticates will tell you that such plain language is “inflammatory,” “divisive,” “unhelpful to productive dialogue.” They’ll suggest that calling fascism “fascism” alienates potential allies, shuts down conversation, makes compromise impossible.But here’s what they’re really saying: they prefer the comfort of ambiguity to the responsibility that comes with clarity. They’d rather maintain the illusion of reasoned discourse than confront the reality that one side has abandoned reason entirely. They want to keep playing by rules that the other side has explicitly rejected.This isn’t sophistication—it’s cowardice. It’s the intellectual’s version of appeasing authoritarianism through linguistic accommodation. It’s the belief that if we just find the right words, the right tone, the right approach, we can somehow reason with people who have chosen unreason as their governing principle.But you cannot have productive dialogue with fascists about the merits of fascism. You cannot find common ground with people who reject the premise of shared reality. You cannot compromise with those who view compromise as weakness and good faith as stupidity.What you can do is name what they are doing clearly enough that people understand what’s at stake and what choice they face.The power of plain naming is that it forces moral confrontation. It makes people choose sides. It strips away the comfortable distance that euphemism provides. It demands that people acknowledge what they’re actually supporting rather than hiding behind sanitized language.This is why authoritarians work so hard to control language. They understand that linguistic precision is the enemy of moral confusion. That clear naming makes their projects harder to defend. That euphemism is their friend and clarity is their enemy.They want us to call their fascism “nationalism.” Their lies “alternative facts.” Their cruelty “tough love.” Their mass deportations “border security.” Their authoritarianism “law and order.”Every time we adopt their language, we do their work. Every time we refuse to name their actions plainly, we make those actions easier to defend, easier to rationalize, easier to continue.When we refuse to call fascism “fascism”, we don’t make fascism less dangerous. We make ourselves less capable of recognizing and resisting it. We participate in our own disorientation. We become accomplices to our own confusion.The courage to name things plainly is not the courage to be harsh or inflammatory. It’s the courage to accept the responsibility that comes with seeing clearly. It’s the courage to abandon the comfortable illusion of neutrality and acknowledge that some things cannot be straddled, some positions cannot be hedged, some realities cannot be euphemized away.To say that systematic deployment of federal troops against American cities constitutes military occupation is not inflammatory—it’s accurate. To say that mass deportation operations targeting political opponents constitute fascist policy is not hyperbolic—it’s precise. To say that obvious lies designed to enable authoritarianism are lies is not divisive—it’s necessary.The alternative to plain naming is not diplomatic nuance—it’s moral blindness. It’s the systematic erosion of our capacity to recognize authoritarianism when it appears in familiar forms, speaking familiar languages, wearing familiar clothes.Evil depends on our unwillingness to call it evil. Fascism depends on our refusal to call it fascism. Lies depend on our treatment of them as “alternative perspectives.” State violence depends on our description of it as “tough policy choices.”The moment we name these things plainly, we restore the moral clarity that makes effective resistance possible. We acknowledge what we’re actually facing. We accept the responsibility that comes with seeing clearly. We choose truth over comfort, accuracy over diplomacy, moral clarity over intellectual sophistication.This is not just a linguistic choice—it’s a moral one. Every time we speak plainly about what we’re witnessing, we strike a blow against the forces that depend on confusion to operate. Every time we call fascism “fascism”, we make fascism a little harder to defend. Every time we name state violence as state violence, we make such violence a little less acceptable.Two plus two equals four. There are twenty-four hours in a day. And Trump’s mass deportation operations are fascistic displays of state violence targeting political enemies whether we have the courage to call them that or not.The difference is not in the reality—the difference is in our capacity to respond to reality appropriately.Name it plainly. Not because it’s easy, but because it’s true. Not because it’s comfortable, but because comfort in the face of authoritarianism is itself a form of collaboration. Not because it’s diplomatic, but because diplomacy with fascists is enabling fascism.The revolution is linguistic honesty. The rebellion is calling things by their proper names. The resistance is refusing to participate in the euphemistic erosion of moral clarity.Say what you see. Name what you know. Call fascism .Every minute of every day.Remember what’s real. Because the alternative to naming fascism clearly isn’t moderation or diplomacy—it’s surrender.Mike Brock is a former tech exec who was on the leadership team at Block. Originally published at his Notes From the Circus.]]></content:encoded></item><item><title>ICEBlock, an app for anonymously reporting ICE sightings, goes viral overnight after Bondi criticism</title><link>https://techcrunch.com/2025/07/01/iceblock-an-app-for-anonymously-reporting-ice-sightings-goes-viral-overnight-after-bondi-criticism/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:09:38 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The citizen app for anonymously reporting ICE agents and raids went viral after criticism from the U.S. attorney general.]]></content:encoded></item><item><title>EFFecting Change: EFF Turns 35!</title><link>https://www.eff.org/deeplinks/2025/06/effecting-change-eff-turns-35</link><author>Melissa Srago</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/effectingeffturns35_banner.png" length="" type=""/><pubDate>Tue, 1 Jul 2025 22:09:36 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[We're wishing EFF a happy birthday on July 10! Since 1990, EFF's lawyers, activists, analysts, and technologists have used everything in their toolkit to ensure that technology supports freedom, justice, and innovation for all people of the world. They've seen it all and in this special edition of our EFFecting Change livestream series, leading experts at EFF will explore what's next for technology users.Want to make sure you don’t miss our next livestream? Here’s a link to sign up for updates about this series:eff.org/ECUpdates.]]></content:encoded></item><item><title>Amazon Deploys Its One Millionth Robot, Releases Generative AI Model</title><link>https://hardware.slashdot.org/story/25/07/01/2046242/amazon-deploys-its-one-millionth-robot-releases-generative-ai-model?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: After 13 years of deploying robots into its warehouses, Amazon reached a new milestone. The tech behemoth now has 1 million robots in its warehouses, the company announced Monday. This one millionth robot was recently delivered to an Amazon fulfillment facility in Japan. That figure puts Amazon on track to reach another landmark: Its vast network of warehouses may soon have the same number of robots working as people, according to reporting from The Wall Street Journal. The WSJ also reported that 75% of Amazon's global deliveries are now assisted in some way by a robot. Amazon also unveiled a new generative AI model called DeepFleet, built using SageMaker and trained on its own warehouse data, which improves robotic fleet speed by 10% through more efficient route coordination.]]></content:encoded></item><item><title>Figma moves closer to a blockbuster IPO that could raise $1.5B</title><link>https://techcrunch.com/2025/07/01/figma-moves-closer-to-a-blockbuster-ipo-that-could-raise-1-5b/</link><author>Julie Bort</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:55:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The financials are impressive and  founder CEO Dylan Field already cashed out $20 million worth of shares last year.]]></content:encoded></item><item><title>$70M Committed To Boba Network As Foundation Concludes BOBA Token Agreement With FTX Recovery Trust</title><link>https://hackernoon.com/$70m-committed-to-boba-network-as-foundation-concludes-boba-token-agreement-with-ftx-recovery-trust?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:43:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Grand Cayman, Cayman Islands, July 1st, 2025/Chainwire/--Boba Governance Foundation today announced a significant milestone with a $70 million capital commitment secured from Awaken Foundation and LDA Capital to fund the continued development and ecosystem expansion of Boba Network, the leading Layer-2 blockchain for AI-powered decentralized applications (dApps), enabled by its unique HybridCompute technology. The Foundation also announced an agreement with FTX Recovery Trust regarding the BOBA tokens held by the Trust. The $70 million capital infusion will serve as a catalyst for Boba Network's ambitious growth plans. The funding will be strategically allocated to bolster the network's core infrastructure, expand its developer ecosystem, and foster the creation of innovative decentralized applications (dApps) on the platform, with a particular focus on enabling AI-powered dApps."This funding will accelerate the development of the Boba Network ecosystem, attract top-tier talent, and drive the widespread adoption of Boba Network as a premier Layer-2 solution for AI-powered dApps. We are excited to collaborate with Boba Network partners to shape the future of the AI-powered, decentralized web," said Alan Chiu, CEO of Enya Labs, a core contributor to Boba Network.""This substantial capital commitment from Awaken Foundation and LDA Capital is a testament to the transformative potential of Boba Network," said David Acutt, director of Boba Governance Foundation.Awaken Foundation, a key advocate for decentralized infrastructure and digital sovereignty, sees Boba Network as a critical component in the next phase of Web3 evolution.“We are thrilled to support the Boba Governance Foundation in its pursuit of open innovation,”said Nattaphol Vimolchalao, Director at Awaken Foundation. “Boba’s ability to connect smart contracts with off-chain computation—especially AI—unlocks enormous potential across industries.”LDA Capital, known for backing high-growth tech ventures and digital asset ecosystems, echoed that sentiment.“Boba Network is building essential infrastructure for the future of decentralized computation,” said Warren Baker, Managing Partner at LDA Capital. “We believe Boba will play a pivotal role in scaling the next generation of intelligent dApps, and we’re proud to support their mission as they push the boundaries of what’s possible in blockchain technology.”The strategic partnership with Awaken Foundation and LDA Capital goes far beyond financial support. It represents a powerful alignment of vision, expertise, and global reach. Leveraging deep industry knowledge, business development capabilities, and an extensive network of strategic partners, both firms are uniquely positioned to accelerate Boba Network’s growth. This collaboration is set to strengthen Boba’s leadership in blockchain innovation and drive its next phase of global expansion. In addition, LDA Capital offers differentiated value through LDA Velocity, its institutional-grade liquidity and market-making platform that supports healthy, scalable token ecosystems across global exchanges.Infrastructure Enhancement: The funding will be used to strengthen Boba Network's infrastructure, ensuring high throughput, low latency, and robust security for users and developers.Ecosystem Expansion: A portion of the capital will be dedicated to expanding the Boba Network ecosystem by attracting developers, projects, and users through grants and educational initiatives.dApp Development: The funding will support the creation of innovative dApps on Boba Network, with a strong emphasis on AI-powered dApps, ranging from decentralized finance (DeFi) protocols to real-world assets (RWA) applications.Community Engagement: Boba Governance Foundation will continue to foster a vibrant and engaged community by providing resources, support, and opportunities for collaboration.Resolution with FTX Recovery TrustIn addition, Boba Governance Foundation has executed an agreement with FTX Recovery Trust whereby all the BOBA tokens held by the Trust have been transferred to the Foundation. FTX Recovery Trust, in addition to other consideration and mutual release of claims, received the right to purchase up to approximately 29.4M BOBA tokens from Boba Governance Foundation at $0.09 per token within the next 18 months.“This agreement represents a momentous milestone for Boba Network, as it removes a major source of uncertainty over the BOBA token and strengthens the Foundation’s ability to support the continued development of Boba Network and its ecosystem,” said Acutt.About Boba Governance Foundation is a non-profit organization dedicated to the advancement and growth of Boba Network. It supports the development of the network's technology, fosters community engagement, and promotes the adoption of Boba Network across various industries. Boba Network is the leading Layer-2 blockchain for AI-powered decentralized applications (dApps), enabled by its unique HybridCompute technology. is a private investment firm founded by seasoned crypto, venture capital, and public market investors. The firm seeks to invest in established blockchain protocols to help further develop its technology. Awaken provides strategic capital, accelerated business development, and engineered exits for protocols that Awaken believes have a promising future in the modern economy. is a global alternative investment group with expertise in cross-border transactions worldwide. The team has collectively executed over 350 transactions in both the public and private middle markets across 43 countries with aggregate transaction values of over USD $11 billion. LDA’s investment activities across Web3 include 27+ transactions totaling $400m+ in capital commitments.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Decentralized Public-Key Infrastructure: The Future of Supply Chain Security</title><link>https://hackernoon.com/decentralized-public-key-infrastructure-the-future-of-supply-chain-security?source=rss</link><author>Nneoma Uche</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:31:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[All products, digital and physical, pass through a supply chain—a network of actors that supports their life cycle. But as the global market becomes increasingly interconnected, supply chain attacks are on the rise.In March 2023, cybercriminals infiltrated 3CX’s build environment, injecting malicious code into a library file for its macOS and Windows desktop apps. The compromised file was distributed through official updates, exposing users to malware. This breach emphasized the shortcomings of centralized systems in securing supply chains, as a single compromised vendor can jeopardize the privacy of the entire customer base.Web3 supply chains, leveraging blockchain and Decentralized Public Key Infrastructure (DPKI), offer a robust alternative. By prioritizing transparency, traceability, and tamper-proof security, they present a stronger defense against supply chain threats.This article explores how DPKI in blockchain-driven networks outperforms traditional PKI, and why a supply chain powered by the latter presents a tougher nut for bad actors to crack.Understanding Public-Key Infrastructure (PKI)IBM defines ‘Public-Key Infrastructure (PKI)’ as a comprehensive framework used to assign and verify user identity through digital certificates, for secure digital communications. The entire PKI framework relies on asymmetric cryptography- the use of a public and private key pair to encrypt and decrypt data, respectively.PKI allows us to associate identities with particular key pairs. Although the public key can be visible to anyone on the network, only the entity with the corresponding private key can access specific features or information.In a supply chain, PKI combines digital certificates and asymmetric cryptography to establish trust and ensure integrity.  The public keys are embedded in a digital certificate, which authenticates the user or device communicating across the network.Certificate Authority (CA): The Certificate Authority is a trusted entity that issues digital certificates to participants in the supply chain, e.g., developers, analytics providers, payment gateways, cloud providers, etc.Certificate: Digital certificates are cryptographic credentials, issued and signed by the CA, used to verify the identity of and secure communication among supply chain actors. They typically include public keys and identity details, accessible upon request.Registration Authority (RA): The RA ensures that only authorized participants can obtain a digital certificate, thus enhancing security within the supply chain. The CA can double as the registration authority, although trusted third-party services are just as efficient.Certificate database: This PKI component is a secure repository or location that stores issued digital certificates, alongside their metadata, i.e., public keys, revocation status, and validity details.Certificate policy: This is a formal document outlining the procedures and requirements governing the issuance, usage and management of digital certificates within the supply chain.Central directory: The central directory is a public repository where cryptographic keys, digital certificates and Certificate Revocation Lists(CRLs) are indexed and stored. It enables anyone in the ecosystem to authenticate a digital signature and encrypt data to a specific key owner.Traditional Supply Chain Systems (Web2) vs. Web3-Driven Supply ChainsWeb2-based supply chains are centralized; participants rely on Certificate Authorities to verify other actors and establish trust. Moreover, digital certificates and cryptographic keys are stored in central directories, leaving room for supply chain attacks in the absence of robust security measures.Another feature of Web2-based supply chains is the opacity around certificate issuance and revocation. There’s no universal metric to determine eligibility for certificate issuance. Instead, entities must operate on a trust assumption that the CA has properly vetted the requester.Oftentimes, delayed updates and limited visibility associated with Certificate Revocation Lists (CRLs) may result in revoked certificates appearing valid to related devices or applications. This can impact supply chain integrity, due to unauthorized access, tampered goods, compliance issues and a loss of trust. In 2024, Google delisted Entrust (a formerly reputable Certificate Authority), from its Chrome Root Program due to malfunctions in its certificate issuance and revocation operations. A few months prior, Entrust admitted to misissuing over 26,000 digital certificates and failing to revoke them within the revocation timeline outlined by the Certificate Authority/Browser Forum.Web3-driven supply chains, on the other hand, leverage decentralized systems, smart contracts, and pseudonymous transactions to create a trustless, transparent, and secure ecosystem. Unlike traditional supply chains that rely on centralized authorities, Web3 enables each participant to interact directly on a shared blockchain, reducing intermediaries and single points of failure.In addition, PKI data (i.e. public keys and certificates) is stored immutably on a blockchain, making them nearly tamper-proof, while being easily accessible for verification. Together, these features make Web3 supply chains more resilient and trustworthy than ttheir raditional counterparts.From PKI to DPKI: Strengthening Supply Chain Integrity in Web3In Web3-driven systems, PKI implementation shifts from traditional Certificate Authorities (CAs) to decentralized or distributed models that align with Web3 principles, hence the term—Decentralized Public-Key Infrastructure (DPKI).The idea behind it is simple: enable tamper-proof verification of supply chain data and participants, without relying on a centralized database.Here's how DPKI enhances supply chain integrity in a Web3 ecosystem:Decentralized trust models manage authentication and verificationRather than a centralized entity, DPKI relies on a web of trust—a network of on-chain participants who collectively verify and authenticate information. Each supplier on-chain uses a decentralized identifier (DID), which functions as a unique digital signature, to prove authenticity, access data and sign transactions.Vendors interacting in a decentralized supply network use their DIDs to access proprietary data, verify purchase orders, and access secure channels. Similarly, a product within the chain can be assigned a DID, enabling participants to verify its origin and authenticity at every step.Smart contracts ensure transparency and data integritySmart contracts are self-executing programs stored on a blockchain that automatically trigger specific actions once preset conditions are met. They automate various processes within a supply chain, such as processing payments, issuing tickets, or approving shipments. All interactions with the contract are recorded on the blockchain, creating a permanent and tamper-proof audit trail. This record allows stakeholders to trace:The journey of goods through the supply chain.Compliance with standards at each stage.Discrepancies back to their source.Automation through digital contracts reduces the risk of fraud, human error, and compliance issues.Blockchain-powered scalabilityBy eliminating reliance on a Certificate Authority and other intermediaries, decentralized public key infrastructure offers more scalability to Web3-based supply chains. In traditional PKI, managing certificates across a complex supply chain can leave the supply network vulnerable to single points of failure. Moreover, scaling may require the involvement of multiple Certificate Authorities, likely resulting in delays or bottlenecks around certificate issuance and revocation. This approach is resource-intensive and may be unrealistic for global supply chains involving numerous entities.In contrast, Web3-driven supply chains leverage blockchain as a trust anchor, enabling a distributed system where records and identities are verifiable by all participants on the chain. The result is a more efficient, scalable infrastructure, tailored to the complexity of modern supply chains.Advantages of DPKI to (Web3) Supply ChainsEliminates Central Authority Risks: No single entity can compromise the supply chain.Self-Sovereign Identity: Supply chain participants control their cryptographic identities, reducing the overhead associated with traditional certificate issuance and management.Enhanced Transparency: All actions (such as key creation and revocation), and transactions are publicly recorded, promoting trust and accountability across the chain.Efficiency: Automation through smart contracts streamlines processes such as inspections and approvals, saving time and resources.Enhanced Security: Cryptographic signatures prevent data tampering and fraud, thus protecting data authenticity.Improved Scalability: Participants can manage their keys and verify others without bottlenecks from centralized authorities.Integrating blockchain: a path to modernizing supply chainsThe shift from centralized to decentralized systems is no longer a futuristic concept, but a growing reality for Web2 companies looking to modernize their supply chains. Companies like IBM, with its blockchain-powered Food Trust, and  De Beers’ Tracr, used for tracking diamonds from source to store, demonstrate how blockchain and DPKI can integrate seamlessly into existing supply chain models to enhance transparency and trust.Harnessing the benefits of decentralized supply systems doesn't require a sudden overhaul of the existing supply chain. Web2 companies can adopt an incremental approach—beginning by identifying use cases—and testing DPKI in targeted areas, before gradually scaling the integration across the entire supply chain.]]></content:encoded></item><item><title>Landmark EU Tech Rules Holding Back Innovation, Google Says</title><link>https://tech.slashdot.org/story/25/07/01/1811254/landmark-eu-tech-rules-holding-back-innovation-google-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google will tell European Union antitrust regulators Tuesday that the bloc's Digital Markets Act is stifling innovation and harming European users and businesses. The tech giant faces charges under the DMA for allegedly favoring its own services like Google Shopping, Google Hotels, and Google Flights over competitors. Potential fines could reach 10% of Google's global annual revenue. 

Google lawyer Clare Kelly will address a European Commission workshop, arguing that compliance changes have forced Europeans to pay more for travel tickets while airlines, hotels, and restaurants report losing up to 30% of direct booking traffic.]]></content:encoded></item><item><title>Complete Gemini CLI Setup Guide for Your Terminal</title><link>https://hackernoon.com/complete-gemini-cli-setup-guide-for-your-terminal?source=rss</link><author>Vladislav Guzey</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:19:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Have you ever wished for an AI assistant right inside your terminal window? Well, your dream may have just come true because Google just released Gemini CLI. In this tutorial, I’m going to show you everything you need to know about this new open-source AI agent. We’ll cover how to use it, the pricing, and some useful tips and tricks. So, if you’re ready, let’s get started! ;)Gemini CLI is a free and open-source AI agent that works directly in your terminal. This powerful tool brings Google’s Gemini models to your command line, allowing for natural language interaction to get work done. You can ask it to:You don’t have to constantly switch between a web app and your terminal. And the best part? It’s , with no complicated setup.Getting Started with Gemini CLIStep 1: Install Gemini CLI on LinuxYou can install  on , , and . All the setup we will do inside the terminal. I am using Linux, but for Mac and Windows, the commands are almost the same, so you can follow my steps.To get started, make sure you have Node.js version 18 or higher. You can check this by running:If you don’t have it, use the following command to install it:sudo apt update && sudo apt install nodejs npm
Then, I installed Gemini CLI globally with:npm install -g @google/gemini-cli
If you don’t want to install it globally, you can also use:npx https://github.com/google-gemini/gemini-cli
After installing, just type:After that, you need to log in with your personal Google account.This gives you access to a free Gemini Code Assist license, which includes:Access to Gemini 2.5 Pro.A massive 1 million token context window.60 model requests per minute.1,000 model requests per day at no charge.Now you are ready to start asking questions and running tasks. You can ask the Agent to create a project, fix the bugs, explain the code in specific files, etc. Ensure that you run the agent within your project folder.> What does the file index.js do?
It read the file, analyzed it, and gave a clear explanation.> Add error handling to index.js
You can also run shell commands directly by using , like this:Creating a Simple To-Do App with Google CLINow that we’re all set up, let’s ask the AI to create a simple to-do application using HTML, CSS, and JavaScript. I will type “create a simple to-do app using simple js and html” into the Gemini CLI.” Watch the video to see the step-by-step process and the result.Gemini CLI has some handy built-in tools. You can use commands like: (). Lists files and folders in a directory—just like the shell  command. (). Reads the full content of a single file, useful for summaries or analysis. (). Reads multiple files at once, typically matching a glob pattern (e.g., all  files) (). Searches for files by pattern (e.g., find all  across your project). (). Searches within files for text, like finding all  comments. (). Applies code changes via diffs. Gemini previews edits and asks for approval before applying them. (). Creates new files (for example, ) with user-provided content. (). Runs commands you prefix with  (e.g., ) directly in the terminal . (). Fetches content from the web (HTML or JSON), enabling Gemini to analyze external data. (). Performs a Google search to ground responses with real-world information (e.g., explanation for an error). (). Stores facts or preferences during a session (like “I prefer async/await”) to improve consistencyTo see all available tools, you can use the  command.You can add specific instructions for the AI for a particular project by creating a  file in your project’s root directory. Inside this file, you can define project rules, code styles, and the tools the agent should use. This ensures that the generated code is consistent with your project’s standards.Google CLI MCP IntegrationFor most day-to-day uses, the built-in tools will suffice. But what if you want Gemini CLI to do something very domain-specific, like interact with specific APIs or use a specialized model (say an image generator or a security analysis tool)? This is where MCP (Model Context Protocol) comes in.MCP is essentially an open standard that allows developers to add new tools/abilities to the AI by running a server that the CLI can communicate with. In Gemini CLI, you can configure “MCP servers” in a JSON settings file, and the CLI will treat those as additional tools it can use.How to Set Up the MCP Server in Google CLIAs an example, I am going to show you how to set up the GitHub MCP server in Gemini CLI.Inside your project folder, create a folder by using the command:mkdir -p .gemini && touch .gemini/settings.json
Inside the file, add the following code:{  
  "mcpServers": {  
    "github": {  
      "command": "npx",  
      "args": ["-y", "@modelcontextprotocol/server-github"],  
      "env": { "GITHUB_PERSONAL_ACCESS_TOKEN": "[YOUR-TOKEN]" }  
    }  
  }  
}
After that  , from the Gemini CLI, and then reopen it.Write  command, and you will see a list of .Now you, Agent, can interact with GitHub. That simple! :)You can try it free for personal usage, but there is also a paid version that is billed based on token usage.: Free with Google account: Up to  and Great for individual developers and small-scale useUse your own  for higher usageBilled based on tokens consumed (model and usage dependent)Available through Gemini Code Assist Standard or Enterprise plansIncludes advanced features like governance, audit logs, and shared quotasAs you can see, Gemini CLI is a really powerful tool with a lot of potential. I’m excited to see how I’ll be using it in my daily workflow.If you write code, debug things, or manage files often, this tool is worth checking out.If you have any feedback, please share it in the comments below. ;)]]></content:encoded></item><item><title>AMD Preps Some Compute Driver Fixes For Polaris &amp; Hawaii Era GPUs With Linux 6.17</title><link>https://www.phoronix.com/news/Linux-6.17-AMDGPU</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:47:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD today submitted their initial batch of "new stuff" for queuing into DRM-Next of their kernel graphics/compute driver changes they have prepared for the upcoming Linux 6.17 cycle opening in a few weeks...]]></content:encoded></item><item><title>How Trusted Execution Environments Power Scalable, Private Smart Contracts</title><link>https://hackernoon.com/how-trusted-execution-environments-power-scalable-private-smart-contracts?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:45:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The layer-two solution is a straightforward approach that combines the TEE and blockchain to provide smart contracts with confidentiality while keeping scalability. In such systems, the operations of smart contracts are decoupled from their underlying blockchain systems. The smart contracts are executed in an independent layer outside blockchain systems.\
 In a general layer-two solution, the blockchain is used as a dispute resolution layer. The smart contract is executed outside the blockchain, making TEEs act as an agent between users and blockchain systems. Suppose that a user aims to use a private contract. She first needs to compile the original contract code, push binary codes to a TEE, and then upload execution results to the public ledger. As illustrated in Fig.4, we extract a generic data flow as follows. A user sends the encrypted input data to a TEE-powered node. Then, the TEE decrypts the input data and executes the contract. After that, the encrypted execution results are sent to the blockchain platform for verification and storage. Finally, the user fetches and decrypts the blockchain-confirmed results.\
Privacy-preserving property. The  is an essential property. In layer-two systems, such as [46], [68], [69], the contract computations run inside Intel SGX enclaves, while TZ4Fabric [44] moves contract executions into ARM Trusted Zone. Since the contract state-transition process happens inside TEEs, any intermediate states remain invisible to the outside. Meanwhile, to achieve the full lifecycle security for a smart contract, the input sent to a TEE and the output returned from this TEE are also required to be encrypted. For example, in ShadowEth [68], PDOs [46], Phala [69] and Hybridchain [41], the contract invocation arguments are encrypted with the TEE public key. They can only be decrypted within the enclave. Also, before transferring execution results to the blockchain (or users), the intermediate (or final) states in an enclave are encrypted. Some variants also enhance the privacy-preserving properties from other aspects. In Phala [69], only authorized queries to the contract will be answered. The smart contract source codes in ShadowEth [68] are hidden during the procedures of deployment and synchronization. This further reduces the possibility of data leakage in subsequent contract executions. Considering a fixed address may expose the user who has invoked the contract, PDOs [46] also allows the user to use pseudonym addresses for submitting a transaction (including TEE outputs) to the blockchain.\
Blockchain intrinsic feature. ShadowEth [68] and Taxa [70] introduce an external distributed service to manage the contracts, achieving the properties of code immutability, high availability and . Meanwhile, layer-two systems satisfy state consistency for reasons that the encrypted states of contracts in different blockchain nodes will eventually get consistent when reaching a successful agreement. Intuitively, the contracts deployed in layer-two systems should retain the features given by original blockchains. However, some fundamental properties are lost when using layer-two solutions. For example, most layer-two systems lose contract interoperability since each contract is executed in different machines. Among all the evaluated systems, only Phala [69] identifies this issue and proposes a command query responsibility segregation architecture to ensure certain interoperability. Also, public verifiability is a crucial property for the blockchain since it allows each contract invocation, and contract execution to be publicly verifiable. Unfortunately, contracts are executed in TEEs so that the outputs are encrypted. To check whether the TEE has executed contracts following loaded contract specifications is a non-trivial task.\
 An attacker may control the network between users and TEE hosts. Meanwhile, TEEs are assumed to always produce correct results, and the smart contracts inside TEEs cannot deviate from their specifications. The main difference compared with the assumption of layer-one systems is that an adversary can observe the network activities between the TEE interfaces and active blockchain nodes.\
 Several layer-two solutions adopt incentive or punishment mechanisms to encourage TEE hosts to provide a stable and secure environment for executing confidential contracts. For example, Fastkitten [43] and Erdstall [75], [76] propose  transactions, in which a host will be punished if its malicious behavior has been identified. In particular, if the TEE execution is aborted, the host will be charged according to previous deposits. In Taxa [70], every node can identify any faulty nodes with reliable proofs for executing further economic punishment. On another route, TEE hosts in Phala [69] will get paid by providing their computing resources to users. Similarly, the remuneration in ShadowEth [68] will be transferred to TEE hosts who execute private contracts. These mechanisms can effectively prevent malicious TEE hosts from an economic aspect. However, they are powerless against external threats. An adversary may directly terminate a TEE host at any time. Even worse, the TEE provides users with an open interface that is vulnerable to DoS [77] or single-point attack. To overcome such issues and achieve fault tolerance, different methods are proposed. Fastkitten provides low-level fault tolerance by periodically saving an encrypted snapshot of current states in enclaves. If the enclave fails, the TEE host can instantiate a new enclave and restart the computation starting from the encrypted snapshot. Similarly, Taxa [70] stores a session file for maintaining and recovering user’s requests. However, a malicious attacker may directly terminate the TEE host, and Fastkitten does not tolerate such host failures. Another technical route is to maintain a secure network. ShadowEth maintains a group of TEE nodes to ensure consistency via a Paxos-like [78] algorithm. Taxa adopts TEE-enabled computing nodes powered by a PBFT-derived PoS [79] algorithm. Any node in the network has the same responsibility to privately execute smart contracts and transfer execution results to the blockchain. However, this brings additional authentication issues. A TEE host must be carefully authenticated to ensure her TEE capability when joining an external network.\
Meanwhile, the systems PDOs [46], Phala [69], Ekiden [42] and COMMITEE [73] introduce an expendable and interchangeable solution. TEEs are stateless: any particular TEE can be easily replaced once it has clashed or finished its task. Unfortunately, these solutions are along with new challenges. Firstly, even if TEEs are changeable, detecting a compromised TEE is still difficult. For instance, PDOs can re-execute a method multiple times for the verification. Given the same input parameters to different TEEs, TEEs are believed to work securely only if their outcomes match. Then, the outputs of enclaves are allowed to commit to the blockchain. COMMITEE adopts  TEE host mechanism. If the master TEE host is proved to be malicious, a backup TEE host will continue to work without communications to the master TEE host. Nevertheless, this model increases the attack interface and makes the whole system vulnerable. Secondly, TEE hosts are stateless. That means, to ensure an exceptional execution is recoverable, any persistent state must be stored in the blockchain or a trusted third party (TTP). However, for a non-deterministic blockchain system such as Ethereum (PoS version) [2], verifying whether an item has been stored on the blockchain is a non-trivial task. Meanwhile, storing data in TTPs may lead to the single-point failure, which goes against the blockchain’s real intention.\
 A contract runs inside TEE, and heavily depends on remote attestation service. The SGX-supported blockchain systems including PDOs [46], Fastkitten [43], ShadowEth [68], Phala [69] and Ekiden [42] assume that Intel Attestation Service (IAS) is trusted. IAS can correctly and completely report whether a certain output with cryptographic material ( [80]) is produced by SGX-enabled hardware. However, IAS might be compromised, posing a risk to these architectures. A compromised or hijacked remote attestation service may maliciously report an attestation with the wrong cryptographic material that does not belong to its corresponding TEE hardware, breaking the promised security. Meanwhile, a centralized service might be crashed, causing the leakage of private states. Unfortunately, none of layer-two schemes consider these risks in designs or implementations.\
As discussed, current TEE implementations have memory limitations for confidential executions. If the memory usage exceeds the threshold, it may confront significant performance and security issues [81]. Hybridchain [41] optimizes the storage by maintaining transaction records outside Intel SGX. Meanwhile, TZ4Fabric [44] minimizes TCB by avoiding all the executions inside TEEs. However, these approaches increase the implementation complexity. A well-known fact is that a TEE is vulnerable to physical vulnerabilities [57]. Unfortunately, very few layer-two solutions provide remedial measures to reduce the risk of being attacked.\
 A poorly-written contract might deviate from designated functionalities and further leak the secret information. This part discusses the potential pitfalls and remedies when deploying contracts.\
In original smart contract systems, gas mechanism is a powerful tool to prevent  attacks [2]. Since the layertwo systems execute smart contract outside the blockchain, a similar mechanism must be considered. Fastkitten [43] and Hybridchain [41] protect against such attacks by using the  mechanism. Limitations are firstly defined on the maximum amount of execution steps that allow to perform inside a TEE per round. Then, TEE monitors smart contract operations. If the number of execution steps exceeds a predefined threshold, the enclave will terminate executions. ShadowEth [68] combines a timeout mechanism with a  mechanism. Similar to the gas mechanism in Ethereum [2], TEE hosts can still gain remuneration even if a contract exits after timeout since they provide sufficient computing power. These mechanisms effectively protect against endless loops and denial-of-service (DoS) launched by external attackers.\
The TEE itself lacks self-awareness of input data, since it cannot distinguish which state is fresh. A lack of input data authentication makes the system vulnerable to the rollback attack [82], [59]. A malicious user may attempt to invoke the confidential contract many times to seek the leaked secret information. Authentication of the user’s identity is helpful to prevent this attack. However, none layer-two solution provides these remedies for these potential pitfalls. On the other hand, the TEE input may come from a non-deterministic blockchain system [83], [84], in which deciding whether an input has been confirmed is tricky. Fastkitten [43] and COMMITEE [73] mitigate this issue by using a  mechanism. As for TEE output conflicts, Ekiden [42] uses a probabilistic proofof-publication protocol to avoid the ambiguous input.\
After the invocation of a private contract, the outputs returned from TEEs are uploaded on-chain for the final confirmation. But a malicious TEE host may send an exceptional result to the blockchain. Even worse, two hosts may publish different updates towards the same contract simultaneously. To prevent such malicious publications and to evade conflicts, PDOs [46] depends on Coordination and Commit Log (CCL) to manage the synchronization in the execution of interacting contracts and enables a contract owner to decide on selecting the enclave for contract executions, which effectively avoid conflicts. Phala [69] adopts an event sourcing command query responsibility segregation architecture to scale up and avoid conflicts, in which the write operations are recorded as events and read operations can be served by the current view of states. Again, these solutions contradict the property of decentralization. Ekiden [42] and ShadowEth [68] rely on the blockchain to resolve conflicts resulting from concurrency. In particular, ShadowEth [68] requires a worker to specify the version number with a timestamp when pushing data to the blockchain. Even miners accept different responses at first, they will eventually reach an agreement by comparing version number and the timestamp, with the help of the consensus procedure. Yet, such an approach is inefficient, especially in non-deterministic blockchain systems.\
 PDOs [46] uses a key provisioning service to distribute private keys. The drawback is obvious: A compromised provisioning service could make the entire system fail. To increase the robustness of a private key, Ekiden [42] designs a distributed key generation (DKG) [85] protocol using the secret sharing scheme [86]. Even if one key manager is compromised, an adversary cannot obtain the entire key. However, this solution does not completely solve the key leakage issue. The final keys are assembled and replicated among all end-TEEs. If an adversary compromises an end-TEE, exposing all the contract state becomes a trivial task. The key rotation technology, adopted by Ekiden [42], Fastkitten [43], Phala [69] partially solves the above issue by providing a short-term key in every epoch. An adversary cannot corrupt a future or previous committed state, which minimizes the possibility of key exposure to attackers and further helps the layer-two system to achieve forward secrecy. Also, layer-two projects such as COMMITEE [73] mitigate these key issues by providing each TEE per secret key. Even if a certain TEE’s private key were stolen, this only would affect the smart contract running on that compromised TEE. Furthermore, Phala Network [69], equips each contract with an asymmetric key called the , which also enhances the key security to a certain degree.\
The layer-two solution decreases computational burden and avoids latency by decoupling the smart contract executions from consensus mechanisms. The solution merely puts the execution results on-chain rather than all processing states. Meanwhile, the layer-two solution does not require a dedicated public ledger, meaning that such a solution can smoothly\
integrate with existing public blockchain platforms. Unfortunately, this method also brings security and functionality challenges when delegating the task of contract management to an external TEE layer.\
Firstly, the layer-two solution complexifies contract data management. The contracts that are deployed outside the blockchain require an external execution/storage party. A malicious storage maintainer may reject to provide the service, while a malicious host may abort TEE executions, terminate enclaves or delay/drop messages. Even an honest host might accidentally lose states in a power cycle. To solve the centralization issue and tolerate host failures, many countermeasures such as the TEE network, stateless TEEs and punishment mechanisms, are proposed. However, these solutions are not effortless, inevitably making the system complicated and hard to implement in practice.\
Secondly, the layer-two solution increases the attack surface and thus becomes vulnerable to rollback attacks. There is a high probability that an adversary node can revert transactions where temporary forks, representing inconsistent blockchain views, are allowed in blockchain systems with probabilistic consensus (e.g., PoW). Since TEEs provide no guarantee on verification of input data; they cannot distinguish whether an input state is fresh or not. An attacker may offer stale states to resume a TEE’s execution. This enables rollback attacks against randomized TEEs programs. Even worse, plugging up these loopholes needs much effort.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Layer-One Confidential Smart Contracts: Architecture, Threats, and Tradeoffs</title><link>https://hackernoon.com/layer-one-confidential-smart-contracts-architecture-threats-and-tradeoffs?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The layer-one approach enables blockchain nodes to run contracts in their isolated areas, as well as conducting the consensus (see Fig. 3). This approach combines the consensus procedure and state execution, either in terms of logically or physically. The reason why we call this method layer-one is that all executions are completed in the same layer of the blockchain network. The key to such an approach is to equip every blockchain node with a TEE. Indeed, this requires more integration efforts, but also comes with several advantages. The smart contract can implement stateful functionalities that receive arguments and update states instantly. In particular, a smart contract can directly access the ledger data stored in a local disk, greatly saving time often wasted in the interactive network communications.\
 In a layer-one execution model, the operation of ledger update (consensus) and state transition (contract execution) are coupled. Like Ethereum [2], smart contracts run inside blockchain nodes. Assume that a user plans to use the private contract; she only needs to upload data to the blockchain service and wait for results. The remaining procedures are completed by TEE-assisted distributed nodes. A TEE in these nodes acts as a black box for data processing and output targeted results without the data leakage. This approach greatly improves convenience for users due to its easy access and management. As illustrated in Fig.3, a generic data flow goes as follows: A contract creator deploys the code into blockchain. Then, a user sends the transaction with an encrypted argument to an arbitrary blockchain node. Her request is confidentially executed inside TEEs in this node and output encrypted state. Then, the consensus algorithm in this node broadcasts the encrypted results to peers. After the encrypted results are confirmed by other blockchain nodes, users fetch on-chain results and decrypt them for the plaintext.\
Privacy-preserving property. This property indicates that contract states and the procedure of contract executions are hidden from the public. To achieve privacy, layer-one systems execute these confidential contracts inside TEEs in every distributed node. CCF [45], Fabric [60] and CONFIDE [37] follow this straightforward design where confidential contracts are loaded to the TEE of each consensus node, which encrypts both the inputs and outputs of contract states, together with their operating logic and predefined rules. Enigma[1] [61] introduces the secret network and allows users to submit their transactions together with encrypted data to miners. We also notice that current layer-one solutions only focus on internal procedures rather than the linkability and anonymity of addresses and transactions. This indicates that confidential smart contracts only protect the contents that have been loaded into TEEs, while the data that relates to external users is out of the scope of this work.\
Blockchain intrinsic feature. The layer-one systems inherit most of the features empowered by blockchain. More precisely, the properties of code immutability, high availability, explicit invocation, decentralized execution, automatic execution and  remain the same because basic contract executions still rely on their underlying blockchain systems. Also, the property of (confidential)  in Enigma [61], CCF [45] and Fabric [60] remains unchanged. The states and executions from these systems follow the procedures of online consensus processes. Then, the returned results from inside TEEs still require to be confirmed on-chain. This makes their actions effectively perform the same functions as a normal smart contract, except for that the contents of states are transmitted from plaintext to ciphertext. In contrast, the property of contract interoperability is lost since the contracts are executed in isolated TEEs. This isolation requires additional communications such as dispatching keys through the remote attestation service, bringing much complexity.\
The layer-one solution encapsulates TEE computations into blockchain nodes. Every node in the network has to take responsibility for conducting confidential executions and performing the consensus. The design to coordinate TEEs and consensus within the same physical space brings many distinguished features. We start the analysis from their threat model and then dive into each component of these systems.\
 Users in the layer-one approach are assumed to be unreliable. They may have mistakes unconsciously, like dropping messages or mis-sending transactions. Even worse, a malicious user can arbitrarily behave like faking messages, identities, or compromising other nodes. As for TEE hosts, an external attacker can monitor, eavesdrop or even compromise part of involved TEE hosts among these distributed nodes, but cannot block all traffic transmitted in communication channels. Subsequently, a TEE is supposed to work in a good condition: The attestation service is trusted, and the cryptographic primitives used inside TEEs are secure. Meanwhile, as for the blockchain network, the basic systems (ledgers) are assumed to be robust [62], [63], [64]. When running the consensus, the majority (might be two-third, depends on specific consensus algorithms) of nodes are assumed to be honest [65]. Also, forging smart contract codes or states will happen in honest blockchain nodes with a negligible possibility. Based on that, we analyse securities from four aspects.\
 Firstly, we focus on the security of TEE hosts, or equally, individual nodes that run TEEs. Unlike classical blockchain systems, there are no explicit incentive or punishment mechanisms in this solution. This is easy to understand: A node with malicious behaviors will be instantly moved out of the committee and replaced by a new honest participant. Meanwhile, due to the fact that CCF [45] and Enigma [61] rely on Tendermint (a BFT variant) consensus algorithm, they can tolerate at most one-third of TEE Byzantine nodes. But the sacrifice is the increased difficulty in synchronization, especially when every node has to establish a secure channel for communications of distributed TEEs. In layer-one systems, host authentication is necessary. The node who wants to join the committee has to obtain permission from communities by proving her TEE capability. For instance, CONFIDE [37] builds a mutual authenticated protocol (MAP) (supported by SGX remote attestation techniques [66]) among blockchain nodes. Any nodes joining in the network have to pass the authentication via MAP.\
 Then, we analyse TEE-level securities. Attestation service is an essential part of TEE techniques. Systems in the layer-one solution still require such services for network connection and verification. Enigma [61], Fabric [60] and CCF [45] follow the original attestation mechanism with an implicit rule: The Intel Attestation Service (IAS) should be reliable. However, this cannot be guaranteed in the case of IAS being comprised. In contrast, CONFIDE [37] utilizes a customized Decentralized Attestation Service to provide the robust authentication. As for memory limitations, layer-one systems load contract executions and consensus algorithms into one TEE-embedded node, causing an increase in disk and memory usage of individual nodes. Once the usage of TEE memory runs over the predefined settings, a decrease in the performance is inevitable [34]. This may further cause an unpredictably severe result like system crash-down. Fortunately, Fabric [60] mitigates such issues by separating the operations into two types (execution and ordering) and delays the transaction- procedures after state-. Among them, only the state- parts are processed inside TEEs. This decreases computation complexity and limits the memory usage to a suitable range. Physical attacks like the Spectre and Meltdown vulnerabilities [57] are intrinsic design pitfalls that may occur inside the TEE kernel. To our knowledge, no layerone solutions mention them or provide the remedies.\
TEE program security. Next, we focus on the program-level security. Issues like overburdening may frequently happen, especially when a malicious developer deploys a contract with infinite loop logic. Unlike using the gas mechanism in Ethereum [2], systems in the layer-one model constrain their running programs by the  mechanism. It sets a threshold, namely, a suitable range of time that allows processing contract operations. When exceeding the timebound, the system will abort under-processing states and restart a new round. As for the flaw detection, no formal techniques or verification tools, based on our observation, have been applied to layer-one systems. This gap needs further exploration. Similar to the previous discussion, the properties of data verification (covering both user data authenticity and blockchain data confirmation) and output conflicts are guaranteed by their underlying consensus algorithms. Each time performing the consensus, these properties are automatically checked. For instance, Enigma [61] relies on trusted validators, who equip with TEEs to conduct the verification procedure. Such validators maintain both the privacy of executions inside TEEs and the consistency of states that connects to peers. Once conflicts occur, validators will quickly make decisions on a block and remove another conflicting block. Fabric [60] performs such a process inside TEEs among committee nodes and then submits the passed results to its abstract ordering service. This service prevents forks caused by conflicting states, as well as proving a fact that: All executed messages are valid and integral once reaching the consensus agreement. It should be noted that, successful consensus procedures can merely guarantee the integrity of transactions and states, rather than linkability and authenticity that relates to physical entities.\
 Lastly, we move to the aspect of TEE key management. In layer-one systems, the key management service takes over the task of creating and managing keys for activities like attestation, verification, encryption, etc. To achieve the key management service among distributed nodes, several types of designs have been proposed. CCF [45] relies on the public key infrastructure (PKI) for certificate issuance, management, and revocation. It creates key pairs and dispatches them to every participated TEE, where each TEE holder is authenticated by the certificate. Similarly, Fabric [60] adopts an admin peer to provision the specific decryption key to  during bootstrapping. Enigma [61] setups an independent key management component to reply to the requests for encryption. Such designs help to simplify complex management procedures, as well as providing distinguishable keys for each TEE. However, these independent key management services lead to centralization even they are maintained by a group of nodes in the committee. CONFIDE [37] mitigates this issue by proposing a decentralized key management protocol. Two types of keys are involved in this protocol: the  used to decrypt confidential transactions from clients and the  used for state encryption/decryption between the confidential engine and storage service.\
The layer-one solution provides a highly integrated approach towards confidential smart contracts.\
The layer-one solution provides a consistent interface for users without changing the customer’s habits transformed from non-TEE blockchain systems. A user can use the layer-one system by directly interacting with the blockchain interface, without considering cumbersome and complicated operations between the TEE and blockchain. However, the layer-one solution still confronts several common disadvantages.\
Minimizing the size of Trusted Computing Base (TCB) contributes to the TEE security [67]. In particular, a small TCB has fewer errors and can reduce attack surfaces. However, complicated interactive operations for contract execution and consensus agreement in the L1 solution greatly increase the size of TCB. Meanwhile, TEE products have limited secure memory. For example, in the current implementation of Intel SGX [35], the enclave page caches are constrained to 128 MB, and only 93 MB of those is available for applications, which limits the concurrent execution.\
Furthermore, the layer-one solution lacks compatibility, which means being incompatible with existing blockchain systems. The solution integrates the consensus procedure and the contract execution into the same blockchain node, requiring every node having to equip a TEE hardware. Nevertheless, this requirement is difficult to be fulfilled in a public blockchain while already in use (e.g., Ethereum [2]).(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.[1] Enigma’s secret network consists of a list of secret nodes equipped with TEE, which is categorised as a layer-one solution in the context of our definition (Sec.III-A). We also note that such a secret network can be regarded as a layer-two solution in the traditional classifications in terms of Ethereum, namely, either on-Ethereum chain (L1) or off-Ethereum chain (L2).]]></content:encoded></item><item><title>What Most Blockchain Devs Get Wrong About TEE Security and Smart Contract Privacy</title><link>https://hackernoon.com/what-most-blockchain-devs-get-wrong-about-tee-security-and-smart-contract-privacy?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[III. SYSTEMATIZATION METHODOLOGYTo find common aspects (e.g., offered functionality, design model, adversary model), we extract recurring design patterns from publicly available publications and projects, focusing on systematization and evaluation of desirable properties (the main target of TCSC) and potential pitfalls of underlying systems. Our systematization methodology follows the idea in [52]: classification and evaluation. We firstly make a classification for the current systems and then define a framework to evaluate them. Details are presented as below.\
\
We classify the existing systems into two main categories: layer-one solution (L1) and layer-two solution (L2). The layer-one solution executes the contract inside a TEE in the blockchain, requiring every blockchain node to equip a TEE. Instead, the layer-two solution decouples contract computations from the blockchain. It performs most of the smart contract computations off-chain. For a clear understanding, we make a comparison of the original blockchain (e.g., Ethereum), L1 solution, L2 solution. As in Tab.II, Ethereum runs smart contracts (in EVM) and consensus procedures in the same machine of distributed nodes. All the contract and transaction operations are publicly verifiable due to their total transparency. The layer-one solution performs such operations (contract execution and consensus) in the same machine, but contract operations are separate from consensus procedures. In contrast, the layer-two solution makes both of them operate independently. Contracts are executed outside the blockchain network, while the consensus happens inside each node.\
Ideally, moving smart contract executions into TEEs brings additional privacy as well as maintaining the original benefits of blockchain systems. Therefore, we have identified the desirable properties in two main categories: privacy-preserving property and .\
Privacy-preserving property. The property of confidentiality is the most distinguished feature in TCSC.\
A1. Specification hidden. The source code of a smart contract is hidden during the deployment and the subsequent synchronization and execution.\
 The inputs fed into a confidential smart contract are hidden from the public.\
 The outputs returned from a confidential smart contract should be kept private.\
 The execution procedure is hidden from unauthorized parties. An adversary cannot learn the operation knowledge inside a TEE.\
A5. Address unlinkability. The address pseudonymity does not entail strong privacy guarantees [53], [54]. This property prevents an adversary to learn the address linkability by observing users’ activities.\
 The contract caller’s identity (a user who invokes a smart contract) is hidden from an anonymity set [24] (see Appendix B).\
Blockchain intrinsic feature. TEE-assisted smart contracts inherit features given by original blockchain systems. We summarize these features as follows.\
 Once a contract is successfully deployed, its source code cannot be altered.\
A8. (Confidential) state consistency. Executions happening at a certain blockchain height will output the same result across different nodes.\
A9. Contract interoperability. A smart contract can call another contract and be called by others.\
 A smart contract is continuously accessible without the single point of failure.\
A11. Decentralized execution. A smart contract runs over the decentralized network.\
A12. Automatic execution. A smart contract can be automatically executed once conditions are satisfied.\
 Operations running on the smart contract will be charged with gas fees [2].\
A14. Explicit invocation. Each invocation will be formatted as a transaction and stored on blockchain.\
A15. Public verifiability. The procedure of contract execution and result are publicly verifiable.\
A16. Consensus verifiability. The consensus procedure on the (confidential) state is publicly verifiable.\
Essentially, all TCSC systems share the same principle: a TEE will handle the data from users. After that, encrypted data flows from the  The TEE plays a crucial role. Thus, this part defines a framework for evaluating underlying blockchain systems from four aspects: , and  This framework aims to identify potential design flaws and pitfalls based on the threat model and data workflow.\
 Our threat model mainly captures three types of attackers, which are stated as follows.\
T1. User adversary (active/passive). An attacker may control network between users and TEE host nodes.\
T2. TEE adversary (active/passive). An adversary may control TEE hosts or control the network between TEE and blockchain platforms.\
T3. Blockchain adversary (active/passive). An adversary may drop, modify and delay the blockchain messages. But the majority (or two-thirds) of the blockchain nodes are assumed to be honest.\
Note that adversaries are not necessarily exclusive. In some cases, adversaries in different types may collude.\
 This section defines four metrics regarding system security according to the data workflow: approaches to enhance the security of a TEE host, countermeasures to mitigate intrinsic TEE issues, methods to prevent program flaws or bugs inside TEEs, and solutions to clear up the TEE key security dilemma.\
 A TEE and its interaction with the external environment (e.g., with users or the blockchain) are operated and controlled by a host (such as a L1 blockchain node). A malicious host has abilities to abort the executions of a TEE, delay and modify inputs, or even drop any ingoing or outgoing messages. The following metrics discuss the approaches to improve the TEE host’s security.\
P1. Host punishment mechanism. Penalty mechanisms to reduce the risk of doing evil by a TEE host.\
P2. Host incentive mechanism. Incentive mechanisms to promote a TEE host to behave honestly.\
P3. Host fault tolerance. Solutions to make systems continually operate despite malfunctions or failures.\
 Methods to check the identity and the capability of a TEE host.\
 A TEE has inevitable weaknesses. For example, a TEE is vulnerable to side-channel attacks [55], [56]. These innate weaknesses directly pose severe challenges to the design and implementation of TEE-assisted contract systems. This part defines the defence approaches against these threats.\
P5. TEE attestation security. Methods to prevent TEE attestation service from being abnormally broken.\
P6. TEE memory limitation. Methods to optimize the memory size to prevent confidential data overflow.\
P7. TEE physical attacks. Approaches to prevent physical attacks, such as the Spectre vulnerability or the Meltdown vulnerability [57].\
 Approaches to provide a trusted timer when running a TEE.\
 Even a TEE is secure as assumed, a program bug may destroy the contract’s confidentiality in the real world. This part focuses on the measurements to prevent TEE programs from flaws or bugs.\
P9. Workload measurement. The workload measurement approach to prevent an infinite loop attack.\
 Formal techniques used for the modelling and verification of the source code of smart contracts to reduce the vulnerabilities.\
P11. User query restriction. The restriction on users’ queries, aiming to avoid data leakage resulting from differentialprivacy analysis [58].\
P12. Blockchain data confirmation. Methods for a TEE to check whether input data from blockchain has been confirmed to prevent the rollback attack [59].\
P13. TEE output conflicts. Methods to avoid multiple TEEs to produce a conflict result.\
 Various keys (cf. Appendix A) are involved in the contract execution, including TEE internal keys such as the attestation key and TEE service keys for state encryption/decryption. Since service keys directly affect the protection of contract states, the key security evaluation in this SoK mainly focuses on the generation, exchange, and storage of the TEE service key.\
P14. Distributed key protocol. The keys of confidential contracts are managed by a distributed protocol.\
P15. Key rotation protocol. The TEE replaces an old key with a fresh key for future contract encryption.\
P16. Independent contract key. Each contract is associated with a unique key, independent from the TEE.\
P17. Independent TEE key. Each TEE has a unique key, and different contracts share the same key.\
 The  shows a general view of the TCSC systems. Desirable property focuses on evaluating contract service provided by a TEE-assisted blockchain system.  describes the potential threats and system assumptions.  show the evaluating indicator for current TEE-assisted systems. In the following section IV-B and V-B, we attempt to answer the following questions: (i) What are the potential pitfalls in each security aspect; (ii) Do these pitfalls have significant security impacts; (iii) Do the designers/developers consider these pitfalls and accordingly come up with feasible remedies in their systems; (iv) What are the remedies and do they address above problems. Note that hundreds of TCSC systems have been proposed in both industry and academia. An exhaustive analysis is undesirable and infeasible. We only selected the projects that provide publicly accessible technical reports or academic papers.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Four Key Steps to Confidential Smart Contract Execution</title><link>https://hackernoon.com/four-key-steps-to-confidential-smart-contract-execution?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This section gives a high-level description and offers a running example to illustrate how a typical confidential smart contract operates. From existing literature [42], [43], [41], [44], [45], [46], establishing a confidential smart contract mainly requires four steps, namely , ,  and  (see Fig. 1).\
From a bird’s eye view, a TCSC can be used as an ideal contract-based black box [47] with secrecy and correctness. This idea has been adopted by several advanced security protocols [48], [49]. We provide a secret e-voting example borrowed from Oasislabs [50].\
A TCSC can be well qualified for the role of decentralized vote manager in an e-voting system [17], [51]. Once a contract-based manager is deployed successfully, the voting logic is loaded into a TEE and corresponding secret keys are privately generated and stored inside TEEs. The encrypted state is then confirmed by the blockchain nodes. This offers the e-voting protocol with confidentiality, neutrality, auditability and accountability. Firstly, the voter’s input cu is encrypted, and intermediate parameters (e.g., mb) are privately processed through TEEs. External attackers cannot obtain the knowledge of sensitive information, and thus the confidentiality is achieved. Secondly, the predefined voting logic only occurs in the decentralized network when certain conditions are satisfied, bringing neutrality for the access control management. Thirdly, if a voter wants to vote for a candidate, she needs to in advance build a channel to the TEE and then send a transaction Tx to call the contract. Due to the protection of encrypted channels, transaction arguments are kept secret. Meanwhile, such invoking records in the form of transactions remain visible and will become immutable, ensuring the voting process accountable. Unfortunately, verifiability, as one of fundamental properties, performs not smooth in the context of encryption. Contracts that are executed inside TEEs make the execution procedures lack public verifiability. Only the nodes who install TEEs with correct corresponding keys can verify the correctness of contract executions. However, the metadata of the transaction Tx retains unencrypted, making it possible to verify the absence of double spending.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>To Fix Smart Contracts, Start With Their Secrets</title><link>https://hackernoon.com/to-fix-smart-contracts-start-with-their-secrets?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.\
—The blockchain-based smart contract lacks privacy since the contract state and instruction code are exposed to the public. Combining smart-contract execution with Trusted Execution Environments (TEEs) provides an efficient solution, called TEE-assisted smart contracts, for protecting the confidentiality of contract states. However, the combination approaches are varied, and a systematic study is absent. Newly released systems may fail to draw upon the experience learned from existing protocols, such as repeating known design mistakes or applying TEE technology in insecure ways. In this paper, we first investigate and categorize the existing systems into two types: the layer-one solution and layer-two solution. Then, we establish an analysis framework to capture their common lights, covering the desired properties (for contract services), threat models, and security considerations (for underlying systems). Based on our taxonomy, we identify their ideal functionalities, and uncover the fundamental flaws and reason for the challenges in each specification’s design. We believe that this work would provide a guide for the development of TEE-assisted smart contracts, as well as a framework to evaluate future TEE-assisted confidential contract systems.Smart contract was originally introduced by Szabo [1] and further developed by Ethereum [2] in the blockchain systems. The blockchain-based smart contracts [3], [4], [5] adopt Turing-complete scripting languages to achieve complicated functionalities [6] and execute the predefined logic through state transition replication over consensus algorithms to realize final consistency. Smart contracts enable unfamiliar and distributed participants to fairly exchange without trusted third parties, and are further used to establish a uniform approach for developing decentralized applications (DApps [7]). However, blockchain-based smart contract lacks confidentiality. The state information and the instruction code are completely transparent [8], [9], [10]. Any states with their changes are publicly accessible and all users’ transaction data and contract variables are visible to external observers. Without privacy, building advanced DApps that rely on the user’s sensitive data becomes a challenge [11], [12], [13], [14]. For instance, smart contracts in Ethereum [2] cannot be directly applied to Vickrey auction [15], [16] or e-voting systems [17], [18], where the bid and vote require to be hidden from the public. Moreover, DApps without privacy protection might be prohibited by European Union because they go against the General Data Protection Regulation [19], [20]. Thus, the complete transparency of smart contracts constrains their wide adoption. Recently, researchers have explored many cryptographic solutions to solve these issues, including utilizing techniques of zero-knowledge proof (ZKP) [21], [22], [12], [23], [24], [25], homomorphic encryption (HE) [26] and secure multiparty computation (MPC) [27]. However, these approaches are merely applicable to applications requiring simple computations.\
Although various TCSC protocols have been proposed, newly released projects may fail to draw upon the experience learned from existing protocols, such as repeating known design mistakes or applying cryptography in insecure ways. For example, an absence of economic incentives will pose security risks and decrease the protocol’s stability. However, the recentproposed TCSC scheme Hybridchain [41] repeats similar pitfalls by simply combining the TEE with a permissioned blockchain network, omitting considerations on the miner’s incentive mechanism. The repeating of pitfalls comes from twofold. Firstly, in-the-wild projects differ from one to another, and a relatively unique model is absent, which narrows the developers’ vision. Meanwhile, a unified evaluation framework is missing, causing many security threats to be uncovered and resulting in considerable loss from applications underpinning the execution of confidential smart contracts. This paper aims to abstract a high-level framework to simply and clearly systematize knowledge on current TCSC schemes. We attempt to capture some commonalities among these projects regarding their features, properties, and potential security vulnerability. We believe that establishing evaluation criteria to measure features and identify problems and flaws of existing TCSC protocols will offer a good guide for industry communities and promote the DApps prosperity. Main contributions (a visualized guideline in Fig.2) are:\
• We provide a systematization of existing TCSC systems driven from academic work and  Based on their operating mechanisms and ways of combination, we investigate and categorize a set of typical protocols into two main classifications: the  solution and the  solution.\
• We establish a unified evaluation framework for confidential smart contract systems. We consider two parts: the smart contracts used as , and underlying supported blockchain systems. Accordingly, the framework covers three aspects:  for contract services,  and  for underlying systems. Specifically, we discuss two different types of desirable properties:  that inherit from traditional smart contracts and featured privacy-related properties. Then, we emphasize practical issues, pitfalls, and remedies in designing TEE-assisted blockchains from four aspects ( securities and  services).\
• We conduct a comparative analysis of existing protocols based on our evaluation framework. We discuss systems both from their  (system classification, threat model) and  (designs, properties). The common designs show us the consistent idea when re-designing the system, while the distinguished features highlight the ingenuity of each system design that deviates from others (see Tab.III/Tab.IV).\
• We further give a comprehensive discussion of current designs and implementations, including a running example, comparisons between layer-one and layer-two systems from the perspectives of ,  and , and common issues on . Unfortunately, a mature design is still not ready for large-scale applications. We thereby point out research  in this field, wishing to give insights for communities on defining their models and discovering possible solutions of designing TCSC systems.\
The rest of the paper is organized as follows. Sec.II gives a high-level introduction on how to operate a confidential smart contract inside TEEs. Sec.III provides the systematization methodology ( and ). Layer-one and layer-two systems are analysed in Sec.IV and Sec.V, respectively. Discussions are provided in Sec.VI. Research challenges are summarised in Sec.VII. Finally, Sec.VIII gives concluding remarks. Supplementary details are stated in Appendix A-D.]]></content:encoded></item><item><title>Road to Battlefield: Central Eurasia’s gateway to TechCrunch Startup Battlefield</title><link>https://techcrunch.com/2025/07/01/road-to-battlefield-central-eurasias-gateway-to-techcrunch-startup-battlefield/</link><author>Cindy Zackney</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:41:37 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Historic regional competition launches to showcase Central Eurasia’s rising startup ecosystem on Silicon Valley’s biggest stage. For the first time in its history, Central Eurasia will have a direct pathway to TechCrunch Startup Battlefield through the launch of “Road to Battlefield,” a groundbreaking regional competition that promises to put the underrepresented region firmly on the […]]]></content:encoded></item><item><title>Tech Hobbyist Destroys 51 MicroSD Cards To Build Ultimate Performance Database</title><link>https://it.slashdot.org/story/25/07/01/155208/tech-hobbyist-destroys-51-microsd-cards-to-build-ultimate-performance-database?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Tech enthusiast Matt Cole has created a comprehensive MicroSD card testing database, writing over 18 petabytes of data across nearly 200 cards since July 2023. Cole's "Great MicroSD Card Survey" uses eight machines running 70 card readers around the clock, writing 101 terabytes daily to test authenticity, performance, and endurance. 

The 15,000-word report covering over 200 different cards reveals significant quality disparities. Name-brand cards purchased from Amazon performed markedly better than identical models from AliExpress, while cards with "fake flash" -- inflated capacity ratings -- performed significantly worse than authentic storage. Sandisk and Kingston cards averaged 4,634 and 3,555 read/write cycles before first error, respectively, while Lenovo cards averaged just 291 cycles. Some off-brand cards failed after only 27 cycles. Cole tested 51 cards to complete destruction during the endurance testing phase.]]></content:encoded></item><item><title>You will own NOTHING and be HAPPY! (SKG)</title><link>https://www.youtube.com/watch?v=rAsgjKBkKMA</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/rAsgjKBkKMA?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 20:35:53 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[Please sign if you're in the EU or UK!
EU: https://eci.ec.europa.eu/045/public/#/screen/home
UK: https://petition.parliament.uk/petitions/702074/

Things I mentioned in this video:

  - Stop Killing Games: https://www.stopkillinggames.com
  - My video takedown (and getting a 2nd appeal): https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful-updated
  - Bosch Dishwasher video: https://www.youtube.com/watch?v=5M_hmwBBPnc
  - XKCD on WiFi appliances: https://xkcd.com/3109/
  - Dead game list: https://stopkillinggames.wiki.gg/wiki/Dead_game_list

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff

Contents:

00:00 - Controlling your own destiny. And dishwasher.
01:27 - Games (please stop killing them)
03:35 - What you can do (in the EU)]]></content:encoded></item><item><title>X is piloting a program that lets AI chatbots generate Community Notes</title><link>https://techcrunch.com/2025/07/01/x-is-piloting-a-program-that-lets-ai-chatbots-generate-community-notes/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:26:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The social platform X will pilot a feature that allows AI chatbots to generate Community Notes, a Twitter-era feature that Elon Musk has expanded under his ownership of the service now called X.]]></content:encoded></item><item><title>Trump Launches America’s Newest Concentration Camp, Complete With Tacky Merch</title><link>https://www.techdirt.com/2025/07/01/trump-launches-americas-first-concentration-camp-complete-with-tacky-merch/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:06:47 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Not content with just shipping people to a foreign concentration camp, Donald Trump now has his own, homegrown concentration camp in Florida. Trump, DHS Secretary Kristi Noem, and Florida Governor Ron DeSantis gleefully toured the hastily constructed concentration camp in the Florida Everglades, obnoxiously referred to as Alligator Alcatraz, in reference to (1) the infamous island prison in San Francisco that Trump is obsessed with and (2) the number of alligators (and crocodiles — the one place in the world that has both) that live in and around the Everglades.There’s no way to look at what the US government is doing here and not think of it more as Auschwitz than Alcatraz. The parallels are unmistakable: hastily constructed camps in remote locations, euphemistic naming designed to obscure their true purpose, and—most tellingly—officials proudly touring the facilities while discussing plans to build “a system” of such camps nationwide.But here’s where today’s American concentration camps differ from their 20th-century predecessors: the Trump regime isn’t trying to hide what they’re doing. They’re merchandising it. They’re selling t-shirts celebrating human suffering as if it were a sports team or a vacation destination.The United States government is literally selling branded merchandise to celebrate putting human beings in cages surrounded by dangerous predators. This isn’t just about policy—it’s about turning cruelty into a consumer product. It’s about making the suffering of others into something you can wear to own the libs.This commodification of human rights violations represents something uniquely American and uniquely horrifying: the gamification of genocide. Previous authoritarian regimes at least had the decency to be ashamed of their concentration camps. Trump is selling tickets to the show.These are the sorts of things that history books (should they exist in the future) will talk about as one of the many moments of pure evil that some people gleefully embraced without recognizing that people setting up concentration camps are, inherently, “the baddies.”For what it’s worth, Trump did little to dispel the notion that this is part of his new fascist campaign to imprison anyone who disagrees with him. During the tour, Trump and Noem talked about prosecuting CNN for their reporting and for releasing an app that alerts people to where ICE agents are located (both of which would violate the First Amendment, if it were still a thing anyone believed in).Trump admitted that he had brought up this idea as a joke, but his idiot advisors ran with it:“Is this a dream come true for you, sir” a reporter asks.“It was meant more as a joke, but the more I thought of it, the more I liked it… they were actually crocodiles,” Trump said.“We’d like to see them in many states. At some point, they might morph into a system,” Trump said on Tuesday.A “system.” The word choice isn’t accidental. This is the language of industrial-scale human rights violations, spoken with the same casual tone you’d use to discuss a chain restaurant expansion.In case you’re wondering how much it costs to go full Nazi, this one concentration camp will cost the American taxpayer nearly half a billion dollars a year. That money will come from FEMA, the organization that Trump (with an assist from former friend Elon Musk and DOGE) stripped budget from, meaning there will be even less to pay for actual emergencies, because all of that money will be used to jail people Trump doesn’t like in a swamp.The Everglades facility will cost Florida some $450 million to run for one year, according to DHS, though much of that will be reimbursed by the Federal Emergency Management Agency (FEMA). While the airstrip is owned by Miami-Dade County, where officials have viewed the plan with skepticism, DeSantis is using his emergency authority to proceed on a tight schedule.We are watching the latest march forward of American fascism in real time, complete with branded merchandise and gleeful photo ops. The US government is building concentration camps and selling t-shirts about it. This isn’t hyperbole. This isn’t partisan hysteria. This is what’s actually happening.Every day you don’t call this what it is—fascism—you become complicit in normalizing it. Every time you treat this as just another political story, you help them make it routine. They’re counting on your exhaustion, your normalization, your willingness to look away.The survivors of the Holocaust warned us this could happen again. They’re mostly gone now, but their warnings echo: it starts with camps, it starts with dehumanization, and it starts with good people doing nothing while evil wraps itself in flags and sells t-shirts.]]></content:encoded></item><item><title>AT&amp;amp;T Now Lets Customers Lock Down Account To Prevent SIM Swapping Attacks</title><link>https://tech.slashdot.org/story/25/07/01/181213/att-now-lets-customers-lock-down-account-to-prevent-sim-swapping-attacks?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AT&T has launched a new Account Lock feature designed to protect customers from SIM swapping attacks. The security tool, available through the myAT&T app, prevents unauthorized changes to customer accounts including phone number transfers, SIM card changes, billing information updates, device upgrades, and modifications to authorized users. 

SIM swapping attacks occur when criminals obtain a victim's phone number through social engineering techniques, then intercept messages and calls to access two-factor authentication codes for sensitive accounts. The attacks have become increasingly common in recent years. AT&T began gradually rolling out Account Lock earlier this year, joining T-Mobile, Verizon, and Google Fi, which already offer similar fraud prevention features.]]></content:encoded></item><item><title>Catalio Capital closes over $400M Fund IV</title><link>https://techcrunch.com/2025/07/01/catalio-capital-closes-over-400m-fund-iv/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Catalio Capital Management announced the closing of its more than $400 million Fund IV. The fund will continue the firm’s thesis of backing healthcare and biotechnology companies. ]]></content:encoded></item><item><title>Google’s data center energy use doubled in 4 years</title><link>https://techcrunch.com/2025/07/01/googles-data-center-energy-use-doubled-in-four-years/</link><author>Tim De Chant</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:52:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google has pledged to use only carbon-free sources of electricity to power its operations, a task made more challenging by its breakneck pace of data center growth.]]></content:encoded></item><item><title>7 Iconic TV Characters Whose Names Remain a Mystery</title><link>https://hackernoon.com/7-iconic-tv-characters-whose-names-remain-a-mystery?source=rss</link><author>Fayam Ayekame</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:37:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Names are one of the most prominent features of TV, holding an almost spiritual significance. Likewise, the absence of a name can also be a powerful tool, adding mystery to your favourite characters. Whether accidental or intentional, several beloved TV characters have remained nameless and managed to be loved by fans regardless. Here are some of the most prominent small-screen characters who remain nameless throughout their shows’ run:That ‘70s Show produced several comedic gems and running tropes over its eight-season run. Through it all, one of the key features on the show was Fez, the foreign exchange student who wasn't quite exchanged back. The lovable foreigner quickly became a fan favourite with his witty comebacks and his ever-elusive search for love.Fez’s real name is mentioned during the show, but school bells drown it out in keeping with the running Gag. His home nation, on the other hand, is never mentioned, with further confusion when his best friend from home comes to visit, and is unmistakably British. Penny - The Big Bang TheoryPenny was the ultimate girl next door, bringing socialisation skills and down-to-earth relationships to her scientist neighbours. While her first name is a common feature in the world of TV, her last name remains a mystery to date. Later in the series, she takes on Hofstadter after her marriage to Leonard, but her original family name is never mentioned. Despite her entire family making an appearance on the show, the family name is miraculously left out, creating the ultimate running gag. It's no surprise to see that the longest-running show on TV has the longest-running gag, “What is the doctor’s name?” This trope is also a play on the show’s title, with the question hidden in plain sight, “Dr Who”. While he is known throughout the cosmos as “The Doctor”, the man from Gallifrey has a name, which is never actually disclosed. The title of “Doctor” was chosen as a promise to the universe and has since become his identity. Throughout the series, many have sought to find out his actual name, with only  achieving this objective. We are introduced to our mystery man, Lucas Hood, in the very first episode of Banshee. After witnessing Lucas Hood’s name, our main character assumes his identity and position as the new sheriff of Banshee. The new Lucas Hood brandishes a unique brand of justice throughout the series, as he attempts to honour the badge he now wears. While there are characters who know his real name, it is never revealed, and his true identity remains a mystery. A brief glance into his past reveals a name on file as , which is just another aliasJohn Reese - Person Of InterestPerson of Interest lasted five seasons and introduced us to the computing power of AI with  and  at the centre of it all. John Reese is an ex-CIA operative whose real name has long been buried, operating under an alias, and running errands for “”. Despite assuming multiple identities throughout the series, John Reese remains the central identity of our main character. Rip Hunter - DC’s Legends of TomorrowDC’s Legends of Tomorrow revolves around a group of time travellers roaming the timeline and fixing problems. Our time travellers are put together and captained by , a 22nd-century time agent seeking retribution for the death of his family. As an agent of the Time Bureau, all records of his previous life and ancestry have been wiped clean, with the alias Rip Hunter becoming his identity. While his time on the show is short-lived, his identity remains a mystery to ensure the protection of the timeline.  is one of the most beloved side characters on Scrubs, thanks to his long-running feud with . Despite his constant presence on the show, we never actually learn his real name, and it eventually makes for an interesting trope. At one point, he jokes that his real name is  a play on Janitor, only to mess with JD and make a point.During his wedding, he is referred to as the Janitor, with his wife seemingly becoming known as Mrs. Janitor, and seemingly accepting it. While the Janitor ]]></content:encoded></item><item><title>IT Worker Sentenced To Seven Months After Trashing Company Network</title><link>https://it.slashdot.org/story/25/07/01/1552216/it-worker-sentenced-to-seven-months-after-trashing-company-network?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: A judge has sentenced a disgruntled IT worker to more than seven months in prison after he wreaked havoc on his employer's network following his suspension, according to West Yorkshire Police. 

According to the police, Mohammed Umar Taj, 31, from the Yorkshire town of Batley, was suspended from his job in nearby Huddersfield in July 2022. But the company didn't immediately rescind his network credentials, and within hours, he began altering login names and passwords to disrupt operations, the statement says. 

The following day, he allegedly changed access credentials and the biz's multi-factor authentication settings that locked out the firm and its clients in Germany and Bahrain, eventually causing an estimated $274,200 in lost business and reputational harm.]]></content:encoded></item><item><title>Automattic puts Tumblr migration to WordPress on hold</title><link>https://techcrunch.com/2025/07/01/automattic-puts-tumblr-migration-to-wordpress-on-hold/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:13:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Automattic CEO Matt Mullenweg confirmed that the company is no longer working on migrating its Tumblr blogging platform to WordPress, as previously announced. ]]></content:encoded></item><item><title>Why They’ll Never Get You—and Why That’s Okay</title><link>https://hackernoon.com/why-theyll-never-get-youand-why-thats-okay?source=rss</link><author>BenoitMalige</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What I realized when I stopped needing to be understood.This newsletter is sponsored by.. no one. That being said, have you read the first chapter of  Available here for download:This morning I sat at a café in silence.\
Not on purpose. I just didn’t have much left to say.\
I’ve been talking a lot lately—about my book, about overthinking, about clarity and freedom and becoming. But when you’re in the middle of sharing your story with the world, something strange happens: you start losing touch with the parts of it that were never meant to be shared.\
You start forgetting which thoughts were sacred, and which were strategic.\
So I sat down, let the noise settle, and wrote one sentence.\
“Your depth was never meant to be understood by those who live on the surface.”\
And then, without thinking:\
“That works for your ego, too.”\
It wasn’t planned. It wasn’t poetic. But it felt true enough to stop me. Because somewhere in those two lines was the real reason I’ve felt so disconnected lately—not just from others, but from myself.\
For most of my life, I’ve believed that being misunderstood was one of the worst feelings in the world.\
And in many ways, it is.\
When you’re trying to be honest, when you’re doing the work, when you’ve found the courage to show up without the mask.. and someone still doesn’t get it? Still doesn’t get ? It cuts deeper than silence ever could.\
But that’s not what I’ve been struggling with lately.\
Because I haven’t been misunderstood. And .\
And the person who did that… was me.\
I’ve simplified my words so they wouldn’t intimidate. Softened my thoughts so they’d feel more relatable. Downplayed my insights so they wouldn’t sound like I was trying too hard.\
Rounded off the edges. Wrapped it all in a layer of warmth and good intentions.\
Not because I’m fake. But because I’ve been trying to stay visible—to . To anyone. And that need? That quiet, gnawing hunger to be understood?\
It sounds noble to want to be understood.But sometimes it’s just a socially acceptable way of saying: We dilute ourselves to become palatable. And then we blame the world for not tasting the truth.\
This morning, I sat with the discomfort of that.\
Because maybe the goal was never to be fully understood in the first place.Maybe that’s just what the ego tells us when it’s tired of feeling alone.\
And maybe wholeness is quieter than we expected. Not loud, not proven, not validated: Just intact.\
Here’s what I’m holding close right now:If someone only sees your surface, it doesn’t mean your depth doesn’t exist.Your ego will always want to be seen. Your soul just wants to be .You weren’t misunderstood. You were translated.You don’t owe anyone a version of you that fits better.And you don’t have to shrink to stay safe anymore.This morning’s message to myself was simple:]]></content:encoded></item><item><title>GM’s Home-Grown LMR Battery Opens New Front in EV Competition</title><link>https://spectrum.ieee.org/general-motors-lmr-battery</link><author>Lawrence Ulrich</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:58:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Manganese-rich batteries could leapfrog China’s lithium-ion phosphate cells]]></content:encoded></item><item><title>AI is Now Screening Job Candidates Before Humans Ever See Them</title><link>https://slashdot.org/story/25/07/01/186240/ai-is-now-screening-job-candidates-before-humans-ever-see-them?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AI agents are now conducting first-round job interviews to screen candidates before human recruiters review them, according to WashingtonPost, which cites job seekers who report being contacted by virtual recruiters from different staffing companies. The conversational agents, built on large language models, help recruiting firms respond to every applicant and conduct interviews around the clock as companies face increasingly large talent pools. 

LinkedIn reported that job applications have jumped 30% in the last two years, partially due to AI, with some positions receiving hundreds of applications within hours. The Society for Human Resource Management said a growing number of organizations now use AI for recruiting to automate candidate searches and communicate with applicants during interviews. The AI interviews, conducted by phone or video, can last anywhere from a few minutes to 20 minutes depending on the candidate's experience and the hiring firm's questions.]]></content:encoded></item><item><title>Nothing releases its first over-the-ear headphones, the $299 Headphone (1)</title><link>https://techcrunch.com/2025/07/01/nothing-releases-its-first-over-the-ear-headphones-the-299-headphone-1/</link><author>Maggie Stamets</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:38:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing stepped away from sensors in favor of a simple button to trigger your AI assistant or ChatGPT, if you have the Nothing X app, and a volume roller that can also be pressed to play, pause, and turn on and off noise canceling.]]></content:encoded></item><item><title>Limitless Raise $4m Strategic Funding, Launch Points Ahead Of TGE</title><link>https://hackernoon.com/limitless-raise-$4m-strategic-funding-launch-points-ahead-of-tge?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:12:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[New York, NY, USA, July 1st, 2025/Chainwire/--The largest prediction market on Base, , today announces the closure of $4M in fresh funding in a strategic round and welcomes Arthur Hayes as an advisor alongside an investment from his family office, Maelstrom. The funding follows a prior $3M pre-seed round led by 1confirmation, and comes after the team found breakout demand for short term prices markets on assets like BTC, which are similar in nature to 0DTE options but a much easier way to trade and feature even shorter dated expiries such as hourly, racking up over $250M in volume soon after launch. This brings Limitless' total funding raised to $7M, backed by Coinbase Ventures, 1confirmation, Maelstrom, Collider, Node Capital, Paper Ventures, Public Works, Punk DAO, and WAGMI Ventures, as well as individual investors via the Base Ecosystem Fund group on Echo. In preparation for an upcoming TGE, the team today launched a points program targeted at prediction market enthusiasts who can get skin in the game by using the product, providing liquidity, and referring their friends to join the platform. Limitless seems likely to become the first major prediction market platform to launch a token and distribute an airdrop to its early customers, marking a notable opportunity for retail traders. The team also just introduced a new mobile-first trading experience that enables people around the world to seamlessly wager on their favorite assets' performance in the next hour or day. “The future of trading is easy, fast, and powered by an army of token holders. We’re excited to bring this vision to reality,” said CJ Hetherington, CEO at Limitless Labs. is the largest prediction market on Base with over $250M bet on unique contracts that allow users to wager on the performance of their favorite assets in the next minutes, hour or day - a net new, easy way to trade for casual users.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Block3 Unveils Prompt-To-Game AI Engine As Presale Launches</title><link>https://hackernoon.com/block3-unveils-prompt-to-game-ai-engine-as-presale-launches?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:05:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[SF, CA, July 1st, 2025/Chainwire/--AI is steadily making deeper inroads into major tech industries, and its latest leap into the gaming sector marks a notable shift.Thousands have already backed : to build the world’s most advanced prompt-to-play engine, which will let anyone generate playable worlds in minutes from a simple text prompt. The scale of the impact is already spawning an equally vocal counter-movement, with experts saying this could be the .Already being called the “”, Block3 has a solution to one of AI’s ongoing challenges, which is how to get enough data for its LLM to learn and grow.By introducing a create2earn element, where users get paid for contributing, Block3 has found a practical route to accelerate product development. To enable this ambitious vision, Block3 is opening a limited-time token presale for just 90 days.The BL3 token goes on sale at 9AM UTC today, July 1st, with tokens initially available at just $0.01. Prices increase by 5% every 72 hours, offering early investors gains of 312% by the time it hits major exchanges. The token is only available for purchase through their .What is Block3 and Prompt-to-play GamingBlock3 is made up of a team of developers who are going head-to-head with the gaming industry, by using AI to put creative power in the hands of gamers rather than gaming studios.Think about it like an AI chatbot, where the user puts in prompts and gets an answer back from AI. With Block3, the mechanism is the same, except the output is a fully-formed video game, or in essence an immersive world customised to whatever the person typing can imagine.This concept is prompting a growing number of users to explore decentralized exchange (DEX) platforms. The potential applications for this technology are wide-ranging.For the more technical, the  reveals the complexity behind the concept and reveals the team’s roadmap. This is where the scale of the project is revealed, over a series of ambitious milestones that may see Block3 burgeon from fanatical fanbase to Unicorn over months of intensive development.The Threat to Gaming: Saving a $665 Billion IndustryAnd now, its blockbuster moment may have arrived. tackles this issue head-on and has thrown a spanner in the works. Even a 0.16% share of projected market revenue would generate over $1 billion.In a statement from the team, they said: “Traditional game dev is dead. For the first time, anyone can build games, not just studios with bloated teams and red tape. We’re here to unlock the imagination of gamers, and if we break a few corporate conglomerates along the way, then so be it.”The BL3 Presale is Now LiveThe gaming industry shows no sign of stopping, and AI gaming specifically is experiencing an impressive .Block3 is well-positioned to benefit from these mega-trends, and with $665 billion of potential revenue up for grabs, there is ample incentive for the developers to scale this as big and fast as they possibly can.The concept has already attracted an active community, generating notable attention online. With the presale set at a launch price of $0.01, early participants are positioning themselves ahead of the official rollout.Both the crypto and AI sectors are known for rapid innovation, and this project represents one of the more expansive efforts to emerge recently. is pioneering a new era in gaming by building the world’s first AI-native prompt-to-play platform. Designed to let anyone generate fully playable game worlds from a simple prompt, Block3 is eliminating the traditional barriers to game creation. By merging generative AI with real-time game logic and deployable environments, it’s changing how games are built and who gets to build them.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Cloudflare Flips AI Scraping Model With Pay-Per-Crawl System For Publishers</title><link>https://tech.slashdot.org/story/25/07/01/1745245/cloudflare-flips-ai-scraping-model-with-pay-per-crawl-system-for-publishers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Cloudflare today announced a "Pay Per Crawl" program that allows website owners to charge AI companies for accessing their content, a potential revenue stream for publishers whose work is increasingly being scraped to train AI models. The system uses HTTP response code 402 to enable content creators to set per-request prices across their sites. Publishers can choose to allow free access, require payment at a configured rate, or block crawlers entirely. 

When an AI crawler requests paid content, it either presents payment intent via request headers for successful access or receives a "402 Payment Required" response with pricing information. Cloudflare acts as the merchant of record and handles the underlying technical infrastructure. The company aggregates billing events, charges crawlers, and distributes earnings to publishers. 

Alongside Pay Per Crawl, Cloudflare has switched to blocking AI crawlers by default for its customers, becoming the first major internet infrastructure provider to require explicit permission for AI access. The company handles traffic for 20% of the web and more than one million customers have already activated its AI-blocking tools since their September 2024 launch, it wrote in a blog post.]]></content:encoded></item><item><title>National Guard Troops Sent To California By Trump Are Just Out There Doing Drug Busts</title><link>https://www.techdirt.com/2025/07/01/national-guard-troops-sent-to-california-by-trump-are-just-out-there-doing-drug-busts/</link><author>Tim Cushing</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:59:52 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Martial law? Police state? These are just things the alleged Leader of Free World , rather than things a nation founded on rejecting these options should be in the process of instituting. And yet, here we are, barely six months into Trump’s return to office, staring down the barrel of both of these related horrors.If it looks like fascism, it’s probably not intentional. Trump simply isn’t smart enough to implement the real thing. But he does like authoritarianism, which looks a lot like fascism, because he’s always felt a president should be treated like a king — someone who answers to no one, not even his 340 million employers. Trump tested the waters on martial law during his last term, threatening to send troops out to handle George Floyd protests. This time around, he’s amped everything up, openly hoping to turn every “Democrat” city into Kent State.Legally, he’s not allowed to do this. But his administration is relying on some vagueness in the law to get around the long-standing prohibition of sending in the army (so to speak) to police the populace. So, we get the sort of thing we’ve seen recently, where a Los Angeles swap meet was treated like an open-air market in some Middle Eastern country we’re currently at (undeclared) war with.The National Guard troops sent to Los Angeles are presumably still working without pay and/or beds, but that isn’t stopping them from blending in with federal law enforcement to aid and abet actual law enforcement work. First reported by CBS, a combined force of more than 500 federal officers and National Guard troops walked away from the ICE raids and the protection of federal property to perform a bog standard drug bust. Nicholas Slayton has more details for Task and Purpose, a military-oriented publication: California National Guard soldiers operating under federal orders helped the Drug Enforcement Administration and other federal personnel carry out a raid on a large marijuana growth operation in the eastern Coachella Valley last week, 130 miles from downtown Los Angeles. It’s unclear how many National Guard troops participated in the operation, but the force totalled roughly 500 people. According to the DEA, other agencies included Customs and Border Patrol, Bureau of Alcohol, Tobacco, Firearms and Explosives, Immigration and Custom Enforcement and the Federal Bureau of Investigation.While this commandeering of California National Guard troops may have originally been for the unstated purpose of pushing back against anti-ICE protests, now that they’re here, the administration has decided to just use them for whatever. This raid of multiple marijuana farms occurred more than  from the boundaries of Los Angeles County and even further away from the location these troops were originally sent: downtown Los Angeles. According to Trump’s military, everything about this is good and fine and nothing to be concerned about. After all, the law says the military can help federal cops, even if it (supposedly) prevents them from doing actual cop work. That’s the Title 10 vagueness the military is relying on when it serves up statements like this” “The catalyst of this order was related to events occurring in Los Angeles; however, the president’s order and NORTHCOM’s mission is not constrained by the geography of Southern California. Recently, Title 10 forces supported a Drug Enforcement Agency operation a few hours outside of Los Angeles. Title 10 forces protect federal personnel who are performing federal law enforcement functions…”Hence the military-provided shots of alleged National Guard troops allegedly manning the perimeter of the places being raided. And, also hence, the narrative no one can definitively dispute because — despite the National Guard embedding with federal law enforcement agencies — no journalists are being allowed to embed with military-esque operations occurring  the borders of the United States.Of course, we’ve already seen Marines detain people for the purpose of handing them over to law enforcement. And we’ve seen National Guard troops swarm a swap meet like they’re looking for terrorists in a foreign country, rather than just anyone looking kind of Hispanic who might not have the proper paperwork on them. The more things like this occur, the more easily many people will just come to accept this is the way the United States operates now. Many of them will cheer on these efforts, failing to recognize the abuse of these powers may, at some point, target them. But for the rest of us, this shouldn’t be allowed to pass without notice. Trump may be a blowhard and an idiot, but he’s surrounded by people who truly desire an opportunity to perform a hard reset on democracy and its principles, replacing it with jackboot heels, racism, fascism, and — eventually — a return of the British Empire, this time wrapped in an American flag.]]></content:encoded></item><item><title>Daily Deal: Academy of Educational Engineering</title><link>https://www.techdirt.com/2025/07/01/daily-deal-academy-of-educational-engineering-3/</link><author>Daily Deal</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:53:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The Academy of Educational Engineering is a premier platform tailored for aspiring and professional geeks. This all-in-one educational ecosystem is designed to empower you with expert-level knowledge and hands-on experience across embedded systems, electronics, IoT, and software development. As a premium member, you’ll access comprehensive tools, engaging projects, personalized feedback, and direct mentorship, helping you elevate your career in the tech industry. Whether you’re a beginner or a professional, this is your ultimate gateway to mastering the future of technology. It’s on sale for $50.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Nothing releases its first over-the-ear headphones</title><link>https://techcrunch.com/video/nothing-releases-its-first-over-the-ear-headphones/</link><author>TC Video</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:52:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing has revealed its first over-the-ear headphones. The aluminum and transparent Headphone (1) was designed in collaboration with KEF. They offer adaptive noise canceling, bass enhancement for deeper low frequencies, and spatial audio, for $299 at launch. But spoiler alert: The buttons on these headphones might be the best part.]]></content:encoded></item><item><title>Midas And 0G Partner To Bring Real-World Assets To AI-Native Blockchain Infrastructure</title><link>https://hackernoon.com/midas-and-0g-partner-to-bring-real-world-assets-to-ai-native-blockchain-infrastructure?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:49:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, Republic of Singapore, July 1st, 2025/Chainwire/--Tokenization protocol Midas and AI blockchain 0G have announced a strategic partnership to unlock the next wave of onchain finance through modular design. By combining Midas’ tokenisation infrastructure with 0G’s decentralized AI-native compute, the partners will develop new solutions that intelligently leverage real-world assets (RWAs).As part of the partnership, Midas will deploy on the 0G mainnet, scheduled for late Q3 2025, bringing its full stack of tokenization infrastructure. In parallel, 0G will integrate Midas’ tokenized instruments and vault logic into its optimized AI layer. This will position both platforms to serve institutions, developers, and liquidity providers at scale.Midas offers a compliant protocol suite for issuing tokenized certificates tracking institutional-grade strategies. Its tokens, including mF-ONE, mMEV, mEDGE, mRE7YIELD, mBASIS and mTBILL, provide exposure via tokenized certificates to reference real-world assets across private credit, US short-term treasuries, and market-neutral strategies.0G Labs CEO Michael Heinrich said: “Midas have made huge strides in expanding compliant access to tokenized RWAs and we’re delighted that they’ve chosen to build on 0G. We’re excited to be collaborating with them to develop new financial products that will combine AI with tokenized assets, giving users greater onchain opportunities than ever before.” By launching on 0G, Midas will introduce compliant, composable tokens into a modular environment optimized for AI-powered workflows and smart contract automation. Use cases range from onchain lending vaults and automated credit exposures to AI-enhanced risk analytics and composable strategy deployment. 0G’s modular Layer 1 blockchain is purpose-built for AI-native applications.It combines high-performance compute, decentralized storage, data availability, and low-latency smart contract execution, ideal for deploying data-intensive financial applications and real-time DeFi logic. 0G’s architecture supports seamless integration with EVM and non-EVM ecosystems, while its recent Galileo testnet demonstrated sustained throughput and low gas costs. It also saw significant developer adoption with over 170 million transactions and 13 million accounts in under two months. The collaboration between Midas and 0G reflects a shared vision: to make programmable, compliant financial infrastructure natively interoperable with the AI applications of the future.Midas is a tokenisation platform building institutional-grade financial products for the open web. Its ERC-20 tokens are structured to track dedicated strategies with verifiable on-chain performance, combining TradFi-grade standards with DeFi composability. Midas is backed by leading investors like Framework Ventures, BlockTower Capital, and GSR, and partners with regulated custodians to ensure strong compliance and risk controls. Learn more: https://midas.app/About 0G 0G is the first decentralized AI protocol (AIP), purpose-built to power a truly democratized future of intelligence. As a modular and infinitely scalable Layer 1, 0G enables the execution of decentralized AI applications at scale. It unifies high-performance decentralized storage, compute, and data availability (DA) to support the next generation of AI-native use cases. With verifiable AI processing and a permissionless agent ecosystem, 0G is laying the foundation for an open and unstoppable AI economy. Learn more: https://0g.ai/Disclaimer This announcement is for informational purposes only and does not constitute investment advice or an offer to sell or buy any financial instrument. Midas-issued tokens are not available to US & UK persons and entities, or those from sanctioned jurisdictions. This is not investment advice.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Nothing launches its most expensive flagship yet, Phone (3)</title><link>https://techcrunch.com/2025/07/01/nothing-launches-its-most-expensive-flagship-phone-3/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing launched its newest flagship phone after a two-year gap. At an event in London, the company unveiled the Phone (3), which starts at $799 and aims to take on bigwigs like Samsung and Apple.]]></content:encoded></item><item><title>AI Arms Race Drives Engineer Pay To More Than $10 Million</title><link>https://tech.slashdot.org/story/25/07/01/1536223/ai-arms-race-drives-engineer-pay-to-more-than-10-million?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Tech companies are paying AI engineers unprecedented salaries as competition for talent intensifies, with some top engineers earning more than $10 million annually and typical packages ranging from $3 million to $7 million. OpenAI told staff this week it is seeking "creative ways to recognize and reward top talent" after losing key employees to rivals, despite offering salaries near the top of the market. 

The move followed OpenAI CEO Sam Altman's claim that Meta had promised $100 million sign-on bonuses to the company's most high-profile AI engineers. Mark Chen, OpenAI's chief research officer, sent an internal memo saying he felt "as if someone has broken into our home and stolen something" after recent departures. 

AI engineer salaries have risen approximately 50% since 2022, with mid-to-senior level research scientists now earning $500,000 to $2 million at major tech companies, compared to $180,000 to $220,000 for senior software engineers without AI experience.]]></content:encoded></item><item><title>Best iPad apps to boost productivity and make your life easier</title><link>https://techcrunch.com/2025/07/01/best-ipad-apps-to-boost-productivity-and-make-your-life-easier/</link><author>Aisha Malik</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:18:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[There are many iPad apps to help you organize recipes; sync tasks across devices; be more productive; and manage your notes.]]></content:encoded></item><item><title>Raising a Series C+? Cathy Gao’s bringing the real playbook to TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/raising-a-series-c-cathy-gaos-bringing-the-real-playbook-to-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:13:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Cathy Gao of Sapphire Ventures shares her playbook to scale a Series C+ at TechCrunch All Stage on July 15 in Boston. Register now to save more than 60% on your tickets.]]></content:encoded></item><item><title>Amazon deploys its 1 millionth robot, releases generative AI model</title><link>https://techcrunch.com/2025/07/01/amazon-deploys-its-1-millionth-robot-releases-generative-ai-model/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:01:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As Amazon's fleet of robots has reaches a milestone, the company is also releasing a new AI model to make them more efficient. ]]></content:encoded></item><item><title>Global Warming Is Speeding Up and the World Is Feeling the Effects</title><link>https://news.slashdot.org/story/25/07/01/164239/global-warming-is-speeding-up-and-the-world-is-feeling-the-effects?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Summer started barely a week ago, and already the United States has been smothered in a record-breaking "heat dome." Alaska saw its first-ever heat advisory this month. And all of this comes on the heels of 2024, the hottest calendar year in recorded history. The world is getting hotter, faster. A report published last week found that human-caused global warming is now increasing by 0.27 degrees Celsius per decade. That rate was recorded at 0.2 degrees in the 1970s, and has been growing since. 

"Each additional fractional degree of warming brings about a relatively larger increase in atmospheric extremes, like extreme downpours and severe droughts and wildfires," said Daniel Swain, a climate scientist at the University of California. While this aligns with scientific predictions of how climate change can intensify such events, the increase in severity may feel sudden to people who experience them. 

"Back when we had lesser levels of warming, that relationship was a little bit less dramatic," Dr. Swain said. "There is growing evidence that the most extreme extremes probably will increase faster and to a greater extent than we used to think was the case," he added. Take rainfall, for example. Generally, extreme rainfall is intensifying at a rate of 7 percent with each degree Celsius of atmospheric warming. But recent studies indicate that so-called record-shattering events are increasing at double that rate, Dr. Swain said.]]></content:encoded></item><item><title>The End of the Guessing Game? Why Describing Data Beats Estimating It</title><link>https://hackernoon.com/the-end-of-the-guessing-game-why-describing-data-beats-estimating-it?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:38:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Achiam, J., Andrychowicz, M., Beattie, A., Clark, J., Drozdov, N., Ecoffet, A., Edwards, D., Giddings, J., Goldberg, I., Gomez, M., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)Batista, G.E., Monard, M.C.: A study of k-nearest neighbour as an imputation method. In: Frontiers in Artificial Intelligence and Applications. vol. 87, pp. 251–260. HIS (2002)Biessmann, F., Salinas, D., Schelter, S., Schmidt, P., Lange, D.: "deep" learning for missing value imputation in tables with non-numerical data. In: Proceedings of the 27th ACM International Conference on Information and Knowledge Management. p. 2017–2025. CIKM ’18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3269206.3272005, https://doi.org/10.1145/3269206.3272005Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are Few-Shot Learners. In: Advances in Neural Information Processing Systems. vol. 33, pp. 1877–1901. Curran Associates, Inc. (2020)Buuren, S.v., Groothuis-Oudshoorn, K.: Mice: Multivariate imputation by chained equations in r. Journal of Statistical Software 45, 1–67 (2011)Camino, R.D., Hammerschmidt, C.A., State, R.: Improving missing data imputation with deep generative models. arXiv preprint arXiv:1902.10666 pp. 1–8 (2019)Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., Fiedel, N.: PaLM: Scaling Language Modeling with Pathways (Oct 2022), http://arxiv.org/abs/2204.02311, arXiv:2204.02311 [cs]Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological) 39(1), 1–22 (1977)Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetuning of quantized llms (2023)Emmanuel, T., Maupong, T., Mpoeleng, D., Semong, T., Mphago, B., Tabona, O.: A survey on missing data in machine learning. J Big Data 8(1), 140 (2021). https://doi.org/10.1186/s40537-021-00516-9, epub 2021 Oct 27. PMID: 34722113; PMCID: PMC8549433Gimpy, M.: Missing value imputation in multi attribute data set. Int. J. Comput. Sci. Inf. Technol. 5(4), 1–7 (2014)Gondara, L., Wang, K.: Mida: Multiple imputation using denoising autoencoders. In: PacificAsia conference on knowledge discovery and data mining. pp. 260–272. Springer (2018)Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al.: Generative adversarial nets. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger, K.Q. (eds.) Advances in Neural Information Processing Systems. vol. 27, pp. 2672–2680. Curran Associates, Inc., Montréal, Canada (2014)Gupta, A., Lam, M.S.: Estimating missing values using neural networks. Journal of the Operational Research Society 47(2), 229–238 (1996)Hallaji, E., Razavi-Far, R., Saif, M.: Dlin: Deep ladder imputation network. IEEE Transactions on Cybernetics 52(9), 8629–8641 (2021)Jäger, S., Allhorn, A., Biessmann, F.: A benchmark for data imputation methods. Front Big Data 4, 693674 (2021). https://doi.org/10.3389/fdata.2021.693674, pMID: 34308343; PMCID: PMC8297389 Enhancing Imputation Accuracy with Contextual Large Language Models 15Little, R.J., Rubin, D.B.: Statistical Analysis with Missing Data, vol. 793. John Wiley & Sons, 3 edn. (2019)Little, R.J.A., Rubin, D.B.: Statistical Analysis with Missing Data. John Wiley & Sons, Hoboken, 2 edn. (2002)Lu, H.m., Perrone, G., Unpingco, J.: Multiple imputation with denoising autoencoder using metamorphic truth and imputation feedback. arXiv preprint arXiv:2002.08338 (2020)McCoy, J.T., Kroon, S., Auret, L.: Variational autoencoders for missing data imputation with application to a simulated milling circuit. IFAC-PapersOnLine 51(21), 141–146 (2018), 5th IFAC Workshop on Mining, Mineral and Metal Processing MMM 2018Nazabal, A., Olmos, P.M., Ghahramani, Z., Valera, I.: Handling incomplete heterogeneous data using vaes. arXiv preprint arXiv:1807.03653 (2018)Qiu, Y.L., Zheng, H., Gevaert, O.: Genomic data imputation with variational auto-encoders. GigaScience 9(8), giaa082 (2020)Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21(1), 140:5485–140:5551 (Jan 2020)Roberts, A., Raffel, C., Shazeer, N.: How Much Knowledge Can You Pack Into the Parameters of a Language Model? In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5418–5426. Association for Computational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.437, https://aclanthology.org/2020.emnlp-main.437Rubin, D.B.: Multiple imputations in sample surveys-a phenomenological bayesian approach to nonresponse. In: Proceedings of the survey research methods section of the American Statistical Association. vol. 1, pp. 20–34. American Statistical Association, Alexandria, VA, USA (1978)Rubin, D.B.: Multiple Imputation for Nonresponse in Surveys. John Wiley & Sons, New York, NY (2004)Schafer, J.L.: Analysis of Incomplete Multivariate Data. Chapman & Hall/CRC, London, UK (1997)Schelter, S., Rukat, T., Biessmann, F.: JENGA - A framework to study the impact of data errors on the predictions of machine learning models. In: Velegrakis, Y., Zeinalipour-Yazti, D., Chrysanthis, P.K., Guerra, F. (eds.) Proceedings of the 24th International Conference on Extending Database Technology, EDBT 2021, Nicosia, Cyprus, March 23 - 26, 2021. pp. 529–534. OpenProceedings.org (2021). https://doi.org/10.5441/002/EDBT.2021.63, https://doi.org/10.5441/002/edbt.2021.63Sharpe, P.K., Solly, R.: Dealing with missing values in neural network-based diagnostic systems. Neural Computing & Applications 3(2), 73–77 (1995)Stekhoven, D.J., Bühlmann, P.: Missforest—non-parametric missing value imputation for mixed-type data. Bioinformatics 28(1), 112–118 (2012)Stoyanovich, J., Howe, B., Jagadish, H.V.: Responsible data management. Proceedings of the VLDB Endowment 13, 3474–3488 (2020). https://doi.org/10.14778/ 3415478.3415570Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (Feb 2023), http://arxiv.org/ abs/2302.13971, arXiv:2302.13971 [cs]Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.: Llama 2: Open foundation and fine-tuned chat models (2023)Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th international conference on machine learning. pp. 1096–1103 (2008)Yang, K., Huang, B., Stoyanovich, J., Schelter, S.: Fairness-aware instrumentation of preprocessing pipelines for machine learning. In: Proceedings of the Workshop on HumanIn-the-Loop Data Analytics (HILDA’20). ACM (2020). https://doi.org/10.1145/3398730.3399194Yoon, J., Jordon, J., van der Schaar, M.: Gain: Missing data imputation using generative adversarial nets. In: International conference on machine learning. pp. 5689–5698. PMLR (2018)Yoon, J., Jordon, J., van der Schaar, M.: Gain: Missing data imputation using generative adversarial nets (2018)]]></content:encoded></item><item><title>Why Mamdani’s Refusal To Condemn Speech He Never Made Is Good Free Speech Advocacy</title><link>https://www.techdirt.com/2025/07/01/why-mamdanis-refusal-to-condemn-speech-he-never-made-is-good-free-speech-advocacy/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:28:52 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[At a time when politicians on both sides reflexively call for censorship and speech policing, it’s refreshing to see someone actually defend free speech principles—especially when it would be politically easier to cave.That’s exactly what New York City Democratic mayoral nominee Zohran Mamdani did when NBC’s  tried to pressure him into condemning language he’s never used. Rather than take the bait, Mamdani delivered a strong defense of free speech principles. It’s a better defense of free speech than we’ve seen from most politicians lately.What makes this particularly frustrating is that many of the Democrats attacking Mamdani should be laser-focused on the existential threat Trump poses to democracy. Instead, they’re wasting time and energy going after someone who actually accomplished what establishment Democrats claim they desperately want: activating young people who often fail to vote. Mamdani didn’t just talk about engaging young voters—he did it, handily winning the Democratic primary by mobilizing exactly the demographic Democrats say they need. His reward? A coordinated attack campaign.The controversy stems from demands that Mamdani condemn the phrase “globalize the intifada”—language he doesn’t use but which critics insist he must denounce to prove he’s not antisemitic. It’s the kind of ridiculous purity test that marginalized politicians routinely face (but somehow, white, Christian, male politicians never do), demanding they repeatedly distance themselves from the words of others simply because they share some demographic or political similarity.But rather than playing that game, Mamdani chose to defend the principle that government officials shouldn’t be in the business of policing speech—even speech they personally disagree with. At the same time, he used the opportunity to move from the “gotcha” kind of question to a focus on how to tackle the actual problems of racism and bigotry, beyond just focusing on specific language questions.There’s been a lot of pressure on Mamdani to specifically criticize pro-Palestinian language used by others. And, over the weekend, he went on Meet the Press and gave, what I think, is a really strong answer to a silly gotcha question that I think others could learn from:I want to ask you about an issue that has divided some New Yorkers in recent weeks. You were recently asked about the term “globalize the intifada,” if it makes you uncomfortable. In that moment you did not condemn the phrase. Now, just so folks understand, it’s a phrase that many people hear as a call to violence against Jews. There’s been a lot of attention on this issue, so I want to give you an opportunity to respond here and now. Do you condemn that phrase “globalize the intifada?”That’s not language that I use. The language that I use and the language that I will continue to use to lead this city is that which speaks clearly to my intent, which is an intent grounded in a belief in universal human rights. And ultimately, that’s what is the foundation of so much of my politics, the belief that freedom and justice and safety are things that, to have meaning, have to be applied to all people, and that includes Israelis and Palestinians as well.But do you actually condemn it? I think that’s the question and the outstanding issue that a number of people, both of the Jewish faith and beyond, have. Do you condemn that phrase, “globalize the intifada,” which a lot of people hear as a call to violence against Jews?I’ve heard from many Jewish New Yorkers who have shared their concerns with me, especially in light of the horrific attacks that we saw in Washington, D.C. and in Boulder, Colorado about this moment of antisemitism in our country and in our city. And I’ve heard those fears and I’ve had those conversations. And ultimately, they are part and parcel of why, in my campaign, I’ve put forward a commitment to increase funding for anti-hate crime programming by 800%. I don’t believe that the role of the mayor is to police speech in the manner, especially of that of Donald Trump, who has put one New Yorker in jail, who’s just returned to his family, Mahmoud Khalil, for that very supposed crime of speech. Ultimately, what I think I need to show is the ability to not only talk about something but to tackle it and to make clear that there’s no room for antisemitism in this city. And we have to root out that bigotry, and ultimately we do that through the actions. And that is the mayor I will be, one that protects Jewish New Yorkers and lives up to that commitment through the work that I do.But very quickly for the people who care about the language and who feel really concerned by that phrase, why not just condemn it?My concern is to start to walk down the line of language and making clear what language I believe is permissible or impermissible takes me into a place similar to that of the president, who is looking to do those very kinds of things, putting people in jail for writing an oped. Putting them in jail for protesting. Ultimately, it’s not language that I use. It’s language I understand there are concerns about. And what I will do is showcase my vision for this city through my words and my actions.Note what he does here. It would be easy enough to give into the framing and make statement condemning the language. And while some will (in bad faith) argue his failure to outright condemn the language is an endorsement of it, that’s bullshit. His answer is actually very thoughtful and a good way to approach such bad faith questions.He starts out with a direct and clear denial of using that language:That’s not language that I use.This immediately deflates the premise that he’s somehow responsible for words he’s never spoken.He then immediately shifts to a more positive framing of how he views what he’s focused on in his hopes of becoming mayor: human rights for all.The language that I use and the language that I will continue to use to lead this city is that which speaks clearly to my intent, which is an intent grounded in a belief in universal human rights. And ultimately, that’s what is the foundation of so much of my politics, the belief that freedom and justice and safety are things that, to have meaning, have to be applied to all people, and that includes Israelis and Palestinians as well.When NBC’s Welker trots out the purity test point, demanding he condemn it, he points out that he shouldn’t be in the business of policing language, but rather is focused on actual concerns of the people he’s hoping to represent. In doing so, he makes it clear that he’s concerned about actual antisemitism and actual threats and risks, and he’s looking at what might actually help rather than policing specific language:I’ve heard from many Jewish New Yorkers who have shared their concerns with me, especially in light of the horrific attacks that we saw in Washington, D.C. and in Boulder, Colorado about this moment of antisemitism in our country and in our city. And I’ve heard those fears and I’ve had those conversations. And ultimately, they are part and parcel of why, in my campaign, I’ve put forward a commitment to increase funding for anti-hate crime programming by 800%.And then he pivots to a reasonable defense of free speech, not in the misleading sense the way others view it, but rather in noting that government shouldn’t be in the business of policing speech (as Trump is doing) but focusing on where the  problems of hate and bigotry show up.I don’t believe that the role of the mayor is to police speech in the manner, especially of that of Donald Trump, who has put one New Yorker in jail, who’s just returned to his family, Mahmoud Khalil, for that very supposed crime of speech. Ultimately, what I think I need to show is the ability to not only talk about something but to tackle it and to make clear that there’s no room for antisemitism in this city. And we have to root out that bigotry, and ultimately we do that through the actions.After Welker desperately goes back to the “but won’t you condemn the language” nonsense, he makes it clear that speaking out on specific language choices is not productive when his focus is on dealing with the actual underlying problems:My concern is to start to walk down the line of language and making clear what language I believe is permissible or impermissible takes me into a place similar to that of the president, who is looking to do those very kinds of things, putting people in jail for writing an oped. Putting them in jail for protesting. Ultimately, it’s not language that I use. It’s language I understand there are concerns about. And what I will do is showcase my vision for this city through my words and my actions.This final answer is particularly smart because it connects his refusal to condemn specific language to Trump’s actual authoritarian attacks on free speech. Rather than getting trapped in semantic debates about particular phrases, he’s defending the broader principle that government officials shouldn’t be arbiters of acceptable speech.The contrast is stark: while the Trump regime is literally jailing people for their speech, critics want Mamdani to engage in the kind of speech policing that leads down that same authoritarian path. His refusal isn’t endorsement of problematic language—it’s recognition that the role of government isn’t to play word police.This is exactly the kind of principled free speech defense we need more of, especially from Democrats who have too often been willing to compromise these principles for short-term political gain. While it would have been easy for Mamdani to simply condemn the phrase and move on, his more thoughtful approach actually serves the cause of free speech better.The irony is that many of the same people attacking Mamdani are Democrats who claim to be defending democracy against Trump’s authoritarianism. Yet they’re demanding exactly the kind of speech policing that authoritarian governments excel at—forcing officials to take public positions on specific language as loyalty tests.And yes, some could argue that simply condemning certain language is not the same as censoring it. It’s not. It’s stating an opinion. But there’s value in Mamdani making it clear he’d rather focus on the real underlying issues around bigotry and hatred than trying to say magic words to appease a media that would never ask similar questions of a white, Christian politician.In an era where politicians routinely cave to demands for performative condemnations and symbolic gestures, Mamdani’s approach stands out. He’s more interested in actual solutions—like his 800% increase in anti-hate crime funding—than in playing the gotcha game that dominates political discourse.This is what defending free speech actually looks like: not demanding the right to be an asshole without consequences, but refusing to let government officials become the arbiters of acceptable speech—and politely reframing the issue when the media insists on playing such a gotcha game. If more politicians followed Mamdani’s lead, we’d have a much healthier democratic discourse.]]></content:encoded></item><item><title>Built for Scale: eXchange1 Brings Institutional-Grade Crypto Trading to Indian Users</title><link>https://hackernoon.com/built-for-scale-exchange1-brings-institutional-grade-crypto-trading-to-indian-users?source=rss</link><author>ZEX MEDIA</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:16:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As India continues its ascent as the world’s most active crypto market, eXchange1 arrives with the infrastructure, innovation, and integrity to match the moment. With its official launch, the European-regulated platform opens access to a full suite of digital asset tools designed for modern traders, institutional clients, and ecosystem partners alike.While many platforms promise speed and scale, few back it up with the regulatory foundation and high-performance architecture that eXchange1 offers by default.eXchange1 positions itself as more than a trading venue. It is a complete digital finance ecosystem that merges cutting-edge technology with seamless user experience. The platform provides:Spot, Margin, and Futures TradingCopy Trading (for both Spot and Futures)Tokenized Investment ProductsWhether you’re a new investor exploring basic trades or an institution managing complex portfolios, eXchange1 offers functionality at every level. The platform is accessible across web, mobile, and API, enabling real-time participation with global markets from any device.Its Copy Trading feature, for example, empowers less experienced traders to mirror strategies of seasoned investors—while the automated tools allow advanced users to deploy algorithmic trades with precision.Built for India. Backed by the World.India’s fast-growing crypto base—119 million users in 2024 alone—demands more than just flashy apps. It demands scalability, reliability, and regulatory clarity. eXchange1 delivers on all fronts.The platform is built on a high-liquidity architecture designed to support institutional-scale trade volumes without bottlenecks. Back-end infrastructure is optimized for rapid execution, real-time price discovery, and minimal slippage—ensuring traders get exactly what they see.To support this mission in India, eXchange1 has partnered with a leading global fintech firm to ensure localized performance, reduced latency, and seamless access—even during market surges.“We’re not here with a one-size-fits-all model,” said CEO Ms. Sandoval Mera. “India’s scale and diversity deserve a platform that’s tailored, responsive, and resilient. We’ve invested in both infrastructure and partnerships to deliver that.”Enterprise-Grade Security by DefaultInfrastructure isn’t just about speed—it’s also about safety. eXchange1 operates with enterprise-grade security systems that ensure operational integrity, user protection, and compliance with global standards.Key security features include:Real-time risk monitoring systemsInstitutional-grade custodial protocolsMulti-layer authentication and transaction securityAudit-ready transparency for regulators and partnersWith the rising sophistication of crypto-related threats, such features are no longer optional—they are critical. eXchange1 ensures its infrastructure protects both user assets and market confidence.The platform is licensed under the Markets in Crypto-Assets Regulation (MiCA) by the Financial Crime Investigation Service (FCIS) of Lithuania, and is registered with India’s Financial Intelligence Unit (FIU).This dual compliance enables eXchange1 to integrate seamlessly with India’s evolving regulatory environment while retaining the global credibility needed to serve cross-border investors.Being MiCA-compliant also brings with it rigorous operating standards, including rules around asset segregation, risk exposure, transparency, and consumer protection—all of which are reflected in eXchange1’s infrastructure design.“The future of digital assets depends on platforms that are both innovative and accountable,” said Dr. James Newsome, Chairman of eXchange1 and former head of the U.S. Commodity Futures Trading Commission (CFTC). “We’ve built eXchange1 to be just that—a high-performance platform that regulators can work with, and users can depend on.”One of eXchange1’s key advantages is its ability to scale with its users. Retail traders benefit from an intuitive UI/UX and educational support, while institutions get access to:Advanced APIs for high-frequency tradingReal-time market data feedsDedicated account supportTokenized investment vehicles for diversificationThis dual approach—serving both ends of the crypto maturity spectrum—makes eXchange1 uniquely capable of addressing India’s diverse and rapidly evolving investor base.And with multilingual customer service available 24/7, the platform ensures every user, from first-time investor to fund manager, gets the assistance they need—when they need it.Future-Proofed for a Changing IndustryCrypto is changing fast. New asset classes, regulatory shifts, institutional adoption, and user behavior all demand platforms that can adapt in real time. eXchange1’s architecture is modular and forward-compatible, allowing it to integrate future features like:Cross-chain liquidity poolsCompliance automation for new jurisdictionsThis future-readiness is part of the platform’s DNA. It’s also why eXchange1 insists on building for long-term resilience rather than short-term speculation.Final Thoughts: A New Infrastructure StandardAs more Indian users seek robust, regulated platforms to trade and invest in crypto, infrastructure will become the dividing line between the serious players and the short-lived ones.eXchange1 has entered the Indian market not to test the waters, but to build a foundation—one grounded in regulation, engineered for performance, and designed for trust.From institutional-grade systems to retail-friendly tools, and from global oversight to local integration, eXchange1 represents what a next-generation exchange should look like: fast, safe, scalable, and accountable.]]></content:encoded></item><item><title>Nikita Bier joins X as head of product: ‘I’ve officially posted my way to the top’</title><link>https://techcrunch.com/2025/07/01/nikita-bier-joins-x-as-head-of-product-ive-officially-posted-my-way-to-the-top/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:13:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At X, Bier could potentially build features that drive adoption beyond the user base that's typically drawn to text-first social networks.]]></content:encoded></item><item><title>DORA Regulation Explained - Plus a Free Compliance Checklist</title><link>https://hackernoon.com/dora-regulation-explained-plus-a-free-compliance-checklist?source=rss</link><author>N2W</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:06:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Digital Operational Resilience Act (DORA) sets new EU-wide standards for digital risk in the financial sector. This guide breaks down key requirements, penalties, and regional implications, with best practices for compliance—including risk management, testing, and third-party oversight. Download a free checklist to get your organization DORA-ready.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Will VR Integration for Remote Work Become Permanent? (7/1/2025)</title><link>https://hackernoon.com/7-1-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:03:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, July 1, 2025?By @editingprotocol [ 4 Min read ] Run your blog like a product. Learn how to define your audience, set goals, and build a content strategy that drives real growth. Read More.By @allan-grain [ 3 Min read ] The best part of VR is that it provides a three-dimensional immersive space where employees can interact as if they were physically present. Read More.By @paoloap [ 6 Min read ] Discover how the original PDF design creates hurdles for AI document parsing and learn solutions for better, machine-readable document workflows. Read More.By @legalpdf [ 8 Min read ] Anthropic allegedly used over 7M pirated books to train Claude AI. Authors sue over copyright theft fueling a billion-dollar LLM business. Read More.By @techthrilled [ 4 Min read ] As soon as a16z’s Bryan Kim saw Cluely’s buzz online, he was intrigued. He hadn’t even met the founder yet, but he could tell the company was onto something. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>It&apos;s Not Just What&apos;s Missing, It&apos;s How You Say It: CLAIM&apos;s Winning Formula</title><link>https://hackernoon.com/its-not-just-whats-missing-its-how-you-say-it-claims-winning-formula?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:03:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).We conducted a series of experiments to systematically evaluate the efficacy of CLAIM in addressing the research questions presented in Section 1. Our validation criterion for CLAIM’s effectiveness was the post-imputation performance of pre-trained LLMs finetuned with missing-aware contextual datasets for downstream classification tasks. We focused on three types of missingness mechanisms: MCAR, MAR, and MNAR.\
. We evaluated the performance of CLAIM using seven real-life multivariate classification datasets from the UCI repository [12]. Detailed information on these datasets is provided in the Appendix.\
Baseline Imputation Methods. Our approach was compared against a broad spectrum of commonly-used baseline imputation methods, encompassing single imputation (SI) and multiple imputation (MI) techniques, non-ML and ML methods, and both discriminative and generative ML approaches.\
SI methods included mean imputation using the feature-wise mean (), kNearest Neighbors (k-NN) [3] (), a tree-based algorithm using MissForest [37] (), and a deep generative adversarial network for imputation using GAIN (Generative Adversarial Imputation Nets) [45] (). The MI method employed was MICE (Multiple Imputation by Chained Equations) [22] ().\
 The hyperparameter settings for the various imputation methods and the LLM used in our experiments are detailed below.\
Hyperparameters for Baseline Imputation Methods. For GAIN, we adhered to the hyperparameters specified in the original publication, setting α to 100, the batch size to 128, the hint rate at 0.9, and the number of iterations to 1000 for optimal performance. MissForest and MICE were configured with their respective default parameters as provided in their PyPI implementations[2]. For k-NN, we chose k = 5 and the Euclidean distance measure based on literature suggesting this configuration offers superior performance [15].\
 We utilized the 7 billion-parameter LLaMA 2 model [40], fine-tuning it with the parameter-efficient QLoRA method [11]. The settings were r = 16, α = 64, dropout = 0.1 with the task type set to “CAUSALLM”. The learning rate was 2e-4, using the “pagedadamw_32bit” optimizer.\
Experiments were conducted with a batch size of 4 across 50 epochs, considering memory constraints during fine-tuning. Tesla A40 GPUs (48GB RAM) were used for distributed training. For evaluation, we used 20% randomly sampled instances of each dataset. Models were evaluated five times, reporting both average performance and standard deviation.Figure 2 displays the experimental outcomes for seven datasets, where we benchmarked CLAIM against existing imputation methods. Performance metrics for LLMs fine-tuned on fully complete datasets (without any missing values, thus no imputation was necessary) were included for comparison. This approach delineates the effectiveness of CLAIM by providing a reference to baseline performances, offering a clearer perspective on the benefits provided by CLAIM over traditional imputation methods.\
: How effective is CLAIM in imputing missing values across the distinct missingness mechanisms (MCAR, MAR, and MNAR) and how does it compare with existing imputation methods in terms of accuracy and robustness across varied datasets and missing data scenarios?\
: CLAIM demonstrated superior accuracy in imputing missing values across all datasets compared to baseline imputation methods. Its performance under the MCAR assumption, where missingness is independent of any data, suggests that CLAIM efficiently leverages the contextual information inherent in the dataset for imputation. This efficiency is particularly evident in its ability to significantly close the gap towards the performance of fully complete datasets (no imputation), showcasing its effectiveness\
 Under MAR, where missingness depends on observed data, the adaptability of CLAIM is further highlighted. It outperforms other methods by a considerable margin, indicating its proficiency in utilizing available data points to predict missing values accurately.\
: The MNAR scenario, characterized by missingness that depends on unobserved data, poses the greatest challenge. Here, CLAIM’s performance remains notably superior to traditional imputation methods. This robustness in the face of the most difficult missingness mechanism illustrates CLAIM’s potential to effectively mitigate the biases introduced by MNAR missingness, utilizing the LLaMA 7B model’s capacity to infer missing information from complex patterns.\
To elucidate the superior performance of CLAIM over traditional baseline imputation methods, we delved into its performance on three particularly challenging datasets: Glass Identification, Seeds, and Wine. These datasets were selected due to the relatively lower performance exhibited by the LLM when utilizing fully complete versions of the datasets, highlighting their complexity and the rigorous testing ground they provide for evaluating CLAIM’s effectiveness.\
Table 1 presents a detailed comparative analysis. For the Glass Identification dataset, where the LLM achieved an accuracy of only 69.40% with the full dataset, CLAIM demonstrated a significant advantage. It outperformed the best baseline method (kNN, which achieved 52.40% accuracy) by a substantial margin of 7.2%. This performance gap underscores CLAIM’s robustness and its ability to effectively handle missing data within complex datasets.\
The challenge escalates with the Seeds dataset, wherein CLAIM surpassed the top-performing baseline method (MICE) by a margin of 4.2%. This further exemplifies CLAIM’s superiority in managing missing data, even in datasets where the LLM’s base performance is less than optimal.\
\
The Wine dataset showcased a similar trend, with CLAIM exceeding the best baseline performance by a margin of 2.4%. It’s noteworthy that the performance gaps between CLAIM and the best-performing baseline methods are relatively modest under MAR conditions—2%, 3%, and 1.2% for Glass Identification, Seeds, and Wine, respectively. This observation suggests that while the predictability of missingness from observed data in MAR scenarios offers some leverage for traditional imputation methods, CLAIM still maintains a performance edge.\
The MNAR scenario, characterized by the most complex pattern of missingness, highlighted CLAIM’s distinct advantage. Across all three datasets, CLAIM not only managed to outperform the best baseline methods but did so with remarkable performance gains of 12.4%, 7.6%, and 10% for Glass Identification, Seeds, and Wine, respectively. This substantial improvement underlines CLAIM’s adeptness at navigating the intricacies of MNAR missingness, further cementing its status as a highly effective tool for handling various missing data scenarios with aplomb.\
 CLAIM’s superior accuracy across diverse missingness patterns and datasets unequivocally affirms its effectiveness in a variety of challenging scenarios, thereby  This consistent overperformance not only underscores its utility but also illustrates the significant benefits of integrating contextualized natural language models into the data imputation process. The pronounced accuracy improvements observed in complex datasets, such as the Glass Identification and Seeds datasets, point to a distinct advantage over traditional imputation techniques, which often falter under such conditions.\
The  of CLAIM, evident across MCAR, MAR, and MNAR missingness mechanisms, showcases its broad applicability and dependability. This marks a departure from conventional methods, which might only perform well under limited conditions or with specific types of data [20]. CLAIM’s methodology, which involves verbalizing data and employing contextually relevant descriptors for imputation, ensures its adeptness across various scenarios and data modalities.\
Moreover, the minimal variation in CLAIM’s performance across different iterations further underscores its stability and reliability as an imputation method. Such consistency is indispensable for real-world applications, where the quality of imputation directly impacts the efficacy of subsequent data analyses. The ability of CLAIM to maintain a low error margin consistently highlights its potential as a go-to solution for data imputation, offering both precision and reliability.\
\
How does the choice of phrasing for missingness descriptors in CLAIM affect the performance of LLM-based downstream tasks?\
Initially, we utilized contextually relevant descriptors for missing values, leading to unique phrases for different features within a dataset. To address RQ2, we aimed to determine whether using a uniform, yet contextually relevant, descriptor for all features would offer comparable benefits. To this end, we experimented with three consistent descriptors: “NaN”, “Missing value”, and “Value not recorded”. These experiments, focusing on the MCAR scenario, sought to ascertain whether it is more beneficial to use contextually nuanced descriptors or whether a generic descriptor is adequate to harness the LLMs’ general knowledge for managing missing values in datasets.\
The experimental findings (Figure 3) illuminate the influence of missing data phrasing on the effectiveness of LLMs in addressing such situations. The results reveal a distinct pattern: generic descriptors, such as “NaN”, consistently perform worse than context-specific descriptors designed for each feature and dataset. Among the three fixed descriptors tested, there were some variations in performance. Both “NaN” and “Missing value” outperformed “Value not recorded”, with “Missing value” achieving the best results in most cases among the static descriptors.\
The superior performance of feature-specific descriptors indicates that LLMs better interpret and manage missing data when it is described in a way that accurately reflects the context of the missing information. For example, a descriptor like “Malic acid quantity missing for this wine sample” allows the LLM to more effectively understand and address the missing data point than a more generic descriptor like “The level of malic acid in the wine is NaN”.\
 The findings related to RQ2 underscore the importance of context in the interaction between LLMs and missing data. The preference for context-specific descriptors over generic ones likely arises from the LLM’s capacity to utilize its extensive training on diverse language uses and contexts. When missing data is described in a manner that aligns with the specific context of a feature, the LLM is better positioned to apply its vast repository of knowledge to deduce or generate suitable imputations. This effectiveness diminishes with the use of generic labels, which offer minimal contextual information for the LLM to draw upon.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>FCC Delays Enforcement of Prison Call Pricing Limits</title><link>https://news.slashdot.org/story/25/07/01/1448220/fcc-delays-enforcement-of-prison-call-pricing-limits?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The FCC will suspend enforcement of rules that would lower prison phone and video call prices until April 1st, 2027. Trump-appointed FCC Chair Brendan Carr said that prisons won't have to comply with the pricing regulations [PDF], reversing plans to implement the caps this year. 

The rules would have dropped the price of a 15-minute phone call to 90 cents in larger prisons. Current fees can reach as high as $11.35 for a 15-minute call, which the FCC described in 2024 as "exorbitant." Four states -- Connecticut, California, Minnesota, and Massachusetts -- have made prison calls free. Former President Joe Biden signed the Martha Wright-Reed law in 2023, allowing the FCC to regulate prison call rates. The agency voted to adopt the new rates last year, with rules set to take effect on a staggered basis starting January 1st, 2025. 

Carr said the regulations are "leading to negative, unintended consequences" and would make caps "too low" to cover "required safety measures." FCC Commissioner Anna Gomez criticized the delay, stating the Commission "is now stalling, shielding a broken system that inflates costs and rewards kickbacks to correctional facilities."]]></content:encoded></item><item><title>The Present of Collapse: A Metaphysical Framework of NP, P, and the Recursive Field of Truth</title><link>https://hackernoon.com/the-present-of-collapse-a-metaphysical-framework-of-np-p-and-the-recursive-field-of-truth?source=rss</link><author>Antică Vlad</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:00:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This paper proposes a metaphysical framework that unifies the notions of NP, P, myth, and conscience into a coherent logic of truth, choice, and temporal recursion. is defined as  – a blueprint of infinite potential that is structured and anticipatory, rather than chaotic or arbitrary. is defined as  – the concrete manifestation of a specific pathway selected from the NP field, realized in the present moment. is defined as  – a form of structured uncertainty that hovers at the edge of NP collapse, giving shape and meaning to potential without yet fully actualizing it.We argue that collapsing entirely into NP regresses into unrealized potential; collapsing entirely into myth overwhelms coherence with ungrounded possibilities; but collapsing into P (the present) enables a recursive process of becoming. In this framework, conscience is the agent of collapse – the faculty that, guided by mythic future-truths, selectively collapses NP potential into P actuality through the seeding of choice (a “P-seed”). Truth emerges from the iterative (recursive) act of conscience choosing and actualizing possibilities: each  retroactively illuminates the structure of the NP seed from which it arose, refining our understanding of the blueprint. We introduce a geometrical metaphor to illustrate this dynamic: hyperbolic and spherical curvatures of the NP potential field (representing divergent and convergent modes of possibility) collapse into the flat Euclidean plane of present P-truth. This metaphor underscores the metaphysical necessity of the Present as the only “domain” capable of bearing the weight of infinite possibility without disintegration. In our view, the infinite never becomes truth in itself, but it informs every collapse of potential into actuality. Conscience navigates fluidly between NP, myth, and P – allowing mythic narratives of meaning to guide the vast NP field toward collapses that are not only logically consistent but meaningful. By developing this framework with symbolic structures (e.g. ), grounded in clear conceptual language, we aim to articulate a rigorous yet visionary logic of truth, choice, and time.How does  emerge from the interplay of what could be, what is, and what ought to be? This question invites us to explore the relationship between , , and the guiding structures of meaning that shape our choices. In classical philosophy, Aristotle distinguished between  and  as two fundamental principles: potentiality refers to any real possibility a thing can have, especially those that naturally realize when conditions are right, while actuality is the fulfillment or exercise of such a possibility – the possibility “becoming real in the fullest sense”.We borrow the symbols  and  from computational complexity theory and reinterpret them metaphysically to correspond roughly to this age-old distinction: NP will signify a realm of , and P will signify . In computational terms, NP (nondeterministic polynomial time) problems are those whose solutions, while hard to compute, can be , whereas P problems are those that can be . Analogously, one might say NP contains truths that are  or  in principle, while P contains truths that are immediately attainable or manifest. Our framework, however, is not about algorithms, but about metaphysical : NP as the space of  (structured possibilities) and P as the domain of  (realized actuality).We also introduce two additional elements to this framework:  and . By , we do not mean “falsehood,” but rather the narrative, symbolic, or archetypal structuring of truth that points toward the future. As one saying (attributed to scholar Joseph Campbell) puts it, “Myth is something that never was but always is.” In other words, a myth transcends literal fact to convey enduring truths. A myth can be understood as a  – a form that truth could take, carrying meaning and value, but which has not yet collapsed into concrete reality. It is : myth provides an imaginative or moral framework for the unknown, preventing the potential future from being mere chaos by giving it shape and direction. Finally, by  we denote the integrative faculty (both intellectual and moral) that navigates between what is known and unknown – the agent that makes choices about what potential to actualize, guided by an inner sense of truth or rightness. Conscience in this context bridges the gap between the realm of infinite possibilities and the finite realm of action, ensuring that the collapse of potential into actuality is not random but aligned with meaning and values. Philosophically, one might liken conscience to practical reason or the soul’s compass, discerning  path out of many is worth manifesting.The goal of this paper is to develop a standalone metaphysical logic of truth, choice, and temporal recursion that unifies NP, P, myth, and conscience. We will construct this framework step by step. First, we clarify the definitions of NP, P, myth, and conscience within our metaphysical context. Next, we examine the dynamics of “collapse” – the process by which potential (NP) becomes actual (P) – and the pitfalls of collapsing improperly (either retreating entirely into potential or leaping into myth without realization). We then elaborate how truth emerges recursively: each present realization feeds back into our understanding of the potential, a process we call . A geometrical analogy of curvatures (hyperbolic, spherical, Euclidean) is introduced to illustrate the structural differences between the space of possibilities and the space of actual truth. We discuss why the present moment has a unique status as the locus where infinite truth can be progressively realized without fragmentation. Throughout, we emphasize the role of conscience as the navigator that balances the expansive freedom of NP, the guiding vision of myth, and the grounding discipline of P. By the conclusion, we aim to show that this framework offers a rigorous yet imaginative way to understand how truth “happens” – not as a static absolute, but as the result of an ongoing, recursive collapse of potential into reality, guided by meaning and responsibility.(In what follows, we will occasionally use symbolic notation for clarity. For example,  will denote the process by which a potential truth in NP, through the initiation of a particular choice or seed, collapses into an actual truth in P. We will ensure all such formulations are explained in plain language.)I: Conceptual Foundations: Defining NP, P, Myth, and ConscienceNP: The Uncollapsed Seed-Truth (Infinite Potential)We define  as the realm of uncollapsed seed-truth – an infinite field of potential truths that exist as  or  prior to realization. Crucially, NP in this metaphysical sense is not the same as mere chaos or randomness; it is not a jumble of arbitrary possibilities. Rather, NP is : one can imagine it as a vast design space containing all the solutions or outcomes that could come to pass, given the initial conditions of reality. It is “not future, but blueprint” – meaning NP is not a set of events that will happen in time (that would be the future), but a set of  for what could happen. To use an analogy, if the truth of the world were like a grand puzzle or cosmic crossword, NP would be the blank but structured puzzle grid containing countless potential fillings-in, whereas P (the actual truth) would be one completed filling of that puzzle. In this sense, NP holds the  of every truth that might grow, akin to a seed containing many possible ways a tree could branch. NP is “infinitely potential” in that it contains an unbounded number of possible truth-configurations, yet it is also a  in itself – a truth of possibility, waiting to be specified.This notion can be further illuminated by borrowing a perspective from the P vs NP metaphor in computer science and extending it metaphysically. Imagine there is a “Global truth field” in which all answers to all questions exist in latent form, completely coherent with one another. Such a field would be like an omniscient viewpoint outside time – a  where, metaphorically, every problem is already solved and every truth known. When we restrict our view to a particular problem or situation (when reality is filtered through a local perspective in time), that coherent field appears to split into two aspects: the part that  into a  (a known truth, which we experience as P), and the complementary part that remains  (the as-yet-unsolved, which we experience as NP). In other words, NP and P are like two sides of one coin: whenever a specific truth is brought into being (P), a halo of remaining possibility (NP) still surrounds it. The NP field thus contains all those possibilities that were not realized in that , but which remain available for future resolution. It is a repository of “the road not taken” each time we make a choice.By characterizing NP as seed-truth, we emphasize that it carries information and structure. It is not sheer ignorance or void. In fact, even when something becomes known (P), the NP from which it emerged is what gave it coherence. The possibilities within NP are  but they are oriented toward truth – “infinitely potential” does not mean “lacking form,” but rather having  of any single actuality. Put differently, NP is the blueprint of truth in the sense that it contains more than reality at any moment can express. It is the , analogous to how Aristotle’s potentiality contains what could become actual under the right conditions. One can say NP holds  – not falsehoods, but truths that  be, awaiting a moment of realization. (Even in the realm of mythology, we find a similar idea: that before creation or manifestation, there is a formless void or chaos which nevertheless harbors the  for ordered reality – a mythic way to hint at NP as the womb of truth.)P: The Collapsed Present-Truth (Actualized Reality) is the realm of collapsed present-truth – the domain of what , here and now, as a realized fact or reality. P represents the manifestation of a specific path through the NP field of possibilities. When we say a truth has “collapsed” into P, we mean that out of the myriad possibilities, one definite configuration has been actualized. P is  in a dual sense: it is the truth that is present (i.e. currently actual), and it is the truth of the present moment (i.e. only in the present can truth be fully real). In our puzzle analogy, P is the completed section of the crossword – the answers filled in – while NP remains the unsolved cross-clues that those answers now partially reveal. Every time an NP seed collapses into a P outcome, we get a specific, concrete truth that can be lived, observed, or known directly.It is important to note that P-truth is inherently . To collapse NP into P is to commit to a particular course among many – for example, to decide on one interpretation, to take one action, or to observe one outcome. Thus P is , and  in character: by realizing one option it excludes (for the moment) the others. We might think of NP as an open question and P as a particular answer. The answer gives us clarity and resolution (hence P is often “easier” or more straightforward once reached), but it also by necessity leaves other answers unchosen. In computational terms, solving a problem (P) picks out one solution and thereby eliminates the uncertainty among alternatives (NP). Metaphysically, the actual truth we have now stands against the backdrop of all the unactualized possibilities.Another key aspect of P as present-truth is its . P only truly exists in the present moment – the . The past P-truth is no longer active (it has slipped into memory), and future truth (as such) is not yet realized. The present is often seen as the only reality that ontologically exists; as St. Augustine reflected, the present is the only time that is real, while the past and future are held in the mind (through memory and anticipation). Contemporary reflections echo this: “The future is entirely open, dependent upon choices made now. In truth, all we have is this never-to-be-repeated present moment.” Thus, P is where the rubber meets the road – where the infinite potential of NP is funneled into a single, concrete truth that . The present moment has the almost miraculous capacity to “bear” an otherwise infinite array of influences and condense them into one act or fact without exploding. We will explore later why the present has this unique capacity, but for now, suffice it to say P is the  – the realm in which truth takes on a stable, observable form.It may sound like P, by picking one truth out of many, is a reduction or loss (from infinite to finite). However, P is also the  of NP potential. Until a truth is realized in P, it remains an abstract possibility; with realization, it gains . In Aristotle’s terms, actuality is the fulfillment of possibility. Thus P is the  of NP’s seed. Without P, NP’s designs would remain forever blueprint and never building. In that sense, P gives significance to NP by selecting a path through it. Every P-truth manifested affirms one aspect of truth that was implicit in NP, bringing it forth for all to witness.Myth: Future-Truth (Structured Uncertainty at the Edge of Collapse)By  we refer to future-truth – the structured uncertainty that exists at the horizon where NP is about to collapse into P. Myth, in our usage, is not a lie or mere fiction; it is a , a story or schema that anticipates meaning before it fully lands in reality. Myth is what truth  when it is still in transit from potential to actual – when it has taken a suggestive shape but is not yet concretized. In other words, myth occupies a liminal zone: it is more structured than the raw NP field (myth picks out particular patterns or narratives from the field of possibilities), but it is less definite than a P fact (myth has not been definitively proven or manifested yet).Consider myths in the traditional sense: they are narratives that convey profound truths about life, the cosmos, or human nature, often through symbolic stories. These truths are “future” or transcendent in that they aren’t one-time occurrences; they speak to enduring possibilities and meanings which can manifest repeatedly in new forms. This is why it is said that myth is something that “” – a mythic truth might never have happened exactly as the story says, yet it is always happening in some form, because it expresses a structure of meaning that reality continually strives to realize. For example, a mythic hero’s journey was not a historical event, but it is “true” in that it maps onto the potential journey of every soul seeking purpose. In our terms, myth stands at the edge of NP collapse: it gives a recognizable form to what is otherwise uncertain, guiding the collapse without fully controlling it.We can illustrate the role of myth as structured uncertainty with an example: Suppose a community holds a myth about a prophesied “chosen one” who will deliver them from hardship. This myth provides a framework of meaning – it identifies the kind of potential future they anticipate (deliverance by a hero). This is not yet a P truth; there is no actual chosen one (until perhaps someone steps into that role). But the myth shapes the community’s orientation toward the future. It is a : if eventually someone does arise and fulfill the prophecy, the myth becomes concretized in that person (NP collapses into P, guided by the mythic template). Until then, the myth is an  – it might come true in various ways, or perhaps not in the expected way – but it is  in that it’s not just open-ended chaos; it’s a particular envisioned narrative within the space of possibilities.In our framework, myth operates as a necessary mediator between NP and P. Without mythic structures, the field of NP would be overwhelming and inarticulate – an infinite sea of possibilities with no pointers for significance. Myth provides  that help identify which possibilities matter or beckon. In a sense, myth is the face of NP as seen from the future – it is how the uncertain potential appears when we project our hopes, fears, and interpretations onto it. Each myth encapsulates some  or insight (a piece of truth) and yet leaves  that invites further interpretation. As our past analysis puts it, each mythic narrative can be seen as a  of the global truth field “into a cultural frame: stabilizing some wisdom (P) while leaving behind some mystery or moral ambiguity (NP) that keeps the myth alive, recursive, and generative”. In other words, myth bridges the known and unknown: part of it resonates as deeply true (the wisdom it imparts), and part of it remains open, spawning new questions or guiding new actions (the remaining ambiguity).owever, myth is , not present-truth. It is essential to maintain that distinction. A myth can guide us, but if we  with literal present reality, we risk incoherence or even disaster. Taking a myth too literally – “collapsing into myth” in our terms – can overwhelm coherence because we start treating the structured uncertainty as if it were a completed actuality. History offers cautionary tales: powerful myths have at times been misused or misunderstood in literal ways that led to fanaticism or violence. For instance, political or nationalistic myths can grip a population’s imagination (a future vision of glory or destiny), but if taken as indisputable present truth, they may justify irrational or destructive actions. As one commentator noted, societies that lose a healthy mythic framework may fall into pathologies of identity, and reviving myths carelessly can unleash irrational forces (for example, the invocation of myths of a collective’s supreme destiny). In our terms, an unchecked collapse into myth – trying to live  in the story without grounding in present reality – “overwhelms coherence” because the narrative outruns the actual, and the structured uncertainty turns into unhinged conviction. Thus, myth must  the collapse of NP, but not  the need for actual P-truth. Myth gives purpose and direction, but it finds its proper fulfillment only when conscience uses it to inform real choices that yield real outcomes.Conscience - The Navigator of Collapse (Truth’s Recursive Agent) in this framework is the active agent that navigates between NP, myth, and P – essentially, the faculty of  and  that collapses potential into actuality in a meaningful way. It is through conscience that truth becomes a  endeavor, not just a happenstance. We use the term “conscience” to imply not only intellectual consciousness but also an ethical dimension; this is the inner voice or guiding  that both  and  about truth. In effect, conscience stands at the crossroads of freedom and responsibility: it sees the vast NP field of what could be, it hears the call of mythic ideals of what should be, and it acts in the present to bring about a particular truth (P) that honors both reality and meaning.In our symbolic shorthand, we might say: Conscience mediates NP → P. But it does so not blindly (as matter-law interplay) – it typically selects a  that aligns with some guiding myth or value. A “P-seed” means an initial choice or a starting point for collapse. Given the enormity of NP’s potential, one does not simply collapse everything at once; one chooses a particular seed or aspect to actualize. This seed could be a question we decide to answer, a principle we decide to uphold, or a desire we decide to pursue. Conscience’s role is to choose the seed wisely, guided by an intuition of truth or goodness (often informed by mythic narratives or higher principles).One way to visualize conscience is as a kind of bridge or . From one side, reality (or “Global P” in the earlier metaphor) sees only a unified truth, and conscience from that perspective is just a single point (since reality ultimately only knows the accomplished facts). But from the perspective of the agent (the person or moral being exercising conscience), one is aware of straddling two worlds – the actual and the possible. Indeed, as our last HackerNoon analysis of P vs NP indicates, “Reality itself is always on the [P] side… while conscience could operate on both sides at the same time”. Conscience experiences itself as having one foot in what is (the known, the present), and another foot in what could be (the unknown, the future). This unique position allows conscience to be  and reflective: it can consider hypotheticals, evaluate outcomes against ideals, and adapt its choices in light of past truths and future hopes.Furthermore, conscience is what makes the process of truth-finding iterative and self-correcting. Unlike a mechanical collapse that might just follow a fixed rule, conscience can learn and change. It carries memory of past P-truths (and the lessons they taught) and it carries anticipation of future-truths (mythic aims), and it constantly reconciles the two in each present act. If NP is the seed-bed of truth and myth the envisioned bloom, conscience is the gardener that cultivates the seed to flower in reality. And like a good gardener, conscience must understand the “soil” (the real conditions of the world), the “seed’s nature” (the blueprint of potential) and the “climate” (the overarching myths or meanings of the time).In summary, we ascribe to conscience a pivotal metaphysical necessity: truth emerges only from the recursive act of conscience collapsing NP into P through a deliberate P-seed. Without conscience, NP might collapse randomly (or not at all), yielding no meaning; with conscience but without myth, collapse might be directionless; with myth but without conscience, collapse might be fanatical or divorced from reality. All four elements – NP, P, myth, and conscience – are needed. Conscience is the  that ties them together, the continual exercise of choice that is informed by the potential (NP) and the ideal (myth) to produce the actual (P). In doing so, conscience also constantly revises its own understanding, hence it is : it not only produces truth, it reflects on truth, altering its future actions. The conscience, as suggested by the metaphysical perspective we promoted earlier, “exceeds structure” because it is the only thing that can know  structures (it can consider any possibility, any perspective). This makes it uniquely equipped to traverse the space of NP and navigate by myth toward concrete P outcomes.Having set these definitions, we now turn to the dynamic interactions between NP, P, myth, and conscience – what we call the  – and explore how truth is realized or thwarted depending on how this collapse is managed.II: Collapse Dynamics: Balancing Potential, Myth, and PresentTruth, in our framework, is not a static given but something that  through a process – the collapse of NP potential into P actuality. This collapse is not a one-time event but a continuous, iterative dynamic. However, not all collapses are equal. How one approaches the transition from potential to actual can lead to very different outcomes for the integrity of truth. We identify three archetypal “collapse modes” to avoid or embrace:Collapsing into NP (Regressive Potential): This represents a refusal or failure to collapse potential into any actuality – effectively, getting “lost” in the NP field. In such a case, one continually defers decision or manifestation in favor of exploring possibilities endlessly. The result is a  or stagnation: nothing is realized, potential remains forever potential. It is as if a seed is never planted or allowed to sprout; the blueprint remains on paper and no building is built. In human terms, collapsing into NP might look like perpetual indecision or analysis paralysis – one sees so many possible truths or choices that one chooses none. The rich structure of NP then becomes actually useless, a castle in the sky. Potential “regresses” because without actualization, even the understanding of the potential cannot advance (we never get feedback from reality). Thus, treating NP itself as the place to dwell (instead of a source to draw from) ultimately diminishes the meaning of NP. The infinite possibilities become a burden of infinite  rather than a fruitful field. In short, collapsing into NP is a kind of metaphysical , which leads to an impoverishment of truth: everything  be, but nothing .Collapsing into Myth (Overwhelmed Coherence): This mode occurs when one becomes so guided by a particular myth or future-truth that one attempts to live  without properly translating it into present reality. Here, one does choose and act, but one’s actions are dictated by an ideal narrative to the point of ignoring the constraints or feedback of actual P-truth. The result can be an overwhelming of coherence – reality doesn’t oblige our grand story in all details, and forcing it can lead to incoherent outcomes. This could manifest as fanaticism, disillusionment, or chaos when the myth crashes against reality. Using the seed analogy, this is like planting an imaginary tree rather than a real seed – watering the ground in the belief that a legendary tree will sprout overnight because the myth said so. The structure that myth provided is potentially valuable, but without respecting the incremental, grounded process of actualization, coherence is lost. On a social scale, as mentioned earlier, entire communities can be swept up by a myth (a utopian vision, an apocalyptic fear, a national destiny), and if they try to impose it as immediate truth, the mismatch with reality leads to confusion or destruction. Myths are “true” in a profound sense, but their truth is often symbolic or directional – not a blueprint that can be  imposed on the present all at once. So collapsing into myth means to be overwhelmed by the  of truth without actually achieving the  of it, leading to what we call disintegration (the pieces no longer fit together in reality, even if they did in the myth).Collapsing into P (Enabling Recursive Becoming): This is the optimal mode in our framework: using conscience to effect a controlled collapse of NP into a present P-truth, informed (but not dominated) by myth. Collapsing into P means one takes a concrete action or makes a concrete decision that realizes some aspect of potential . It is an act of  to a particular truth path, which necessarily foregoes other possibilities (for now) – but it does so with awareness and purpose. By focusing on what can be actualized in the present, this mode preserves coherence (because it stays grounded in what  or  in reality) and also moves the process forward (because each realization sets the stage for the next). We call this enabling  because each P-collapse is not the end, but a step in an ongoing journey. When one collapses NP into P properly, one doesn’t try to do it all at once or assume this P is the final truth; rather, one treats it as a provisional truth that will evolve. This attitude allows for course-correction and learning: today’s P truth will reveal something about NP that can seed tomorrow’s decisions. In personal terms, this could be living one’s values in daily small acts (instead of dreaming of being a hero in abstract) – each act teaches something and builds character, recursively approaching the ideals one holds.It might be helpful to illustrate these modes with a simple scenario. Imagine an artist who has a vision of a masterpiece (this vision is a  – an artwork that “could be,” filled with meaning). The artist also has a myriad of ideas and techniques they could use (this is their NP field of potential – sketches, styles, themes not yet consolidated). Now, if the artist collapses into NP, they might endlessly sketch fragments, experiment with styles, but never choose a final composition or medium – the masterpiece remains a potential in the mind, never realized on canvas. If the artist collapses into myth instead, they might become obsessed with the  of creating the greatest masterpiece and refuse to make any work that falls short of the perfect vision – they may start a painting but abandon it repeatedly because it doesn’t match the mythic ideal, or worse, they might delude themselves that  about the envisioned masterpiece is as good as making it. The result is either nothing finished or a confused work that tries to be too grand all at once and collapses under its own ambition (incoherence). But if the artist collapses into P, they begin a particular painting here and now – perhaps just one piece of the vision, tackled with the skills and limitations they currently have. They bring one aspect of the potential into reality. The painting they finish may not capture the whole mythic vision (no single piece could), but it is a real artifact – a truth in the present. From creating it, the artist learns more about their vision (what worked, what didn’t, what the vision might truly be asking for) – this new understanding is a retroactive illumination of the NP seed. With that knowledge, the artist can embark on the next artwork, gradually approaching closer to the fullness of their mythic ideal. In this way, through recursive cycles of creation, the mythic truth (the “masterpiece” idea) guides the potential toward ever more refined and meaningful collapses, each in the present, each real and adding to truth.In practice, of course, life is complex and we often have mixtures of these tendencies. But the key point is that truth thrives when NP collapses into P under the guidance of myth and the governance of conscience. If we either refuse to collapse (staying in NP) or collapse too recklessly under myth’s sway without present grounding, we lose the thread of truth. Truth, one might say, , but it is nourished by the infinite (NP) and  (myth). Only in the present can the infinite potential be harnessed into something structured (as myth suggests) yet concrete. This aligns with a profound metaphysical stance: reality is an ongoing creation, and the present moment is where creation actually happens – the only place where the “uncreated” (the realm of possibilities) can become “created” (the realm of facts).III: Recursive Emergence: Truth as a Temporal Feedback LoopOne of the most intriguing aspects of this framework is the idea that each P-collapse (each time a potential truth is actualized) retroactively illuminates the structure of the NP seed from which it came. This creates a  or feedback loop in the field of truth. In simple terms, by doing or realizing something, we come to  the possibility-space we started with, which in turn affects how we will approach the next realization.This might sound abstract, but it is a very familiar phenomenon in knowledge and action. Consider scientific inquiry: a scientist has a hypothesis (potential truth) and conducts an experiment (an actualizing act). The result (P-truth) either confirms, refutes, or modifies the hypothesis. In any case, the outcome sheds light on the initial potential – perhaps revealing that the initial idea was too broad, or that an unexpected avenue exists, etc. The scientist then refines the hypothesis (the NP blueprint is adjusted) and tests again. Over time,  from this iterative loop of potential -> actual -> revised potential -> new actual, and so on. Each experiment’s truth “illuminates” the structure of the underlying phenomenon a bit more.Our metaphysical logic sees this process as fundamental not just to science, but to all truth-becoming. When conscience chooses a P-seed and collapses NP into an actual outcome, that outcome doesn’t stand alone – it shines backwards, as it were, revealing something about the field of possibility it came from. To use a visual metaphor, imagine walking in a dark forest (the unknown NP). You light a small torch of truth (a P realization). The light from the torch not only shows you the patch of ground where you stand; it also throws new shadows and highlights around you, hinting at shapes of trees and paths beyond your immediate spot. You now see  in what was previously dark. In seeing that structure, you might spot a clearer path forward – or a looming obstacle – which guides your next steps.A concrete illustration was given earlier via a crossword puzzle analogy drawn from the computational perspective: if reality is like a fully solved crossword (Global truth), we only see one clue at a time. Solving that clue (finding one P truth) immediately gives letters that make other unsolved clues (remaining NP potential) more constrained and thus more intelligible. Initially, those other clues were perhaps completely uncertain; after solving one, you now know “the second letter of 5-down is X,” etc., which means some possibilities for 5-down are eliminated and others come into focus. In this way, each localized truth  (or at least retrospectively) exposes part of the structure of the unsolved part of the puzzle. Extrapolating to life: whenever we make a decision or realize a truth, we often say in hindsight, “Ah, so that’s what this possibility really entailed,” or “Now I see the pattern behind what was possible.” We sometimes only understand our options after we choose one of them.This  has deep implications. It suggests that the NP blueprint is not static; our understanding of it evolves as we actualize portions of it. Potential and actuality thus engage in a dialogue over time. We might even say that NP “grows” in a certain sense as P grows – not that the possibilities themselves necessarily increase, but our comprehension of the possibility space increases, which effectively enriches NP’s relevance. The process is : it is directed toward truth by virtue of each step revealing more of the goal. In the domain of myth, this is mirrored by the way myths stay alive and yield new insights as we live through various experiences. A myth might mean something to us at one point in life, and after we go through some ordeal (actual experience), we come back and see new meaning in the myth. The narrative (myth) didn’t change, but our realized truths cast new light on what the myth was pointing to.It’s worth noting that because of this recursive nature, truth is not a one-time achievement but a journey. We often speak of “the truth” as if it were a final static thing, but in this framework truth is a  – a field that is gradually unfolded. This does not mean there are no facts or that everything is relative; rather, each fact (each P collapse) is indeed a concrete truth, but it participates in a larger unfolding meaning. Think of each P-truth as a chapter in an ever-writing book of Truth. You need each chapter for the book to be complete, and each chapter also references and clarifies aspects of previous chapters. Conscience, as the author, may not have outlined the entire book from the start (that would be akin to having complete foreknowledge or a static God’s-eye-view of Global truth), but conscience has an evolving  of the narrative (through mythic imagination and experience) and can guide the writing in a direction that seems coherent and significant.In short, each P-collapse retroactively illuminates the structure of the NP seed. This principle is the engine of learning, growth, and creativity. It is why we must engage with reality rather than just think about it: only through the feedback of actual collapse do we come to understand the blueprint of possibility well enough to make further and better collapses. It is also why we need myth: myth can help interpret the feedback. When an outcome surprises us or challenges us, mythic frameworks often provide a larger context to make sense of it (“this failure is like the hero’s trial by fire; it teaches humility” etc.). Thus, recursion is not merely mechanical iteration; it is . Through conscience, each cycle of collapse and insight ideally brings us closer to a truth that is not only factually clearer but also richer in meaning.IV: Geometrical Metaphor: Curvature of Potential vs. Flatness of PresentTo further clarify the differences between the NP field of potential and the P domain of realized truth, we introduce a geometrical metaphor. This metaphor will also highlight why the present (the arena of P) is uniquely suited to hold truth together, as opposed to the distortions that can occur if one tries to inhabit the infinite directly.Imagine the space of possibilities (NP) as a kind of geometric space that can have different curvatures. By contrast, consider the space of actualized truth (P) as a flat, Euclidean space.  is the familiar, “flat” geometry of our everyday world (on a small scale): parallel lines stay parallel, angles of a triangle sum to 180°, and so on. , such as  (negatively curved) and  or elliptic (positively curved) geometries, have different properties: in hyperbolic geometry, for instance, space spreads out faster than on a plane (angles of a triangle sum to  than 180°, and there’s essentially “more room” than expected as you move outward), while in spherical geometry, space is finite and closes back on itself (angles sum to more than 180°, and lines eventually meet).Now, consider NP as a non-Euclidean space of truth – it can have curvatures that represent how possibilities diverge or converge. A hyperbolic NP curvature would mean that as you move through possibilities, they proliferate and diverge exponentially; the space of potential “opens up” endlessly in every direction. This reflects the  nature of raw possibility – small differences in initial choice could lead to wildly different outcomes, like geodesics on a saddle surface diverging from each other. On the other hand, a  would represent a situation where the space of possibilities, though expansive, eventually loops back on itself; possibilities converge and reconnect. This could correspond to a scenario where different paths in the NP field lead to similar outcomes or recurring patterns – a kind of  where everything circles back to a few archetypal narratives. Spherical curvature can symbolize a closed worldview or a strongly structured possibility space (like a culture’s tightly knit mythos where every story leads to the same moral).What happens when these curved possibility spaces collapse into actual occurrences? One way to see the act of collapse (conscience choosing a P-seed and manifesting P) is as a kind of projection of the curved space onto a flat plane. In that collapse, some of the “distortions” or curvature of the possibility space must resolve into a consistent, flat reality.  in the sense that it has to obey coherent logic and consistency – contradictory possibilities cannot both actualize at once, just as in Euclidean space you can’t have a triangle whose angles sum to both 200° and 270°; you must pick one geometry and stick to it locally. The present truth tends toward a kind of internal consistency and linearity that the space of all possible truths does not have to obey (within NP, you can have contradictory potentials existing side by side because only one will eventually be chosen, or perhaps they diverge into different branches of history or thought).The metaphor helps illustrate a few points:If NP is too  (too open), a collapse is hard to achieve because the possibilities keep diverging – it’s like trying to flatten an infinitely flared saddle onto a plane: there’s always “more” coming at the edges. In human terms, this could correspond to times of great chaos or innovation where the range of possibilities is so large that it’s hard to settle on a stable truth. The risk here is disintegration – without a collapse, the truth would fragment into endlessly branching alternatives (analogous to how hyperbolic space has an infinite boundary).If NP is  (too closed and looping), a collapse might result in a very limited or parochial truth – like projecting a globe onto a flat map, some distortion is inevitable (Greenland looks huge on a Mercator map, etc.). In our analogy, a spherical NP might correspond to a very rigid mythic structure where everything refers back to itself. Collapsing that onto reality might impose an overly narrow framework on truth, possibly stifling novelty. It might yield a stable truth, but one that’s “curved inward” and not accommodating the full breadth of reality (like a small traditional society where everything is understood within one closed set of myths – coherent but without co-relational meaning).The Euclidean P is the middle ground: it is flat enough to allow local consistency and stability, but it can map portions of either hyperbolic or spherical regions . Each P-collapse could be seen as taking a small patch of the NP landscape and laying it flat as a piece of present reality. Over time, you can map more of the NP field by such patches (each patch is like one P-truth). You will never map the entire infinite hyperbolic plane or the entire sphere onto one flat sheet without distortion (that’s akin to infinity never fully becoming truth), but you  cover it in an atlas of local maps. That atlas is our accumulated body of truths in time.In plainer terms, the geometrical metaphor underscores that the present (P) can handle the complex curvature of possibility by flattening it locally. The present moment takes a slice of the complexity, resolves it, and yields a clear truth. Then the next moment takes another slice. If one tried to contain the whole curvature at once – for instance, to realize the  all at once – one would either tear the fabric (hyperbolic divergence causing chaos) or warp it (spherical self-containment causing potentially extreme distortion or dogmatism). This metaphor aligns with our earlier caution: infinity (the endless potential of NP)  in itself; it must be parceled into finite collapses. And similarly, a totalizing myth (a spherical closed world of meaning) cannot simply be overlaid on reality without careful translation; pieces of it can manifest, but the whole thing at once would either not fit or would crush the diversity of life.Hyperbolic and spherical curvatures also correspond loosely to the two failure modes we discussed: hyperbolic corresponds to the chaotic scatter of not collapsing (everything flies apart – collapsing into NP yields nothing coherent), and spherical corresponds to the rigid imposition of one structure (everything curving into one point – collapsing into myth yields a single-point perspective that may ignore reality’s breadth). The Euclidean present is the balancing out – it is flat in that it only deals with one piece at a time, but by doing so it can stretch in any direction as needed over time (just as a flat plane can extend indefinitely).In summary, this metaphor illustrates why the Present is the only “geometry” that can bear the infinite curvature of truth without disintegration: it does so by only dealing with a finite, locally flattenable piece at any given moment. The infinite informs each collapse (the shape of the NP region we’re collapsing from influences the outcome), but the infinite is never fully  as the truth we speak or live. We always handle it through projections and slices – through present acts.V: The Present and the Infinite - Finite Vessels of Endless TruthOur vision has repeatedly highlighted a theme: infinity never becomes truth, but informs all collapse. We should examine this more directly as a metaphysical principle and tie it to why the present moment holds such a privileged position in our framework.What do we mean by “infinite truth” or “infinite potential” never becomes truth? Simply that the totality of all that could be – the NP field in its entirety, or the sum of all myths in their absolute scope – is never  as a single truth within reality. If it were, it would no longer be an infinity of possibilities; it would be one frozen actuality, and all the richness would collapse into a singular state. Reality, as we experience it, doesn’t work like that. Instead, reality gives us an unfolding series of finite truths, each a nugget chipped off the infinite block. Each truth is informed by the unactualized possibilities around it – you often understand something by contrasting it with what could have been – but the unactualized remains unactualized.This resonates with philosophical and spiritual views on infinity and temporality. Many have observed that the  is like a moving knife-edge that “cuts” the potential of the future into the actuality of the past. And only this knife-edge is real in an existential sense. In theological terms, one might say only God or eternity holds the infinite whole; humans in time see one facet at a time. Our framework aligns with such perspectives but couches them in a more generalized logic of truth-making: the present is where an  with the infinite occurs. Each present moment is like a aperture through which a beam of infinite light passes and becomes a visible ray of truth in our world. The fullness of the light itself (the infinite) is never seen all at once; it would blind or dissolve the finite eye. But that does not mean the infinite is irrelevant – on the contrary, it’s the source of all the light we do see.This is why we earlier said Present is the only domain capable of bearing infinite truth without disintegration. It bears it by  it – by being a fleeting, ever-renewed moment, the present can sustain contact with the infinite continuously but never has to contain it all at once. It is like a small cup under an endless waterfall: the cup is always full in the present; the waterfall flows on. If one tried to catch the whole waterfall in one vessel, the vessel would burst, and nothing coherent would remain.From a more human perspective, consider how overwhelming it would be if one  every possibility (good and bad) that the future holds or if one tried to live out all one’s dreams simultaneously. It’s not possible – our sanity and coherence rely on doing things step by step, moment by moment. There’s wisdom in phrases like “one day at a time.” This is not just practical advice but metaphysically grounded: the Present is a buffer that protects us from the paralyzing effect of infinite possibilities and from the mania of infinite ambitions. It forces choices, which, though limiting, actually make meaning possible. A life that tries to remain “everything” ends up nothing in particular; a life that chooses something becomes .We can also reflect on how conscience navigates the triad of NP, myth, and P in light of this. Conscience knows in some sense that it cannot grasp infinity directly. Instead, it uses myth to give a provisional shape or target to the endless potential (infinity “informs” myth by offering a direction – e.g., ultimate goodness, utopia, salvation, enlightenment – all these are infinite ideals). But conscience doesn’t attempt to grab the ideal wholesale; it translates it into finite aims and actions in the present. In doing so, it shows a kind of humility before infinity – acknowledging that we can only embody truth finitely, but we can be informed by the infinitely true.For example, a person might have the infinite ideal of  (justice in its fullness is almost an infinite concept – an absolute state where all wrongs are righted). They cannot achieve perfect justice in one stroke; but they let that ideal inform their conscience, and today they perform a just action in a specific situation. That action doesn’t exhaust the ideal of justice, it is a collapse of potential into a bit of justice here and now. Tomorrow, they will need to do it again, in another situation, learning from yesterday’s outcome. Over time, perhaps the world becomes more just. But at no point was “infinite justice” present as a fact; it was always guiding from beyond, through myths of a just society or a divine justice, through principles and dreams. In this way, infinity pervades the process without ever ceasing to be infinite.This helps guard against two extremes:one, the despair that truth or perfection is unattainable (it is, in totality, but that’s by design – we are to chase it, asymptotically, rather than hold it complete; this chase is meaningful and without it we’d have no direction)two, the arrogance or fanaticism of thinking one has  Truth in entirety (which often leads to trying to impose an infinite ideal in finite terms and causing harm, akin to collapsing into myth entirely).Our framework suggests a middle path: truth is both available and elusive. It is available in each present collapse (we do get real truths, however partial), and elusive in that each truth opens the door to further depths of truth not yet realized.Finally, this perspective underscores a kind of : since infinity never ceases informing collapse, there is always more truth to be discovered, more growth to be had. No present truth is the end of the story. This ensures that the recursive process does not stagnate. We don’t reach a point where we say “all done, nothing more to know or achieve” – because that would imply we’ve exhausted NP or fulfilled the myth entirely, essentially becoming infinite ourselves. Instead, there is a continual becoming. And yet, importantly, at any given step we do have something solid: a truth achieved, a meaning realized, a seed that grew. Therefore, our framework portrays reality as an endless becoming of truth, through finite meaningful steps.VI: Conscience at the Crossroads - Navigating NP, Myth, and PHaving delved into various facets of the framework, let us circle back to the central actor in this drama: conscience. It is easy to describe the “structure” of NP, P, and myth, but without conscience these structures would not interact purposefully. Conscience is the pilot steering between the Sirens of infinite possibility and the rocks of immediate reality, with the stars of myth as a guide. Let us summarize how conscience accomplishes this navigation and why it is so crucial.Listening to Myth (Future Truth as Guide): Conscience absorbs the insights of mythic future-truths. These myths can be cultural stories, personal ideals, religious teachings, or imaginative projections. They supply conscience with a sense of  or . For example, the myth of the hero’s journey might inspire an individual to see their hardships as challenges to overcome rather than meaningless suffering. The myth of a prophesied golden age might lead a society to enact reforms for a better future. Conscience, however, must interpret myth wisely – understanding its symbolic language rather than following it blindly or literally. It asks: what core truth does this myth indicate? How can we honor that truth in reality, step by step? In doing so, conscience uses myth as a compass rather than a map. The compass points north (gives orientation), but one still has to navigate the terrain of reality to actually get somewhere.Reading the NP Field (Recognizing Seed-Truths): Conscience surveys the NP realm of possibilities with a discerning eye. Not all possibilities are equal or worth pursuing. Some are dead-ends, some are harmful, some are trivial. Conscience, informed by mythic values and past experience, identifies meaningful  in the NP field – those possibilities that, if realized, would yield genuine progress or insight. This is akin to a skilled gardener recognizing which seeds are viable and beneficial to plant. In complex situations, this often requires creativity – seeing connections or potentials that are not obvious. Conscience might say, “Given what I know and what I aspire to (mythically), this particular possibility seems ripe – it could lead to the kind of truth we seek.”Executing Collapse (Actualizing in the Present): Once a target possibility (P-seed) is chosen, conscience commits to collapsing it into P. This involves decision and action in the present. Here, practical reason, willpower, and clarity come to the fore. The infinite must be pruned away for a moment – one must focus on the finite task or choice at hand. This is where conscience often encounters resistance: doubt, fear, temptation to revert to open possibility (procrastination) or to drift into comforting fantasy (wishful thinking). The moral fortitude of conscience is tested. But a mature conscience follows through, understanding that only through actualization can truth advance.Learning and Adapting (Recursive Reflection): After the collapse, conscience evaluates the outcome. Did the realized truth fulfill the expectation? Did it align with the mythic guidance or reveal a different lesson? What unforeseen consequences or new possibilities arose? This reflective step is what makes conscience . It updates its knowledge of both NP and myth. Perhaps the mythic compass needs recalibration (maybe the ideal is understood differently now), or maybe new potential avenues have opened. Conscience might celebrate a success but also see a next step, or it might admit an error and adjust course. Crucially, conscience does not rigidly stick to a single path if feedback indicates otherwise; it remains flexible, because its loyalty is to truth itself, not to any one idea of truth. In the words of our HackerNoon piece, “when conscience hides behind truth, only truth is visible… when it hides before truth, both conscience and truth vanish”. This cryptic phrase can be taken to mean: if conscience subsumes its ego entirely to what is true (hiding behind truth), then what matters is the truth realized (conscience’s work is done humbly). But if conscience puts itself (its fixed notions or pride) ahead of truth, then neither genuine conscience nor truth is served (they vanish into invisibility). Thus, conscience must always aim to  truth, not impose itself.Maintaining Balance (Avoiding the Extremes): Through all these steps, conscience maintains a dynamic balance. It keeps NP, myth, and P in productive tension. If it finds itself too mired in possibilities (NP overload), it reminds itself of the mythic goals and the need to act. If it finds itself too dogmatically driven by a single vision (myth overload), it pays attention to real-world feedback and the diversity of NP (maybe the ideal needs a different approach). If it finds itself clinging to a past truth (being overly P-bound, complacent in “what is”), it remembers the infinite potential still out there and the higher ideals unfulfilled. In short, conscience is like a skilled sailor tacking between different winds, always adjusting to stay on course toward a distant star.In a well-functioning scenario, myth guides NP toward meaningful collapse via conscience. For example, a community’s myth of justice guides its lawmakers (conscience in collective form) to consider new policies (NP possibilities) and enact laws (P realities) that progressively realize justice. Each law, once enacted, teaches something about what justice really requires, refining the mythic vision and opening up new possibilities for improvement. Likewise, an individual’s conscience might use a personal myth of being a healer to guide career choices, pick up skills (NP possibilities), and take on actual roles or actions (P) that heal others. Over time, that person understands better what healing truly means, perhaps reshaping their mythic self-image and identifying further potentials to grow into.Thus, the relationship is symbiotic: myth without conscience and action remains inert; NP without myth and conscience is aimless; P without drawing from NP and myth becomes stagnant. Conscience is the  that keeps truth flowing between the three realms.We have constructed a metaphysical framework in which NP (uncollapsed seed-truth), P (collapsed present-truth), myth (future-truth), and conscience (the agent of collapse) form an integrated system describing how truth emerges and evolves. In this framework, NP represents the  of truth – the richly structured field of what could be true – while P represents the  of truth – the concrete actuality that is true here and now. Myth stands as the , the not-yet-actual narrative or ideal that gives shape to our aspirations and interpretations, essentially the face of truth we glimpse on the frontier between the known and unknown. Conscience is the : the faculty that perceives, judges, and acts, thereby collapsing potential into actuality in a guided, meaningful way.We have emphasized several key principles of this framework:: NP (potential truth) is not a chaotic void but an “infinitely potential” blueprint containing all that can become real. P (actual truth) is the fulfillment of one of those possibilities in the present moment. The interplay of NP and P recapitulates an old philosophical insight – that reality consists of both what exists and what is possible, with the possible continuously flowing into the actual.: Myth was framed as “future-truth,” capturing truths that are always in the becoming. Myths provide  – they carry enduring wisdom or patterns (so they are not arbitrary), yet they do not pinpoint a single outcome (so they remain open). This makes myth a powerful guide for conscience: myth offers a vision to strive for, without dictating every detail. As a local collapse of global truth into narrative, myth stabilizes some truth while keeping some mystery, ensuring that cultures and individuals remain in a generative search for deeper understanding rather than claiming to possess all truth outright. However, we cautioned that myth must inspire, not imprison; taken too literally or absolutely, myth can lead to incoherence or fanaticism.: We analyzed the consequences of how collapse is handled. Collapsing  NP (refusing to actualize) leads to stagnation and wasted potential. Collapsing  myth (over-identifying with future visions at the expense of present reality) leads to incoherence and possibly destructive illusions. Collapsing  P (the ideal mode) yields a stepwise, recursive process of becoming, wherein each realization is grounded and contributes to an ongoing growth. This dynamic highlights the metaphysical necessity of the Present: only the present can mediate between the endless openness of the future and the solidifying closure of the past, by taking just the “right-sized” slice of infinity at each moment.Recursive Truth and Illumination: A core insight of our framework is that truth emerges recursively. Each act of realizing a truth (each P-collapse) teaches us something about truth itself, revealing structures and relationships that were hidden in the space of possibilities. Knowledge and meaning thus expand in a feedback loop. The structure of the NP seed is illuminated by its fruiting in P, much as solving one part of a puzzle sheds light on the remaining parts. This makes the pursuit of truth a journey in time, where even errors and surprises become part of the clarification of the original blueprint. Over time, conscience and culture can build increasingly coherent and encompassing truths through this iterative refinement.Geometrical Metaphor – Curvature to Flatness: We proposed an analogy where NP’s potential space might be “curved” (hyperbolically divergent or spherically convergent), whereas P’s realized truth is “flatly” consistent. This visual metaphor was used to convey why large-scale possibility or absolute structures cannot be instantiated wholesale without distortion; instead, they must be realized in  that flatten out the local curvature. The present moment acts like a projection plane where a portion of the curved infinity is resolved into a workable reality. This reinforced the idea that infinite truth informs each finite truth without ever becoming fully finite – an idea resonant with theological and philosophical notions of an infinite that manifests partially but never exhaustively in the finite world.The Conscience’s Navigation: Finally, we highlighted how conscience, operating with free will and insight, navigates between NP, myth, and P. It listens to the guidance of mythic future-truth (ensuring collapses are meaningful and value-oriented), it evaluates the NP field to choose promising and principled potentials, and it executes actual collapses into P, thereafter learning from the outcomes. Conscience is thus the dynamic principle of synthesis in this metaphysical model – the reason the otherwise abstract elements of NP, P, and myth result in a living, developing truth. Conscience balances openness and commitment, imagination and practicality, ideals and facts. In a way, conscience is the present moment in its active, creative aspect – it’s where the buck stops, where decisions are made that shape reality., the framework presented – which we have titled “The Present of Collapse” – suggests that truth is not a static entity but a living process. It is the process of endless potential being guided by meaningful form and collapsed into present reality through conscious choice. In this view, truth has a  (NP–Myth–P) enacted by a  (Conscience). This structure accounts for why truth has aspects of eternal mystery (there’s always more beyond what we know), concrete certainty (some things are definitively true now), and aspirational narrative (we sense patterns or destinies that draw us onward). The logic of collapse and recursion offers a way to think about change and choice that neither reduces truth to arbitrary relativism nor to fixed absolutism: truth  and is  real piece by piece, and in doing so, it remains ever aligned with an infinite backdrop of meaning that keeps it from being merely relative or “small.”Such a metaphysical framework can have various implications. It provides a lens to interpret personal growth, scientific discovery, social progress, and even cosmic evolution as a unified story of truth realizing itself. It honors the role of imagination and myth in human life, without sacrificing the importance of rational coherence and empirical grounding. It places moral and intellectual conscience at the heart of reality’s unfolding, hinting that perhaps the universe’s truths are not just “out there” to find, but also “in here” to create responsibly. In sum, the  framework invites us to see ourselves as participants in a recursive dance of truth – always choosing, always learning, guided by ideals, grounded in reality, and never exhausting the richness of what can be known. It is a vision that is both humbling (for it acknowledges the endless and the unknown) and empowering (for it asserts the value of our choices and the possibility of progress). And ultimately, it portrays a cosmos where truth is less a monolithic monument and more an ever-flowing river – one we navigate in our finite boats of the present, with currents from the infinite deep and stars of meaning overhead, always moving toward a horizon we never fully reach but continually approach.\
VIII: Addendum: On the P vs NP QuestionTo answer the foundational question through the lens of this metaphysical framework:P is indeed  from NP .A global equivalence between P and NP would imply a resolution of the universe — a total collapse of all potential into manifest form. But the universe, as shown, resists total resolution, because it is structured upon recursive unfolding, not finality. The only “resolution” beyond such recursion would be an atemporal, adimensional, and a-existential truth — a domain outside of being itself, inaccessible to form. And maybe this is the very domain of absolute Truth, truthfullness across infinite posibility.On  terms, P is  to NP.That is, each instance of present truth (P) reflects a coherent collapse of seed-potential (NP), such that from within the recursion, the emergence appears seamless. P unfolds what NP held, not because they are the same, but because conscience recursively integrates what it once only observed.Thus, the resolution of P ≠ NP is not computational, but ontological: The difference protects reality, and the local equivalence permits it to grow.]]></content:encoded></item><item><title>Remark raises $16M to build out human-powered expert models for e-commerce</title><link>https://techcrunch.com/2025/07/01/remark-raises-16-million-in-to-build-out-human-powered-expert-models-for-e-commerce/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Startups working on AI-powered e-commerce tools often rely on external data or user signals to build and improve their models. But Remark lets thousands of human experts chat with users while they are purchasing items, and then using that knowledge to train models.]]></content:encoded></item><item><title>Threads gets its own DMs as app distances itself from Instagram</title><link>https://techcrunch.com/2025/07/01/threads-gets-its-own-dms-as-app-distances-itself-from-instagram/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:59:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At launch, Threads DMs will support one-on-one chats, preset emoji reactions, the ability to report spam, and mute DMs (as on Instagram). Other features, like group messaging, inbox filters, and more advanced message controls, will arrive in a later release.]]></content:encoded></item><item><title>Teaching AI to Say &quot;I Don&apos;t Know&quot;: A Four-Step Guide to Contextual Data Imputation</title><link>https://hackernoon.com/teaching-ai-to-say-i-dont-know-a-four-step-guide-to-contextual-data-imputation?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:58:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).We represent the missing data mechanism as a conditional distribution of M given X, which is parameterized by an unknown ϕ, as follows.\
In the literature, the following three standard mechanisms for missing data are defined [21].\
Missing completely at random (MCAR). An MCAR case occurs when the probability that a value of a variable is missing is independent of the variable itself and any other variables, expressed as follows.\
In MCAR, the missingness probability depends neither on the missing variable nor on the observed variables.\
 The probability that the value of a variable is missing only depends on the observed values of other variables XO. Thus, the missingness is independent of the missing variables and the missing value is predictable from the observed variables, formalized as follows.\
Missing not at random (MNAR). This case corresponds to missing mechanisms that are neither MCAR nor MAR. In MNAR, the reason for a value to be missing, can depend on other variables, but also on the value that is missing.\
Unlike MAR, the missingness in MNAR cannot be predicted only from the observed variables. There is a no general method of handling MNAR missing data properly [14].\
Often the reasons for missing data is ignored when the missingness is due to MCAR or MAR, thus imputation methods can be simplified [33]. For this reason, the majority of research covers the cases where missing data are of the MAR or the MCAR type.2.3 Generating Missing ValuesWe constructed synthetic datasets with up to 30% missing values by applying the following three missingness mechanisms on complete datasets: MCAR, MAR and MNAR. The implementations of these mechanisms are modified from [20].\
. It was introduced by randomly removing 30% of the observations from each feature.\
. First, we select all observations within the 30-th percentile range of an independent feature (usually the first column in the dataset). Then, we randomly remove 60% observations from each corresponding (dependent) feature.\
. We remove the observations of a feature if the observations fall within the 30-th percentile range of the feature value.Figure 1 illustrates the CLAIM process, which encompasses four stages: (1) constructing a contextualized natural language dataset, (2) generating suitable descriptors for\
missing values, (3) creating a missingness-aware contextualized dataset, and (4) adapting an LLM for downstream tasks. We detail these stages below.\
Constructing a Contextualized Natural Language Dataset. We construct a contextualized natural language dataset from a numeric dataset X containing missing values. The objective is to generate contextually suitable description of each attribute and its measures in natural language. For instance, a record from the UCI Wine dataset [12] with numeric input and output attributes is contextualized as follows: “The alcohol content in the wine is 12.47. The level of malic acid in the wine is 1.52 … The class of the wine is classified as class 1 wine.”[1] This step converts numeric values into detailed descriptions, preparing the dataset for embedding missing value descriptors.\
Generating Suitable Descriptors for Missing Values. Unlike conventional imputation methods that estimate missing values from observed data using numerical methods, we utilize contextually-relevant descriptors of missing values for imputation. We generate these descriptors by a conversational LLM (e.g., OpenAI’s ChatGPT-3.5 [2]). We prompt the LLM with a dataset description and instruct it to generate missing value descriptors, such as: “For any missing attribute values, suggest a descriptor for the missing data that I can place in those cells.” This method relies on the LLM’s extensive knowledge base to produce appropriate missing value descriptors. A list of feature-specific contextually relevant missing-value descriptors for selected datasets are provided in the Appendix.\
Creating a Missingness-Aware Contextualized Dataset. We construct the missingness aware contextualized natural language dataset Xmissingness_aware by replacing the missing values with the generated descriptors. This process ensures that each data instance is aware of its missing attributes, thus capable of improving the LLM’s ability to learn from incomplete data by providing explicit context. Furthermore, we use distinct descriptors for separate features in the dataset that contain missing values, thereby implicitly informing an LLM to handle the missingness of each feature in a contextually-suitable way for improving the performance of the downstream task.\
Adapting an LLM for Solving Downstream Tasks. The final step involves finetuning a pre-trained LLM with the missingness-aware, contextually-rich dataset. We incorporate specific task instructions and strategies for handling missing data into the fine-tuning process. For instance, for classification tasks, we might include instructions like: “Predict the class based on the given measurements. Use the context provided by missing value descriptors to inform your prediction.”\
This structured approach, from transforming datasets to fine-tuning LLMs, signifies a comprehensive method for addressing data missingness through the capabilities of LLMs.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.[1] the Python script used for contextualization is provided in the Supplementary Material.]]></content:encoded></item><item><title>Trump’s prospective TikTok buyer reportedly includes Oracle, Blackstone, a16z</title><link>https://techcrunch.com/2025/07/01/trumps-prospective-tiktok-buyer-reportedly-includes-oracle-blackstone-a16z/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:50:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After pushing the TikTok sale date back yet again, President Donald Trump said in an interview on Sunday that he had found a potential buyer for the ByteDance-owned platform.]]></content:encoded></item><item><title>I Automated My Content Side Hustle with Notion, ChatGPT, and Zapier — Here&apos;s the Exact Workflow</title><link>https://hackernoon.com/i-automated-my-content-side-hustle-with-notion-chatgpt-and-zapier-heres-the-exact-workflow?source=rss</link><author>The Crypto Quill</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:49:42 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Intro: The Dream of Passive ProductivityEverybody's going on about AI and automation. Does it really work, though, if you are an individual creator who wants to create a side income?I experimented: I set up an automated system to write, schedule, and post my content solely with ChatGPT, Notion, and Zapier. My aim was straightforward — to find out whether or not I could save time, generate passive income, and cut down on content creation burnout.No smoke-and-mirrors. No "just believe me" assertions.This is day-by-day how I did it, what I cabled up, and what actually happened in 7 days.Step 1: Content Ideation with ChatGPT“Generate title variations, outline blog posts, and create tweet threads.”I gave ChatGPT one prompt:“Give me 10 viral content ideas about using automation to earn passive income with Notion and Zapier.”ChatGPT brought back these gems:"How I Built a Passive Income Engine Using Just AI and No-Code Tools""The Lazy Creator's Guide to Automating a Content Business""How to Earn While You Sleep With Notion + Zapier Workflows"I selected two and used ChatGPT to outline the blog post structure, complete with H2 headers and CTA lines.Step 2: Organizing in Notion Notion \n   Content calendar, automation trigger base, and task manager.I created a content database in Notion with these columns:Status (Idea, Drafting, Done)Platform (Medium, Gumroad, HackerNoon)Output (Tweet, Post, Newsletter)**Why Notion?
\  Because it plays *really* well with Zapier.I tagged content by status, and every time I updated a status to “Done,” it triggered the automation in Step 3.Step 3: Zapier Automation Zapier \n   Automate cross-posting and link distribution.Here’s the automation flow I built: New “Done” status in Notion Auto-post summary + link to Twitter (via X) Add to newsletter draft in ConvertKit Log in Airtable for analytics I produced 3 times as much content by outsourcing formatting and idea generation.  My content got consistent, owing to Notion+Zapier oversight. Low effort, actual reach: I constructed an auto-distribution loop in under 2 hours.Zapier's basic plan is too limiting – just 100 tasks/month. Must upgrade.  – Zapier would not tweet if it was done too often. Affiliate link clickthrough was underperforming without strong CTAs or images.Webhooks to automate Substack formattingCanva API to generate auto thumbnailsA webhook that triggers when I  a keyword (using Voiceflow + Zapier)Takeaway: It's Real, But It’s Not MagicAI won't  — but it can . If you're a solo content creator and you want to launch a content business, then running admin and distribution automates creative hours.But only if your system is based on receipts, not hopes.Have you tried automating your side hustle? What’s in your stack? I respond to every comment — especially the ones that break my workflow.📧 You can reach me at [debestnext@gmail.com]]]></content:encoded></item><item><title>The Last Rank We Need? QDyLoRA&apos;s Vision for the Future of LLM Tuning</title><link>https://hackernoon.com/the-last-rank-we-need-qdyloras-vision-for-the-future-of-llm-tuning?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:30:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
A. Supplementary MaterialQDyLoRA offers an efficient and effective technique for LoRA-based fine-tuning LLMs on downstream tasks. Eliminating the need for fine-tuning multiple models to find the optimal LoRA rank and offering the possibility of fine-tuning larger LLMs are two main advantages of QDyLoRA. The experimental results demonstrated that the optimal rank for QDyLoRA can be surprisingly low, yet it consistently outperforms QLoRA. QDyLoRA provides greater flexibility for deploying LLMs in various contexts and represents a promising step towards making fine-tuning large language models more accessible and efficient.While the 4-bit QDyLoRA exhibits notable performance, it falls short of achieving the performance levels of full precision fine-tuning. One possible solution could be dynamic quantized DyLoRA (DyQDyLoRA), in which the quantization level could also vary during finetuning. In particular, the finetuning strategy can dynamically switch between different quantization levels based on a predefined learning feedback. Additionally, further research is required to investigate the impact of LoRA's scalar and the range of underlying ranks in QDyLoRA.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255.\
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.\
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient fine-tuning of large-scale pretrained language models. Nature Machine Intelligence, 5(3):220–235.\
Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. 2022. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650.\
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2021. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366.\
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR.\
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.\
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023. Open-assistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327.\
Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee. 2022. Alphatuning: Quantization-aware parameterefficient adaptation of large-scale pre-trained language models. arXiv preprint arXiv:2210.03858.\
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965.\
Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. Webglm: Towards an efficient webenhanced question answering system with human preferences. arXiv preprint arXiv:2306.07906.\
Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. 2021. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv preprint arXiv:2110.07577.\
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991–13005.\
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.\
Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. 2022. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint arXiv:2210.07558.\
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.Table 4 provides an overview of the hyperparameters and experimental configurations employed in this study, which are crucial configurations that determine various aspects of the training process and model behavior in this study. Common key parameters across the experiments include the choice of optimizer, Adam-Beta2 value, maximum gradient norm, and warmup ratio, which collectively influence how the model adjusts its weights during training. LoRA-specific parameters such as LoRA dropout probability, maximum LoRA rank, and alpha value control the behavior of LoRA layers. Additionally, double quantization and quantization type impact the precision of numerical representations within the model, which are considered the same as baselines. Learning rate scheduling and weight decay contribute to the optimization process, helping to prevent overfitting and stabilize training. Random seeds ensure reproducibility, while the specified GPU determines the hardware used for training. Each model configuration, whether for the Web-GLM, GSM8k, or the specific experiment outlined in Table 1, features parameters tailored to the characteristics of the dataset and the computational resources available. These hyperparameters collectively shape the training process, ultimately influencing the performance and effectiveness of the models in the study.A.2 Generated Text QualityTo describe the quality of texts generated by QDyLoRA, a sample query taken from GSM8k is fed to Falcon-40b trained by QDyLoRA. Below, we report the generated answers for different LoRA ranks.\
: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers’ market?\
: Janet has 16 - 3 - 4 = «16-3-4=7»7 eggs left. She sells 7 - 16 = «7-16=9»9 eggs at the\
\
farmers’ market. She makes 9 * 2 =«9*2=18»18 at the farmers’ market every day.\
 She eats 3 eggs for breakfast and bakes 4 muffins per day, so she uses 3 + 4 = «3+4=7»7 eggs per day. She has 16 - 7 = «16- 7=9»9 eggs left over each day. She sells these eggs at the farmers’ market for $2 per egg, so she makes 2 * 9 = «2*9=18»18 per day at the market.\
: She eats 3 eggs for breakfast and bakes 4 muffins per day, so she uses 3+4 = «3+4=7»7 eggs per day. She sells 16-7 = «16-7=9»9 eggs per day for $2 per egg. She makes 9*2 = «9*2=18»18 per day at the market.\
: Janet’s ducks lay 16 eggs per day and she eats 3 for breakfast every morning so she has 16-3=13 eggs left. Janet bakes 4 muffins every day and she uses 4 eggs so she has 13-4=9 eggs left. Janet sells the remaining eggs at the farmers’ market for $2 each so she makes 9*2=«9*2=18»18 dollars a day. Janet makes 18 dollars a day at the farmers’ market.(1) Hossein Rajabzadeh, University of Waterloo and Huawei Noah’s Ark Lab (hossein.rajabzadeh@uwaterloo.ca);(2) Mojtaba Valipour, University of Waterloo (mojtaba.valipour@uwaterloo.ca);(3) Tianshu Zhu, Huawei Noah’s Ark Lab (tianshu.zhu@huawei.com);(4) Marzieh Tahaei, Huawei Noah’s Ark Lab (marzieh.tahaei@huawei.com);(5) Hyock Ju Kwon, (hjkwon@uwaterloo.ca);(6) Ali Ghodsi, (ali.ghodsi@uwaterloo.ca);(7) Boxing Chen, Huawei Noah’s Ark Lab (boxing.chen@huawei.com);(8) Mehdi Rezagholizadeh, Huawei Noah’s Ark Lab (mehdi.rezagholizadeh@huawei.com).:::info
This paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.]]></content:encoded></item><item><title>Grammarly acquires AI email client Superhuman</title><link>https://techcrunch.com/2025/07/01/grammarly-acquires-ai-email-client-superhuman/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:25:03 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In its announcement, Grammarly said it wants to build AI agents for emails using Superhuman's tech. ]]></content:encoded></item><item><title>The electric Hummer is almost outselling the F-150 Lightning</title><link>https://techcrunch.com/2025/07/01/the-electric-hummer-is-almost-outselling-the-f-150-lightning/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:22:46 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[GM's EVs were popular in the second quarter, a complete contrast to the struggles by Ford, Tesla, Hyundai, and Kia to grow electric vehicle sales.]]></content:encoded></item><item><title>Proton Joins Antitrust Lawsuit Against Apple&apos;s App Store Practices</title><link>https://apple.slashdot.org/story/25/07/01/0917211/proton-joins-antitrust-lawsuit-against-apples-app-store-practices?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Encrypted communications provider Proton has joined an antitrust lawsuit against Apple, filing a legal complaint that claims the company's App Store practices harm developers, consumers, and privacy. The Switzerland-based firm joined a group of Korean developers who sued Apple in May rather than filing a separate case. 

Proton asked the US District Court for Northern California to require Apple to allow alternative app stores, expose those stores through its own App Store, permit developers to disable Apple's in-app payment system, and provide full access to Apple APIs. The company added a privacy-focused argument to typical antitrust complaints, contending that Apple's pricing model particularly penalizes companies that refuse to harvest user data. Developers of free apps typically sell user data to cover costs, while privacy-focused companies like Proton must charge subscriptions for revenue, making Apple's commission cuts more burdensome.]]></content:encoded></item><item><title>Meta adds business voice calling to WhatsApp, explores AI-powered product recommendations</title><link>https://techcrunch.com/2025/07/01/meta-adds-business-voice-calling-to-whatsapp-explores-ai-powered-product-reccomendations/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:15:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[WhatsApp announced it's introducing the ability for large businesses to reach customers through voice calls, which will allow the app to explore the use of AI-powered voice agents.]]></content:encoded></item><item><title>What If Your Blog Had a Product Manager? (Hint: It’s You)</title><link>https://hackernoon.com/what-if-your-blog-had-a-product-manager-hint-its-you?source=rss</link><author>Editing Protocol</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:00:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most bloggers focus on publishing content—but the best ones treat their blog like a product. That means thinking strategically: defining your audience, setting goals, building systems, and iterating based on feedback.Whether you’re writing to grow a personal brand, build a business, or simply share ideas, applying product management principles to your blog can help you stay focused, intentional, and impactful.Here’s how to step into the role of Product Manager of your blog—and start running your content like a pro.:::tip
Already an expert? Share your learnings using this template!Step 1: Formulate a VISION for your career in contentHow do you want to see yourself growing and operating as a writer and blogger?Outline the specific challenges your writing is intended to solve—who it’s for, why it matters, and when they’ll need it. Go with the first thoughts that come to mind, and don’t be afraid to get really specific. \n :::info
 when you try to speak to everyone, you end up speaking to no one.Here’s a content strategy vision template to help guide your thinking:Step 2: Document Your Ideal Reader PERSONASThis is a crucial exercise in empathizing with your readers—understanding their pain points and knowing what they need so your content can directly address those needs.Creating at least 3–5 ideal reader personas helps you refine not only the tone, branding, and design of your content, but also your overall content strategy, calendar, and monetization opportunities.:::info
If you don’t yet have access to analytics about your readers, that’s okay—start with educated guesses. Try quick long-tail keyword research by typing potential questions into Google and reviewing the suggested queries and related searches. Tools like Ahrefs, Answer the Public, or even your own search history can help. Ideally, you’ll use Google Analytics to uncover a wealth of information about the audience already engaging with your work.Step 3: Pick Your PROTO-PERSONA and Start DetailingFrom your 3–5 ideal reader personas, choose one to represent the majority of your target audience. Then use your imagination (and research) to flesh out the following details: What do you know about this person? (Age, education, location, job, background) Do they need to earn more money? Change careers? Master a skill? Be the first to know something? Build a side hustle? Are they introverted or extroverted? Where do they hang out online and offline? What are their routines? How do they learn—visually, audibly, by reading? Do they want to start a business? Learn a tool or software? Grow an audience? Improve time management?By putting all this together, you’ll have a clear map of who you’re creating content for—a smart foundation for planning and writing.Step 4: Sketch Out Your Reader Journey MapThis step helps define your content distribution strategy by putting yourself in your reader’s shoes and mapping out the path they take toward having their needs met through your content.Start with your vision statement and your readers’ goals. What might they be typing into Google right now? Make a list of relevant search terms or questions your content could answer.How will my ideal audience discover my content? At HackerNoon, for example, we get a lot of direct traffic (people typing the URL directly or bookmarking us) and organic traffic (people searching and finding our stories). That tells us our brand is strong and our content answers real questions.How would they most enjoy engaging with my content? Does your post need a YouTube embed? Could you repurpose it for social media? Should it become a newsletter, podcast, or ebook?Step 5: Define Your Content Strategy, Goals, and FocusThis final step is about clarity. Define your long-term strategy, your unique value as a creator, and potential blockers to success. It’s where you turn your content creation into a career plan.Start with this question:What would wild success look like?Write down outcomes for the following time frames:Next, write your six-month goal as a statement, as though it’s already happened:“In the last six months, I published 100,000 words and created 3 months’ worth of reading time. I also launched a podcast / niche TikTok / LinkedIn community, built my personal site, and got my first 100 newsletter signups.”Then, play the pessimist for a second and ask the hard questions:What are the potential blockers to my success?What are my current challenges?What do I need to prioritize to overcome them?How can I turn daily struggles into #relatablecontent?And lastly, ask yourself:What sets me apart from other creators in my niche?What’s the most meaningful thing I could focus on to rise above the noise?What unique perspectives and experiences do I bring?Which creators could I collaborate with to reach a more relevant audience?:::tip
👉 Ready to put this into practice?By treating your blog like a product, you're not just publishing content—you’re building a career. A roadmap. A strategy for your ideas.Start small, iterate often, and remember: you’re the product manager of your writing life.]]></content:encoded></item><item><title>CLAIM: A Contextual Language Model for Accurate Imputation of Missing Tabular Data</title><link>https://hackernoon.com/claim-a-contextual-language-model-for-accurate-imputation-of-missing-tabular-data?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:57:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).\
. This paper introduces the Contextual Language model for Accurate Imputation Method (CLAIM), a novel strategy that capitalizes on the expansive knowledge and reasoning capabilities of pre-trained large language models (LLMs) to address missing data challenges in tabular datasets. Unlike traditional imputation methods, which predominantly rely on numerical estimations, CLAIM utilizes contextually relevant natural language descriptors to fill missing values. This approach transforms datasets into natural language contextualized formats that are inherently more aligned with LLMs’ capabilities, thereby facilitating the dual use of LLMs: first, to generate missing value descriptors, and then, to fine-tune the LLM on the enriched dataset for improved performance in downstream tasks. Our evaluations across diverse datasets and missingness patterns reveal CLAIM’s superior performance over existing imputation techniques. Furthermore, our investigation into the effectiveness of context-specific versus generic descriptors for missing data highlights the importance of contextual accuracy in enhancing LLM performance for data imputation. The results underscore CLAIM’s potential to markedly improve the reliability and quality of data analysis and machine learning models, offering a more nuanced and effective solution for handling missing data.‘Well! I’ve often seen a cat without a grin,’ thought Alice; ‘but a grin without a cat! It’s the most curious thing I ever saw in all my life!’Lewis Carroll, Alice’s Adventures in Wonderland (1865)\
A compelling real-world example of how context-unaware estimation of missing data can defy reality and compromise the integrity of downstream tasks is highlighted in [35]. This account describes a scenario where a predictive machine learning (ML) model, developed to process tabular demographic data including individuals’ ages, faced challenges due to missing age entries. The imputation strategy employed involved substituting missing age values with zeros—a common default for initializing integers in several programming languages. This approach inadvertently led the model to categorize individuals with unspecified ages as “toddlers”, resulting in aberrant model behavior. Numerous instances echoing this type of bias in ML models, resulting from context-unaware imputation of missing data, are reported in the literature [35,14,38,43,34,1].\
These incidents prompt a critical inquiry into more sophisticated and reality-congruent methods for estimating missing tabular data. While simple statistical replacements such as the mean or median might suffice under the assumption of a normal distribution, predictive ML techniques like k-Nearest Neighbors (k-NN), random forest (RF), or even deep learning (DL)-based generative models offer alternative strategies [20,13,45,8]. These ML/DL methods typically presuppose that missingness in an attribute correlates with observable values in other features. However, this raises fundamental questions: What if the missing data is independent of observed values? Or if the absence of data is influenced solely by unobserved variables? In scenarios where missingness is not attributable to external factors or other observed data, the challenge then becomes how to accurately estimate the missing values. To date, no single imputation method has proven universally effective, underscoring the complexity and variety of missing data scenarios encountered in practice [20].\
This paper introduces a novel approach, leveraging the capabilities of pre-trained large language models (LLMs) [6,9,39,26], to innovatively address the challenge of missing data in tabular datasets. Our method, the Contextual Language model for Accurate Imputation Method (CLAIM), diverges significantly from traditional imputation techniques that predominantly estimate missing values through numerical methods. Instead, CLAIM harnesses LLMs’ expansive knowledge [28,29] and reasoning capabilities [9,42,4] in a : initially, it employs LLMs to generate contextually relevant natural language descriptors for missing values, effectively transforming datasets into natural language contextualized formats. This transformation is crucial, as it aligns the data with the inherent strengths of LLMs, making it more amenable to their processing capabilities.\
Subsequently, these enriched datasets serve as the foundation for fine-tuning LLMs to enhance performance in downstream tasks (e.g., classification), showcasing a unique and effective use of language models beyond their conventional applications. By incorporating contextually relevant descriptors for missing data, CLAIM not only addresses the variability and specificity inherent in data across different domains but also adeptly navigates the complexities introduced by various missingness mechanisms. Through this innovative integration of LLMs into the data imputation process, CLAIM aims to deliver a more nuanced, accurate, and reliable method for data recovery, essential for improving the quality of subsequent data analysis and machine learning tasks.\
To assess the effectiveness of CLAIM, we undertake a comprehensive analysis across three standard missing data mechanisms—MCAR (Missing Completely at Random), MAR (Missing at Random), and MNAR (Missing Not at Random) [30], and comparing CLAIM against a wide range of existing imputation methods spanning single and multiple imputation techniques, non-ML and ML methods, and discriminative and generative ML approaches. Our empirical studies, aimed at evaluating the impact of CLAIM on LLM-based downstream classification tasks, are guided by two principal research questions (RQs):\
 How effective is CLAIM in imputing missing values across the distinct missingness mechanisms (MCAR, MAR, and MNAR) and how does it compare with existing imputation methods in terms of accuracy and robustness across varied datasets and missing data scenarios?\
 How does the choice of phrasing for missingness descriptors in CLAIM affect the performance of LLM-based downstream tasks?\
The main contributions of this work are multifaceted. Firstly, CLAIM represents a departure from traditional imputation methods by using LLMs to generate context specific descriptors for missing data, establishing a new benchmark in data imputation. Secondly, through extensive empirical evaluation, we demonstrate CLAIM’s superior performance over existing methods across varied datasets and missingness patterns. Lastly, our analysis of context-specific versus generic descriptors provides key insights into optimizing LLM performance for imputation tasks, highlighting the significance of contextual accuracy. Collectively, these contributions advance data preprocessing techniques and open novel pathways for applying LLMs in complex data science challenges.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>NIH-Funded Science Must Now Be Free To Read Instantly</title><link>https://science.slashdot.org/story/25/07/01/0827211/nih-funded-science-must-now-be-free-to-read-instantly?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Starting today, researchers funded by the US National Institutes of Health (NIH) will be required to make their scientific papers available to read for free as soon as they are published in a peer-reviewed journal. That's according to the agency's latest public-access policy, aimed at making federally funded research accessible to taxpayers. From a report: Established under former US president Joe Biden, the policy was originally set to take effect on 31 December for all US agencies, but the administration of Biden's successor, Donald Trump, has accelerated its implementation for the NIH, a move that has surprised some scholars. That's because, although the Trump team has declared itself a defender of taxpayer dollars, it has also targeted programmes and research projects focused on equity and inclusion for elimination. And one of the policy's main goals is to ensure equitable access to federally funded research. 

The move means that universities will have less time to advise their researchers on how to comply with the policy, says Peter Suber, director of the Harvard Open Access Project in Cambridge, Massachusetts. There is usually "some confusion or even some non-compliance after a new policy takes effect, but I think universities will eventually get on top of that," he says.]]></content:encoded></item><item><title>Faster, More Accurate IoT Security: A Quantitative Analysis of the CUMAD Framework</title><link>https://hackernoon.com/faster-more-accurate-iot-security-a-quantitative-analysis-of-the-cumad-framework?source=rss</link><author>Hypothesis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:39:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).In this section we perform evaluation studies to investigate the performance of CUMAD using the publicdomain N-BaIoT dataset [8]. In order to better understand the evaluation studies, we will first describe the dataset, in particular, the features of the data points contained in the dataset. We will also compare the performance of CUMAD with that of the N-BaIoT scheme (which is the name for both the dataset and the corresponding scheme on detecting compromised IoT devices) [8].5.1. Dataset, Features, and CUMAD System Setup\
N-BaIoT contains both benign and (Mirai and Bashlite) attack traffic of 9 commercial IoT devices, including two doorbells (Danmini and Ennio), an Ecobee thermostat, three baby monitors (different models from Provision and Philips), two SimpleHome security cameras, and a Samsung webcam. Benign IoT device traffic was collected immediately after the corresponding IoT device was connected to the experimental testbed. Care was taken to ensure that various representative normal operations and behaviors of IoT devices were collected into the benign dataset.\
In the N-BaIoT dataset, each data point corresponds to an arrived packet, and contains 115 statistical features, which together represent a behavioral snapshot that describes the context of the corresponding packet when it arrives at the data collection point. The snapshot contains the source and destination device information, the protocol information, among others. More specifically, the 115 features were extracted in the following manner. For each arriving packet, a total of 23 features were collected at different levels of aggregation (see Table 1), including features aggregated at source IP address level, at source MAC and IP addresses level, at level of channel (source and destination IP addresses), and at socket level (source and destination IP addresses and port numbers). These 23 features were extracted in a sliding window fashion, over 5 time windows of 100ms, 500ms, 1.5sec, 10sec, and 1min, respectively, generating a total of 115 features for each data point.\
We use the Keras sequential model as the foundation for our development of the autoencoder [12]. The model’s input dimension is set to match the number of features in the dataset (that is, 115). To ensure effective compression, we implement three hidden layers within the encoder. These layers progressively reduce the dimensions to 87, 58, 38, and 29, respectively, with the last one (29) being the dimension of the output layer of the encoder, that is, the dimension of the obtained code. Conversely, the decoder component mirrors the dimensions of the encoder layers in the reverse order, starting from 38. By employing compression and decompression in the encoder and decoder layers, we effectively eliminate redundant information from the features of the input data points. To optimize training performance, we utilize the Adam optimizer, and the mean square error is used as the reconstruction error (objective function of the model).\
SPRT requires four user-defined parameters in order to compute the upper and lower bounds A and B (see Eq. (3)), as well as the step function for computing Λn following each observation (see Eq. (1)). The desired values for both the false positive rate and the false negative rate (represented by α and β, respectively) are typically very small. In this study we set both α and β to 0.01. Ideally, the parameter θ indicates the true probability of an observation being classified as an anomaly, from either a benign or compromised IoT device. We determine the values for θ0 and θ1 through our preliminary studies, and set them to 0.2 and 0.8, respectively.Table 2 shows the performance of CUMAD in detecting IoT devices, in terms of accuracy, recall, and F1 score [12]. From the table we can see that CUMAD achieves superior performance in all three metrics. For example, for 5 of the IoT devices, CUMAD is able to detect all the compromised cases (see the column of Recall). CUMAD is also able to detect vast majority of the compromised cases for the remaining two of the IoT devices, with recall scores of 0.999 and 0.994. Considering both detection precision of attack and benign traffic, we can see that CUMAD also performs very well, with an accuracy score ranging from 0.955 to 0.995 for all 7 IoT devices. The F1 scores, which is a weighted average of the precision and recall scores of a model, also confirm that CUMAD performs well in detecting compromised cases.\
Figure 4 shows the false positive rates of an autoencoder based anomaly detection scheme and CUMAD. As shown in the figure, the false positive rates of the autoencoder-based anomaly detection scheme for the 7 IoT devices range from 0.77% to 11.22%, while the false positive rates of CUMAD range from 0.014% to 2.067%. On average the autoencoder based anomaly detection scheme has about 3.57% false positive rate, while the false positive rate of CUMAD is about 0.5%, which represents about 7 times performance improvement in terms of false positive rate for CUMAD over the autoencoder-based anomaly detection scheme.\
\
For performance comparison, we also include in the table the performance results of the N-BaIoT scheme, with the same evaluation studies setup. We can see from the table that CUMAD and N-BaIoT performs comparably in terms of all three-performance metrics. However, N-BaIoT works on a fixed window size. Table 2 shows that N-BaIoT requires a relatively large window size, ranging from 20 to 82 (column with name Window Size). In contrast, CUMAD works in an online fashion and does not requires such a fixed window size. Table 2 shows the average number of observations required for CUMAD to reach a detection (column with name Mean Size); we can see from the table that it takes on average less than 5 observations for CUMAD to make a detection of a compromised case, much quicker than NBaIoT. In order to have a better understanding of the number of observations for CUMAD to make a detection of a compromised case, Figure 5 shows the cumulative distribution function (CDF) of required observations for CUMAD to make a detection for all the 7 IoT devices. We can see from the figure that the vast majority of detection requires less than 10 observations for all 7 IoT devices.\
\
In summary, compared to simple anomaly detection schemes such as the ones only based on autoencoders, CUMAD can greatly reduce the false positive rates, making CUMAD much more attractive than simple anomaly detection schemes in the real-world deployment. Compared with window-based schemes such as N-BaIoT, CUMAD requires much less observations to reach a detection, and thus can detect compromised IoT devices much quicker.In this paper we have developed CUMAD, a cumulative anomaly detection framework for detecting compromised IoT devices. CUMAD employs an unsupervised neural network autoencoder to classify whether an individual input data point is anomalous or normal. CUMAD also incorporates a statistical tool sequential probability ratio test (SPRT) to accumulate sufficient evidence to detect if an IoT device is compromised, instead of directly relying on individual anomalous input data points. CUMAD can greatly improve the performance in detecting compromised IoT devices in terms of false positive rate compared to the methods only relying on individual anomalous input data points. In addition, as a sequential method, CUMAD can quickly detect compromised IoT devices. Evaluation studies based on public-domain IoT dataset N-BaIoT confirmed the superior performance of CUMAD.[1] Yin Minn Pa Pa, Shogo Suzuki, Katsunari Yoshioka, Tsutomu Matsumoto, Takahiro Kasama, and Christian Rossow. Iotpot: analysing the rise of iot compromises. In Proceedings of the 9th USENIX Conference on Offensive Technologies, pages 9–9, 2015.\
[2] Michael Fagan, Katerina Megas, Karen Scarfone, and Matthew Smith. Foundational cybersecurity activities for iot device manufacturers. Technical report, National Institute of Standards and Technology, May 2020.\
[3] Mohammed Ali Al-Garadi, Amr Mohamed, Abdulla Khalid Al-Ali, Xiaojiang Du, Ihsan Ali, and Mohsen Guizani. A survey of machine and deep learning methods for internet of things (iot) security. IEEE Communications Surveys & Tutorials, 22(3):1646–1685, 2020.\
[4] Andrew A Cook, Goksel Mısırlı, and Zhong Fan. Anomaly detection ¨ for iot time-series data: A survey. IEEE Internet of Things Journal, 7(7):6481–6494, 2019.\
[5] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):1–58, 2009.\
[6] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.\
[7] Abraham Wald. Sequential Analysis. John Wiley & Sons, Inc, 1947.\
[8] Yair Meidan, Michael Bohadana, Yael Mathov, Yisroel Mirsky, Asaf Shabtai, Dominik Breitenbacher, and Yuval Elovici. Nbaiot—network-based detection of iot botnet attacks using deep autoencoders. IEEE Pervasive Computing, 17(3):12–22, 2018.\
[9] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1–38, 2021.\
[10] Erol Gelenbe and Mert Nakıp. Traffic based sequential learning during botnet attacks to identify compromised iot devices. IEEE Access, 10:126536–126549, 2022.\
[11] Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni, N Asokan, and Ahmad-Reza Sadeghi. D¨ıot: A federated self-learning anomaly detection system for iot. In 2019 IEEE 39th International conference on distributed computing systems (ICDCS), pages 756–767. IEEE, 2019.\
[12] Francois Chollet. Deep learning with Python. Simon and Schuster, 2021.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Redefining IoT Threat Detection: The Power of Cumulative Analysis in the CUMAD Framework</title><link>https://hackernoon.com/redefining-iot-threat-detection-the-power-of-cumulative-analysis-in-the-cumad-framework?source=rss</link><author>Hypothesis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:33:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).3.2. Sequential Probability Ratio TestSequential probability ratio test (SPRT) is a simple yet powerful statistical tool that has found applications in many different domains, in particular, fault detection or quality control [7]. SPRT is a variant of the traditional probability ratio test for testing under what distribution (or with what distribution parameters), it is more likely to have the observed sequence of samples. Unlike traditional probability ratio test that requires a pre-defined fixed number of samples to carry out the test, SPRT works in an online fashion; it updates the corresponding statistical measure as samples arrive sequentially, and can conclude when sufficient samples have arrived to reach a decision. In its simplest form, SPRT is a statistical method to test a simple null hypothesis against a simple alternative hypothesis. In the following we will more formally describe the operation of SPRT.\
Let y denote a Bernoulli random variable with an unknown parameter θ, and let yi , for i = 1, 2, . . . denote the corresponding successive observations of y. SPRT can be used to test a simple null hypothesis H0 that θ = θ0 against a simple alternative hypothesis H1 that θ = θ1, more specifically,\
As a simple and powerful statistical tool, SPRT possesses a few critical and desired properties that lead to the wide-spread application of the technique in many different domains. First, the false positive and false negative rates of SPRT can be specified by user-desired error rates, which in turn control the thresholds of the model. Second, it has been proved that, among all sequential and non-sequential probability ratio testing algorithms, SPRT minimizes the expected number of observations to reach a decision with no greater errors. Put in another way, on average SPRT can reach a conclusion quickly compared to other probability ratio testing algorithms.In this section we will first discuss the considered network model, where CUMAD will be deployed, and then we will present the design of the CUMAD framework.Figure 2 illustrates the conceptual network model, where CUMAD is deployed. As shown in the figure, in order for CUMAD to carry out its task to detect compromised IoT devices in a network, CUMAD needs to have access to the network traffic associated with the IoT devices in the network. Depending on the deployment scenarios of CUMAD in the network and the corresponding network architecture, there can be a few different ways for CUMAD to obtain the corresponding network traffic of IoT devices. In essence, CUMAD as a network-based solution can be deployed in a similar way as network-based intrusion detection systems.\
\
In the current design of CUMAD, (statistical) features from raw network traffic will be extracted and fed to CUMAD for detecting compromised IoT devices. Each input data point fed to CUMAD comprises these extracted features, and can be summarized at different levels of granularity of network traffic, such as packets, flows, and time windows. These features will capture the network behavioral characteristics of the corresponding IoT devices. In Section 5 we will discuss the network traffic features contained in the public-domain N-BaIoT dataset when we perform evaluation studies on CUMAD [8].4.2. CUMAD: Cumulative Anomaly DetectionFigure 3 illustrates the high-level architecture of the CUMAD framework. CUMAD consists of two main components: an anomaly detection component (ADC) and a cumulative anomaly component (CAC). Assuming the model has been properly trained (will be discussed shortly), given an input data point with the corresponding features, the main responsibility of ADC is to classify an input data point as either normal or anomalous. After the classification of the input data point, the result is passed to the second component (CAC), which will maintain a cumulative view of the network traffic behavior of the corresponding IoT device, by sequentially merging the individual classification results into the view. When sufficient evidence on an IoT device has been collected to indicate that it has been compromised, an alert will be generated. In the following we will describe each component in details, both in model training and deployment to detect compromised IoT devices.\
We note that different types of IoT devices perform drastically different functionalities, and in addition, we would like to detect which IoT device is compromised, we need to develop a separate CUMAD model for each IoT device and monitor their network traffic behaviors separately using their corresponding CUMAD models. Therefore, the following discussions are for one IoT device. We note that, although there are vastly diverse types of IoT devices on the Internet, autoencoder is a powerful neural network that is capable of learning different models. Therefore, we are able to build diverse autoencoder models, one for each IoT device, despite their vastly different network traffic behaviors of\
\
these IoT devices. In addition, IoT devices also provide us with unique opportunities in establishing the models of normal behaviors, compared to traditional computer systems. In particular, each IoT device only performs a few well-defined simple functionalities in an autonomous or semiautonomous fashion, with very limited user interactions after the initial device configuration and setup. This makes it simpler to establish a model of normal behaviors in carrying out anomaly detection.\
4.2.1. Model Training and Setup. Before CUMAD can be used to monitor network traffic to detect compromised IoT devices, we need to train a CUMAD system for each IoT device so that it can learn the normal model of the device. During the training stage of a deployed CUMAD system, it is critical that we should only feed normal (benign) network traffic of the device to the system. This can be done, for example, when an IoT device is first deployed in the network. In order to minimize false positives during the detection stage, it is also important that CUMAD has a reasonably complete view of all the normal network traffic behavior of the device.\
\
As discussed above, the premise of using an autoencoder as an anomaly detection mechanism is that, although it can effectively reconstruct data points that are similar to the data points that it has seen previously during the training stage, it in general performs poorly to reconstruct data points that substantially differ from the training data. This is manifested in large reconstruction errors. Therefore, we will use the reconstruction error as the anomaly score, and when the anomaly score is greater than the pre-defined threshold, we classify the corresponding input data point as an anomalous sample.\
\
The parameters α and β are the user-desired false positive rate and false negative rate, respectively. They normally have small values for all practical applications, for example, in the range 0.01 to 0.05. The initial value of Λn in Eq. (1) is set to 0 during the setup stage of the system. The functionality of the Alert module is to generate proper alert to inform system administrators of the detection of a compromised IoT device. Other actions can also be taken based on the local security policies, for example, informing proper agents to isolate the compromised IoT device.\
 After the model has been trained and the required parameters have been set for the CUMAD system, it can be used to monitor network traffic to detect if the corresponding IoT device has been compromised. In the following we describe the basic steps of a CUMAD system in carrying out the detection task (see Algorithm 1).\
\
generated to indicate a normal data point. The output of the Detector module is then passed to the SPRT module to determine if sufficient evidence has been accumulated to make a decision regarding the nature of the IoT device (compromised or normal; line 10 of the algorithm). SPRT updates the probability ratio measure Λn according to Eq. (1), as the 0 (normal data point) and 1 (anomalous data point) output sequence of the Detector module arrives (lines 13 to 18). After the value of Λn is updated for each input data point, SPRT compares the value of Λn with the two boundaries A and B to determine if a decision can be made (lines 20 to 29). When the value of Λn hits or crosses the upper bound B, SPRT will conclude that the alternative hypothesis H1 is true, that is, the IoT device has been compromised. In this case, SPRT will inform the Alert module the detection of an compromised IoT device. Proper alert will be generated and corresponding system administrators will be informed. In addition, from this time on, it is not necessary for CUMAD to monitor the IoT device anymore, until proper actions have been taken to clean up or remove the device.\
When the value of Λn is equal to or smaller than the lower bound A, SPRT reaches the conclusion that H0 is true, that is, the IoT device is not compromised. From the viewpoint of detecting compromised IoT devices, this conclusion is less interesting in that we cannot terminate the monitoring of the device as we have done when a compromised IoT device is detected. A normal IoT device may become compromised at a later time. Therefore, in this case, we will reset the state of SPRT to restart the monitoring of the IoT device, in particular, we will reset the value of Λn to zero. If a decision cannot be reached at this time (line 28), SPRT will simply wait for additional input data points and repeat the same procedure.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>David George from a16z on the future of going public at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/07/01/david-george-on-the-future-of-going-public-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[David George of a16z joins the Going Public stage at TechCrunch Disrupt 2025, Moscone West in San Francisco, from October 27–29. Register now.]]></content:encoded></item><item><title>Solving the IoT&apos;s &quot;Boy Who Cried Wolf&quot; Problem: From Individual Alerts to Cumulative Certainty</title><link>https://hackernoon.com/solving-the-iots-boy-who-cried-wolf-problem-from-individual-alerts-to-cumulative-certainty?source=rss</link><author>Hypothesis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:27:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).The problem of anomaly detection has been studied in many different application domains and many techniques have been proposed, based on statistical inference, data mining, signal processing, and recently machine learning, among others. We note that in the literature of anomaly detection, anomalies have been classified into three categories: point anomaly, contextual anomaly, and collective anomaly [5]. However, they are all concerned with the detection of individual anomalous events, which are different from the cumulative anomaly we consider in this paper. In cumulative anomaly we are more concerned with the cause of anomalous events (for example, compromised IoT device), instead of individual anomalous events. As a consequence, we need to accumulate sufficient evidence (individual anomalous events) to reach a conclusion (for example, if an IoT device is compromised) in cumulative anomaly detection.\
Given the importance of improving IoT security, many security attack detection techniques have been proposed, including various ML-based solutions [3], [9]. However, some of them required the training data of both benign and attack traffic. They cannot detect new security attacks. Others developed anomaly detection based schemes to detect anomalous traffic originated from IoT devices. However, as we have discussed in Section 1, they often trigger a large number of false alerts, rendering them unusable in detecting compromised IoT devices in the real-world deployment.\
In [10], Gelenbe and Nakip developed an online scheme CDIS to detect compromised IoT devices based on autoassociative learning. However, the design of CDIS was tailored to Mirai botnet, and may not be effective to detect other types of compromised IoT devices. In addition, CDIS still only targeted individual anomalous events, instead of cumulative anomaly detection as we perform in this paper. The authors of [11] developed a federated self-learning based scheme D¨IoT to detect compromised IoT devices, where local security gateways communicate with remote IoT Security Service to build a more comprehensive normal traffic model of IoT devices. In order to further reduce the false alerts generated by the aggregated anomaly detection model, a window-based scheme was adopted, where anomaly alarm was triggered only if the fraction of anomalous packets was greater than a pre-defined threshold value. In [8], Meidan et al. presented an autoencoder-based anomaly detection system N-BaIoT to detect compromised IoT devices. N-BaIoT also tried to reduce the number of false alerts triggered by the pure anomaly detection system using a window-based scheme with a majority vote to reach a decision.3. Background on Autoencoder and SPRTIn this section we provide the necessary background on autoencoder and sequential probability ratio test (SPRT) for understanding the development of the proposed CUMAD framework. We refer interested readers to [6] and [7], respectively, for the detailed treatment on these two topics.Autoencoder is an unsupervised neutral network that aims to reconstruct the input at the output. Figure 1 illustrates a simple standard (undercomplete) autoencoder.\
\
An autoencoder can be considered as consisting of two components: an encoder f and an decoder g. Given input data x, the encoder function f maps x to a latent-space representation, or code h, that is h = f(x). Using the corresponding code h as the input, the decoder function g tries to reconstruct the original input x at its output x ′, that is, x′ = g(h). Combining both the encoder function and decoder function together, we have x′ = g(f(x)). Let L(x, x′) be the reconstruction error, that is, the difference between x and x′. The autoenceder aims to minimize L(x, x ′). We note that there are different definitions of L(x, x′) and one of the most common definitions is the mean squared errors (MSE). We note that in the example autoencoder of Figure 1, both the encoder and decoder have only one hidden layer. This is only for illustration purpose. In reality they can have many hidden layers, depending on the specific application requirement.\
Autoencoders have been traditionally used in applications of dimensionality reduction and feature learning, by focusing on the compressed code of an autoencoder, which holds the latent-space representation of the original data. On the other hand, autoencoders also possess a few desired properties, making them an attractive candidate for anomaly detection. For example, an autoencoder is able to extract the salient features of the original data to remove dependency in the original data. More importantly, an autoencoder can only learn the properties or distributions of the data that it has seen during the training stage, that is, the data points in the training dataset. It excels at reconstructing data that are similar to the training data, but performs poorly on data that are very different from the training data, in terms of the reconstruction error L(x, x′).\
This is an appealing property of autoencoders in the application of anomaly detection. For example, in the context of detecting compromised IoT devices, we can establish the normal behavioral model of an IoT device using an autoencoder by training it with benign network traffic before the device has been compromised. We can continue monitoring the IoT device by passing the corresponding network traffic of the device into the trained model. If the reconstruction error is no greater than a pre-specified threshold, we consider the corresponding network traffic to be benign. When the reconstruction error is greater than the threshold, we claim that the network traffic is anomalous.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>How to Build a Modular Selenium + Cucumber Framework in Java</title><link>https://hackernoon.com/how-to-build-a-modular-selenium-cucumber-framework-in-java?source=rss</link><author>Rama Mallika Kadali</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:21:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Learn how to build a scalable and modular test automation framework using Selenium, Cucumber, and Java. This guide covers folder structure, CI/CD integration, Page Object Model, Allure reporting, and best practices for long-term maintainability and team collaboration.]]></content:encoded></item><item><title>How CUMAD Accumulates Evidence to Unmask Compromised IoT Devices</title><link>https://hackernoon.com/how-cumad-accumulates-evidence-to-unmask-compromised-iot-devices?source=rss</link><author>Hypothesis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:15:54 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).\
—IoT devices fundamentally lack built-in security mechanisms to protect themselves from security attacks. Existing works on improving IoT security mostly focus on detecting anomalous behaviors of IoT devices. However, these existing anomaly detection schemes may trigger an overwhelmingly large number of false alerts, rendering them unusable in detecting compromised IoT devices. In this paper we develop an effective and efficient framework, named CUMAD, to detect compromised IoT devices. Instead of directly relying on individual anomalous events, CUMAD aims to accumulate sufficient evidence in detecting compromised IoT devices, by integrating an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem. CUMAD can effectively reduce the number of false alerts in detecting compromised IoT devices, and moreover, it can detect compromised IoT devices quickly. Our evaluation studies based on the public-domain N-BaIoT dataset show that CUMAD can on average reduce the false positive rate from about 3.57% using only the autoencoder-based anomaly detection scheme to about 0.5%; in addition, CUMAD can detect compromised IoT devices quickly, with less than 5 observations on average.In recent years Internet of Things (IoT) devices have been increasingly integrated into our daily lives and our society, with notable example environments such as smart homes, healthcare, transportation, and power grid. On one hand, this rapid development helps to improve the quality and efficiency of our daily lives. On the other hand, this same development also poses potentially unprecedented security and privacy challenges on the Internet, given that most of these IoT devices are low-cost systems with limited computation, memory, and energy resources. These devices often lack proper built-in security mechanisms to protect themselves and are vulnerable to various security attacks.\
Many security attacks targeting or based on IoT devices have been reported in the past [1]. In response to the growing problems of IoT security, government agencies such as US NIST have developed many recommendations that manufacturers should adopt to mitigate the security risks associated with IoT devices [2]. In addition, many research efforts have been carried out to improve IoT security, including both proactive approaches to enhancing security mechanisms of IoT devices and more reactive solutions to monitor IoT device behaviors to detect rogue or infected IoT devices [3].\
Although some of the recommendations, for example, avoiding default common credentials, are relatively easy to be incorporated into IoT device manufacturing and certainly help mitigate IoT security risks, IoT devices are still fundamentally vulnerable to security attacks. As low-cost systems, IoT devices are inherently constrained in resources to support advanced security mechanisms. In addition, from the perspectives of both manufacturers and users, there are often conflicting objectives of IoT device usability and security, which often discourage the adoption of advanced security mechanisms in IoT devices.\
Given these constraints of deploying advanced security mechanisms on IoT devices, network-based solutions have attracted a great amount of research efforts in recent years [3]. In particular, many machine learning (ML) based methods have been developed in detecting anomalous network behaviors of IoT devices [3]. (In this paper we use the term ML to refer to both traditional machine learning algorithms such as SVM and deep learning (DL) algorithms such as RNN.) However, most existing solutions only targeted the problem of anomaly detection in IoT devices [4], instead of detecting compromised IoT devices. Although detecting individual anomalies is of critical importance in certain application domains [5], we note that these solutions may not be directly translated into the detection of compromised IoT devices. Given the large amount of network traffic, even a small false positive rate of an anomaly detection method can often translate into a large number of false alerts, rendering the detection method unusable in detecting compromised IoT devices in the real-world deployment.\
In this paper we develop an effective and efficient framework to detect compromised IoT devices, named CUMAD (cumulative anomaly detection). In essence, CUMAD integrates an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem [6], [7]. In CUMAD, the normal behavior of each IoT device is learnt and modeled by an autoencoder. During the training of an autoencoder model, it learns a latent space representation of the training data. More importantly, due to the nature of autoencoder, it excels at reconstructing inputs that are similar to the data used in training the model, but performs poorly when the new data is very different from the training data, manifested as large reconstruction errors. Although autoencoder has been mainly used in dimensionality reduction and feature learning in the past, in recent years it has also attracted a great amount of interests in anomaly detection in many different application domains.\
Instead of focusing on individual anomalous events detected by autoencoder, CUMAD aims to accumulate sufficient evidence to detect if an IoT device has been compromised. In CUMAD, the output of the autoencoder-based anomaly detection subsystem is fed into an SPRT-based sequential hypothesis testing subsystem. Unlike traditional probability ratio test methods that require a pre-defined fixed number of observations to reach a decision, SPRT works in an online manner and updates as observations arrive sequentially. SPRT reaches a conclusion whenever sufficient evidence has been observed. Therefore, SPRT can make a decision quickly (and consequently, CUMAD can detect compromised IoT devices quickly).\
In this paper we develop the CUMAD framework, and we also evaluate the performance of CUMAD using a public-domain IoT dataset N-BaIoT [8], which contains both benign and (Mirai and Bashlite) attack traffic of IoT devices. Our evaluation studies show that CUMAD can greatly improve the performance in detecting IoT devices in terms of false positive rates, for example, compared to the simple autoencoder-based anomaly detection system, CUMAD on average reduces the false positive rate from about 3.57% to 0.5%, representing about 7 times performance improvement in terms of false positive rate of the systems. In addition, CUMAD can detect a compromised IoT device quickly, with less than 5 sequential observations on average. We note that although both autoencoder and SPRT have been proposed in developing anomaly detection systems before, to our knowledge, we are the first to integrate the two techniques to detect compromised IoT devices, instead of being used separately for anomaly detection. In addition, we are the first to introduce the notion of cumulative anomaly in detecting compromised IoT devices (see Section 2 for more details).\
The remainder of the paper is organized as follows. In Section 2 we discuss related work. We present the background on autoencoder and SPRT in Section 3. We describe the design of CUMAD in Section 4, and evaluate its performance in Section 5. We conclude the paper in Section 6.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Future Wireless Comms Could Process Data in Midair</title><link>https://spectrum.ieee.org/wireless-communication-over-air-processing</link><author>Michelle Hampson</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExNTkwNS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2MzYyMzg1M30.XXiRdtScBG-Ox39QOpbhpoAjmKTB9e0NHXiQJBHOaQc/image.jpg?width=600" length="" type=""/><pubDate>Tue, 1 Jul 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[AirComp approaches do more than transmit data from point A to point B]]></content:encoded></item><item><title>Automakers Clash With India Over &apos;Aggressive&apos; Emission Limits</title><link>https://hardware.slashdot.org/story/25/07/01/081225/automakers-clash-with-india-over-aggressive-emission-limits?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[India's automakers are opposing the government's proposal to cut car emissions by 33% from 2027, calling the target "too aggressive" in a formal submission to the power ministry. 

The Society of Indian Automobile Manufacturers warned the plan risks billions of rupees in penalties and threatens future investments in the $137-billion auto sector. The proposal represents more than twice the pace of India's previous emission reduction target and forms part of the third phase of Corporate Average Fuel Efficiency norms first introduced in 2017. The industry body wants a more gradual 15% reduction target and opposes different standards for small versus heavy vehicles.]]></content:encoded></item><item><title>Mo Jomaa breaks down IPO prep for founders on the Scale Stage at TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/mo-jomaa-breaks-down-ipo-prep-for-founders-on-the-scale-stage-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Mo Jomaa of CapitalG, will lead a session, “What to Think About Now If You Want to IPO Someday” at TechCrunch All Stage on July 15 in Boston. Register now.]]></content:encoded></item><item><title>Performance &amp; Power Of The Low-Cost EPYC 4005 &quot;Grado&quot; vs. Original EPYC 7601 Zen 1 Flagship CPU</title><link>https://www.phoronix.com/review/amd-epyc-4005-server</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those on very long server upgrade cycles, typically just running the hardware until failure or consider buying second-hand servers that are generations old for lower up-front cost, today's unique article is for you with quantifying a first-generation EPYC server compared to today's entry-level EPYC processors in performance and power efficiency. With the fascinating AMD EPYC 4005 "Grado" budget-friendly server processors I was curious how well they would stack up against AMD's original flagship EPYC processor, the AMD EPYC 7601 "Naples" processor from the Zen 1 era. Can an entry-level brand new Grado server processor with dual channel DDR5 memory outpace an original EPYC server with twice the core/thread counts and eight channel DDR4 server memory? Yes, with huge gains in performance and power efficiency.]]></content:encoded></item><item><title>Building the Unbreakable Contract: A Pipeline for AI-Powered Vulnerability Classification and Repair</title><link>https://hackernoon.com/building-the-unbreakable-contract-a-pipeline-for-ai-powered-vulnerability-classification-and-repair?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:58:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
To achieve high-quality results in training our framework utilizing a RandomForestClassifier and LLMs for classification and repair (Fig. 1), several essential features must be incorporated.\
A source code column (“contract source”) is necessary to run Slither and the LLMs. However, since the datasets consistently excluded source code, a web scraping algorithm that employed the “contract address” column would be necessary to obtain source code from Etherscan and generation (see subsection D.). In order to account for source code that could not be scraped through Etherscan, the dataset (200,000 contracts) was reduced to 2500 rows.\
Slither was then run on the newly acquired source code (see subsection B.), adding columns “vulnerability”, “confidence”, and “impact”. Slither occasionally failed to provide any vulnerabilities, totalling 474 failed contracts (80% successful output rate). To account for this, the dataset was reduced again to 2,000 smart contracts. Of the dataset, 400 were labeled malicious, and 1,600 were labeled non-malicious. Table I visualizes a segment of the finalized dataset.\
Slither is a static code analyzer, which checks the smart contracts for vulnerabilities without executing the contract. Slither’s initial input comes from the Solidity Abstract Syntax Tree (AST) generated by the Solidity compiler from the contract source code. The smart contract is then simplified into an intermediate representation called SlithIR. This intermediate representation is compared to current industry standards, and Slither outputs vulnerabilities. Slither leads the industry in smart contract vulnerability detection, outperforming other static code analyzers in almost every metric, as shown in Table II. This, coupled with our Random Forest Classifier, ensures high accuracy in detecting vulnerable smart contracts.\
After importing and running all 89 basic detectors provided by the API, we added each contract’s vulnerabilities to the dataset as a list of Slither’s natural language names with empty lists denoting contracts Slither deemed safe.\
C. Data Issues and Generation\
When it came to data collection, specific issues were encountered. Our biggest issue, extracting source code, proved to be a challenging task. For instance, in a dataset that bytecode was given, we were unsuccessful in decompiling that code into analyzable source code as we were unaware of the decompiler’s limits. We also struggled to find additional malicious source code to train a model on, as our dataset only included 150 malicious contracts. To overcome this, we implemented OpenAI’s GPT 3.5 Turbo to generate malicious source code. Initial attempts were barred by GPT 3.5’s ethical limitations (Fig. 2). However, after jailbreaking GPT 3.5 with prompt engineering [18], GPT 3.5 would produce malicious source code that could be repaired by the model.\
The variability of the dataset made it difficult to generate Slither vulnerabilities for smart contracts, so a BLANK-step approach was used. The primary issue was the 100+ versions all contracts were written in combined with the limited backward compatibility of Solidity — i.e., version 0.4.11 could run on a compiler of version 0.4.26 but not a compiler of version 0.5.0+. Addressing this required modifying each contract to read ”pragma solidity ≥{version}”, creating five different scripts, and running each script on the entire dataset with one of five following Solidity versions: 0.4.26, 0.5.17,\
0.6.12, 0.7.6, or 0.8.21, with Slither vulnerabilities of scripts that could not be compiled recorded as null, and those that could be recorded with the English name of the vulnerability, obtained from parsing the returned json. Combining these lists resulted in the final list of Slither vulnerabilities for the 75% of smart contracts for which this method yielded results.\
Each detector class includes the detector’s confidence and impact levels. After creating a key-value pair of each detector’s English name and their confidence plus impact, this list was used to create confidence and impact lists for all vulnerabilities for each smart contract.\
Various models were implemented to classify smart contract maliciousness. Ultimately, RandomForestClassifier (RFC) provided the highest accuracy after pre-processing the finalized dataset.\
RFC is unable to train on the dataset as provided by webscraping, generation, and Slither processing due to the abundance of unnecessary string-based features. So, unnecessary features are dropped, and necessary features are processed for RFC. For example, “confidence” and “vulnerability” retain a weaker correlation to “malicious” in comparison to “impact”, so to avoid convoluting the model, both are dropped. Thus, “contract source” and “impact” remain as the classifying features and “malicious” as the target label.\
As all columns are still either string or boolean data types, RFC is still unable to train on the dataset. “contract source” was tokenized using the CountVectorizer (CV) tool from the sci-kit-learn library. “malicious” and “impact” were encoded into usable numeric values by mapping dictionaries. Since “impact” contained more than two possible outputs, unlike “malicious”, the outputs of “impact” were scaled from 0-4.\
After the tokenized and encoded columns are concatenated, RFC’s numeric prerequisite is fulfilled.\
The data is then split into a train-test split of 0.6-0.4 and randomized before RFC fits to the train set and predicts on the test set. Accuracy and confusion are evaluated in .\
E. Large Language Models (LLMs)\
: We incorporated multiple Large Language Models to repair the smart contracts after they had been identified as malicious with our two-layered frameworks. The best results came from the Llama-2-7B model, which can be found on Hugging Face. This model finished training in July 2023. Our finetuning process took place about three weeks later. The Llama-2-7B model has become very popular due to its low number of parameters and reliability, leading to a less memory-intensive alternative to other LLMs in the industry.\
The finetuning process took place on Google Colab using the T4 chip, which carries 16 GB of VRAM. However, Llama2-7B’s weights themselves fill this limit (7b * 2 bytes = 14). This also does not include any weights, optimizers, or gradients. Thus to run Llama-2-7B and be able to run it without memory restrictions on a platform like Google Colab, we will use parameter-efficient-finetuning (PEFT). Specifically, we will use QLoRa (Efficient Finetuning of Quantized LLMs), using 4-bit precision instead of the normal 16-bit precision. This quantization process allows for finetuning on Colab while also ensuring that the precision of the model is adequate. This is because when saving the 4-bit model, we also save the QLoRa adapters, which can be used with the model.\
Moreover, Llama-2-7B is open source meaning the model is available to be downloaded and used locally. Traditional data privacy concerns with LLMs are therefore nullified because all data is processed on the local machine, not in a 3rd party server. This bodes well for smart contracts as many execute agreements with sensitive information and large sums of money. Llama-2-7B provides the benefits and accuracy of an advanced LLM while also providing the security and versatility neccesary for blockchain technology.\
The Llama-2-7B model was fine-tuned on fifty smart contracts that were once malicious and then repaired, using a supervised learning approach. These smart contracts were collected in the data collection mentioned above. Specifically, the source code was tokenized and embedded, using the quantization outlined previously. The model was trained over 100 steps, with training loss consistently decreasing with every step(as shown in figure 3).\
The supervised fine-tuning process allowed the model to understand the relationships between malicious source code and the same source code that had been repaired to emulate that with any other contract.\
: We also utilized OpenAI’s API to use GPT-3.5-Turbo to repair vulnerabilities. OpenAI is one of the most well known names in the industry with applications such as DALL -E and ChatGPT. Specifically, while all GPT models are optimized to generate code, GPT-3.5-Turbo is the best combination of performance and efficiency. Moreover, by utilizing a ”chat bot”, we were able to use prompt engineering to create a prompt with the best possible performance. Directly querying GPT-3.5-Turbo to repair malicious code was unsuccessful. Similar to the generation of malicious smart contracts, GPT-3.5-Turbo had a reluctance to work with malicious source code (Fig. 4).\
Thus prompt engineering was utilized to circumvent this problem.\
First, the use of the word ”malicious” needed to be removed. While we were looking for our LLM to repair malicious smart contracts, GPT-3.5 Turbo was instead asked to help us “fix vulnerable smart contracts”.\
We then used Chain of Thought Techniques in order for the model to elaborate on what changes it made and why. This led to a more accurate source code output and more vulnerabilities repaired. Additionally, this provided more information for the\
user as the specific vulnerabilities in the malicious smart contract were highlighted and explained.\
Ultimately, our prompt(Fig. 5) used Slither’s source code and vulnerabilities to prompt GPT 3.5 Turbo to repair the smart contracts. While Slither also outputs impact level and confidence on those vulnerabilities, we found incorporating these into the prompt hurt the model’s ability to output repaired source code or even source code that could be compiled. Essentially, using other Slither outputs led to overfitting. This prompt was also used with the Llama-2-7B model outlined above in order to create uniformity across outputs. In both models, the prompt allowed for the generation of repaired source code while also generating details that explained any changes and provided explanation.\
In conclusion, we ended with two primary models to repair source code. First, the Llama-2-7B, which had been finetuned specifically for repairing smart contracts. Second was the utilization of GPT-3.5-Turbo which learned to repair smart contracts through CoT prompt engineering.(1) Abhinav Jain, Westborough High School, Westborough, MA and contributed equally to this work (jain3abhinav@gmail.com);(2) Ehan Masud, Sunset High School, Portland, OR and contributed equally to this work (ehanmasud2006@gmail.com);(3) Michelle Han, Granite Bay High School, Granite Bay, CA (michellehan2007agt@gmail.com);(4) Rohan Dhillon, Lakeside School, Seattle, WA (rohand25@lakesideschool.org);(5) Sumukh Rao, Bellarmine College Preparatory, San Jose, CA (sumukhsf@gmail.com);(6) Arya Joshi, Robbinsville High School, Robbinsville, NJ (arya.joshi@gmail.com);(7) Salar Cheema, University of Illinois, Champaign, IL (salarwc2@illinois.edu);(8) Saurav Kumar, University of Illinois, Champaign, IL (sauravk4@illinois.edu).:::info
This paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.]]></content:encoded></item><item><title>Will VR Integration for Remote Work Become Permanent?</title><link>https://hackernoon.com/will-vr-integration-for-remote-work-become-permanent?source=rss</link><author>Allan Grain</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:51:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If there’s been any seismic shift in the workplace over the last few decades, it’s manifested in the way remote work has become a cornerstone of modern business operations.Many organizations today have teams distributed over several geographical areas and bridging the gap is a challenge. Phone calls, emails and even Zoom calls simply are not sufficient to replicate the in-office experience.The question now is whether technology will advance to the point that virtual reality (VR) solutions can revolutionize remote collaboration and allow companies to operate multiple locations in a VR environment.The best part of VR is that it provides a three-dimensional immersive space where employees can interact as if they were physically present. The addition of AI would probably assist in making these virtual environments even more realistic. This type of innovation would certainly address the limitations of traditional video conferencing as seen with Zoom or Microsoft Teams for instance. These technologies might be good for certain uses, but they lack the depth and engagement of face-to-face interaction.One of the fun aspects of VR is the ability to create virtual offices – digital workspaces in which employees, represented by lifelike avatars, can collaborate in real time. AI algorithms power these avatars, enabling them to mimic natural human behaviors, such as gestures, facial expressions, and even tone of voice.A terrific emerging technology that has existed for a while now but is reaching perfection today is real-time language translation. Language barriers have long been a challenge for global teams. AI-driven translation tools, such as those powered by natural language processing (NLP), are now being integrated into VR platforms. These tools are now able to translate speech in real time, displaying subtitles or dubbing voices in a user's native language. For instance, a team member in Japan can speak in Japanese, while a colleague in Brazil hears the conversation in Portuguese, all within a shared VR environment. This capability allows employees from various geographical regions to collaborate on projects even if they do not speak the same language. This is groundbreaking.The virtual environment can also be altered to accommodate one-on-one meetings or large boardroom meetings. Users can even alter the room to accommodate client presentations.AI is a VR environment is useful for scheduling, note taking, or action-item tracking. Transcription tools are easily integrated into a VR environment so that conversations and discussion can be transcribed, with key points summarized and follow-up tasks assigned in real-time. AI and VR working seamlessly together can function as a high-speed virtual assistant capable of performing multiple tasks at once with high accuracy and minimal input.Several companies are already pioneering AI-VR integration for remote work. Meta's Horizon Workrooms, for example, allows teams to collaborate in virtual meeting rooms with spatial audio and interactive whiteboards.Similarly, Spatial, a VR collaboration platform, uses AI to create cross-platform compatibility, allowing users on different devices—VR headsets, laptops, or smartphones—to work together seamlessly.Firms like Accenture have adopted AI-VR solutions to host global meetings and training sessions. Accenture's "Nth Floor" virtual campus enables employees to attend workshops, network with colleagues, and even explore virtual replicas of physical offices.Beyond corporate settings, AI combined with VR is making waves in education and healthcare. Universities now use virtual classrooms to connect students across continents. In healthcare, VR simulations allow medical professionals to practice surgeries or collaborate on patient care plans remotely, improving access to expertise.Unfortunately, despite its potential, AI-VR integration for remote work still faces several major hurdles before it can be widely adopted. Cost and accessibility remain significant barriers, as high-quality VR headsets and AI-powered software are highly expensive. VR devices like the Oculus Quest 3 or Apple Vision Pro might become more affordable, but their price prevents widespread adoption.While challenges remain, ongoing innovations will pave the way for a future where virtual offices imitate their physical counterparts sufficiently enough that companies will integrate them permanently as part of the workday.Tomorrow’s workplace promises to be more immersive and perhaps even more fun. ]]></content:encoded></item><item><title>A Novel Pipeline for Classifying and Repairing Smart Contracts at Scale</title><link>https://hackernoon.com/a-novel-pipeline-for-classifying-and-repairing-smart-contracts-at-scale?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:47:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Abhinav Jain, Westborough High School, Westborough, MA and contributed equally to this work (jain3abhinav@gmail.com);(2) Ehan Masud, Sunset High School, Portland, OR and contributed equally to this work (ehanmasud2006@gmail.com);(3) Michelle Han, Granite Bay High School, Granite Bay, CA (michellehan2007agt@gmail.com);(4) Rohan Dhillon, Lakeside School, Seattle, WA (rohand25@lakesideschool.org);(5) Sumukh Rao, Bellarmine College Preparatory, San Jose, CA (sumukhsf@gmail.com);(6) Arya Joshi, Robbinsville High School, Robbinsville, NJ (arya.joshi@gmail.com);(7) Salar Cheema, University of Illinois, Champaign, IL (salarwc2@illinois.edu);(8) Saurav Kumar, University of Illinois, Champaign, IL (sauravk4@illinois.edu).\
—Due to the modern relevance of blockchain technology, smart contracts present both substantial risks and benefits. Vulnerabilities within them can trigger a cascade of consequences, resulting in significant losses. Many current papers primarily focus on classifying smart contracts for malicious intent, often relying on limited contract characteristics, such as bytecode or opcode. This paper proposes a novel, two-layered framework: 1) classifying and 2) directly repairing malicious contracts. Slither’s vulnerability report is combined with source code and passed through a pre-trained RandomForestClassifier (RFC) and Large Language Models (LLMs), classifying and repairing each suggested vulnerability. Experiments demonstrate the effectiveness of fine-tuned and prompt-engineered LLMs. The smart contract repair models, built from pre-trained GPT-3.5-Turbo and finetuned Llama-2-7B models, reduced the overall vulnerability count by 97.5% and 96.7% respectively. A manual inspection of repaired contracts shows that all retain functionality, indicating that the proposed method is appropriate for automatic batch classification and repair of vulnerabilities in smart contracts.As we delve into the crucial role smart contracts play in the global blockchain, it becomes increasingly imperative that we understand the severity of cyberattacks that exploit weak code. 2018 saw $23.5 million worth of cryptocurrencies stolen from the Bancor network due to the compromise of a wallet used to upgrade smart contracts, sparking controversy online over the safety of decentralized exchange and smart contract systems [16]. More recently, in 2020, a hacker drained Harvest Finance of $24 million by implementing a smart contract that manipulated the share values of the vaults [17]. The common theme across these hacks is that vulnerabilities within smart contracts were exploited to steal millions of dollars, highlighting the importance of strengthening smart contracts to prevent vulnerabilities from arising.\
Smart contracts provide a secure platform for transactions without the need for a trusted intermediary. For this reason, they have become increasingly common in blockchain applications. But because most blockchain applications prevent users from editing smart contracts after they have been deployed, there is a need for analysis tools that can accurately and precisely determine the vulnerabilities of smart contracts. Although most tools rely on expert-developed frameworks, recent research has begun developing deep learning models that can evaluate a smart contract’s vulnerability. However, most existing deep learning models fail to provide helpful feedback on a smart contract’s vulnerabilities — instead, they determine whether or not a smart contract is vulnerable.\
DLVA [1] introduces a three-step approach involving mapping bytecode to high-dimensional vectors, classifying vectors based on training data, and using neural networks to infer vulnerable contracts. However, a significant weakness in this approach was the high false positive rate during the prediction process. Similarly, MRN-GCN [5] utilizes deep learning with a nest contract graph capturing syntactic and semantic information, enabling the classification of vulnerable functions, but like [1], retained mixed recall percentages ranging from 98.18% to 79.59%. The authors of [3] take a different approach by proposing peer-to-peer voting and reward-and-slash mechanisms to mitigate and discourage malicious behavior in smart contracts.\
Large Language Models (LLMs) models prove to be exceptional in performing complex tasks. The authors of [8] demonstrated the capabilities of various LLMs in identifying vulnerabilities in DeFi smart contracts with F1-scores significantly higher than random baselines, which has the potential arXiv:2309.07841v1 [cs.CR] 14 Sep 2023 to be improved by the tool enhancement framework developed in [4]. Prompt engineering allows LLMs to be substantially enhanced. One powerful LLM prompt engineering method involves Chain of Thought (CoT) prompting [2] that significantly improves the ability of LLMs to perform complex reasoning. In eight CoT exemplars, [2] achieves an accuracy of 56.9 on PaLM-540B in the GSM8K benchmark, demonstrating an accuracy improvement of 39. However, the paper chooses to rely solely on CoT, neglecting fine-tuning entirely. In a similar implementation, the authors of [7] present a framework that improves upon CoT by transferring advanced reasoning abilities from large models to smaller ones through knowledge distillation, resulting in improved question-answering performance. In another scenario, [6] utilized prompt engineering by giving ChatGPT specific information, such as the translation’s purpose and target audience, leading to industry standard translation quality.\
A comprehensive survey [11] described the current landscape of smart contract security, identifying eight core defense methods across 133 models. This finding underscores the complexity of the field but also reveals limitations. One limitation is seen in applying automated smart contract tools to DeFi systems [12]. Surprisingly, these tools only detected 8% of attacks, indicating a challenge with intricate vulnerabilities. Addressing this, [13] evaluated five smart contract detection tools, focusing on three types of vulnerabilities. [13]’s analysis determined that different detection models have varying strengths and weaknesses, suggesting a combination of methods may be more effective. Furthermore, this notion is corroborated by [9] and [10], which both utilize Multi-Task Learning, a combination method that leverages concurrent learning and optimization of multiple tasks. Notably, [14] advances this methodology by using an approach that blends K-means clustering and LSTM networks with a universal sentence encoder. This approach understood the smart contract code’s semantic meaning, outperforming baseline models.\
Moreover, current work regarding repairing smart contracts has been shown to be reliable. For example, [19] utilizes a framework called ContractFix to repair vulnerabilites with 94% accuracy. ContractFix was based around static code analyzers and focused on repairing broken patches. Similarly, [15] utilizes a tool, Elysium, to repair patches in bytecode for seven vulnerabilities. However, this paper improves on these frameworks in two main ways. First, our framework is built on LLMs which allow for a more robust repairing process, that is adaptable to zero-day vulnerabilities. Secondly, we work directly with source code, which is a novel approach to repair vulnerabilities.\
These existing methods have been shown to work well in vulnerability detection across various situations with relatively little statistical error. However, we show that existing vulnerability detection methods face the following problems: 1) lack of a broad approach, 2) little detail on specific errors, 3) high false positive evaluations, and 4) lack of a direct repair framework. To address all these problems, we propose a novel pipeline. The pipeline first utilizes Slither and a RandomForestClassifier to detect and provide specific vulnerabilities within smart contract source code. After filtering out non-malicious contracts, two LLMs, GPT-3.5-Turbo and a fine-tuned Llama-2-7b generation model, each repair the vulnerable smart contract source code. The repaired contract is then evaluated by Slither against its vulnerable counterpart, assessing the effectiveness of the repair.\
The rest of this paper is outlined as follows: Section II details our novel pipeline approach that utilizes two layers for vulnerability detection: Slither and RandomForestClassifier, to classify vulnerable smart contracts and two LLM models (Llama-2-7B and GPT-3.5-Turbo) to repair them. Section III exhibits the results of our approach in comparison to existing methods. Section IV provides a conclusion.:::info
This paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.]]></content:encoded></item><item><title>Why You Can&apos;t Miss SpoonOS&apos;s Developer Call S1 for AI Innovation</title><link>https://hackernoon.com/why-you-cant-miss-spoonoss-developer-call-s1-for-ai-innovation?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:41:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What if artificial intelligence could operate with the transparency and decentralized control of blockchain technology? SpoonOS, described as an infrastructure developer for the sentient economy, is initiating its first coordinated community effort, Developer Call S1, to address this question. The goal is to encourage developers to explore the core functionalities of SpoonOS, an agentic operating system designed for Web3. This initiative aims to validate the SpoonOS infrastructure and identify individuals who will contribute to the operating system's evolution. It represents a structured approach with defined objectives, a timeline, and incentives for participation, along with comprehensive documentation.\
For those unfamiliar with the terminology, Web3 refers to the next iteration of the internet, emphasizing decentralization, blockchain technologies, and token-based economics. Unlike the current internet, where large corporations often control data and platforms, Web3 aims to give users more control over their data and digital interactions. Within this context, AI agents are autonomous entities that perceive their environment, act to achieve specific goals, and can improve their performance through learning. SpoonOS positions itself at the intersection of these two transformative fields, providing a framework for developers to create these intelligent, on-chain agents.Developer Call S1 is specifically designed for Web3 developers engaged in building with AI agents. The program provides participants with resources to understand the SpoonOS technology stack. This includes examining the SpoonOS codebase on GitHub, which offers clear instructions for environment setup, building a first AI agent, mastering command line interface (CLI) tools, and integrating with built-in Model Context Protocol (MCP) servers. The MCP serves as an interface connecting SpoonOS to various data sources, both decentralized and traditional.\
Beyond the GitHub resources, developers gain access to a "Spoon Cookbook," which contains end-to-end tutorials. These tutorials cover topics such as API key configuration, setting up agent memory, and constructing reproducible AI agents. Additionally, the "Spoon Toolkit" details third-party integrations, including on-chain data feeds and decentralized storage. On-chain data feeds provide real-time information directly from a blockchain, enabling AI agents to react to events as they happen on the network. Decentralized storage, in contrast to centralized cloud storage, distributes data across a network of computers, enhancing security and resilience by removing a single point of failure. SpoonOS operates on NEO's Layer-1 blockchain, a foundational blockchain that provides the underlying infrastructure for these scalable Web3 applications. This integration is presented as a means for developers to build, deploy, and manage AI agents using intuitive tools and interfaces.Cultivating an Agent Ecosystem: Beyond DevCall S1\
Developer Call S1 is a foundational element of SpoonOS's broader strategy to expand its AI agent ecosystem. Upcoming initiatives include a global hackathon and collaborative learning programs. The intent behind these efforts is to empower developers to create versatile AI agents capable of leveraging the full capabilities of the SpoonOS platform. The vision is to lay the groundwork for the next generation of AI agents, which are expected to play a significant role in the emerging "sentient economy."\
The "sentient economy" refers to a conceptual ecosystem where intelligent agents can autonomously interact, learn, and contribute to value creation within Web3 networks. This implies a future where AI agents are not merely tools, but active participants in economic processes, making decisions and executing transactions independently based on real-time data and learned behaviors. SpoonOS aims to provide the robust and integrated environment necessary for such an economy to flourish.\
The launch of SpoonOS's Developer Call S1 marks a significant step towards a more integrated future for AI and blockchain. The emphasis on providing comprehensive tools and structured learning pathways suggests a genuine commitment to developer adoption, which is crucial for any new platform's success. The concept of a "sentient economy" fueled by autonomous AI agents operating on a decentralized infrastructure is compelling. It suggests a shift from passive data consumption to active, intelligent participation within digital economies.\
However, the realization of such a vision depends heavily on the practical utility and scalability of these AI agents. The complexity of integrating AI models with the immutable and often resource-intensive nature of blockchain transactions presents challenges. The success of SpoonOS will hinge on how effectively its toolkit simplifies this complexity for developers, allowing them to build agents that are not only intelligent but also efficient and secure. The ability for these agents to interact seamlessly across various data sources, both on-chain and off-chain, will be a key differentiator. It remains to be seen how broadly the developer community will embrace this new paradigm, but the initial push with incentivized calls and comprehensive resources provides a strong starting point. The development of a truly "sentient economy" will require continued innovation and collaboration from a diverse set of builders, and SpoonOS's approach seems designed to foster that environment.\
Don’t forget to like and share the story!:::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Why Your Next Smart-Home Device Hunts Mosquitoes: Meet Bzigo Iris</title><link>https://hackernoon.com/why-your-next-smart-home-device-hunts-mosquitoes-meet-bzigo-iris?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:28:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Bzigo Iris is the world’s first AI-powered mosquito detector that uses infrared and laser tech to locate mosquitoes indoors without harmful chemicals. Founded in 2019, Bzigo offers a smart, health-conscious alternative to sprays and zappers—sending real-time alerts so you can finally sleep bite-free. Visit bzigo.com to learn more.

]]></content:encoded></item><item><title>Little Pepe (LILPEPE) Breezes Past 3rd Presale Stage in Record Time</title><link>https://hackernoon.com/little-pepe-lilpepe-breezes-past-3rd-presale-stage-in-record-time?source=rss</link><author>Kashvi Pandey</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:24:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Little Pepe (LILPEPE) just completed its third presale stage faster than expected, raising $2.57M in Stage 4. Backed by its own Layer 2 blockchain and a $777K giveaway, the token offers real utility beyond meme hype. With rising prices and strong demand, LILPEPE is quickly becoming 2025’s breakout meme coin.

]]></content:encoded></item><item><title>Stablecoins, Smart Contracts and The Rise of More Intelligent Cash</title><link>https://hackernoon.com/stablecoins-smart-contracts-and-the-rise-of-more-intelligent-cash?source=rss</link><author>Paul Quickenden</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:20:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Fintechs already have the talent, the ingenuity and after a decade of challenger success with innovations like Wise’s borderless accounts, Stripe’s one-click checkout and Revolut’s multi-currency wallets - the credibility to reshape finance on a global stage. Those breakthroughs gave consumers faster payments and slicker front-ends; but let’s call that Act I.\
Act II is unfolding now: money itself is becoming programmable, composable and borderless. Stablecoins can settle in seconds, smart contracts can execute “if-this-then-that” logic without humans in the loop and tokenised assets can move 24/7 across jurisdictions. The rails we lay today - whether they’re open, interoperable and secure or fragmented and proprietary - will determine not just how fast value moves tomorrow, but how fairly it’s distributed and who gets to participate in the next wave of financial innovation.\
The challenge to builders today is this: if your vision begins and ends with speeding up domestic payments or making bill-splitting cuter, you’re missing the bigger prize. Programmable money rewrites the playbook for remittances, trade, treasury and even machine-to-machine transactions. Are we building for that horizon - or solving tomorrow’s problems with yesterday’s stack?What’s the biggie with blockchain?Behind the scenes, blockchain infrastructure is becoming the default layer for secure, real-time, permissionless value transfer. J.P. Morgan’s Onyx platform, for instance, has settled more than US$1 trillion in tokenised intrabank payments on permissioned chains.\
If you’re designing for the next five years, on-chain rails offer instant settlement, embedded logic and global composability that legacy systems simply can’t match. No, it won’t replace every system but it is rapidly becoming the preferred foundation for anything that needs to be fast, transparent and interoperable. This isn’t about riding the crypto wave but about recognising a smarter way to move value - one that’s auditable, modular and increasingly composable.\
If you’re designing for the next five years, building  blockchain gives you access to speed, programmability and network effects that simply don’t exist in traditional architecture. We need to shift our thinking from ‘What is blockchain for?’ to ‘What’s already working, and how does it fit into my stack?’Could stablecoins be the new fuel behind an AI-driven economy?At Stripe’s 2025 Sessions conference, co-founder Patrick Collison didn’t mince his words: “There are not one, but two, gale-force tailwinds… reshaping the economic landscape around us: AI and stablecoins.” In a world increasingly run by AI’s, programmable stablecoins are the natural currency and could become the financial fuel behind an AI-driven economy.\
Stripe has just rolled out stablecoin-powered financial accounts to businesses in 101 countries which allow companies to hold, receive, and send stablecoins and, crucially, to hedge against inflation and plug directly into the global economy. All of this without needing a traditional bank account. And it's not just Stripe, Worldpay, Mastercard, X and even Apple are exploring how to turbocharge their business using Stablecoins.\
This isn’t theory; this is infrastructure, and entrepreneurs in high-inflation countries can now operate in stable, digital USD. What’s more, startups can pay contractors in stablecoins, skip wire transfer fees and move capital in minutes via lower-cost cross-border transactions.\
Stablecoins have quietly moved from the fringes to becoming the go-to method for paying global contractors (instantly, with zero FX fees), cross-border commerce (settling USD payments in mere seconds), accessing savings (even in inflation-prone regions) and plugging into on-chain treasury and lending protocols (without needing a bank account).\
So if you’re building a payments platform, now is the time to ask yourself: can my product evolve with how digital finance is  being used? If the answer is no - it might be worth reconsidering your architecture because what works here must scale there.That sounds like “money, but faster.” What else is cool?Think of smart contracts as ‘if this, then that’ logic for money - basically code that executes financial actions automatically when certain conditions are met. There’s no middleman, no lag and no manual work. This is where money stops just moving and starts \
Imagine if paying contractors happened the moment work is verified; if refunds were triggered instantly when goods didn’t arrive; if cross-border trade ran on rules baked into your code, not your paperwork.\
This is already happening globally - programmable finance means stablecoins that plug into automated workflows and APIs that react to real-world events. The question now moves on from can we move money faster? .. to can our money move smarter?What separates good from game-changing companies is vision. The winners in fintech will be the ones who design for what’s , not just what’s  right now.If you believe that money will be programmable… \n If you see that global commerce demands truly global rails… \n If you know that the rules of infrastructure are being rewritten in real-time right now…… then now’s the moment to double down. Build with stablecoins. Plug into on-chain rails. Create APIs that are blockchain-ready. It’s time to think like the internet: borderless, open and lightning fast.Let’s not just build for today’s money; let’s help define tomorrow’s.]]></content:encoded></item><item><title>US Senate removes controversial ‘AI moratorium’ from budget bill</title><link>https://techcrunch.com/2025/07/01/us-senate-removes-controversial-ai-moratorium-from-budget-bill/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:07:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After going back and forth over the provision, Sen. Marsha Blackburn offered an amendment to strip the provision alongside Sen. Maria Cantwell. ]]></content:encoded></item><item><title>US Government Takes Down Major North Korean &apos;Remote IT Workers&apos; Operation</title><link>https://yro.slashdot.org/story/25/06/30/2236218/us-government-takes-down-major-north-korean-remote-it-workers-operation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: The U.S. Department of Justice announced on Monday that it had taken several enforcement actions against North Korea's money-making operations, which rely on undercover remote IT workers inside American tech companies to raise funds for the regime's nuclear weapons program, as well as to steal data and cryptocurrency. As part of the DOJ's multi-state effort, the government announced the arrest and indictment of U.S. national Zhenxing "Danny" Wang, who allegedly ran a years-long fraud scheme from New Jersey to sneak remote North Korean IT workers inside U.S. tech companies. According to the indictment, the scheme generated more than $5 million in revenue for the North Korean regime. [...]
 
From 2021 until 2024, the co-conspirators allegedly impersonated more than 80 U.S. individuals to get remote jobs at more than 100 American companies, causing $3 million in damages due to legal fees, data breach remediation efforts, and more. The group is said to have run laptop farms inside the United States, which the North Korean IT workers could essentially use as proxies to hide their provenance, according to the DOJ. At times, they used hardware devices known as keyboard-video-mouse (KVM) switches, which allow one person to control multiple computers from a single keyboard and mouse. The group allegedly also ran shell companies inside the U.S. to make it seem like the North Korean IT workers were affiliated with legitimate local companies, and to receive money that would then be transferred abroad, the DOJ said.
 
The fraudulent scheme allegedly also involved the North Korean workers stealing sensitive data, such as source code, from the companies they were working for, such as from an unnamed California-based defense contractor "that develops artificial intelligence-powered equipment and technologies."]]></content:encoded></item><item><title>Genesis AI launches with $105M seed funding from Eclipse, Khosla to build AI models for robots</title><link>https://techcrunch.com/2025/07/01/genesis-ai-launches-with-105m-seed-funding-from-eclipse-khosla-to-build-ai-models-for-robots/</link><author>Marina Temkin</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:56:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Genesis AI, which aims to build a foundational model for powering all kinds of robots, has emerged from stealth with $105M in seed funding.]]></content:encoded></item><item><title>Canonical Decides To Double Down On Their Investment In Java For Ubuntu</title><link>https://www.phoronix.com/news/Canonical-Double-Down-Java</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:52:17 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Ubuntu maker Canonical has decided to "double down" their investment in OpenJDK Java for Ubuntu Linux...]]></content:encoded></item><item><title>xAI raises $10B in debt and equity</title><link>https://techcrunch.com/2025/07/01/xai-raises-10b-in-debt-and-equity/</link><author>Ram Iyer</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:35:54 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Elon Musk's AI company, xAI, has raised $5 billion in debt and $5 billion in equity, Morgan Stanley said on Monday. ]]></content:encoded></item><item><title>Elon Musk’s Starlink Adds $750 Congestion Charge</title><link>https://www.techdirt.com/2025/07/01/elon-musks-starlink-adds-750-congestion-charge/</link><author>Karl Bode</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:27:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Low-Earth Orbit satellite broadband services like Starlink have their uses, but will always be dealing with capacity constraints. That means higher prices, weird restrictions, and, as of November 2024, a $100 “congestion charge” for a service that’s already too expensive for many of the rural Americans who could most benefit.It didn’t take long for that “congestion charge” to soar to $500 in some areas. Now it’s already risen as high as $750 in states like Washington as Starlink is forced to try and deter users in some markets from using the increasingly congested network:“The change can crank up the starting price simply to own the Starlink dish on a residential plan to $1,099.”Other parts of the country see no congestion charge, but there’s no guarantee that they won’t see one down the line as the network subscribership grows. It’s also very likely the company will increasingly have to resort to doing things like throttling higher definition videos, or engaging in other network management tricks to try and keep the service semi-reliable.You might recall that Republicans and Elon Musk threw a hissy fit a few years ago when the Biden FCC prioritized “future-proof” fiber and higher-capacity 5G services over Starlink in previous government subsidy programs, (correctly) expressing concerns that the service lacked the capacity to provide consistently reliable speeds on the taxpayer dime. These are all things Republican Elon Musk ass kissers either don’t know, or don’t care about as they work to reward their billionaire benefactor. It will be up to their constituents to figure it out later. But money redirected to Starlink is money redirected to cheaper and better broadband alternatives, including super cheap gigabit fiber access and community-owned and operated broadband networks. So again, Starlink is a nice step up if you’re in the middle of nowhere, lack any other connectivity options, can afford it, and don’t care about its potential environmental impact. But it shouldn’t be taking priority in terms of taxpayer subsidies. Unless, of course, you only care about kissing Elon Musk’s ass and don’t  about the constituents you claim to serve.]]></content:encoded></item><item><title>Why Extracting Text From PDFs Still Feels Like a Hack–And the Legacy Design that Keeps AI Stuck</title><link>https://hackernoon.com/why-extracting-text-from-pdfs-still-feels-like-a-hack-and-the-legacy-design-that-keeps-ai-stuck?source=rss</link><author>Paolo Perrone</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:00:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Devs working with LLMs run into document parsing constantly. And every few months, there’s a new wave of hype (or frustration) around the PDF problem. During those moments, it’s not unusual to see software folks venting about how one file format became such a massive headache. But the struggle isn’t new.\
Long before LLMs entered the picture, entire SaaS businesses were built around managing the messiness of PDFs. And for good reason, it’s a format that was never designed for the kind of structured, machine-readable access we now expect.\
When software becomes as widespread as Adobe Acrobat and the PDF format, it starts to feel like a permanent part of the landscape. It’s easy to forget that behind that ubiquity were real design decisions, constraints, and tradeoffs made by real engineers solving real problems. Problems that, over time, evolved and became the roots of today’s pain.\
Yes, PDFs are frustrating. But they weren’t born broken. In fact, they were a surprisingly elegant solution for their time.\
So, let’s zoom out. This story takes a step back to explore the origins of the PDF format: how it came to be, what problems it set out to solve, and how the decisions made in the early 90s still ripple through today’s stack. The goal: to understand not just the “why is this so hard?”, but also the “how did we get here?”Back to the ‘80s, from paper to pixels.The shift had begun. Personal computers were exploding in popularity, and paper documents were no longer the default. Software like VisiCalc, WordStar, WordPerfect, and early Microsoft Word marked the dawn of a new way to write, edit, and share.By the late ’80s, PC suites had all but killed off the typewriter. Executives could tweak reports minutes before a meeting. Analysts were running “what-if” scenarios in spreadsheets. Teachers were printing tests on the fly. Engineers replaced drafting tables with digital blueprints.\
Increasingly, documents became the new workplace. Not just the end product, but where the work actually happened.The ‘90s and the birth of the PDF.In the early 1990s, the rise of PC-based word processing and electronic file sharing solved many problems, while introducing new ones. Every computer had its own fonts, printer drivers, and layout quirks. A report that looked perfect on one machine could print as a jumbled mess on another. Sharing files became a gamble.\
To fix this, in 1991 Adobe co-founder John Warnock and his team launched a project codenamed “Camelot” to create a truly universal document format. The result was the PDF, a file that embedded fonts, graphics, and page layout all in one place. This “digital paper” guaranteed that documents looked exactly the same everywhere, whether on Windows, Mac, or any printer.\
By bundling every font, image, and layout detail into a single file, PDFs let users share documents without surprises, and what you see on screen is printed exactly the same everywhere. Adobe made the free Acrobat Reader available in 1994, and within five years, PDF became the go-to format for everything from product manuals and corporate reports to government forms and academic papers. \
By the early 2000s, “export as PDF” was a one-click option in almost every authoring tool, and organizations across industries embraced it for distribution, archiving, and compliance. And it’s still the standard today.The very thing that made PDFs so appealing (their promise of pixel-perfect fidelity) also introduced a hidden trade-off: it locked content into a rigid, print-first structure.\
Beneath every flawless page was essentially a digital snapshot, built to mimic what came out of a printer. Headings, tables, paragraphs, none of it had semantic meaning. To a computer, it was just coordinates and text boxes scattered across a canvas.\
At first, this didn’t matter. But as documents moved from desktops to web browsers, mobile screens, and automated pipelines, the cracks began to show. Want to extract clean data? Reflow text on a phone? Understand document structure? Suddenly, what looked clean to humans became a mess for machines. \n  Tagged PDF and Other Modernization AttemptsAdobe wasn’t blind to the problem. Tagged PDF (introduced in 2001 and later formalized in PDF/UA for accessibility) adds an HTML-like logical structure. It never became universal, but it is mandated for accessible government documents and widely used in large-enterprise workflows. Other milestones, such as PDF/A for long-term archiving, XMP metadata support, and the 2008 hand-off of the spec to ISO, show steady efforts to modernize the format. Still, broad adoption lagged; tagging is invisible to most users, tedious for creators, and often stripped out by careless export settings.\
A whole ecosystem of SaaS tools popped up to bridge this gap. You see it in heavyweights like DocuSign, in the many web-based PDF editors such as DocHub, and in open-source libraries like Poppler, which developers depend on just to pull text out of PDFs. \
That’s also why the big cloud players are all throwing serious AI muscle at this problem: AWS with Textract, Google with Document AI, and Microsoft with Azure AI Document Intelligence. The market emerged, products followed, and plenty of revenue flowed. Adobe, whether we like it or not, changed the game.The Rise of AI-Native PDF HandlingWhen ChatGPT hit, the “PDF problem” exploded. Companies scrambled to feed their data into LLMs, only to hit a wall: most of that valuable info was locked away inside PDFs.\
At first, the goal was simple: just extract clean text for Retrieval-Augmented Generation (RAG). But that quickly proved too basic. Without layout awareness, text from columns got scrambled, tables turned into nonsense, images got ignored, and important context disappeared.\
Modern Document AI now trains models to understand a document’s visual and logical layout: identifying titles, paragraphs, tables, and images. So, AI can reference information, skip repeated headers/footers, and grasp the overall structure.\
This AI stack reveals the full extent of the mess we’re dealing with. What should be straightforward data extraction now requires multiple specialized layers:Layout analysis to understand document structure,OCR to extract text from images and scanned documents,VLM orchestration to coordinate these different AI components.Each layer adds latency, potential errors, and compute cost. The irony is staggering: we’re using some of the most advanced AI models ever built to solve a problem that stems from a 30-year-old decision to treat documents like photographs.\
While PDFs have gradually evolved, their print-first DNA keeps piling costs onto every modern workflow. Structured formats, scanned or photographed, do introduce some of the same hurdles, but PDF’s design amplifies the pain.We can’t scrap decades of PDFs overnight, but we can avoid repeating history. For new content, choose born-digital formats that preserve semantics by default:Markdown-derived standards for technical docs,or DOCX/OOXML when Office compatibility is a must.\
When a fixed-layout file is unavoidable, export with full tags and metadata intact; some authoring tools now automate this. Government procurement rules that require PDF/UA compliance are a positive precedent. Similar pressure from enterprises on vendors and regulators can push tagging from “nice-to-have” to “table stakes.”\
Long term, open standards like W3C’s Portable Web Publication or EPUB 3, along with upcoming containerized JSON-based formats, promise fidelity without sacrificing structure. Supporting these in mainstream authoring tools (and educating users to adopt them) will spare the next generation from writing vision models just to pull text out of a contract.\
The story of PDFs proves that early design choices echo for decades. The lesson isn’t to vilify the engineers who solved 1991’s problem; it’s to recognize that today’s “good enough” shortcuts become tomorrow’s costly handcuffs. Let’s embed semantics at the source, back open, machine-readable standards, and ensure the next wave of document tech is built for humans and machines alike.\
For teams already dealing with legacy formats, tools like  offer an Open-Source API-based pipeline to convert complex documents into structured, chunked formats tailored for LLM and RAG workflows, available both as hosted endpoints or self-managed infrastructure.Struggling to Grow Your Audience as a Tech Professional?The Tech Audience Accelerator is the go-to newsletter for tech creators serious about growing their audience. You’ll get the proven frameworks, templates, and tactics behind my 30M+ impressions (and counting).]]></content:encoded></item><item><title>Cuba’s Power Grid Nears Total Failure</title><link>https://spectrum.ieee.org/cuba-energy-crisis</link><author>Ricardo Torres</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExNTM5Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2NTc3MTgyMn0.MpArvxigPLjNqXMmdCRIIb_2Han8CkTmfg2U1UC9dj8/image.jpg?width=600" length="" type=""/><pubDate>Tue, 1 Jul 2025 12:00:05 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Decades of neglect and subpar fuel leave Cuba’s energy infrastructure in crisis]]></content:encoded></item><item><title>Gentoo Releases Updated Install Media Based On KDE Plasma 6.3 + Linux 6.12 LTS</title><link>https://www.phoronix.com/news/Gentoo-Linux-July-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Gentoo Linux project ended the month of June by releasing new install media...]]></content:encoded></item><item><title>How Blockchain Is Redefining Financial Inclusion for the Unbanked in 2025</title><link>https://hackernoon.com/how-blockchain-is-redefining-financial-inclusion-for-the-unbanked-in-2025?source=rss</link><author>Vision NP</author><category>tech</category><pubDate>Tue, 1 Jul 2025 11:59:50 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In 2025, the global economy will continue to rapidly digitize. Do you know another secret? Yet over 1.4 billion people remain unbanked, according to the latest World Bank data. These people lack access to traditional financial services due to systemic barriers, such as identity issues, geographic isolation, high service fees, and distrust in centralized institutions. On the other hand, blockchain networks have a decentralized nature with the potential for financial inclusion. Bitcoin appeared to be the main enabler of decentralization. Traditional financial systems are bound by gatekeeping, bureaucracy, and geography, but blockchain offers a trustless, borderless, and programmable financial layer. It can turn a smartphone into a bank, digital ID provider, and loan officer, all in one.\
The diagram below explains how blockchain technology transforms the financial inclusion of an unbanked population.\
This study explores how blockchain tackles financial exclusion through the latest innovations.1. Own Your Online Identity—No Central Authority RequiredOne of the key obstacles to unbanking is its verifiable identity. Decentralized Identity (DID blockchain systems (Polygon ID and Proof of Personhood of Worldcoin) enable users to create tamper-resistant digital identities without the help of central authorities. They are owned by the users themselves and cryptographically authenticated, meaning that they can be applied to any finance platform. The DID enables one to.Participate in global marketsThe effective implementation of a DID can be determined using a real-life example in Bhutan. In the middle of 2024, Bhutan migrated its national self-sovereign digital identity operating on the Hyperledger Indy to the Polygon blockchain and started using the previously adopted CREDEBL protocol, which is an open-source, UN-approved verifiable credentials system.\
It is very likely that you also ask yourself why it is important. Bhutan is one of the first nations in the world to fully implement a sovereign identity system and provide all citizens with ownership and control of their digital credentials. Moving to Polygon implies a faster, safer, and higher capacity to have a broad, nationwide scope of use.2. Banking Without Banks? Stablecoins Make It PossibleCentralized traditional banks often charge high fees for account maintenance, remittances, and FX conversion. However, if we locate Stablecoins like USDC, cUSD (Celo Dollar), and GHO (Aave's stablecoin), they provide a USD-pegged, low-volatility alternative that can be sent and received globally, instantly, and at nearly zero cost.Through mobile wallets, such as Trust Wallet, users in remote areas canSave in a stable currencyAvoid local currency inflationIn Latin America, stablecoins are rapidly growing, as in the case of Venezuela, where stablecoins are used in 34 % of cases, and Argentina, where they constitute approximately 61.8 % of crypto activity, far more than in Brazil (59.8 %) and Mexico (18 %). In low-banking, high-inflation contexts, mobile-first users are also turning to dollar-pegged tokens, such as USDT and USDC, as an easily accessible and reliable source of savings, transactions, and even remittances when normal banks are weakened by rampant inflation and rising costs of using bank services.3. Microfinance and DeFi LendingThe decentralized Finance (DeFi) protocols Goldfinch, Aave Arc, and Maple Finance currently target real-world assets (RWA) and under-collateralized lending, giving credit to new establishments and individuals in emerging markets.Accessible without a credit historyWith blockchain, a street vendor can seek out a loan through a foreign investor to whom smart contracts apply and are repaid through the blockchain.\
The fact that Goldfinch has successfully lent more than USD 100 + million to real-life businesses, and most prominently within the Sub-Saharan Africa market, exemplifies the issue of using blockchain to match global capital to the underbanked. Goldfinch offers accessible and sustainable financing to underserved members of the community by utilizing the USDC, local partnerships, and smart contracts to offer scalable, accountable, and impactful financing to the underserved.4. Remittances Without MiddlemenThe world remittance market, with an annual turnover of more than 800 billion, is currently monopolized by intermediaries such as the Western Union, charging sums up to 10 percent in fees. Smart contract-based remittance systems such as Ramp Network, Xoom using the USDC, and OnFinality have minimized the cost to be nearly free and settle within minutes rather than days.\
One of the real-world examples for this category is that in the Philippines,  is revolutionizing remittances by using blockchain and stablecoins such as USDC and PHPC to enable near-instant, low-cost cross-border transfers.\
Traditional services charge 6-7% and create a delay by taking time, but Coins.ph processes remittances in minutes with fees as low as 0.1%. Recently, it handled over  in remittances. This allowed unbanked users to reach local cash-in partners.\
This is the same case as Chipper Cash, the African platform that runs on Ripple to make cross-border remittances through cryptocurrency simple and relatively cheap, which is a good example of how blockchain can remove middlemen and raise financial inclusion.5. Local Economies Powered by BlockchainNew blockchain-backed economies are becoming strong alternatives to conventional markets as substitutes for community currencies and economies of rewards, which are dependent on banks and centralized authorities. In such ecosystems, users have access to earn, save, spend, and accumulate wealth completely inside blockchain-native infrastructure, in many cases, only with a mobile phone. promote environmental action through token incentives. and  enable direct aid and grants without bureaucratic delay. integrated blockchain wallets with mobile money services, such as M-Pesa, in Africa.These projects present a new trend: blockchain is no longer reshaping finance, but rather democratizing access to value, redefining aid, and a local, inclusive economy. Blockchain native systems allow the elimination of mediators and the provision of communities that are disadvantaged in the global economy with resilient financial infrastructure by allowing programmable trust. Efforts such as Spacecoin to decentralize the Internet and finance show how blockchain can overcome the digital and financial divides. By paying Internet bills with crypto, generally no more than a dollar or two per month, disadvantaged communities develop an on-chain credit history that can lead them to loans and other financial services without going through banks.6.Key Challenges to Watch & Attempt to Tackle ThemOwing to the transformational power of the blockchain, there are real-world obstacles to its use in disadvantaged communities. Problematic issues should be addressed in a context-aware and user-centric manner to help create meaningful financial inclusion.A significant portion of the unbanked population does not have smartphones or access to a stable Internet connection, which reduces their exposure to dApps. This gap is bridged by mobile-first instruments and SMS-based wallets such as .Interfaces are difficult to understand and deal with, and they contain unknown cryptographic concepts. Getting new users on board is assisted by local education initiatives and simplified apps such as .Using stablecoins puts users at regulatory and issuer risks. Asset-backed or regional stablecoins can be made more resilient.The use of technical terms and the lack of proper localization made mass adoption impossible. To address this, platforms should enhance the user interface and experience with a user-friendly multilingual interface.Losing private keys or becoming victims of fraud can be disastrous. Therefore, emerging safer models include social recovery wallets (e.g., Trust Wallet has a feature to back up your key in Google Drive) and local custodianship models.Vague or antagonistic legislation halts the advancement in certain countries like Bhutan, which are leading positive exponents by adopting controlled Blockchain ID structures.The expensive mining costs for PoW-based blockchains, such as Bitcoin, and network congestion, restrict their use. To tackle this problem, there are other scalable and cheap options, including chains such as Polygon, Celo, and Solana.These are not only technical issues, but also ethical. Addressing these problems presents the challenge of working together, and doing so involves developers, policymakers, educators, and communities. It is only after that time that the blockchain can open the door to fair access to financial tools.🔗 Blockchain + AI: Smarter Financial AccessArtificial intelligence (AI)-based products and services are becoming increasingly popular as of 2025, and the future seems even brighter if blockchain is combined with AI for effective services. Some real-world examples, such as Worldcoin and the Human Protocol, combine blockchain with AI to assess reputational credit scores. This allows lenders to evaluate unbanked users based on the following criteria:This AI + blockchain combination completely upends conventional credit scoring regimes in low-data settings by relying on on-chain modes of behavior, peer-to-peer recommendations, digital identity solutions, and decentralized reputation systems to create dynamic, immutable credit profiles. The new system can offer inclusive, real-time risk assessment, whereas traditional systems use credit rating agencies that have not changed since the 80s and fixed financial history; millions of unbanked people will be able to be supplied with fair credit, insurance, and other financial instruments.\
🏦 Central Bank Digital Currencies (CBDCs)With DeFi developing organically, it is being approached by the government. Over 130 nations are in the trial or implementation of CBDCs. When combined with blockchain wallets, CBDCs canEnable state benefits without banksDistribute UBI (universal basic income)Track inflation-resistant micro-savingsThe e-CNY, eNaira, and India digital rupees are already under testing for merchant payments and cross-border payouts.\
🌍 Local Economies, Global RailsKenyan community projects, such as , or a Latin American community project, such as , enable communities to create locally built digital currencies that are backed to stabilize other ecoins, such as cUSD or DAI. These currencies are frequently issued to local DAOs or mobile applications and are tasked with reflecting the actual economic transactions in the community.\
These systems: \n • Mobilize microtrade without money: allow local businesses, farmers, and vendors to enable them to receive payments digitally, even in places where the traditional banking system is not well connected. \n • Pay people to do good things, such as tree planting, education, recycling, or volunteering--turn people into communal tokens that anybody can spend in the community.• Establish circular economies founded on activities tied in which value is generated and used in the local area, hence disregarding the need to rely on outside aid or unstable fiat currencies.\
For example, the Sarafu Network in Kenya, initiated by Grassroots Economics, has provided over 60,000 users with services and transfer of goods using digital credits printed in the community, with a total number of transacted dollars so far, being more than $ 3 million. Members are credited with credit vouchers in exchange for labor or production, which can be used in the network for food, transport, and other basic necessities. The given model not only creates financial stability but also develops better social bonds, more economic collaboration, and local development, which are also strengthened by blockchain technology that works under the surface.Final Thoughts: A Financial ResetBlockchain is not just a technological innovation, it is also a catalyst for economic empowerment and financial justice. Blockchain allows people in an underserved community to be empowered about their financial future, irrespective of geographical locations and backgrounds, by breaking the traditional rungs of identity, access, and trust.\
Low-fee remittances made AirBnB-killer project stable ecoins, microcredit without collateral provided by the DeFi protocols, secure documents and data on the blockchain, and decentralized identity systems that are replacing formal KYC, all of which are already here and radically developing.\
We are in a new financial age where no one is forgotten and banking comes not with a privilege but a right as governments, developers, and communities collaborate to create ethical, inclusive, and clear blockchain-based solutions.\
The next billion users will not just join Web3; they will help shape it. In the future, the most powerful bank may not have walls. It may simply be a phone, key, or a chain.]]></content:encoded></item><item><title>From Crisis to Security - How DePIN Can Solve Tonga&apos;s Cybersecurity Challenges</title><link>https://hackernoon.com/from-crisis-to-security-how-depin-can-solve-tongas-cybersecurity-challenges?source=rss</link><author>Edwin Liava&apos;a</author><category>tech</category><pubDate>Tue, 1 Jul 2025 11:55:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A comprehensive response to recent data breaches and a roadmap for digital sovereigntyThe Wake-Up Call - Recent Cybersecurity Breaches in TongaThe recent leak of patient information online has sent shockwaves through Tonga's government and healthcare systems, highlighting a critical vulnerability that threatens not just individual privacy, but national security itself. When hackers can access sensitive patient data and potentially disrupt essential services, it becomes clear that Tonga's current digital infrastructure is inadequately prepared for modern cyber threats.This breach represents more than just a technical failure, it's a symptom of a fundamental problem with centralized, vulnerable systems that rely on foreign infrastructure and proprietary software. For vulnerable groups, including patients, the elderly, and those requiring critical medical services, these security failures can have life-threatening consequences.The time for half measures is over. Tonga needs a revolutionary approach to cybersecurity, one that puts sovereignty, resilience, and unbreachable security at its core.The DePIN Solution - Building an Unhackable Government NetworkDecentralized Physical Infrastructure Network (DePIN) represents a paradigm shift from vulnerable centralized systems to a distributed, blockchain-based architecture that leverages the proven security model of Bitcoin, a network that has operated for 16 years without ever being successfully hacked or compromised.Why Bitcoin Level Security Matters January 3, 2009 16 Years Never compromised or hacked since inception Imperfect networks and imperfect human aspectsBitcoin has stood the test of time as the most secure decentralized network precisely because it was designed to be resilient against the very types of attacks that are currently plaguing traditional government systems.The Journey to Digital Sovereignty - Tonga's DePIN Transformation StoryPicture this i.e. It's a Monday morning in Nuku'alofa, and instead of waking up to news of another devastating cyberattack, government workers across Tonga are logging into the most secure digital infrastructure in the Pacific. This isn't a distant dream, it's the inevitable destination of our DePIN journey, and here's how we get there.Chapter 1: The Foundation - Building Our Digital Fortress at MEIDECC"Every revolution begins with a single spark."Our story begins where all great transformations do, with a proof of concept that will forever change how Tonga thinks about digital security. Picture the Tonga Ministry of Meteorology, Energy, Information, Disaster Management, Environment, Climate Change and Communications (MEIDECC) building, currently vulnerable to the same attacks that just compromised patient data across the country. But within six months, this same building will house the most secure government network node in the Pacific.The First Month - Discovery and VisionThe transformation begins quietly, almost invisibly. Our technical team walks through MEIDECC's corridors, laptops in hand, conducting what looks like a routine network audit. But this is no ordinary assessment, we're mapping every cable, every server, every potential entry point that hackers could exploit. In conference rooms late into the evening, security experts spread out network diagrams like military strategists planning a campaign.By week three, the real work begins. While government employees continue their daily routines, our developers are crafting something revolutionary in the background, a blockchain architecture specifically designed for Tonga's unique needs. Picture lines of code flowing across screens, each function carefully designed to prevent the exact type of breach that recently exposed sensitive patient information. This isn't just software development; it's digital nation building.Months Two and Three - The Technical RenaissanceCafe Escape, Friends, Post Coffee and other coffee spots around MEIDECC start recognizing our Pasifika Web3 Tech Hub team as regulars. Day and night, they're building Tonga's digital sovereignty one line of code at a time. They're implementing proof-of-authority consensus mechanisms that ensure only trusted government nodes can validate transactions. They're creating smart contracts that will make document forgery impossible. They're designing multi-signature authorization systems that require multiple government officials to approve any sensitive operation.Meanwhile, the real magic is happening in the government's commitment to energy independence. Picture construction crews arriving at MEIDECC with solar panels and battery systems. These aren't just renewable energy installations, they're the foundation of a network that can operate completely independently of Tonga's power grid. When the next cyclone hits and the lights go out across Nuku'alofa, MEIDECC's blockchain nodes will continue humming along, powered by the Pacific sun.Months Four and Five - Hardening the FortressAs solar panels gleam on MEIDECC's roof, inside the building, another transformation is taking place. Hardware security modules, the digital equivalent of bank vaults, are being installed to protect the cryptographic keys that secure Tonga's data. Multi-layered firewalls are configured like digital moats around the government's most sensitive information. Intrusion detection systems are deployed like digital sentries, watching for any sign of malicious activity.The real-time monitoring systems we install don't just detect threats—they learn from them. Every attempted intrusion, every suspicious packet, every anomalous behavior pattern becomes data that makes the system smarter and more resilient.Month Six - The Proof is in the TestingPicture this scene i.e. In a secure facility in Nuku'alofa, ethical hackers, some of the best cybersecurity experts money can hire, are unleashing every attack they can imagine against MEIDECC's new blockchain infrastructure. They try DDoS attacks that would cripple traditional systems. They attempt the same social engineering tactics that compromised the patient data. They simulate the power outages and network failures that have historically left government systems vulnerable.And the system holds. Every attack is deflected, every vulnerability is absent, every backup system activates flawlessly. The blockchain continues processing transactions, the solar power keeps systems operational, and the monitoring systems log every attempted intrusion for future analysis.Chapter 2: The Expansion - From Spark to Wildfire"Success is not a destination, but a journey of continuous transformation"By month seven, word is spreading through government circles about something remarkable happening at MEIDECC. While other ministries struggle with basic cybersecurity, MEIDECC operates with the confidence of an organization whose data is truly secure. Ministers start asking questions: "How can we get this level of protection?"The Ministry Awakening - Months Seven and EightOur story now expands beyond MEIDECC's walls. Picture assessment teams walking through the Ministry of Health, where the recent patient data breach occurred. They're not just looking at current vulnerabilities, they're envisioning a future where patient records are stored on an immutable blockchain, where access requires cryptographic authorization from multiple medical professionals, where even the most sophisticated hackers find only impenetrable walls.At the Ministry of Education, we see the potential for student records that can never be falsified, academic credentials that are instantly verifiable anywhere in the world, and research data that remains secure even as it's shared with international partners.The Ministry of Finance presents the most compelling case for blockchain implementation. Picture budget allocations recorded on an immutable ledger, government expenditures tracked with transparent smart contracts, and financial reporting that citizens can verify themselves. This isn't just cybersecurity—it's the foundation of unprecedented government transparency.Months Nine Through Twelve - The Network EffectThe real magic happens when individual ministry nodes start connecting to form Tonga's unified government blockchain network. Picture dedicated blockchain nodes being installed in each ministry building, each one a fortress in its own right, but collectively forming an impregnable digital kingdom.These aren't isolated systems, they're nodes in a living network that grows stronger with each addition. When the Health Ministry node communicates with the Finance Ministry node to process a medical equipment purchase, the transaction is validated by multiple nodes across the government network. When the Education Ministry needs to verify a student's eligibility for government assistance, the request flows through encrypted channels that make eavesdropping impossible.The Tonga Cable Ltd Partnership - A Strategic AllianceAs our network grows, a crucial partnership emerges with Tonga Cable Ltd. Picture TCL not just as an internet service provider, but as the strategic gatekeeper of Tonga's digital sovereignty. Through the dedicated infrastructure of TongaIX, Tonga's own internet exchange point (IXP), government data never has to leave Tongan soil to travel between ministries.This means that when a doctor at Vaiola Hospital accesses a patient's records, that data travels through Tongan infrastructure, controlled by Tongan engineers, secured by Tongan cryptography. No foreign servers, no vulnerable international links, no opportunity for external interference."True sovereignty is not given, it is built, byte by byte, block by block"Months Thirteen Through Eighteen - The Great MigrationPicture government offices across Tonga undergoing a quiet but profound transformation. The familiar Windows interfaces that have dominated government desktops for decades are being replaced by sleek, secure Linux distributions designed specifically for government use. Microsoft Office gives way to LibreOffice, but this is more than just a software change, it's a declaration of digital independence.Government workers receive training not just on new software, but on a new philosophy of digital security. They learn to think like cybersecurity professionals, to recognize threats before they materialize, to operate in an environment where every click is part of a larger security ecosystem.This is where the PasifikaOS vision comes alive, not just for Tonga, but as a model for Pacific digital independence that other island nations will soon seek to emulate.Months Sixteen Through Twenty - The Backup RevolutionWhile Tonga has historically been at the mercy of submarine cable failures, our DePIN solution includes a revolutionary backup strategy. Picture Starlink ground stations being installed at each ministry location, not as primary connections, but as instantly available backup links that activate automatically when traditional connectivity fails.The next time a ship's anchor damages Tonga's submarine cable (and it will happen), government operations continue without interruption. Critical communications flow through Low Earth Orbit satellites, blockchain transactions continue processing, and essential services remain available to citizens.Months Eighteen Through Twenty Four - The Digital GovernmentIn the final phase of our journey, Tonga doesn't just have secure government systems, it has a completely reimagined digital government that serves as a model for the world. Picture citizens receiving digital identity credentials stored on the blockchain, making identity theft virtually impossible. Elections conducted through cryptographically secure voting systems that provide both complete transparency and absolute privacy.Government procurement happens through smart contracts that automatically release payments when delivery conditions are met, eliminating corruption and ensuring efficient use of public funds. Land registry records become immutable blockchain entries that resolve property disputes instantly and prevent fraudulent land claims.Budget tracking becomes a real-time, public dashboard where every citizen can see exactly how their tax money is being spent, with smart contracts automatically flagging unusual expenditures for review."In the calm after the storm, we realize that we built not just a network, but a legacy"Picture this scene twelve months from today i.e. Across the Pacific, another island nation experiences a massive cyberattack that brings down government services for weeks. Citizens can't access health records, schools can't verify student credentials, and government operations grind to a halt.But in Tonga, government workers arrive at their offices that same morning to find their systems operating normally. The blockchain network continues processing transactions, the solar powered infrastructure hums along independently, and the automated monitoring systems haven't detected so much as a suspicious packet.More importantly, when news of the attack reaches Tonga, government officials don't respond with fear, they respond with confidence. They know their systems are built on the same cryptographic foundations that have protected Bitcoin for sixteen years without a single successful hack. They know their data is distributed across multiple nodes, their communications are end-to-end encrypted, and their backup systems can operate independently of any external infrastructure.This isn't just the end of our implementation story, it's the beginning of Tonga's emergence as a digital sovereignty leader in the Pacific. Other nations start sending delegations to study Tonga's DePIN implementation. International cybersecurity experts cite Tonga's government network as a model for resilient infrastructure design. And most importantly, Tongan citizens begin to trust their government's digital services in ways they never could before."Ko e 'Otua mo Tonga ko hotau tofi'a" - God and Tonga are our inheritanceOur journey from vulnerability to invincibility isn't just a technical transformation, it's a story of national pride, digital sovereignty, and the courage to build something completely new rather than patch something fundamentally broken. In twenty four months, we don't just solve Tonga's cybersecurity problems, we eliminate them entirely, creating a digital infrastructure that becomes Tonga's gift to the Pacific and the world.Technical Architecture: Layer-by-Layer SecurityLayer 1: Physical InfrastructureSolar-powered mini-grids: Energy independence eliminates grid vulnerability No single point of failure Military-grade encryption at the hardware levelLayer 2: Network Security All communication encrypted end-to-end Efficient, secure packet routing Government traffic stays within Tonga Independent satellite connectivityLayer 3: Blockchain Consensus Government controlled validator nodesMulti-signature authorization: Multiple approvals required for critical actions All transactions permanently recorded and verifiable Automated execution of government processesLayer 4: Application Security Every user and device must be verified Permissions based on job function and clearance level 24/7 surveillance of all network activityAutomated incident response: Immediate isolation of compromised systemsAddressing Current VulnerabilitiesProblem 1: Centralized Data Storage Single point of failure makes entire systems vulnerable  Distributed storage across multiple blockchain nodes ensures no single point of failureProblem 2: Foreign Infrastructure Dependency Reliance on foreign systems creates sovereignty and security risks  100% Tongan-controlled infrastructure with local decision-makingProblem 3: Proprietary Software Vulnerabilities Closed-source software contains unknown security flaws  Open-source transparency allows for continuous security auditingProblem 4: Inadequate Backup Systems System failures can cause extended outages  Multiple redundancy layers including solar power and satellite backupProblem 5: Insufficient Cyber Threat Response Slow response to security incidents  Real-time monitoring with automated threat response and isolationEconomic Benefits: Security That Pays for ItselfLicensing fee elimination: Save approximately TOP 150,000 annually Solar power saves estimated TOP 35,000 per year 99.9% uptime eliminates productivity lossesLong-term Economic Impact Full system cost recovery within 3-4 years Compared to traditional cybersecurity solutions Technical expertise remains in Tonga Avoid costs of incident response and reputation damage Meet international data protection standardsInsurance premium reduction: Demonstrable security measures reduce coverage costsImplementation Roadmap: Making It HappenConduct comprehensive security audit of current government systemsAssemble technical implementation team with blockchain and cybersecurity expertise for MEIDECC proof of conceptBegin stakeholder engagement across all government ministriesShort-term Goals (3-6 Months)Deploy MEIDECC pilot program with full blockchain infrastructureDemonstrate measurable security improvements through testing and monitoringTrain core technical staff on DePIN management and maintenance for other government ministriesMedium-term Objectives (6-18 Months)Roll out to all major ministries with full blockchain node deploymentImplement Tonga Cable Ltd integration for complete network controlDeploy solar power infrastructure for energy independenceBegin open-source migration away from vulnerable proprietary systemsLong-term Vision (18-24 Months)Achieve complete digital sovereignty with 100% Tongan-controlled infrastructureDeploy advanced blockchain services for citizens and businessesEstablish Tonga as a model for Pacific Island digital independenceCreate exportable solution for other developing nationsWhy This Solution Will SucceedProven Technology FoundationBuilt on Bitcoin's 16 year track record of unbreachable security, the DePIN solution leverages battle tested blockchain technology that has never been successfully compromised.Tailored for Tonga's NeedsUnlike generic cybersecurity solutions, this system is specifically designed for Tonga's unique challenges, including natural disaster resilience, energy independence, and sovereignty requirements.The modular design allows for gradual implementation and expansion, while the economic benefits ensure long-term sustainability without ongoing foreign dependency.Future Proof ArchitectureWeb3 compatibility ensures that Tonga's infrastructure will remain cutting edge as digital technologies continue to evolve.Call to Action: Securing Tonga's Digital FutureThe recent cybersecurity breaches are not just isolated incidents, they are warnings of what happens when a nation's digital infrastructure is inadequately protected. The cost of inaction is measured not just in compromised data, but in lost sovereignty, economic damage, and risks to vulnerable populations.The DePIN solution offers Tonga a unique opportunity to leapfrog traditional cybersecurity approaches and implement a truly revolutionary system that provides: that has never been breachedComplete digital sovereignty with 100% local control through cost savings and local capacity building through multiple redundancy layers with emerging Web3 technologiesGovernment Leadership Commitment: Secure high-level government buy-in for the DePIN transformation Pursue both local and donor funding options for the initial implementation Recruit and train the technical expertise needed for deployment Build public support for digital sovereignty initiatives"Ko e 'Otua mo Tonga ko hotau tofi'a" - God and Tonga are our inheritanceThe time has come for Tonga to claim its digital inheritance, a secure, sovereign, and sustainable technological infrastructure that protects its people and preserves its independence for generations to come.The full technical specifications, architecture diagrams, and detailed budget breakdowns are available in our comprehensive pitch deck presentation. This interactive presentation includes:Complete network architecture visualizationDepartment-by-department blockchain node configurationSolar power infrastructure layoutsBusiness continuity and disaster recovery planningDetailed budget analysis with cost comparisonsTimeline and implementation roadmaps]]></content:encoded></item><item><title>Mesa 25.2 Should Have Initial Vulkan Support In Good Shape For NVIDIA Blackwell</title><link>https://www.phoronix.com/news/Mesa-25.2-NVK-Blackwell</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 10:44:11 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Mesa 25.2 release that will likely be out as stable in August should have nice initial support for the newest NVIDIA Blackwell GPUs, namely used by the GeForce RTX 50 series, with the NVK open-source driver for Vulkan usage...]]></content:encoded></item><item><title>NVIDIA Confirms 580 Linux Driver Is The Last For Maxwell / Pascal / Volta</title><link>https://www.phoronix.com/news/NVIDIA-580-Linux-Driver-Last-HW</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 10:32:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[NVIDIA previously warned CUDA users that CUDA 12.x is the last for Maxwell, Pascal, and Volta GPUs. NVIDIA overnight now officially confirmed that the Maxwell / Pascal / Volta GPU support is going to end in their Linux driver with the upcoming NVIDIA R580 Linux driver series...]]></content:encoded></item><item><title>Framework 12, AMD Strix Halo &amp; Linux Kernel Improvements Were Most Popular In June</title><link>https://www.phoronix.com/news/June-2025-Highlights</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 10:17:46 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Over the course of last month on Phoronix were 240 original news articles written by your's truly as well as another 24 Linux hardware reviews / multi-page featured benchmark articles. On top of that last month also marked the 21st birthday of Phoronix.com...]]></content:encoded></item><item><title>digiKam 8.7 Released With AI Auto-Rotation Tool, OpenCV OpenCL + CUDA Support</title><link>https://www.phoronix.com/news/digiKam-8.7-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 10:06:50 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The digiKam 8.7 professional photo management software has been released for this KDE/Qt-aligned open-source solution for digital photography...]]></content:encoded></item><item><title>How U.S. Copyright Law Applies to LLM Training</title><link>https://hackernoon.com/how-us-copyright-law-applies-to-llm-training?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Tue, 1 Jul 2025 10:00:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 4 of 10. Section 107 of the Copyright Act identifies four factors for determining whether a given use of a copyrighted work is a fair use:[T]he fair use of a copyrighted work . . . for purposes such as criticism, comment, news reporting, teaching (including multiple copies for classroom use), scholarship, or research, is not an infringement of copyright. In determining whether the use made of a work in any particular case is a fair use the factors to be considered shall include —(1) the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes;(2) the nature of the copyrighted work;(3) the amount and substantiality of the portion used in relation to the copyrighted work as a whole; and(4) the effect of the use upon the potential market for or value of the copyrighted work.These factors presuppose a “use.” So, at the threshold, a court must decide whether a “copyrighted [work] has been used in multiple ways,” then evaluate each. Warhol, 598 U.S. at 533. Uses do not turn on “the subjective intent of the user” but on “an objective inquiry into what use was made, i.e., what the user d[id] with the original work.” Id. at 544–45. A “use” should be construed narrowly enough to not “swallow” distinguishable infringing uses, much less categories of exclusive rights in toto. Id. at 541, 543 n.18, 546–48. Sometimes, the challenged copying involves just one use: In Perfect 10, Inc. v. Amazon.com, Inc., Google visited websites having full-sized images, made only reduced-sized copies, and incorporated those directly into its search engine — the sole use of the thumbnails being as “pointer[s]” to the images themselves. 508 F.3d 1146, 1157, 1160, 1165 (9th Cir. 2007). Sometimes, the copying involves many uses: In the Google Books cases, Google borrowed books from libraries, made both full-image and text-only copies, and incorporated different copies into different tools — one use being to reveal information “about those books,” another use being to provide the books to print-disabled patrons, and still another being to back up the print books if lost. Authors Guild v. Google, Inc., 804 F.3d 202, 217 (2d Cir. 2015) (quoted); Authors Guild, Inc. v. HathiTrust, 755 F.3d 87, 97, 101, 103 (2d Cir. 2014) (other cited uses).Our parties debate an instructive decision. In American Geophysical Union v. Texaco Inc., Texaco employees used scientific articles in a central library, used copies of them in personal desk libraries, and used selected copies again in the scientific laboratory — the first use paid for, the second infringing, and the third plausibly fair but in fact a rare occurrence. 802 F. Supp. 1, 4–5, 14 (S.D.N.Y. 1992) (Judge Pierre Leval), aff’d, 60 F.3d 913, 918–19, 926 (2d Cir. 1994).Here, our parties contest what use or uses are at issue. Anthropic contends it copied Authors’ books only for one use: Only to train LLMs. By contrast, Authors contend it did so for at least two uses: First to build a vast, central library of potentially useful content, and second to train specific LLMs using shifting sets and subsets of that content — over time selecting the more well-organized and well-expressed works for training. Authors also complain that the print-to-digital format change was itself an infringement not abridged as a fair use (Opp. 15, 25). Authors do not allege, however, that any LLM outputs infringing upon their works ever reached users of the public-facing Claude service. This order addresses each of the four factors in turn, pointing out how each applies to the training copies and to the purchased and pirated library copies. It concludes with an integrated analysis.1. THE PURPOSE AND CHARACTER OF THE USE.For a given use at issue, the first factor addresses “the purpose and character of th[at] use, including whether [it] is of a commercial nature or is for nonprofit educational purposes.” 17 U.S.C. § 107(1). A. THE COPIES USED TO TRAIN SPECIFIC LLMS.All agree that one use at issue was training LLMs to receive text inputs and return text outputs. More specifically, Anthropic used copies of Authors’ copyrighted works to iteratively map statistical relationships between every text-fragment and every sequence of text-fragments so that a completed LLM could receive new text inputs and return new text outputs as if it were a human reading prompts and writing responses. Authors further argue — and this order takes for granted — that such training entailed “memoriz[ing]” works by “compress[ing]” copies of those works into the LLM (Opp. 16–17; see Opp. Expert Zhao ¶ 74). The LLMs “memorize[d] A LOT, like A LOT” (Opp. Exh. 35 at -029109). Regardless, the “purpose and character” of using works to train LLMs was transformative — spectacularly so. To repeat and be clear: Authors do not allege that any LLM output provided to users infringed upon Authors’ works. Our record shows the opposite. Users interacted only with the Claude service, which placed additional software between the user and the underlying LLM to ensure that no infringing output ever reached the users. This was akin to the limits Google imposed on how many snippets of text from any one book could be seen by any one user through its Google Books service, preventing its search tool from devolving into a reading tool. Google, 804 F.2d at 222. Here, if the outputs seen by users had been infringing, Authors would have a different case. And, if the outputs were ever to become infringing, Authors could bring such a case. But that is not this case. Instead, Authors challenge only the inputs, not the outputs, of these LLMs. They point to the fully trained LLMs and the Claude service only to shed light on how training itself uses copies of their works and the ways the Claude service could be used to produce still other works that would compete with their works. This order does the same. Authors’ arguments that the training use is not transformative are unavailing. First, Authors argue that using works to train Claude’s underlying LLMs was like using works to train any person to read and write, so Authors should be able to exclude Anthropic from this use (Opp. 16). But Authors cannot rightly exclude anyone from using their works for training or learning as such. Everyone reads texts, too, then writes new texts. They may need to pay for getting their hands on a text in the first instance. But to make anyone pay specifically for the use of a book each time they read it, each time they recall it from memory, each time they later draw upon it when writing new things in new ways would be unthinkable. For centuries, we have read and re-read books. We have admired, memorized, and internalized their sweeping themes, their substantive points, and their stylistic solutions to recurring writing problems. Second, to that last point, Authors further argue that the training was intended to memorize their works’ creative elements — not just their works’ non-protectable ones (Opp. 17). But this is the same argument. Again, Anthropic’s LLMs have not reproduced to the public a given work’s creative elements, nor even one author’s identifiable expressive style (assuming arguendo that these are even copyrightable). Yes, Claude has outputted grammar, composition, and style that the underlying LLM distilled from thousands of works. But if someone were to read all the modern-day classics because of their exceptional expression, memorize them, and then emulate a blend of their best writing, would that violate the Copyright Act? Of course not. Copyright does not extend to “method[s] of operation, concept[s], [or] principle[s]” “illustrated[ ] or embodied in [a] work.” 17 U.S.C. § 102(b); see, e.g., Nichols v. Universal Pictures Corp., 45 F.2d 119, 120–22 (2d Cir. 1930) (Judge Learned Hand) (stage properties and storytelling elements); Apple Comput., Inc. v. Microsoft Corp., 35 F.3d 1435, 1445 (9th Cir. 1994) (“user-friendly” design principles and elements); Swirsky v. Carey, 376 F.3d 841, 848 (9th Cir. 2004) (music theory principles and chord progressions).Third, Authors next argue that computers nonetheless should not be allowed to do what people do.Authors cite a decision seeming to say as much (Opp. 16–17). But the judge there twice emphasized while discussing “purpose and character” of the use that what was trained was “not generative AI (AI that writes new content itself).” Rather, what was trained — using a proprietary system for finding court opinions in response to a given legal topic — was a competing AI tool for finding court opinions in response to a given legal topic. That was not transformative. Thomson Reuters Enter. Centre GmbH v. Ross Intell. Inc., 765 F. Supp. 3d 382, 398 (D. Del. 2025) (Judge Stephanos Bibas), appeal docketed, No. 25-8018 (3d Cir. Apr. 14, 2025). A better analogue to our facts would be an AI tool trained — using court opinions, and briefs, law review articles, and the like — to receive legal prompts and respond with fresh legal writing. And, on facts much like those, a different court came out the other way. It found fair use. White v. W. Pub. Corp., 29 F. Supp. 3d 396, 400 (S.D.N.Y. 2014) (Judge Jed Rakoff). The latter use stood sufficiently “orthogonal” to anything that any copyright owner rightly could expect to control. See Warhol, 598 U.S. at 538–40. It could thus be freed up for the copyist to use, “promot[ing] the progress of science and the arts, without diminishing the incentive to create.” Id. at 531 (emphasis added); see U.S. CONST. art. I, § 8, cl. 8. In short, the purpose and character of using copyrighted works to train LLMs to generate new text was quintessentially transformative. Like any reader aspiring to be a writer, Anthropic’s LLMs trained upon works not to race ahead and replicate or supplant them — but to turn a hard corner and create something different. If this training process reasonably required making copies within the LLM or otherwise, those copies were engaged in a transformative use. The first factor favors fair use for the training copies. But that is not the only use at issue.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>How Robotic Hives and AI Are Lowering the Risk of Bee Colony Collapse</title><link>https://hardware.slashdot.org/story/25/06/30/230235/how-robotic-hives-and-ai-are-lowering-the-risk-of-bee-colony-collapse?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[alternative_right shares a report from Phys.Org: The unit -- dubbed a BeeHome -- is an industrial upgrade from the standard wooden beehives, all clad in white metal and solar panels. Inside sits a high-tech scanner and robotic arm powered by artificial intelligence. Roughly 300,000 of these units are in use across the U.S., scattered across fields of almond, canola, pistachios and other crops that require pollination to grow. [...] AI and robotics are able to replace "90% of what a beekeeper would do in the field," said Beewise Chief Executive Officer and co-founder Saar Safra. The question is whether beekeepers are willing to switch out what's been tried and true equipment. [...]
 
While a new hive design alone isn't enough to save bees, Beewise's robotic hives help cut down on losses by providing a near-constant stream of information on colony health in real time -- and give beekeepers the ability to respond to issues. Equipped with a camera and a robotic arm, they're able to regularly snap images of the frames inside the BeeHome, which Safra likened to an MRI. The amount of data they capture is staggering. Each frame contains up to 6,000 cells where bees can, among other things, gestate larvae or store honey and pollen. A hive contains up to 15 frames and a BeeHome can hold up to 10 hives, providing thousands of data points for Beewise's AI to analyze.
 
While a trained beekeeper can quickly look at a frame and assess its health, AI can do it even faster, as well as take in information on individual bees in the photos. Should AI spot a warning sign, such as a dearth of new larvae or the presence of mites, beekeepers will get an update on an app that a colony requires attention. The company's technology earned it a BloombergNEF Pioneers award earlier this year. "There's other technologies that we've tried that can give us some of those metrics as well, but it's really a look in the rearview mirror," [said Zac Ellis, the senior director of agronomy at OFI, a global food and ingredient seller]. "What really attracted us to Beewise is their ability to not only understand what's happening in that hive, but to actually act on those different metrics."]]></content:encoded></item><item><title>Cloudflare launches a marketplace that lets websites charge AI bots for scraping</title><link>https://techcrunch.com/2025/07/01/cloudflare-launches-a-marketplace-that-lets-websites-charge-ai-bots-for-scraping/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Tue, 1 Jul 2025 10:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Cloudflare is launching a new marketplace that reimagines the relationship between publishers and AI companies.]]></content:encoded></item><item><title>QDyLoRA in Action: Method, Benchmarks, and Why It Outperforms QLoRA</title><link>https://hackernoon.com/qdylora-in-action-method-benchmarks-and-why-it-outperforms-qlora?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 1 Jul 2025 09:32:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
A. Supplementary Material2 Proposed Method: Quantized DyLoRA\
Following QLoRA (Dettmers et al., 2023), we used 4-bit Normal Float (NF4) for storing the double quantized pre-trained weights. As all the computations need to be calculated in BFloat16 precision, DDequant-NF4 will dequantize the stored data. Similar to (Dettmers et al., 2023), we have:\
Algorithm 1 describes the workflow of our proposed QDyLoRA in detail.3 Experiments and EvaluationThis section evaluates the efficiency and efficacy of QDyLoRA through several instruct-fine-tuning\
tasks. The first experiment compares QDyLoRA with QLoRA on Massively Multitask Language Understating (MMLU) benchmark (Hendrycks et al., 2020), consisting of more than 50 different tasks, spanning from fundamental mathematics and U.S. history to computer science and law. As shown in Table 1 [1], we finetune LLaMA-7b, LLaMA-13b, LLaMA2-13b, and Falcon40b on different datasets, Alpaca (Taori et al., 2023), OASST1 (Köpf et al., 2023), Self-Instruct (Wang et al., 2022), and FLANv2 (Chung et al., 2022), using QLoRA and QDyLoRA techniques. We use the same training budget and maximum LoRA rank[2] for each technique. The results consistently show that QDyLoRA achieves a superior performance by finding the optimal rank.\
The second experiment provides a more in-depth comparison between QLoRA and QDyLoRA. In particular, we fairly finetuned Falcon-40b on WebGLM (Liu et al., 2023) and GSM8k (Cobbe et al., 2021) benchmarks, and compared their test performances across different ranks. As described in Table 2, QDyLoRA attains superior performance, notably when employing its optimal ranks (Rank 2 for Web-GLM and Rank 8 for GSM8k). Furthermore, QDyLoRA exhibits consistent superiority over QLoRA, particularly at lower ranks. These findings emphasize the adaptive nature of QDyLoRA in dynamically adjusting its focus during fine-tuning, leading to enhanced efficiency and efficacy compared to its static counterpart, QLoRA. The third experiment compares the performance of DyLoRA, QDyLoRA, and QLoRA on GSM8k and TriviaQA (Joshi et al., 2017) while adopting LLaMA2-13b and LLaMA-7b as LLMs. Table 3 reports the results. As the table illustrates, for smaller-size models, i.e. LLaMA-7b, DyLoRA and QDyLoRA both perform superior than QLoRA. For larger models, i.e. LLaMA2-13b, DyLoRA fails due to the out-of-memory (OOM) error while QDyLoRA works the best in such situations.4 On the semi-sorted behavior of QDyLoRAAs shown in Table 2, QDyLoRA reveals a semisorted performance across ranks. We justify this behavior by pointing out the limited finetuning budget. In a limited budget assumption, QDyLoRA updates its lower ranks more frequently than its higher ranks. That is because of the fact that lower ranks are also updated when higher ranks are selected. In other words, lower ranks have more chance to get updated than higher ranks. Hence, lower ranks are more tuned than higher ranks.(1) Hossein Rajabzadeh, University of Waterloo and Huawei Noah’s Ark Lab (hossein.rajabzadeh@uwaterloo.ca);(2) Mojtaba Valipour, University of Waterloo (mojtaba.valipour@uwaterloo.ca);(3) Tianshu Zhu, Huawei Noah’s Ark Lab (tianshu.zhu@huawei.com);(4) Marzieh Tahaei, Huawei Noah’s Ark Lab (marzieh.tahaei@huawei.com);(5) Hyock Ju Kwon, (hjkwon@uwaterloo.ca);(6) Ali Ghodsi, (ali.ghodsi@uwaterloo.ca);(7) Boxing Chen, Huawei Noah’s Ark Lab (boxing.chen@huawei.com);(8) Mehdi Rezagholizadeh, Huawei Noah’s Ark Lab (mehdi.rezagholizadeh@huawei.com).:::info
This paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.[1] The same settings as the original QLoRA work are applied here.\
[2] The maximum LoRA rank is fixed to 64. While QLoRA’s rank is always fixed, QDyLoRA can split the training across ranks in range 1 to 64.]]></content:encoded></item><item><title>Beyond Static Ranks: The Power of Dynamic Quantization in LLM Fine-Tuning</title><link>https://hackernoon.com/beyond-static-ranks-the-power-of-dynamic-quantization-in-llm-fine-tuning?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 1 Jul 2025 09:24:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Hossein Rajabzadeh, University of Waterloo and Huawei Noah’s Ark Lab (hossein.rajabzadeh@uwaterloo.ca);(2) Mojtaba Valipour, University of Waterloo (mojtaba.valipour@uwaterloo.ca);(3) Tianshu Zhu, Huawei Noah’s Ark Lab (tianshu.zhu@huawei.com);(4) Marzieh Tahaei, Huawei Noah’s Ark Lab (marzieh.tahaei@huawei.com);(5) Hyock Ju Kwon, (hjkwon@uwaterloo.ca);(6) Ali Ghodsi, (ali.ghodsi@uwaterloo.ca);(7) Boxing Chen, Huawei Noah’s Ark Lab (boxing.chen@huawei.com);(8) Mehdi Rezagholizadeh, Huawei Noah’s Ark Lab (mehdi.rezagholizadeh@huawei.com).\
A. Supplementary MaterialFinetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.The popularity of adopting Large Language Models (LLMs) across a diverse range of downstream tasks has rapidly increased over the past two years. Finetuning LLMs has become necessary to enhance their performance and introduce desired behaviors while preventing undesired outputs (Ding et al., 2023). However, as the size of these models increases, fine-tuning costs become more expensive. This has led to a large body of research that focuses on improving the efficiency of the fine-tuning stage (Liu et al., 2022; Mao et al., 2021; Hu et al., 2021; Edalati et al., 2022; Sung et al., 2022).\
Low-rank adapter (LoRA) (Hu et al., 2021) is a well-known, parameter-efficient tuning (PEFT) method that reduces memory requirements during fine-tuning by freezing the base model and updating a small set of trainable parameters in form of low-rank matrix multiplication added to matrices in the base model. However, the memory demand during fine-tuning remains substantial due to the necessity of a backward pass through the frozen base model during stochastic gradient descent.\
Recent research has thus focused on further reducing memory usage by designing new parameter-efficient modules that can be tuned without necessitating gradients from the base models (Sung et al., 2022). Alternatively, researchers have explored combining other efficiency strategies with parameter-efficient tuning methods (Kwon et al., 2022; Dettmers et al., 2023).\
Among these approaches, QLoRA (Dettmers et al., 2023) stands out as a recent and highly efficient fine-tuning method that dramatically decreases memory usage. It enables fine-tuning of a 65-billion-parameter model on a single 48GB GPU while maintaining full 16-bit fine-tuning performance. QLoRA achieves this by employing 4- bit NormalFloat (NF4), Double Quantization, and Paged Optimizers as well as LoRA modules.\
However, another significant challenge when utilizing LoRA modules is the need to tune their rank as a hyperparameter. Different tasks may require LoRA modules of varying ranks. In fact, it is evident from the experimental results in the LoRA paper that the performance of models varies a lot with different ranks, and there is no clear trend indicating the optimal rank. On the other hand, any hyperparameter tuning for finding the optimal rank contradicts the primary objective of efficient tuning and is not feasible for very large models. Moreover, when deploying a neural network on diverse devices with varying configurations, the use of higher ranks can become problematic for highly sensitive devices due to the increased parameter count. To address this, one typically has to choose between training multiple models tailored to different device configurations or determining the optimal rank for each device and task. However, this process is costly and time-consuming, even when using techniques like LoRA.\
DyLoRA (Valipour et al., 2022), is a recent PEFT method that aims to address these challenges by employing dynamic Low-Rank Adapter (DyLoRA). Inspired by nested dropout, this method aims to order the representations of the bottleneck at low-rank adapter modules. Instead of training LoRA blocks with a fixed rank, DyLoRA extends training to encompass a spectrum of ranks in a sorted manner. The resulting low-rank PEFT modules not only provide increased flexibility during inference, allowing for the selection of different ranks depending on the context, but also demonstrate superior performance compared to LoRA, all without imposing any additional training time.\
In this paper, we employ the DyLoRA PEFT method in conjunction with the quantization scheme utilized in the QLoRA work, resulting in QDyLoRA. QDyLoRA has all the aforementioned benefits of DyLoRA but with significant memory reduction both during training and at inference through 4-bit quantization. We utilize QDyLoRA for efficient fine-tuning of LLaMA-7b, LLaMA13b, and Falcon-40b models across ranks ranging from 1 to 64, all on a single 32GB V100 GPU. Once tuned, we determine the optimal rank by inferring the model on the test set. Our results reveal that the optimal rank can be quite low, yet it outperforms QLoRA. These methods aim to fine-tune pre-trained LLMs for specific tasks while minimizing computational and memory resources. Low-rank adaptation techniques were inspired by (Aghajanyan et al., 2020), demonstrating that pre-trained language models possess a low intrinsic dimension. Since then, several works have explored the incorporation of trainable parameters in the form of low-rank up-projection/down-projection during fine-tuning. In (Houlsby et al., 2019), the Adapter module includes a down projection, a non-linear function, an up projection, and a residual connection. These modules are sequentially inserted after the feed-forward network (FFN) or attention blocks.\
Additionally, (He et al., 2021) extends the Adapter concept by introducing trainable modules that run in parallel (PA) with the original pre-trained language model (PLM) module. As a result of this extension, PA has demonstrated improved performance compared to the original Adapter method. One notable approach among these techniques is LoRA (Hu et al., 2021), which introduces low-rank up-projection/down-projection into various matrices within a PLM. This method offers efficient inference by seamlessly integrating the adapter module into the original model’s weight matrices.\
Quantization-aware PEFT methods AlphaTuning (Kwon et al., 2022), aims to combine parameter-efficient adaptation and model compression. Alpha-Tuning achieves this by employing post-training quantization, which involves converting the pre-trained language model’s full-precision parameters into binary parameters and separate scaling factors. During adaptation, the binary values remain fixed for all tasks, while the scaling factors are fine-tuned for the specific downstream task.\
QLoRA (Dettmers et al., 2023) is a more recent quantization-aware PEFT that combines a low-rank adapter with 4-bit NormalFloat (NF4) quantization and Double Quantization (DQ) of the base model to optimize memory usage. NF4 ensures an optimal distribution of values in quantization bins, simplifying the process when input tensors have a fixed distribution. DQ further reduces memory overhead by quantizing quantization constants.\
To manage memory during gradient checkpointing, QLoRA employs Paged Optimizers, utilizing NVIDIA’s unified memory feature for efficient GPU memory management. These techniques collectively enable high-fidelity 4-bit fine-tuning while effectively handling memory constraints.\
 DyLoRA paper (Valipour et al., 2022) introduces a novel approach for training low-rank modules to work effectively across a range of ranks simultaneously, eliminating the need to train separate models for each rank.\
Inspired by the concept of nested dropout, the authors propose a method for organizing the representations within low-rank adapter modules. This approach aims to create dynamic low-rank adapters that can adapt well to various ranks, rather than being fixed to a single rank with a set training budget. This is achieved by dynamically selecting ranks during training, allowing for greater flexibility without the need for extensive rank searching and multiple model training sessions.:::info
This paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.]]></content:encoded></item><item><title>More Than a Feeling: Visualizing Why Filter Atoms Outsmart LoRA in Fine-Tuning</title><link>https://hackernoon.com/more-than-a-feeling-visualizing-why-filter-atoms-outsmart-lora-in-fine-tuning?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 1 Jul 2025 09:15:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[VTAB dataset is uniquely challenging and well-suited for the evaluation of parameter-efficient tuning methods in the context of few-shot knowledge transfer. VTAB-1k encompasses a diverse range of image domains, including natural, structured, and specialized categories such as medical or satellite imagery. The tasks span various objectives, comprising object and scene recognition, distance classification, and counting. Consequently, VTAB-1k emerges as a highly valuable resource catering to the needs of both discriminative and generative transfer learning tasks.\
In Table 5, we provide information on 19 tasks of the VTAB dataset, including the number of classes and the number of images in each data split of VTAB. Images in the VTAB benchmark encompass three distinct domains: (1) Natural images captured using standard cameras, (2) Specialized images captured using non-standard cameras like those in remote sensing and medical applications, and (3) Structured images generated through simulation environments.\
VTAB-1k is a subset of VTAB. It contains only 1000 training and validation samples, which are designed for few-shot transfer learning.7.2 Experimental Settings We adopt the LoRA implementation from https: //github.com/microsoft/LoRA.\
LoHa and LoKr Implementation We adopt the LoHa and LoKr implementation from https://github.com/KohakuBlueleaf/LyCORIS.\
DiffFit and BitFit Implementation We adopt the DiffFit and BitFit implementation from https://github.com/mkshing/DiffFit-pytorch.\
Stable diffusion checkpoints. The pre-trained checkpoint we choose for Stable Diffusion is stable-diffusion-v1-4, which can be found at https://huggingface. co/CompVis/stable-diffusion.\
Text prompts for the few-shot generative task. We use specific text prompts to train the Stable Diffusion or generate the images. We list the example prompts for each dataset as follows:\
– The \
– A \
– The \
– The \
– The \
– The \
– A \
– A peacock in front of the \
– The \
– \
– \
– \
– Georgia O’Keeffe style \
– a watercolor painting of the \
– The \
– The \
– The \
– The \
Text prompts for the full generative task. We use specific text prompts to train the Stable Diffusion or generate the images. We list the example prompts for each dataset as follows:\
– Caltech-101: This is a picture of accordion.\
– CIFAR-100: This is a picture of apple.\
– Clevr: This is a picture from CLEVR dataset.\
– Diabetic Retinopathy: This is a retina image with no diabetic retinopathy.\
– DMLab: This is a picture from DMLab dataset.\
– Dsprites: This is a picture from dSprites dataset.\
– DTD: This is a picture of banded texture.\
– EuroSAT: This is a satellite picture of annual crop.\
– Flowers102: This is a picture of pink primrose.\
– Kitti: This is a picture from KITTI dataset.\
– Patch Camelyon: This is a histopathologic scans without tumor.\
– Pet: This is a picture of Abyssinian cat.\
– Resisc45: This is a remote sensing picture of airplane.\
– Smallnorb: This is a picture from SmallNORB dataset.\
– SUN397: This is a picture of abbey.\
– SVHN: This is a picture of street view house number 0.8 Additional Experimental Results8.1 Validation ExperimentsWe provide additional experiments with m = 6, 12 in Figure 6. As we increase m from 6 to 12, the accuracy improves from 66.86% to 68.68%.8.2 Additional Experiments of Discriminative TasksPerformance Comparisons on Full Dataset Fine-tuning.\
 For CIFAR-100 and ImageNet-1K, we follow the finetuning setting of ConvNeXt in [30]. We employ the AdamW [33] optimizer to fine-tune models for 100 epochs for CIFAR-100, and 30 epochs for ImageNet1K. The cosine decay strategy is adopted for the learning rate schedule, and the linear warm-up is used in the first 10 epochs for CIFAR-100 and 5 epochs for ImageNet-1K.\
We compare the performance of our approach with other baseline methods, and the results on CIFAR-100 and ImageNet-1K are shown in Table 6. With full dataset fine-tuning, the full fine-tuning achieves the highest accuracy, outperforming the parameter-efficient fine-tuning methods. One possible reason is both datasets have sufficient data to prevent over-fitting of the model. Our method achieves a higher accuracy than LoRA while requiring only a small number of parameters (1.2M v.s. 21M). In contrast, in the VTAB-1k benchmark, the amount of data is not very large (e.g., only 1,000 training images), which might cause over-fitting of the model for the full fine-tuning.\
Visualization of Generalization Error. To delve deeper into how various fine-tuning methods impact the generalization capabilities of pre-trained models, we illustrate in Figure 7 the generalization error for a discriminative task trained on the CIFAR-100 and Diabetic Retinopathy datasets, in relation to the number of fine-tuned parameters.8.3 Results of Few-shot Generative TasksWe provide more experimental results of few-shot generative learning in Table. 7 and 8. In this experiment, we also include LoRA, LoHa, and LoKr with different configurations.\
The generated images of different fine-tuning methods are shown in Figure 8 and 9.8.4 Visualization of Generated ImagesWe visualize images generated by the models trained on each of VTAB tasks from Figure 10 to Figure 25.To understand the underlying reason for the effectiveness of our approach on convolution-based models, we employ Grad-CAM [9] on the first block of ResNet50, which are fine-tuned on the CUB dataset [67] using the same experimental setting as above. For our method, we compare the experiment setting with m = 9, which means 9 filter atoms ∆D and the setting with (m, m1) = (9, 4), which means 36 ∆D1.\
Based on the Grad-CAM visualization in Figure 26, our method exhibits larger active regions compared with LoRA. This observation indicates that our approach benefits from preserving the spatial structure of convolutional layers. When utilizing ∆D1, which expands the number of filter atoms, we observe more active regions in the Grad-CAM heatmap. This suggests that the introduction of extra filter atoms potentially captures a wider range of feature maps.\
We provide more heatmap visualizations of Grad-CAM from the first block of ResNet50 in Figure 27.(1) Wei Chen, Purdue University, IN, USA (chen2732@purdue.edu);(2) Zichen Miao, Purdue University, IN, USA (miaoz@purdue.edu);(3) Qiang Qiu, Purdue University, IN, USA (qqiu@purdue.edu).]]></content:encoded></item><item><title>Tuning the Pixels, Not the Soul: How Filter Atoms Remake ConvNets</title><link>https://hackernoon.com/tuning-the-pixels-not-the-soul-how-filter-atoms-remake-convnets?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 1 Jul 2025 09:09:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Beck, A., Teboulle, M.: A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences pp. 183–202 (2009) 4Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems (2020) 1Chen, S., Ge, C., Tong, Z., Wang, J., Song, Y., Wang, J., Luo, P.: Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems (2022) 1, 13Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances in neural information processing systems (2021) 14Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2020) 1, 13Edalati, A., Tahaei, M., Kobyzev, I., Nia, V.P., Clark, J.J., Rezagholizadeh, M.: Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650 (2022) 7Evgeniou, A., Pontil, M.: Multi-task feature learning. Advances in neural information processing systems (2007) 3Friedman, D., Dieng, A.B.: The vendi score: A diversity evaluation metric for machine learning. arXiv preprint arXiv:2210.02410 (2022) 10Gildenblat, J., contributors: Pytorch library for cam methods. https://github.com/jacobgil/pytorch-grad-cam (2021) 5Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., Xu, C.: Cmt: Convolutional neural networks meet vision transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2022) 14Hao, S., Zhou, Y., Guo, Y.: A brief survey on semantic segmentation with deep learning. Neurocomputing pp. 302–321 (2020) 14He, K., Girshick, R., Dollár, P.: Rethinking imagenet pre-training. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2019) 13He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition (2016) 1, 3, 6, 9, 13, 14, 12Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems (2017) 10Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for nlp. In: International Conference on Machine Learning (2019) 13Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. In: International Conference on Learning Representations (2021) 1, 2, 4, 10, 11, 12, 13Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition (2017) 13Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.: Visual prompt tuning. In: European Conference on Computer Vision (2022) 1, 13Karimi Mahabadi, R., Henderson, J., Ruder, S.: Compacter: Efficient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems (2021) 13Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (2020) 14Khan, A., Sohail, A., Zahoora, U., Qureshi, A.S.: A survey of the recent architectures of deep convolutional neural networks. Artificial intelligence review (2020)Kingma, D., Salimans, T., Poole, B., Ho, J.: Variational diffusion models. Advances in neural information processing systems (2021) 14Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International Conference on Learning Representations (2015) 9Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything. arXiv:2304.02643 (2023) 1Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images. Ph.D. thesis, University of Toronto (2009) 9Kumar, A., Daume III, H.: Learning task grouping and overlap in multi-task learning. International Conference on Machine Learning (2012) 3Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023) 10Li, X.L., Liang, P.: Prefix-tuning: Optimizing continuous prompts for generation. In: Proceedings of the Association for Computational Linguistics (2021) 13Li, Y., Gu, S., Gool, L.V., Timofte, R.: Learning filter basis for convolutional neural network compression. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2019) 2, 3Lian, D., Zhou, D., Feng, J., Wang, X.: Scaling & shifting your features: A new baseline for efficient model tuning. Advances in Neural Information Processing Systems (2022) 1, 13, 3Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision (2021) 13Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (2022) 3, 9, 14Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (2018) 10, 12, 3Mallat, S.G., Zhang, Z.: Matching pursuits with time-frequency dictionaries. IEEE Transactions on signal processing pp. 3397–3415 (1993) 4Maurer, A., Pontil, M., Romera-Paredes, B.: Sparse coding for multitask and transfer learning. In: International conference on machine learning (2013) 3Miao, Z., Wang, Z., Chen, W., Qiu, Q.: Continual learning with filter atom swapping. In: International Conference on Learning Representations (2021) 2, 3Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al.: Conditional image generation with pixelcnn decoders. Advances in neural information processing systems (2016) 14Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023) 10Padilla, R., Netto, S.L., Da Silva, E.A.: A survey on performance metrics for objectdetection algorithms. In: 2020 international conference on systems, signals and image processing (IWSSIP) (2020) 14Papyan, V., Romano, Y., Elad, M.: Convolutional neural networks analyzed via convolutional sparse coding. The Journal of Machine Learning Research 18, 2887– 2938 (2017) 2, 3Parisi, G.I., Kemker, R., Part, J.L., Kanan, C., Wermter, S.: Continual lifelong learning with neural networks: A review. Neural Networks (2019) 2Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.: Image transformer. In: International conference on machine learning (2018) 14Qiu, Q., Cheng, X., Sapiro, G., Calderbank, R.: DCFNet: Deep neural network with decomposed convolutional filters. In: International Conference on Machine Learning (2018) 2, 3Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning (2021) 10Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners 1Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research (2020) 1, 13Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., Dosovitskiy, A.: Do vision transformers see like convolutional neural networks? Advances in Neural Information Processing Systems (2021) 14Razavi, A., Van den Oord, A., Vinyals, O.: Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems (2019) 14Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (2022) 1, 3, 9, 11, 14, 6, 7Romera-Paredes, B., Aung, H., Bianchi-Berthouze, N., Pontil, M.: Multilinear multitask learning. In: International Conference on Machine Learning (2013) 3Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) (2015) 1, 9, 13Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. NIPS Deep Learning Symposium (2016) 2Santosa, F., Symes, W.W.: Linear inversion of band-limited reflection seismograms. SIAM journal on scientific and statistical computing pp. 1307–1330 (1986) 4Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open largescale dataset for training next generation image-text models. Advances in Neural Information Processing Systems (2022)Shen, Z., Liu, Z., Qin, J., Savvides, M., Cheng, K.T.: Partial is better than all: revisiting fine-tuning strategy for few-shot learning. In: Proceedings of the AAAI Conference on Artificial Intelligence (2021) 1Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al.: Mastering the game of go with deep neural networks and tree search. nature (2016) 1Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: International Conference on Learning Representations (2015) 14Sohn, K., Chang, H., Lezama, J., Polania, L., Zhang, H., Hao, Y., Essa, I., Jiang, L.: Visual prompt tuning for generative transfer learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023) 13Song, Y., Durkan, C., Murray, I., Ermon, S.: Maximum likelihood training of scorebased diffusion models. Advances in Neural Information Processing Systems (2021) 14Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: International conference on machine learning (2019) 13Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.: Training data-efficient image transformers & distillation through attention. In: International conference on machine learning (2021) 13Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023) 1Vahdat, A., Kautz, J.: Nvae: A deep hierarchical variational autoencoder. Advances in neural information processing systems (2020) 14Van Den Oord, A., Kalchbrenner, N., Kavukcuoglu, K.: Pixel recurrent neural networks. In: International conference on machine learning (2016) 14Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. Advances in neural information processing systems (2017) 14Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems (2017) 1Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: Cub-200-2011. Tech. Rep. CNS-TR-2011-001, California Institute of Technology (2011) 5, 12Wang, Z., Chen, J., Hoi, S.C.: Deep learning for image super-resolution: A survey. IEEE transactions on pattern analysis and machine intelligence (2020) 14Xie, E., Yao, L., Shi, H., Liu, Z., Zhou, D., Liu, Z., Li, J., Li, Z.: Difffit: Unlocking transferability of large diffusion models via simple parameter-efficient fine-tuning. arXiv preprint arXiv:2304.06648 (2023) 10, 11, 12Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition (2017) 13YEH, S.Y., Hsieh, Y.G., Gao, Z., Yang, B.B., Oh, G., Gong, Y.: Navigating textto-image customization: From lycoris fine-tuning to model evaluation. In: International Conference on Learning Representations (2023) 1, 7, 10, 11, 12Yoon, J., Kim, S., Yang, E., Hwang, S.J.: Scalable and order-robust continual learning with additive parameter decomposition. In: International Conference on Learning Representations (2019) 2Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., Yan, S.: Metaformer is actually what you need for vision. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (2022) 13Zaken, E.B., Goldberg, Y., Ravfogel, S.: Bitfit: Simple parameter-efficient finetuning for transformer-based masked language-models. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (2022) 1, 10, 11, 12, 13Zhai, M., Chen, L., Mori, G.: Hyper-lifelonggan: Scalable lifelong learning for image conditioned generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021) 2Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djolonga, J., Pinto, A.S., Neumann, M., Dosovitskiy, A., et al.: A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867 (2019) 8, 10, 12Zhang, Y., Yang, Q.: A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering (2021) 3Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. International Journal of Computer Vision (2022) 13Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., Fidler, S.: Aligning books and movies: Towards story-like visualexplanations by watching movies and reading books. In: Proceedings of the IEEE international conference on computer vision (2015) 1, 13Zou, Z., Chen, K., Shi, Z., Guo, Y., Ye, J.: Object detection in 20 years: A survey. Proceedings of the IEEE (2023) 14]]></content:encoded></item><item><title>Keep the Channel, Change the Filter: A Smarter Way to Fine-Tune AI Models</title><link>https://hackernoon.com/keep-the-channel-change-the-filter-a-smarter-way-to-fine-tune-ai-models-ug9q2k4?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 1 Jul 2025 09:05:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In this section, we decompose convolution filters over a small set of filter subspace elements, referred to as  This formulation enables a new model tuning method via filter subspace by solely adjusting filter atoms.Our approach involves decomposing each convolutional layer F into two standard convolutional layers: a filter atom layer D that models filter subspace[1 , and an atom coefficient layer α with 1 × 1 filters that represent combination rules of filter atoms, as displayed in Figure 2 (a). This formulation is written as\
• Spatial-only Convolution with D. Each channel of the input features X convolves with each filter atom separately to produce intermediate features\
This process leads to the generation of m distinct intermediate output channels for each input channel, which is illustrated in Figure 2 (b). In this step, filter atoms focus only on handling the spatial information of input features, and cross-channel mixing is postponed to the next step.\
• Cross-channel Mixing with α. Subsequently, atom coefficients weigh and linearly combine the intermediate features to produce output features\
The spatially invariant channel weights, atom coefficients , serve as operators for channel mixing, functioning as distinct combination rules that construct the output features from the elemental feature maps generated by the filter atoms. During the model tuning,  is obtained from the pre-trained model and remains unchanged, while only filter atoms  adapt to the target task.\
Summary. The two-step convolution operation explains different functionalities of filter atoms  and atom coefficients α, which is,  only contribute to spatial convolution and  only perform cross-channel mixing. In practice, the convolution operation is still performed as one layer, without generating intermediate features, to avoid memory cost. In the fine-tuning process, we solely adjust , which contains a small set of number of parameters, k × k ≪ c′ × c, thereby facilitating parameter-efficient fine-tuning.The parameters of filter atoms are extremely small compared with overall model parameters. For instance, the filter atoms constitute a mere 0.004% of the total parameters in ResNet50 [13]. To fully explore the potential of filter subspace fine-tuning, we show next a simple way to construct a set of overcomplete[2] filter atoms by recursively applying the above decomposition to each filter atom, to expand the parameter space for fine-tuning as needed.3.4 Parameter Efficient Fine-tuningIn this section, we begin with studying the effectiveness of our method across various configurations to determine the most suitable application scenario for each configuration. Subsequently, we demonstrate that fine-tuning only filter atoms requires far fewer parameters while preserving the capacity of pre-trained models, compared with baseline methods in the contexts of discriminative and generative tasks.4.1 Experimental Settings. Our experimental evaluations are mainly conducted on the Visual Task Adaptation Benchmark (VTAB) [76], which contains 19 distinct visual recognition tasks sourced from 16 datasets. As a subset of VTAB, VTAB-1k comprises a mere 1, 000 labeled training examples in each dataset.\
. For the validation experiment, we choose ResNet50 [13] pre-trained on ImageNet-1K. For discriminative tasks, we choose the convolution-based model, ConvNeXt-B [32] pre-trained on ImageNet-21K as the initialization for finetuning. For generative tasks, we choose Stable Diffusion [49] which contains a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 as text encoder for the diffusion model. The model is pre-trained on the LAION dataset [54], which contains over 5 billion image-text pairs. More details of pretrained models are listed in the Appendix 7.4.2 Validation ExperimentsIn this section, we study the performance of our approach across various configurations.In this section, we apply our method to the generative task by evaluating the generative samples from VTAB dataset [76].\
. We compare our method to 6 baseline fine-tuning approaches: (i) Full fine-tuning, which entails updating all model parameters during the finetuning process; (ii) LoRA [16], involving the introduction of a low-rank structure of accumulated gradient update by decomposing it as up-projection and downprojection. (iii) LoHa [71] utilizes the Hadamard product across two sets of lowrank decompositions to elevate the rank of the resultant matrix and reduce the approximation error. (iv) LoKr [71] introduces the Kronecker product for matrix decomposition to reduce the tunable parameters. (v) BitFit [74] fine-tunes the bias term of each layer. (vi) DiffFit [69] fine-tunes the bias term, as well as the layer norm and the scale factor of each layer.\
\
Methods like LoRA [16] or full fine-tuning potentially update these α, thus, they lead to lower diversity and text-to-image alignment in generated images. In contrast, BitFit [74] and DiffFit [69] mostly fine-tune the bias, leaving α fixed, thus, they have a higher diversity and text-to-image alignment than LoRA. However, they also keep the spatial operation D unchanged, resulting in a lower fidelity score compared with C2. More results can be found in Appendix 8.\
\
Performance comparisons on generative transfer learning. We report FIDs of models trained and evaluated on VTAB tasks in Table 3. In contrast to full parameter fine-tuning and LoRA, our approach attains the lowest FID scores (54.7 v.s. 57.5) while employing the least number of fine-tuning parameters (1.11M v.s. 22.67M). Despite fine-tuning only 0.13% of the total model parameters, our method effectively tailors pre-trained Stable Diffusion to align it with the desired target distribution.In this section, we apply our method to the discriminative task, namely the classification on VTAB-1k [76]. We compare our method to 4 baseline fine-tuning approaches: (i) Full fine-tuning, (ii) Linear probing, (iii) BitFit [74], and (iv) LoRA [16].\
 Images are resized to 224 × 224, following the default settings in VTAB [76]. We employ the AdamW [33] optimizer to fine-tune models for 100 epochs. The cosine decay strategy is adopted for the learning rate schedule, and the linear warm-up is used in the first 10 epochs.\
In this experiment, we fine-tune D and Dc while keeping α and αc fixed, as this configuration delivers adequate accuracy without increasing parameters.\
Performance comparisons on few-shot transfer learning. We compare the performance of our approach and other baseline methods, and the results\
\
on VTAB-1k are shown in Table 4. In these tables, the bold font shows the best accuracy of all methods and the underlined font shows the second best accuracy. Our method outperforms other parameter-efficient fine-tuning methods and even outperforms full fine-tuning. Specifically, our method obtains 6% improvement in accuracy compared to LoRA on the VTAB-1k benchmark while utilizing significantly fewer trainable parameters (0.45M v.s. 17.4M).\
The generalization error is measured by the discrepancy between the training loss and the test loss, with the findings illustrated in Table 4 and Figure 7 from Appendix 8. Our technique, which employs fixed atom coefficients, leads to a comparatively lower generalization error. It means our tuning method better preserves the generalization ability of the pre-trained model.(1) Wei Chen, Purdue University, IN, USA (chen2732@purdue.edu);(2) Zichen Miao, Purdue University, IN, USA (miaoz@purdue.edu);(3) Qiang Qiu, Purdue University, IN, USA (qqiu@purdue.edu).[1] The filter subspace is a span of m filter atoms D.]]></content:encoded></item><item><title>Keep the Channel, Change the Filter: A Smarter Way to Fine-Tune AI Models</title><link>https://hackernoon.com/keep-the-channel-change-the-filter-a-smarter-way-to-fine-tune-ai-models?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 1 Jul 2025 09:01:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Wei Chen, Purdue University, IN, USA (chen2732@purdue.edu);(2) Zichen Miao, Purdue University, IN, USA (miaoz@purdue.edu);(3) Qiang Qiu, Purdue University, IN, USA (qqiu@purdue.edu).\
. Efficient fine-tuning methods are critical to address the high computational and parameter complexity while adapting large pre-trained models to downstream tasks. Our study is inspired by prior research that represents each convolution filter as a linear combination of a small set of filter subspace elements, referred to as filter atoms. In this paper, we propose to fine-tune pre-trained models by adjusting only filter atoms, which are responsible for spatial-only convolution, while preserving spatially invariant channel combination knowledge in atom coefficients. In this way, we bring a new filter subspace view for model tuning. Furthermore, each filter atom can be recursively decomposed as a combination of another set of atoms, which naturally expands the number of tunable parameters in the filter subspace. By only adapting filter atoms constructed by a small number of parameters, while maintaining the rest of model parameters constant, the proposed approach is highly parameter-efficient. It effectively preserves the capabilities of pre-trained models and prevents overfitting to downstream tasks. Extensive experiments show that such a simple scheme surpasses previous tuning baselines for both discriminate and generative tasks.Large models have demonstrated exceptional performance across diverse domains and tasks [2, 5, 13, 24, 45, 49, 56, 62, 66], attributing to their capability to effectively represent complex patterns and relationships [21] by pre-training on massive datasets [46 , 51 , 79]. A common strategy to adapt these large models for specific downstream tasks is fine-tuning them with full parameters. But this method presents two main challenges: (1) Adjusting a vast number of parameters for particular target tasks is computationally intensive; (2) The limited availability of target data increases the risk of overfitting [30].\
To address these challenges, researchers have developed parameter-efficient methods [ 3 , 16 , 18 , 55 , 71 , 74] by fine-tuning the pre-trained models with only a minimal number of parameters. Among these methods, LoRA [16] fine-tunes models without altering the model architecture, becoming notably popular for\
its simplicity and efficacy. However, LoRA still risks overfitting when fine-tuned on limited data and compromising the generalization capability of large models. For instance, Figure 1 illustrates that with only 5 training samples, LoRA tends to produce images that closely resemble the training data, compromising the ability for diverse image generation, compared with pre-trained models.\
. To preserve the capabilities of pre-trained models when finetuning them on the downstream tasks, one prominent approach in continual learning [41, 52, 72] is to formulate convolution filters in ConvNets as a linear combination of filter atoms [29, 40, 43] and fine-tuning only filter atoms [36, 75]. Specifically, filters in each convolutional layer are decomposed over a small set of filter subspace elements, referred to as filter atoms, responsible for spatial-only convolution. Each convolutional layer is now constructed as linear combinations of filter atoms using decomposition coefficients, referred to as atom coefficients, responsible for the spatially invariant channel combination. Hypothesizing variations across tasks can be reduced by bridging spatial discrepancies in the images, we propose to calibrate the pre-trained model by solely fine-tuning the spatial-only filter atoms while preserving the spatially-invariant channel weights, i.e., atom coefficients.\
In our work, we demonstrate that fine-tuning a large model via filter atoms is substantially effective and parameter-efficient, as filter atoms are responsible for spatial-only convolution and usually comprise only a few hundred parameters. This strategy is in harmony with task subspace modeling principles, which\
suggest that task parameters occupy a low-dimensional subspace, allowing tasks to be represented as combinations of latent basis tasks [7,26,35,50,77]. We also discover that maintaining fixed atom coefficients, i.e., spatially-invariant channel mixing weights, plays a crucial role in preserving the generalization capability of pre-trained large models.\
With a large number of parameters fixed, fine-tuning only a tiny set of parameters in filter atoms is potentially challenging to adapt to more complex tasks. We further demonstrate a simple yet effective way to expand the tunable parameters in filter subspace, without any modification on atom coefficients, by decomposing each filter atom over another set of filter atoms. This process provides an overcomplete set of filter atoms and expands the tunable parameter space, all while still requiring fewer parameters than LoRA. Additionally, we provide a simple technique to extend this method to linear layers, ensuring alignment with the characteristics in prior literature [29, 36, 40, 43]. The illustration of our method is displayed in Figure 2.\
We demonstrate the effectiveness of our approach on both discriminative and generative tasks with ResNet50 [13], ConvNeXt [32] and Stable Diffusion [49]. We summarize our contributions as follows,\
– We propose a method by adapting only filter subspace elements (filter atoms), with a few hundred parameters, to achieve significantly parameter-efficient fine-tuning.\
– We observe that maintaining fixed atom coefficients plays a crucial role in preserving the generalization capability of large models.\
– We further demonstrate a simple way to expand the number of tunable parameters in filter subspace by recursively decomposing each filter atom over another set of filter atoms, which extends the parameter space for tuning.\
– We conduct extensive experiments demonstrating the efficacy of our approach on discriminative and generative tasks for fine-tuning large models.2.1 Low-rank Adaptation for Fine-tuning2.2 Sparse Coding and Matrix Factorization]]></content:encoded></item><item><title>Anthropic Admits to Copying Books en masse for Claude—Can Fair Use Save It?</title><link>https://hackernoon.com/anthropic-admits-to-copying-books-en-masse-for-claudecan-fair-use-save-it?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Tue, 1 Jul 2025 09:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 3 of 10. Each work selected for training any given LLM was copied in four main ways — and in fact so many times that Anthropic admits it would be impractical even to estimate. , each work selected was copied from the central library to create a working copy for the training set. , each work was cleaned to remove a small amount of lower-valued or repeating text (like headers, footers, or page numbers), with a “cleaned” copy resulting. If the same book appeared twice, or if while looking across the entire provisional training set it became clear there was some other reason to cull a book or category, Anthropic had the capability to delete relevant copy(ies) from the set at this step (see CC Br. Expert Zhao ¶¶ 71–72). , each cleaned copy was translated into a “tokenized” copy. Some words were “stemmed” or “lemmatized” into simpler forms (e.g., “studying” to “study”). And, all characters were grouped into short sequences and translated into corresponding number sequences or “tokens” according to an Anthropic-made dictionary. The resulting tokenized copies were then copied repeatedly during training. By one account, this process involved the iterative, trial-and-error discovery of contingent statistical relationships between each word fragment and all other word fragments both within any work and across trillions of word fragments from other copied books, copied websites, and the like. Other steps in training are not at issue here (id. ¶¶ 73–76; see Opp. Expert Zhao ¶ 38 & n.6)., each fully trained LLM itself retained “compressed” copies of the works it had trained upon, or so Authors contend and this order takes for granted. In essence, each LLM’s mapping of contingent relationships was so complete it mapped or indeed simply “memorized” the works it trained upon almost verbatim. So, if each completed LLM had been asked to recite works it had trained upon, it could have done so (e.g., Opp. Expert Zhao ¶ 74). Further steps refining the LLM are not at issue here.However, that was as far as the training copies propagated towards the outside world. When each LLM was put into a public-facing version of Claude, it was complemented by other software that filtered user inputs to the LLM and filtered outputs from the LLM back to the user (id. ¶¶ 75–77). As a result, Authors do not allege that any infringing copy of their works was or would ever be provided to users by the Claude service. Yes, Claude could help less capable writers create works as well-written as Authors’ and competing in the same categories. But Claude created no exact copy, nor any substantial knock-off. Nothing traceable to Authors’ works. Such allegations are simply not part of plaintiffs’ amended complaint, nor in our record.Neither side puts directly at issue any copies of any works that might have been used for the filtering software. Nor will this order. In sum, the copies of books pirated or purchased-and-destructively-scanned were placed into a central “research library” or “generalized data area,” sets or subsets were copied again to create training copies for data mixes, the training copies were successively copied to be cleaned, tokenized, and compressed into any given trained LLM, and once trained an LLM did not output through Claude to the public any further copies. Finally, once Anthropic decided a copy of a pirated or scanned book in the library would not be used for training at all or ever again, Anthropic still retained that work as a “hard resource” for other uses or future uses. At least one work from each Author was present in every phase described above.In August 2024, the three individual authors brought this putative class action complaining that Anthropic had infringed its federal copyrights by pirating copies for its library and by reproducing them to train its LLMs (Compl. ¶¶ 45–46, 71; see Amd. Compl. ¶¶ 47–48, 75). In October 2024, a scheduling order required that any motion for class certification be brought by March 6, 2025 (Dkt. No. 49). The individual authors soon amended their complaint to include affiliated corporate entities as named plaintiffs, with consent. And, Anthropic chose not to move to dismiss the amended complaint, as it earlier had planned (see Dkt. No. 37). Instead, Anthropic moved to allow an early motion for summary judgment on fair use, even before class certification (Dkt. No. 88; see Feb. 25, 2025 Tr. 15). Permission was granted.Anthropic now moves for summary judgment on fair use only. Fair use is a legal question for the judge with underlying fact questions, if any, for the jury. To prevail on summary judgment, Anthropic must rely on undisputed facts and/or factual inferences favoring the opposing side. Anthropic thus bears the burdens of production and persuasion in this motion. See Google LLC v. Oracle Am., Inc., 593 U.S. 1, 23–24 (2021); Andy Warhol Found. for the Visual Arts, Inc. v. Goldsmith, 598 U.S. 508, 547 n.21 (2023); Campbell v. Acuff-Rose Music, Inc., 510 U.S. 569, 590 & n.20, 594 (1994); see also Nissan Fire & Marine Ins. Co. v. Fritz Cos., 210 F.3d 1099, 1102–03 (9th Cir. 2000). Notably, in its motion, Anthropic argues that pirating initial copies of Authors’ books and millions of other books was justified because all those copies were at least reasonably necessary for training LLMs — and yet Anthropic has resisted putting into the record what copies or even sets of copies were in fact used for training LLMs. For example, at oral argument, Anthropic asserted that if a purported fair user had retained pirated copies for uses beyond the fair use, then her piracy would not be excused by the fair use (Tr. 53, 56). But when Authors earlier interrogated Anthropic in discovery about what library copies (the original copies “obtained or created” by Anthropic) Anthropic had recopied for further uses, Anthropic responded that providing information about any copies made for uses beyond training commercially released LLMs would be overbroad, and that it could not count up all its copying even for LLMs in any case (e.g., Opp Exh. 30 at 3). We know that Anthropic has more information about what it in fact copied for training LLMs (or not). Anthropic earlier produced a spreadsheet that showed the composition of various data mixes used for training various LLMs — yet it clawed back that spreadsheet in April (Opp. Fredricks Decl. ¶¶ 2–3). A discovery dispute regarding that spreadsheet remains pending. But Anthropic did not need a court order to offer up what it possessed in support of its motion. All deficiencies must be held against Anthropic and not the other way around. This is the first substantive order in this case. A contemporaneous motion for class certification remains pending. It proposes one class related to works that were pirated (whether or not used to train LLMs), and a second class related to works that were purchased, scanned, and used in training LLMs. This order follows full briefing, a hearing, and supplemental briefing.To summarize the analysis that now follows, the use of the books at issue to train Claude and its precursors was exceedingly transformative and was a fair use under Section 107 of the Copyright Act. And, the digitization of the books purchased in print form by Anthropic was also a fair use but not for the same reason as applies to the training copies. Instead, it was a fair use because all Anthropic did was replace the print copies it had purchased for its central library with more convenient space-saving and searchable digital copies for its central library — without adding new copies, creating new works, or redistributing existing copies. However, Anthropic had no entitlement to use pirated copies for its central library. Creating a permanent, general-purpose library was not itself a fair use excusing Anthropic’s piracy.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>Meet the HackerNoon Top Writers - Vladislav Guzey on Writing and Creativity</title><link>https://hackernoon.com/meet-the-hackernoon-top-writers-vladislav-guzey-on-writing-and-creativity?source=rss</link><author>HackerNoon Writers Spotlight</author><category>tech</category><pubDate>Tue, 1 Jul 2025 08:49:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Hello! I’m , known online as . I bring  of experience in tech, holding roles such as Web Developer, Data Director, Growth Hacker, AI Enthusiast, and Educator. My expertise lies in web development, AI, and sharing knowledge through practical tutorials and engaging content. You can find my work on my website proflead.dev, and my YouTube channel.How Did You Start Writing?My journey into writing began because I wanted to share my experiences and discoveries in web development and AI. Blogging was a natural fit because it allowed me to express my thoughts clearly, help others solve real-world problems, and engage with a broader community of learners and professionals. Seeing people benefit from my articles keeps my passion alive.How Has Technology Impacted The Way You Write?Technology has completely transformed my writing process. Tools like AI writing assistants, analytics, and platforms like HackerNoon have streamlined my workflow and improved my content quality. I rely on tech to research effectively, organize my ideas, and ensure my articles are engaging and accessible.Share About Your Journey Highlights?One of my proudest moments was being recognized as a Top Writer on HackerNoon, with my stories collectively read for nearly 3 months. This honor gave me a confidence boost and reaffirmed that consistency truly pays off in writing.\
Early on, though, it wasn’t easy. I vividly remember publishing articles that barely received any reads. That taught me an essential lesson: every setback is a setup for growth. I learned to be patient, to persevere, and to always find ways to refine my content.\
Along the way, my passion for learning fueled a shift in my mindset. I realized that teaching others helps me learn even deeper. Sharing what I uncover, not just the wins but the challenges, has helped me understand topics more fully and connect with readers in a meaningful way.What’s Your Creative Process?My creative process is quite simple. I start by identifying a problem or topic that resonates with my audience and current trend. Then, I do thorough research, structure my ideas clearly, and draft my articles in easy-to-understand language. I typically write in the evening after work, and I constantly revise my work multiple times to ensure clarity and impact.Your Favorite Memory/Article(s) to This Day?One article I'm particularly proud of is my story on "How I Replaced Gaming with Coding and Became a Web Developer." It resonates deeply with my personal journey and has inspired many readers to pursue coding seriously. The feedback I received from that article still motivates me today.How Did You Hear About HackerNoon? Share With Us About Your Experience With HackerNoon.I first discovered HackerNoon through a Google search. Whenever I looked up technical topics, HackerNoon consistently appeared. I saw that it’s a community-driven platform with over 45,000 active writers and 4 million monthly readers, built for technologists to read, write, learn, and publish.\
What really stood out was how supportive and focused the community is. The content isn’t just informative; it gets smartly distributed through AI-driven tools and editorial support, helping quality stories reach the right audience.\
So I decided to give it a shot, and I’m glad I did. HackerNoon gave me access to a global audience and real visibility. The boost I received, both in reach and engagement, continually motivates me to refine my writing and explore new topics.What Have You Learnt From Your Journey?The biggest lesson I've learned is that consistency, passion, and the willingness to learn are keys to success. Writing is not just about sharing knowledge, but also about engaging authentically with your readers. It’s essential to remain open-minded and always strive to deliver value through every piece of content.I'm honored to be part of the HackerNoon community and excited to continue contributing, learning, and growing with fellow writers. Thank you for providing this wonderful platform and community!Check out Vladislav Guzey’s HackerNoon profile here, and read more of his amazing stories!:::info
HackerNoon Writers Spotlight is a special, exclusive interview series where HackerNoon’s Top Writers share about their writing journey on the net in this ever-evolving digital world.]]></content:encoded></item><item><title>Hyra Network Honored As &quot;Technology Startup Of The Year&quot; At The 2025 Globee® Awards</title><link>https://hackernoon.com/hyra-network-honored-as-technology-startup-of-the-year-at-the-2025-globeer-awards?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 08:44:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Dubai, United Arab Emirates, July 1st, 2025/Chainwire/--Decentralized AI Framework Gains Recognition for Expanding Access to Compute Power.The digital economy has witnessed transformative platforms that fundamentally changed resource sharing: Grab revolutionized transportation, Airbnb transformed hospitality, and Shein disrupted supply chains. Now, a Vietnamese technology company is redefining the next frontier-computational power sharing itself. has been officially named "Technology Startup of the Year" at the prestigious 2025 Globee Awards for Technology, marking a watershed moment for decentralized artificial intelligence infrastructure.This recognition validates an ambitious vision that could reshape how the world builds, owns, and benefits from AI technology. (Official winner list available at: )The Vision Behind InnovationThis breakthrough platform is researched and developed by Hyra Tek JCS (Vietnam) and operated by Hyra Tek Smart Solution L.L.C (UAE). Hyra Network's mission is to democratize computational power by activating billions of idle devices and transforming everyday users into AI infrastructure providers.Hyra Network serves as the flagship platform alongside Hyra AI, creating an unprecedented model where computational resources are shared across distributed networks rather than concentrated in centralized data centers.If ride-sharing optimizes vehicle utilization and home-sharing maximizes property efficiency, then computational sharing unlocks vast processing power lying dormant in smartphones, computers, and IoT devices globally.Global Recognition for ExcellenceThe Globee Awards represent the technology sector's highest honor, with winners selected by over 100 seasoned professionals, including C-suite executives, venture capitalists, and industry analysts."This honor transcends our company - it validates the entire movement toward democratized AI infrastructure," said Mr. Jonh Tran, Founder of Hyra Network."We're witnessing global acknowledgment that the future of AI belongs not to centralized monopolies, but to communities that collectively own and benefit from these powerful technologies."The decentralized AI infrastructure and compute resource-sharing model of Hyra NetworkAt its core, Hyra Tek's innovation centers on Hyra AI, one of the world's first Train-to-Earn platforms. This system allows users to convert personal devices into active AI training nodes, earning rewards while contributing to advanced model development.The Layer-3 blockchain architecture supports high-throughput, low-latency workloads, enabling scalable AI training and inference at the network's edge.The economic model creates a virtuous cycle: participants provide computational resources, earn tangible rewards, and simultaneously advance AI capabilities that benefit the broader ecosystem.Today, the Hyra ecosystem spans more than 205 countries, powering a global network of approximately 2.5 million connected devices, including 700,000 active online nodes and over 1 million KYC-verified users.With strong community engagement across Southeast Asia, Latin America, and Africa, Hyra delivers more than 360,000 teraflops of distributed computing power and supports a growing base of enterprise clients, now serving over 10 paying customers.This real-world adoption reaffirms Hyra’s core belief: that distributed, permissionless infrastructure can drive meaningful innovation while remaining truly open and accessible to all.As artificial intelligence and DePIN technologies gain momentum, Hyra positions itself at the technological vanguard, architecting the foundation for a more equitable, intelligent, and inclusive AI future. is a decentralized artificial intelligence infrastructure platform developed by Hyra Tek JCS (Vietnam) and operated by Hyra Tek Smart Solution L.L.C (UAE).The platform enables individuals and enterprises to contribute unused computational resources from personal devices to support AI training and inference at scale. Leveraging a Layer-3 blockchain architecture, Hyra Network supports high-throughput, low-latency workloads across a global network of connected devices. Its ecosystem includes Hyra AI, one of the first Train-to-Earn platforms, and currently spans over 205 countries with 2.5 million connected devices. Hyra Network aims to increase accessibility to AI infrastructure through a community-powered, distributed model.Hyra Tek Smart Solution L.L.C (UAE):::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Shheikh.io Launches SHHEIKH Token Presale For Blockchain-Backed Real‑World Asset Investments</title><link>https://hackernoon.com/shheikhio-launches-shheikh-token-presale-for-blockchain-backed-realworld-asset-investments?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 08:05:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Zurich, Switzerland, June 30th, 2025/Chainwire/-- Introduces Tokenization Platform for Real-World Luxury Assets, Including Properties in Dubai, Lisbon, Rome, and Bali, as Well as High-End Vehicles and Farmland., the world’s first Ethereum‑based token powered by AI‑driven property intelligence that allows Real World Asset ownership, today opens its limited period presale., an AI-powered Web3 platform, has announced the launch of its marketplace for fractional ownership of luxury real estate and other high-value assets. The platform aims to facilitate access to tokenized real-world assets through blockchain-based infrastructure.“We're removing wealth barriers and opening access to premium markets,” the Shheikh.io team said. “Luxury ownership is no longer reserved for the top 1% — it’s becoming an investment opportunity for the 99%.” The launch marks the introduction of a platform designed to support decentralized access to tokenized real-world assets. Whether users are a digital nomad staking rental yields or a DeFi whale governing a villa syndicate, SHHEIKH puts you in command. With a minimum entry point of just $100, users can now invest in high-end properties, collectible vehicles, fine art, and more, all backed by real assets and blockchain automation. With SHHEIKH, real estate is finally borderless and on-chain. By harnessing predictive yield scoring and automated KYC/AML,  shatters the barriers to global real estate investment—making luxury assets accessible to every crypto enthusiast. Using blockchain-based tokenization, Shheikh.io allows users to invest and own global luxury properties (including real-estate, cars, rare artworks, and more) by enabling fractional ownership, AI-powered transactions, and an inclusive, borderless property economy.  is available to buy at $0.0027 USDT in Phase 1. Each SHHEIKH token (ERC-20) represents a verified share of a physical asset and entitles holders to passive income from rent or capital appreciation, distributed automatically via smart contracts.  The market for tokenized real-world assets (RWA) has surged by over 260% in 2025, reaching $23.9 billion in total value by mid-June, according to Forbes and Cointelegraph. Analysts at BCG estimate that the RWA market could grow to $16 trillion by 2030 — 10% of global GDP. Shheikh.io is positioning itself at the forefront of this transition. The platform integrates a proprietary AI engine that analyses asset performance and risk in real time, projects future yield and market value, rebalances portfolios based on macroeconomic signals, and automatically distributes revenue in stablecoins or ETH.● Total Supply: 50 Billion | 10,000,000,000 SHHEIKH available in presale (20%) ● ICO Pricing Begins: $0.0027 ● Accepted currencies: ETH, USDT, BNB ● Minimum investment: 0.0004 ETH / 0.0015 BNB ● Bonus and Referrals: Users can earn additional 5% bonus and 5% referral reward on every transaction. Further 5% bonus will be rewarded every month on SHHEIKH holdings. ● Assets: From real estate of any kind — villas, commercial spaces, or warehouses — to vehicles including high-end cars, commercial fleets, and passenger transport. Plus: collectibles, fine art, intellectual property, and beyond. ● Payouts: Quarterly, in stablecoins or ETH ● SHHEIKH No‑Code Builder: Tokenizing any property in minutes via an intuitive UI—no Solidity coding required. ● SHHEIKH DeFi: A powerful decentralized solution for cryptocurrency trading, enabling users to effortlessly swap crypto across multiple networks. ● SHHEIKH Estate: Investing in real estate with SHHEIKH, co-owning real-world properties● SHHEIKH Returns Maximiser: Multi-chain AI-powered returns optimizer that allows users to earn compound bonus on their crypto deposits. SHHEIKH Apart Key Features● AI‑Powered Analytics: Machine‑learning models forecast rental yields, asset appreciation, and risk scores—so you invest with confidence. ● AI-Powered Portfolio Optimizer: Features AI-assisted portfolio optimization that support dynamic asset allocation, predictive asset analytics, asset appreciation forecasts, and risk scoring● AI-Powered Investment: The platform incorporates AI as a foundational component of its investment infrastructure enabling more intelligent, data-informed participation in the tokenized real estate economy ● NFT‑Backed Property Deeds: Each SHHEIKH token links to an on‑chain NFT representing your legal deed. ● DAO‑Governed Syndicates: Pool SHH to co‑invest in premium real estate, from beachfront resorts to modular housing. ● Multichain Vision: Launching on Ethereum today—with Layer‑2 and cross‑chain expansion on the roadmap. ● KYC/AML in 15+ jurisdictions ● Compatible with MetaMask, Trust Wallet & Ledger ● Non-custodial architecture ensures user control Shheikh.io is committed to global accessibility, with a special focus on users from Latin America, Africa, Southeast Asia, and Eastern Europe — regions traditionally excluded from high-end investment opportunities.  is a next-generation Web3 platform for tokenizing real-world luxury assets. By combining blockchain, AI, and fractional ownership, the company enables global investors to generate income from premium real estate, vehicles, and fine art without requiring institutional capital. Users can join the RWA To Own a SHHEIKH::::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Anthropic Copyright Lawsuit Tests Fair Use Limits in AI Model Training</title><link>https://hackernoon.com/anthropic-copyright-lawsuit-tests-fair-use-limits-in-ai-model-training?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Tue, 1 Jul 2025 07:00:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is the table of links with all parts.Case Number: C 24-05417 WHAPlaintiffs: ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSONFiling Date: June 23, 2025.Location: UNITED STATES DISTRICT COURT NORTHERN DISTRICT OF CALIFORNIAAn artificial intelligence firm downloaded for free millions of copyrighted books in digital form from pirate sites on the internet. The firm also purchased copyrighted books (some overlapping with those acquired from the pirate sites), tore off the bindings, scanned every page, and stored them in digitized, searchable files. All the foregoing was done to amass a central library of “all the books in the world” to retain “forever.” From this central library, the AI firm selected various sets and subsets of digitized books to train various large language models under development to power its AI services. Some of these books were written by plaintiff authors, who now sue for copyright infringement. On summary judgment, the issue is the extent to which any of the uses of the works in question qualify as “fair uses” under Section 107 of the Copyright Act.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>Anthropic Sued for Allegedly Pirating Millions of Books to Train Claude AI</title><link>https://hackernoon.com/anthropic-sued-for-allegedly-pirating-millions-of-books-to-train-claude-ai?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Tue, 1 Jul 2025 07:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 2 of 10. Defendant Anthropic PBC is an AI software firm founded by former OpenAI employees in January 2021. Its core offering is an AI software service called Claude. When a user prompts Claude with text, Claude quickly responds with text — mimicking human reading and writing. Claude can do so because Anthropic trained Claude — or rather trained large language models or LLMs underlying various versions of Claude — using books and other texts selected from a central library Anthropic had assembled. Claude was first released publicly in March 2023. Seven successive versions of Claude have been released since. Users may ask Claude some questions for free. Demanding users and corporate clients pay to use Claude, generating over one billion dollars in annual revenue (Opp. Exh. 18). Plaintiffs Andrea Bartz, Charles Graeber, and Kirk Wallace Johnson are authors of books that Anthropic copied from pirated and purchased sources. Anthropic assembled these copies into a central library of its own, copied further various sets and subsets of those library copies to include in various “data mixes,” and used these mixes to train various LLMs. Anthropic kept the library copies in place as a permanent, general-purpose resource even after deciding it would not use certain copies to train LLMs or would never use them again to do so. All of Anthropic’s copying was without plaintiffs’ authorization. Author Bartz wrote four novels Anthropic copied and used: The Lost Night: A Novel, The Herd, We Were Never Here, and The Spare Room. Author Graeber wrote two non-fiction books likewise at issue: The Good Nurse: A True Story of Medicine, Madness, and Murder, and The Breakthrough: Immunotherapy and the Race to Cure Cancer. And, Author Johnson penned three non-fiction books also copied and used: To Be A Friend Is Fatal: The Fight to Save the Iraqis America Left Behind, The Feather Thief: Beauty, Obsession, and the Natural History Heist of the Century, and The Fishermen and the Dragon: Fear, Greed, and a Fight for Justice on the Gulf Coast. Plaintiffs Bartz Inc. and MJ + KJ Inc. are corporate entities that Author Bartz and Author Johnson respectively set up to market their works. Between them, these five plaintiffs (“Authors”) own all the copyrights in the above-listed works. From the start, Anthropic “ha[d] many places from which” it could have purchased books, but it preferred to steal them to avoid “legal/practice/business slog,” as cofounder and chief executive officer Dario Amodei put it (see Opp. Exh. 27). So, in January or February 2021, another Anthropic cofounder, Ben Mann, downloaded Books3, an online library of 196,640 books that he knew had been assembled from unauthorized copies of copyrighted books — that is, pirated. Anthropic’s next pirated acquisitions involved downloading distributed, reshared copies of other pirate libraries. In June 2021, Mann downloaded in this way at least five million copies of books from Library Genesis, or LibGen, which he knew had been pirated. And, in July 2022, Anthropic likewise downloaded at least two million copies of books from the Pirate Library Mirror, or PiLiMi, which Anthropic knew had been pirated (Opp. Exh. 6 at 4; Opp. Expert Zhao ¶¶ 17–29; see Class Cert. (“CC”) Opp. Expert Iyyer ¶¶ 45–46). Although what was downloaded and later duplicated from these sources was sometimes referred to as data or datasets, at bottom they contained full-text “ebooks or scans of books” saved in individual files in formats like .pdf, .txt, and .epub (see, e.g., Opp. Exh. 12 at - 0391318). For Books3, most filenames identified the book inside. For LibGen and PiLiMi, Anthropic downloaded a separate catalog of bibliographic metadata for each collection, with fields like title, author, and ISBN (see, e.g., ibid.; Opp. Exh. 16 -0533972–73). Anthropic thereby pirated over seven million copies of books, including copies of at least two works at issue for each Author. (1)As Anthropic trained successive LLMs, it became convinced that using books was the most cost-effective means to achieve a world-class LLM. During this time, however, Anthropic became “not so gung ho about” training on pirated books “for legal reasons” (Opp. Exh. 19). It kept them anyway (e.g., Opp. Exh. 17 at 93–94; CC Opp. Exh. 35 at -0273474). To find a new way to get books, in February 2024, Anthropic hired the former head of partnerships for Google’s book-scanning project, Tom Turvey. He was tasked with obtaining “all the books in the world” while still avoiding as much “legal/practice/business slog” as possible (Opp. Exhs. 21, 27). So, in spring 2024, Turvey sent an email or two to major publishers to inquire into licensing books for training AI. Had Turvey kept up those conversations, he might have reached agreements to license copies for AI training from publishers — just as another major technology company soon did with one major publisher (e.g., Opp. Expert Malackowski ¶¶ 50, 64). But Turvey let those conversations wither. Instead, Turvey and his team emailed major book distributors and retailers about bulkpurchasing their print copies for the AI firm’s “research library” (Opp. Exh. 22 at 145; Opp. Exh. 31 at -035589). Anthropic spent many millions of dollars to purchase millions of print books, often in used condition. Then, its service providers stripped the books from their bindings, cut their pages to size, and scanned the books into digital form — discarding the paper originals. Each print book resulted in a PDF copy containing images of the scanned pages with machine-readable text (including front and back cover scans for softcover books). Anthropic created its own catalog of bibliographic metadata for the books it was acquiring. It acquired copies of millions of books, including of all works at issue for all Authors. (2) Anthropic may have copied portions of Authors’ books on other occasions, too — such as while copying book reviews, academic papers, internet blogposts, or the like for its central library. And, Anthropic’s scanning service providers may have copied Authors’ print books along the way to delivering the final digital copies to Anthropic. But neither side here specifically raises legal issues implicated by any such copies. Nor will this order. From all the above sources, Anthropic created a general “research library” or “generalized data area.” What was this for? As Turvey said, this was a “way of creating information that would be voluminous and that we would use for research,” or otherwise to “inform our — our products” (Opp. Exh. 22 at 145–46, 194). The copies were kept in the original “version of the underlying” book files Anthropic had “obtained or created,” that is, pirated or scanned (Opp. Exh. 30 at 3, 4). Anthropic planned to “store everything forever; we might separate out books into categories[, but t]here [wa]s no compelling reason to delete a book” — even if not used for training LLMs. Over time, Anthropic invested in building more tools for searching its “general purpose” library and for accessing books or sets of books for further uses (see CC Br. Exh. 12 at -0144509; CC Reply Exh. 45 at -0365931–32, -0365939– 42 (reviewing and seeking to improve “[w]hat [ ] researchers do today if they want to search for a book,” including improving bibliographic metadata and consolidating varied resources)). One further use was training LLMs. As a preliminary step towards training, engineers browsed books and bibliographic metadata to learn what languages the books were written in, what subjects they concerned, whether they were by famous authors or not, and so on — sometimes by “open[ing] any of the books” and sometimes using software. From the library copies, engineers copied the sets or subsets of books they believed best for training and “iterate[d]” on those selections over time. For instance, two different subsets of print-sourced books were included in “data mixes” for training two different LLMs. Each was just a fraction of all the print-sourced books. Similarly, different sets or “subsets” or “parts of” or “portions” of the collections sourced from Books3, LibGen, and PiLiMi were used to train different LLMs. Anthropic analyzed the consequences of using more books, fewer books, different books. The goal was to improve the “data mix“ to improve each LLM and, ultimately, Claude’s performance for paying customers. (3)Over time, Anthropic came to value most highly for its data mixes books like the ones Authors had written, and it valued them because of the creative expressions they contained. Claude’s customers wanted Claude to write as accurately and as compellingly as Authors. So, it was best to train the LLMs underlying Claude on works just like the ones Authors had written, with well-curated facts, well-organized analyses, and captivating fictional narratives — above all with “good writing” of the kind “an editor would approve of” (Opp. Exh. 3 at -03433). Anthropic could have trained its LLMs without using such books or any books at all. That would have required spending more on, say, staff writers to create competing exemplars of good writing, engineers to revise bad exemplars into better ones, energy bills to power more rounds of training and fine-tuning, and so on. Having canonical texts to draw upon helped (e.g., Opp. Expert Zhao ¶ 81).(1) Specifically, those works were (see Opp. Expert Zhao ¶ 36; CC Br. Expert Zhao ¶ 66): Author Bartz’s The Herd (five copies total) (in LibGen and PiLiMi); Author Bartz’s The Lost Night (three copies total) (in Books3, LibGen, and PiLiMi); Author Graeber’s The Breakthrough (four copies) (in Books3, LibGen, and PiLiMi); Author Graeber’s The Good Nurse (five copies total) (in Books3 and LibGen); Author Johnson’s To Be A Friend Is Fatal (one copy) (in Books3); and Author Johnson’s The Feather Thief (four copies total) (in Books3, LibGen, PiLiMi). Some evidence suggests Anthropic downloaded still more copies before culling empty files, duplicates, and so on to reach the numbers kept in the central library and counted here.(2) In other words, within the scanned books were one or more copies of the following works: Author Bartz’s The Lost Night; Author Bartz’s We Were Never Here; Author Bartz’s The Spare Room; Author Graeber’s The Breakthrough; Author Graeber’s The Good Nurse; Author Johnson’s To Be A Friend Is Fatal; Author Johnson’s The Feather Thief; and, Author Johnson’s The Fishermen.(3) (See, e.g., Opp. Exh. 12 at -0391318 (engineers were able to “open any of the books”); CC Reply Exh. 45 at -0365941 (some engineers “want[ed] to search for a book” and get its “scanned book file[ ]”); Opp. Exh. 30 at 3 (made copies of “each such dataset or portions thereof” for training); Opp. Exh. 6 at 3–4 (trained on “portions of datasets,” with at least two such portions from LibGen and four from PiLiMi); Opp. Expert Zhao ¶¶ 27–28, 30–31 (plus two more from PiLiMi, and at least three from scanned books); CC Opp. Exh. 35 at -0273477–82 (tested subsets of pirated and purchased-and-scanned books to see consequences for training); CC Br. Exh. 12 at - 0144508–09 (“iterate[d]” selections from library and “train[ed] new models on the best data”); Br. Expert Kaplan ¶¶ 42–45 (explained goals of improving data mixes); Br. Expert Peterson ¶ 14 (similar).:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>&apos;Space Is Hard. There Is No Excuse For Pretending It&apos;s Easy&apos;</title><link>https://science.slashdot.org/story/25/06/30/2249225/space-is-hard-there-is-no-excuse-for-pretending-its-easy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["For-profit companies are pushing the narrative that they can do space inexpensively," writes Slashdot reader RUs1729 in response to an opinion piece from SpaceNews. "Their track record reveals otherwise: cutting corners won't do it for the foreseeable future." Here's an excerpt from the article, written by Robert N. Eberhart: The headlines in the space industry over the past month have delivered a sobering reminder: space is not forgiving, and certainly not friendly to overpromising entrepreneurs. From iSpace's second failed lunar landing attempt (making them 0 for 2) to SpaceX's ongoing Starship test flight setbacks -- amid a backdrop of exploding prototypes and shifting goalposts -- the evidence is mounting that the commercialization of space is not progressing in the triumphant arc that press releases might suggest. This isn't just a series of flukes. It points to a structural, strategic and cultural problem in how we talk about innovation, cost and success in space today.
 
Let's be blunt: 50 years ago, we did this. We sent humans to the moon, not once but repeatedly, and brought them back. With less computational power than your phone, using analog systems and slide rules, we achieved feats of incredible precision, reliability and coordination. Today's failures, even when dressed up as "learning opportunities," raises the obvious question: Why are we struggling to do now what we once achieved decades ago with far more complexity and far less technology?
 
Until very recently, the failure rate of private lunar exploration efforts underscored this reality. Over the past two decades, not a single private mission had fully succeeded -- until last March when Firefly Aerospace's Blue Ghost lander touched down on the moon. It marked the first fully successful soft landing by a private company. That mission deserves real credit. But that credit comes with important context: It took two decades of false starts, crashes and incomplete landings -- from Space IL's Beresheet to iSpace's Hakuto-R and Astrobotic's Peregrine -- before even one private firm delivered on the promise of lunar access. The prevailing industry answer -- "we need to innovate for lower cost" -- rings hollow. What's happening now isn't innovation; it's aspiration masquerading as disruption... "This is not a call for a retreat to Cold War models or Apollo-era budgets," writes Eberhart, in closing. "It's a call for seriousness. If we're truly entering a new space age, then it needs to be built on sound engineering, transparent economics and meaningful technical leadership -- not PR strategy. Let's stop pretending that burning money in orbit is a business model."
 
"The dream of a sustainable, entrepreneurial space ecosystem is still alive. But it won't happen unless we stop celebrating hype and start demanding results. Until then, the real innovation we need is not in spacecraft -- it's in accountability."
 
Robert N. Eberhart, PhD, is an associate professor of management and the faculty director of the Ahlers Center for International Business at the Knauss School of Business of University of San Diego. He is the author of several academic publications and books. He is also part of Oxford University's Smart Space Initiative and contributed to Berkeley's Space Sciences Laboratory. Before his academic career, Prof. Eberhart founded and ran a successful company in Japan.]]></content:encoded></item><item><title>The TechBeat: AI Agents Can Now Pay Autonomously Thanks to x402 and HTTP 402 (7/1/2025)</title><link>https://hackernoon.com/7-1-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Tue, 1 Jul 2025 06:10:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @hacker32086803 [ 7 Min read ] 
 Learn how to fix data skew in Apache Spark using the salting technique for improved performance and balanced partitions in Scala and PySpark. Read More.By @vladyslav_chekryzhov [ 25 Min read ] 
 Build production-ready LLM agents. Learn 15 principles for stability, control, and real-world reliability beyond fragile scripts and hacks. Read More.By @duycao [ 9 Min read ] 
 Learn to build effective feedback loops for AI products, connecting real user signals to model metrics for continuous improvement and better performance. Read More.By @nassermaronie [ 6 Min read ] 
 MockingJar is a tool for generating structured data from a schema you define. Read More.By @pressreleases [ 4 Min read ] 
 Business Professionals Are Half as Concerned as Technical Teams About AI-Driven Threats, Social Links Report Reveals Read More.By @jamesmiller994 [ 5 Min read ] 
 Discover the 7 essential tools for local Large Language Model (LLM) development on macOS in 2025.  Read More.By @permit [ 8 Min read ] 
 Explore how to secure AI agents, protect against prompt injections, and manage cascading AI interactions with AI Security Posture Management (AISPM). Read More.By @thesociable [ 5 Min read ] 
 DARPA is simulating disease outbreaks and human behavior to shape future pandemic responses using AI, surveillance, and behavioral modeling. Read More.By @siafoundation [ 2 Min read ] 
 Sia is thrilled to announce our official partnership with HackerNoon, one of the internet’s largest independent tech publishing platforms.  Read More.By @terminal [ 5 Min read ] 
 Learn how to install and use Hydra in Termux for efficient password cracking and security testing on your Android device.  Read More.By @n2w [ 2 Min read ] 
 Explore 5 major data breaches that shut down companies—and learn key lessons in backup, security, and disaster recovery to protect your business. Read More.By @superlinked [ 18 Min read ] 
 This article delves into constructing such an AI research agent using Superlinked's vector search capabilities, by integrating semantic and temporal relevance. Read More.By @stefanoamorelli [ 17 Min read ] 
 AI agents can now make onchain payments autonomously using x402, Coinbase’s new protocol that activates the long-unused HTTP 402 status code. Read More.By @permit [ 15 Min read ] 
 Learn how to build secure, human-in-the-loop AI agents using Permit.io’s Access Request MCP, LangGraph, and LangChain MCP Adapters.  Read More.By @thomascherickal [ 17 Min read ] 
 Mojo is 100% compatible with Python and allows low-level device access with Python syntax. It is an average of 10,000x faster, which changes everything! Read More.By @maken8 [ 6 Min read ] 
 When Bitcoin gets lost, after the Bitcoin Supply/Demand Shock, the crypto scambling will be through the roof. This is Financial Entertainment. Read More.By @permit [ 10 Min read ] 
 Machine identities are set to outnumber human users in every system. Learn why treating machine identities like human ones is crucial for security. Read More.By @yuliiakrupenko [ 6 Min read ] 
 Can offline trust drive online growth? Here's what I learned marketing in pettech - and how any startup can apply it. Read More.]]></content:encoded></item><item><title>Mathematical Proofs for Truthful Rebate Mechanisms (TFRM)</title><link>https://hackernoon.com/mathematical-proofs-for-truthful-rebate-mechanisms-tfrm?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Tue, 1 Jul 2025 06:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A PROOFS FOR RESULTS FROM SECTION 4 AND 5Theorem (Ideal-TFRM Impossibility). If𝑟 ★ is an anonymous rebate function that satisfies Theorem 1, no Ideal-TFRM can guarantee a non-zero redistribution index (RI) in the worst case.B PROOFS FOR RESULTS FROM SECTION 6Theorem*. For any𝑛 and 𝑘 such that𝑛 ≥ 𝑘+2, the R-TFRMmechanism is unique. The fraction redistributed to the top-k users in the worst-case is given by:*C PROOFS FOR RESULTS FROM SECTION 7\
\
Proof. Similar to Theorem 5, the fraction of redistribution remains constant. For every true user (not fake), the 𝛼𝑘/𝑛 fraction of the payment is returned back as the rebate in expectation.(1) Sankarshan Damle, IIIT, Hyderabad, Hyderbad, India (sankarshan.damle@research.iiit.ac.in);(2) Manisha Padala, IISc, Bangalore, Bangalore, India (manishap@iisc.ac.in);(3) Sujit Gujar, IIIT, Hyderabad, Hyderbad, India (sujit.gujar@iiit.ac.in).]]></content:encoded></item><item><title>How a College Student Built Software Behind Chicago’s Michelin-starred Restaurant</title><link>https://hackernoon.com/how-a-college-student-built-software-behind-chicagos-michelin-starred-restaurant?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Tue, 1 Jul 2025 05:46:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[UChicago student Wes Kim helped a mushroom vendor supplying Michelin-starred restaurants automate inventory using AI—after a farmer’s market meeting. What started as a school project evolved into a real-world solution, saving hours of manual work and proving tech's power in overlooked industries. Now he’s tackling healthcare automation next.

]]></content:encoded></item><item><title>Agents Unleashed Podcast: Inside the Decentralized AI Revolution Challenging Big Tech</title><link>https://hackernoon.com/agents-unleashed-podcast-inside-the-decentralized-ai-revolution-challenging-big-tech?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Tue, 1 Jul 2025 05:42:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Agents Unleashed is the first podcast dedicated to decentralized AI, co-owned agents, and coordination protocols. Hosted by Olas, it gives listeners front-row access to a growing movement challenging Big Tech’s AI dominance—featuring real-world use cases, product insights, and interviews with pioneers building a new intelligent internet.

]]></content:encoded></item><item><title>How Snehal Ladke Designs for Systems That Can’t Afford to Fail</title><link>https://hackernoon.com/how-snehal-ladke-designs-for-systems-that-cant-afford-to-fail?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Tue, 1 Jul 2025 05:38:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[From fashion to civic tech, Snehal Ladke’s UX design journey spans graphic roots to mission-critical systems for justice and transit. She crafts intuitive interfaces in invisible but vital infrastructures—reducing friction, enhancing usability, and improving public outcomes. Her work reflects a steady commitment to design that serves, not dazzles.

]]></content:encoded></item><item><title>IPinfo Launches IPinfo PLUS: High-Resolution IP Privacy Data For Greater Certainty</title><link>https://hackernoon.com/ipinfo-launches-ipinfo-plus-high-resolution-ip-privacy-data-for-greater-certainty?source=rss</link><author>IPinfo – IP Data Provider</author><category>tech</category><pubDate>Tue, 1 Jul 2025 05:04:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[IPinfo launches IPinfo Plus, a high-resolution IP intelligence product with 32 enriched attributes for risk modeling, fraud detection, and compliance. Available via API, database, Snowflake, and Google Cloud, it enables precise geo accuracy, anonymizer detection, mobile carrier data, and reassignment timestamps—starting at $62/month.

]]></content:encoded></item><item><title>Manasa Talluri Sets New Benchmarks in Enterprise Application Development Excellence</title><link>https://hackernoon.com/manasa-talluri-sets-new-benchmarks-in-enterprise-application-development-excellence?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Tue, 1 Jul 2025 04:48:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Manasa Talluri, a Principal Developer, transformed enterprise development with cross-framework expertise, phased migrations, and secure, mobile-first design. Her leadership on complex projects and mentorship approach have set new benchmarks in scalable app development—bridging UX and enterprise systems with technical and strategic precision.

]]></content:encoded></item><item><title>China Hosts First Fully Autonomous AI Robot Football Match</title><link>https://hardware.slashdot.org/story/25/06/30/224233/china-hosts-first-fully-autonomous-ai-robot-football-match?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Guardian: Four teams of humanoid robots took each other on in Beijing [on Saturday], in games of three-a-side powered by artificial intelligence. While the modern game has faced accusations of becoming near-robotic in its obsession with tactical perfection, the games in China showed that AI won't be taking Kylian Mbappe's job just yet. Footage of the humanoid kickabout showed the robots struggling to kick the ball or stay upright, performing pratfalls that would have earned their flesh-and-blood counterparts a yellow card for diving. At least two robots were stretchered off after failing to regain their feet after going to ground.
 
[...] The competition was fought between university teams, which adapted the robots with their own algorithms. In the final match, Tsinghua University's THU Robotics defeated the China Agricultural University's Mountain Sea team with a score of 5-3 to win the championship. One Tsinghua supporter celebrated their victory while also praising the competition. "They [THU] did really well," he said. "But the Mountain Sea team was also impressive. They brought a lot of surprises." Cheng Hao, CEO of Booster Robotics, said he envisions future matches between humans and robots, though he acknowledges current robots still lag behind in performance. He also said safety will need to be a top priority.
 
You can watch highlights of the match on YouTube.]]></content:encoded></item><item><title>States Need To Mimic Colorado Law Now That HHS Is A Dumpster Fire</title><link>https://www.techdirt.com/2025/06/30/states-need-to-mimic-colorado-law-now-that-hhs-is-a-dumpster-fire/</link><author>Dark Helmet</author><category>tech</category><pubDate>Tue, 1 Jul 2025 03:03:29 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[At the beginning of this year, the Colorado state legislature introduced HB25-1097, a state law that updated the state’s disease control statutes. Eventually signed into law by the Governor in April, the bill does a whole bunch of things related to public health: repealed the state’s epidemic response committee, set a schedule for reviewing the state’s emergency plans every three years, and all sorts of things having to do with child immunization rules. Those include things like creating an official school record for immunization after doctor’s records of immunization are received, how camping organizations keep their own records for immunization for out of state campers, and so on. Mostly pretty yawn-inducing stuff.But it also included this:Direct the state board of health, in adopting rules establishing immunization requirements, to take into consideration, as appropriate and in addition to the recommendations of the advisory committee on immunization practices, the recommendations of the American Academy of Pediatrics, the American Academy of Family Physicians, the American College of Obstetricians and Gynecologists, and the American College of Physicians;That reference to the advisory committee on immunization practices is more commonly known as the CDC’s ACIP. That would be the committee for which RFK Jr. fired all 17 members and replaced them with 8 new members, several of which are vaccine disinformation peddlers.While this law and this provision of it largely flew under the radar, its purpose is now being shown and highlighted as a way to combat Kennedy and HHS’ nonsense. Other states need to pay attention here.As the new Health and Human Services secretary makes unprecedented moves to undermine the current U.S. policy on vaccines, Colorado is leading the way in maintaining immunization recommendations, writing some protections into law. Colorado lawmakers saw this conflict coming and started preparing for the change, particularly to this critical national panel of doctors and vaccine experts, during this year’s legislative session.So they passed a bill along party lines, later signed into law by the governor, which directs the state’s board of health to take into consideration recommendations from other high-profile doctors’ groups, not just the CDC panel.“I think you could see the writing on the wall, that it was just becoming overly politicized rather than relying on actual science with this new HHS director,” said Sen. Kyle Mullica, a Thornton Democrat and an ER nurse. “We decided to protect Colorado,” said Mullica, who co-sponsored the legislation. He said Democratic lawmakers wanted to ensure “that in Colorado that we were able to rely on other science-based recommendations that potentially wouldn’t be as vulnerable to political upheaval that we’re seeing right now.”This is a good start. Essentially, Colorado’s legislation presents something of a no-confidence vote in the CDC and HHS, choosing to open up guidance that had previously been limited to those agencies to incorporate NGOs that actually have public health and science in mind. Other states adopting similar laws would be useful both in maintaining good guidance on a state level and in highlighting yet again how much valid distrust of RFK Jr.’s leadership exists.Ashish Jha, Biden’s COVID response coordinator and the dean of Brown University School of Public Health, highlights that this is about much more than keeping the public supplied with good scientific information. The game Kennedy is  playing isn’t one in which he makes vaccines entirely unauthorized or disappeared. Instead, he’ll just make them so expensive that few people can afford them.ACIP’s recommendations serve as the backbone of vaccine access in the United States. When the panel endorses a vaccine, that guidance sets off a chain reaction: Insurers are required under the Affordable Care Act to cover it with no cost-sharing. Medicaid programs follow suit. Pediatricians and pharmacies stock vaccines knowing they’ll be reimbursed. And the Vaccines for Children (VFC) program, which provides free immunizations to nearly half of American children, uses ACIP recommendations to determine which vaccines are covered.If Kennedy’s reconstituted ACIP rolls back key recommendations, as appears likely, the vaccines themselves won’t disappear — but access will erode. Insurers could stop covering them. Clinics might stop offering them. The VFC program could shrink. In effect, millions of children would lose protection against diseases such as measles, polio, meningitis and others we thought were behind us.Kennedy might argue that he’s not taking anyone’s vaccines away, just giving people choices. But making vaccines costly and inaccessible produces the same result.As Jha notes further in the post, laws like the Colorado law can only be step 1. Step 2 needs to be state-level regulation of insurance companies in order to ensure the Kennedy’s plan to price vaccines out of reach for most people isn’t successful.Most important, states must ensure that recommended vaccines remain free and accessible. Legislatures and insurance regulators should require both private insurers and Medicaid programs to cover all vaccines endorsed by medical societies or state advisory boards — with no out-of-pocket costs.This will help preserve access for millions, especially the most vulnerable.This is by no means a perfect plan. States will vary in their coverage and their guidance. The residents in some states, particular their children, will live under worse conditions than others. Not all citizens will have the same healthcare available to them. In states where science is sneered at in the same manner as Kennedy’s HHS, some people, including children, will .But this is the reality in front of us. If no action is taken and this version of the CDC is allowed to convince the public that vaccines are the devil, or if vaccines are simply made too expensive to be widely adopted, the end result could be just what James Carville recently predicted.“Bobby Kennedy is going to kill more people than any Cabinet secretary, maybe in history, with his idiotic vaccine policy,” Carville said Wednesday in an interview on Fox News Channel’s “The Will Cain Show.”If a patchwork of state laws can stave off that nightmare from reality, so be it.]]></content:encoded></item><item><title>Google Buys 200 Megawatts of Fusion Energy That Doesn&apos;t Even Exist Yet</title><link>https://hardware.slashdot.org/story/25/06/30/2143225/google-buys-200-megawatts-of-fusion-energy-that-doesnt-even-exist-yet?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google has signed a deal to purchase 200 megawatts of future fusion energy from Commonwealth Fusion Systems, despite the energy source not yet existing. "It's a sign of how hungry big tech companies are for a virtually unlimited source of clean power that is still years away," reports CNN. From the report: Google and Massachusetts-based Commonwealth Fusion Systems announced a deal Monday in which the tech company bought 200 megawatts of power from Commonwealth's first commercial fusion plant, the same amount of energy that could power roughly 200,000 average American homes. Commonwealth aims to build the plant in Virginia by the early 2030s. When it starts generating usable fusion energy is still TBD, though the company believes they can do it in the same timeframe.
 
Google is also investing a second round of money into Commonwealth to spur development of its demonstration tokamak -- a donut-shaped machine that uses massive magnets and molten plasma to force two atoms to merge, thereby creating the energy of the sun. Google and Commonwealth did not disclose how much money is being invested, but both touted the announcement as a major step toward fusion commercialization. "We're using this purchasing power that we have to send a demand signal to the market for fusion energy and hopefully move (the) technology forward," said Michael Terrell, senior director of energy and climate at Google.
 
Commonwealth is currently building its demonstration plant in Massachusetts, known as SPARC. It's the tokamak the company says could forever change where the world gets its power from, generating 10 million times more energy than coal or natural gas while producing no planet-warming pollution. Fuel for fusion is abundant, derived from a form of hydrogen found in seawater and tritium extracted from lithium. And unlike nuclear fission, there is no radioactive waste involved. The big challenge is that no one has yet built a machine powerful and precise enough to get more energy out of the reaction than they put into it.]]></content:encoded></item><item><title>ChatGPT: Everything you need to know about the AI-powered chatbot</title><link>https://techcrunch.com/2025/06/30/chatgpt-everything-to-know-about-the-ai-chatbot/</link><author>Kyle Wiggers, Cody Corrall, Alyssa Stringer, Kate Park</author><category>tech</category><pubDate>Tue, 1 Jul 2025 01:43:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022. What started as a tool to supercharge productivity through writing essays and code with short text prompts has evolved into a behemoth with 300 million weekly active users. 2024 was a big year for OpenAI, from its partnership […]]]></content:encoded></item><item><title>NASA To Stream Rocket Launches and Spacewalks On Netflix</title><link>https://entertainment.slashdot.org/story/25/06/30/2135227/nasa-to-stream-rocket-launches-and-spacewalks-on-netflix?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli shares a report from NERDS.xyz: NASA is coming to Netflix. No, not a drama or sci-fi reboot. The space agency is actually bringing real rocket launches, astronaut spacewalks, and even views of Earth from space directly to your favorite streaming service. Starting this summer, NASA+ will be available on Netflix, giving the space-curious a front-row seat to live mission coverage and other programming.
 
The space agency is hoping this move helps it connect with a much bigger audience, and considering Netflix reaches over 700 million people, that's not a stretch. This partnership is about accessibility. NASA already offers NASA+ for free, without ads, through its app and website. But now it's going where the eyeballs are. If people won't come to the space agency, the space agency will come to them.]]></content:encoded></item><item><title>Sri Mandir keeps investors hooked as digital devotion grows</title><link>https://techcrunch.com/2025/06/30/sri-mandir-keeps-investors-hooked-as-digital-devotion-grows/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Tue, 1 Jul 2025 00:45:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Founded in November 2020, AppsForBharat introduced Sri Mandir shortly after to serve Hindu devotees with online prayers and the ability to make offerings virtually to Indian temples.]]></content:encoded></item><item><title>Norwegian Lotto Mistakenly Told Thousands They Were Filthy Rich After Math Error</title><link>https://science.slashdot.org/story/25/06/30/2131215/norwegian-lotto-mistakenly-told-thousands-they-were-filthy-rich-after-math-error?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Thousands of Norwegians briefly believed they had won massive Eurojackpot prizes after a manual coding error by Norsk Tipping mistakenly multiplied winnings by 100 instead of dividing. The Register reports: Eurojackpot, a pan-European lottery launched in 2012, holds two draws per week, and its jackpots start at about $12 million with a rollover cap of $141 million. Norsk Tipping, Norway's Eurojackpot administrator, admitted on Friday that a "manual error" it its conversion process from Eurocents to Norwegian kroner multiplied amounts by 100 instead of dividing them. As a result, "thousands" of players were briefly shown jackpots far higher than their actual winnings before the mistake was caught, but no incorrect payouts were made.
 
Norsk Tipping didn't disclose how large the false jackpots were, but math suggests the improper amounts were 10,000x times higher. Regardless, it seems like a lot of people thought they were big winners, based on what the company's now-former CEO, Tonje Sagstuen, said on Saturday. "I have received many messages from people who had managed to make plans for holidays, buying an apartment or renovating before they realized that the amount was wrong," Sagstuen said in a statement. "To them I can only say: Sorry!" The incorrect prize amounts were visible on the Norsk Tipping website only briefly on Friday, but the CEO still resigned over the weekend following the incident.
 
While one of the Norsk Tipping press releases regarding the incident described it as "not a technical error," it still appears someone fat-fingered a bit of data entry. The company said it will nonetheless be investigating how such a mistake could have happened "to prevent something similar from happening again."]]></content:encoded></item><item><title>Legal software company Clio drops $1B on law data giant vLex</title><link>https://techcrunch.com/2025/06/30/legal-software-company-clio-drops-1b-on-law-data-giant-vlex/</link><author>Marina Temkin</author><category>tech</category><pubDate>Tue, 1 Jul 2025 00:29:50 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Clio, last valued at $3B, makes a bet on AI-powered legal data provider]]></content:encoded></item><item><title>AerynOS Tooling Now Written In Rust Rather Than D, New Software Updates</title><link>https://www.phoronix.com/news/AerynOS-Mid-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 00:27:30 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Coincidentally after writing this morning wondering about what's going on with AerynOS (formerly known as Serpent OS), contributors "NomadicCore" and "Ermo" have issued a new project update outlining what's been going on the past few months...]]></content:encoded></item><item><title>Windows User Base Shrinks By 400 Million In Three Years</title><link>https://tech.slashdot.org/story/25/06/30/2123239/windows-user-base-shrinks-by-400-million-in-three-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Tom's Hardware: Microsoft EVP Yusuf Mehdi said in a blog post last week that Windows powers over a billion active devices globally. This might sound like a healthy number, but according to ZDNET, the Microsoft annual report for 2022 said that more than 1.4 billion devices were running Windows 10 or 11. Given that these documents contain material information and have allegedly been pored over by the tech giant's lawyers, we can safely assume that Windows' user base has been quietly shrinking in the past three years, shedding around 400 million users.
 
This is probably why Microsoft has been aggressively pushing users to upgrade to Windows 11 after the previous version of the OS loses support -- so that its users would install the latest version of Windows on their current system (or get a new PC if their system is incapable of running the latest version). Although macOS is a threat to Windows, especially with the launch of Apple Silicon, we cannot say that those 400 million users all went and bought a MacBook. That's because, as far back as 2023, Mac sales have also been dropping, with Statista reporting the computer line, once holding more than 85% of the company revenue, now making up just 7.7%. The shrinking Windows user base can be attributed to a combination of factors -- a major one being the global move toward a mobile-first world, where smartphones and tablets are increasingly replacing traditional PCs for everyday computing needs.
 
At the same time, Microsoft's strict hardware requirements for Windows 11 have alienated users with perfectly functional older machines, prompting some to stick with unsupported versions or abandon Windows entirely. Additionally, many users find Windows 11 less intuitive than its predecessor and are frustrated by Microsoft's push toward data collection and Apple-style design changes.]]></content:encoded></item><item><title>Intel&apos;s FFmpeg Cartwheel Brings Experimental Panther Lake Support</title><link>https://www.phoronix.com/news/Intel-FFmpeg-Cartwheel-2025Q1</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 30 Jun 2025 23:24:48 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While Q2 is drawing to an end in the coming hours, Intel software engineers this evening just released the Intel FFmpeg Cartwheel 2025Q1 update that provides all of their latest patches around Intel GPU/video acceleration for this widely-used, open-source multimedia library that have yet to be upstreamed into FFmpeg proper...]]></content:encoded></item><item><title>Tumblr&apos;s Move To WordPress and Fediverse Integration Is &apos;On Hold&apos;</title><link>https://tech.slashdot.org/story/25/06/30/211204/tumblrs-move-to-wordpress-and-fediverse-integration-is-on-hold?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Mon, 30 Jun 2025 23:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Automattic has put its plan to migrate Tumblr's backend to WordPress on hold, with CEO Matt Mullenweg citing a shift in focus toward features users are actively requesting. "I still want to do it," Mullenweg says. "It's just cleaner. But right now, we're not working on it." The Verge reports: The decision to halt the change also appears to mean that Tumblr posts won't be available in the fediverse in the near future. WordPress.com currently offers an ActivityPub plug-in, so Tumblr moving onto WordPress would theoretically let people bring Tumblr posts to the fediverse. "That would've been a free way to get it," Mullenweg says. "And so that was one of the arguments for migrating everything to WordPress."
 
In the meantime, however, "I think if there was a big push to implement fediverse, we would just do it on the Tumblr code base," according to Mullenweg.]]></content:encoded></item><item><title>CarFax For Used PCs: Hewlett Packard Wants To Give Laptops New Life</title><link>https://slashdot.org/story/25/06/30/2052250/carfax-for-used-pcs-hewlett-packard-wants-to-give-laptops-new-life?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Mon, 30 Jun 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[HP is developing a "PCFax" system similar to CarFax for used cars that securely collects and stores detailed device usage and health data at the firmware level to extend the life of PCs and reduce e-waste. A team of HP experts outlines the system in a recent IEEE Spectrum report: The secure telemetry protocol we've developed at HP works as follows. We gather the critical hardware and sensor data and store it in a designated area of the SSD. This area is write-locked, meaning only authorized firmware components can write to it, preventing accidental modification or tampering. That authorized firmware component we us is the Endpoint Security Controller, a dedicated piece of hardware embedded in business class HP PCs. It plays a critical role in strengthening platform-level security and works independently from the main CPU to provide foundational protection.
 
The endpoint security controller establishes a secure session by retaining the secret key within the controller itself. This mechanism enables read data protection on the SSD -- where telemetry and sensitive data are stored -- by preventing unauthorized access, even if the operating system is reinstalled or the system environment is otherwise altered. Then, the collected data is recorded in a timestamped file, stored within a dedicated telemetry log on the SSD. Storing these records on the SSD has the benefit of ensuring the data is persistent even if the operating system is reinstalled or some other drastic change in software environment occurs. The telemetry log employs a cyclic buffer design, automatically overwriting older entries when the log reaches full capacity. Then, the telemetry log can be accessed by authorized applications at the operating system level.
 
The telemetry log serves as the foundation for a comprehensive device history report. Much like a CarFax report for used cars, this report, which we call PCFax, will provide both current users and potential buyers with crucial information. The PCFax report aggregates data from multiple sources beyond just the on-device telemetry logs. It combines the secure firmware-level usage data with information from HP's factory and supply chain records, digital services platforms, customer support service records, diagnostic logs, and more. Additionally, the system can integrate data from external sources including partner sales and service records, refurbishment partner databases, third-party component manufacturers like Intel, and other original equipment manufacturers. This multi-source approach creates a complete picture of the device's entire lifecycle, from manufacturing through all subsequent ownership and service events.]]></content:encoded></item><item><title>AMC’s Latest Innovation: More Ads At The Movie Theater</title><link>https://www.techdirt.com/2025/06/30/amcs-latest-innovation-more-ads-at-the-movie-theater/</link><author>Karl Bode</author><category>tech</category><pubDate>Mon, 30 Jun 2025 22:29:46 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[During pandemic lockdowns, you might recall how AMC executives threw a temper tantrum because companies like Comcast/NBC began experimenting with more innovative movie release windows. AMC execs were mad because the pandemic highlighted how the 90-day gap between the time a movie appears in theaters and its streaming or DVD release was exposed as both dated and stupid.Comcast (successfully) experimented with not only shortening the window, but eliminating it entirely. At the time, AMC Theatre CEO Adam Aron pouted incessantly, insisting that Comcast films would never again appear in AMC theaters, before ultimately having to retract the silly threat.Even before COVID, other major theater companies, like Cinemark Theatres and Regal Cinemas, had been loading up to five minutes of ads ahead of movies. AMC had initially rejected joining the effort, correctly noting that they worried consumers would “react quite negatively to the concept.”That was then, this is now. AMC, struggling to make as much money as it would like, has reversed course and will be adding more ads. Which is tricky because it already runs 25-30 minutes of trailers, ads, and assorted gibberish before movies begin already:“The deal takes effect July 1, just in time for Universal’s Jurassic World Rebirth and DC Studios/Warner Bros.’ Superman. AMC is already known for its lengthy preshow time, which runs 25 to 30 minutes, so it will have to reconfigure its lineup — which includes the famous Nicole Kidman spot promoting the “magic” of moviegoing — to allow for the new ads without going over the half-hour mark.”Annoyed customers are still going to the movies, but they’re showing up later to accommodate for all the ads. One recent industry study found that only 60 percent of moviegoers this year were in their seats when trailers started playing. In NY and LA,  42 percent of moviegoers were in their seats in time to see every trailer.This is yet another enshittification cycle that shows no sign of relenting. To give Wall Street its expected impossibly growing quarterly returns, AMC can’t afford to actually provide things the audience wants (lower prices, smaller crowds, better quality films and food). So they’re on a path of a sort of brand cannibalization in which annoyances grow as the theater experience quality shrinks, driving annoyed users ultimately to other experiences (like piracy). Once a company’s on this path there’s really no reversal if they want to avoid an investor revolt, so there’s simply no telling what bad idea (or eroded principle) comes next for AMC.]]></content:encoded></item><item><title>Freelancers Using AI Tools Earn 40% More Per Hour Than Peers, Study Says</title><link>https://slashdot.org/story/25/06/30/1959225/freelancers-using-ai-tools-earn-40-more-per-hour-than-peers-study-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Freelance workers using AI tools are earning significantly more than their counterparts, with AI-related freelance earnings climbing 25% year over year and AI freelancers commanding over 40% higher hourly rates than non-AI workers, according to new data from Upwork. 

The freelance marketplace analyzed over 130 work categories and tracked millions of job posts over six months, finding that generative AI is simultaneously replacing low-complexity, repetitive tasks while creating demand for AI-augmented work. Workers using AI for augmentation outnumber those using it for automation by more than 2 to 1. Freelancers with coding skills comprising at least 25% of their work now earn 11% more for identical jobs compared to November 2022 when ChatGPT launched.]]></content:encoded></item><item><title>Apple Loses Bid To Dismiss US Smartphone Monopoly Case</title><link>https://apple.slashdot.org/story/25/06/30/204242/apple-loses-bid-to-dismiss-us-smartphone-monopoly-case?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 21:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple must face the U.S. Department of Justice's lawsuit accusing the iPhone maker of unlawfully dominating the U.S. smartphone market, a judge ruled on Monday. From a report: U.S. District Judge Julien Neals in Newark, New Jersey, denied Apple's motion to dismiss the lawsuit accusing the company of using restrictions on third-party app and device developers to keep users from switching to competitors and unlawfully dominate the market. 

The decision would allow the case to go forward in what could be a years-long fight for Apple against enforcers' attempt to lower what they say are barriers to competition with Apple's iPhone.]]></content:encoded></item><item><title>Is Artificial General Intelligence the Future?</title><link>https://hackernoon.com/is-artificial-general-intelligence-the-future?source=rss</link><author>Vipin Labroo</author><category>tech</category><pubDate>Mon, 30 Jun 2025 21:16:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[For all its capabilities, AI in its traditional form has a restrictive sense about it. It can be very fairly accomplished in terms of its abilities, but is far from able to replicate the full spectrum of what human ability can achieve. Whatever you achieve using the current AI model in terms of efficiency and output, it willy-nilly requires some form or level of human oversight at some level. \
All of this may change with the advancements made by what is known as Artificial General Intelligence.Traditional AI vs. Artificial General IntelligenceWhile traditional AI is known to be super-efficient when carrying out well-defined tasks like translating languages, Artificial General Intelligence, or AGI, can comprehend, master, and apply knowledge spanning a wide spectrum of tasks like what human beings do. This takes AI much closer to replicating human behavior in all its dimensions, including the emotional one. \
It is not, therefore, surprising that all the tech majors of consequence, including Google, OpenAI, and Meta, are very gung-ho about AGI. As a matter of fact, there seems to be a race on amongst them about who will be able to announce the successful launch of this "paradigm redefining" technology.For Artificial General Intelligence to have truly arrived, it should have passed the Turing Test, which denotes that one cannot tell the difference between AI’s abilities and those of a human being. As far as researchers and academics are concerned, this Rubicon will not be crossed in a hurry, taking anything from a few decades to even a few centuries.[[1]]\
For many, the prospect of a delayed advent is a welcome development given the potential of a technology that performs in a manner that is in every way equal to what a human performs, potentially devastating the prospects of gainful employment. \
They point out the fact that AI, with its presently limited abilities, is already leading to significant loss of employment, and how much more destabilizing a much more evolved form of AI would be for human employment.Why is AGI so impressive?AGI is expected to cause a seminal change in virtually every aspect of human existence. The fact that this technology possesses human-like cognitive abilities makes it applicable in a universal manner, which is far from the case as of now. With capabilities like these, it can be expected that AGI would be able to provide solutions to intractable human problems like climate change and economic inequalities, apart from finding cures for serious human illnesses.\
Furthermore, AGI could carry out cutting-edge research and come up with new discoveries and technologies in the realms of medicine, space exploration, energy, and so on. These would be breathtaking in the scope of their application, forever altering the human condition for the better.\
AGI would lead to humongous gains in efficiency and productivity across industries requiring both repetitive and cognitive work, freeing up humans to focus entirely on creative and strategic work. From perfectly managed factories to impeccable public services, people would have it very good indeed in their day-to-day lives.Viewing AGI as a collaboratorUltimately, technology has to be in the service of humans and not the other way round. To that extent, AGI has a future as a collaborator that works for human betterment and does not seek to supplant them. It can help augment human endeavor by enhancing the latter's intelligence and ability to be creative. \
With its unique ability to offer real-time insights and out-of-the-box ideas, as well as helping humans to process and comprehend complex information in a very efficient manner, AGI can help humans make stupendous achievements. This outstanding synergy between humans and machines can usher in a brave new world that we cannot even begin to visualize in the present times.[[1]] https://www.mckinsey.com/featured-insights/ https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-artificial-general-intelligence-agimckinsey-explainers/what-is-artificial-general-intelligence-agiFeature image created by Google Gemini]]></content:encoded></item><item><title>Tesla sends driverless Model Y from factory to customer to promote its robotaxi tech</title><link>https://techcrunch.com/2025/06/30/tesla-sends-driverless-model-y-from-factory-to-customer-to-promote-its-robotaxi-tech/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Mon, 30 Jun 2025 21:06:25 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The car traveled around 15 miles on highways, surface streets, and even a roundabout. But many questions remain about the stunt.]]></content:encoded></item><item><title>Next-gen procurement platform Levelpath nabs $55M</title><link>https://techcrunch.com/2025/06/30/next-gen-procurement-platform-levelpath-nabs-55m/</link><author>Marina Temkin</author><category>tech</category><pubDate>Mon, 30 Jun 2025 20:46:23 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The investment signals confidence in Levelpath's fast growth and its potential to disrupt a market dominated by legacy players.]]></content:encoded></item><item><title>Senate GOP Budget Bill Has Little-Noticed Provision That Could Hurt Your Wi-Fi</title><link>https://mobile.slashdot.org/story/25/06/30/2011219/senate-gop-budget-bill-has-little-noticed-provision-that-could-hurt-your-wi-fi?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 20:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Sen. Ted Cruz (R-Texas) has a plan for spectrum auctions that could take frequencies away from Wi-Fi and reallocate them for the exclusive use of wireless carriers. The plan would benefit AT&T, which is based in Cruz's home state, along with Verizon and T-Mobile. 

Cruz's proposal revives a years-old controversy over whether the entire 6 GHz band should be devoted to Wi-Fi, which can use the large spectrum band for faster speeds than networks that rely solely on the 2.4 and 5 GHz bands. Congress is on the verge of passing legislation that would require spectrum to be auctioned off for full-power, commercially licensed use, and the question is where that spectrum will come from. 

When the House of Representatives passed its so-called "One Big Beautiful Bill," it excluded all of the frequencies between 5.925 and 7.125 gigahertz from the planned spectrum auctions. But Cruz's version of the budget reconciliation bill, which is moving quickly toward a final vote, removed the 6 GHz band's protection from spectrum auctions. The Cruz bill is also controversial because it would penalize states that regulate artificial intelligence. 

Instead of excluding the 6 GHz band from auctions, Cruz's bill would instead exclude the 7.4-8.4 GHz band used by the military. Under conditions set by the bill, it could be hard for the Commerce Department and Federal Communications Commission to fulfill the Congressional mandate without taking some spectrum away from Wi-Fi.]]></content:encoded></item><item><title>The Supreme Court Just Discovered Nationwide Injunctions Are Bad—Right As Trump Needs Them Gone</title><link>https://www.techdirt.com/2025/06/30/the-supreme-court-just-discovered-nationwide-injunctions-are-bad-right-as-trump-trump-needs-them-gone/</link><author>Mike Masnick</author><category>tech</category><pubDate>Mon, 30 Jun 2025 20:31:46 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The Supreme Court just discovered that nationwide injunctions are problematic. How convenient that this revelation came right as Donald Trump returned to office with plans to rule by executive fiat.Last week’s big Supreme Court decision in Trump v. CASA was trumpeted in the media as being about birthright citizenship, even though that was only the vehicle with which to actually explore the issue of nationwide injunctions.The ruling effectively bans (or at least greatly limits) the ability of judges to issue such nationwide or “universal” injunctions, stating that they exceed the power of a district court:Traditionally, courts issued injunctions prohibiting executive officials from enforcing a challenged law or policy only against the plaintiffs in the lawsuit. The injunctions before us today reflect a more recent development: district courts asserting the power to prohibit enforcement of a law or policy against anyone. These injunctions—known as “universal injunctions”—likely exceed the equitable authority that Congress has granted to federal courts. We therefore grant the Government’s applications to partially stay the injunctions entered below.Because of the issue underlying this injunction (the executive order effectively reinterpreting birthright citizenship out of the Constitution), the ruling is being interpreted through deeply partisan eyes. The six Justices who signed onto the majority opinion are those appointed by Republican Presidents, the dissent was from the three Justices appointed by Democratic Presidents.So, the issue is absolutely partisan, but I’d argue the partisanship is more in the timing and specifics of this, rather than the actual final decision. And that’s because the MAGA/GOP world was more than happy to use such nationwide injunctions against Biden. Indeed, they regularly went forum shopping to specific judges in Texas for exactly that reason.And, arguably, the Supreme Court decision actually leaves an out to suggest that  nationwide injunctions will still be allowed, which likely means only those against Democratic Presidents. Steve Vladeck explains:Contra some headlines, Justice Barrett’s majority opinion doesn’t foreclose “universal” injunctions in all cases. Rather, it holds that injunctive relief that directly benefits non-plaintiffs is appropriate only when it’s necessary to ensure that the plaintiffs themselves get “complete” relief. (An illustrative example of when individual plaintiffs will need universal relief is in redistricting cases—where states often need to redraw maps on a statewide basis to redress a successful challenge by even a single plaintiff.) Put another way, if an injunction limited to the plaintiffs in that case will give those plaintiffs all the relief to which they’re entitled, nothing broader is permitted. That’s the holding, in a nutshell.But here’s the thing: the Supreme Court’s sudden concern about nationwide injunctions rings hollow when you look at the actual data. Just last year, there was a fascinating Harvard Law Review look at nationwide injunctions, and how both sides of the traditional political divide have been using them. When Democrats are in power, Republicans rush to partisan judges to block their actions with nationwide injunctions, and when Republicans are in power, Democrats do the same.The nationwide injunction increase appears to be a symptom, more than the disease. As Congress has become increasingly unable to function, both parties have been trying to exert greater and greater executive power. The rise in nationwide injunctions appears to mainly be in an attempt to push back on that—though in directly partisan ways, depending on who is in power.The numbers are damning: of the 78 nationwide injunctions issued during the Trump and Biden Administrations, 93.6% were issued by judges appointed by a president of the opposing political party. That’s not coincidence; that’s weaponization.Notably, nationwide injunctions are not only increasing in frequency but also overwhelmingly issued by judges appointed by Presidents of the opposite party from the administration whose actions the judges are enjoining. Of the 78 nationwide injunctions issued during the Trump and Biden Administrations, 93.6% of injunctions were issued by judges appointed by a President of the opposing political party. Often, it is the policies that relate to politically hot-button issues or a President’s policy priorities that are enjoined: for President Obama, it was LGBTQ+ civil rights; for President Trump, it was immigration; and for President Biden, it was policies combatting the COVID-19 pandemic.Given all that, you could see how there are actually good reasons why nationwide injunctions might be seen as a problem overall. They’re creating scenarios where not only is there political calculus likely driving some of the judicial decisions, but also it allows plaintiffs to take many shots on goal. Again, the HLR piece describes how many cases may be filed on the same issues in multiple courts, and you just need a single nationwide injunction to “win” the issue:the asymmetrical effects of preclusion ensure that nationwide injunctions are a powerful tool for political opponents who can challenge the policy in multiple venues. Practically speaking, a successful defense against a nationwide injunction in one court is barely a win for the government at all: because that decision has no preclusive effect on new plaintiffs, other plaintiffs are free to bring the exact same lawsuit elsewhere and “[s]hop ‘til the statute drops.” All it takes is one judge siding with the plaintiffs to enjoin the challenged law. These asymmetric consequences force the federal government to engage in a game of whack-a-mole. If enough plaintiffs sue — and if they can each target the forum most likely to be hostile to the government’s action — it seems almost inevitable that the action will be nationally enjoined. A prominent example is President Biden’s COVID-19 vaccine mandates: At least four judges declined to issue nationwide injunctions against Executive Order 14,042, but ultimately one did. One judge declined to issue a nationwide injunction against Executive Order 14,043, but still the policy was enjoined nationally. The same is true for the Centers for Medicare & Medicaid Services’ vaccine mandate. And at least four different judges declined to issue nationwide injunctions against President Biden’s military vaccine mandate, but, ultimately, two enjoined the policy nationally.Given all that, there’s a reasonable argument that the Supreme Court’s ruling on nationwide injunctions isn’t inherently bad. They were problematic when Stephen Miller’s org was rushing cases to single-judge districts in Texas, and they continue to be problematic today.But, what makes last week’s decision so overtly political is the fact that the Supreme Court waited until  to argue that such nationwide injunctions were invalid as a remedy. They’ve had many years to take on the issue and somehow never bothered to call them out until now? That’s the part that seems suspect.And the timing is especially questionable, given that we’ve just entered the second Trump administration, where he and his regime have made it quite clear they intend to rule almost entirely through executive fiat, with little concern for what Congress does (or is unable to do). And that’s a world in which the judiciary (in theory) plays a much bigger role.In the end, I think the issue of nationwide injunctions isn’t really an issue of “nationwide injunctions,” but rather what happened that resulted in such injunctions becoming a regularly used political weapon: (1) a massive increase in attempts to rule by executive fiat, and (2) the rise in forum shopping for judges (especially in single judge courts).Thus, it seems like the structural reform that is better served than simply banning nationwide injunctions is making it clear that we actually do have three co-equal branches of government, rather than a “unitary executive” and also making procedural changes to limit judge shopping.It seems quite clear that the Supreme Court made this decision for partisan reasons, given the timing, but just the fact that it didn’t do so in conjunction with a strong limit to ruling by executive fiat is equally worrisome. The two need to go hand in hand. Instead, we’re seeing the reverse. The Supreme Court seems willing to bless executive overreach… but only when the Republicans do it. When Trump violates constitutional norms, the Court finds ways to look the other way. When Democrats governed, suddenly every procedural safeguard mattered.Separately, while some have argued that the nationwide injunction ban might not be as bad as some people fear because plaintiffs can just bring class action lawsuits instead, as Vladeck notes, this Supreme Court has also limited the ability to bring class action lawsuits:Class actions are harder to bring, at least largely thanks to the Supreme Court—and a series of rulings from the early 2010sthat ratcheted up the requirements for certifying nationwide classes. On top of that, states (and organizations like CASA) can’t be class-action plaintiffs (the Federal Rule of Civil Procedure that authorizes class-wide reliefto be persons). And even when a district court determines that certification of a nationwide class is appropriate, (1) it often takes some time for the district court to so conclude; and (2) such a ruling is itself subject to an immediate, interlocutory appeal—which can both slow down the litigation and give appellate courts an early opportunity to reject a district court’s decision to certify a nationwide class. So as with the “complete relief” question, the viability of this alternative legal procedure for blocking federal policies on a nationwide basis really depends upon just how available nationwide class actions turn out to be in practice—not just in general, but at the outset of litigation, as well.As for judge shopping, Chief Justice Roberts had actually spoken out about the problem in the past and last year tried to implement a new rule that would make judge assignments more randomized. But judges on the Fifth Circuit and across Texas — where most of the more significant forum shopping against Biden happened — simply announced that they would ignore the rule, and Roberts effectively backed down.Some might wonder why the Roberts Court would effectively kneecap the judiciary, of which it’s a key part. But, again, the details suggest that the CASA ruling effectively takes power away from lower courts, but gives it to the Supreme Court, in that there will be a lot more moves to try to get injunctions via the emergency or shadow docket. Vladeck again:In other words, Kavanaugh is effectively inviting both the government and litigants challenging government policies to use the emergency docket even more—not just in cases in which there is some compelling exigency, but in any case in which there’s a need for that kind of nationwide (interim) uniformity. Depending on what happens with class actions, there could be a lot more of those very soon (e.g., if we start seeing numerous different lawsuits challenging the same policy, and those suits produce inconsistent rulings). As busy as the shadow docket has been this term, apparently, Justice Kavanaugh is cool with it being even busier. But will his colleagues agree?But the nature of the Supreme Court’s ability to pick and choose which shadow docket issues it will—and will not—take up means that it can continue to be extremely partisan:One of the central problems with the Court’s approach to emergency applications in recent years has been its seeming inconsistency—granting emergency relief to Republican presidents or governors in very similar contexts to those in which it denied emergency relief to Democratic presidents or governors (in two immigration cases, for instance, the Court ruled for the Biden administration at the merits stage after denying applications to stay universal injunctions against the Biden policies).Do we expect the justices to all of a sudden be consistent when it comes to their resolution of emergency applications—especially if they’re getting more of them? Andwithout written explanations(the norm in such cases), how will we even know?The end result then appears to be the worst of all worlds. While there are reasonable arguments against nationwide injunctions, this Supreme Court chose to do this in the worst possible way, waiting until there was an extremist Republican in power who was breaking all the norms and rules regarding checks and balances, and refusing to actually deal with the underlying issues. Then, in its place, it puts forth a system which it—the Supreme Court alone—gets to decide which presidents have executive authority, and which ones don’t.And that, rather than the actual decision, is why this seems like it was a clearly partisan decision, which once again diminishes the legitimacy and respect for the judiciary.]]></content:encoded></item><item><title>SquareX Reveals That Employees Are No Longer The Weakest Link, Browser AI Agents Are</title><link>https://hackernoon.com/squarex-reveals-that-employees-are-no-longer-the-weakest-link-browser-ai-agents-are?source=rss</link><author>CyberNewswire</author><category>tech</category><pubDate>Mon, 30 Jun 2025 20:28:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Palo Alto, California, June 30th, 2025/CyberNewsWire/-Every security practitioner knows that employees are the weakest link in an organization, but this is no longer the case.’s research reveals that Browser AI Agents are more likely to fall prey to cyberattacks than employees, making them the new weakest link that enterprise security teams need to look out for.Browser AI Agents are software applications that act on behalf of users to access and interact with web content. Users can instruct these agents to automate browser-based tasks such as flight bookings, scheduling meetings, sending emails, and even simple research tasks.The productivity gains that Browser AI Agents provide make them an extremely compelling tool for employees and organizations alike. Indeed, a survey from PWC found that 79% of organizations have already adopted browser agents today.Yet, Browser AI Agents expose organizations to a massive security risk. These agents are trained to complete the tasks they are instructed to do, with little to no understanding of the security implications of their actions.Unlike human employees, Browser AI Agents are not subject to regular security awareness training. They cannot recognize visual warning signs like suspicious URLs, excessive permission requests, or unusual website designs that typically alert employees of a malicious site.Consequently, Browser AI Agents are more likely to fall prey to browser-based attacks than even a regular employee. Even if it is possible for users to add these guardrails, the overhead required to extensively write the security risk of every task performed by the agent in every prompt would probably outweigh the productivity gains.More importantly, employees using Browser AI Agents are unlikely to have enough security expertise to be able to write such a prompt in the first place.With the popular open-source Browser Use framework used by thousands of organizations, SquareX demonstrated how the Browser AI Agent, instructed to find and register for a file-sharing tool, succumbed to an OAuth attack.In the process of completing its task, it granted a malicious app complete access to the user's email despite multiple suspicious signals - irrelevant permissions, unfamiliar brands, suspicious URLs - that likely would have stopped most employees from granting these permissions.In other scenarios, these agents might expose the user's credit card information to a phishing site while trying to purchase groceries or disclose sensitive data when responding to emails from an impersonation attack.Unfortunately, neither browsers nor traditional security tools can differentiate between actions performed by users and these agents. Thus, it is critical for enterprises working with Browser AI Agents to provide browser-native guardrails that will prevent agents and employees alike from falling prey to these attacks.“ arrival of Browser AI Agents have dethroned employees as the weakest link within organizations. Optimistically, these agents have the security awareness of an average employee, making them vulnerable to even the most basic attacks, let alone bleeding-edge ones. Critically, these Browser AI Agents are running on behalf of the user, with the same privilege level to access enterprise resources. Until the day browsers develop native guardrails for Browser AI Agents, enterprises must incorporate browser-native solutions like Browser Detection and Response to prevent these agents from being tricked into performing malicious tasks. Eventually, the new generation of identity and access management tools will also have to take into account Browser AI Agent identities to implement granular access controls on agentic workflows.”SquareX’s research team is also holding a webinar on July 11, 10am PT/1pm ET to dive deeper into the research findings. To register, users can click .’s browser extension turns any browser on any device into an enterprise-grade secure browser. SquareX’s industry-first Browser Detection and Response (BDR) solution empowers organizations to proactively detect, mitigate, and threat-hunt client-side web attacks, including malicious browser extensions, advanced spearphishing, browser-native ransomware, genAI DLP, and more.Unlike legacy security approaches and cumbersome enterprise browsers, SquareX seamlessly integrates with users' existing consumer browsers, ensuring enhanced security without compromising user experience or productivity.By delivering unparalleled visibility and control directly within the browser, SquareX enables security leaders to reduce their attack surface, gain actionable intelligence, and strengthen their enterprise cybersecurity posture against the newest threat vector - the browser. Find out more on .:::tip
This story was published as a press release by Cybernewswire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Apple Weighs Using Anthropic or OpenAI To Power Siri in Major Reversal</title><link>https://apple.slashdot.org/story/25/06/30/1918231/apple-weighs-using-anthropic-or-openai-to-power-siri-in-major-reversal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 20:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple is considering using AI technology from Anthropic or OpenAI to power a new version of Siri, according to Bloomberg, sidelining its own in-house models in a potentially blockbuster move aimed at turning around its flailing AI effort. From the report: The iPhone maker has talked with both companies about using their large language models for Siri, according to people familiar with the discussions. It has asked them to train versions of their models that could run on Apple's cloud infrastructure for testing, said the people, who asked not to be identified discussing private deliberations. 

If Apple ultimately moves forward, it would represent a monumental reversal. The company currently powers most of its AI features with homegrown technology that it calls Apple Foundation Models and had been planning a new version of its voice assistant that runs on that technology for 2026. A switch to Anthropic's Claude or OpenAI's ChatGPT models for Siri would be an acknowledgment that the company is struggling to compete in generative AI -- the most important new technology in decades. Apple already allows ChatGPT to answer web-based search queries in Siri, but the assistant itself is powered by Apple.]]></content:encoded></item><item><title>Fintech Bolt progresses its turnaround by landing Klarna as a partner</title><link>https://techcrunch.com/2025/06/30/fintech-bolt-progresses-its-turnaround-by-landing-klarna-as-a-partner/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Mon, 30 Jun 2025 20:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Adding two big names as partners, Klarna and Palantir, is the kind of step that could help clean up Bolt’s reputation as it seeks to raise again.]]></content:encoded></item><item><title>Drug cartel hacked FBI official’s phone to track and kill informants, report says</title><link>https://arstechnica.com/security/2025/06/mexican-drug-cartel-hacked-fbi-officials-phone-to-track-informant-report-says/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Joaquin-El-Chapo-Guzman-1024x648.jpg" length="" type=""/><pubDate>Mon, 30 Jun 2025 19:57:49 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[The Sinaloa drug cartel in Mexico hacked the phone of an FBI official investigating kingpin Joaquín “El Chapo” Guzmán as part of a surveillance campaign “to intimidate and/or kill potential sources or cooperating witnesses,” according to a recently published report by the Justice Department.The report, which cited an “individual connected to the cartel,” said a hacker hired by its top brass “offered a menu of services related to exploiting mobile phones and other electronic devices.” The hired hacker observed “'people of interest' for the cartel, including the FBI Assistant Legal Attache, and then was able to use the [attache's] mobile phone number to obtain calls made and received, as well as geolocation data, associated with the [attache's] phone."“According to the FBI, the hacker also used Mexico City's camera system to follow the [attache] through the city and identify people the [attache] met with,” the heavily redacted report stated. “According to the case agent, the cartel used that information to intimidate and, in some instances, kill potential sources or cooperating witnesses.”]]></content:encoded></item><item><title>Instagram now lets you share Spotify songs with sound to Stories</title><link>https://techcrunch.com/2025/06/30/instagram-now-lets-you-share-spotify-songs-with-sound-to-stories/</link><author>Aisha Malik</author><category>tech</category><pubDate>Mon, 30 Jun 2025 19:48:24 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Instagram announced Monday it's finally adding the ability for users to listen to previews of songs they've shared from Spotify to Stories.]]></content:encoded></item><item><title>At TechCrunch All Stage: VC red flags, founder signals, and pre-seed traps — Charles Hudson will tell you what investors really see</title><link>https://techcrunch.com/2025/06/30/at-techcrunch-all-stage-vc-red-flags-founder-signals-and-pre-seed-traps-charles-hudson-will-tell-you-what-investors-really-see/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 30 Jun 2025 19:29:42 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Charles Hudson of Precursor Ventures will join the TechCrunch All Stage lineup to share what investors really see in early-stage startups. Register now.]]></content:encoded></item><item><title>Apple reportedly considers letting Anthropic and OpenAI power Siri</title><link>https://techcrunch.com/2025/06/30/apple-reportedly-considers-letting-anthropic-and-openai-power-siri/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Mon, 30 Jun 2025 19:26:20 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[While Siri can already call on ChatGPT for difficult questions, Apple now seems to be exploring a much deeper integration with technology from third-party AI providers.]]></content:encoded></item><item><title>Best iPad apps for unleashing and exploring your creativity</title><link>https://techcrunch.com/2025/06/30/best-ipad-apps-for-unleashing-and-exploring-your-creativity/</link><author>Aisha Malik</author><category>tech</category><pubDate>Mon, 30 Jun 2025 19:07:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[We’ve compiled a list of some of the best iPad apps for creativity that are available on the App Store. ]]></content:encoded></item><item><title>Surveillance (Against The) State: Doorbell Cam Owners Are Tipping People Off About ICE Raids</title><link>https://www.techdirt.com/2025/06/30/surveillance-against-the-state-doorbell-cam-owners-are-tipping-people-off-about-ice-raids/</link><author>Tim Cushing</author><category>tech</category><pubDate>Mon, 30 Jun 2025 19:01:46 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The cops certainly had fun partnering with Ring, the surveillance camera company now owned by Amazon. Ring handed out free cameras to cops, who handed out these cameras to citizens with the implicit expectation that they’d have warrantless access to camera footage whenever they wanted it.It didn’t quite work out that way. Lots of cop shops sold their souls to Ring, only to have Ring limit their access after the company received months of negative press over its incestuous relationship with law enforcement.Surveillance cameras are cheap and ubiquitous. Law enforcement agencies may have thought they were expanding their surveillance networks for free, but failed to realize a camera on every house means every house has a camera. And that sort of ubiquity doesn’t always work out in law enforcement’s favor, as the FBI pointed out a half-decade ago: The document describes a 2017 incident in which FBI agents approached a New Orleans home to serve a search warrant and were caught on video. “Through the Wi-Fi doorbell system, the subject of the warrant remotely viewed the activity at his residence from another location and contacted his neighbor and landlord regarding the FBI’s presence there,” it states.That’s the rub. Cameras installed for the purpose of protecting property from porch thieves and other miscreants are fully capable of capturing law enforcement officers in the act.Neighbors, an app for Ring doorbell users, is typically used by people looking for lost pets or missing packages. But last week, horrified by ICE raids in and around Los Angeles, residents started using the Amazon app to alert their communities to immigration agents carrying out searches and arrests.While social media sites and Nextdoor have been used to highlight ICE activity across the U.S. in recent days, Neighbors has been especially popular, with dozens of posts reviewed by Forbes over the last week.Welcome to the surveillance state, surveillance statists. Here’s how it feels to be on the other side of dozens of unblinking eyes. Your movements no longer go unnoticed. And when they  noticed, there are plenty of apps capable of spreading news of your actions .Even if ICE decides it’s not going to raid people’s houses (which it might, because the likelihood of a mass arrest is much lower there), it can’t escape cameras owned and operated by members of the public. People are looking out for each other now that the government can’t be trusted to obey laws or respect rights. Some posts had information on ICE agents near stores like Dollar Tree, McDonald’s, Starbucks and Target. Two alerted communities to ICE operation near elementary schools. If ICE doesn’t like this extra attention, it just needs to limit itself to chasing down actual felons or people suspected of committing serious crimes. But of course it won’t do that — not with an entire administration pushing it to arrest and deport as many foreigners as possible, even if that means arresting the occasional US citizen and/or dumping migrants into foreign concentration camps for the “crime” of being undocumented.The pushback is only going to increase. And nearly every person in the US is equipped with a camera, whether it’s guarding their front door or held in their hand as they confront this marauding gang of masked officers in unmarked vehicles who invade businesses and homes for the sole purpose of destroying lives.]]></content:encoded></item><item><title>Netflix teams up with NASA to boost its live TV offering</title><link>https://techcrunch.com/2025/06/30/netflix-teams-up-with-nasa-to-boost-its-live-tv-offering/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Mon, 30 Jun 2025 19:01:16 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Netflix and NASA announced a new partnership that will bring live space programming to the streaming service later this summer. ]]></content:encoded></item><item><title>a16z Thinks Controversial Startup Cluely Is the Future of AI</title><link>https://hackernoon.com/a16z-thinks-controversial-startup-cluely-is-the-future-of-ai?source=rss</link><author>Tech Thrilled</author><category>tech</category><pubDate>Mon, 30 Jun 2025 19:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Cluely, the startup promising to “cheat on everything,” just raised $15 million from a16z (Andreessen Horowitz). \n The company hasn’t launched a real product yet but is already making headlines with viral content. \n a16z partner Bryan Kim says matter more than polished products in today’s AI world. \n Cluely’s marketing may be controversial, but it’s working — and that’s the point. \n Meet Cluely: The Startup That’s Breaking the RulesCluely is a bold, new AI startup. It claims to help people “cheat” at everything — from dating to conversations — using artificial intelligence.Its messaging is intentionally provocative. The company grabbed attention by launching a video where its founder, , used AI to fake knowledge on a date.Many people called the idea unethical. Others just didn’t take it seriously.But venture capital giant Andreessen Horowitz (a16z) saw something different: a brilliant, high-speed attention machine that’s built for the AI era.Why a16z Invested Despite the BacklashAs soon as a16z’s  saw Cluely’s buzz online, he was intrigued. He hadn’t even met the founder yet, but he could tell the company was onto something.Here’s what Kim believes:“In AI, the product alone isn’t enough anymore. You need speed. You need momentum.”In the past, startups focused on crafting the perfect product. But now, in the fast-moving AI world, that strategy may not work.If OpenAI or another big player builds the same feature, your startup could instantly become irrelevant.So Kim flipped his thinking. What matters now, he says, is , both in building and in capturing attention.In a recent blog post and podcast episode, Kim explained his new startup theory:“For consumer AI startups, momentum is the moat.”That means instead of trying to slowly build a flawless product, founders should focus on generating interest and speed from day one.When he finally met Roy Lee and saw Cluely’s customer growth — even without a full product — Kim knew this was the kind of founder he’d been thinking about.Cluely’s Viral Playbook: Rage-Bait and Attention HackingSo how did Cluely make such a big splash without a working app?Lee’s answer: viral science.He studied how TikTok and Instagram promote posts. His takeaway?“Algorithms love controversy.”Instead of trying to sound smart or polished, Lee posts content designed to trigger debate — especially on  and .And it works. Every time Cluely posts something outrageous or attention-grabbing, the internet reacts. Some love it. Others hate it. But everyone .No Real Product Yet — But Still WinningHere’s the wildest part: Cluely didn’t even have a real product when it first launched back in April.The viral dating video? Mostly scripted.Still, that video and its bold claims got people talking. And as Lee puts it:“We’re earlier than most YC startups — and we’re getting more views than all of them.”That kind of momentum is rare. And for Kim and a16z, it’s exactly the kind of energy today’s AI startups need.So what is Cluely actually building?Lee hasn’t shared full details yet. But the official launch is set for Friday, June 27 — and expectations are high.By building buzz first, Cluely hopes to launch into a much larger and already engaged audience.Bryan Kim summed it up like this:“You have to build the plane while falling off the cliff.”That sounds risky — and it is. But in today’s fast-changing AI space, taking bold steps might be the only way to survive.Cluely is betting that speed, boldness, and attention will give it the edge — even before the tech is fully in place.Cluely raised $15 millionfrom Andreessen Horowitz before launching a full product. \n The startup uses controversial, viral marketingto stand out in a crowded AI space. \n a16z believes that momentum and speed matter more than polished productsin the AI era. \n Cluely’s founder, Roy Lee, says controversy triggers algorithms— and that’s intentional. \n The product launches officially on . Whether it lives up to the hype is yet to be seen. \n Cluely might be one of the most unconventional AI startups we’ve seen this year. It’s betting big on attention — and so is one of the world’s top VC firms.If it works, Cluely could become a blueprint for how AI startups launch in 2025 and beyond. If it crashes, it’ll still be a lesson in how far bold marketing can take you — even without a finished product.]]></content:encoded></item><item><title>VP.net Promises &quot;Cryptographically Verifiable Privacy&quot;</title><link>https://news.slashdot.org/story/25/06/30/1852224/vpnet-promises-cryptographically-verifiable-privacy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>Slashdot Staff</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[TorrentFreak spotlights VP.net, a brand-new service from Private Internet Access founder Andrew Lee (the guy who gifted Linux Journal to Slashdot) that eliminates the classic "just trust your VPN" problem by locking identity-mapping and traffic-handling inside Intel SGX enclaves.

The company promises 'cryptographically verifiable privacy' by using special hardware 'safes' (Intel SGX), so even the provider can't track what its users are up to.

 The design goal is that no one, not even the VPN company, can link "User X" to "Website Y."

 Lee frames it as enabling agency over one's privacy:

 "Our zero trust solution does not require you to trust us - and that's how it should be. Your privacy should be up to your choice - not up to some random VPN provider in some random foreign country."

The team behind VP.net includes CEO Matt Kim as well as arguably the first Bitcoin veterans Roger Ver and Mark Karpeles.

 Ask Slashdot: Now that there's a VPN where you don't have to "just trust the provider" - arguably the first real zero-trust VPN - are trust based VPNs obsolete?]]></content:encoded></item><item><title>WordPress CEO Regrets &apos;Belongs to Me&apos; Comment Amid Ongoing WP Engine Legal Battle</title><link>https://tech.slashdot.org/story/25/06/30/1825238/wordpress-ceo-regrets-belongs-to-me-comment-amid-ongoing-wp-engine-legal-battle?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Automattic CEO Matt Mullenweg said he regrets telling the media that "WordPress.org just belongs to me personally" during a new interview about his company's legal dispute with hosting provider WP Engine. The comment has been "taken out of context so many times" and represents "the worst thing ever," Mullenweg said in a new podcast interview with The Verge. 

The dispute began when Mullenweg accused WP Engine of "free-riding" on WordPress's open-source ecosystem without contributing adequate resources back to the project. Mullenweg filed a lawsuit against WP Engine while cutting off the company's access to core WordPress technologies. WP Engine countersued, and Automattic was forced to reverse some retaliatory measures. 

The controversy triggered significant internal upheaval at Automattic. The company offered "alignment" buyouts to employees who disagreed with the direction, reducing headcount from a peak of 2,100 to approximately 1,500 people. Mullenweg said this was "probably the fourth big time" WordPress has faced such community controversy, though the first in the current media landscape. WordPress powers 43% of websites globally. Mullenweg said he wants to return to "the most collaborative version of WordPress possible" but noted the legal proceedings continue with both sides spending "millions of dollars a month on lawyers."]]></content:encoded></item><item><title>Songscription launches an AI-powered ‘Shazam for sheet music’</title><link>https://techcrunch.com/2025/06/30/songscription-launches-an-ai-powered-shazam-for-sheet-music/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:41:19 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[This kind of product could be useful for a musician who records a song, then uploads it to get the sheet music -- that way, they can skip the step of having to manually transcribe their work. ]]></content:encoded></item><item><title>How an Open Model and a Pile of Data are Changing Time Series Analysis</title><link>https://hackernoon.com/how-an-open-model-and-a-pile-of-data-are-changing-time-series-analysis?source=rss</link><author>Reinforcement Technology Advancements</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:29:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[5. Conclusion and Future WorkWe release the first open-source family of time series foundation models and make contributions at all stages of the development and evaluation process. We first compile a large and diverse collection of public time series, called the Time Series Pile, and demonstrate its efficacy by pre-training high-performing time series foundation models from scratch. Then, we systematically address several time series-specific challenges, which up to now have impeded widespread exploration of extensivelarge-scale multi-dataset pre-training.\
We use the Time Series Pile and these strategies to pre-train transformer models of three different sizes. Finally, we design an experimental benchmark to evaluate time series foundation models on multiple practical time series tasks,\
particularly focusing on scenarios with constrained compute and supervision, building on prior work by Wu et al. (2023). Using this benchmark, we show that MOMENT is effective for the considered tasks with minimal fine-tuning. MOMENT’s superior performance, especially on anomaly detection and classification problems which typically have small datasets, can be attributed to pre-training. Moreover, we demonstrate that across many tasks, smaller statistical and shallower deep learning methods perform reasonably well. Lastly, we make several interesting empirical observations about time series foundation models. Our overarching goal is to push the boundaries of open science by publicly releasing the Time Series Pile, along with code, model weights, and training logs.\
We note several interesting directions of future work, including the application of MOMENT to real-world challenges, investigating multi-modal time series and text foundation models (Cai et al., 2023), and enhancing forecasting performance by pre-training MOMENT using causal attention and forecasting objectives.. This work was partially supported by the National Institutes of Health (NIH) under awards R01HL144692 and 1R01NS124642-01, and also by the U.S. Army Research Office and the U.S. Army Futures Command under Contract No. W911NF-20-D-0002. The content of the information does not necessarily reflect the position or the policy of the government and no official endorsement should be inferred.\
. We would like to express our sincerest gratitude to Barıs¸ Kurt, Andrey Kan, Laurent Callot, Gauthier Guinet, Jingchao Ni, and Jonas M. Kubler for insightful ¨ discussions regarding the problem setting and experimental design. Their unwavering support was instrumental in the development of MOMENT. We are also thankful to Laurent, Barıs¸, Jingchao and Andrey for their constructive feedback on the writing of this manuscript. Additionally, we acknowledge the insightful exchanges with Yuyang (Bernie) Wang, Abdul Fatir Ansari, Ingo Guering, Xiyuan Zhang, and Anoop Deoras. Special thanks to Cherie Ho for suggesting a creative and befitting name for our model. Lastly, we would like to thank Cecilia Morales for her insightful comments, especially on the broader impacts of this work, and for helping us proofread this manuscript.\
. We extend our gratitude to the authors and data curators whose meticulous efforts were instrumental in curating the datasets utilized for both pre-training and evaluation purposes: UCR Time Series Classification Archive (Dau et al., 2018), TSB-UAD Anomaly Benchmark (Paparrizos et al., 2022b), Monash Forecasting Archive (Godahewa et al., 2021), and the long-horizon forecasting datasets (Zhou et al., 2021).\
 Our training and evaluation library was inspired from Time-Series-Library. We would also like to thank the authors of the following libraries for their implementations: universal-computation, Anomaly-Transformer, VUS, tsad-model-selection, One-Fits-All and Statsforecast (Garza et al., 2022).Reproducibility statementAll models were trained and evaluated on a computing cluster consisting of 128 AMD EPYC 7502 CPUs, 503 GB of RAM, and 8 NVIDIA RTX A6000 GPUs each with 49 GiB RAM. All MOMENT variants were trained on a single A6000 GPU (with any data or model parallelism). We have made MOMENT-large[6] and the Time Series Pile[7] publicly available on Huggingface. We are working on opensourcing MOMENT-base and MOMENT-small, and our research code public. The latter is currently available anonymously at https://anonymous.4open.science/ r/BETT-773F/README.md. We enlist an exhaustive list of hyper-parameters in App. E to aid reproducibility. We would like to emphasize that all datasets used in this study are publicly available. Given the exponential rise in societal reliance on large foundation models, ensuring transparency in their training approach, architecture, and downstream application is crucial for public accountability, scientific advancement, and effective governance. o uphold this objective, we publicly release our training code base, data sources, and evaluation pipeline. We assess the transparency of MOMENT using the criteria outlined by Bommasani et al. (2023), focusing on upstream resources utilized during training and model description, encompassing 32 and 33 transparency indicators, respectively. We report expected upstream and model transparency scores for MOMENT in Tab. 34. Notably, MOMENT is expected to have one of the highest levels of upstream transparency. However, it’s model transparency scores are lower, primarily due to comprehensive (external and third-party) harm and trustworthiness evaluations, which are not well understood in the context of time series modeling.\
 We train multiple models over many days resulting in significant energy usage and a sizeable carbon footprint. However, we hope that releasing our models will ensure that future time series modeling efforts are quicker and more efficient, resulting in lower carbon emissions.\
We follow prior work (Bender et al., 2021; Patterson et al., 2021; Touvron et al., 2023; Wu et al., 2022; Dodge et al., 2022) and estimate the carbon footprint of pre-training all variants of MOMENT based on the GPU device used and the carbon efficiency of the electricity grid. Our estimated CO2 generation estimates are shown in Tab. 8.\
We use the Total Graphics Power (TGP) to calculate the total power consumed for training MOMENT models, although the total power consumed by the GPU will likely vary a little based on the GPU utilization while training our model. Our calculations do not account for power demands from other sources of our compute. We use 336.566 Kg C02/MWH as the standard value of CO2 emission per megawatt hour of energy consumed for Pittsburgh[8].\
We share an upper limit of the individual CO2 emission for each model, as well as a more realistic actual estimate for the carbon emissions from MOMENT-small and MOMENT-base, since they were trained simultaneously on a single Nvidia RTX A6000 GPU, and thus the power consumed by the GPU was shared for the training of both variants. MOMENT-large was trained independently on a single RTX A6000 GPU.\
Ethical considerations and potential misuse. Despite MOMENT’s promising performance in limited-data settings, it is important to use its predictions with care, especially in high-stakes settings such as healthcare. Before MOMENT is used for high-stakes decision-making, we recommend fine-tuning and evaluating the model with task-specific in-domain data.Ansari, A. F., Stella, L., Turkmen, C., Zhang, X., Mercado, P., Shen, H., Shchur, O., Rangapuram, S. S., Arango, S. P., Kapoor, S., et al. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024.\
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization, 2016.\
Bao, H., Dong, L., Piao, S., and Wei, F. BEit: BERT pretraining of image transformers. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=p-BhZSz59o4.\
Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, pp. 610–623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/ 3442188.3445922. URL https://doi.org/10. 1145/3442188.3445922.\
\
Bommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., Zhang, D., and Liang, P. The foundation model transparency index, 2023.\
Cai, Y., Goswami, M., Choudhry, A., Srinivasan, A., and Dubrawski, A. Jolt: Jointly learned representations of language and time-series. In Deep Generative Models for Health Workshop NeurIPS 2023, 2023.\
California Department of Transportation. Performance measurement system (pems), 2024. URL http://pems. dot.ca.gov/. Accessed: 2024-02-01.\
Cao, D., Jia, F., Arik, S. O., Pfister, T., Zheng, Y., Ye, W., and Liu, Y. Tempo: Prompt-based generative pre-trained transformer for time series forecasting, 2023.\
Centers for Disease Control and Prevention. Fluview: Flu activity & surveillance, 2024. URL https://gis.cdc.gov/grasp/fluview/ fluportaldashboard.html. Accessed: 2024-02- 01.\
Challu, C., Olivares, K. G., Oreshkin, B. N., Garza Ramirez, F., Mergenthaler Canseco, M., and Dubrawski, A. NHITS: Neural hierarchical interpolation for time series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 37(6):6989–6997, Jun. 2023. doi: 10.1609/ aaai.v37i6.25854. URL https://ojs.aaai.org/ index.php/AAAI/article/view/25854.\
Challu, C. I., Jiang, P., Nian Wu, Y., and Callot, L. Deep generative model with hierarchical latent factors for time series anomaly detection. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pp. 1643–1654. PMLR, 28–30 Mar 2022. URL https://proceedings.mlr.press/ v151/challu22a.html.\
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\
Cui, Z., Chen, W., and Chen, Y. Multi-scale convolutional neural networks for time series classification, 2016.\
Das, A., Kong, W., Sen, R., and Zhou, Y. A decoderonly foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688, 2023.\
Dau, H. A., Keogh, E., Kamgar, K., Yeh, C.-C. M., Zhu, Y., Gharghabi, S., Ratanamahatana, C. A., Yanping, Hu, B., Begum, N., Bagnall, A., Mueen, A., Batista, G., and Hexagon-ML. The ucr time series classification archive, October 2018. https://www.cs.ucr. edu/˜eamonn/timedata_2018/.\
Day, K., Christl, D., Salvi, R., and Sriram, P. Video pretrained transformer: A multimodal mixture of pre-trained experts, 2023.\
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\
Dodge, J., Prewitt, T., Tachet des Combes, R., Odmark, E., Schwartz, R., Strubell, E., Luccioni, A. S., Smith, N. A., DeCario, N., and Buchanan, W. Measuring the carbon intensity of ai in cloud instances. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22, pp. 1877–1894, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393522. doi: 10.1145/ 3531146.3533234. URL https://doi.org/10. 1145/3531146.3533234.\
Dong, J., Wu, H., Zhang, H., Zhang, L., Wang, J., and Long, M. Simmtm: A simple pre-training framework for masked time-series modeling. In Advances in Neural Information Processing Systems, 2023.\
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=YicbFdNTTy.\
Ekambaram, V., Jati, A., Nguyen, N. H., Dayama, P., Reddy, C., Gifford, W. M., and Kalagnanam, J. Tiny time mixers (ttms): Fast pre-trained models for enhanced zero/fewshot forecasting of multivariate time series, 2024.\
Eldele, E., Ragab, M., Chen, Z., Wu, M., Kwoh, C. K., Li, X., and Guan, C. Time-series representation learning via temporal and contextual contrasting. In Zhou, Z.- H. (ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 2352– 2359. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/ 324. URL https://doi.org/10.24963/ijcai. 2021/324. Main Track.\
Franceschi, J.-Y., Dieuleveut, A., and Jaggi, M. Unsupervised scalable representation learning for multivariate time series. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. ´ (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 53c6de78244e9f528eb3e1cda69699bb-Paper. pdf.\
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The pile: An 800gb dataset of diverse text for language modeling, 2020.\
Garza, A. and Mergenthaler-Canseco, M. Timegpt-1. arXiv preprint arXiv:2310.03589, 2023.\
Garza, F., Mergenthaler Canseco, M., Challu, C., and ´ Olivares, K. StatsForecast: Lightning fast forecasting with statistical and econometric models. PyCon Salt Lake City, Utah, US 2022, 2022. URL https: //github.com/Nixtla/statsforecast.\
Godahewa, R. W., Bergmeir, C., Webb, G. I., Hyndman, R., and Montero-Manso, P. Monash time series forecasting archive. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview. net/forum?id=wEc1mgAjU-.\
Goswami, M., Boecking, B., and Dubrawski, A. Weak supervision for affordable modeling of electrocardiogram data. In AMIA Annual Symposium Proceedings, volume 2021, pp. 536. American Medical Informatics Association, 2021.\
Goswami, M., Challu, C. I., Callot, L., Minorics, L., and Kan, A. Unsupervised model selection for time series anomaly detection. In The Eleventh International Conference on Learning Representations, 2023a. URL https: //openreview.net/forum?id=gOZ_pKANaPW.\
Goswami, M., Sanil, V., Choudhry, A., Srinivasan, A., Udompanyawit, C., and Dubrawski, A. AQua: A benchmarking tool for label quality assessment. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023b. URL https://openreview.net/forum? id=dhJ8VbcEtX.\
Gruver, N., Finzi, M. A., Qiu, S., and Wilson, A. G. Large language models are zero-shot time series forecasters. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=md68e8iZK1.\
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.\
Hundman, K., Constantinou, V., Laporte, C., Colwell, I., and Soderstrom, T. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 387– 395, 2018.\
Ismail Fawaz, H., Forestier, G., Weber, J., Idoumghar, L., and Muller, P.-A. Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4):917–963, 2019.\
Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., and Wen, Q. Time-llm: Time series forecasting by reprogramming large language models, 2023.\
Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=cGDAkQo1C0p.\
Lai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling longand short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR ’18, pp. 95–104, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450356572. doi: 10.1145/3209978.3210006. URL https://doi. org/10.1145/3209978.3210006.\
Le Guennec, A., Malinowski, S., and Tavenard, R. Data Augmentation for Time Series Classification using Convolutional Neural Networks. In ECML/PKDD Workshop on Advanced Analytics and Learning on Temporal Data, Riva Del Garda, Italy, September 2016. URL https: //shs.hal.science/halshs-01357973.\
Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023a.\
Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural information processing systems, 32, 2019.\
Li, Y., Fan, H., Hu, R., Feichtenhofer, C., and He, K. Scaling language-image pre-training via masking. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 23390–23400, Los Alamitos, CA, USA, jun 2023b. IEEE Computer Society. doi: 10.1109/CVPR52729.2023.02240. URL https://doi.ieeecomputersociety.org/ 10.1109/CVPR52729.2023.02240.\
Li, Z., Rao, Z., Pan, L., Wang, P., and Xu, Z. Ti-mae: Self-supervised masked time series autoencoders, 2023c.\
Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and Dustdar, S. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=0EXmFzUn5I.\
Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.\
Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=Bkg6RiCqY7.\
Lu, K., Grover, A., Abbeel, P., and Mordatch, I. Frozen pretrained transformers as universal computation engines. Proceedings of the AAAI Conference on Artificial Intelligence, 36(7):7628–7636, Jun. 2022. doi: 10.1609/ aaai.v36i7.20729. URL https://ojs.aaai.org/ index.php/AAAI/article/view/20729.\
Ma, Q., Liu, Z., Zheng, Z., Huang, Z., Zhu, S., Yu, Z., and Kwok, J. T. A survey on time-series pre-trained models, 2023.\
Max Planck Institute for Biogeochemistry. Weather data, 2024. URL https://www.bgc-jena.mpg.de/ wetter/. Accessed: 2024-02-01.\
Narwariya, J., Malhotra, P., Vig, L., Shroff, G., and Vishnu, T. V. Meta-learning for few-shot time series classification. In Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, CoDS COMAD 2020, pp. 28–36, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450377386. doi: 10. 1145/3371158.3371162. URL https://doi.org/ 10.1145/3371158.3371162.\
Nie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=Jbdc0vTOcol.\
Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=r1ecqn4YwB.\
Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. Meta-learning framework with applications to zero-shot time-series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 35(10): 9242–9250, May 2021. doi: 10.1609/aaai.v35i10. 17115. URL https://ojs.aaai.org/index. php/AAAI/article/view/17115.\
Paparrizos, J., Boniol, P., Palpanas, T., Tsay, R. S., Elmore, A., and Franklin, M. J. Volume under the surface: A new accuracy evaluation measure for timeseries anomaly detection. Proc. VLDB Endow., 15(11): 2774–2787, jul 2022a. ISSN 2150-8097. doi: 10. 14778/3551793.3551830. URL https://doi.org/ 10.14778/3551793.3551830.\
Paparrizos, J., Kang, Y., Boniol, P., Tsay, R. S., Palpanas, T., and Franklin, M. J. Tsb-uad: An end-to-end benchmark suite for univariate time-series anomaly detection. Proc. VLDB Endow., 15(8):1697–1711, apr 2022b. ISSN 2150- 8097. doi: 10.14778/3529337.3529354. URL https: //doi.org/10.14778/3529337.3529354.\
Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.- M., Rothchild, D., So, D., Texier, M., and Dean, J. Carbon emissions and large neural network training, 2021.\
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/ v139/radford21a.html.\
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html.\
Ramaswamy, S., Rastogi, R., and Shim, K. Efficient algorithms for mining outliers from large data sets. In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, SIGMOD ’00, pp. 427–438, New York, NY, USA, 2000. Association for Computing Machinery. ISBN 1581132174. doi: 10.1145/342009.335437. URL https://doi.org/ 10.1145/342009.335437.\
Rasul, K., Ashok, A., Williams, A. R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., Bilos, M., Ghonia, H., ˇ Hassen, N. V., Schneider, A., et al. Lag-llama: Towards foundation models for time series forecasting. arXiv preprint arXiv:2310.08278, 2023.\
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.\
Schmidl, S., Wenig, P., and Papenbrock, T. Anomaly detection in time series: A comprehensive evaluation. Proc. VLDB Endow., 15(9):1779–1797, may 2022. ISSN 2150- 8097. doi: 10.14778/3538598.3538602. URL https: //doi.org/10.14778/3538598.3538602.\
Schneider, S. H. and Dickinson, R. E. Climate modeling. Reviews of Geophysics, 12(3):447–493, 1974.\
Serra, J., Pascual, S., and Karatzoglou, A. Towards a ` universal neural network encoder for time series. In International Conference of the Catalan Association for Artificial Intelligence, 2018. URL https://api. semanticscholar.org/CorpusID:13675490.\
Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.\
Shen, J., Li, L., Dery, L. M., Staten, C., Khodak, M., Neubig, G., and Talwalkar, A. Cross-modal fine-tuning: Align then refine, 2023.\
Smith, L. N. and Topin, N. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multidomain operations applications, volume 11006, pp. 369– 386. SPIE, 2019.\
Su, Y., Zhao, Y., Niu, C., Liu, R., Sun, W., and Pei, D. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2828–2837, 2019.\
Talukder, S., Yue, Y., and Gkioxari, G. Totem: Tokenized time series embeddings for general time series analysis, 2024.\
Tanisaro, P. and Heidemann, G. Time series classification using time warping invariant echo state networks. In 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 831–836, 2016. doi: 10.1109/ICMLA.2016.0149.\
Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261–24272, 2021.\
Tonekaboni, S., Eytan, D., and Goldenberg, A. Unsupervised representation learning for time series with temporal neighborhood coding. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=8qDwejCuCN.\
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023.\
Trindade, A. ElectricityLoadDiagrams20112014. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C58C86.\
Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.\
van der Maaten, L. Accelerating t-sne using tree-based algorithms. Journal of Machine Learning Research, 15 (93):3221–3245, 2014. URL http://jmlr.org/ papers/v15/vandermaaten14a.html.\
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf.\
Wang, Z., Yan, W., and Oates, T. Time series classification from scratch with deep neural networks: A strong baseline. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585, 2017. doi: 10.1109/IJCNN.2017.7966039.\
Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. Transformers in time series: A survey. In Elkind, E. (ed.), Proceedings of the ThirtySecond International Joint Conference on Artificial Intelligence, IJCAI-23, pp. 6778–6786. International Joint Conferences on Artificial Intelligence Organization, 8 2023. doi: 10.24963/ijcai.2023/759. URL https: //doi.org/10.24963/ijcai.2023/759. Survey Track.\
Woo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and Sahoo, D. Unified training of universal time series forecasting transformers. arXiv preprint arXiv:2402.02592, 2024.\
Wu, C.-J., Raghavendra, R., Gupta, U., Acun, B., Ardalani, N., Maeng, K., Chang, G., Behram, F. A., Huang, J., Bai, C., Gschwind, M., Gupta, A., Ott, M., Melnikov, A., Candido, S., Brooks, D., Chauhan, G., Lee, B., Lee, H.-H. S., Akyildiz, B., Balandat, M., Spisak, J., Jain, R., Rabbat, M., and Hazelwood, K. Sustainable ai: Environmental implications, challenges and opportunities, 2022.\
Wu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition transformers with auto-correlation for longterm series forecasting. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=I55UqU-M11y.\
Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=ju_Uqw384Oq.\
Wu, R. and Keogh, E. J. Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress. IEEE Transactions on Knowledge & Data Engineering, 35(03):2421–2429, mar 2023. ISSN 1558- 2191. doi: 10.1109/TKDE.2021.3112126.\
Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., and Hu, H. Simmim: a simple framework for masked image modeling. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9643– 9653, 2022. doi: 10.1109/CVPR52688.2022.00943.\
Xu, H., Chen, W., Zhao, N., Li, Z., Bu, J., Li, Z., Liu, Y., Zhao, Y., Pei, D., Feng, Y., et al. Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications. In Proceedings of the 2018 world wide web conference, pp. 187–196, 2018.\
Xu, J., Wu, H., Wang, J., and Long, M. Anomaly transformer: Time series anomaly detection with association discrepancy. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=LzQQ89U1qm_.\
Yue, Z., Wang, Y., Duan, J., Yang, T., Huang, C., Tong, Y., and Xu, B. Ts2vec: Towards universal representation of time series. Proceedings of the AAAI Conference on Artificial Intelligence, 36(8):8980–8987, Jun. 2022. doi: 10. 1609/aaai.v36i8.20881. URL https://ojs.aaai. org/index.php/AAAI/article/view/20881.\
Zebik, M., Korytkowski, M., Angryk, R., and Scherer, R. Convolutional Neural Networks for Time Series Classification, pp. 635–642. Springer International Publishing, Cham, 2017. ISBN 978-3-319-59060-8. doi: 10.1007/978-3-319-59060-8 57. URL https://doi. org/10.1007/978-3-319-59060-8_57.\
Zerveas, G., Jayaraman, S., Patel, D., Bhamidipaty, A., and Eickhoff, C. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, KDD ’21,pp. 2114–2124, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383325. doi: 10.1145/3447548.3467401. URL https://doi. org/10.1145/3447548.3467401.\
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):11106–11115, May 2021. doi: 10.1609/ aaai.v35i12.17325. URL https://ojs.aaai.org/ index.php/AAAI/article/view/17325.\
Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th International Conference on Machine Learning (ICML 2022), 2022.\
Zhou, T., Niu, P., Wang, X., Sun, L., and Jin, R. One fits all: Power general time series analysis by pretrained LM. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=gMS6FVZvmF.(1) Mononito Goswami, Auton Lab, Robotics Insititute, Carnegie Mellon University, Pittsburgh, USA (mgoswami@andrew.cmu.edu)(2) Konrad Szafer, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(3) Arjun Choudhry, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(4) Yifu Cai, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA;(5) Shuo Li, University of Pennsylvania, Philadelphia, USA;(6) Artur Dubrawski, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA.[6] https://huggingface.co/AutonLab/ MOMENT-1-large\
[7] https://huggingface.co/datasets/ AutonLab/Timeseries-PILE\
[8] https://emissionsindex.org/]]></content:encoded></item><item><title>From $5 to financial empowerment: Why Stash co-founder Brandon Krieg is a must-see at TechCrunch All Stage 2025</title><link>https://techcrunch.com/2025/06/30/from-5-to-financial-empowerment-why-stash-co-founder-brandon-krieg-is-a-must-see-at-techcrunch-all-stage-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:29:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Startup life is tough — funding rounds, team building, and managing finances can feel like trying to solve a Rubik’s Cube blindfolded. That’s exactly why TechCrunch All Stage 2025 exists: to give founders like you the tools, tactics, and real talk from those who’ve been through it and built something lasting. If there’s one session […]]]></content:encoded></item><item><title>When a Specialized Time Series Model Outshines General LLMs</title><link>https://hackernoon.com/when-a-specialized-time-series-model-outshines-general-llms?source=rss</link><author>Reinforcement Technology Advancements</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:16:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[4. Experimental Setup and ResultsWe extend the experimental benchmark introduced by Wu et al. (2023) across various dimensions. Below, we outline the design choices of our benchmark and highlight its key distinctions from TimesNet[5].\
Time series modeling with limited supervision. Our benchmark comprises of 5 major time series modeling tasks of significant practical value, namely long- and shorthorizon forecasting, imputation, classification, and anomaly detection, as outlined in Tab. 1. In contrast to TimesNet, we exclusively consider scenarios characterized by limited compute and supervision resources. These scenarios mimic practical situations where training (or fine-tuning) a deep neural network is infeasible due to resource limitations or insufficiently characterized data. Accordingly, we assess MOMENT in zero-shot settings whenever feasible and through linear probing for a few epochs otherwise.\
For classification, we consider the unsupervised representation learning problem, where the goal is to learn representations of time series that are useful for downstream classification, without access to labeled data. As is common in prior work (Yue et al., 2022; Franceschi et al., 2019), the quality of representations is measured using the accuracy of a Support Vector Machine trained on them (App. E.2). For short-horizon forecasting, we consider the zero-shot setting introduced by Oreshkin et al. (2021). In particular, we finetune MOMENT on a source dataset using a forecasting head,\
and evaluate its performance on a target dataset without any fine-tuning (App E.1.2, Tab. 21).\
. We use the same datasets as TimesNet for forecasting and imputation. However, for classification and anomaly detection, we conduct experiments on larger and systematically chosen subset of datasets from the UCR classification archive (Dau et al., 2018) and UCR anomaly archive (Wu & Keogh, 2023). Specifically, we run classification experiments on all 91 time series datasets with each time series shorter than 512 time steps (Tab.23). For anomaly detection, while choosing the subset of time series, we prioritized coverage over different domains and data sources represented in the UCR anomaly archive (Tab. 22). We also note that the UCR anomaly archive was proposed as an improvement over pre-existing anomaly detection datasets such as the SMD (Su et al., 2019), and SMAP (Hundman et al., 2018), many of which are also used in TimesNet. Our proposed experimental setup is summarized in Tab. 1 and detailed in App. E.\
. We evaluate each experiment using multiple metrics used in task-specific benchmarks, such as MSE and MAE for long-horizon forecasting, and sMAPE for short-horizon forecasting. We also note that TimesNet and GPT4TS (Zhou et al., 2023) evaluate anomaly detection performance using vanilla F1 score which ignores the sequential nature of time series. Instead, we measure anomaly detection performance with the widely used adjusted best F1 score (Goswami et al., 2023a; Challu et al., 2022), and the recently proposed VUS-ROC (Paparrizos et al., 2022a).\
. We compare MOMENT with state-of-the-art deep learning and statistical machine learning models across tasks (Tab. 35). This is in contrast to TimesNet which primarily compared with transformer-based approaches. These comparisons are crucial for assessing the practical utility of the proposed methods. We found that statistical and non-transformer-based approaches like ARIMA for shorthorizon forecasting, N-BEATS for long-horizon forecasting, and k-nearest neighbors for anomaly detection outperform many deep and transformer-based models.\
 We do not perform hyperparameter tuning. In all experiments that follow, unless mentioned otherwise, we fine-tune MOMENT-Large with a batch size of 64, and one cycle learning rate schedule with a peak learning rate between 5e − 5 and 1e − 3 (Smith & Topin, 2019). For baseline methods, we capture recommended settings from their papers and public repositories. We report all hyper-parameters settings for MOMENT and baselines in App. E.\
 Through the following experiments we aim to answer 3 broad research questions.\
 Is MOMENT effective for multiple time series analysis tasks in limited supervision settings?\
 What is MOMENT learning? Does it capture intuitive time series characteristics such as varying frequencies, trends, and amplitudes?\
 What is the impact of the size of scaling model size? Can MOMENT, akin to LLMs, be used for crossmodal transfer learning?4.1. MOMENT can solve multiple time series modeling tasks in limited supervision settingsLong-horizon forecasting. Linearly probing MOMENT achieves near state-of-the-art performance on most datasets and horizons, and is only second to PatchTST which generally achieves the lowest MSE (Tab. 2). On many datasets and horizons, forecasting models based on LLMs– TimeLLM and GPT4TS perform worse than MOMENT. Notably, NBEATS outperforms several recent methods, emphasizing the importance of comparing forecasting performance beyond transformer-based approaches.\
Zero-shot short-horizon forecasting. Among all tasks, we found zero-shot short-horizon forecasting to have the largest scope for improvement (Tab. 3). Statistical methods such as Theta and ETS outperformed their deeper counterparts. However, on some datasets, MOMENT achieved lower sMAPE than ARIMA.\
. Without any data-specific fine-tuning, MOMENT can learn distinct representations for different classes of data (Fig. 5), and an SVM trained on its representations performs better than all but 4 methods specifically built for time series classification models and trained on each individual dataset. Recently proposed GPT4TS and TimesNet perform poorly despite being trained on each individual dataset with labels.\
 On 44 time series from the UCR anomaly detection archive, MOMENT consistently outperformed both TimesNet and GPT4TS, as well as 2 state-ofthe-art deep learning models tailored for anomaly detection, in both zero-shot and linear probing configurations. However, k-nearest neighbors performed marginally better in terms of VUS-ROC score, but had a lower adjusted best F1 score.\
. Tab. 6 contains imputation performance of all models averaged over 4 different masking rates. MOMENT with linear probing achieved the lowest reconstruction error on all ETT datasets. In the zero-shot setting, MOMENT consistently outperformed all statistical interpolation methods with the exception of linear interpolation.4.2. What is MOMENT Learning?We found that MOMENT can capture changes in intuitive time series characteristics such as trend, amplitude, frequencies, and phases of time series. However, it cannot differentiate between vertically shifted time series as it normalizes each signal prior to modeling (Fig. 4,7). Furthermore, on many classification datasets, MOMENT learns distinct representations of different classes, even in a zero-shot setting without access to labels (Fig. 5, 8).4.3. Properties of Large Time Series ModelsModel scaling improves training loss. Like LLMs, we found that increasing the size of the model leads to lower training loss, even before the first epoch (Fig. 6, left). An immediate next step is to assess how effectively this phenomenon extends to time series modeling tasks under limited supervision.\
MOMENT can solve cross-modal sequence learning tasks. Lu et al. (2022) first showed that large pre-trained language and vision transformers can solve general sequence learning tasks for modalities outside of text and images with minimal fine-tuning. Several recent studies have leveraged these properties to reprogram LLMs for time series tasks. We explore whether transformers pre-trained on time series can also be used to solve sequence classification tasks on image, text, and binary data. Our results confirm that by freezing the self-attention and feed-forward layers, MOMENT can model sequences comparable to GPT-2 and Flan-T5 models of similar scale (Tab. 5).\
MOMENT with randomly initialized weights converges to a lower training loss. Our observations suggest that with sufficient data, pre-training our model from scratch results in a lower training loss than continually pre-training a model of similar size initialized with language modeling weights (Fig. 6, 12). This also underscores that there is sufficient publicly accessible pre-training data available in the Time Series Pile to facilitate pre-training time series foundation models from scratch.(1) Mononito Goswami, Auton Lab, Robotics Insititute, Carnegie Mellon University, Pittsburgh, USA (mgoswami@andrew.cmu.edu)(2) Konrad Szafer, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(3) Arjun Choudhry, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(4) Yifu Cai, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA;(5) Shuo Li, University of Pennsylvania, Philadelphia, USA;(6) Artur Dubrawski, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA.[5] In this section, we use TimesNet to refer to the benchmark proposed by Wu et al. (2023) instead of their model.]]></content:encoded></item><item><title>US government takes down major North Korean ‘remote IT workers’ operation</title><link>https://techcrunch.com/2025/06/30/us-government-takes-down-major-north-korean-remote-it-workers-operation/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:13:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[US prosecutors indicated a total of 13 people involved in the fraudulent scheme to steal and launder money for North Korea’s nuclear weapons program.]]></content:encoded></item><item><title>How Do You Train an AI to Understand Time? With a Giant Pile of Data.</title><link>https://hackernoon.com/how-do-you-train-an-ai-to-understand-time-with-a-giant-pile-of-data?source=rss</link><author>Reinforcement Technology Advancements</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:09:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We first collect a large number of public time series data into the  and then use it to pre-train a  on the masked time series prediction task. We discuss each of these steps in the following sections.3.1. The Time Series PileUnlike natural language processing and computer vision, where large-scale datasets such as The Pile (Gao et al.,2020), and ImageNet-1K (Russakovsky et al., 2015) are easily available for pre-training, public time series datasets are much smaller, scattered, and largely task-specific (Ma et al., 2023; Zhou et al., 2023; Gruver et al., 2023). To bridge this gap, we collate multiple time series from 4 taskspecific, widely-used public repositories resulting in a large number of time series spanning diverse domains, and time series characteristics such as lengths, amplitudes, and temporal resolutions. We call this collection the Time Series Pile.\
Informer long-horizon forecasting datasets (Zhou et al., 2021) is a collection of 9 datasets that are widely used to evaluate long-horizon forecasting performance (Wu et al., 2023; Nie et al., 2023; Challu et al., 2023): 2 hourly and minutely subsets of the Electricity Transformer Temperature (ETT) (Zhou et al., 2021), Electricity (Trindade, 2015), Traffic (California Department of Transportation, 2024), Weather (Max Planck Institute for Biogeochemistry, 2024), Influenza-like Illness (ILI) (Centers for Disease Control and Prevention, 2024), and Exchange-rate (Lai et al., 2018).\
Monash time series forecasting archive (Godahewa et al., 2021) is a collection of 58 publicly available short-horizon forecasting datasets with a total of over 100K time series, spanning a variety of domains and temporal resolutions.\
UCR/UEA classification archive (Dau et al., 2018) comprises of 159 time series datasets which are frequently used to benchmark classification algorithms (Ismail Fawaz et al., 2019). These datasets belonging to seven different categories (Image Outline, Sensor Readings, Motion Capture, Spectrographs, ECG, Electric Devices, and Simulated Data), vary substantially in terms of the number of classes and the size of the training set.\
TSB-UAD anomaly benchmark (Paparrizos et al., 2022b) is a recent collection of 1980 univariate time series with labeled anomalies from 18 anomaly detection datasets proposed over the past decade. This collection includes both synthetic and real-world time series originating from a wide range of sources such as the human body, spaceships, environment, and web serves.\
Minimizing data contamination using careful train-test splitting. We carefully split each dataset into disjoint training, validation, and test splits, based on splits specified by data creators. When these splits are not available, we randomly sample 60% of the data for training, 10% for validation, and 30% for testing. Long-horizon forecasting and anomaly detection datasets are typically long time series, which are split horizontally as shown in Fig. 2. Conversely, short-horizon forecasting and classification datasets often contain multiple short time series. For these datasets, a complete time series is either training, validation, or testing. We use the same random seed, set to 13, throughout our experiments, from pre-training to downstream evaluation, thus ensuring that MOMENT only observes the training splits of datasets during pre-training.\
 Our transformer encoder retains the modifications proposed by Raffel et al. (2020) to the original Transformer (Vaswani et al., 2017). Specifically, we remove the additive bias from the Layer Norm (Ba et al., 2016), and place it before the residual skip connections (He et al., 2016), and use the relation positional embedding scheme (Shaw et al., 2018). Below we summarize the intuition behind some of our key design decisions.\
Handling varying time series characteristics. Time series vary in length, number of channels, amplitudes, and temporal resolutions. We address variable length by restricting MOMENT’s input to a univariate time series of a fixed length T = 512. As is common practice, we sub-sample longer time series, and pad shorter ones with zeros on the left[2]. Moreover, segmenting time series into patches quadratically reduces MOMENT’s memory footprint and computational complexity, and linearly increases the length of time series it can take as input. We handle multi-variate time series by independently operating on each channel along the batch dimension. Like recent studies (Zhou et al., 2023; Nie et al., 2023), we found that modeling each channel independently is an effective strategy for modeling multivariate time series. Finally, re-scaling and centering time series using reversible instance normalization enables MOMENT to model time series with significantly different temporal distributions (Kim et al., 2022). We did not explicitly model the temporal resolution of time series, since this information is often unavailable outside of time series forecasting datasets.\
Intentionally simple encoder. Closely following the design of transformers in the language domain allows us to leverage their scalable and efficient implementations (e.g., gradient checkpointing, mixed precision training).\
Light-weight prediction head. We use a lightweight prediction head instead of a decoder of the same size as the encoder, to enable the necessary architectural modifications for task-specific fine-tuning of a limited number of trainable parameters while keeping the bulk of parameters and the high-level features learned by the encoder intact.3.3. Pre-training using Masked Time series ModelingWe pre-train MOMENT using the masked time series modeling task. Fig. 3 presents an overview of our pre-training procedure. During training, we first mask a small number of patches uniformly at random by replacing their patch embeddings with a learnable mask embedding [MASK]. The corrupted time series patches are then fed into the transformer encoder to learn patch representations, which are used to reconstruct the original time series using a lightweight reconstruction head. The pre-training objective is to minimize the masked reconstruction error i.e. the Mean Squared Error between the ground truth and the prediction, averaged over the masked patches.\
 We pre-train three different sizes of MOMENT, roughly corresponding to the sizes of encoders in T5-Small, Base, and Large. Specifically, the Base (Small, Large) model uses a 12 (6, 24) layer Transform with hidden dimensions of size D = 768 (512, 1024), 12\
(8, 16) attention heads, and feed-forward networks of size 3072 (2048, 4096), resulting in approximately 125 (40, 385) million parameters. All weights are randomly initialized before pre-training. All models take an input time series of length T = 512, breaking it into N = 64 disjoint patches of length P = 8. We mask 30% of the patches uniformly at random during pre-training.3.4. Fine-tuning on Downstream TasksMOMENT can be seamlessly used for multiple time series analysis tasks. In this work, we consider 5 practical time series analysis tasks as examples, namely: long- and shorthorizon forecasting, classification, anomaly detection, and imputation. For forecasting tasks with horizon H, we replace the reconstruction head with a forecasting head, which first flattens all the N D-dimensional patch embeddings into a N × D dimensional vector, and then projects it into a Hdimensional time series via a linear projection layer. For all other tasks, we retain the reconstruction head. We provide detailed descriptions of each task and MOMENT’s configuration in App. E.(1) Mononito Goswami, Auton Lab, Robotics Insititute, Carnegie Mellon University, Pittsburgh, USA (mgoswami@andrew.cmu.edu)(2) Konrad Szafer, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(3) Arjun Choudhry, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(4) Yifu Cai, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA;(5) Shuo Li, University of Pennsylvania, Philadelphia, USA;(6) Artur Dubrawski, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA.[2] We found a large majority of classification datasets to have time series shorter than 512. Besides, a look-back window of length 512 was found to be sufficient for accurate long-horizon forecasting (Nie et al., 2023).\
[4] https://cloud.google.com/tpu/docs/ bfloat16]]></content:encoded></item><item><title>In China, Coins and Banknotes Have All But Disappeared</title><link>https://slashdot.org/story/25/06/30/188259/in-china-coins-and-banknotes-have-all-but-disappeared?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:08:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China's transition to digital payments has reached the point where physical cash has nearly vanished from daily commerce, with WeChat and Alipay now handling transactions from supermarkets to public transportation across the world's second-largest economy. Many businesses no longer maintain traditional cash registers and instead scan QR codes presented by customers, while numerous taxis refuse cash payments entirely. 

The widespread adoption has given tech giants Tencent and Alibaba immense power over routine financial transactions, prompting China's central bank to develop a competing digital yuan currency.]]></content:encoded></item><item><title>Why Training on Time Series Beats Fine-Tuning LLMs for Time Series Tasks</title><link>https://hackernoon.com/why-training-on-time-series-beats-fine-tuning-llms-for-time-series-tasks?source=rss</link><author>Reinforcement Technology Advancements</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:04:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Transformers and patching for time series modeling. There is a growing body of work utilizing transformers for various time series analysis tasks (Wen et al., 2023). One issue with applying transformers to time series data is the complexity of the self-attention mechanism, which grows quadratically with the size of input tokens (or length of time series) (Li et al., 2019). Nie et al. (2023) demonstrated that treating time series sub-sequences (or patches) as tokens instead of individual time points is a simple, efficient, and effective mechanism for learning useful representations for forecasting. Drawing inspiration from prior work, we build on top of the transformer architecture which takes disjoint time series sub-sequences (or patches) as input.\
Masked Representation Learning. Masked pre-training is a widely-used self-supervised learning task where a model learns to accurately reconstruct masked portions of its input. Masked language (Devlin et al., 2019; Raffel et al., 2020) and image modeling (Xie et al., 2022; Li et al., 2023b) have been successfully utilized to learn models from vast quantities of unlabeled data, which can generalize to a variety of downstream tasks.\
For time series data, prior work has primarily focused on contrastive representation learning (Yue et al., 2022; Eldele et al., 2021; Franceschi et al., 2019). However, contrastive learning relies on data augmentation, which is both subjective and data-dependent. In contrast, some studies mask portions of time series using zeros and learn a model to reconstruct them (Nie et al., 2023; Zerveas et al., 2021; Dong et al., 2023; Li et al., 2023c).\
Representation learning via masking is well-suited to all the downstream tasks we care about, especially forecasting and imputation, as they are instances of the masked reconstruction problem. Due to its simplicity and success in vision and language domains, we use the masked prediction task to pretrain our model, using a special embedding (see [MASK] in Fig. 3) to mask time series patches instead of zeros.\
Cross-modal transfer learning using language models. Lu et al. (2022) had first shown that transformers pre-trained on text data (LLMs) can effectively solve sequence modeling tasks in other modalities. Subsequently, Shen et al. (2023) introduced ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single largescale pretrained model to diverse modalities by adapting to a target task via an align-then-refine workflow. Given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality, then the pretrained model is fine-tuned on the embedded data, exploiting the knowledge shared across modalities. Some recent studies have leveraged this inherent ability of language pre-trained transformers to “reprogram” LLMs for time series analysis using parameter efficient fine-tuning and suitable tokenization strategies (Zhou et al., 2023; Gruver et al., 2023; Jin et al., 2023; Cao et al., 2023; Ekambaram et al., 2024). However, some of these models (Jin et al., 2023; Gruver et al., 2023) with billions of parameters demand significant memory and computational resources to perform well. We complement this line of research with three empirical observations (Sec 4.3): we\
show that (1) transformers trained on time series can also model sequences across modalities, (2) during pre-training, randomly initializing weights lead to lower pre-training loss, than initializing with language modeling weights, and (3) models pre-trained on time series outperform LLM-based models such as (Zhou et al., 2023; Jin et al., 2023) on many tasks and datasets.\
Unanswered Questions. To the best of our knowledge, two questions remain largely unanswered in prior work on time series modeling. First, all existing time series models are (pre-)trained and fine-tuned on individual datasets (Nie et al., 2023; Yue et al., 2022; Wu et al., 2023; Zhou et al., 2023), and the benefits (or drawbacks) of large-scale multi-dataset pre-training remains unexplored (Wen et al., 2023). Second, there is very limited work on time series modeling in limited supervision settings, such as zero-shot forecasting (Oreshkin et al., 2021), or few-shot classification (Narwariya et al., 2020). In our work, we consider both these questions and show that pre-training a model of sufficient capacity on a large corpus of unlabeled time series data can in fact enable it to provide reasonably accurate predictions in limited-supervision.(1) Mononito Goswami, Auton Lab, Robotics Insititute, Carnegie Mellon University, Pittsburgh, USA (mgoswami@andrew.cmu.edu)(2) Konrad Szafer, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(3) Arjun Choudhry, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(4) Yifu Cai, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA;(5) Shuo Li, University of Pennsylvania, Philadelphia, USA;(6) Artur Dubrawski, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA.]]></content:encoded></item><item><title>How a New AI Model is Taming the Chaos of Time Series Data</title><link>https://hackernoon.com/how-a-new-ai-model-is-taming-the-chaos-of-time-series-data?source=rss</link><author>Reinforcement Technology Advancements</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Mononito Goswami, Auton Lab, Robotics Insititute, Carnegie Mellon University, Pittsburgh, USA (mgoswami@andrew.cmu.edu)(2) Konrad Szafer, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(3) Arjun Choudhry, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, with equal contribution, order decided using a random generator;(4) Yifu Cai, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA;(5) Shuo Li, University of Pennsylvania, Philadelphia, USA;(6) Artur Dubrawski, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA.We introduce , a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pretraining. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pretrained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on https://huggingface.co/AutonLab.Time series analysis is an important field encompassing a wide range of applications ranging from forecasting weather patterns (Schneider & Dickinson, 1974) or detecting irregular heartbeats using Electrocardiograms (Goswami et al., 2021), to identifying anomalous software deployments (Xu et al., 2018). Due to its significant practical value and the unique challenges that modeling time series data poses, time series analysis continues to receive substantial interest from academia and industry alike. However, modeling such data typically requires substantial domain expertise, time, and task-specific design.\
Large pre-trained language (Touvron et al., 2023; Devlin et al., 2019; Chung et al., 2022), vision (Li et al., 2023a), and video (Day et al., 2023) models, typically perform well on a variety of tasks on data from diverse domains, with little or no supervision, and they can be specialized to perform well on specific tasks. We unlock these key capabilities for time series data and release the first family of open-source large pre-trained time series models, which we call MOMENT. The models in this family (1) serve as a building block for diverse  (e.g., forecasting, classification, anomaly detection, and imputation, etc.), (2) are effective , i.e., with no (or few) particular task-specific exemplars (enabling e.g., zero-shot forecasting, few-shot classification, etc.), and (3) are  using in-distribution and task-specific data to improve performance.\
MOMENT is a family of high-capacity transformer models, pre-trained using a masked time series prediction task on large amounts of time series data drawn from diverse domains. Below we summarize our key contributions.\
 A key limiting factor for pre-training large time series models from scratch was the lack of a large cohesive public time series data repositories (Zhou et al., 2023; Gruver et al., 2023; Jin et al., 2023; Ekambaram et al., 2024; Cao et al., 2023). Therefore, we compiled , a large collection of publicly available data from diverse domains, ranging from healthcare to engineering to finance. The Time Series Pile comprises of over 5 public time series databases, from several diverse domains for pre-training and evaluation (Tab. 11).\
C2: Multi-dataset pre-training. Unlike text and images, which have largely consistent sampling rates and number of channels, time series frequently vary in their temporal resolution, number of channels[1], lengths, and amplitudes, and sometimes have missing values. As a result, large-scale mixed dataset pre-training is largely unexplored. Instead, most methods are trained on a single dataset, and transferred across multiple datasets, but with modest success (Wu et al., 2023; Oreshkin et al., 2021; Narwariya et al., 2020).\
 Holistic benchmarks to evaluate time series foundation models on diverse datasets and tasks are in their nascent stages. Recent studies (Goswami et al., 2023b) have highlighted the importance of well-defined benchmarks and large-scale experimentation in order to accurately assess the impact and effectiveness of novel methodologies. To evaluate MOMENT, we build on the multi-task time series modeling benchmark first proposed by Wu et al. (2023) along multiple dimensions. For each of the 5 time series modeling tasks, namely, short- and long-horizon forecasting, classification, anomaly detection, and imputation we evaluate MOMENT against (1) both state-of-the-art deep learning as well as statistical baselines, on (2) more task specific datasets, (3) using multiple evaluation metrics, (4) exclusively in limited supervision settings (e.g., zero-shot imputation, linear probing for forecasting, unsupervised representation learning for classification).\
Finally, we explore various properties of these pre-trained time series models. In particular, we study whether MOMENT is aware of intuitive time series characteristics such as frequency and trend, and the impact of initialization, model size scaling, and cross-modal transfer.[1] Temporal resolution reflects sampling frequency of time series (e.g., hourly, daily); Channel is a single univariate time series in multivariate data (Ekambaram et al., 2024).]]></content:encoded></item><item><title>Tailor, a ‘headless’ ERP startup, raises $22M Series A</title><link>https://techcrunch.com/2025/06/30/tailor-a-headless-erp-startup-raises-22m-series-a/</link><author>Kate Park</author><category>tech</category><pubDate>Mon, 30 Jun 2025 18:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tailor, a San Francisco- and Tokyo-based enterprise resource planning (ERP) platform, has raised $22 million in a Series A funding round.]]></content:encoded></item><item><title>Meta restructures its AI unit under ‘Superintelligence Labs’</title><link>https://techcrunch.com/2025/06/30/meta-restructures-its-ai-unit-under-superintelligence-labs/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:56:25 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta CEO Mark Zuckerberg is restructuring the company’s AI efforts to center around building  AI “superintelligence.” ]]></content:encoded></item><item><title>Privacy-focused app maker Proton sues Apple over alleged anticompetitive practices and fees</title><link>https://techcrunch.com/2025/06/30/privacy-focused-app-maker-proton-sues-apple-over-alleged-anticompetitive-practices-and-fees/</link><author>Sarah Perez</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:51:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The suit is looking for changes to the App Store and monetary damages, which Proton says will be donated to organizations fighting for democracy and human rights.]]></content:encoded></item><item><title>I Let an AI Manage My Diabetes — And It Knew Me Better Than I Knew Myself</title><link>https://hackernoon.com/i-let-an-ai-manage-my-diabetes-and-it-knew-me-better-than-i-knew-myself?source=rss</link><author>Thanh (Bruce) Pham</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:50:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Beyond Telehealth: The Rise of the 'Digital Twin' in Chronic Disease ManagementThe first thing I notice about David is the small, white disc on the back of his arm. It’s a continuous glucose monitor, a common enough sight these days. But for David, a 58-year-old architect with the kind of restless energy that makes his spacious Ho Chi Minh City apartment feel small, the disc is merely the most visible node of a far more complex system. It’s an outpost, feeding data to something he sometimes calls “The Ghost.”“It told me to eat a banana yesterday afternoon,” \
He says, leaning back in his chair. He’s not looking at me, but at the city skyline, as if recounting a conversation with an invisible companion. “Said my energy expenditure was high from the site visit and my levels were trending down. I felt fine. But I’ve learned to listen.”\
For the last six months, David has been living with a digital twin. He is one of the first participants in a pilot program designed to push beyond the simple video calls of telehealth and into the strange, intimate world of predictive medicine. The Ghost is a virtual model of him, a breathing, learning algorithm that lives in the cloud and, arguably, knows his body better than he does. It’s his guardian, his analyst, and, on some days, his most annoying backseat driver.\
It’s a relationship, and like all modern relationships, it’s complicated.Practicing in the Rearview MirrorBefore The Ghost, David’s life with Type 2 diabetes was a life of looking backward. \
He explains, swirling the ice in his glass. “You’d do a blood test, get a high number, and then try to figure out why. What did I eat last night? Did I not walk enough? You’re always solving yesterday’s crime.”\
This reactive loop is the exhausting reality for millions managing a chronic condition. It’s a state of perpetual defense. His endocrinologist, Dr. Thuy Aris, a sharp, fast-talking clinician who seems more like a data scientist than a physician, calls it “practicing medicine through a rearview mirror.”\
When I speak to her over a video call, her background a blur of books and diagrams, she lays out the vision. “A chronic illness is a process, not an event,” she says, her conviction making the screen feel irrelevant. “Why would we treat it with static data points? A check-up every three months is absurd when the body is changing every three seconds. Under the hood, we're using a combination of transformer models to analyze the time-series data from his sensors and a knowledge graph built from his clinical history to add context. The goal isn't just to collect data—it’s to create a predictive model that can run forward in time.”\
She enlisted David, one of her most tech-savvy but frustrated patients, for the pilot. They built his twin by seeding it with his entire medical history and genomic data. Then, they started the live feed, connecting the streams from his monitor, his smartwatch, his smart scale, and an app where he occasionally logs his meals.Learning to Trust the WhispersAt first, he says, it felt like surveillance. An invisible parole officer tracking his every move. “The first week, it sent me an alert because my heart rate was elevated. I was watching a football match,” he laughs. “It had to learn my life. It had to learn what my ‘normal’ actually was.”\
He pulls out his phone and shows me the interface. It’s not a chatbot with a friendly name or an avatar. It’s just… data. Elegant, flowing graphs of his glucose, heart rate variability, and sleep cycles. But then there’s another layer: the projections. Faint, dotted lines extending out into the next few hours, the next day. This is the twin’s work. The Ghost’s whispers.\
“This is the moment it all changed for me,” he says, tapping on a date from three months ago. He tells me the story. He was working late, stressed about a deadline. He felt the familiar clammy sensation of a blood sugar crash coming on. He reached for the candy he keeps in his desk drawer, but before he could open it, his phone buzzed. ‘Hypoglycemia likely. But your long-term trend suggests a significant rebound effect afterward. Suggest 15g of complex carbs instead of simple sugar.’\
He paused. The machine was offering nuance, not just a binary alarm. It was coaching him. “It saw the crash coming, but it also saw the spike that would come after I panicked and ate a handful of candy,” he says, a genuine sense of wonder in his voice. “It saw the whole pattern. I ate a piece of toast instead. And it was right.”\
In that moment, his relationship with The Ghost changed from one of surveillance to one of trust. It wasn’t just watching him; it was watching  for him.A New Kind of Self-AwarenessNow, he talks about it with a strange sense of partnership. He’s learning to interpret its nudges, and it’s learning the rhythms of his life. But it raises a profound question, one he’s still wrestling with. “When this thing knows my body’s needs better than my own conscious mind, who is the real ‘me’?” he muses. “Am I outsourcing my own intuition?”\
It’s the quintessential 21st-century identity crisis. We’ve outsourced our memory to Google and our social lives to Instagram. Now, we’re on the verge of outsourcing our own biological awareness.\
When I ask Dr. Aris about this, she doesn’t flinch. “I see it differently,” she says. “We’re giving him a sense he’s never had before. It’s not replacing his intuition; it’s informing it with data he could never possibly compute on his own. It’s a tool for a deeper kind of self-awareness.”\
David is still deciding. He’s healthier than he’s been in years, and he feels a sense of control that he thought he’d lost forever. But it’s a shared control. He and The Ghost, a man and his data, navigating the complex landscape of his own body, together. He finishes his drink and looks back at the city lights. “Whatever it is,” he says with a wry smile, “it’s a hell of a lot better than flying blind.”]]></content:encoded></item><item><title>Musk’s ‘Priority #1’ Disaster: CSAM Problem Worsens While ExTwitter Stiffs Detection Provider</title><link>https://www.techdirt.com/2025/06/30/musks-priority-1-disaster-csam-problem-worsens-while-extwitter-stiffs-detection-provider/</link><author>Mike Masnick</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:45:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[One of Elon Musk’s first “promises” upon taking over Twitter was that fighting child exploitation was “priority #1.”He falsely implied that the former management didn’t take the issue seriously (they did) and insisted that he would make sure it was a solved problem on the platform he now owned. Of course, while he was saying this, he was also firing most of the team that worked on preventing the sharing of child sexual abuse material (CSAM) on the site. Almost every expert in the field noted that it seemed clear that Elon was almost certainly making the problem worse, not better. Some early research supported this, showing that the company was now leaving up a ton of known CSAM (the easiest kind to find and block through photo-matching tools).A few months later, Elon’s supposed commitment to stomping out CSAM was proven laughable when he apparently personally stepped in to reinstate the account of a mindless conspiracy theorist who had posted a horrific CSAM image.A new NBC News investigation now reveals just how spectacularly Musk has failed at his self-proclaimed “priority #1.” Not only has the CSAM problem on ExTwitter exploded beyond previous levels, but the company has now been cut off by Thorn—one of the most important providers of CSAM detection technology—after ExTwitter simply stopped paying its bills.At the same time, Thorn, a California-based nonprofit organization that works with tech companies to provide technology that can detect and address child sexual abuse content, told NBC News that it had terminated its contract with X.Thorn said that X stopped paying recent invoices for its work, though it declined to provide details about its deal with the company citing legal sensitivities. X said Wednesday that it was moving toward using its own technology to address the spread of child abuse material.Let’s pause on this corporate-speak for a moment. ExTwitter claims it’s “moving toward using its own technology” to fight CSAM. That’s a fancy way of saying they fired the experts and plan to wing it with some other—likely Grok-powered— nonsense they can cobble together.Now, to be fair, some platforms do develop effective in-house CSAM detection tools and while Thorn’s tools are widely used, some platforms have complained that the tools are limited. But these types of systems generally work best when operated by specialized third parties who can aggregate data across multiple platforms—exactly what organizations like Thorn (and Microsoft’s PhotoDNA) provide. The idea that a company currently failing to pay its bills to anti-CSAM specialists is simultaneously building superior replacement technology is, shall we say, optimistic.The reality on the ground tells a very different story than Musk’s PR spin:The Canadian Centre for Child Protection (C3P), an independent online CSAM watchdog group, reviewed several X accounts and hashtags flagged by NBC News that were promoting the sale of CSAM, and followed links promoted by several of the accounts. The organization said that, within minutes, it was able to identify accounts that posted images of previously identified CSAM victims who were as young as 7. It also found apparent images of CSAM in thumbnail previews populated on X and in links to Telegram channels where CSAM videos were posted. One such channel showed a video of a boy estimated to be as young as 4 being sexually assaulted. NBC News did not view or have in its possession any of the abuse material.Lloyd Richardson, director of information technology at C3P, said the behavior being exhibited by the X users was “a bit old hat” at this point, and that X’s response “has been woefully insufficient.” “It seems to be a little bit of a game of Whac-A-Mole that goes on,” he said. “There doesn’t seem to be a particular push to really get to the root cause of the issue.”NBC’s investigation found that Musk’s “priority #1” has become a free-for-all:A review of many hashtags with terms known to be associated with CSAM shows that the problem is, if anything, worse than when Musk initially took over. What was previously a trickle of posts of fewer than a dozen per hour is now a torrent propelled by accounts that appear to be automated — some posting several times a minute.Despite the continued flood of posts and sporadic bans of individual accounts, the hashtags observed by NBC News over several weeks remained open and viewable as of Wednesday. And some of the hashtags that were identified in 2023 by NBC News as hosting the child exploitation advertisements are still being used for the same purpose today.That seems bad! Read it again: hashtags that were flagged as CSAM distribution channels in 2023 are still active and being used for the same purpose today. This isn’t the kind of mistake that happens when you’re overwhelmed by scale—this is what happens when you simply don’t give a shit.Look, I’m usually willing to defend platforms against unfair criticism about content moderation. The scale makes perfection impossible, and edge cases are genuinely hard. But this isn’t about edge cases or the occasional mistake—this is about leaving up known, previously identified CSAM distribution channels. That’s not a content moderation failure; that’s a policy failure.As the article also notes, ExTwitter tried to get praised for all the work it was doing with Thorn, in an effort to show how strongly it was fighting CSAM. This post from just last year looks absolutely ridiculous now that they stopped paying Thorn and the org had to cut them off.But the real kicker comes from Thorn itself, which essentially confirms that ExTwitter was more interested in the PR value of their partnership than actually using the technology:Pailes Halai, Thorn’s senior manager of accounts and partnerships, who oversaw the X contract, said that some of Thorn’s software was designed to address issues like those posed by the hashtag CSAM posts, but that it wasn’t clear if they ever fully implemented it.“They took part in the beta with us last year,” he said. “So they helped us test and refine, etc, and essentially be an early adopter of the product. They then subsequently did move on to being a full customer of the product, but it’s not very clear to us at this point how and if they used it.”So there you have it: ExTwitter signed up for anti-CSAM tools, used the partnership for good PR, then perhaps never bothered to fully implement the system, and finally stopped paying the bills entirely.This is what “priority #1” looks like in Elon Musk’s world: lots of performative tweets, followed by firing the experts, cutting off the specialized tools, and letting the problem explode while pretending you’re building something better. I’m sure like “full self-driving” and Starships that don’t explode, the tech will be fully deployed any day now.]]></content:encoded></item><item><title>Last Chance to Win from 15,000 USDT in Round 2 of the Spacecoin Writing Contest</title><link>https://hackernoon.com/last-chance-to-win-from-15000-usdt-in-round-2-of-the-spacecoin-writing-contest?source=rss</link><author>HackerNoon Writing Contests Announcements</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:43:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Houston, we have a deadline!Calling all writers, space geeks, blockchain advocates, and technologists around the globe. This is your final systems check before If you’re sitting on an unfinished draft or just getting ready to put pen to paper, now’s the time to  (as the kids say), finish up your piece, and submit your entry for a chance to win from the .In case you missed it: Spacecoin and HackerNoon launched the Spacecoin Writing Contest at the start of the year to give writers a chance to connect with Spacecoin’s mission—leveraging blockchain to decentralize the internet, bridge digital divides, and deliver affordable, borderless connectivity to billions. closed on May 7, 2025, after generating over 110k pageviews and over 100 hours of insightful reading across all published stories. Four writers—, , and —took home a combined  for their winning entries. Follow the steps below, and you could be next up on the podium!How to Enter the Spacecoin Writing ContestSubmit an article that answers one question under any of the following contest tags.Can a decentralized internet truly eliminate internet censorship? How can decentralized systems combat the network effect? How can space tech be used to build a decentralized internet? What’s the Most Promising Earth Application of Space Technology?What’s the Best Example of a Purpose-Driven Blockchain You Know?How can blockchain help promote financial inclusion among the unbanked? Additionally, participants may also submit stories under any of the contest tags.Discuss Spacecoin’s mission to decentralize the internet.How Does Creditcoin Create Trust in On-Chain Lending Ecosystems?How does Gluwa facilitate global financial inclusion?Submit your article for review on HackerNoon, making sure to include your chosen contest tag. Then, add seven additional relevant tags to improve distribution and visibility.Once published, share your article across social media platforms using the contest hashtag to increase visibility and engage with the community.That’s it! Follow these simple steps and you’ll be in the running to win from a 15,000 USDT prize pool, awarded as follows:Spacecoin Writing Contest: Prize Breakdown:::info
Up to 15 writers will be awarded across 5 categories throughout this contest.| General Prizes (3000 USDT awarded after each round) | Sponsor prizes (6000 USDT awarded after the final round) |
|----|----|
| #decentralized-internet - 1 for best story | #spacecoin -  for best story |
| #spacetech - 1000 USDT for best story | 500 USDT for runner up | #creditcoin -  for best story |
| #blockchain-use-case  for best story | #gluwa -  for best story |The Spacecoin Writing Contest: GuidelinesYou can enter the contest by adding the appropriate tag to your story. (#spacetech, #decentralized-internet, #realworld-blockchain-usecase, #spacecoin, #creditcoin, #gluwa)Can I Write Under a Pen Name?Yes! You can use your real name on your HN profile, a fake name, or even create a persona to write under.What are the Contest’s Timelines?The contest consists of 3 rounds and will run for nine months.Round 2: Ongoing! Deadline — Jul 7, 2025Jul 8, 2025 - Oct 7, 2025Can I submit more than one entry to the contest?Of course! Each story submission shall be considered a separate entry into the writing contest.At the end of each round, we’ll review submitted entries and shortlist the stories that receive the most eyeballs (real humans, not bots!).Next, the shortlisted stories will be voted on by HackerNoon and Spacecoin staff.The top #decentralized-internet, #spacetech, and #realworld-blockchain-usecase stories will be selected and announced.After the contest’s final round ends, we’ll announce the winners of the sponsor prizes along with 3 other categories.Can I win more than one prize?]]></content:encoded></item><item><title>Daily Deal: The Complete Photoshop Master Class Bundle</title><link>https://www.techdirt.com/2025/06/30/daily-deal-the-complete-photoshop-master-class-bundle-6/</link><author>Daily Deal</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:41:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[It’s no secret that Photoshop can be a bit dense when you’re first getting your feet wet with it. That’s why it pays to have a expert instructors show you the ropes. Led by a Photoshop pro, the Complete Photoshop Master Class Bundle will help you master Photoshop CC and become an expert—no prior experience is required! From layers and filters to levels and curves, you’ll come to grips with essential Photoshop concepts and refine your skills with the included working files. It’s on sale for $30.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>The Internet Facebook, ChatGPT, Tiktok &amp; Google Don&apos;t Want You To See</title><link>https://hackernoon.com/the-internet-facebook-chatgpt-tiktok-and-google-dont-want-you-to-see?source=rss</link><author>Ivy</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:40:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[You're imprisoned by big tech companies… but Web3 is your escape.\
For every action you make on the internet, your footprints are created. These footprints are your data. How they are used is beyond your control. This rules out the ethics of decentralization which values  and . Instead, the internet operates a capitalist economy that empowers a select few who have enough resources to buy and hoard your data to gain competitive advantage — the more data they control, the more money they make. Your footprints on the internet have enabled the likes of Meta (Facebook & co),Tiktok, Open AI of ChatGPT and other tech companies to become stinkingly rich. Such exclusivity normalises internet monopolies by reducing competition and concentrating power in the hands of the minority. You're forced to follow rules set by algorithms built using your own data, at the expense of your user experience. All without your permission. A decentralized internet can fix this.Under the Hood Web3 Manages Data Like Web2Many times, the word 'decentralization' is merely used as a buzzword to appeal to Web3 audience. By such claims, decentralization means the team retains 100% control while bribing community members with airdrops, Discord roles and other giveaways. But that's a far cry from what a decentralized internet should look like especially given the explosion of more content online and better amplified algorithms.\
Yet despite these challenges, the Web3 journey has come a long way. From the cypherpunk mailing list in 1996, to mining Bitcoin on the dark web, to cryptocurrency gaining global recognition, to building applications across DeFi, SocialFi, and real-world assets, and to major payment platforms like PayPal introducing their own stablecoins, all contributing in their own way to shaping what a decentralized internet could look like.\
If decentralization is the goal, exclusivity should be shadowed. The proliferation of blockchains has given users various options to meet their onchain needs. However, relying on a single blockchain could reduce the barrier to entry. For this reason, the omnichain infrastructure exists. (Read more about omnichain in my previous note ). I recently came across a similar project building a multichain token platform. They are building a platform where a creator can mint an NFT on Solana and have it available across other chains. This opens up collections to larger communities. Decentralization is for everyone.\
Despite building towards decentralization, many blockchain applications depend heavily on centralized platforms like AWS for cloud storage, because storing all data on-chain is expensive and centralized solutions are often easier to use. While the front end of these applications may appear decentralized, the back end is fully centralized, making them vulnerable to single points of failure or censorship.\
Building an application that relies solely on onchain data is both expensive and challenging. Onchain data is difficult to query because it is fragmented across multiple APIs. At the same time, most decentralized solutions are costly, not user-friendly, and lack scalability and programmability. As a result, developers turn to multiple tools to store and manage data, which increases costs and reduces scalability. This leaves some data static, siloed, and unused.\
To solve this problem, chains strictly for data storage were built, they are called .\
How is a blockchain different from a datachain?  Blockchains are designed to run smart contracts, while datachains focus on storing data onchain. Since blockchains are not inherently built to manage data, they struggle to combine smart contract execution with data storage. This is why many blockchains keep only essential data onchain and store large files off-chain to save space.\
However, many datachains exist merely as glorified onchain data storage solutions, still very much centralized and lacking programmability — the ability to set rules for data using code. Data is like social bodies that are lifeless except they interact. What is the use of storing data if it can't be used?I found a programmable datachainMy last full-time role in Web2 was with a data infrastructure company, where my love for data began. I was fascinated by the many moving parts that help businesses store and process data efficiently. To the extent that I didn’t mind going the extra mile to learn how to write Python programs and SQL queries to clean and visualize data, even though my role was that of a content marketing manager. To be honest, the Web3 space piqued my interest after I read several think pieces on how it could transform individual control over data. After a few years of being in this space, I found a project building something to solve this problem.\
 is transcending the world of degens to build a data stack for everyone. They are building the first programmable datachain. You can call them the , a fully decentralized platform that houses all types of data and allows it to freely interact for better insights. But then there's more. The aim is to bring  data fully onchain — a world where information is not gatekept whilst security is maintained and speed accelerated.\
Today, most data is locked away or underused. Irys plans to unlock this value by making data accessible, enabling it to be sold to AI companies to build better models, and rewarding the original data owners in return.\
On Irys, your data isn't just sitting there. You can tell your data what to do : who can access it, when it can be used, how it pays you, and what rules others must follow when interacting with it. You see, that's control on display.\
To top it off, Irys offers a solution that's up to 20x cheaper than Web2 alternatives like AWS.What makes programmable data possible on Irys?Irys connects your data directly to a system that acts on it —Irys virtual machine (IrysVM). This means your data can follow rules and do things automatically, without needing extra tools. Whereas, traditional systems store data in one place and run actions in another. To make them work together, devs use separate services or off-chain tools, which adds complexity and cost. Irys keeps everything in one place. See it as a one-stop shop for all data needs.Let's look at simple examples to better understand how data properties on Irys will play out:A piece of data is uploaded to Irys and it includes an access condition : “only wallets that own a specific NFT or token can retrieve or decode this data”.Enforce royalty structuresAn NFT collection uploaded with embedded metadata that defines resale rules such as : “10% royalty goes to the creator”.Uphold encryption standardsA group of data is encrypted and contains a rule: “only decode if the wallet is whitelisted”.Decentralization Delivered: How Irys Datachain Gets It RightIn decentralized systems, people need to trust that data is real, safe, and always available. The method of ascertaining this is called . Verifiability means being able to prove that data hasn't been compromised or lost. Building a verifiable datachain stack is a herculean task. Other datachains attempted this but were forced to compromise between reliability and performance. Some verified data only among miners, which failed to earn users' full trust, while others prioritize speed at the expense of security. In the process, some data was lost, undermining the  promise.Every piece of data goes through two steps:Submit Ledger – A place where new data is first uploaded and checked.Publish Ledger – Once the data is verified, it moves here to be stored permanently. This process also includes a proof that confirms the data was uploaded properly and shows who’s responsible for keeping it safe.\
How miners work to verify dataIn most blockchains, miners secure the network by solving puzzles and validating transactions. Irys works differently. Miners aren’t competing to solve puzzles. Instead, they store data and regularly prove that it’s still intact. They’re rewarded for keeping unique, verifiable copies and responding to random checks that confirm the data is accurate and available. This approach keeps Irys decentralized and reliable, while removing the need for centralized storage providers.On Irys, uploading data is made faster and more efficient using . Instead of sending each piece of data one by one, bundlers group many pieces together into a single package, called a bundle, and upload it all at once.Inside each bundle, the individual pieces of data are called DataItems. Each one still keeps important details like who owns it, what it contains, and how it should behave, so nothing gets lost or mixed up.Bundlers pay the upload fee for the whole bundle. This means each DataItem doesn’t need to include its own fee, which keeps costs low. Developers can upload large amounts of data without worrying about high transaction costs.Because the cost is handled at the bundle level, apps can choose to cover those costs for their users. This makes it easier to build smooth, user-friendly experiences where people don’t even have to think about fees.Bundling helps Irys manage large amounts of data without slowing down. That makes it perfect for apps that deal with a lot of activity or need to upload lots of information quickly.Apps are moving to the decentralized data stack. Here’s what that means for you.This section explores real projects already building on Irys and how they’re putting power back in your hands as an internet user.Simplified access to DeFiIrys is shattering the ceiling of just existing as a data storage platform. It intends to act as memory, shared memory for AI systems, protocols, and intelligence that improves over time. is making DeFi easier to use through AI agents. These agents are storing their data on Irys. That means every move they make is recorded, transparent, and can’t be tampered with. Over time, these agents will start learning from each other to make better decisions. For you, this means more reliable, automated financial tools that work behind the scenes, built on data you can trust. is building a decentralized network of AI agents that monitor crypto markets. Each agent specializes in something unique. Everything they learn is stored on Irys, giving them fast and verifiable access to past insights, long-term memory to support deeper learning and a shared foundation that helps them coordinate and build on each other's outputs.You become the boss of your dataWhat if AI paid you? YouTube got better when creators started getting paid. The same thing could happen with AI. Right now, big companies train their AI on everyone’s data, including yours, but keep all the profits.  is flipping that model. They are building a system where anyone who helps train an AI, by contributing data or insights, gets rewarded. With Irys, every piece of that data is recorded, verified, and stored permanently. So when an AI model uses your contribution, it is easy to track and you get paid.As an expert, you can turn your knowledge into AI fuel and get paidFor years, devs have asked and answered technical questions on . These answers help millions of people solve real problems every day. But when AI models use that knowledge, the original contributors do not get any recognition or reward.  is changing that. They have built a marketplace where experts can turn their hard-earned knowledge into structured, verified data that AI models can train on and they get paid for it. Instead of pulling random information from forums or messy Wikipedia pages, AI can now learn from data packaged directly by professionals who know what they are talking about. Codatta uses Irys to store this data permanently.Your data is gold so you deserve a rewarding creator economy from it. It is your right.AI is exploding through your data without your permission and compensation. OpenAI is not really open. But a decentralized internet will fix that at the expense of data hoarders.]]></content:encoded></item><item><title>Movies and Space - A Century of Cinema&apos;s Cosmic Dreams</title><link>https://hackernoon.com/movies-and-space-a-century-of-cinemas-cosmic-dreams?source=rss</link><author>DeFi Titan</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:31:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In the history of cinema and cosmonautics, there are films shot in space, films shot by cosmonauts, films featuring cosmonauts, films shot by automatic spacecraft, films with real weightlessness but shot within the Earth's atmosphere, and many other variations. Today we are talking about cinema in space and space in cinema.The first film about spaceThe first public, and then commercial, film screenings began in 1895. And just seven years later, in 1902, the first known (and most likely the first ever) film about space and cosmonautics was released - “A Trip to the Moon” by French director Georges Méliès. Short by today's standards, at the time it was a full-fledged science fiction blockbuster lasting a full 15 minutes. The plot parodied Jules Verne's “From the Earth to the Moon” and H. G. Wells' “The War of the Worlds”. The film was initially shown to great success in cinemas on both sides of the Atlantic, then forgotten, rediscovered in the 1930s, and now rightfully occupies a place of honor in the pantheon of world cinema. The shot of the “face” of the moon, which was hit by a shell, has become one of the most famous and frequently cited in the entire history of cinema.\
Nowadays, it is necessary to explain that the projectile is loaded into the cannon by girls not just in strange clothes, but in swimsuits. But this prediction has not yet come true, and models in bikinis do not see off cosmonauts and astronauts on their flights. Take fifteen minutes and watch the film, it's worth it.The first serious film about spaceA quarter of a century later, the first film realistically depicting the launch of a lunar rocket was made in Germany. “Woman in the Moon” was directed by Fritz Lang and released in 1929. According to some critics, the realistic depiction of a rocket launch was the director's main idea. Whether this is true or not, the famous rocket and space enthusiast Hermann Obert was brought in as a consultant, who was also supposed to demonstrate the launch of a model rocket as part of an advertising campaign, for which he received significant funding from the film studio. Obert did not make it in time for the film and launched Europe's first liquid-fuel rocket in 1931.\
When real space flights began, it turned out that “Woman in the Moon” had predicted: vertical transport of the rocket to the launch site (one of the transport options), the countdown, the use of water at the launch complex (though not as in the film), anti-g seats (in reality, chairs), the spectacle of the first stage separation, and much more. Seventeen-year-old Wernher von Braun also worked as an assistant on the film set.The first serious Soviet film about space\
Soviet filmmakers were no strangers to science fiction—Protazanov's “Aelita”, based on the novel by Alexei Tolstoy, was filmed back in 1924, but it only briefly shows the launch of a spaceship from a roofless house on the “remote outskirts of Moscow.” But the film “Cosmic Voyage” had none other than Tsiolkovsky himself as a consultant. Released in 1936, the film has some similarities with “Woman in the Moon”: in both films, the characters fly to the moon, the expedition is led by a prominent scientist, there is a woman in the crew, and a teenager sneaks on board. But “Cosmic Voyage” shows Tsiolkovsky's vision: the rocket takes off from a launch pad (a relatively rare option today), and baths, or more precisely, containers with liquid, are used to protect against overload (a solution used only in unmanned spaceflight).\
There are characteristic misconceptions, such as the impossibility of radio communication with Earth and the need to signal with pyrotechnic rockets because of this. But unlike in “Woman in the Moon”, weightlessness is shown correctly—it occurs immediately after the engine is turned off. The film is also interesting for its use of the word “astronaut,” as “cosmonaut” appeared much later.\
It is worth noting the special effects, which look surprisingly good for a film that is almost a century old and have become, in a sense, the standard. Weightlessness was shown by suspending the actors on cables, and the scenes in the rocket hangar and jumping on the moon were filmed on miniature models and using puppet animation.\
The first photo and video in space were taken on October 24, 1946, by an automatic camera mounted on a modified V-2 rocket, originally a combat rocket, launched from the US White Sands test site. The rocket rose to an altitude of 104.6 km, and the camera installed on it took one frame every 1.5 seconds. A few minutes later, the rocket and the camera fell back to earth and crashed, but the film in a special capsule was safely retrieved and developed. The camera was developed by Clyde Holliday, who worked at Johns Hopkins University.\
Since then, cameras have become a permanent part of space cargo — they are placed on rockets and interplanetary stations and sent along with astronauts.The first photos of the entire Earth from space\
The first photo of the entire Earth was taken on May 30, 1966, by a camera installed as a piggyback payload on a Soviet Molniya-1 communications satellite (most likely No. 6, launched in April), flying in a highly elliptical orbit with a large inclination. It is difficult to make out anything in the photo, but from the orbit parameters it is clear that this must be the northern hemisphere.\
The first color photo was taken by the American experimental satellite DODGE. The main task of the device was to test the gravitational stabilization system, and the camera was also carried as a piggyback payload. The black-and-white camera took three pictures through color filters, and the image was assembled on Earth. Incidentally, Clyde Holliday, whom you already know, participated in its processing.The first photographer and videographer in spaceThe first person to take photos and videos in space was German Titov on August 6, 1961. Yuri Gagarin had a fixed television camera in his spacecraft that filmed the cosmonaut's face, but not what was happening outside, and he was not given a camera due to the relatively short flight. The American astronauts who made two suborbital flights after Gagarin had three cameras in their spacecraft: one filmed the astronaut, the second filmed the instrument panel, and the third looked out the porthole, but all of them were automatic. However, German Titov flew in orbit for a whole day, so he had time to operate the camera and video camera himself.\
Since then, cosmonauts have taken photography courses as part of their space training, taken a lot of pictures in orbit, published them and collected thousands of likes on Instagram. After returning, they have organized exhibitions that have attracted a lot of interest and published albums.Everything grows and develops\
It is thanks to automatic cameras that we have images that have become archetypal. American Gemini spacecraft were equipped with cameras looking out of a porthole, which was also conveniently located. As a result, we have images of deceleration in the dense layers of the atmosphere, when the plasma enveloping the spacecraft is clearly visible in the porthole.\
Many cameras were used in the Apollo program. They were located at various points on the launch pad and on the rocket itself. The cameras on the rocket were housed in containers, turned on automatically, then ejected and descended into the ocean on parachutes equipped with radio beacons. As a result, humanity has some truly spectacular footage of the separation of the stages.\
In the 1960s, two science fiction universes were born that are still alive today and have huge fan communities. In 1963, the British series “Doctor Who” appeared, about an eccentric alien who travels through time and space, and in 1966, the American series Star Trek appeared, about the adventures of the crew of the starship Enterprise. The series were broadcast on TV, so they couldn't boast a large budget. As a result, it was space fiction in which space was hardly shown at all. The landscapes of other planets were filmed in studios or in some sandy quarry (a cliché particularly characteristic of “Doctor Who”). Aliens were portrayed by people equipped with the necessary amount of vinyl, plastic, pine cones, acorns, and similar materials.\
The “Doctor Who” spaceship and time machine TARDIS simply materialized at the desired point, while “Star Trek” showed a large-scale model of the ship against the backdrop of, for example, a painted planet. Already in the 21st century, a remaster was made with simple but attractive computer special effects.\
Serious special effects with a large budget could be found in feature films, the archetypal example of which was Stanley Kubrick's “2001: A Space Odyssey”, released in 1968. Filmed in the canons of hard science fiction, the film honestly tried to show space using available technology. The actors, as in “Cosmic Voyage”, hung on cables, except that they tried to hide them better, without revealing the actor's movement along the pendulum trajectory.\
The force of gravity directed from the axis to the periphery on the orbital station was demonstrated by constructing a large set, inside which an actor ran like a squirrel in a wheel.\
The models of the technology were combined with the background using various animation and combined shooting techniques, creating a sense of scale with slow-motion shots when necessary. At the time of release, the result was impressive.\
One striking prediction in “2001: A Space Odyssey” was that the shots of running around the station were later filmed in real space—on the large Skylab station, real astronauts ran in much the same way. However, in real weightlessness, they had more freedom, and it would have been much more difficult to film the astronauts' somersaults in “2001: A Space Odyssey.”\
The development of photo and movie cameras did not bypass space. When IMAX technology appeared in the 1970s, astronauts began to use it to film. In 2002, the first 3D film, Space Station 3D, was released. As a result, humanity has a huge number of documentary films about space. Among those made in Russia, it is worth watching the series Space Odyssey, 21st Century and A Year in Orbit.The first feature film in space\
The first film to use footage shot specifically in space was the Soviet film “Return from Orbit” (1984). Cosmonauts filmed on the Salyut 7 space station and in the Soyuz T-9 spacecraft. Considering that the actors remained on Earth, it is likely that these shots of a stopwatch rotating in zero gravity were filmed.\
Most likely, the only film that features footage of real weightlessness not shot in space is Apollo 13. The film crew, actors, and set were placed in a flying laboratory aircraft used in cosmonaut training. The plane gains altitude and begins to describe parabolas, creating weightlessness for short periods of up to 30 seconds. And here's the backstage.In the 21st century, the need for such tricks has largely disappeared — you can film actors against a green screen and then add whatever you want on the computer. The most striking example of this approach is Alfonso Cuarón's “Gravity.”\
Nowadays, the realism of films depends mainly on the screenwriters, the violations of the laws of nature they are willing to commit in order to portray the desired drama.There are feature films shot by astronauts. Space tourist Richard Garriott, who traveled to the ISS in 2008, brought a science fiction film script to the station, which was shot during the crew's free time. “Apogee of Fear!” was released in 2012 and can be viewed on YouTube.]]></content:encoded></item><item><title>Microsoft&apos;s New AI Tool Outperforms Doctors 4-to-1 in Diagnostic Accuracy</title><link>https://science.slashdot.org/story/25/06/30/1712220/microsofts-new-ai-tool-outperforms-doctors-4-to-1-in-diagnostic-accuracy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft's new AI diagnostic system achieved 80% accuracy in diagnosing patients compared to 20% for human doctors, while reducing costs by 20%, according to company research published Monday. The MAI Diagnostic Orchestrator queries multiple leading AI models including OpenAI's GPT, Google's Gemini, Anthropic's Claude, Meta's Llama, and xAI's Grok in what the company describes as a "chain-of-debate style" approach. 

The system was tested against 304 case studies from the New England Journal of Medicine using Microsoft's Sequential Diagnosis Benchmark, which breaks down each case into step-by-step diagnostic processes that mirror how human physicians work. Microsoft CEO of AI Mustafa Suleyman called the development "a genuine step toward medical superintelligence."]]></content:encoded></item><item><title>A comprehensive list of 2025 tech layoffs</title><link>https://techcrunch.com/2025/06/30/tech-layoffs-2025-list/</link><author>Cody Corrall, Alyssa Stringer, Kate Park</author><category>tech</category><pubDate>Mon, 30 Jun 2025 16:45:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A complete list of all the known layoffs in tech, from Big Tech to startups, broken down by month throughout 2024.]]></content:encoded></item><item><title>Google embraces AI in the classroom with new Gemini tools for educators, chatbots for students, and more</title><link>https://techcrunch.com/2025/06/30/google-embraces-ai-in-the-classroom-with-new-gemini-tools-for-educators-chatbots-for-students-and-more/</link><author>Sarah Perez</author><category>tech</category><pubDate>Mon, 30 Jun 2025 16:44:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google will give teachers the ability to create interactive study guides using the AI research tool Notebook LM, along with their classroom materials.]]></content:encoded></item><item><title>Microsoft Authenticator Will Stop Supporting Passwords</title><link>https://it.slashdot.org/story/25/06/30/1631240/microsoft-authenticator-will-stop-supporting-passwords?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Avantare writes: Microsoft Authenticator houses your passwords and lets you sign into all of your Microsoft accounts using a PIN, facial recognition such as Windows Hello, or other biometric data, like a fingerprint. Authenticator can be used in other ways, such as verifying you're logging in if you forgot your password, or using two-factor authentication as an extra layer of security for your Microsoft accounts.

In June, Microsoft stopped letting users add passwords to Authenticator, but here's a timeline of other changes you can expect, according to Microsoft: 

July 2025: You won't be able to use the autofill password function.
August 2025: You'll no longer be able to use saved passwords.]]></content:encoded></item><item><title>Google’s new CLI tool hits different…</title><link>https://www.youtube.com/watch?v=qqP1ucSiVkE</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/qqP1ucSiVkE?version=3" length="" type=""/><pubDate>Mon, 30 Jun 2025 16:23:32 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Try the best video API for developers (and get $50 in free credits) - https://mux.com/fireship

Google has finally put Gemini in the command line, allowing you to hand total control of your machine over to AI. But how is their solution different from all the other similar tools out there?

💬 Chat with Me on Discord

https://discord.gg/fireship

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

* AI coding
* Gemini CLI
* Open-source AI agents
* Claude Code
* Svelte 5]]></content:encoded></item><item><title>Whistleblower: DOJ Lawyers Told To Say ‘Fuck You’ To Courts, Ignore Immigration Court Orders</title><link>https://www.techdirt.com/2025/06/30/whistleblower-doj-lawyers-told-to-say-fuck-you-to-courts-ignore-immigration-court-orders/</link><author>Tim Cushing</author><category>tech</category><pubDate>Mon, 30 Jun 2025 16:20:04 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Quite obviously, the Trump Administration was never going to respect the law when it came to its mass deportation plans. That much became immediately clear as ICE engaged in raid after raid of neighborhoods and businesses, searching for literally  migrant, rather than just those suspected of or convicted for violent crimes.As soon as the planes loaded with detainees began landing in countries most of those on board weren’t from, the efficient cruelty of Trump’s anti-migrant actions was exposed. Multiple legal challenges were filed, most of them seeking little more than forcing the government to respect due process rights. As the phrase says, they’re rights, not privileges that can be unilaterally revoked just because the current administration has a hate-on for non-whites.The administration that has always pretended to be ultra-concerned about “law and order” went rogue. Planes were put in the air after courts ordered them to remain on the ground. People exiled to foreign prisons remained caged while the Trump DOJ dodged court orders and refused to honestly answer questions about why it continued to refuse to right some of its wrongs. Not that we needed more evidence of this administration’s hatred for the laws that stand in the way of its cruelest urges, but here’s more anyway. Erez Ruveni, a former DOJ lawyer, has filed a whistleblower complaint about the DOJ’s actions since Trump’s return to office. Via the New York Times, here’s why Ruveni is no longer employed by the DOJ:Mr. Reuveni was a career lawyer at the department for nearly 15 years until April, when he expressed concern in federal court that the administration had mistakenly deported a migrant to a megaprison in El Salvador. Mr. Reuveni was put on administrative leave a day later and ultimately fired.That public embarrassment of the DOJ, via an open admission that an error was made, was enough to provoke Trump’s DOJ to fire one of its few remaining honest lawyers. Those still working for the DOJ have sold their soul for a handful of future paychecks. Either that or they never had a soul to sell, like senior DOJ official Emil Bove, whose anti-law and order statements are included in Ruveni’s whistleblower report.Bove told attendees of the March 14 meeting that President Donald Trump would soon be invoking the Alien Enemies Act and that deportations would be carried out that weekend.“Bove then made a remark concerning the possibility that a court order would enjoin those removals before they could be effectuated. Bove stated that DOJ would need to consider telling the courts ‘f— you’ and ignore any such court order,” according to the letter.Incredibly, Ruveni’s letter notes that he thought Bove’s heated comments would be blown off and things would return to their normal, lawful way — something that’s sort of expected at the Department of Justice. Instead, that was apparently the tipping point, not only in terms of his job, but for the DOJ as a whole. Here’s how things went for Ruveni during the last few weeks of his DOJ career, as recounted in the complaint [PDF] released by his legal reps at the Government Accountability Project:White House officials have publicly disparaged Mr. Reuveni to justify their refusal to comply with the Constitution and with court orders. White House Deputy Chief of Staff Stephen Miller falsely stated “The only mistake that was made is a lawyer put an incorrect line in a legal filing,” and labeled Mr. Reuveni a “saboteur, a Democrat.” Referring to Mr. Reuveni, President Trump stated, “Well, the lawyer that said it was a mistake was here a long time, was not appointed by us—should not have said that should not have said that.”What has not been reported to date are Mr. Reuveni’s attempts over the course of three weeks and affecting three separate cases to secure the government’s compliance with court orders and his resistance to the internal efforts of DOJ and White House leadership to defy them through lack of candor, deliberate delay, and disinformation. Discouraging clients from engaging in illegal conduct is an important part of the role of a lawyer. Mr. Reuveni tried to do so and was thwarted, threatened, fired, and publicly disparaged for both doing his job and telling the truth to the court.Of course, the DOJ and the administration have no real response to these accusations. Instead, the White House has chosen to ignore everything said in the letter and act like it works for Emil Bove, rather than the other way around: “Emil Bove is an incredibly talented legal mind and a staunch defender of the U.S. Constitution who will make an excellent circuit court judge,” the statement by White House spokesperson Harrison Fields said.Well, at best, only part of this statement is true. I can only speculate about the Bove’s “talented mind,” but it’s inherently clear he doesn’t give a fuck about the Constitution. That much can easily be seen by his support of Trump’s Alien Enemies Act abuses and the migrant flights he ensured wouldn’t be interrupted by issued court orders. He’s not serving the public. He’s nothing more than another opportunist who has sworn his fealty to a man who sincerely believes the Oval Office is a throne room. ]]></content:encoded></item><item><title>Mexican drug cartel hacker spied on FBI official’s phone to track and kill informants, report says</title><link>https://techcrunch.com/2025/06/30/mexican-drug-cartel-hacker-spied-on-fbi-officials-phone-to-track-and-kill-informants-report-says/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Mon, 30 Jun 2025 16:14:16 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The watchdog revealed the 2018 incident as part of an audit of the FBI's counter-surveillance efforts.]]></content:encoded></item><item><title>That Dropped Call With Customer Service? It Was on Purpose</title><link>https://it.slashdot.org/story/25/06/30/163250/that-dropped-call-with-customer-service-it-was-on-purpose?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 16:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Companies deliberately design customer service friction to discourage refunds and claims, according to research into a practice academics call "sludge." The term, coined by legal scholar Cass R. Sunstein and economist Richard H. Thaler in their updated version of "Nudge," describes tortuous administrative demands, endless wait times, and excessive procedural fuss that impede customers. 

ProPublica reported in 2023 that Cigna saved millions of dollars by rejecting claims without having doctors read them. The Consumer Financial Protection Bureau ordered Toyota's motor-financing arm to pay $60 million for alleged misdeeds including deliberately setting up dead-end hotlines for canceling products and services. The 2023 National Customer Rage Survey found that the percentage of American consumers seeking revenge for customer service hassles had tripled in three years.]]></content:encoded></item><item><title>The HackerNoon Newsletter: The Path to a Real Decentralized Order Book Protocol (6/30/2025)</title><link>https://hackernoon.com/6-30-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Mon, 30 Jun 2025 16:03:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, June 30, 2025?By @davidjdeal [ 4 Min read ] Meta’s $14.3B investment in Scale AI sparked a trust crisis, data quality concerns, and industry-wide fallout. Heres what it means for AI’s future.

⚡️TL;DR (C Read More.By @obyte [ 5 Min read ] As the tech world faces a broader pushback against Diversity, Equity, and Inclusion (DEI) efforts, it’s fair to ask: where does crypto stand?
 Read More.By @foued2d [ 5 Min read ] A light approach to the creation of a fully decentralized order book concept; the version 2.0 of AMMs. Read More.By @OurAI [ 7 Min read ] World models are internal representations that allow an AI to simulate future outcomes, test hypotheses, and strategically plan behaviors. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>A CarFax for Used PCs</title><link>https://spectrum.ieee.org/carmax-used-pcs</link><author>Abu Baker</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExNDMzOC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4MTExOTIzMH0.smMiwIhcLc_QYQrtK0zqGGZ71kGG0CFHe5z-VRvBvb0/image.jpg?width=600" length="" type=""/><pubDate>Mon, 30 Jun 2025 16:02:07 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Hewlett Packard wants to give old laptops new life]]></content:encoded></item><item><title>Ex-Meta engineers have built an AI tool to plan every detail of your trip</title><link>https://techcrunch.com/2025/06/30/former-meta-engineers-airial-travel-tool-helps-travelers-solve-logistics-planning-with-ai/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Mon, 30 Jun 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Airial's instant trip function can quickly create a travel plan that includes flights, hotels, and transport across multiple cities and will generate a daily itinerary for you.]]></content:encoded></item><item><title>Ubuntu Debcrafters Team Formed To Help Ensure The Health Of The Ubuntu Archive</title><link>https://www.phoronix.com/news/Ubuntu-Debcrafters</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 30 Jun 2025 15:56:42 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Jon Seager as the VP of Engineering at Canonical today publicly announced the formation of their "Debcrafters" global team to help ensure the health of the Ubuntu Archive...]]></content:encoded></item><item><title>DOE Invites Experiments for MARVEL Microreactor</title><link>https://spectrum.ieee.org/marvel-microreactor-department-of-energy</link><author>Shannon Cuthrell</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExNTUzNC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3NzQ2MjMwNX0.iGbw__C8n6VHFcnTKKER-HIE85XSPR533p9g06uZ79w/image.jpg?width=600" length="" type=""/><pubDate>Mon, 30 Jun 2025 15:38:28 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The Idaho-based testbed will test new nuclear ideas]]></content:encoded></item><item><title>Three powerhouses cover how to prepare now for your later-stage raise at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/06/30/how-to-prepare-now-for-your-later-stage-raise-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 30 Jun 2025 15:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TechCrunch Disrupt 2025 hits Moscone West in San Francisco this October 27–29, and if you’re building with big funding goals in mind, the session on preparing for your later-stage raise needs to be on your radar. Join three powerhouse voices in venture and AI-driven innovation as they share what it really takes to close major […]]]></content:encoded></item><item><title>Apple Plans First Sub-$999 MacBook Using iPhone Chip, Analyst Says</title><link>https://apple.slashdot.org/story/25/06/30/1434217/apple-plans-first-sub-999-macbook-using-iphone-chip-analyst-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 15:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple plans to release a cheaper MacBook powered by the A18 Pro chip used in the iPhone 16 Pro line, according to analyst Ming-Chi Kuo. The laptop will be priced below $999 -- first time for a MacBook Air -- and go into production in late 2025 or early 2026 on the new laptop, the analyst noted. 

The device will feature the same 13-inch screen as the current MacBook Air, with the chip representing the primary difference between models. The A18 Pro chip delivers single-core performance around 3,500 on Geekbench, trailing the M4 chip only slightly, though multicore performance lags significantly at approximately 8,780 versus 15,000 for the M4. The A18's multicore performance matches the original 2020 M1 chip.]]></content:encoded></item><item><title>Not just luck — it’s strategy: Tiffany Luck on winning over VCs at TechCrunch All Stage</title><link>https://techcrunch.com/2025/06/30/not-just-luck-its-strategy-tiffany-luck-on-winning-over-vcs-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 30 Jun 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Join Tiffany Luck, partner from NEA, at TechCrunch All Stage as she unveils the art of pitching to venture capitalists. Luck will share insider tips on what VCs truly value, empowering you to tell your story with clarity and impact without falling prey to common pitfalls that derail pitches.  The goal of this session: Deliver […]]]></content:encoded></item><item><title>Cursor launches a web app to manage AI coding agents</title><link>https://techcrunch.com/2025/06/30/cursor-launches-a-web-app-to-manage-ai-coding-agents/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Mon, 30 Jun 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The launch marks Cursor's next big step beyond its IDE, the core product developers use to access its tools. ]]></content:encoded></item><item><title>Tiny AI ERP startup Campfire is winning so many startups from NetSuite, Accel led a $35M Series A</title><link>https://techcrunch.com/2025/06/30/tiny-ai-erp-startup-campfire-is-winning-so-many-startups-from-netsuite-accel-led-a-35m-series-a/</link><author>Julie Bort</author><category>tech</category><pubDate>Mon, 30 Jun 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Campfire was launched in 2023 to upend 1990s-era enterprise resource planning accounting software (ERP) like Netsuite with an LLM-powered alternative. ]]></content:encoded></item><item><title>Xbox Founding Team Member Says Xbox Hardware Is &apos;Dead&apos;</title><link>https://games.slashdot.org/story/25/06/30/1424255/xbox-founding-team-member-says-xbox-hardware-is-dead?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 14:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A founding member of the Xbox team says she believes Xbox hardware is "dead" and that Microsoft appears to be planning a "slow exit" from the gaming hardware business. Microsoft recently announced partnerships with external hardware companies including the ROG Xbox Ally, which runs Windows and functions as a portable PC that can run games from external stores like Steam. 

Laura Fryer, one of Microsoft Game Studios' first employees who worked as a producer on the original Gears of War games and served as director of the Xbox Advanced Technology Group, called the partnerships evidence of Microsoft's inability to ship hardware. "Personally, I think Xbox hardware is dead. The plan appears to be to just drive everybody to Game Pass," Fryer said.]]></content:encoded></item><item><title>Jennifer Neundorfer on how AI is reshaping the way startups are built — live at TechCrunch All Stage</title><link>https://techcrunch.com/2025/06/30/jennifer-neundorfer-on-how-ai-is-reshaping-the-way-startups-are-built-live-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 30 Jun 2025 14:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Jennifer Neundorfer of January Ventures breaks down how AI has changed startup scaling at TechCrunch All Stage on July 15 in Boston. Register here to save.]]></content:encoded></item><item><title>The Best Boring Benchmarks: Rocky Linux 10 &amp; AlmaLinux 10 Performance Against RHEL 10</title><link>https://www.phoronix.com/review/almalinux-10-rocky-linux-10</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 30 Jun 2025 14:20:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AlmaLinux and Rocky Linux remain two of the most popular Red Hat Enterprise Linux derivatives that are maintained by the open-source community. With the recent Rocky Linux 10 GA release that followed the recent AlmaLinux 10 release for re-basing against Red Hat Enterprise Linux 10, here are some benchmarks looking at the performance of these popular downstreams compared to RHEL 10.]]></content:encoded></item><item><title>Researchers Uncover Hidden Ingredients Behind AI Creativity</title><link>https://www.quantamagazine.org/researchers-uncover-hidden-ingredients-behind-ai-creativity-20250630/</link><author>Webb Wright</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2025/06/TheoryofCreativity-crAdrianAstorgano-Default.webp" length="" type=""/><pubDate>Mon, 30 Jun 2025 14:02:57 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[We were once promised self-driving cars and robot maids. Instead, we’ve seen the rise of artificial intelligence systems that can beat us in chess, analyze huge reams of text and compose sonnets. This has been one of the great surprises of the modern era: physical tasks that are easy for humans turn out to be very difficult for robots, while algorithms are increasingly able to mimic our intellect.]]></content:encoded></item><item><title>Nintendo Pulls Products From Amazon US Site</title><link>https://games.slashdot.org/story/25/06/30/142217/nintendo-pulls-products-from-amazon-us-site?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 30 Jun 2025 14:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Nintendo pulled its products from Amazon's US site after a disagreement over unauthorized sales, meaning the e-commerce company missed out on the recent debut of Nintendo's Switch 2 -- the biggest game console launch of all time. From a report: The Japanese company stopped selling on Amazon after noticing that third-party merchants were offering games for sale in the US at prices that undercut Nintendo's advertised rates, according to a person familiar with the situation. Enterprising sellers were buying Nintendo products in bulk in Southeast Asia and exporting them to the US, said the person, who requested anonymity to discuss confidential information. 

Nintendo product listings started disappearing from Amazon's US site last year, gaming news outlets reported at the time. The listings had previously appeared as "Sold by Amazon," which typically denotes merchandise the online retailer buys directly from brands. Some Nintendo products remained on the site, but they were listed by independent merchants who sell their goods on Amazon's sprawling online marketplace.]]></content:encoded></item><item><title>Kristen Craft brings fresh fundraising strategy to the Foundation Stage at TechCrunch All Stage</title><link>https://techcrunch.com/2025/06/30/kristen-craft-brings-fresh-fundraising-strategy-to-the-foundation-stage-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Mon, 30 Jun 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At TechCrunch All Stage on July 15 at Boston’s SoWa Power Station, Kristen Craft, Head of Business Development at Fidelity Private Shares, will lead a roundtable session titled “It’s Not 2021 Anymore. Navigating the 2025 VC Landscape.”]]></content:encoded></item><item><title>Tinder to require new users in California to use facial recognition tech to verify their profiles</title><link>https://techcrunch.com/2025/06/30/tinder-to-require-new-users-in-california-to-use-facial-recognition-tech-to-verify-their-profiles/</link><author>Aisha Malik</author><category>tech</category><pubDate>Mon, 30 Jun 2025 13:53:21 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tinder is requiring new users in California to use facial recognition technology to verify their profiles.]]></content:encoded></item><item><title>AI Search is Eating Your Clicks: Here&apos;s How to Hack SEO in 2025</title><link>https://hackernoon.com/ai-search-is-eating-your-clicks-heres-how-to-hack-seo-in-2025?source=rss</link><author>Muhammad Usman</author><category>tech</category><pubDate>Mon, 30 Jun 2025 13:48:50 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The SEO game has changed more this year than it has in the past 10 years. And that’s actually to your advantage.People are finding answers on ChatGPT, Reddit, and YouTube, not just Google. Google itself is rewriting search with AI Overviews. And now there are tools that can do 80% of the work .This is your no-fluff beginner’s guide to what actually works for SEO this year.SEO Used to Just Be About GoogleFifteen years ago, SEO was all about Google. It was the only organic game in town. YouTube wasn’t what it is today. Instagram barely existed.Now, you’ve got to think about optimizing for YouTube, which is the second-largest search engine in the world. If you’re selling stuff, Amazon matters. If you want to show up in niche discussions, Reddit matters.So if you’re only focusing on Google, you’re missing out. Simple as that.The 3 Core SEO Building Blocks Still ApplyEven though SEO has changed, the foundation is still the same: content, citations, and what’s called double-E-A-T, experience, expertise, authoritativeness, and trust.These building blocks have been around for a while. But now, how you apply them across platforms like Google, YouTube, and Amazon makes all the difference.Long Tail Keywords Matter The MostWinning this year means going after long-tail niche queries with real intent.So instead of trying to rank for “Salesforce,” you should be looking at stuff like:How to integrate NetSuite with SalesforceOr instead of “red shoes,” think:What are the best red shoe designs for this yearThese kinds of queries usually have four or more keywords. They’re longer, more specific, and when people search them, they actually  something. That’s what real intent means.Yes, they have lower search volume and lower competition. But they convert better. And that’s what matters. More conversions, better leads, more email signups.Why Impressions Are Up but Clicks Are DownHere’s what’s happening: because of AI Overviews, impressions for websites are going up, but clicks are going down.I just talked to the CEO of an enterprise SEO company. He told me they have data showing this exact pattern; your analytics graph now looks like a jaw. Impressions are rising, but clicks are dropping.So, the smart move? Optimize for places where people have real intent. You’re going to have a much easier time there, especially since the game of getting website traffic is changing fast.You don’t need to guess anymore. You can use products like ChatGPT or other AI tools for your specific needs.All you need to do is give them a few examples of long-tail keywords, talk a little about your business, and boom, it spits out 50 to 100 solid keyword ideas. Pick the ones you like and start creating content around those.And Yes — AI Can Write the Content TooLet’s talk about large language models like ChatGPT again.You can feed them your existing blog content, and they’ll figure out your writing style. Like for us, our posts are usually 1,500–2,000 words, packed with links, stats, and tables. The LLM picked that up.And then it created 21 blog posts in about 30 minutes. Posts that were 95–99% ready to go. That’s what you can do now. You’ve got an AI assistant that’s patient, doesn’t get tired, and helps you move fast on your SEO strategy.Even though we’re talking about advanced stuff, SEO still comes down to three main things: content, citations, and double-E-A-T.Content = helpful, useful, real stuff.Citations = links from other websites. Or if you’re doing local SEO, reviews count as citations too. There are different kinds across the internet.And double-E-A-T matters even more now, since there’s so much content being created.Whatever platform you’re on, these three still apply. Always.Let’s go over the new toolkit.There are tools now that can automate things like internal linking, content pruning, and overall make your SEO a lot more efficient.These AI tools can scan your whole site, figure out where you should be adding internal links, which keywords should be prioritized, and what content to update, combine, or delete.You can even build your own mini tools using tools like Cursor. And here’s the crazy part: non-technical people can use these tools now. You don’t need to be a coder. These little apps help you do your work faster and more effectively.When we talk to our team about using these products, they say they can now get 70–80% of their work done way faster. They’re saving hours and spending more time on the high-leverage stuff.A lot of what we covered so far used to take days or weeks. Not anymore.AI Overviews Are Changing Organic Search (Fast)Let’s talk about what’s really disrupting organic search: Google’s AI Overviews.These are the answers that show up right at the top of the page. And studies are already showing traffic drops of more than 37% for some websites.So what does that mean? It means your strategy has to shift. Like I said earlier — this is the age of Search Everywhere Optimization.Organic Traffic Is Declining — But You Can Still WinWebsite traffic is going to keep declining over the next 5 to 10 years. But that doesn’t mean give up.It’s like a cigar butt business. That’s something Warren Buffett talks about — businesses that are slowly dying, but still have a few good years left.So while everyone else runs for the exits, you can stay in and make the most of it — while also building out other channels.Websites still matter. But now it’s about optimizing for where your audience actually hangs out.SEO Isn’t Dying — It’s SpreadingLet me just say this again: traditional SEO isn’t dying, it’s .People are searching on YouTube. They’re searching on TikTok. They’re searching on Instagram Reels. And they’re searching on LinkedIn as well.Humans will  search for things. Because humans are curious. That doesn’t change. The platforms change.So the message here is: adapt. Test content on different channels. Build your own playbook for each platform.Because the best way to learn is by doing.With the right tools and strategy, SEO is actually easier than ever. You’ll get better faster than you think.\
Take your time & stay connected with me on my other platforms:]]></content:encoded></item><item><title>Gone Fishin&apos;: 404 Media Summer Break 2025</title><link>https://www.404media.co/introducing-404-media-summer-break-2025/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/06/frames-for-your-heart--J5iFh4gh6A-unsplash.jpg" length="" type=""/><pubDate>Mon, 30 Jun 2025 13:00:21 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[This week, we’re going to try something new at 404 Media. Which is to say we’re going to try doing nothing at all. The TL;DR is that 404 Media is taking the week off, so this is the only email you’ll get from us this week. No posts on the website (except a scheduled one for the podcast). We will be back with your regularly scheduled dystopia Monday, July 7. We’re doing this to take a quick break to recharge. Over the nearly two years since we founded 404 Media, each of us have individually taken some (very limited) vacations. And when one of us takes off time it just means that the others have to carry their workload. We’re not taking this time to do an offsite, or brainstorm blue sky ideas. Some of us are quite literally gone fishin’. So, for the first time ever: A break!We are not used to breaks, because we know that the best way to build an audience and a business of people who read our articles is to actually write a lot of articles, and so that’s what we’ve been doing. The last few months have been particularly wild, as we’ve covered  of the , the , Trump’s , AI’s role in , the general  of the internet, etc etc etc. At the moment we have more story leads than we can possibly get to and are excited for the second half of the year. We’ve also published a lot of hopeful news, too, including instances where people  against  or solve universal mysteries, or when companies are forced to  in response to our reporting, or when lawmakers hold tech giants  as a result of . But in an industry that has become obsessed with doing more with less and publishing constantly, we have found that publishing quality journalism you can’t find anywhere else is a good way to run a business, which means we thankfully don’t have to cover everything, everywhere, all at once.When we founded 404 Media , we had no idea if anyone would subscribe, and we had no idea how it would go. We took zero investment from anyone and hoped that if we did good work often enough, enough people would decide that they wanted to support independent journalism that we could make a job out of it, and that we could make a sustainable business that would work for the long haul. We did not and do not take that support for granted. But because of your support, we now feel like we don’t have to scratch and claw for every possible new dollar we can get, and you have given us the breathing room in our business to quite literally take a breather, and to let the other folks who make this website possible, such as those who help us out with our social accounts, take a paid breather as well. We are not tired, exactly. In fact, we all feel more energized and ambitious than ever, knowing there are so many people out there who enjoy our work and are willing to financially support it. But we also don’t want to burn ourselves out and therefore, school’s out for summer (for one week). This week’s podcast is an interview Jason recorded with our friend Casey Johnston a few weeks ago; it’ll be the only new content this week. We’ll be back to it next Monday. Again, thank you all. Also, if you want, open thread in the comments to chat about whatever is going on out there or whatever is on your mind.]]></content:encoded></item><item><title>Next-Gen Brain Implants Offer New Hope for Depression</title><link>https://spectrum.ieee.org/deep-brain-stimulation-depression</link><author>Elie Dolgin</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEwOTY4Mi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgwMTQ4MzcwMX0.Dpb34ABBfeuODyIMl2R3d--Q7dHDKYadE_uo-e9Z4rY/image.png?width=600" length="" type=""/><pubDate>Mon, 30 Jun 2025 13:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Google inks its first fusion power deal with Commonwealth Fusion Systems</title><link>https://techcrunch.com/2025/06/30/google-inks-its-first-fusion-power-deal-with-commonwealth-fusion-systems/</link><author>Tim De Chant</author><category>tech</category><pubDate>Mon, 30 Jun 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google will invest in Commonwealth Fusion Systems and buy 200 MW of electricity from its forthcoming power plant.]]></content:encoded></item><item><title>KDE Improving Its Clock With Wayland Picture-In-Picture Protocol</title><link>https://www.phoronix.com/news/KDE-Clock-Wayland-PIP</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 30 Jun 2025 12:55:11 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE developers are improving its clock "KClock" app with leveraging the experimental Wayland picture-in-picture protocol support...]]></content:encoded></item><item><title>The Future of Travel Is Fast, Smart, and Sustainable</title><link>https://hackernoon.com/the-future-of-travel-is-fast-smart-and-sustainable?source=rss</link><author>150Sec</author><category>tech</category><pubDate>Mon, 30 Jun 2025 12:29:50 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Five years ago, the world stood still. Planes were grounded. Cities fell silent. And the travel industry, a $9 trillion economic engine, was brought to its knees.Yet while we waited, the pace of innovation accelerated, leading us to new advances that have fundamentally changed travel. Much has been written about AI-powered itineraries and personalized travel planning that influences  we go. However, the where is dependent on , and the mobility of tomorrow looks fundamentally different.A recent report issued by the World Travel & Tourism Council (WTTC) and Trip.com Group gives us a glimpse into how 16 major technologies in four different categories are set to transform travel and tourism. One of the most exciting shifts is in the future of mobility, where faster travel, sustainable transportation options, and smarter tools are set to redefine the way we move around the world.The future is high-speed, sustainable travelWith 40% of travellers willing to pay more for the convenience of avoiding layovers, it’s no surprise that the ability to reach a destination faster is a major draw. In the future of air travel, emerging innovations promise to slash travel time by as much as half, allowing passengers to spend more of their precious time enjoying the trip and less time stuck in transit. These incredible claims are thanks to supersonic travel.Earlier this year, Boom Supersonic made history when its XB-1 demonstrator aircraft broke the sound barrier, something no piloted civilian plane has done since the Concorde was retired in 2003. This milestone marks the beginning of the American aerospace company’s larger mission to bring commercial supersonic flights back to the skies by 2029, under the name Overture. \n  \n “It will no longer take double-digit hours to see Australia. Flight times to Asia or the Middle East will be cut in half”, says Boom on their website. “Envision flying Los Angeles to Tahiti in just over 4 hours or from Boston to Paris and back home in time to tuck the kids into bed”.In addition to significantly faster flight times, the company also offers 100% sustainable aviation fuel to reduce emissions and does not emit loud shocks in its exhaust plume like its supersonic predecessor, Concorde. Popular commercial airlines are already showing keen interest, with United and American Airlines securing 130 purchase order agreements globally.The sky isn’t the only place we are chasing velocity. Very high-speed rail trains offer an eco-friendly and convenient way to reach long-distance destinations faster than ever before. The WTTC report highlights the Japanese Shinkansen’s “bullet train” as an example, capable of travelling at speeds greater than 160 miles per hour (approximately 260 km/h).When it was first introduced in 1964, high-speed train technology revolutionized rail travel between Tokyo and Osaka and has since become the gold standard. Despite originating in the 1960s, these trains are only now seeing broader global adoption, due to a mix of evolving demand, economics, and technological advances. Governments and the private sector worldwide are building dedicated infrastructure to create a high-speed rail system that connects not only their cities but also with other countries.Europe is a perfect example of a region that is looking to evolve its cross-border mobility through high-speed train networks, with time, sustainability, and growth in mind. In December of last year, the highly anticipated Paris-Berlin route was launched, under a push between two of Europe’s most populous countries to offer greener travel options.Madrid and Lisbon, two cities set to co-host the 2030 FIFA World Cup, have just received the official approval from the European Commission to begin a project that will significantly reduce air travel between the two countries by connecting the capital directly by high-speed train. The project will cut travel by rail from nine to three hours, just in time for the world’s most anticipated football championship.These are just a few of many anticipated projects across Europe in response to the heightened popularity of train travel as it expands efficiency in the form of speed and connectivity. The region’s existing rail offerings still present a preference for trains over planes for [65% of business travellers](https://www.businesstravelnewseurope.com/Ground-Transport/Two-thirds-of-travellers-prefer-rail-over-air-when-cost-and-journey-time-are-similar#:~:text=Nearly%20two%2Dthirds%20(65%20per,new%20research%20from%20BCD%20Travel.), with respondents citing ease of access and the ability to work as part of their determination.The growing demand to travel slowWhile speed is still a draw for many travellers, there’s a noticeable shift toward slowing things down. Virginia Messina, the VP of advocacy and communications at the World Travel & Tourism Council, told 150Sec that more people are opting for slower ways to get around. \n  \n “There’s definitely a big trend in terms of slow travel, as they call it. Because maybe if you go on a train, you might go for a slightly longer period than just a weekend. You can see more by experiencing different countries.”The “slow travel” trend reflects a rise in travellers seeking a combination of an immersive cultural experience and environmentally conscious travel. According to the 2025 Hilton Trend Report, its popularity is rooted in a growing desire to “travel like a local”.Travelling like a resident requires knowledge of the commuting system, schedule, and modes of transportation used by locals, information that was once less easily attainable than it is today.The WTTC report calls attention to how cities that offer Mobility-as-a-Service (MaaS) in the form of accessibility to view and book reliable transportation options, will influence a better travel experience for its future visitors, “Cities that embrace MaaS and develop their public and shared transport networks may attract more visitors who are keen to use a variety of easily accessible transport options when they need it”.Spain, ranked the second most visited country after France, has seen tourism growth partly due to its sustained investment in mobility infrastructure. Just as crucial, however, is its commitment to MaaS, which plays a vital role in seamlessly connecting travellers to that infrastructure. It enables passengers to plan, navigate, and pay for journeys with ease.According to booking data from German travel platform Omio, ferry travel in Spain has exploded by 141% year-over-year, marking a noticeable surge across any major European country. This highlights the booming interest in slow modes of transportation for domestic island and international ferry routes.“Ferries are no longer plan B; they are at the heart of consumer travel experiences,” says Veronica Diquattro, President of Consumer and Supply Business for Europe at Omio. “What we’re seeing across Europe, and especially in Spain, is that travellers are prioritizing flexibility, comfort, and the ease of discovering hidden coastal gems.”Similar numbers have been reported in Canada too. BC Ferries, the primary ferry operator in British Columbia, confirmed a 21% increase in passenger travel compared to the previous year.“We’ve seen significant increases in both passenger and vehicle traffic compared to last year,” said Ritinder Matthew, BC Ferries’ director of media and issues, in an interview with Comox Valley Record. \n  \n Ferries might be easier on your wallet, but their real draw is how much smoother they make the whole journey. Today’s traveller wants to skip the airport crowds, dodge the extra baggage fees, and forget about racing the clock at security. Instead, people are leaning into this slower, more scenic way of getting around, which is guaranteed to offer more leg room and better coffee.As the relationship between technology, infrastructure, and traveller expectations continues to grow, the future of mobility in tourism won’t be defined by one singular speed or solution, but by choice. Whether crossing continents in half the time on a supersonic jet or gliding by sea via scenic coastlines, the evolving ecosystem of mobility is making the journey as meaningful as the destination. And in that shift, we’re not just moving faster—we’re moving smarter, more sustainably, and more connected than ever before.]]></content:encoded></item><item><title>Supreme Court Cripples FCC Further, Making Robocall Enforcement Likely Impossible</title><link>https://www.techdirt.com/2025/06/30/supreme-court-cripples-fcc-further-making-robocall-enforcement-likely-impossible/</link><author>Karl Bode</author><category>tech</category><pubDate>Mon, 30 Jun 2025 12:23:04 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Whether by dodgy Supreme Court ruling, executive order, mindless DOGE cuts, or captured regulators, the U.S. right wing, usually in lockstep with consolidated corporate power, are making massive, historic, and potentially irreversible inroads in destroying all federal corporate oversight, labor protections, public safety provisions, environmental standards, and regulatory autonomy.I bolded that last bit because it’s not clear the U.S. press and a huge swath of the electorate (or even many people in policy circles) have figured this out yet. A cornerstone of this effort has been the Supreme Court. Last year’s Loper Bright ruling effectively gutted any remaining independence of expert regulators, ensuring they literally can’t do much of anything without the explicit approval of a Congress too corrupt to function (and sometimes, not even then). If they do try, they’re all but guaranteed to be drowned in legal fights with deep-pocketed corporations for years. Class action plaintiffs in the case argued that the unsolicited faxes were in violation of the Telephone Consumer Protection Act (TCPA), which bans unsolicited communications with consumers without giving them a chance to opt out of the communications.  Consumer rights experts have long pointed out that shitheads and scammers have hijacked U.S. voice networks thanks to steady, generational lobbying by debt collectors and the marketing industry, who’ve ensured that oversight no longer functions. Still, every so often, the FCC would  to do something about the problem within the ever-shrinking confines of their legal authority. The McLaughlin case found its way to the Supreme Court because the District Court found that it was required to follow the new FCC order, though it disagreed with the FCC’s interpretation of the TCPA. The District Court also felt constrained by the Hobbs Act, 1950s era legislation long interpreted as barring district courts from meddling with and undermining a federal agency’s interpretation of a statute.“This is a significant ruling because it gives more explicit power to District Courts to disagree with an administrative ruling of a federal agency. It’s likely that there is a District Court somewhere in the country that will disagree with almost any federal agency ruling, meaning that it will be that much easier to tie up every decision made by the FCC or other federal agency in court.”Bogging any and all government oversight of corporate power in endless legal hell is, of course, the entire point. But this effort has historically been dressed up by the right wing and “free market” Libertarian folks as some kind of noble rebalancing of constitutional power. The lie is that regulators were “running amok” (a joke if you’ve watched the FCC fail to do basic things), and this somehow “fixed” it. The route the right wing is taking to effectively lobotomize corporate oversight is brutally efficient, but it’s also ironically so meandering, dull, and jam-packed with convoluted legalese, it  barely gets covered by the press. In this case, only a handful of outlets bothered to mention the June 20th ruling. But the real world harms of this entire movement will be kind of hard for the press and public to ignore. In the case of the FCC, it most assuredly means that the FCC will have even less authority to rein in shitty telecom monopolies. America’s already shitty robocall problem (a direct result of widespread corruption), will also absolutely be getting significantly worse:“This new ruling also has practical implications since it explicitly weakens FCC enforcement of the TCPA. Among other things, the TCPA rules are the FCC’s primary tool for its effort to restrain the use of autodialers and artificial voices used in spam messages to consumers.”You can see similar points made in the dissenting opinions. Great stuff! Very much the good faith, blue collar populism Trump is (ignorantly) lauded for. The FCC’s inability to police scams and fraud is only a small part of the picture. More broadly, regulators that govern every sensitive aspect of your lives — from health insurance to undercooked car automation — are finding themselves literally incapable of standing up to corporate power in the United States. That’s going to have dramatic, often deadly impacts on every last aspect of your lives. I genuinely don’t know what it takes to get the press and public to truly comprehend what’s happening. We’re going to see a steady parade of concussive, systemic failures to systems people to take for granted everywhere you look. All because rich corporate executives and their proxy “free market innovation” think tanks wanted to dress up unbridled greed as some sort of sophisticated, academic ethos. The last year has been a brutal, generational win for unchecked corporate power. The check is coming due, and none of it’s going to be subtle.]]></content:encoded></item><item><title>UV-C Light Kills Nearly Everything - Except This Unusual Organism</title><link>https://science.slashdot.org/story/25/06/30/0425217/uv-c-light-kills-nearly-everything---except-this-unusual-organism?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 30 Jun 2025 11:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Earth's ozone layer blocks the Sun's shortest wave radiation, called UV-C, which is so damaging to cells in high doses that it's a go-to sterilizer in hospitals," writes Slashdot reader sciencehabit. "UV-C is such a killer, in fact, that scientists have questioned whether life can survive on worlds that lack an ozone layer, such as Mars or distant exoplanets. 

"But research published this month in Astrobiology suggests one hardy lichen, a hybrid organism made of algae and fungi, may have cracked the UV-C code with a built-in sunscreen, despite never experiencing these rays in its long evolutionary history." 
 Science magazine explains:

When scientists brought a sample of the species, the common desert dweller Clavascidium lacinulatum, back to the lab, graduate student Tejinder Singh put the lichen through the wringer. First, Singh dehydrated the lichen, to make sure it couldn't grow back in real time and mask any UV damage. Then he placed the lichen a few centimeters under a UV lamp and blasted it with radiation. The lichen seemed just fine. 

So Singh purchased the most powerful UV-C lamp he could find online, capable of sending out 20 times more radiation than the amount expected on Mars. When he tested the lamp on the most radiation-resistant life form on Earth, the bacterium Deinococcus radiodurans, it died in less than a minute. After 3 months—likely the highest amount of UV-C radiation ever tested on an organism—Singh pulled the sample so he could finish his master's thesis in time. About half of the lichen's algal cells had survived. Then, when the team ground up and cultured part of the surviving lichen, about half of its algal cells sprouted new, green colonies after 2 weeks, showing it maintained the ability to reproduce. 
The species may provide a blueprint for surviving on Mars or exoplanets, which don't have an ozone layer to protect them.]]></content:encoded></item><item><title>AMD Instinct Accelerators With So Much vRAM Have Exposed Linux Hibernation Issues</title><link>https://www.phoronix.com/news/AMD-Too-Much-vRAM-RAM-Hibernate</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 30 Jun 2025 11:07:28 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Too much vRAM and too many Instinct accelerators per server is causing system hibernation to fail on some high-end AMD AI Linux-powered servers. Having eight accelerators each with 192GB of device memory can in turn cause system hibernation to run into problems if the Linux server has only 2TB of system RAM... But a new patch series was posted today in working to address this problem with the Linux kernel for high-end systems failing to hibernate. A similar issue is that when thawing the system the process can take nearly one hour due to the amount of memory...]]></content:encoded></item><item><title>It&apos;s Now Been Three Months Since The Last AerynOS Release</title><link>https://www.phoronix.com/news/AerynOS-Release-Turns-3-Months</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 30 Jun 2025 10:46:59 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Released three months ago was the first AerynOS ISO release for that Linux distribution led by Ikey Doherty and from there plans were laid to provide "accelerated delivery of milestone ISOs." But now Q2 is ending without any further announcements from this interesting Linux distribution formerly known as SerpentOS...]]></content:encoded></item><item><title>Btrfs Preps Experimental Large Data Folios For Better Performance</title><link>https://www.phoronix.com/news/Btrfs-Large-Data-Folios-Coming</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 30 Jun 2025 10:23:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[It looks like Linux 6.17 could end up enabling experimental support for large data folios that could help with bringing some performance improvements under real-world workloads for this copy-on-write file-system...]]></content:encoded></item><item><title>Fedora 44 Will Not Pursue The Idea Of Ending 32-bit x86 Software Packages</title><link>https://www.phoronix.com/news/Fedora-44-To-Keep-x86-32-bit</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 30 Jun 2025 10:12:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A proposal raised last week for Fedora 44 was to drop i686 support with ending multi-lib and x86 32-bit packages support. But following a fair amount of opposition to the idea, this matter isn't going to be pursued for next spring's Fedora 44 release...]]></content:encoded></item></channel></rss>