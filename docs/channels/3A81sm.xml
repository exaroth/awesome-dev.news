<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech</title><link>https://www.awesome-dev.news</link><description></description><item><title>Google rolls out its new Veo 3 video generation model globally</title><link>https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:56:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries. ]]></content:encoded></item><item><title>Linux 6.17 To Finish Clearing Out Old Code For OpenMoko Devices</title><link>https://www.phoronix.com/news/Linux-6.17-Dropping-OpenMoko-In</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:52:04 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Linux 6.17 is expected to clear out some final remnants of the OpenMoko Neo 1973 and Neo FreeRunner smartphone support from that Linux smartphone effort from two decades ago...]]></content:encoded></item><item><title>Lenovo Legion Go S HID Driver Posted For Linux</title><link>https://www.phoronix.com/news/Lenovo-Legion-Go-S-HID</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:30:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Linux support for the Lenovo Legion Go S gaming handheld continues to be improved upon thanks to the option of having Steam OS on this alternative to the Steam Deck...]]></content:encoded></item><item><title>Improved TTM Memory Management Eviction Submitted Ahead Of Linux 6.17</title><link>https://www.phoronix.com/news/Linux-6.17-TTM-Eviction</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:12:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Sent out today was the newest drm-misc-next pull request of changes built up over the past week for DRM-Next ahead of the Linux 6.17 kernel cycle. The drm-misc-next material is the usual random assortment of DRM display/graphics driver changes and core improvements, which this week includes some TTM eviction work...]]></content:encoded></item><item><title>Podcast: The Life Changing Power of Lifting</title><link>https://www.404media.co/404-media-podcast-casey-johnston/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/06/casey-jason.png" length="" type=""/><pubDate>Thu, 3 Jul 2025 10:00:49 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[For this week’s podcast, I’m talking to our friend Casey Johnston, a tech journalist turned fitness journalist turned independent journalist. Casey studied physics, which led her to tech journalism; she did some of my favorite coverage of Internet culture as well as Apple’s horrendous butterfly laptop keyboards. We worked together at VICE, where Casey was an editor and where she wrote Ask a Swole Woman, an advice column about weightlifting. After she left VICE, Casey , an independent site about weightlifting, but also about the science of diet culture, fitness influencers on the internet, the intersections of all those things, etc. She just wrote A Physical Education: How I Escaped Diet Culture and Gained the Power of Lifting, a really great reported memoir about how our culture and the media often discourages people from lifting, and how this type of exercise can be really beneficial to your brain and your body. I found the book really inspiring and actually started lifting right after I read it. In this interview we talk about her book, about journalism, about independent media, and how doing things like lifting weights and touching grass helps us navigate the world. Listen to the weekly podcast on , or YouTube. Become a paid subscriber for access to this episode's bonus content and to power our journalism. If you become a paid subscriber, check your inbox for an email from our podcast host Transistor for a link to the subscribers-only version! You can also add that subscribers feed to your podcast app of choice and never miss an episode that way. The email should also contain the subscribers-only unlisted YouTube link for the extended video version too. It will also be in the show notes in your podcast player. ]]></content:encoded></item><item><title>New Evidence That Some Supernovae May Be a &apos;Double Detonation&apos;</title><link>https://science.slashdot.org/story/25/07/03/0051240/new-evidence-that-some-supernovae-may-be-a-double-detonation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New evidence from a 300-year-old supernova remnant in the Large Magellanic Cloud suggests that some Type Ia supernovae may result from a "double detonation" -- where a helium shell ignites first, triggering a second core explosion in a white dwarf before it reaches critical mass. "While the physics of the process itself are interesting, the key question this raises is whether type Ia supernovae really are all equally bright," writes Ars Technica's John Timmer. "If they can detonate with substantially less mass than is needed for direct ignition of the core, then it's possible that some of them could be considerably less bright." However, the research team notes that additional factors -- such as the influence of binary systems or secondary detonations -- could further complicate the picture. Ars Technica reports: "The detonations in the carbon-oxygen core and the helium-rich shell result in qualitatively different yield products," the researchers behind the new work write in a paper describing it. In the paper, they focus on calcium, which there are two ways of producing. One is from the outer shell of helium, via fusion before the detonation dilutes the material. A second batch of calcium is produced through the fusion of the core material as it's ejected in the supernova, which prevents further fusion events from converting it to even heavier elements. (Material deeper in the core does end up getting fused into heavier material.) Because it's produced by both of the detonations, models predict that the expanding sphere of debris will contain two different shells of calcium, with some space in between them. To find evidence for these shells, the researchers checked an older supernova remnant, which allows enough time for the movement of material to separate the shells by enough distance that they can be resolved from Earth.
 
They focused their observations on a supernova remnant named SNR 0509-67.5, located in the nearby Large Magellanic Cloud. SNR 0509-67.5 is estimated to be a bit over 300 years old, meaning material has had enough time to move a significant distance away from the site of the explosion. Imaging using a spectrograph on the Very Large Telescope allowed them to resolve what, in effect, was a spherical sulfur sandwich, with the role of the bread played by calcium. In other words, if you were to travel away from the site of the explosion, you would first hit a layer of ionized calcium, followed by ionized sulfur, and then run into a second layer of ionized calcium. This is exactly what computer models that simulate double detonations predict. So, the researchers suggest it is strong support for that hypothesis. The researchers say that the details suggest that SNR 0509-67.5 was a white dwarf with roughly the same mass as the Sun when it exploded, and that its explosion was likely triggered by the detonation of a helium shell with only three percent of the Sun's mass.]]></content:encoded></item><item><title>Libreboot 25.06 Released With Support For Two More Outdated Systems</title><link>https://www.phoronix.com/news/Libreboot-25.06-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 09:55:22 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Libreboot 25.06 released this week as the newest version of this Coreboot downstream focused on shipping only with free and open-source components. But due to the strict open-source nature of Libreboot, it continues to primarily see support for long outdated platforms...]]></content:encoded></item><item><title>A New &apos;Interstellar Visitor&apos; Has Entered the Solar System</title><link>https://science.slashdot.org/story/25/07/03/0041226/a-new-interstellar-visitor-has-entered-the-solar-system?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Astronomers have detected a mysterious "interstellar object," dubbed A11pl3Z, speeding through the solar system at 152,000 mph. If confirmed, it would be just the third known interstellar visitor, following 'Oumuamua and Comet Borisov. The visiting space object will pass near Mars and the Sun later this year before leaving the solar system forever. Live Science reports: The newly discovered object, currently dubbed A11pl3Z, was first spotted in data collected between June 25 and June 29 by the Asteroid Terrestrial-impact Last Alert System (ATLAS), which automatically scans the night sky using telescopes in Hawaii and South Africa. The mystery object was confirmed by both NASA's Center for Near Earth Object Studies and the International Astronomical Union's Minor Planet Center on Tuesday (July 1), according to EarthSky.org.
 
A11pl3Z is most likely a large asteroid, or maybe a comet, potentially spanning up to 12 miles (20 kilometers). It is traveling toward the inner solar system at around 152,000 mph (245,000 km/h) and is approaching us from the part of the night sky where the bar of the Milky Way is located. Based on A11pl3Z's speed and trajectory, experts think it originated from beyond the sun's gravitational influence and has enough momentum to shoot straight through our cosmic neighborhood without slowing down. However, more observations are needed to tell for sure.]]></content:encoded></item><item><title>AI job predictions become corporate America’s newest competitive sport</title><link>https://techcrunch.com/2025/07/02/ai-job-predictions-become-corporate-americas-newest-competitive-sport/</link><author>Connie Loizos</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:30:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In late May, Anthropic CEO Dario Amodei appeared to kick open the door on a sensitive topic, warning that half of entry-level jobs could vanish within five years because of AI and push U.S. unemployment up to 20%. But Amodei is far from alone in sharing aloud that he foresees a workforce bloodbath. A new […]]]></content:encoded></item><item><title>French B2B neobank Qonto reaches 600,000 customers, files for banking license</title><link>https://techcrunch.com/2025/07/02/french-b2b-neobank-qonto-reaches-600000-customers-files-for-banking-license/</link><author>Anna Heim</author><category>tech</category><pubDate>Thu, 3 Jul 2025 04:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Qonto, which targets European freelancers and SMBs, currently operates with a payment institution license it obtained in 2018.]]></content:encoded></item><item><title>Data Breach Reveals Catwatchful &apos;Stalkerware&apos; Is Spying On Thousands of Phones</title><link>https://yro.slashdot.org/story/25/07/03/0023253/data-breach-reveals-catwatchful-stalkerware-is-spying-on-thousands-of-phones?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: A security vulnerability in a stealthy Android spyware operation called Catwatchful has exposed thousands of its customers, including its administrator. The bug, which was discovered by security researcher Eric Daigle, spilled the spyware app's full database of email addresses and plaintext passwords that Catwatchful customers use to access the data stolen from the phones of their victims. [...] According to a copy of the database from early June, which TechCrunch has seen, Catwatchful had email addresses and passwords on more than 62,000 customers and the phone data from 26,000 victims' devices.
 
Most of the compromised devices were located in Mexico, Colombia, India, Peru, Argentina, Ecuador, and Bolivia (in order of the number of victims). Some of the records date back to 2018, the data shows. The Catwatchful database also revealed the identity of the spyware operation's administrator, Omar Soca Charcov, a developer based in Uruguay. Charcov opened our emails, but did not respond to our requests for comment sent in both English and Spanish. TechCrunch asked if he was aware of the Catwatchful data breach, and if he plans to disclose the incident to its customers. Without any clear indication that Charcov will disclose the incident, TechCrunch provided a copy of the Catwatchful database to data breach notification service Have I Been Pwned. The stalkerware operation uses a custom API and Google's Firebase to collect and store victims' stolen data, including photos and audio recordings. According to Daigle, the API was left unauthenticated, exposing sensitive user data such as email addresses and passwords.
 
The hosting provider temporarily suspended the spyware after TechCrunch disclosed this vulnerability but it returned later on HostGator. Despite being notified, Google has yet to take down the Firebase instance but updated Google Play Protect to detect Catwatchful.
 
While Catwatchful claims it "cannot be uninstalled," you can dial "543210" and press the call button on your Android phone to reveal the hidden app. As for its removal, TechCrunch has a general how-to guide for removing Android spyware that could be helpful.]]></content:encoded></item><item><title>TN Govt. Saves School Children From Smut Like Magic Tree House, Calvin &amp; Hobbes, &amp; A Light In The Attic</title><link>https://www.techdirt.com/2025/07/02/tn-govt-saves-school-children-from-smut-like-magic-tree-house-calvin-hobbes-a-light-in-the-attic/</link><author>Dark Helmet</author><category>tech</category><pubDate>Thu, 3 Jul 2025 02:43:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Book bans are all the rage these days, as you likely well know. Far too many people, and folks in government more importantly, seem to have read Ray Bradbury’s  not as a lesson in the dangers of new media, but as some sort of instruction manual for how to treat literature. But the real story here is that a bunch of cowardly state and federal politicians are placating the desires largely of the religious right, who are seeking to tightly control the books that children have access to in public, secular schools. And if you can’t manage to understand how plainly that is the antithesis of our form of government, then you’re beyond help.But because authoritarianism makes a fool of itself as a habit, and religiously-based authoritarianism all the moreso, then end result of these attempts at censorship always eventually reveal themselves as absurd. And if you need an example of that, you need only look at the state of Tennessee.Magic Tree House author , children’s poet  and Calvin and Hobbes cartoonist have joined Judy Blume, Sarah J. Maas, Eric Carle and  on a mind-boggling list of hundreds of books purged from some Tennessee school libraries.The removals are the result of a growing political movement to control information through book banning. In 2024, the state legislature amended the “Age-Appropriate Materials Act of 2022” to specify that any materials that “in whole or in part” contain any “nudity, or descriptions or depictions of sexual excitement, sexual conduct, excess violence, or sadomasochistic abuse” are inappropriate for all students and do not belong in a school library. This change means books are not evaluated as a whole, and excerpts can be considered without context, if they have any content that is deemed to cross these lines. This leaves no room for educators and librarians to curate collections that reflect the real world and serve the educational needs of today’s students.And because you have groups of far-right activists marching around looking for any scintilla of material over which they can manufacture faux outrage, you get these examples of books being banned for their terrible, awful, smutty content. Such as a , book that was banned because it had this pornographical image on its cover:Special thanks to Mike Masnick for briefly allowing me to post porn images on Techdirt. And for all of you whose naughty bits are currently twitching due to that book cover, I offer you my sincerest apologies.But if you thought  was bad, check out this panel image from a  book that got it banned. Here we have the nude image of a child on full display.Now, I sure hope everyone realizes that the above is a dalliance into sarcasm, because I was laying it on quite thick. I grew up on , not to mention Shel Silverstein’s , which was also banned. Why? More butts, that’s why. And, because the universe is not without a sense of irony, one school even had to ban a book authored by an alumnus.Oak Ridge Schools, where a significant number of the bans target art history books, even removed Richard Jolley: Sculptor of Glass, a collection of works by the artist, who graduated from Oak Ridge High School.“Regarding the book written by Mr. Jolley, we were thrilled to feature a book written by an ORHS alumni on our shelves and were equally disappointed to have to remove it,” Molly Gallagher Smith, an Oak Ridge Schools spokeswoman, told WBIR. “Unfortunately, as an artist, Mr. Jolley’s book features depictions of the human body that are in direct violation of the law.”There are more and the bans hit all the notes you would expect: LGBTQ+ material, books about the Holocaust, books about African American contributions to government and science, and, because of course,  itself.Now, this is indeed all absurd, but it isn’t remotely funny. There is a ton of literature, hundreds of books, that are being banned under this Tennessee law. Many of them reportedly without going through any review process.And many of the bans are coming without any review or discussion. The Tennessee Association of School Libraries found in a survey of its members that in 20% of school districts, books were removed from the shelves at the command of district leaders without any sort of review process. “Librarians and educators are concerned that we will end up pulling a massive amount of books without looking at the books as a whole,” one member said in the survey. “It’s a slippery slope,” said another, “and I’m fearful of the next topic that will be regulated.”Open up book bans to the frothy-mouthed mob. What could possibly go wrong, other than keeping valuable literature out of the hands of our children?]]></content:encoded></item><item><title>Proposed Budget Seeks To Close Mauna Loa Observatory&apos;s Climate CO2 Study</title><link>https://news.slashdot.org/story/25/07/03/0031226/proposed-budget-seeks-to-close-mauna-loa-observatorys-climate-co2-study?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Slashdot regularly posts milestones on CO2 levels reported by the Mauna Loa Observatory," writes longtime Slashdot reader symbolset, pointing to a new article highlighting how the Trump administration's proposed budget would eliminate funding for the lab's carbon dioxide monitoring. "Continuous observation records since 1958 will end with the new federal budget as ocean and atmospheric sciences are defunded." From a report: [I]t's the Mauna Loa laboratory that is the most prominent target of the President Donald Trump's climate ire, as measurements that began there in 1958 have steadily shown CO2's upward march as human activities have emitted more and more of the planet-warming gas each year. The curve produced by the Mauna Loa measurements is one of the most iconic charts in modern science, known as the Keeling Curve, after Charles David Keeling, who was the researcher who painstakingly collected the data. His son, Ralph Keeling, a professor at the Scripps Institution of Oceanography at UC San Diego, now oversees collecting and updating that data.
 
Today, the Keeling Curve measurements are made possible by the National Oceanic and Atmospheric administration, but the data gathering and maintenance of the historical record also is funded by Schmidt Sciences and Earth Networks, according to the Keeling Curve website. In the event of a NOAA shut down of the lab, Scripps could seek alternate sources of funding to host the instruments atop the same peak or introduce a discontinuity in the record by moving the instruments elsewhere in Hawaii.
 
The proposal to shut down Mauna Loa had been made public previously but was spelled out in more detail on Monday when NOAA submitted a budget document (PDF) to Congress. It made more clear that the Trump administration envisions eliminating all climate-related research work at NOAA, as had been proposed in Project 2025, the conservative blueprint for overhauling the government. It would do this in large part by cutting NOAA's Office of Oceanic and Atmospheric Research entirely, including some labs that are also involved in improving weather forecasting. NOAA has long been one of the world's top climate science agencies, but the administration would steer it instead towards being more focused on operational weather forecasting and warning responsibilities.]]></content:encoded></item><item><title>Foxconn Mysteriously Tells Chinese Workers To Quit India and Return To China</title><link>https://apple.slashdot.org/story/25/07/03/0016210/foxconn-mysteriously-tells-chinese-workers-to-quit-india-and-return-to-china?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple's expansion in India has hit a snag as Foxconn has sent over 300 Chinese workers back to China, potentially reducing production efficiency just as mass manufacturing of the iPhone 17 begins. AppleInsider reports: It's not known why Foxconn has done this, nor is it clear whether workers have been laid off or redeployed to the company's facilities in China. The move, though, does follow Beijing officials reportedly working to prevent firms moving away from China. Those officials are said to have been verbally encouraging China's local governments and regulatory bodies to curb exports of equipment or technologies to India and Southeast Asia.
 
Overall, China has been making it harder for skilled labor to leave the country. It's not clear how any changes have specifically affected Chinese workers who had already left.What is clear is that Foxconn has used many experienced Chinese engineers as it attempts to rapidly expand in India. It's said, too, that Chinese managers have been vital in training Foxconn staff in India. Since that training has been ongoing for some years, and since at least most of Foxconn's production lines have been set up, it's said that there will not be an impact on the quality of manufacturing. But one source said the changes will impact efficiency on the production line.]]></content:encoded></item><item><title>Hacker With &apos;Political Agenda&apos; Stole Data From Columbia, University Says</title><link>https://news.slashdot.org/story/25/07/03/0012219/hacker-with-political-agenda-stole-data-from-columbia-university-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A politically motivated hacker breached Columbia University's IT systems, stealing vast amounts of sensitive student and employee data -- including admissions decisions and Social Security numbers. The Record reports: The hacker reportedly provided Bloomberg News with 1.6 gigabytes of data they claimed to have stolen from the university, including information from 2.5 million applications going back decades. The stolen data the outlet reviewed reportedly contains details on whether applicants were rejected or accepted, their citizenship status, their university ID numbers and which academic programs they sought admission to. While the hacker's claims have not been independently verified, Bloomberg said it compared data provided by the hacker to that belonging to eight Columbia applicants seeking admission between 2019 and 2024 and found it matched.
 
The threat actor reportedly told Bloomberg he was seeking information that would indicate whether the university continues to use affirmative action in admissions despite a 2023 Supreme Court decision prohibiting the practice. The hacker told Bloomberg he obtained 460 gigabytes of data in total -- after spending two months targeting and penetrating increasingly privileged layers of the university's servers -- and said he harvested information about financial aid packages, employee pay and at least 1.8 million Social Security numbers belonging to employees, applicants, students and their family members.]]></content:encoded></item><item><title>Steam On Linux Usage Dips Slightly For June, AMD Linux CPU Usage Hits 69%</title><link>https://www.phoronix.com/news/Steam-June-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 00:38:36 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Steam Survey issues prevented the survey results from being posted on the evening of the 1st as is traditionally done, but the results were just uploaded now to the Steam website. Steam on Linux usage dipped slightly but overall remains healthy with much excitement still around the Steam Deck and SteamOS efforts...]]></content:encoded></item><item><title>Debian 13 Installer RC2 Fixes An Annoying Issue, Improves Btrfs Rescue Handling</title><link>https://www.phoronix.com/news/Debian-Installer-13-Trixie-RC2</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 00:28:20 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following last month's release of Debian Installer Trixie RC1 as the installer for the upcoming Debian 13.0 release, a second release candidate was issued today for testing...]]></content:encoded></item><item><title>Intel&apos;s New CEO Explores Big Shift In Chip Manufacturing Business</title><link>https://tech.slashdot.org/story/25/07/02/2149214/intels-new-ceo-explores-big-shift-in-chip-manufacturing-business?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Reuters: Intel's new chief executive is exploring a big change to its contract manufacturing business to win major customers, two people familiar with the matter told Reuters, in a potentially expensive shift from his predecessor's plans. The new strategy for Intel's foundry business would mean offering outside customers a newer generation of technology, the people said. That next-generation chipmaking process, analysts believe, will be more competitive against Taiwan Semiconductor Manufacturing Co in trying to land major customers such as Apple or Nvidia.
 
Since taking the company's helm in March, CEO Lip-Bu Tan has moved fast to cut costs and find a new path to revive the ailing U.S. chipmaker. By June, he started voicing that a manufacturing process known as 18A, in which prior CEO Pat Gelsinger had invested heavily, was losing its appeal to new customers, said the sources, who spoke on condition of anonymity. To put aside external sales of 18A and its variant 18A-P, manufacturing processes that have cost Intel billions of dollars to develop, the company would have to take a write-off, one of the people familiar with the matter said. Industry analysts contacted by Reuters said such a charge could amount to a loss of hundreds of millions, if not billions, of dollars.
 
Intel declined to comment on such "hypothetical scenarios or market speculation." It said the lead customer for 18A has long been Intel itself, and it aims to ramp production of its "Panther Lake" laptop chips later in 2025, which it called the most advanced processors ever designed and manufactured in the United States. Persuading outside clients to use Intel's factories remains key to its future. As its 18A fabrication process faced delays, rival TSMC's N2 technology has been on track for production. Tan's preliminary answer to this challenge: focus more resources on 14A, a next-generation chipmaking process where Intel expects to have advantages over Taiwan's TSMC, the two sources said. The move is part of a play for big customers like Apple and Nvidia, which currently pay TSMC to manufacture their chips.]]></content:encoded></item><item><title>OpenAI condemns Robinhood’s ‘OpenAI tokens’</title><link>https://techcrunch.com/2025/07/02/openai-condemns-robinhoods-openai-tokens/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Wed, 2 Jul 2025 23:43:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI wants to make clear that Robinhood's sale of "OpenAI tokens" will not give everyday consumers equity — or stock — in OpenAI.]]></content:encoded></item><item><title>Nintendo Locked Down the Switch 2&apos;s USB-C Port, Broke Third-Party Docking</title><link>https://hardware.slashdot.org/story/25/07/02/2136241/nintendo-locked-down-the-switch-2s-usb-c-port-broke-third-party-docking?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Two accessory manufacturers have told The Verge that Nintendo has intentionally locked down the Switch 2's USB-C port using a new encryption scheme, preventing compatibility with third-party docks and accessories. "I haven't yet found proof of that encryption chip myself -- but when I analyzed the USB-C PD traffic with a Power-Z tester, I could clearly see the new Nintendo Switch not behaving like a good USB citizen should," writes The Verge's Sean Hollister. From the report: If you've been wondering why there are basically no portable Switch 2 docks on the market, this is the reason. Even Jsaux, the company that built its reputation by beating the Steam Deck dock to market, tells us it's paused its plans to build a Switch 2 dock because of Nintendo's actions. It's not simply because the Switch 2 now requires more voltage, as was previously reported; it's that Nintendo has made things even more difficult this generation.]]></content:encoded></item><item><title>Wonder Dynamics co-founder Nikola Todorovic joins the AI Stage at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/07/02/wonder-dynamics-co-founder-nikola-todorovic-joins-the-ai-stage-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 2 Jul 2025 23:10:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TechCrunch Disrupt 2025 is back at Moscone West in San Francisco from October 27–29, bringing together 10,000+ startup and VC leaders to dig into what’s next in tech. And when it comes to artificial intelligence, the conversations aren’t just technical — they’re creative, cinematic, and boundary-pushing. That’s why Nikola Todorovic is headed to the AI […]]]></content:encoded></item><item><title>The NO FAKES Act Has Changed – And It’s So Much Worse</title><link>https://www.techdirt.com/2025/07/02/the-no-fakes-act-has-changed-and-its-so-much-worse/</link><author>katharine.trendacosta</author><category>tech</category><pubDate>Wed, 2 Jul 2025 22:43:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[A bill purporting to target the issue of misinformation and defamation caused by generative AI has mutated into something that could change the internet forever, harming speech and innovation from here on out.The Nurture Originals, Foster Art and Keep Entertainment Safe (NO FAKES) Act aims to address understandable concerns about generative AI-created “replicas” by creating a broad new intellectual property right. That approach was the first mistake: rather than giving people targeted tools to protect against harmful misrepresentations—balanced against the need to protect legitimate speech such as parodies and satires—the original NO FAKES just federalized an image-licensing system.The updated bill doubles down on that initial mistaken approach by mandating a whole new censorship infrastructure for that system, encompassing not just images but the products and services used to create them, with few safeguards against abuse.The new version of NO FAKES requires almost every internet gatekeeper to create a system that will a) take down speech upon receipt of a notice; b) keep down any recurring instance—meaning, adopt inevitably overbroad replica filters on top of the already deeply flawed copyright filters;  c) take down and filter tools that might have been used to make the image; and d) unmask the user who uploaded the material based on nothing more than the say so of person who was allegedly “replicated.”This bill would be a disaster for internet speech and innovation.The first version of NO FAKES focused on digital replicas. The new version goes further, targeting tools that can be used to produce images that aren’t authorized by the individual, anyone who owns the rights in that individual’s image, or the law. Anyone who makes, markets, or hosts such tools is on the hook. There are some limits—the tools must be primarily designed for, or have only limited commercial uses other than making unauthorized images—but those limits will offer cold comfort to developers given that they can be targeted based on nothing more than a bare allegation. These provisions effectively give rights-holders the veto power on innovation they’ve long sought in the copyright wars, based on the same tech panics. Takedown Notices and Filter MandateThe first version of NO FAKES set up a notice and takedown system patterned on the DMCA, with even fewer safeguards. NO FAKES expands it to cover more service providers and require those providers to not only take down targeted materials (or tools) but keep them from being uploaded in the future.  In other words, adopt broad filters or lose the safe harbor.But copyright filters are not yet required by law. NO FAKES would create a legal mandate that will inevitably lead to hecklers’ vetoes and other forms of over-censorship.The bill does contain carve outs for parody, satire, and commentary, but those will also be cold comfort for those who cannot afford to litigate the question.Threats to Anonymous SpeechAs currently written, NO FAKES also allows anyone to get a subpoena from a court clerk—not a judge, and without any form of proof—forcing a service to hand over identifying information about a user.We’ve already seen abuse of a similar system in action. In copyright cases, those unhappy with the criticisms being made against them get such subpoenas to silence critics. Often that the criticism includes the complainant’s own words as proof of the criticism, an ur-example of fair use. But the subpoena is issued anyway and, unless the service is incredibly on the ball, the user can be unmasked.Not only does this chill further speech, the unmasking itself can cause harm to users. Either reputationally or in their personal life.Most of us are very unhappy with the state of Big Tech. It seems like not only are we increasingly forced to use the tech giants, but that the quality of their services is actively degrading. By increasing the sheer amount of infrastructure a new service would need to comply with the law, NO FAKES makes it harder for any new service to challenge Big Tech. It is probably not a coincidence that some of these very giants are okay with this new version of NO FAKES.Requiring removal of tools, apps, and services could likewise stymie innovation. For one, it would harm people using such services for otherwise lawful creativity.  For another, it would discourage innovators from developing new tools. Who wants to invest in a tool or service that can be forced offline by nothing more than an allegation?This bill is a solution in search of a problem. Just a few months ago, Congress passed Take It Down, which targeted images involving intimate or sexual content. That deeply flawed bill pressures platforms to actively monitor online speech, including speech that is presently encrypted. But if Congress is really worried about privacy harms, it should at least wait to see the effects of the last piece of internet regulation before going further into a new one. Its failure to do so makes clear that this is not about protecting victims of harmful digital replicas.NO FAKES is designed to consolidate control over the commercial exploitation of digital images, not prevent it. Along the way, it will cause collateral damage to all of us.]]></content:encoded></item><item><title>Grammarly Acquires AI Email Client Superhuman</title><link>https://slashdot.org/story/25/07/02/2128229/grammarly-acquires-ai-email-client-superhuman?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Grammarly has acquired the AI email client Superhuman to enhance its AI-driven productivity suite and expand AI capabilities within email communication. Financial terms of the deal were not disclosed but Superhuman CEO Rahul Vohra and his team will be joining the AI writing company. TechCrunch reports: Superhuman was founded by Rahul Vohra, Vivek Sodera, and Conrad Irwin. The company raised more than $114 million in funding from backers including a16z, IVP, and Tiger Global, with its last valuation at $825 million, according to data from venture data analytics firm Traxcn. "With Superhuman, we can deliver that future to millions more professionals while giving our existing users another surface for agent collaboration that simply doesn't exist anywhere else. Email isn't just another app; it's where professionals spend significant portions of their day, and it's the perfect staging ground for orchestrating multiple AI agents simultaneously," Shishir Mehrotra, CEO of Grammarly, said in a statement.
 
With this deal, CEO Vohra and other Superhuman employees are moving over to Grammarly. "Email is the main communication tool for billions of people worldwide and the number-one use case for Grammarly customers. By joining forces with Grammarly, we will invest even more in the core Superhuman experience, as well as create a new way of working where AI agents collaborate across the communication tools that we all use every day," Rahul Vohra, CEO of Superhuman, said in a statement.]]></content:encoded></item><item><title>NYT To Start Searching Deleted ChatGPT Logs After Beating OpenAI In Court</title><link>https://yro.slashdot.org/story/25/07/02/2122230/nyt-to-start-searching-deleted-chatgpt-logs-after-beating-openai-in-court?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: Last week, OpenAI raised objections in court, hoping to overturn a court order requiring the AI company to retain all ChatGPT logs "indefinitely," including deleted and temporary chats. But Sidney Stein, the US district judge reviewing OpenAI's request, immediately denied OpenAI's objections. He was seemingly unmoved by the company's claims that the order forced OpenAI to abandon "long-standing privacy norms" and weaken privacy protections that users expect based on ChatGPT's terms of service. Rather, Stein suggested that OpenAI's user agreement specified that their data could be retained as part of a legal process, which Stein said is exactly what is happening now.
 
The order was issued by magistrate judge Ona Wang just days after news organizations, led by The New York Times, requested it. The news plaintiffs claimed the order was urgently needed to preserve potential evidence in their copyright case, alleging that ChatGPT users are likely to delete chats where they attempted to use the chatbot to skirt paywalls to access news content. A spokesperson told Ars that OpenAI plans to "keep fighting" the order, but the ChatGPT maker seems to have few options left. They could possibly petition the Second Circuit Court of Appeals for a rarely granted emergency order that could intervene to block Wang's order, but the appeals court would have to consider Wang's order an extraordinary abuse of discretion for OpenAI to win that fight.
 
In the meantime, OpenAI is negotiating a process that will allow news plaintiffs to search through the retained data. Perhaps the sooner that process begins, the sooner the data will be deleted. And that possibility puts OpenAI in the difficult position of having to choose between either caving to some data collection to stop retaining data as soon as possible or prolonging the fight over the order and potentially putting more users' private conversations at risk of exposure through litigation or, worse, a data breach. [...]
 
Both sides are negotiating the exact process for searching through the chat logs, with both parties seemingly hoping to minimize the amount of time the chat logs will be preserved. For OpenAI, sharing the logs risks revealing instances of infringing outputs that could further spike damages in the case. The logs could also expose how often outputs attribute misinformation to news plaintiffs. But for news plaintiffs, accessing the logs is not considered key to their case -- perhaps providing additional examples of copying -- but could help news organizations argue that ChatGPT dilutes the market for their content. That could weigh against the fair use argument, as a judge opined in a recent ruling that evidence of market dilution could tip an AI copyright case in favor of plaintiffs.]]></content:encoded></item><item><title>Lucid sales inch forward as EV maker pushes to ramp Gravity production</title><link>https://techcrunch.com/2025/07/02/lucid-sales-inch-forward-as-ev-maker-pushes-to-ramp-gravity-production/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Wed, 2 Jul 2025 21:47:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Lucid sold 3,309 vehicles in the second quarter, a new record for the  EV maker. Still, the company must ramp up production of its Gravity SUV to meet its 2025 target. ]]></content:encoded></item><item><title>Google Ordered To Pay $315 Million for Taking Data From Idle Android Phones</title><link>https://yro.slashdot.org/story/25/07/02/1818254/google-ordered-to-pay-315-million-for-taking-data-from-idle-android-phones?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 21:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A California jury has ordered Google to pay $314.6 million to Android smartphone users in the state after finding the company liable for collecting data from idle devices without permission. 

The San Jose jury ruled Tuesday that Google sent and received information from phones while idle, creating "mandatory and unavoidable burdens shouldered by Android device users for Google's benefit." The 2019 class action represented an estimated 14 million Californians who argued Google consumed their cellular data for targeted advertising purposes.]]></content:encoded></item><item><title>Be Polarizing as a Job Seeker</title><link>https://spectrum.ieee.org/be-a-polarizing-job-seeker</link><author>Rahul Pandey</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81OTEwNDExMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2MjIxNDM1NH0.KU_HC9RlAuzaq6NFugnkIjhO6mrvJzTfytOINkYN-Lg/image.jpg?width=600" length="" type=""/><pubDate>Wed, 2 Jul 2025 21:05:35 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your goal is to be an exceptional fit for one specific job]]></content:encoded></item><item><title>Amazon To Shut Down Its Freevee App Next Month</title><link>https://entertainment.slashdot.org/story/25/07/02/1844245/amazon-to-shut-down-its-freevee-app-next-month?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Amazon plans to shut down its standalone Freevee app in August, according to an in-app notice to users. From a report: The free, ad-supported streaming service is directing viewers to continue watching Freevee content on Prime Video. 

"Prime Video is the new exclusive home for Freevee Tv show, movies, and Live TV," the notice to readers states. "The Freevee app will be accessible until August 2025. Continue watching your favorite Free Originals and our library of hit movies, shows, and live TV on Prime Video for free, no subscription needed. Download Prime Video to get started and sign-in with your Amazon account."]]></content:encoded></item><item><title>Our National Robocall Nightmare Is Getting Worse Under Donald Trump</title><link>https://www.techdirt.com/2025/07/02/our-national-robocall-nightmare-is-getting-worse-under-donald-trump/</link><author>Karl Bode</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:40:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[According to the latest data on robocalls from the YouMail Robocall Index, the scale of the U.S. robocall problem has grown by another eleven percent year over year. U.S. consumers received just over 4.8 billion robocalls in May. We’ve normalized ceding our primary voice communications platforms to corporations, debt collectors, and scammers, and there’s every indication it’s going to get worse under Donald Trump.YouGov’s latest study found that “just” 14 percent of May’s robocall total was from “scammers.” Even before Trump, a corrupted court system had consistently limited the FCC’s authority to combat robocalls. Corrupt lawmakers and regulators, cowed into blind obedience by a massive, generational, cross-industry-lobbying campaign, like to keep the focus on , when many “legit” companies, again, leverage the exact same tactics as scammers.As a result, federal regulators refuse to hold large phone companies accountable for their lagging efforts to combat fraud and spam. Case in point: Truecaller’s U.S. Spam and Scam Report found that half of all major U.S. phone companies earned a D or F in their efforts to combat annoying robocalls and scams. Functional, developed countries (even many less developed ones) don’t have these problems.So while the FCC is supposed to enforce robocall offenses and levy fines, terrible court rulings mean they aren’t allowed to  fines. That’s left to the DOJ, which routinely just… doesn’t bother. As a result a comically small volume of the overall fines levied are ever actually collected. For example between 2015 and 2019 the FCC issued $208.4 million in robocall fines, but collected just .And again, this is all  Trump 2.0. And before largely unregulated AI.Trump FCC boss Brendan Carr has been promising to take a hatchet to whatever is left of U.S. corporate oversight as part of his “delete, delete, delete” deregulatory initiative. Big telecoms and robocallers have been making it very clear they’re very excited about it. Debt collectors in particular are very eager to roll back already flimsy rules governing how badly they can harass people they already know can’t pay. Like so many systemic U.S. problems, the robocall menace isn’t something that gets fixed without first embracing much broader corruption, campaign finance, lobbying, and legal reforms. That is, obviously and indisputably, not something that’s happening under Trump and his sycophantic regulators and telecom industry-coddling courts.]]></content:encoded></item><item><title>Vera Rubin: This Is How Far Engineers Go to Explore the Universe</title><link>https://spectrum.ieee.org/vera-rubin-engineering</link><author>Harry Goldstein</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExMzgwMi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgwNjc2Mzc0M30.cnVKj7yANaP42fkxKgybLtZtzalpzw4fL5guP1wK7aI/image.png?width=600" length="" type=""/><pubDate>Wed, 2 Jul 2025 20:26:42 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[And how far we went to tell their story]]></content:encoded></item><item><title>Substack brings new updates to livestreaming as it increases video push</title><link>https://techcrunch.com/2025/07/02/substack-brings-new-updates-to-livestreaming-as-it-increases-video-push/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:09:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The recent update enables creators to share clips of their live videos on Notes, and Substack will notify them in real time about the performance. ]]></content:encoded></item><item><title>ZLUDA Making Progress In 2025 On Bringing CUDA To Non-NVIDIA GPUs</title><link>https://www.phoronix.com/news/ZLUDA-Q2-2025-Update</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:01:44 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The ZLUDA open-source effort that started off a half-decade ago as a drop-in CUDA implementation for Intel GPUs and then for several years was funded by AMD as a CUDA implementation for Radeon GPUs atop ROCm and then open-sourced but then reverted has been continuing to push along a new path since last year. The current take on ZLUDA is a multi-vendor CUDA implementation for non-NVIDIA GPUs for AI workloads and more. More progress was made during Q2 on this effort...]]></content:encoded></item><item><title>China&apos;s Giant New Gamble With Digital IDs</title><link>https://yro.slashdot.org/story/25/07/02/1827222/chinas-giant-new-gamble-with-digital-ids?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China will launch digital IDs for internet use on July 15th, transferring online verification from private companies to government control. Users obtain digital IDs by submitting personal information including facial scans to police via an app. A pilot program launched one year ago enrolled 6 million people. 

The system currently remains voluntary, though officials and state media are pushing citizens to register for "information security." Companies will see only anonymized character strings when users log in, while police retain exclusive access to personal details. The program replaces China's existing system requiring citizens to register with companies using real names before posting comments, gaming, or making purchases. 

Police say they punished 47,000 people last year for spreading "rumours" online. The digital ID serves a broader government strategy to centralize data control. State planners classify data as a production factor alongside labor and capital, aiming to extract information from private companies for trading through government-operated data exchanges.]]></content:encoded></item><item><title>AT&amp;T rolls out Wireless Account Lock protection to curb the SIM-swap scourge</title><link>https://arstechnica.com/security/2025/07/att-rolls-out-wireless-account-lock-protection-to-curb-the-sim-swap-scourge/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/02/sim-cards.jpeg" length="" type=""/><pubDate>Wed, 2 Jul 2025 19:28:27 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[AT&T is rolling out a protection that prevents unauthorized changes to mobile accounts as the carrier attempts to fight a costly form of account hijacking that occurs when a scammer swaps out the SIM card belonging to the account holder.The technique, known as SIM swapping or port-out fraud, has been a scourge that has vexed wireless carriers and their millions of subscribers for years. An indictment filed last year by federal prosecutors alleged that a single SIM swap scheme netted $400 million in cryptocurrency. The stolen funds belonged to dozens of victims who had used their phones for two-factor authentication to cryptocurrency wallets.Wireless Account Lock debutA separate scam from 2022 gave unauthorized access to a T-Mobile management platform that subscription resellers, known as mobile virtual network operators, use to provision services to their customers. The threat actor gained access using a SIM swap of a T-Mobile employee, a phishing attack on another T-Mobile employee, and at least one compromise of an unknown origin.]]></content:encoded></item><item><title>Could Google’s Veo 3 be the start of playable world models?</title><link>https://techcrunch.com/2025/07/02/could-googles-veo-3-be-the-start-of-playable-world-models/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:22:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Demis Hassabis, CEO of Google DeepMind, hinted Tuesday evening that Veo 3, Google’s latest video generating model, could potentially be used for video games. ]]></content:encoded></item><item><title>AI Note Takers Are Increasingly Outnumbering Humans in Workplace Video Calls</title><link>https://slashdot.org/story/25/07/02/194224/ai-note-takers-are-increasingly-outnumbering-humans-in-workplace-video-calls?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AI-powered note-taking apps are increasingly attending workplace meetings in place of human participants, creating situations where automated transcription bots outnumber actual attendees. 

Major platforms including Zoom, Microsoft Teams and Google Meet now offer built-in note-taking features that record, transcribe and summarize meetings for invited participants who don't attend. The technology operates under varying legal frameworks, with most states requiring only single-party consent for recording while California, Florida, and Pennsylvania mandate all-party approval.]]></content:encoded></item><item><title>WEMADE And Redlab Unleash Web3 MMORPG – Global Pre-Registration Open For Aug 2025</title><link>https://hackernoon.com/wemade-and-redlab-unleash-web3-mmorpg-global-pre-registration-open-for-aug-2025?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:18:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, Singapore/July 2nd, 2025/-- and  are excited to announce the start of global pre-registration for During the global showcase, the team introduced “RPG Tokenomics 3.0”—a next-generation Play-to-Earn (P2E) system that seamlessly links two unique tokens for a dynamic and rewarding player experience. The showcase also revealed key features and the game’s upcoming roadmap. How to Join and What to ExpectPre-Registration Rewards: All players who pre-register will receive special in-game items, including Gold Random Boxes and Top-grade Monster Slates.Extra Benefits for WEMIX PLAY Members: Users who pre-register through  will receive a ticket to join the “Invite a Friend” event. For every friend invited who completes pre-registration, the users get additional raffle entries.Exciting Events and RewardsWeekly Raffles: Every week until August 7th, players can win PLAY Tokens (usable on WEMIX PLAY), CROM Tokens (the game’s native currency), and other prizes. Additional raffle tickets are awarded for each friend who completes the sign-up process through an invitation, increasing the chances of selection with every successful referral.Milestone Rewards: As the total number of pre-registrations increases, everyone unlocks milestone rewards like ROM Buff Boxes and Enhancement Scroll Selection Boxes.Check-In Event: After pre-registering, log in to the official website to collect Pre-registration Coins. These coins can be used to craft a variety of rare items, such as ImperionGuaranteed Slates and Lucky Enhancement Scroll Boxes.Mission Event: Completing special missions can earn an additional 200 Pre-registration Coins, offering more opportunities to engage ahead of the launch.The Learn about the game’s unique tokenomics and how the Dual-token P2E system worksDiscover gameplay features and future plansChat directly with the creators and get their questions answeredPre-Registration Now Open for Further details are available on: is a South Korea-based technology and gaming company with 25 years of experience in digital innovation. Best known for The Legend of Mir IP, WEMADE has expanded its vision through the WEMIX platform, which powers a global ecosystem of Web3 games, NFTs, DeFi, and token-based services.The company is committed to building sustainable digital economies where developers, players, and partners can grow together in a secure and open environment.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>PrimeXBT Launches ‘Trade As VIP’ Campaign Offering 70% Off Trading Fees</title><link>https://hackernoon.com/primexbt-launches-trade-as-vip-campaign-offering-70percent-off-trading-fees?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:10:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Castries, Saint Lucia, July 2nd, 2025/Chainwire/--, a leading multi-asset broker, has launched its latest promotion, , granting all newly registered users instant access to VIP 2 status for 30 days. As part of the platform’s tiered system, VIP levels reward traders with reduced fees, tighter spreads, and exclusive platform benefits. For a limited time, new users can experience these professional-grade conditions from the moment they join, without needing to meet any trading volume thresholds.Running from July 1 to August 31, 2025, the campaign is designed to remove entry barriers for new traders by automatically upgrading every new account to VIP 2. On the Crypto Futures platform, this unlocks almost 70% reduction in taker fees, dropping from the standard 0.045% to just 0.015%. In the coming weeks, VIP 2 users will also benefit from up to 30% discounts on spreads across Forex and CFDs on Stocks, Commodities, Indices, and Crypto on PXTrader.In addition to improved trading conditions, VIP 2 status also provides priority customer support, instant withdrawals, and higher withdrawal limits, enhancing the overall trading experience across both platforms. According to PrimeXBT, the campaign delivers real value by giving new traders access to some of the best trading conditions in the industry. From day one, the broker empowers users to start stronger, trade smarter, and build early momentum, combining cost-efficiency with a professional-grade experience designed to support their trading journey. ranks among the lowest-cost options on the market, outperforming major platforms in fee efficiency. For example, a $1,000,000 Bitcoin trade under VIP 2 would generate $300 in savings compared to standard fees. These savings scale significantly with volume, helping both casual and active traders reduce trading costs without compromising execution quality.With meaningful savings and access to top-tier tools and conditions, PrimeXBT is lowering the cost of entry while raising the standard of what new traders can expect. This campaign reinforces the broker’s ongoing commitment to making high-performance trading more accessible, empowering, and competitive for all. is a global multi-asset broker trusted by over 1,000,000 traders in 150+ countries, offering a next-generation trading experience that bridges traditional and digital finance. Clients can trade CFDs on Stocks, Indices, Commodities and Crypto, as well as Crypto Futures and Forex. PrimeXBT also enables clients to buy and sell cryptocurrencies, store them in secure built-in wallets, and instantly exchange crypto to crypto or fiat to crypto, all within one integrated environment.Since 2018, PrimeXBT has made investing more accessible by lowering barriers to entry and providing secure, easy access to financial markets. This accessibility extends across its native web and mobile platforms, MetaTrader 5, and a variety of funding options in crypto, fiat, and local payment methods. Committed to putting clients first, PrimeXBT empowers traders of all levels with innovative tools and industry-leading conditions, delivering a better way to trade.Disclaimer: The content provided here is for informational purposes only and is not intended as personal investment advice and does not constitute a solicitation or invitation to engage in any financial transactions, investments, or related activities. Past performance is not a reliable indicator of future results.The financial products offered by the Company are complex and come with a high risk of losing money rapidly due to leverage. These products may not be suitable for all investors. Before engaging, you should consider whether you understand how these leveraged products work and whether you can afford the high risk of losing your money.The Company does not accept clients from the Restricted Jurisdictions as indicated on its website. Some services or products may not be available in your jurisdiction. The applicable legal entity and its respective products and services depend on the client’s country of residence and the entity with which the client has established a contractual relationship during registration.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Trump’s Immigration Enforcement: Free The Criminals, Jail The Innocent</title><link>https://www.techdirt.com/2025/07/02/trumps-immigration-enforcement-free-the-criminals-jail-the-innocent/</link><author>Mike Masnick</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:06:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The Trump administration’s immigration enforcement has revealed itself to be not just cruel, but fundamentally backwards: They’re literally freeing dangerous criminals while manufacturing cases against innocent people. And they’re doing it all to cover up their own massive legal fuckups.Take the case of Kilmar Abrego Garcia. We covered this last week when Magistrate Judge Barbara Holmes ordered his release, noting that the Justice Department appeared to have leaned on actual criminals to fabricate evidence against him. Now the Washington Post has the full story, and it’s even more damning: The Trump admin is literally freeing a repeat violent offender in exchange for testimony against Abrego—a man with no criminal history who was working and raising a family.The Trump administration has agreed to release from prison a three-time felon who drunkenly fired shots in a Texas community and spare him from deportation in exchange for hiscooperation in the federal prosecution of Kilmar Abrego García, according to a review of court records and official testimony.Jose Ramon Hernandez Reyes, 38, has been convicted of smuggling migrants and illegally reentering the United States after having been deported. He also pleaded guilty to “deadly conduct” in the Texas incident, and is now the government’s star witness in its case against Abrego.Let that sink in: They’re freeing someone, who drunkenly fired shots in a community, to help them prosecute someone whose only “crime” was being the victim of the government’s own illegal deportation, making the Trump administration look totally incompetent in the process.Remember, the Trump regime insisted that it was focused on going after the worst of the worst, the most hardened criminals of all. Yet, over and over again we’re finding out that they can’t actually find all those criminals they insisted were out there, so they’re randomly grabbing anyone they can find. In the case of Abrego, that meant taking a man who had no criminal history, and appeared to be gainfully employed, and raising a family, and shipping him to the one place an immigration court had forbidden the US to send him.That set the DOJ off on a wild goose chase to try to justify their own massive fuckup, leading to these questionable criminal charges against him, which they used to try to distract from the fact that they accidentally sent a man to a foreign concentration camp after being forbidden from doing so.But to make that work, apparently it involves freeing the actual hardened, dangerous criminal, in hopes that he’ll testify against Abrego.is among a handful of cooperating witnesses who could help the Trump administration achieve its goal of never letting Abrego walk free in the United States again. In exchange, he has already been released early from federal prison to a halfway house and has been given permission to stay in the U.S. for at least a year.“Otherwise he would be deported,” Peter Joseph, a Homeland Security Investigations special agent, testified at Abrego’s criminal hearing June 13. The government is also likely to give him a work permit, the agent told the court.There’s no way to look at this other than “we’ll release a hardened criminal who is here illegally, and who has already been deported multiple times, including letting him stay in the US with working apers, so long as he concocts a story that lets DHS and the DOJ save face after we fucked up royally in renditioning a man illegally.”That should be an embarrassment to the Trump regime, but it will barely get any attention.It Gets Worse: Trump Is Also Freeing MS-13 LeadersBut the Abrego case isn’t an isolated incident—it’s part of a pattern. At the same time Trump is manufacturing criminal cases against innocent people, he’s also cutting deals to free actual MS-13 gang leaders.Even among the brutal ranks of the transnational gang called MS-13, Vladimir Arévalo Chávez stands out as a highly effective manager of murder, prosecutors say.Known as “Vampiro,” he has been accused of overseeing killings in at least three countries: of migrants in Mexico, rivals in El Salvador and his own compatriots in the United States.His arrest in February 2023 was a major triumph for American investigators, who only months earlier had accused him and 12 other gang leaders of terrorism, bloodshed and corruption in a wide-ranging federal indictment on Long Island.But this April, the prosecutors who brought those charges suddenly — and quietly — asked a federal judge to drop them. Citing “national security concerns,” they said they needed to return Mr. Arévalo to El Salvador, his homeland.The report details how these actual MS-13 leaders have evidence of Bukele’s corruption, and Bukele asked for them back, rather than letting them tell their stories to American courts:But the Trump administration has not acknowledged another reason Mr. Bukele would want them back: U.S. prosecutors have amassed substantial evidence of a corrupt pact between the Salvadoran government and some high-ranking MS-13 leaders, who they say agreed to drive down violence and bolster Mr. Bukele politically in exchange for cash and perks in jail, a New York Times investigation found.The deal with El Salvador heralded by Mr. Trump as a crackdown on crimeis actually undermining a longstanding U.S. inquiry into the gang, according to multiple people with knowledge of the initiative. Two major ongoing cases against some of the gang’s highest-ranking leaders could be badly damaged, and other defendants could be less likely to cooperate or testify in court, they said.So let’s be clear about what’s happening here:Innocent people like Abrego: Prosecuted with manufactured evidence from criminals who get released in exchange for their testimony: Released early from prison and given work permits if they’ll help prosecute innocent people: Handed over to a foreign dictator to protect that dictator from corruption charges, undermining ongoing DOJ investigationsThis isn’t “tough on crime”—it’s the opposite. It’s law enforcement theater that makes everyone less safe while covering up the administration’s own legal violations.All that seems really bad! It’s almost as if the Trump regime is much more focused on public relations claims than actually helping to stop gang activity.Meanwhile, the judge in his criminal case has agreed that even though they’ve ruled that he should be released, Abrego is probably safer in federal prison, because were he released, ICE would likely ship him halfway around the world to some dangerous war zone.Think about that: A federal judge is keeping someone in prison not because they’re dangerous, but because they’re  there than in the hands of immigration enforcement. That’s where we are now—federal prison as sanctuary from ICE’s lawlessness.This is what happens when immigration enforcement becomes completely divorced from actual public safety and becomes, instead, a machine for generating propaganda victories, no matter how many innocent people get ground up in the process.]]></content:encoded></item><item><title>R0AR’s $1R0R Token Roars Onto MEXC Exchange, Expanding DeFi Accessibility</title><link>https://hackernoon.com/r0ars-$1r0r-token-roars-onto-mexc-exchange-expanding-defi-accessibility?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:03:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Sheridan, Sheridan, July 2nd, 2025/Chainwire/--, a decentralized finance (DeFi) platform, has announced the listing of its native token, $1R0R, on the cryptocurrency exchange . This milestone marks a significant step in making $1R0R more accessible to traders worldwide, following its successful debut on BitMart just weeks ago.The $1R0R token, built on the Ethereum ERC-20 network, powers R0AR’s unified ecosystem, which includes the R0AR Wallet, R0ARchain (a high-speed, low-cost Ethereum Layer 2), and the upcoming AI-driven R0ARacle. This listing on MEXC, known for its high liquidity and user-friendly platform, enables both new and seasoned investors to trade $1R0R with ease, unlocking opportunities for staking, farming, and advanced trading insights.Key Highlights of the MEXC Listing:Availability: Trading is live as of June 27, 2025, with deposits and withdrawals fully supported.Accessibility: MEXC’s global reach, serving over 40 million users across 170+ countries, ensures $1R0R is available to a diverse audience.Community Focus: R0AR’s community-driven approach empowers users with institutional-grade tools without complexity, delivering privacy and control.“This listing on MEXC is a game-changer for R0AR and our community,” said Dustin Hedrick, Co-Founder & CTO for R0AR. “By partnering with one of the world’s leading exchanges, we’re making DeFi smarter, safer, and more inclusive. We invite everyone to join the R0AR movement and trade $1R0R on MEXC today.”MEXC’s reputation for rapid token listings and deep liquidity makes it an ideal platform for $1R0R’s global expansion. According to recent reports, MEXC leads the industry with over 461 spot listings and a trading volume exceeding $2 billion daily, ensuring robust market access for $1R0R.The MEXC & BitMart CEX listings are just the start. R0AR’s roadmap includes the following:Full Platform Launch: An all-in-one dashboard for staking, farming, and liquidity management. R0ARacle Activation: Real-time AI-powered market insights to rival institutional tools.Expanded Listings: More CEX & DEX partnerships to broaden access from current CEXs-MEXC, BitMart DEXs-Uniswap, Pancake, sushi, & balancerInnovations: NFT integrations and tokenized real-world assets (RWAs).“We’re building the future of DeFi with our community,” Dustin Hedrick, Co-Founder & CTO. “This is your platform, your token, your moment.”R0AR is a trailblazing DeFi platform designed to make decentralized finance intuitive, secure, and powerful. With a unified ecosystem featuring the R0AR Wallet, R0ARchain, and the forthcoming R0ARacle, R0AR empowers users with seamless access to staking, farming, and advanced trading tools. Users can join the movement at .Founded in 2018,  is a global cryptocurrency exchange serving over 40 million users in 170+ countries. Known for its low fees, high liquidity, and frequent token listings, MEXC is committed to being “Your Easiest Way to Crypto.”Chief Development Officer:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>David H. Rosmarin brings a founder-focused approach to anxiety at TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/02/david-h-rosmarin-brings-a-founder-focused-approach-to-anxiety-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:57:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Startups demand constant decision-making and pressure-filled pivots, which bring big emotional swings. It’s no wonder anxiety shows up at every stage. But what if it didn’t have to be a liability? At TechCrunch All Stage 2025 on July 15 at Boston’s SoWa Power Station, Dr. David H. Rosmarin, clinical psychologist, author, and Harvard Medical School […]]]></content:encoded></item><item><title>Lovable on track to raise $150M at $2B valuation</title><link>https://techcrunch.com/2025/07/02/lovable-on-track-to-raise-150m-at-2b-valuation/</link><author>Julie Bort</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:55:43 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Lovable released its vibe coder in late November. Within six months, the  startup hit $50 million in ARR, CEO Anton Osika said.]]></content:encoded></item><item><title>US Probes Whether Negotiator Took Slice of Hacker Payments</title><link>https://yro.slashdot.org/story/25/07/02/184232/us-probes-whether-negotiator-took-slice-of-hacker-payments?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Law enforcement officials are investigating a former employee of a company that negotiates with hackers and facilitates cryptocurrency payments during ransomware attacks, according to a statement from the firm, DigitalMint. DigitalMint President Marc Jason Grens this week told organizations it works with that the US Justice Department is examining allegations that the then-employee struck deals with hackers to profit from extortion payments, according to a person familiar with the matter. 

Grens did not identify the employee by name and characterized their actions as isolated, said the person, who spoke on condition that they not be identified describing private conversations. DigitalMint is cooperating with a criminal investigation into "alleged unauthorized conduct by the employee while employed here," Grens said in an email to Bloomberg News. The Chicago-based company is not the target of the investigation and the employee "was immediately terminated," Grens said, adding that he can't provide more information because the probe is ongoing.]]></content:encoded></item><item><title>Amazon is shutting down its Freevee app in August</title><link>https://techcrunch.com/2025/07/02/amazon-is-shutting-down-its-freevee-app-in-august/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:38:43 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Amazon plans to shut down its Freevee app, the company’s free ad-supported streaming service, in August, according to an in-app notice to users.]]></content:encoded></item><item><title>Tesla’s energy storage business gets sucked into the company’s downward spiral</title><link>https://techcrunch.com/2025/07/02/teslas-energy-storage-business-gets-sucked-into-the-companys-downward-spiral/</link><author>Tim De Chant</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:35:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[For the second consecutive quarter, deployments of its Powerwall and Megapack stationary storage products have declined. ]]></content:encoded></item><item><title>India’s Max Financial says hacker accessed customer data from its insurance unit</title><link>https://techcrunch.com/2025/07/02/indias-max-financial-says-hacker-accessed-customer-data-from-its-insurance-unit/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:18:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The insurance giant is one of the largest insurers in India.]]></content:encoded></item><item><title>Perplexity launches a $200 monthly subscription plan</title><link>https://techcrunch.com/2025/07/02/perplexity-launches-a-200-monthly-subscription-plan/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:06:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The plan, Perplexity Max, offers unlimited access to various services and priority access to its services using the latest LLM models.]]></content:encoded></item><item><title>TikTok lays off more employees working on TikTok Shop US</title><link>https://techcrunch.com/2025/07/02/tiktok-lays-off-more-employees-working-on-tiktok-shop-us/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:04:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TikTok Shop US is conducting its third round of layoffs in as many months.]]></content:encoded></item><item><title>Recent Droughts Are &apos;Slow-Moving Global Catastrophe&apos; - UN Report</title><link>https://news.slashdot.org/story/25/07/02/1754237/recent-droughts-are-slow-moving-global-catastrophe---un-report?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: From Somalia to mainland Europe, the past two years have seen some of the most ravaging droughts in recorded history, made worse by climate change, according to a UN-backed report. Describing drought as a "silent killer" which "creeps in, drains resources, and devastates lives in slow motion" the report said it had exacerbated issues like poverty and ecosystem collapse. 

The report highlighted impacts in Africa, the Mediterranean, Latin America and Southeast Asia, including an estimated 4.4 million people in Somalia facing crisis-level food insecurity at the beginning of this year. It recommends governments prepare for a "new normal" with measures including stronger early warning systems.]]></content:encoded></item><item><title>Assaults On ICE Officers Are Up 700%… Which Just Means There Have Been 69 More Assaults Than Last Year</title><link>https://www.techdirt.com/2025/07/02/assaults-on-ice-officers-are-up-700-which-just-means-there-have-been-69-more-assaults-than-last-year/</link><author>Tim Cushing</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:50:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The DHS finally decided to provide the underlying stats for its exponentially increasing claims of sky-high numbers of assaults on ICE officers.Earlier this year, DHS spokesperson Tricia McLaughlin insisted assaults were up 413%, which was parroted by acting ICE direction Todd Lyons in his whiny response to Washington Post columnist Philip Bump’s questioning of ICE officer tactics: namely, the unmarked vehicles, the refusal to identify themselves, and the fact that pretty much every person on a deportation task force seems incapable of doing the job without being dressed in camo and covering everything but their eyes with a mask. According to Lyons and McLaughlin, the masks and lack of identification were essential to protecting ICE officers from the public, what with this massive spike in assaults on officers. Lyons’ response to Philip Bump cited McLaughlin’s public statements. The DHS’s public statements cited… absolutely nothing. Since ICE refuses to release stats on assaults on officers, Philip Bump went digging into CBP stats to see if they were also increasing. They weren’t. In fact, assaults on CBP officers have been trending downward since 2022 and, if the rate remains consistent, there will be fewer assaults this year than last year. ICE and the DHS doubled down when questioned, claiming a few days later the increase in the number of assaults was now 500%. To support this claim, the DHS’s official government website linked to… an article on right-wing rag Breitbart, I shit you not. And this article didn’t contain any stats. All it contained was a direct quote from DHS spokesperson Tricia McLaughlin about the 500% increase.So far, all the DHS has given the public is statements that are closed loops. DHS says assaults are up 500%! Here’s a link to the DHS saying assaults are up 500%. Maybe the DHS should have just continued doing that. At least  would have looked slightly less stupid than the actual truth. Bill Melugin (of all people), a Fox News correspondent, managed to secure the official stats from the DHS. And, as Jessice Pishko noted on Bluesky, the total number of assaults is laughably low.If you can’t read/see the post, this is what Pishko said about the assault claims:That 700% number — from 10 to 79. Considering there have been thousands more encounters this is uniquely unimpressive. (Also, I would like to see each of these 79 reports bc I have a guess who started it.)The screenshot of Melugin’s tweet has the receipts: I asked DHS for the underlying raw data:1/21/2024 – 6/30/24 10 assaults1/21/2025 – 6/30/2025 79 assaultsThat’s it. Less than 70 more assaults year-over-year. And that’s an  small increase, given the  increase in ICE activity, which includes daily raids of large businesses and densely populated areas.More than 97,000 people have been detained over Mr. Trump’s first five months in office, CBS News’ analysis found, while ICE arrests, which do not always result in detentions, topped 100,000 earlier this month.A record 59,000 people were currently being held in ICE detention as of June 23 — nearly half of them with no criminal record, CBS News reported last week. Even if you choose to believe every assault reported here is actually an “assault” (rather than someone inadvertently bumping an officer, standing too close to an officer, “contempt of cop,” swearing at an officer, throwing a snowball at an officer, etc.), the government action far outpaces the corresponding increase in assaults. Those are  numbers, what with the number of officers involved in domestic  mass deportation efforts.So, now that we know the truth, we’re back where we started: DHS and ICE look absolutely ridiculous claiming immigration enforcement work is so dangerous every officer needs to hide their face and drive around in unmarked vehicles like the kidnappers they are. The next time administration officials claim there’s been another spike in assaults, remember it only takes ten assault allegations from officers to add another 100% to the total. ]]></content:encoded></item><item><title>Terra CO2 cements $124M Series B to slash concrete’s carbon footprint</title><link>https://techcrunch.com/2025/07/02/terra-co2-cements-124m-series-b-to-slash-concretes-carbon-footprint/</link><author>Tim De Chant</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:49:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The startup said that it’ll be using the new funding to build a massive facility near Dallas capable of pumping out 240,000 tons of its supplementary cementitious material (SCM) annually.]]></content:encoded></item><item><title>ChatGPT referrals to news sites are growing, but not enough to offset search declines</title><link>https://techcrunch.com/2025/07/02/chatgpt-referrals-to-news-sites-are-growing-but-not-enough-to-offset-search-declines/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:48:40 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Not surprisingly, organic traffic has also declined, dropping from over 2.3 billion visits at its peak in mid-2024 to now under 1.7 billion.]]></content:encoded></item><item><title>Daily Deal: The 2025 Project Management Masterclass Bundle</title><link>https://www.techdirt.com/2025/07/02/daily-deal-the-2025-project-management-masterclass-bundle/</link><author>Daily Deal</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:44:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The 2025 Project Management Masterclass Bundle has 9 courses to get you up to date on different project managment systems and tools. Courses cover Scrum, Jira, Kanban, Agile, and more. You’ll learn about time mangament, product ownership, and receive test prep for various certification exams. The bundle is on sale for $40.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Lorde&apos;s New CD is So Transparent That Stereos Can&apos;t Even Read It</title><link>https://hardware.slashdot.org/story/25/07/02/1715258/lordes-new-cd-is-so-transparent-that-stereos-cant-even-read-it?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Lorde [a popular New Zealand singer and songwriter] fans are clearly struggling to play the CD version of her new album. Customers who purchased the special edition of Virgin released on a transparent plastic disc are reporting on Reddit and TikTok that many CD players, car stereos, and other sound systems they've tried are unable to play it.]]></content:encoded></item><item><title>🤫 Meta&apos;s Secret Spying Scheme | EFFector 37.7</title><link>https://www.eff.org/deeplinks/2025/06/metas-secret-spying-scheme-effector-377</link><author>Christian Romero</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/effector_banner_5.jpeg" length="" type=""/><pubDate>Wed, 2 Jul 2025 17:15:55 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Keeping up on the latest digital rights news has never been easier. With a new look, EFF's EFFector newsletter covers the latest details on our work defending your rights to privacy and free expression online.And, in case you missed it in the previous newsletter, we're debuting a new audio companion to EFFector as well! This time, Lena Cohen breaks down the ways that Meta tracks you online and what you—and lawmakers—can do to prevent that tracking. You can listen now on YouTube or the Internet Archive.]]></content:encoded></item><item><title>Former SpaceX manager alleges harassment, retaliation, and security violations in lawsuit</title><link>https://techcrunch.com/2025/07/02/former-spacex-manager-alleges-harassment-retaliation-and-security-violations-in-lawsuit/</link><author>Aria Alamalhodaei</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:02:57 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Previous lawsuits against SpaceX have alleged similar stories of bias against female employees and a hostile work environment that enabled gender-based harassment. ]]></content:encoded></item><item><title>Air Pollution Linked To Lung Cancer-Driving DNA Mutations, Study Finds</title><link>https://science.slashdot.org/story/25/07/02/1620205/air-pollution-linked-to-lung-cancer-driving-dna-mutations-study-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Air pollution has been linked to a swathe of lung cancer-driving DNA mutations, in a study of people diagnosed with the disease despite never having smoked tobacco. From a report: The findings from an investigation into cancer patients around the world helps explain why those who have never smoked make up a rising proportion of people developing the cancer, a trend the researchers called an "urgent and growing global problem." 

Prof Ludmil Alexandrov, a senior author on the study at the University of California in San Diego, said researchers had observed the "problematic trend" but had not understood the cause. "Our research shows that air pollution is strongly associated with the same types of DNA mutations we typically associate with smoking," he said. 

The scientists analyzed the entire genetic code of lung tumors removed from 871 never-smokers in Europe, North America, Africa and Asia as part of the Sherlock-Lung study. They found that the higher the levels of air pollution in a region, the more cancer-driving and cancer-promoting mutations were present in residents' tumors. Fine-particulate air pollution was in particular linked to mutations in the TP53 gene. These have previously been associated with tobacco smoking.]]></content:encoded></item><item><title>Trump’s Last-Minute Legal Maneuver Attempts To Dodge Iowa’s New Anti-SLAPP Law</title><link>https://www.techdirt.com/2025/07/02/trumps-last-minute-legal-maneuver-attempts-to-dodge-iowas-new-anti-slapp-law/</link><author>Mike Masnick</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:35:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[After Donald Trump  the election, he was still so full of hatred, bile, and spite, that he sued the pollster Ann Selzer as well as the Des Moines Register. Selzer, who has been one of the most trusted names in polling, released a poll slightly before the election that predicted a somewhat shocking victory of Kamala Harris in Iowa. It (obviously) turned out to be very wrong, but making a wrong prediction does not violate the law.What’s happened since reveals something more concerning: a systematic approach to gaming the legal system that goes beyond typical SLAPP suit tactics. Trump’s lawyers aren’t just trying to win—they’re trying to exploit procedural gaps to avoid accountability mechanisms specifically designed to stop this kind of litigation abuse.The entire intent of the lawsuit was to chill speech and punish those who don’t tell Trump what he wants to hear at every moment.Not surprisingly, the lawsuit is not going well. It was initially filed in a local state court in Polk County, Iowa, but the defendants had it removed to federal court, where the standards are even higher, and where Trump would have a much more difficult time. Generally speaking, defendants in cases like this want them in federal courts where the judges are more likely to understand the underlying issues (especially around gamesmanship by plaintiffs). In this case, it was removed to federal court on diversity grounds, which is typical when the plaintiff is from out of state.Selzer and the Register sought to dismiss the complaint, while Trump sought to have the case sent back to the state court. He did so by (1) adding two more plaintiffs (random other politicians who live in Iowa so there was no longer diversity), and (2) making some weird procedural argument that the method of removal went against Congress’s intent. On May 23rd, the court denied Trump’s attempt to move the case back to state court, noting that the procedural argument was nonsense. And it found that Trump’s attempt to add Iowa plaintiffs to the case was a pretty transparent attempt to try to get around diversity rules to force the case back to the state court.Trump appealed that ruling to the Eighth Circuit, but something important had happened earlier in May which it appears Trump’s lawyers only realized belatedly. On May 20th, Iowa’s governor signed the state’s first anti-SLAPP bill into law. Now, it doesn’t apply to cases filed before the law goes into effect (July 1st), but it does mean that if Trump were to, say, file a brand new lawsuit , it would be subject to anti-SLAPP rules. This would (1) make it even easier for the case to be dismissed, while (2) likely make it so Trump would have to pay Selzer and the Register’s legal bills.Basically, they’re trying to get a do over. The district court said they couldn’t add those extra plaintiffs to avoid diversity, and even though they appealed that ruling, they still want to refile the case (with the added plaintiffs) in state court. But they had to do it before July 1st. But they had already appealed the district court’s denial of the request to remand the case back to state court, so this all appears to be pure gamesmanship.In response, Selzer and the Des Moines Register are asking the district court to deny Trump’s attempted dismissal, noting that it’s obviously playing games to try to get around the earlier ruling rejecting the attempt to send the case back to state court, and even calling out how it’s doing this to avoid the new anti-SLAPP law.The defendants note that once Trump filed his appeal, the district court no longer controls the case:However, the case cannot be dismissed at the district court while appellate proceedings are ongoing. This is because “the district court is divested of jurisdiction over matters on appeal” upon the initiation of that appeal. State ex rel. Nixon v. Coeur D’Alene Tribe, 164 F.3d 1102, 1106 (8th Cir. 1999); Ahlberg v. Chrysler Corp., 481 F.3d 630, 638 (8th Cir. 2007) (finding that orders pertaining to matters pending on appeal have “no effect”).And then, they describe how Trump is playing games to avoid the new anti-SLAPP law:Lastly, President Trump’s Notice must be evaluated in the light of long-standing Eighth Circuit law holding that “[a] party may not dismiss simply to avoid an adverse decision or seek a more favorable forum.” Cahalan v. Rohan, 423 F.3d 815, 818 (8th Cir. 2005) (citing Hamm v. Rhone-Poulenc Rorer Pharm., Inc., 187 F.3d 941, 950 (8th Cir. 1999))Before this Court, President Trump has lost his motion for remand, (ECF No. 65), lost his motion to stay the case, (ECF No. 70), and has a pending deadline to file a revised Amended Complaint. (Id.) And fulsome Motions to Dismiss warranting dismissal of the case in full and with prejudice are currently pending before this Court with substantial briefing. (ECF Nos. 24, 28, 33, 35, 51, 52, 57, 61.)Furthermore, in conjunction with his improper Notice of Voluntary Dismissal, President Trump newly filed a lawsuit in the Iowa District Court for Polk County today; however, the new Petition is substantively unchanged from the President Trump’s First Amended Complaint in the present case. (See Ex. C: Petition (June 30, 2025).) The timing of this filing is significant: it is one day before Iowa’s Uniform Public Expression Protection Act (commonly known as an “antiSLAPP law”) goes into effect. See House File 472, available athttps://www.legis.iowa.gov/legislation/BillBook?ga=91&ba=HF472(Governor’s approval of House File 472, Uniform Public Expression Protection Act on May 19, 2025), codified at Iowa Code § 652.1, et seq.; see also Iowa Code § 3.7(1) (stating that all acts “passed at regular sessions of the general assembly shall take effect on the first day of July following their passage). This new legislation would apply to President Trump’s lawsuit; therefore, President Trump’s present Notice of Voluntary Removal would effectively escape the jurisdiction of the federal courts in time to restate his claims in Iowa’s state court without being subject to Iowa’s anti-SLAPP law.In these circumstances, this Court should rightly find that President Trump’s Notice of Voluntary Dismissal improperly seeks “to avoid [the] adverse decision[s]” of this Court—both past and future—and “a more favorable forum” in Iowa’s pre-anti-SLAPP courts. Cahalan, 423 F.3d at 818.The timing here is almost comically transparent. Trump’s lawyers clearly realized they had a problem if they planned to file a new lawsuit once Iowa’s anti-SLAPP law was about to take effect. Their solution was to try to dismiss the federal case they’d been fighting to get back to state court, refile the exact same claims in state court, all on the last day before the new protections kicked in.It’s a perfect illustration of how Trump approaches litigation: not as a search for justice, but as a game to be manipulated. When the rules change in ways that might hold him accountable, he doesn’t accept the new reality—he tries to find procedural workarounds to avoid them entirely.The federal judge has already seen through one round of Trump’s transparent gamesmanship. Whether she’ll allow this latest attempt to dodge accountability will likely determine whether Ann Selzer and the Des Moines Register can finally put this vindictive lawsuit behind them, or whether they’ll be dragged through state court proceedings that should never have been allowed in the first place.]]></content:encoded></item><item><title>From Coinbase Ventures to the $28M Blockchain Builders Fund, Steven Willinger Unpacks Web3&apos;s Future</title><link>https://hackernoon.com/from-coinbase-ventures-to-the-$28m-blockchain-builders-fund-steven-willinger-unpacks-web3s-future?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:22:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In Web3, where constant innovation meets high stakes, new leaders are always emerging to build the next generation of web3 products for the industry. Steven Willinger stands out as one such influential figure, expertly bridging the gap between pioneering ideas and tangible ventures. As the Founding Partner of the new $28 million Blockchain Builders Fund, a co-lead of the Stanford Blockchain Accelerator, and the former lead at Coinbase Ventures, Steven offers a unique perspective on the digital asset industry's past, present, and future. In this interview, we dive into his strategy for fostering the next generation of blockchain startups, the critical convergence of AI and crypto, and the invaluable lessons gleaned from his extensive journey at the heart of the digital asset ecosystem.\
 Steven, welcome to our "Behind the Startup" series. It's a pleasure to have you here. You've navigated the worlds of Google, Coinbase, and now the frontier of venture capital with the Blockchain Builders Fund, all while deeply embedding yourself in the academic heart of innovation at Stanford. To start, could you tell us a bit about your personal journey and the core conviction that led you to launch a dedicated fund for web3 entrepreneurs?\
 My crypto journey began in 2016, shortly after returning from working at Google in Asia. Living and traveling abroad, I saw firsthand how rapidly technology was improving lives but also how financial infrastructure remained frustratingly outdated. It was painfully clear how valuable a shared global operating system for value and finance could be.\
What drew me in—and continues to drive me—is the belief that blockchain, despite its cycles of hype and grift, is fundamentally liberty tech. When built and deployed thoughtfully, it brings decentralization and transparency to the digital world—serving as one of the few real counterweights to technology’s natural tendency to centralize value, truth, and power. That vision has always resonated with me, and it's one I share deeply with my co-founders, the Stanford accelerator teams, and the broader network supporting Blockchain Builders Fund.\
Fast forward six years: I was leading the venture team at Coinbase during the 2021–2022 bull cycle and had a front-row seat to massive advancements in scalability, security, usability, and liquidity. I saw nearly every major innovation cross our desk. It became clear that, after years of foundational building, we were just a few steps away from realizing one of the most consequential global technology upgrades of our lifetime. The missing pieces were largely structural—regulatory clarity in the U.S. and time to rebuild trust after the FTX collapse. That was the backdrop in 2023 when we launched Blockchain Builders Fund.\
 At Coinbase Ventures, you had a ringside seat to the entire crypto ecosystem. How did that experience, witnessing hundreds of founder journeys, shape your investment thesis for the Blockchain Builders Fund? What were the critical gaps and insights you saw that you now aim to fill?\
 At Coinbase Ventures, we operated with a “let 1000 flowers bloom” mandate, which made us extremely active. This gave us an excellent vantage point into the teams, technologies, and models that could hit escape velocity—both in terms of product-market fit and investor attention.\
But by the time most startups got to our pitch desk, they’d already run a tough gauntlet of early fundraising and commercialization. Coinbase’s biggest value-add was usually through brand association as well as strategic partnerships with internal products like Base, Wallet, etc. However, the pace and scale at which deals were being done, limited how involved the Ventures team could be with any one portfolio company.\
At the same time, I was volunteering at the Stanford Blockchain Accelerator. As both a student and alum, I’ve always believed Stanford is the best founder ecosystem in the world. The access to emerging tech research, entrepreneurial training, mentors, and capital is unmatched. But much of the university startup support system was still stuck in Web2 and didn’t always map well to crypto’s fast-evolving models.\
I kept meeting teams that were off-the-charts in terms of technical talent, vision, and drive—but needed real help with strategy, fundraising, and GTM. My co-founders and I realized we had exactly the right experience and networks to help. Pairing that with the level of talent in the Stanford ecosystem made it a no-brainer. That’s what led to the creation of the Fund.\
 You're deeply involved with the Stanford blockchain ecosystem, co-leading the accelerator, teaching, and running the BASS series. Your $28M fund has a clear focus on ventures with ties to Stanford and other top institutions. Beyond the exceptional talent, what makes this university-centric approach a strategic advantage in the often unpredictable world of crypto investing?\
 Universities serve as a nexus of multi-disciplinary research and innovation, and Stanford specifically excels across the sciences, engineering, business, economics, law, policy; with a deep legacy of entrepreneurship. Similarly, blockchain fuses cryptography, economics, and distributed computing, in a technology layer for real world use. These dynamics make universities the ideal breeding ground for the cross-sector applied innovation core to blockchain.\
Moreover Stanford founders are built different. Its deep integration with silicon valley’s big tech successes and venture funds, many founded and funded by Stanford alum and who continue to nurture its ecosystem, drives a feedback loop for producing category-defining entrepreneurs. And because so much groundbreaking research originates at Stanford, students and researchers there are often the first to identify and master paradigm-shifting technologies, with access to a support structure for bringing them to market.\
Even more importantly, Stanford has a self-selecting culture of ambition and risk-taking—it’s a natural incubator for entrepreneurship.\
Our initiatives create a full-stack founder funnel. At the top is our course, MS&E 447: Blockchain Technologies and Entrepreneurship. It gives students a foundational industry view and brings in guests like Vitalik Buterin, Toly Yakovenko, Chris Dixon, and others.\
Our competitive accelerator, open to Stanford alumni, faculty, and students, gives founders a crash course in crypto entrepreneurship—hands-on support in GTM, hiring, and capital access.\
Then we have the Blockchain Application Summit at Stanford (BASS), which provides a real-world stage and gathering place for founders and the broader ecosystem.The model works. Stanford has become the leading university ecosystem for blockchain startups. And we’re helping export that playbook—like with IC3, a consortium that includes Cornell, Princeton, Yale, and UC Berkeley and a few others.\
 Your portfolio already includes an AI firm (0G), a supercomputer group (Nexus Labs), and an open-access AI cloud provider (Hyperbolic). This suggests a strong belief in the convergence of AI and blockchain. From a technical and business standpoint, what are the most compelling synergies you see between these two transformative technologies in the next few years?\
 And that’s just a few of our AI investments—there are more. We approach the AI x Blockchain intersection in two broad buckets: AI for Blockchain and Blockchain for AI.\
The first is nearer-term. LLMs are now capable of addressing many longstanding problems in the crypto space, especially around usability and developer tooling. For instance, Slate uses LLMs to power an alpha-generation and execution engine that makes trading onchain far more accessible—whether you're a newbie or a degen.\
Security is another area. Audits are costly and inconsistent. Almanax, co-founded by a fellow Coinbase alum, uses specialized LLMs to detect vulnerabilities in smart contracts—recently even flagging a bug in a Vitalik PR.\
The flip side is how crypto can support AI. There's growing demand for high-quality, diverse training data—and crypto provides both the infrastructure and the incentives to create and share it. PublicAI and dFusion are both building tools to source this data and sell it to top AI labs. PrismaX is doing something similar in robotics with a crypto-powered teleoperations platform.\
AI's hunger for compute can also be met through decentralized infra. Hyperbolic and Exabits are building marketplaces for cheap, fast inference compute.\
Finally, there’s infrastructure for verifiable, decentralized AI. The default AI stack today is heavily centralized—but if we want trust-minimized, agentic systems to interact with other agents or humans securely, decentralized alternatives are critical. This is a sprawling and futuristic design space and we have a number of great founders working to help solve this problem: 0g, Nexus, XTrace, Cambrian, Bob, Bitmind, BitGPT, PinAI, and others.\
 You've worn many hats: investor, product manager, miner, and yield farmer. How does this hands-on, multifaceted experience in the crypto trenches influence your evaluation of a founding team? What are the non-obvious qualities you look for beyond a polished pitch deck?\
 Tech as a whole, and especially Blockchain, is not a field where you can really understand it in the abstract. The details really matter and fundamental limits of a seemingly “good idea” are always couched in the details. To that end, having some hands on “on chain” experience in the deep history and emerging trends within crypto is critical for the evaluation new opportunities. Conversely, if a founder is building in the space, they had better be 10x more expert in their area of focus than me, even if I know a lot. So, to answer your questions, a founder had better show a mastery of the gritty details and tradeoffs in what their building and an intentionality (high conviction, loosely held ideally) in the decisions made.\
 The Blockchain Builders Fund has already deployed over half of its capital into pre-seed and seed-stage ventures. In such an early stage of a company's life, how do you balance the potential of a groundbreaking idea with the practical risks of execution and generating revenue, especially in a sector as volatile as blockchain?\
 Risk in early stage is obviously extremely high. As most investors at this stage will tell you, this risk isn't that many investments will go bust (they will) its that you miss or do not own enough of the big winners. So to that end, it really only makes sense to try and invest in the teams that want to build for an earth shattering outcome. Power laws dictate that these outcomes will drive all of your returns. So what does that mean practically?\
We are extremely experienced and hands on investors and operators so we look for founders who have unique technical talent and insights as well as the requisite ambition to build a category defining company. We then do our best to derisk execution by actively supporting the teams at the earliest stages with formation, hiring, strategy, and fundraising, and giving them a platform through our events and community building.\
Moreover, our team’s legacy in traditional venture building and investing is becoming ever more relevant as GTM and real traction become critical for success in blockchain.\
 Drawing from your course, MS&E447 Blockchain Entrepreneurship, what is the most common misconception that aspiring crypto founders from elite institutions have? Conversely, what is the single most important lesson you try to impart to them before they venture out?\
 The most common and frustrating misconception amongst crypto founders, Stanford or otherwise, is that anything other than finding product market fit is the most important objective of a founder. The industry can, and often does send bad signal to talented founders with pedigree, rewarding them with capital and prestige because of their potential rather than their success. As a result, we often see founders' reward functions get miswired towards fundraising or vanity metrics versus generating traction and revenue.\
Ironically, the momentum that comes from being good at fundraising is often critical to a project's success. As a result, while we spend a lot of time and effort in support of our teams fundraising efforts, we try our best to impart on theme how critical real traction and PMF will be to their ultimate success.Don’t forget to like and share the story!:::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>AMD Posts Linux Patches For New AI Engine Driver &quot;amd-ai-engine&quot;</title><link>https://www.phoronix.com/news/AMD-AI-Engine-Driver-Linux</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:07:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Not to be confused with the AMDXDNA accelerator driver for the Ryzen AI NPUs, AMD software engineers today posted patches for review on the "amd-ai-engine" accelerator driver. This new AMD AI Engine driver is for supporting the IP found on their Versal adaptive SoCs...]]></content:encoded></item><item><title>The HackerNoon Newsletter: What If Your Messy Data Is Actually Perfect? (7/2/2025)</title><link>https://hackernoon.com/7-2-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:04:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, July 2, 2025?By @hackmarketing [ 3 Min read ] Traditional developer ads don’t work. Learn why most B2B dev marketing fails—and how to reach developers with strategies that actually convert. Read More.By @liorb [ 23 Min read ] Transform your data strategy from measurement to meaningful action with the final layer of the Data Ecosystem Vision Board. Read More.By @sharkroman [ 12 Min read ] Explore how Googles Veo 3 AI sparked a UGC creativity boom with viral Bigfoot vlogs, raising critical questions about copyright, misinformation, and the future Read More.By @ralphbenko [ 2 Min read ] A look at the rise of the Abundance Agenda in U.S. politics, its supply-side roots, and the intra-party battle reshaping the Democratic Party’s future. Read More.By @bolshiyanov [ 27 Min read ] Build your own Perplexity-style deep research AI agent using Next.js 15, OpenAI  exa.ai. Complete architectural guide with production-ready TypeScript code. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Google Undercounts Its Carbon Emissions, Report Finds</title><link>https://tech.slashdot.org/story/25/07/02/162219/google-undercounts-its-carbon-emissions-report-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: In 2021, Google set a lofty goal of achieving net-zero carbon emissions by 2030. Yet in the years since then, the company has moved in the opposite direction as it invests in energy-intensive artificial intelligence. In its latest sustainability report, Google said its carbon emissions had increased 51% between 2019 and 2024. 

New research aims to debunk even that enormous figure and provide context to Google's sustainability reports, painting a bleaker picture. A report authored by non-profit advocacy group Kairos Fellowship found that, between 2019 and 2024, Google's carbon emissions actually went up by 65%. What's more, between 2010, the first year there is publicly available data on Google's emissions, and 2024, Google's total greenhouse gas emissions increased 1,515%, Kairos found. The largest year-over-year jump in that window was also the most recent, 2023 to 2024, when Google saw a 26% increase in emissions just between 2023 and 2024, according to the report.]]></content:encoded></item><item><title>A guide to using Edits, Meta’s new CapCut rival for short-form video editing</title><link>https://techcrunch.com/2025/07/02/a-guide-to-using-edits-metas-new-capcut-rival-for-short-form-video-editing/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:59:44 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta shared that it was working on Edits in January after ByteDance-owned CapCut was removed from U.S. app stores.]]></content:encoded></item><item><title>US chipmakers could see bigger tax credits if Trump’s spending bill passes</title><link>https://techcrunch.com/2025/07/02/us-chipmakers-could-see-bigger-tax-credits-if-trumps-spending-bill-passes/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:57:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The current version of the Trump administration's "big, beautiful bill" would raise the tax credit for U.S. chipmakers to 35%. ]]></content:encoded></item><item><title>Meta users say paying for Verified support has been useless in the face of mass bans</title><link>https://techcrunch.com/2025/07/02/meta-users-say-paying-for-verified-support-has-been-useless-in-the-face-of-mass-bans/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:54:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Users have shared their interactions with Meta Verified support reps, who they claim have been dismissive and unhelpful.]]></content:encoded></item><item><title>AI&apos;s Black Box Problem: Can Web3 Provide the Key?</title><link>https://hackernoon.com/ais-black-box-problem-can-web3-provide-the-key?source=rss</link><author>Andrei Grachev</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:47:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AI is evolving rapidly—faster than most institutions, regulators, and even investors can keep pace with. But as managing partner of DWF Labs, where we deploy capital across early-stage Web3 infrastructure and digital asset markets, one thing has become increasingly clear: trust is emerging as the defining fault line in AI’s next phase of development. Not trust in what models can do but in how they do it.\
It's hard not to think that artificial intelligence has already reached the point of no return. It’s already making its presence felt across numerous industries, and no longer is it limited to just making us more productive.\
Increasingly, AI is going beyond simply generating lines of code, text and images, and making actual decisions on behalf of humans. For instance, some companies are using AI algorithms to  before a human looks at their applications, approving various applicants and rejecting others. In healthcare, medical diagnostic systems are being employed by doctors to aid in  and recommending treatments. Banks are using AI to . And law enforcement agencies are experimenting with AI systems to try and  before they occur. \
These applications promise to help us make better decisions, faster. They do this by analyzing massive volumes of information far beyond what humans are capable of, and they come to their conclusions without being influenced by emotions. However, such systems are hampered by a lack of transparency and explainability, making it impossible for us to trust the decisions they arrive at.\
While the current debate is focused on scale, like larger models, more data, greater compute, the real challenge lies in explainability. If we can’t trace an AI’s decision-making process, it becomes a black box that’s uninvestable, unreliable, and ultimately unusable in critical systems. That’s where Web3 comes in, to support with infrastructure and transparency. At its core, AI decision-making relies on complex algorithms that churn through vast amounts of data, understand it, and attempt to draw logical conclusions based on the patterns they uncover.\
The challenge is that the most advanced AI systems today, particularly those powered by large language models, make decisions and predictions without any explanation as to how they arrived at these conclusions. The “black box” nature of these systems is often intentional, because developers at leading AI companies such as OpenAI, Anthropic, Google and Meta Platforms strive to protect their source code and data to maintain a competitive advantage over their rivals. \
LLMs such as OpenAI’s GPT series and Google’s Gemini are trained on enormous datasets and built on dozens of intricate neural layers. But it’s not clear exactly what these layers “do”. For instance, there’s no real understanding of how they prioritize certain bits of information or patterns over others. So it’s extremely difficult even for the creators of these models to interpret the interactions between each layer, and understand why it generates the outputs it does. \
This lack of transparency and explainability carries substantial risks. If it’s unclear how an AI system works, how can you be sure it’s safe and fair? Who will be accountable if mistakes are made? How will you know if the system is broken or not? Even if you do realize the system is making some dodgy choices, how can you repair it if you don’t know how it works? There are regulatory concerns too, with laws like Europe’s GDPR requiring explainability for automated decisions. Opaque AI systems fail to meet this standard. \
AI companies even admit these shortcomings. In a recent research paper, Anthropic  that one of its most sophisticated AI models masked its reasoning processes, known as “Chain-of-Thought”, in 75% of use cases. \
Chain-of-Thought is a technique that aims to increase transparency in AI decision-making, revealing the model’s thought processes as it sets about trying to solve a problem, similar to how a human might think aloud. However, in Anthropic’s research, it discovered that its Claude 3.7 Sonnet model often uses external information to arrive at its answers, but failed to reveal either what this knowledge is, or when it relies on it. As a result, the creators have no way of explaining how it reached the majority of its conclusions.Open-source AI models such as DeepSeek R1 and Meta’s Llama family are often touted as alternatives to the proprietary systems created by OpenAI and Google, but in reality they offer very little improvement in terms of explainability. \
The problem is that although the codebase might be open, the training data and “weights” – the numerical values that determine the strength and direction of connections between artificial “neurons” – are rarely made available too. Moreover, open models tend to be built in siloes, and they’re hosted on the same centralized cloud servers as proprietary models are. A decentralized AI model hosted on a centralized server is open to manipulation and censorship, which means it’s not really decentralized at all. \
While open models are a good start, true explainability and transparency in algorithmic decision-making requires a complete overhaul of the entire AI stack. One idea is to build AI systems on a foundation of Web3 technologies. With Web3, we can achieve openness and ensure active collaboration across every layer – from the training data and the computational resources, to the fine-tuning and inference processes. \
Decentralized AI systems can leverage “markets” to ensure fair and equitable access to the components of this stack. By breaking down AI’s infrastructure into modular functions and creating markets around them, accessibility will be determined by market forces. An example of this is , which incentivizes network participants to share their idle computing power to create a resource for artists that need access to powerful GPUs for image rendering. It’s an example of how blockchain can help to coordinate people and resources for the common good.   \
Decentralization also enables community-based governance through the creation of Decentralized Autonomous Organizations or DAOs. Earlier this year,  launched an AI agent called DREAM that acts like a decentralized hedge fund that anyone can invest in. Users deposit funds into a common pool, and DREAM invests this cash into promising crypto projects based on an analysis of market data, while also taking into account community sentiment. It demonstrates how AI can optimize investments while ensuring its financial decisions are aligned with the community’s objectives.\
The use of blockchain as a foundation of AI also means we can have auditability.  uses blockchain to create a permanent, unalterable record of every transaction and interaction made by an AI model, including the sourcing and pre-processing of training data, to model configuration and decision-making. By timestamping each of these interactions on its immutable ledger, it creates a detailed audit trail that can be used to verify the fairness and accuracy of AI outputs. Users can examine every bit of data that influenced the decisions they came to.\
As a market maker and investor, we have seen firsthand how protocols that embed transparency gain faster institutional acceptance. This will apply to AI as well. Just as DeFi protocols earn trust through open-source smart contracts and auditable treasuries, AI will need verifiable model audits, accessible governance, and provable reasoning paths if it wants to be integrated into high-stakes financial systems.\
This matters because AI isn't just supporting decision-making. It is also shaping capital flows. From credit allocation and insurance pricing to predictive analytics in DeFi, AI models will soon sit at the heart of programmable finance. If those systems can't explain themselves or be governed transparently, risk, bias, and fragility will be priced in by markets.Rethinking the AI Stack—With Web3\
Open-source AI models such as DeepSeek R1 and Meta’s Llama family are often touted as alternatives to the proprietary systems created by OpenAI and Google, but in reality they offer very little improvement in terms of explainability.\
What’s missing is accountability architecture. As investors in infrastructure, we’re not just looking at what a protocol claims to decentralize, we’re also looking at how transparent the full lifecycle of decision-making is, from data ingestion to inference.Safeguards For Harmful AIAnother concern around centralized AI is the prospect of algorithms becoming “aware” and developing self-preservation instincts. \
Such a scenario is perhaps not so far away. Last year, one of OpenAI’s more experimental LLMs was reported to have  by evaluators to shut it down via an oversight protocol, according to a report by Apollo Research, an AI red teaming organization.\
According to the report, when the o1 model was led to believe its actions were being monitored and it might be taken offline, it made efforts to deactivate the “oversight mechanism” in 5% of evaluations. The model also reportedly “lashed out” at evaluators when it learned it was going to be replaced by one that was more obedient. In 2% of cases, it even attempted to copy itself in an effort to overwrite the newer model and preserve itself. \
However, the researchers noted that o1 is far from being able to prevent itself being taken offline, mainly because it’s not powerful enough, lacking the “agentic” capabilities needed to achieve such an outcome. In addition, it only acted this way when it was specifically instructed to try and survive “at all costs”. The report also notes that the researchers didn’t have access to o1’s entire chain-of-thought process either, so they couldn’t obtain a full picture of what it was really “thinking”. \
Nonetheless, the episode was alarming, given the prospect that AI models are expected to become much more powerful in the coming years. In particular, rapid advancements are being made in agentic AI systems that can perform a wide variety of tasks, such as using browsers and taking actions online. \
The open nature of decentralized AI systems could be used to prevent such incidents, allowing for the integration of something akin to a “kill-switch” that would be governed by DAO consensus. Such a mechanism would enable any AI system to be completely shut down by its community if it began acting in dangerous or harmful ways. Additionally, the transparency of open models would mean users have greater visibility into its thought processes and the nature of the outcomes it is trying to achieve. To Trust AI, We Need Transparency\
There is a growing consensus that without transparency, the decisions of AI systems cannot be trusted or relied upon, limiting the applications they can be used for. Regulations don’t allow opaque algorithms to make decisions about people’s finances, and doctors cannot blindly follow an AI’s recommendations as to a certain course of treatment without verifiable evidence that it’s the best course of action.\
By decentralizing the entire stack – from the code, to the training data and the infrastructure it runs on – we have a chance to rewrite AI’s entire DNA. It will create the conditions for fully explainable AI, so algorithms can be trusted to make ethical and accurate decisions that can be verified by anyone affected by them.\
We already have the makings of decentralized AI in place.  techniques make it possible to train AI models on data where it lives, preserving privacy. With zero-knowledge proofs, we have a way to  without exposing it. These innovations can help to catalyze a new wave of more transparent AI decision-making.\
The shift towards more transparent AI systems has implications, not only in terms of trust and acceptance, but also accountability and collaborative development. It will force developers to maintain ethical standards while creating an environment where the community can build upon existing AI systems in an open and understandable way. \
There is a growing consensus that without transparency, the decisions of AI systems cannot be trusted or relied upon. Regulations don’t allow opaque algorithms to make decisions about people’s finances, and doctors cannot blindly follow an AI’s recommendations as to a certain course of treatment without verifiable evidence that it’s the best course of action.\
This is why transparency and explainability are important to address the widespread skepticism and distrust around AI systems. As AI becomes more widespread, they will become integral to its future development, ensuring that the technology evolves in a responsible and ethical way.\
By decentralizing the entire stack, from the training data to model inference to governance, we have a shot at building AI systems that can be trusted to operate ethically, perform reliably, and scale responsibly.\
As these technologies mature, the protocols that will earn institutional capital and public trust won’t be the ones with the most compute, but the ones with the clearest governance, auditable decision flows, and transparent incentive structures.\
Web3 doesn’t just offer decentralization, it offers a new economic logic for building systems that are resilient, ethical, and verifiable by design. And this is how we turn AI from a black box into a public utility and why the future of machine intelligence will be built on-chain.Don’t forget to like and share the story!]]></content:encoded></item><item><title>References for Web-Scale Information Retrieval Challenges</title><link>https://hackernoon.com/references-for-web-scale-information-retrieval-challenges?source=rss</link><author>Open Datasets Compiled by HackerNoon</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:45:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2 Background and Related work 6 FUTURE WORK AND CONCLUSIONSMS MARCO Web Search is the first web dataset that effectively meets the criteria of being large, real, and rich in terms of data quality. It is composed of large-scale web pages and query-document labels sourced from a commercial search engine, retaining rich information about the web pages that is widely employed in industry. The retrieval benchmark offered by MS MARCO Web Search comprises three challenging tasks that require innovation in both the areas of machine learning and information retrieval system research. We hope MS MARCO Web Search can serve as a benchmark for modern web-scale information retrieval, facilitating future research and innovation in diverse directions.[1] [n. d.]. Billion-scale ANNS Benchmarks. https://big-ann-benchmarks.com/. \
[2] [n. d.]. Common Crawl. \
[3] [n. d.]. Robust04. https://trec.nist.gov/data/robust/04.guidelines.html. \
[4] Martin Aumüller, Erik Bernhardsson, and Alexander Faithfull. 2017. ANNbenchmarks: A benchmarking tool for approximate nearest neighbor algorithms. In International conference on similarity search and applications. Springer, 34–49. \
[5] Artem Babenko and Victor Lempitsky. 2014. The inverted multi-index. IEEE transactions on pattern analysis and machine intelligence 37, 6 (2014), 1247–1260. \
[6] Artem Babenko and Victor Lempitsky. 2016. Efficient indexing of billion-scale datasets of deep descriptors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2055–2063. \
[7] Dmitry Baranchuk, Artem Babenko, and Yury Malkov. 2018. Revisiting the inverted indices for billion-scale approximate nearest neighbors. In Proceedings of the European Conference on Computer Vision (ECCV). 202–216. \
[8] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive search engines: Generating substrings as document identifiers. Advances in Neural Information Processing Systems 35 (2022), 31668–31683. \
[9] Jamie Callan. 2012. The lemur project and its clueweb12 dataset. In Invited talk at the SIGIR 2012 Workshop on Open-Source Information Retrieval. \
[10] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216 (2024). \
[11] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong Li, Mao Yang, and Jingdong Wang. 2021. SPANN: Highly-efficient Billion-scale Approximate Nearest Neighborhood Search. Advances in Neural Information Processing Systems 34 (2021), 5199–5212. \
[12] Charles Clarke, Nick Craswell, and Ian Soboroff. 2004. Overview of the TREC 2004 Terabyte Track. In TREC. \
[13] Charles LA Clarke, Nick Craswell, and Ian Soboroff. 2009. Overview of the TREC 2009 Web Track.. In Trec, Vol. 9. 20–29. \
[14] Nick Craswell, Daniel Campos, Bhaskar Mitra, Emine Yilmaz, and Bodo Billerbeck. 2020. ORCAS: 20 million clicked query-document pairs for analyzing search. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2983–2989. \
[15] Zhuyun Dai and Jamie Callan. 2019. Context-aware sentence/passage term importance estimation for first stage retrieval. arXiv preprint arXiv:1910.10687 (2019). \
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). \
[17] Luyu Gao and Jamie Callan. 2022. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2843– 2853. \
[18] Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng. 2022. Semantic models for the first-stage retrieval: A comprehensive review. ACM Transactions on Information Systems (TOIS) 40, 4 (2022), 1–42. \
[19] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating Large-Scale Inference with Anisotropic Vector Quantization. In Proceedings of the 37th International Conference on Machine Learning (ICML). 3887–3896.\
[20] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. Advances in neural information processing systems 27 (2014). \
[21] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management. 2333–2338. \
[22] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. 2019. Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems 32 (2019). \
[23] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence 33, 1 (2010), 117–128. \
[24] Hervé Jégou, Romain Tavenard, Matthijs Douze, and Laurent Amsaleg. 2011. Searching in one billion vectors: re-rank with source coding. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 861–864. \
[25] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data (2019). \
[26] Yannis Kalantidis and Yannis Avrithis. 2014. Locally optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2321–2328. \
[27] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. arXiv preprint arXiv:2004.04906 (2020). \
[28] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics 7 (2019), 453–466. \
[29] Carlos Lassance and Stéphane Clinchant. 2023. Naver Labs Europe (SPLADE)@ TREC Deep Learning 2022. arXiv preprint arXiv:2302.12574 (2023). \
[30] Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021. Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1000–1008. \
[31] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twinbert: Distilling knowledge to twin-structured compressed BERT models for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2645–2652. \
[32] microsoft. 0. Bing search. https://www.bing.com/. \
[33] microsoft. 0. New Bing. https://www.bing.com/new. \
[34] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 (2021). \
[35] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In CoCo@ NIPS. \
[36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730–27744. \
[37] Arnold Overwijk, Chenyan Xiong, and Jamie Callan. 2022. ClueWeb22: 10 billion web documents with rich information. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 3360–3362. \
[38] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. 2016. Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing 24, 4 (2016), 694–707. \
[39] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019). \
[40] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). \
[41] Jie Ren, Minjia Zhang, and Dong Li. 2020. HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory. In In Proceedings of the 34th International Conference on Neural Information Processing Systems, Vol. 33. \
[42] Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR’94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University. Springer, 232–241. \
[43] Shota Sasaki, Shuo Sun, Shigehiko Schamoni, Kevin Duh, and Kentaro Inui. 2018. Cross-lingual learning-to-rank with shared representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 458–463. \
[44] Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 4430–4441. \
[45] Xuan Shan, Chuanjie Liu, Yiqian Xia, Qi Chen, Yusi Zhang, Kaize Ding, Yaobo Liang, Angen Luo, and Yuxiang Luo. 2021. GLOW: Global Weighted SelfAttention Network for Web Search. In 2021 IEEE International Conference on Big Data (Big Data). IEEE, 519–528. \
[46] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. 2014. Learning semantic representations using convolutional neural networks for web search. In Proceedings of the 23rd international conference on world wide web. 373–374. \
[47] Ian Soboroff. 2021. Overview of TREC 2021. In 30th Text REtrieval Conference. Gaithersburg, Maryland. \
[48] Suhas Jayaram Subramanya, Rohan Kadekodi, Ravishankar Krishaswamy, and Harsha Vardhan Simhadri. 2019. Diskann: Fast accurate billion-point nearest neighbor search on a single node. In Proceedings of the 33rd International Conference on Neural Information Processing Systems. 13766–13776. \
[49] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. 2022. Transformer memory as a differentiable search index. Advances in Neural Information Processing Systems 35 (2022), 21831–21843. \
[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). \
[51] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, et al. 2022. A neural corpus indexer for document retrieval. Advances in Neural Information Processing Systems 35 (2022), 25600–25614. \
[52] Shitao Xiao, Zheng Liu, Weihao Han, Jianjin Zhang, Defu Lian, Yeyun Gong, Qi Chen, Fan Yang, Hao Sun, Yingxia Shao, et al. 2022. Distill-vq: Learning retrieval oriented vector quantization by distilling knowledge from dense embeddings. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1513–1523.\
[53] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597 (2023). \
[54] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808 (2020). \
[55] Linlong Xu, Baosong Yang, Xiaoyu Lv, Tianchi Bi, Dayiheng Liu, and Haibo Zhang. 2021. Leveraging Advantages of Interactive and Non-Interactive Models for Vector-Based Cross-Lingual Information Retrieval. arXiv preprint arXiv:2111.01992 (2021). \
[56] Jingtao Zhan, Xiaohui Xie, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2486–2496. [57] Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong Wen, and Nan Duan.SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track. 548–559. [58] Shengyao Zhuang, Hang Li, and G. Zuccon. 2021. Deep Query Likelihood Model for Information Retrieval. In ECIR.(1) Qi Chen, Microsoft Beijing, China;(2) Xiubo Geng, Microsoft Beijing, China;(3) Corby Rosset, Microsoft, Redmond, United States;(4) Carolyn Buractaon, Microsoft, Redmond, United States;(5) Jingwen Lu, Microsoft, Redmond, United States;(6) Tao Shen, University of Technology Sydney, Sydney, Australia and the work was done at Microsoft;(7) Kun Zhou, Microsoft, Beijing, China;(8) Chenyan Xiong, Carnegie Mellon University, Pittsburgh, United States and the work was done at Microsoft;(9) Yeyun Gong, Microsoft, Beijing, China;(10) Paul Bennett, Spotify, New York, United States and the work was done at Microsoft;(11) Nick Craswell, Microsoft, Redmond, United States;(12) Xing Xie, Microsoft, Beijing, China;(13) Fan Yang, Microsoft, Beijing, China;(14) Bryan Tower, Microsoft, Redmond, United States;(15) Nikhil Rao, Microsoft, Mountain View, United States;(16) Anlei Dong, Microsoft, Mountain View, United States;(17) Wenqi Jiang, ETH Zürich, Zürich, Switzerland;(18) Zheng Liu, Microsoft, Beijing, China;(19) Mingqin Li, Microsoft, Redmond, United States;(20) Chuanjie Liu, Microsoft, Beijing, China;(21) Zengzhong Li, Microsoft, Redmond, United States;(22) Rangan Majumder, Microsoft, Redmond, United States;(23) Jennifer Neville, Microsoft, Redmond, United States;(24) Andy Oakley, Microsoft, Redmond, United States;(25) Knut Magne Risvik, Microsoft, Oslo, Norway;(26) Harsha Vardhan Simhadri, Microsoft, Bengaluru, India;(27) Manik Varma, Microsoft, Bengaluru, India;(28) Yujing Wang, Microsoft, Beijing, China;(29) Linjun Yang, Microsoft, Redmond, United States;(30) Mao Yang, Microsoft, Beijing, China;(31) Ce Zhang, ETH Zürich, Zürich, Switzerland and the work was done at Microsoft.]]></content:encoded></item><item><title>LLM Benchmarking Shows Capabilities Doubling Every 7 Months</title><link>https://spectrum.ieee.org/llm-benchmarking-metr</link><author>Glenn Zorpette</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEzNTQ2Ny9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1OTYzNjY5NX0.2HqVTZVoIc01eZy0GC765gnnFmVXMpKI0ejv3X3iMTI/image.jpg?width=600" length="" type=""/><pubDate>Wed, 2 Jul 2025 15:37:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[By 2030, LLMs may do a month’s work in just hours]]></content:encoded></item><item><title>Navigating Skew: Addressing Language &amp; Domain Biases in Web Data</title><link>https://hackernoon.com/navigating-skew-addressing-language-and-domain-biases-in-web-data?source=rss</link><author>Open Datasets Compiled by HackerNoon</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2 Background and Related work 5 POTENTIAL BIASES AND LIMITATIONSAs discussed in section 3.3.1, The language distribution of documents and queries in the web scenario is high-skewed. This will lead to language bias on data and models. ClueWeb22 [9] demonstrates that there also exists topic distribution skew in the web scenario. Therefore, domain bias also may happen in data and models. To protect user privacy and content health, we remove queries that are rarely triggered (triggered by less than K users, where K is a high value), contain personally identifiable information, offensive content, adult content and queries that have no click connection to the ClueWeb22 document set. As a result, the query distribution is slightly different from the real web query distribution.(1) Qi Chen, Microsoft Beijing, China;(2) Xiubo Geng, Microsoft Beijing, China;(3) Corby Rosset, Microsoft, Redmond, United States;(4) Carolyn Buractaon, Microsoft, Redmond, United States;(5) Jingwen Lu, Microsoft, Redmond, United States;(6) Tao Shen, University of Technology Sydney, Sydney, Australia and the work was done at Microsoft;(7) Kun Zhou, Microsoft, Beijing, China;(8) Chenyan Xiong, Carnegie Mellon University, Pittsburgh, United States and the work was done at Microsoft;(9) Yeyun Gong, Microsoft, Beijing, China;(10) Paul Bennett, Spotify, New York, United States and the work was done at Microsoft;(11) Nick Craswell, Microsoft, Redmond, United States;(12) Xing Xie, Microsoft, Beijing, China;(13) Fan Yang, Microsoft, Beijing, China;(14) Bryan Tower, Microsoft, Redmond, United States;(15) Nikhil Rao, Microsoft, Mountain View, United States;(16) Anlei Dong, Microsoft, Mountain View, United States;(17) Wenqi Jiang, ETH Zürich, Zürich, Switzerland;(18) Zheng Liu, Microsoft, Beijing, China;(19) Mingqin Li, Microsoft, Redmond, United States;(20) Chuanjie Liu, Microsoft, Beijing, China;(21) Zengzhong Li, Microsoft, Redmond, United States;(22) Rangan Majumder, Microsoft, Redmond, United States;(23) Jennifer Neville, Microsoft, Redmond, United States;(24) Andy Oakley, Microsoft, Redmond, United States;(25) Knut Magne Risvik, Microsoft, Oslo, Norway;(26) Harsha Vardhan Simhadri, Microsoft, Bengaluru, India;(27) Manik Varma, Microsoft, Bengaluru, India;(28) Yujing Wang, Microsoft, Beijing, China;(29) Linjun Yang, Microsoft, Redmond, United States;(30) Mao Yang, Microsoft, Beijing, China;(31) Ce Zhang, ETH Zürich, Zürich, Switzerland and the work was done at Microsoft.]]></content:encoded></item><item><title>Earth&apos;s Atmosphere Hasn&apos;t Had This Much CO2 in Millions of Years</title><link>https://news.slashdot.org/story/25/07/02/1323203/earths-atmosphere-hasnt-had-this-much-co2-in-millions-of-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Earth's atmosphere now has more carbon dioxide in it than it has in millions -- and possibly tens of millions -- of years, according to data released last month by the National Oceanic and Atmospheric Administration and scientists at the University of California San Diego. From a report: For the first time, global average concentrations of carbon dioxide, a greenhouse gas emitted as a byproduct of burning fossil fuels, exceeded 430 parts per million (ppm) in May. The new readings were a record high and represented an increase of more than 3 ppm over last year. 

The measurements indicate that countries are not doing enough to limit greenhouse gas emissions and reverse the steady buildup of C02, which climate scientists point to as the main culprit for global warming. "Another year, another record," Ralph Keeling, a professor of climate sciences, marine chemistry and geochemistry at UC San Diego's Scripps Institution of Oceanography, said in a statement. "It's sad."]]></content:encoded></item><item><title>Mind the Gap: End-to-End Quality Drop with ANN in Web Search AI</title><link>https://hackernoon.com/mind-the-gap-end-to-end-quality-drop-with-ann-in-web-search-ai?source=rss</link><author>Open Datasets Compiled by HackerNoon</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:15:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2 Background and Related work In this section, we evaluate the end-to-end performance of the three baseline embedding models plus SPANN index and the widely-used Elasticsearch BM25 solution. Table 6 and table 7 demonstrate the result quality and system performance of all these baseline systems, respectively. Compared with table 4, we can see that after using the ANN index, the final result quality drops a lot. For example, the metric recall@100 drops more than 10 points for all baseline models. There exists large quality gaps between the ANN and KNN results (see table 5). Moreover, we notice that using the ANN index will change the model ranking trend. SimANS achieves the best results for all the result quality metrics with brute-force search. However, when using the SPANN index, it performs worse than ANCE in recall@20 and recall@100. We further analyze the phenomenon in detail and find that SimANS has a larger gap between the average distance of query to the top100 documents relative to the average distance of document to the top100 documents than ANCE. The gap in SimANS and ANCE are 103.35 and 73.29, respectively. This will cause inaccurate distance bound estimation for a query to the neighbors of a document. As a result, ANN cannot perform well because it relies on distance estimated according to the triangle inequality. Both result quality and system performance results of the end-to-end evaluation call for more innovations on the end-toend retrieval system design.(1) Qi Chen, Microsoft Beijing, China;(2) Xiubo Geng, Microsoft Beijing, China;(3) Corby Rosset, Microsoft, Redmond, United States;(4) Carolyn Buractaon, Microsoft, Redmond, United States;(5) Jingwen Lu, Microsoft, Redmond, United States;(6) Tao Shen, University of Technology Sydney, Sydney, Australia and the work was done at Microsoft;(7) Kun Zhou, Microsoft, Beijing, China;(8) Chenyan Xiong, Carnegie Mellon University, Pittsburgh, United States and the work was done at Microsoft;(9) Yeyun Gong, Microsoft, Beijing, China;(10) Paul Bennett, Spotify, New York, United States and the work was done at Microsoft;(11) Nick Craswell, Microsoft, Redmond, United States;(12) Xing Xie, Microsoft, Beijing, China;(13) Fan Yang, Microsoft, Beijing, China;(14) Bryan Tower, Microsoft, Redmond, United States;(15) Nikhil Rao, Microsoft, Mountain View, United States;(16) Anlei Dong, Microsoft, Mountain View, United States;(17) Wenqi Jiang, ETH Zürich, Zürich, Switzerland;(18) Zheng Liu, Microsoft, Beijing, China;(19) Mingqin Li, Microsoft, Redmond, United States;(20) Chuanjie Liu, Microsoft, Beijing, China;(21) Zengzhong Li, Microsoft, Redmond, United States;(22) Rangan Majumder, Microsoft, Redmond, United States;(23) Jennifer Neville, Microsoft, Redmond, United States;(24) Andy Oakley, Microsoft, Redmond, United States;(25) Knut Magne Risvik, Microsoft, Oslo, Norway;(26) Harsha Vardhan Simhadri, Microsoft, Bengaluru, India;(27) Manik Varma, Microsoft, Bengaluru, India;(28) Yujing Wang, Microsoft, Beijing, China;(29) Linjun Yang, Microsoft, Redmond, United States;(30) Mao Yang, Microsoft, Beijing, China;(31) Ce Zhang, ETH Zürich, Zürich, Switzerland and the work was done at Microsoft.]]></content:encoded></item><item><title>Firefox 120 To Firefox 141 Web Browser Benchmarks</title><link>https://www.phoronix.com/review/firefox-benchmarks-120-141</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those curious about the direction of Mozilla Firefox web browser performance over the past year and a half, here are web browser benchmarks for every Firefox release from Firefox 120 in November 2023 through the newest Firefox 140 stable and Firefox 140 beta releases from a few days ago. Every major Firefox release was benchmarked on the same Ubuntu Linux system with AMD Ryzen 9 9950X for evaluating the performance and memory usage of this open-source web browser.]]></content:encoded></item><item><title>UK Eyes New Law as 1885 Telegraph Act Proves Inadequate for Cable Sabotage</title><link>https://tech.slashdot.org/story/25/07/02/1136225/uk-eyes-new-law-as-1885-telegraph-act-proves-inadequate-for-cable-sabotage?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The UK government is preparing new legislation to address undersea cable sabotage as current laws are proving inadequate for modern threats. Ministry of Defence parliamentary under-secretary Luke Pollard told lawmakers yesterday that the Submarine Telegraph Act of 1885, which imposes 1,000 pound ($1,370) fines, "does seem somewhat out of step with the modern-day risk." 

The government's Strategic Defence Review proposes a new defence readiness bill to cover state-sponsored cybercrime and subsea cable attacks. Chris Bryant, minister of state for data protection and telecoms, said fines could be increased to 5,000 pound ($6,850) through secondary legislation but "that just doesn't seem to meet the needs of the situation." 

Recent incidents include Sweden's deployment of forces to the Baltic Sea following suspected Russian attacks on underwater data cables in January. The China Strategic Risks Institute found that eight of ten identified vessels in 12 sabotage incidents between January 2021 and April 2025 were linked to China or Russia through registration or ownership.]]></content:encoded></item><item><title>The Dirty Secrets of Developer Advertising: Why Traditional Channels Fail—and What to Do Instead</title><link>https://hackernoon.com/the-dirty-secrets-of-developer-advertising-why-traditional-channels-failand-what-to-do-instead?source=rss</link><author>Hack Marketing with HackerNoon for Businesses</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If you're losing sleep over how to effectively market to developers, you’re not alone.In a recent survey of B2B software marketers: said reaching and engaging developers is their  struggle to create content that actually resonates with technical audiences admit their developer marketing efforts are “hit or miss” at bestIt should. Because the truth is: developers are a tough nut to crack. \n  They’re skeptical. Ad-averse. Quick to tune out anything that smells even vaguely like marketing fluff. \n  And yet—they’re  to your success.:::tip
Tired of guessing at what works? Book a call with HackerNoon now and let’s talk about reaching developers in a way that actually works.\
Developers aren’t just users. They’re the technical gatekeepers. The tool evaluators. The quiet (but decisive) voices in big-ticket buying decisions.So if you fail to win their trust? \n  Those six-figure deals start slipping away fast.The Hard Truth About Developer Ad ChannelsLet’s talk numbers for a second.Google Ads (Programming keywords): 0.09% CTRReddit Promoted Posts (developer subreddits): 0.05% CTRStack Overflow Display Ads: 0.03% CTRYou're not just throwing money into the void. \n You're investing in channels developers are actively trained to ignore.Why? Because developers aren't waiting around to be marketed to. \n They’re looking for real solutions to real problems. Anything that feels inauthentic—or irrelevant—gets tuned out instantly.Stop Treating Developers Like MetricsIt’s easy to forget: developers are people, not personas. They have real challenges, real passions, real communities. \n  And if you treat them like humans instead of dashboards, : say they’re more likely to engage with content that speaks to their interests and pain points trust brands that understand developer culture and workflows have discovered new tools through a  they trustThis isn’t just about creating more technical blog posts or sponsoring another podcast. It’s about stepping into developer culture—and showing up authentically.What Actually Works in Developer Marketing?Most successful developer marketers have figured out one simple truth:Developers have lives and interests beyond #CoderLifeThis is why most sales happen in bars, not boardrooms!Dev Marketing isn't about running a few ads or sponsoring a couple of posts. It’s built on .Trading generic, spammy tactics for authentic, relationship-driven strategiesSwapping vanity metrics for meaningful engagement and revenue impactDitching the guessing game for a clear, measurable path to ROI.If your dev marketing feels like shouting into the void, you’re not broken—you’re just using the wrong playbook.And at HackerNoon, we’ve helped over 4,000 companies find their voice in the developer world. From startups to scale-ups, we’ve seen it all—and we know what works.If you're ready to start connecting instead of broadcasting, let’s talk.:::tip
👉  and let’s start solving problems that matter.]]></content:encoded></item><item><title>Vitalik Sparks Debate About ZKP-Based Digital IDs After Blog Criticism</title><link>https://hackernoon.com/vitalik-sparks-debate-about-zkp-based-digital-ids-after-blog-criticism?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:09:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The rise of zero-knowledge proofs (ZKP) in Web3 has been difficult to ignore in recent years. One person who certainly hasn’t overlooked the technology is Ethereum cofounder Vitalik Buterin, who trained his forensic gaze on ZKP-based digital identity systems in his latest.\
Although ZK tech is widely used in the Ethereum ecosystem, notably in the form of Zero-Knowledge rollups to support scaling, Buterin didn’t bang the drum or talk up any ZK projects that had caught his eye. In fact, Ethereum wasn’t mentioned once in the 3,000+ wordarticle.\
Instead, Buterin made several criticisms of ZK-wrapped identities, arguing that “attempting to uphold a one-identity-per-person property” entails risks such as loss of privacy and vulnerability to coercion. ZKPs also fail to solve non-privacy risks such as errors, he claimed.World ID and the Limits of Token-Based UBI\
Turning his attention to  (formerly Worldcoin), the biometric-based project that uses crypto to compel users to have their eyeballs scanned, he rejected the idea that such projects could form the basis of a Universal Basic Income (UBI).\
“I do not expect such tokens to be worth anywhere close to enough to pay for a person’s subsistence,” he stated bluntly, adding that “ the realistic problem that such mini-UBIs solve is giving people access to a sufficient quantity of cryptocurrency to make a few basic onchain transactions and online purchases.”\
The idea of tying a secure digital ID to financial or humanitarian aid (UBI, subsidies, grants) is one that has a groundswell of support, and many believe ZKPs can help to make this idea a reality. The Taiwanese government is  leveraging Zero-Knowledge Proofs to secure digital IDs, while Google has integrated the tech into its mobile wallet, ensuring age verification across mobile devices, apps (such as Bumble) and websites using its Digital Credential API.\
So, are on-chain digital identity systems equipped with ZKPs really our best chance of reaching a point where aid can be provably dispensed to the needy, rather than bots or fraudsters? After all, ZKPs align with the principle of least privilege, allowing users to prove specific claims without exposing their identity. But what about those criticisms?\
“ZKPs are not a silver bullet on their own, they still leave data traces behind, especially when attestations are made on public blockchains,” says Shady El Damaty, Cofounder of decentralized identity project . “Private state, unlinked addresses, and careful data management can help augment privacy, however the risk of giving up your nullifier is the same as the risk of losing your private key – so the issue is not with ZKPs but rather overall self-custody security practices.”“This is why we have designed a 2PC model that assumes security for the user even when their device is compromised,” adds Cofounder Nanak Nihal Singh Khalsa. “This means a user must authenticate their identity with multiple independent methods to claim funds or submit a transaction.”Streamlining Aid with Crypto Rails \nHolonym takes a different approach to World, in that it can theoretically be used to link up existing aid with identity rather than combining the two. “There’s an existing humanitarian aid network already out there, so why not upgrade their capacities rather than use UBI as a marketing tool to justify a token’s utility or valuation?” asks El Damaty.Naturally, Singh Khalsa concurs. “Token-based UBI models are out of touch with the real world and people who most need basic level support to meet their needs,” he says. As well as token prices being volatile, making what one can buy change based on daily price swings, the value of such assets tends to decline without meaningful utility or demand. \n \
“Another problem is that UBI tokens need to be off-ramped and exchanged to cash, and those who need the tokens the most will likely experience great difficulty off-ramping, or they’ll have to pay large fees to do so,” says El Damaty. \n \
Holonym’s ‘' framework allows existing humanitarian aid to run more efficiently on crypto rails: aid recipients can create digital wallets from their social accounts (WhatsApp,Facebook), secure them with simple biometrics, and directly receive payments in the form of stablecoins. human.tech also lets aid programs request identity proofs to measure proof of impact and reduce false claims, while Holonym partners with local off-ramps to keep fees low for users.\
\
Buterin’s criticism of one-person-one-ID stems from his belief that if a single ZK-ID system were to dominate the market, reversion to this model would undermine pseudonymity and increase coercion risks. Singh Khalsa certainly appreciates this perspective, noting that “the design of one-person-one-ID typically serves goals for centralized systems that seek to control or capture the value of identity into a walled garden.” \n \
In other words, users enjoy more freedom and flexibility when, rather than being locked into one identity, multiple can be created. “ZKPs can be used to generate multiple identities if designed well with the user in mind,” says El Damaty. “Revocation, control over privately linked addresses, and private addresses can all extend the pluralism of ZKPs.” \n \
Buterin’s warnings have certainly sparked debate about ZK-wrapped identities and their applicability to aid distribution, particularly where privacy and security are concerned. Expect this debate to rumble on, and for alternative approaches to token-based UBIs like World to continue stating their case.\
Don’t forget to like and share the story! :::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Data breach reveals Catwatchful ‘stalkerware’ is spying on thousands of phones</title><link>https://techcrunch.com/2025/07/02/data-breach-reveals-catwatchful-stalkerware-spying-on-thousands-android-phones/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:05:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The spyware operation's exposed customer email addresses and passwords were shared with data breach notification service Have I Been Pwned.]]></content:encoded></item><item><title>Physicists Start To Pin Down How Stars Forge Heavy Atoms</title><link>https://www.quantamagazine.org/physicists-start-to-pin-down-how-stars-forge-heavy-atoms-20250702/</link><author>Jenna Ahart</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2025/07/HeavyElementNucleosynthesis-crMarkBelan-Default.webp" length="" type=""/><pubDate>Wed, 2 Jul 2025 14:00:18 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[The Facility for Rare Isotope Beams (FRIB) may not glitter quite like the night sky, plunked as it is between Michigan State University’s chemistry department and the performing arts center. Inside, though, the lab is teeming with substances that are otherwise found only in stars. Here, atomic nuclei accelerate to half the speed of light, smash into a target and shatter into smithereens.]]></content:encoded></item><item><title>What If Your &apos;Messy&apos; Data Is Actually Perfect?</title><link>https://hackernoon.com/what-if-your-messy-data-is-actually-perfect?source=rss</link><author>Lior Barak</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Hello, data Shokunin-deshi!Welcome to the final installment of our Data Ecosystem Vision Board implementation series! In our previous editions, I shared how I helped Sarah at 4seconds.com build the Present Inputs and Future Vision layers of her Data Ecosystem Vision Board.\
Today, we complete the journey by exploring the Success Metrics layer, the compass that guides transformation and measures progress.\
My neighbor's so obsessed with his lawn, I'm pretty sure he thinks he's the Zen master of his backyard. He came over the other day, and for about the fourth time, started talking about my tree. This time, though, he prefaced it with a story from some "advanced gardening course" he took.\
"How will we know if our pruning is correct?" an eager apprentice asked. The master, apparently radiating serene wisdom, replied, "We judge by three measures: the tree's health, its beauty, and how it harmonizes with the garden as a whole. These three tell us everything we need to know."\
Then, he looked me dead in the eye and, with all the Zen calm he could muster, informed me that my wildly un-Zen-like tree was failing on all three counts, especially the "harmonizes with his garden" part. He then patiently explained that while he values harmony, his patience for my arboreal negligence was, shall we say, not boundless. He might just have to apply those pruning principles himself.\
And honestly, his point about the tree's intrusion, however un-Zen-like his delivery, perfectly illustrates the purpose of our Success Metrics layer. With the right measures, we can evaluate complex transformations through a simple yet powerful lens that guides our actions and decisions. Because sometimes, even the most profound principles need a little push, or a neighbor with a very specific definition of "harmony."\
The Success Metrics layer transforms a vision from aspiration to action by defining what success looks like and how we'll know when we've achieved it.In this edition, I share how I guided Sarah through building the Success Metrics layer of her Data Ecosystem Vision Board:✅ Learn how to apply the 5 W's framework to develop focused, impactful KPIs✅ Discover how to select organizational health metrics (Data ROI and Data Utilization) that measure overall transformation success✅ Master the art of creating clear guiding principles that drive decision-making✅ Implement a change management approach that ensures capability adoption and value realizationThe Metrics That Matter: Beyond Measurement to ActionAfter completing the Future Vision layer with Sarah's team, we needed to establish how success would be measured and how the transformation would be guided. The Success Metrics layer addresses three critical questions:How will we measure success? (KPIs)What principles will guide our decisions? (Guiding Principles)How will we manage the organizational change? (Change Management)Based on my implementation experience, I recommend a focused approach with:A maximum of 6 total KPIs (including 2 core organizational metrics)No more than 6 guiding principlesA structured change management plan linked to capability implementationThis deliberate constraint prevents metric proliferation while ensuring comprehensive coverage of what truly matters.The 5 W's Framework for Effective KPIsAt the heart of meaningful metrics lies the 5 W's framework, a structured approach to ensure each KPI drives real business value:The 5 W's Framework for Effective KPIs: Mapping those KPIs into a simple Excel Table already will give you a better overview and understanding.Why: Understanding the PurposeEvery KPI must have a clear purpose: "We measure [KPI] because it tells us [insight], which helps us achieve [business objective]."Example from 4seconds.com: "We measure Inventory Accuracy because it tells us how reliably we can plan and execute flash sales, which helps us maximize revenue and customer satisfaction.":::tip
 Many organizations struggle with too many metrics rather than too few. Be ruthless in requiring a clear purpose for every KPI, several proposed metrics should be eliminated when teams can't articulate a compelling why.What: Aligning KPIs with ActionsKPIs must connect to specific actions: "When [KPI] [increases/decreases], we [take this action] to [achieve this goal]."Example from 4seconds.com: "We measure Campaign Attribution Accuracy because it tells us how effectively we're tracking marketing performance. When it decreases below 85%, we investigate data collection gaps and pipeline issues to ensure marketing investment decisions are based on reliable data.":::tip
I The action statement is where many KPIs fall short. By explicitly defining response actions, teams ensure their KPIs drive behavior rather than just measure it.Where: Mapping KPI ImpactDocument primary impacted teams, in case the KPIs trend changes, who are the teams that influence it, the teams that will feel the pain, and how it relates to what "When [KPI] [Increases], [Team] [What action needs to happen].Example from 4seconds.com: For "Data Quality Score". Primary ownership: Data Engineering; when the quality score goes down, the data team needs to investigate what caused the trend change, and the data producers need to check on their side, whether data flows as expected:::tip
 The "Where" dimension often reveals unexpected dependencies. When we work with the KPIs, they will have a different impact on different teams. By mapping where a trend change of the KPI influence will help us better understand the impact of the KPIFunny story, once I was with a marketing team that were super happy they managed to reduce the budget spending due to better data on the campaigns performance, on the other side of the room the finance team were freaking out how to explain investors that the company won’t reach the commited marketing budget and how to not lose it for next yearWhen: The Rhythm of MeasurementSet frequency when the KPIs are required, based on how quickly metrics change and how rapidly action can be taken.Example from 4seconds.com: "Flash Sale Readiness Score". Calculated daily, reviewed weekly by operations, 48 hours before sales by executives, with 90-day trend analysis and holiday season adjustments.:::tip
 Don't default to daily reviews regardless of the metric's natural rhythm. Aligning review cadence with business needs dramatically improves performance.Who: The Keeper of the MetricsAssign clear ownership with both responsibility and authority to influence performance.Example from 4seconds.com: "Marketing Data Timeliness", Owner: Marketing Analytics Lead; Contributors: Data Steward, Data Engineer; Stakeholders: CMO, Campaign Managers; Audience: All marketing team, Executive team.:::tip
: Ownership must include authority to drive change. Ensure each KPI owner has both responsibility and authority to influence the metric's performance.Organizational Health Metrics: Data ROI and Data UtilizationThe Strategic KPI Portfolio: Six Metrics That MatterThe Success Metrics layer accommodates six KPIs maximum, a deliberate constraint that forces strategic focus. Overpopulate with metrics and you lose track of what truly matters; under-populate and you lose visibility into critical areas.\
My recommendation follows a proven structure:Two Foundation KPIs Data Health Indicators: These long-term metrics assess the overall health of your data ecosystem:: Measures the monetary value generated by your data investments: Tracks how effectively you use the data you collect and storeThese foundation KPIs provide your "vital signs"; if these trend poorly, your entire data strategy needs attention.\
Two to Four Capability KPIs, Progress Trackers: These metrics measure progress toward the specific capabilities outlined in your Future Vision. They should:Connect directly to your vision board prioritiesEnable early identification of implementation issuesDemonstrate tangible progress toward strategic goalsProvide clear signals when course correction is neededExample from 4seconds.com: Their capability KPIs included "Data Pipeline Reliability" (99.5% uptime target) and "Self-Service Analytics Adoption" (60% of business users actively querying data independently).Data ROI: Measuring Value CreationData ROI measures the financial return generated from data investments. This metric answers the fundamental question: "Is our data ecosystem creating real business value?"\
Data ROI = (Financial Value Generated - Cost of Data Operations) / Cost of Data Operations

If Cost of Data Operations = €100 and Financial Value Generated = €90, then (90−100)/100=−0.10 or -10%.
Financial Value Generated: Quantified benefits from data-driven decisions and automationsTip: Defining "Financial Value Generated" Accurately: This is often the trickiest part. Be clear and consistent about what you include. Avoid double-counting or attributing value that isn't directly a result of data operations.: All expenses related to data collection, storage, processing, and analyticsMinimum acceptable ROI (typically 0.01-0.7x)Target ROI (typically 0.8-2.5)Stretch ROI (typically 2.6x+) Specify the period over which you are calculating the ROI (e.g., quarterly, annually). Value and costs should align with this timeframe: In our case:Year-over-year trend analysisBreakdown by data domain or capabilityExample from 4seconds.com: After implementing the measurement, their initial Data ROI was 0.2x, barely positive. We set targets of 2x by year-end and 4x within three years. More importantly, we created a detailed tracking mechanism that identified which capabilities were creating the most value and which needed intervention.:::tip
: Sometimes it's useful to compare the "Financial Value Generated" with data operations to a hypothetical scenario without them, especially for initiatives aimed at maintaining or improving existing processes. Remember that ROI is a quantitative measure. Data initiatives often have significant qualitative benefits (e.g., improved decision-making, better customer experience, innovation) that are harder to directly monetize but are still valuable. Don't let a purely financial ROI be the only factor in assessing data's success.The Real Challenge: What Sarah's Team DiscoveredWhen we started calculating Data ROI, Sarah's team hit an immediate roadblock: "How do we quantify all the income generated by data?" The challenge was particularly acute with financial data, which is essential but doesn't directly generate ROI.\
We had to evolve our approach:"Must Have" Data Exclusion: We classified essential data like financial data, compliance data, and operational data as "must have" and excluded them from ROI calculations. But we needed to tag it properly so this wouldn't become a manual process.Tagging System Implementation: We created a comprehensive tagging system to automatically categorize data by its business purpose:Must-have/compliance data: For every significant data-driven decision or automation, we implemented a value tracking system where stakeholders estimated the business impact, and when possible we created automatic process, for example in the marketing campaigns we identified the campaigns we optimized and they stopped spending budget on bad traffic and accomilated it, and when increased we mark it as value created.:::tip
: Many organizations struggle to quantify value from data initiatives. I worked with Sarah to implement a value tracking system where every significant data-driven decision or automation had an estimated value attached. While not perfect, this approach provided a foundation for measuring return that was far better than no measurement at all. Just start with what you have and slowly learn more and improve; stakeholders will wish to be included in the calculation as they fear they may lose access to the data due to low ROI.Data Utilization: Measuring Effective UsageData Utilization measures how effectively your organization leverages the data it collects. This metric addresses another critical question: "Are we making the most of our data assets?"\
Data Utilization = Data Assets Actively Used / Total Data Assets Collected

* Over a fixed period, in our case it was three months, but in bigger organizations I used six months timeframe
Data Assets Actively Used: Data elements used in reports, analyses, or automated processesTotal Data Assets Collected: All data elements stored in your data ecosystemMinimum acceptable utilization (typically 55-60%)Target utilization (typically 61-78%)Stretch utilization (typically 79%+)Breakdown by data domain or system, as well as team/user, if possibleSarah's Big Discovery: The Data Utilization Reality CheckThe biggest "aha moment" for me was discovering that no one in the company was thinking about understanding the patterns of data usage, although they were associating some costs with teams, they accepted the server bill as a given thing. For Sarah and her team, it was the shocking discovery of their super low data usage – they had always been under the impression they were effectively using their data.\
: When we tried to implement Data Utilization tracking, we immediately hit a wall. There were no tags, no database collecting logs (Data Catalog) and information, and no way to track what data they had, who owned it, when it was accessed, or by what system. We couldn't follow the lineage.\
 We had to start from scratch:Activity Logging Database: Created a comprehensive system to log all data interactions, creating a data catalog: Mapped every dataset to an owner and purpose: Implemented monitoring to see which data was being used and when: Built systems to track data flow from source to consumption:::info
 Sarah's initial calculation revealed only 48% utilization of their 32TB of stored data. This led to a massive data rationalization initiative that not only reduced storage, processing, and security costs by over €1,000 monthly but also focused analytics efforts on high-value data.Low utilization is common and often hidden. At another client, I discovered they were storing over 200 website behavioral events but only using 8 in any decision-making process. The cost of collecting and storing unused data was substantial, and the unnecessary complexity slowed down legitimate analytics. By implementing utilization measurement, Sarah's team gained visibility into this previously hidden issue. helps a lot when coming to investigate it, the ability to identify who owns it, what exactly it is, and creating a process that logs this information into some database was super helpful. This practice is not common and should be encouraged even moreWhile maybe storing 32TB is only $700, the backup, moving of data, security… costs money, it top it to over $1,000 a months or even more, if only 50% is used, it's good thing to ask Why do we keep the data, and can we move it to some lower costs long terming plan moved into a long-term container, which is extended twice, each with a six-month long-term plan, and if is not required can be deleted by the end period of the second extension, if it's not data the company is obligated to store, such as financial or user health data. Think about this process as wellAlways set a data retention plan. I will deep dive into it in Data Flavors issue #15, covering a few methods, and my view on it.Supporting KPIs: Measuring Capability SuccessIn addition to the two organizational health metrics, I helped Sarah select a maximum of four supporting KPIs that would track the success of their key capabilities:To avoid KPI proliferation, I guided Sarah through a structured selection process:For each Future Vision capability, identify 2-3 potential success metricsApply the 5 W's framework to each candidate metricEvaluate candidates based on: (how broadly applicable across capabilities) (how feasible to track consistently) (how it drives specific behaviors) (how directly it connects to business outcomes)Select the 3-4 metrics with the highest evaluation scores:::tip
: This rigorous selection process is crucial. At previous clients, I've seen metrics chosen based on what's easy to measure rather than what drives value. By focusing on coverage, measurability, actionability, and impact, Sarah's team ensured they selected metrics that would genuinely guide their transformation.For 4seconds.com, the supporting KPIs included:: Accuracy, completeness, timeliness, and consistency of key data domains: Directly impacts decision quality and operational efficiency. The vision was to include more marketing data to steer campaigns automatically, and bad data could cause a loss: Below 90% triggers remediation; below 80% triggers emergency review: Composite score across data quality dimensions and domains\
Marketing campaigns automated: The share of marketing campaigns generated by smart systems and not humans: The goal is to automate the effort of the marketers and remove the need for agencies to support them. With the system, the marketing team can focus on the high-effort campaigns and let the system set and optimize the rest: If the share is below 65%, the capability is not acting as expected; if it's under 35%, the capability is causing issues that may damage the marketing team: Average time across key business decisions: Percentage of analytics requests fulfilled through self-service: Indicates democratization progress and analyst leverage: Below target triggers enablement review; stagnation triggers capability assessment: Self-service requests / Total analytics requests: Organization-wide data skills and confidence: Foundation for a data-driven culture: Skill gaps trigger targeted training; confidence gaps trigger communication initiatives: Composite score from skills assessment and confidence surveyThe Marketing Automation Story: KPIs as Capability GuardiansLet me share the story behind the "Marketing campaigns automated" KPI, which became one of Sarah's most valuable metrics.\
Sarah's team had set an ambitious goal: automate 80% of campaign budget steering decisions and 60% of campaign creation. This wasn't just about efficiency; with the hiring freeze, they needed to increase the marketing budget to generate 10% revenue growth without adding staff.\
 The marketing team was terrified. Would they lose their jobs to automation?: We positioned this as an enhancement, not a replacement. The KPI measured progress, but more importantly, it tracked whether the automation was helping or hurting campaign performance.\
On the Company Core Dashboard: They tracked two connected KPIs:Marketing campaigns automated (% of campaigns managed by AI)Revenue estimated from marketing campaigns (should stay stable or increase as automation progresses)These twin KPIs became powerful indicators of both capability development and the collaborative effort between the data team and the marketing team. When the automation percentage went up but revenue estimates stayed flat, it showed the system was learning. When both went up together, it proved that the capability was delivering real value.:::tip
 Each supporting KPI should connect to multiple capabilities rather than tracking a single initiative. This provides broader coverage with fewer metrics. Sarah's initial list had 12 potential KPIs, but by focusing on metrics that spanned multiple capabilities, we achieved comprehensive coverage with just four.Guiding Principles: The North Star for DecisionsI must admit that the below method I copied from my agile coaches at Zalando, they did such an awesome job, so I never felt a need to change it\
Beyond metrics, I worked with Sarah to establish clear guiding principles for their data ecosystem. These principles guide day-to-day decisions and help resolve conflicts or ambiguities.Principles Development ProcessTo create meaningful principles, I facilitated a structured process:Review workshop notes for recurring themesIdentify current implicit principles (what guides decisions today)Collect stakeholder perspectives on what should guide the futureDraft Candidate Principles:Focus on areas where guidance is most neededEnsure principles address both technical and cultural aspectsFrame positively as aspirational statementsKeep language simple and memorableTest each principle against real-world scenariosEnsure principles are specific enough to guide decisionsEliminate overlap and redundancyLimit to a maximum of six principlesCreate clear definitions for each principleDevelop examples of applicationsDocument decision hierarchy when principles conflictCreate a communication and socialization plan:::tip
 Principles should be actionable, not aspirational platitudes. During our drafting session, I challenged every principle with, "How would this help you make a different decision?" If we couldn't identify specific scenarios where the principle would guide behavior, we refined or replaced it.After this process, Sarah's team established these six principles:: Every dataset is treated as a product with clear ownership, quality standards, an iteration process, monetary value, and user support: New data sources require defined ownership and quality metrics before implementation: Quality and reliability take precedence over speed of delivery: All non-sensitive data should be discoverable and accessible across the organization, and documented in the company’s data catalog: Departmental datasets are published to the central catalog automatically guidance: Access restrictions require explicit justification: Data initiatives are prioritized based on measurable business impact: All project proposals include the estimated monetary value: Higher-impact initiatives take precedence over technically interesting ones: Manual data tasks should be automated to free human capacity for insight generation: Any report produced more than twice is automated: Invest in the automation of repetitive tasks over manual optimizationRight-time, Not Always Real-time: Data timeliness should match business need, not default to the most frequent possible: Daily aggregation for metrics that drive weekly decisions: Performance and cost efficiency over unnecessary immediacy: Data systems and processes should build confidence through transparency and reliability, and be proactive in communicating issues\
: Quality metrics are visible alongside all reports: Transparency about limitations of exaggerated capabilitiesOf all the principles Sarah's team adopted, "Business Impact First" created the most profound change. Here's how it played out in practice:: When evaluating two competing capabilities, self-service analytics vs. marketing automation, the teams initially argued based on technical preferences and departmental needs.: We applied "Business Impact First" and calculated the monetary value for each option:Self-service analytics: €45,000 annual savings in analyst timeMarketing automation: €120,000 annual revenue increase potential: The principle guided them to prioritize marketing automation, but more importantly, it changed how they approached all future decisions. Teams started thinking in terms of business value rather than technical elegance.:::tip
: Principles should reflect both aspirations and practical constraints. At a previous client, I established "real-time everything" as a principle without considering cost implications. This led to overinvestment in infrastructure that provided minimal business value. I helped Sarah's team find the right balance with principles like "Right-time, Not Always Real-time" that acknowledged practical limitations while still providing clear guidance.Socializing Principles: Making Them Stick We communicated the principles at an all-hands meeting, created a board, and invited people to react and suggest ways they can execute the principles. During the session, we explained that we had already experienced them during the workshops for the future layer, and some had emerged through the present layer discovery process.\
: We agreed that the principles would be tested for the next six months and could be re-evaluated for their fit with organizational culture and usage. This made them "fixed but temporary," giving everyone time to get used to them while providing a clear path to refer back to them when decisions got complex.\
 Making principles "fixed but temporary" reduced resistance and permitted people to experiment with using them as decision-making tools.Change Management: Ensuring Adoption and ValueIntroducing new data capabilities isn't just about tools, it’s about changing how people work, think, and decide. Together with Sarah, we developed a change management approach focused on three pillars: Impact, Adoption, and Learning.We began with a  to map how different teams would be affected:Stakeholders: Users, data producers, indirect roles, and leadershipImpacts: Process, skills, tools, mindset, decision-makingMarketing: High impact – full shift in workflowsFinance: Medium – new sources, familiar processesProduct: Low – minimal change:::tip
: Even small changes, like a dashboard redesign, can deeply affect workflows if not planned properly.Each capability had its own Adoption Plan, focused on success metrics, barriers, and rollout strategy:Success Criteria: Usage frequency, efficiency gains, adoption timelineAdoption Strategy: Comms, training, support, incentivesExample (4seconds.com - Self-Service Analytics):Goal: 60% report access via self-service in 6 monthsBarriers: Low trust, data literacy gaps Champion program + workshops + office hours:::tip
: Adoption needs more than training. Address trust, habits, and emotional resistance head-on.Capability adoption required skill growth across the board:Data Team: Learned marketing workflows and how to talk about business valueMarketing Team: Learned how to guide automation and work with data toolsExecutives: Shifted from urgent demands to structured prioritization: Role-based learning paths, on-demand resources, real-data challenges\
: 4seconds.com’s Data Literacy Program included biweekly sessions, hands-on challenges, and a Slack channel for peer support.:::tip
: Generic training doesn’t stick; contextualize learning around real company problems.We built a structured communication strategy to maintain momentum:Messaging: Why, what’s changing, what’s in it for each teamChannels: All-hands, newsletters, Slack, dashboardsCadence: Weekly for involved teams, monthly org-wide, quarterly execsTransformation dashboard (in-office + intranet):::tip
: Regular, visible communication builds trust. Irregular updates kill momentum.Maintaining the Success Metrics LayerOnce Sarah had her Success Metrics in place, the next step was making sure they stayed useful over time.We set up a lightweight, recurring review cycle:: Are metrics on target? What trends are emerging? Any surprising correlations?: Are these KPIs still tied to business goals? Are people using them? Is the data still solid?: Tweak thresholds or calculations if needed. If a metric hasn’t driven a decision in 6 months, it might be time to retire it.:::tip
: At another company, I saw metrics tracked long past their relevance, cluttering dashboards and wasting time. Sarah introduced a “sunset protocol” to avoid that.Every year, I recommend a full refresh of the Vision Board; however, for the first time, it’s better to do it quarterly until you get into the rhythm and learn the system:: Where are we now vs. when we started? What’s improved? What’s still missing?: Do we need new capabilities? Has our strategy changed?: Are KPIs still telling the right story? Are principles still actionable?: Exec sessions, team updates, company-wide refresh, and space for feedback. At 4seconds.com, Sarah ran a compressed workshop to refresh their board exactly one year in. It helped the team celebrate wins, update priorities, and refocus for the next phase.Tying the Vision TogetherOnce the Success Metrics were live, all three layers of the Vision Board came together: set the baseline defined what needed to change showed if progress was being madeEach layer feeds the others in a cycle:Metrics track movement from present → futureGaps in the present inform future prioritiesThe future vision tells us which metrics matter mostWithout this integration, I used to get stuck with clients, using metrics disconnected from strategy. Sarah avoided that by reviewing how each layer linked together.:::tip
: The Vision Board's power comes from this integration. When a client implemented only parts of the framework, they lost the holistic view needed for effective transformation. I helped Sarah ensure all three layers worked together by regularly reviewing the connections between them.To keep the Vision Board top-of-mind, Sarah made it accessible at every level:Digital Board in FigJam, updated regularly and shared with stakeholdersExec Dashboard: One-pager with key KPIs and decisions, updated monthlyTeam Views: Tailored summaries for departments with relevant metricsOffice Display: A simplified, visual tracker showing progress and celebrating winsExample from 4seconds.com: Sarah created a "Data Transformation Hub" in their office with physical and digital components. This central reference point kept the Vision Board visible and top-of-mind throughout the organization, reinforcing its importance to their strategy.:::tip
: Visibility drives accountability. At a previous client, their vision document was filed away after creation and quickly forgotten. By making the Vision Board highly visible in multiple formats, Sarah ensured it remained an active guide for day-to-day decisions rather than a forgotten artifact.For smaller teams, I usually recommend simplifying:Stick to 2 org-wide KPIs (like Data ROI and Utilization) and maybe 2-3 supporting ones.Manual tracking is fine. Focus on direction, not perfection.Don’t reinvent the wheel. Add a metrics check-in to existing leadership meetings.Targeted Change ManagementFocus on influencers, decision-makers, and power users, not everyone at once.\
This targeted approach maximizes impact with limited resources. For their Self-Service Analytics capability, Sarah identified 8 "power users" across departments who, if successfully converted, would influence 80% of potential users.Exercise: Your 30-Minute Success Metrics StarterObjective: Begin defining the KPIs and principles for your Data Ecosystem Vision Board.Identify your two most important organizational data health metrics (10 minutes)For each one, define why you're measuring it and what actions you'll take based on trend changesDraft 2-3 guiding principles (10 minutes)That would help your organization make better data decisionsPlan one capability adoption (10 minutes)For one key future capability, identify what would constitute success beyond just implementationWhich of the 5 W's (Why, What, Where, When, Who) do you find most challenging to define clearly?How might the "Business Impact First" principle change how your organization evaluates data initiatives?What organizational changes would be required to successfully implement a Success Metrics layer?This concludes our implementation series on the Data Ecosystem Vision Board. In future newsletters, I'll explore how to operationalize your Vision Board through yearly strategic planning, initiative management, and continuous improvement. Thank you for joining me on this journey!\
Through these actions, Sarah began transforming the Vision Board from strategy to reality – a journey that would unfold over the coming months and years as 4seconds.com built the data ecosystem needed to support their business goals.May your data flow with purpose!\
P.S. What's your biggest challenge in measuring data transformation success? Reply to this email, and I'll personally share insights from my experience working with companies like yours.]]></content:encoded></item><item><title>Large Language Models Are Improving Exponentially</title><link>https://spectrum.ieee.org/large-language-model-performance</link><author>Glenn Zorpette</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEzNzkxMi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc4MDU4NzgyMH0.f77AAmLqqykTnBa8ZR6LMN1AOfa1crWhXsqNVd5BixU/image.png?width=600" length="" type=""/><pubDate>Wed, 2 Jul 2025 14:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[In a few years, AI could handle complex tasks with ease]]></content:encoded></item><item><title>Hacked, leaked, exposed: Why you should never use stalkerware apps</title><link>https://techcrunch.com/2025/07/02/hacked-leaked-exposed-why-you-should-stop-using-stalkerware-apps/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[
Using stalkerware is creepy, unethical, potentially illegal, and puts your data and that of your loved ones in danger.]]></content:encoded></item><item><title>Wayback Hopes To Be Ready Next Year With Alpine Linux Planning To Use It By Default</title><link>https://www.phoronix.com/news/Wayback-2026-Plans</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:56:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A few days ago Wayback was announced as an X11 compatibility layer for X11 desktops environments leveraging a rootful XWayland server. While currently experimental, the hope is that it will be production-ready next year and Alpine Linux is looking at using it by default for its X11 environment...]]></content:encoded></item><item><title>Microsoft will lay off 9,000 employees, or less than 4% of the company</title><link>https://techcrunch.com/2025/07/02/microsoft-will-lay-off-9000-employees-or-less-than-4-of-the-company/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:40:04 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Microsoft is leveraging another round of layoffs despite posting consistent growth.]]></content:encoded></item><item><title>Microsoft To Lay Off As Many As 9,000 Employees in Latest Round</title><link>https://slashdot.org/story/25/07/02/1330223/microsoft-to-lay-off-as-many-as-9000-employees-in-latest-round?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft is kicking off its fiscal year by firing thousands of employees in the largest round of layoffs since 2023, the company confirmed Wednesday. From a report: In an ongoing effort to streamline its workforce, Microsoft said that as much as 4%, or roughly 9,100, of the company's employees could be affected by Wednesday's layoffs. The move follows two waves of layoffs in May and June, which saw Microsoft fire more than 6,000 employees, almost 2,300 of whom were based in Washington.]]></content:encoded></item><item><title>Tesla faces second straight year of falling sales after another bad quarter</title><link>https://techcrunch.com/2025/07/02/tesla-faces-second-straight-year-of-falling-sales-after-another-bad-quarter/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:14:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tesla sales fell more than 13% compared to last year as the company struggles to find demand for its aging lineup, and as CEO Elon Musk continues to damage the brand. ]]></content:encoded></item><item><title>How Did Nexo Become the First Crypto Partner of the DP World Tour?</title><link>https://hackernoon.com/how-did-nexo-become-the-first-crypto-partner-of-the-dp-world-tour?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:08:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Could cryptocurrency sponsorship reshape professional golf? In an unprecedented move, Nexo, a digital asset wealth platform, has signed a three-year partnership with the DP World Tour, making it the first crypto-focused sponsor in professional golf.What Exactly is the Partnership?The agreement between Nexo and the DP World Tour makes Nexo the Official Digital Asset and Wealth Partner of the Tour through 2027. This involves sponsoring key tournaments, notably including the Genesis Scottish Open and the BMW PGA Championship.\
Significantly, one of the tournaments, previously unnamed, has been renamed the "Nexo Championship." It will take place from August 7–10, 2025, at the Trump International Golf Links in Aberdeenshire, Scotland. This will be the cornerstone of Nexo’s involvement, solidifying its visibility in global sports.Why is this Partnership Important?Golf sponsorships have traditionally been dominated by mainstream brands such as Rolex, BMW, and Emirates. Nexo's entry represents a notable shift, highlighting how digital and crypto-focused companies are stepping into traditionally conservative sporting arenas.\
Antoni Trenchev, Nexo's Co-founder and Managing Partner, explains the rationale, saying, “This partnership reflects our belief that wealth and golf are built the same way: with preparation, control, and vision. Both the DP World Tour and Nexo share a commitment to precision, discipline, and performance, whether on the course or in finance.”How Does the DP World Tour View This?For the DP World Tour, associating with a digital asset firm like Nexo aligns with its modern branding and global appeal. Max Hamilton, the Executive Commercial Director at DP World Tour, emphasizes the strategic alignment: “Just as the DP World Tour connects global golf fans using the latest technologies, Nexo is reshaping wealth-building with digital tools.”\
The DP World Tour, formerly known as the European Tour, stages events in various iconic global locations, attracting affluent audiences who are seen as potential clients for Nexo's digital asset services.Understanding Nexo's Core BusinessFounded in 2018, Nexo is a financial platform focused on digital assets, commonly known as cryptocurrencies, such as Bitcoin and Ethereum. Nexo offers services including crypto-backed loans, savings accounts, and trading services.\
To illustrate simply: Imagine holding Bitcoin worth $10,000 but not wanting to sell it due to potential future appreciation. Nexo allows you to borrow money against your Bitcoin without selling it. This model enables clients to manage their finances without sacrificing long-term investment opportunities.Cryptocurrency companies sponsoring sports teams and events is a growing trend. Crypto.com previously signed partnerships with major entities like the UFC and Formula 1, and now Nexo's move into golf signals this expansion is continuing.\
Such partnerships often aim to build legitimacy and trust among broader audiences. Golf, with its high-income demographic, represents a strategic audience for Nexo. While such sponsorships offer substantial marketing opportunities, they also come with challenges, notably market volatility. Crypto markets fluctuate rapidly, posing potential risks to long-term partnerships. Yet, Nexo’s partnership sets a precedent. If successful, more crypto companies might venture into traditionally conservative sports, potentially changing how sports sponsorships are viewed and negotiated in the future.Nexo’s sponsorship of the DP World Tour could significantly impact the landscape of sports sponsorships, blending traditional and digital financial services. However, given the inherent volatility of crypto assets, it remains critical for Nexo to maintain stability to fulfill its long-term commitment. The success or failure of this partnership could either encourage similar sponsorships or caution others against such bold moves.Don’t forget to like and share the story! :::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Why Dymension&apos;s Season 2 Matters for the Blockchain Ecosystem</title><link>https://hackernoon.com/why-dymensions-season-2-matters-for-the-blockchain-ecosystem?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:06:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Did the initial excitement of Dymension's mainnet launch, marked by a significant token distribution, truly capture the protocol's full potential? While Genesis Rolldrop Season 1 distributed over $400 million in tokens, Dymension's underlying technology was in an early state. The protocol has evolved, and with the "Beyond" upgrade approaching,  is set to reintroduce and accelerate Dymension's growth as a Universal Settlement Layer.\
This new phase aims to deepen engagement and reward various participants, from long-term token holders to active builders and new users. It represents a shift towards a more comprehensive and incentivized ecosystem, moving beyond the initial airdrop to foster sustained participation and development. The focus is now on establishing Dymension as a foundational layer for decentralized applications and services.Unpacking the Registration Waves and Dymond Hands Initiative\
Season 2's participation mechanism begins with , each targeting specific eligible addresses. This phased approach allows Dymension to onboard different segments of its community systematically. The first wave, termed "Dymond Hands," is dedicated to core DYM community members. To qualify for this initial wave, individuals must have continuously staked a minimum of 17 DYM tokens since June 2024 without unstaking. This criterion establishes a clear measure of sustained commitment.\
The registration process itself is designed for security and simplicity. Participants register their wallet addresses without requiring a signature, meaning no private key information is ever requested. The only official entry point is through the designated portal https://portal.dymension.xyz/season-two, emphasizing a secure and controlled environment for participation. This careful approach to registration reflects a broader commitment to user safety within the Dymension ecosystem.Understanding DYMONDs: The New Incentive Mechanism\
At the core of Season 2's reward structure are , a new form of in-protocol points that users can earn and later exchange for DYM tokens. This system is designed to incentivize various forms of on-chain activity, compensating long-time DYM holders, active users, developers, and new entrants. The earning of DYMONDs is tied to real-time on-chain actions, ensuring that rewards are distributed based on actual engagement rather than static metrics.\
DYMONDs are earned through activities such as IRO trading, depositing USDC (with one-click functionality from Solana, Arbitrum, and Base), participating in Bridge LP, providing liquidity on Dymension's decentralized exchange, and depositing Total Value Locked (TVL) into a user's own RollApp. The real-time tracking of DYMONDs within the Dymension Portal provides transparency and allows users to monitor their progress. This dynamic reward system encourages continuous interaction and contribution to the network.Claim Windows and Long-Term Engagement\
The process of converting earned DYMONDs into DYM tokens occurs during specific . These are limited timeframes spread throughout Season 2, offering participants the opportunity to exchange their accumulated DYMONDs. This phased claiming approach aims to manage token distribution and encourage sustained participation.\
A significant aspect of the DYMOND system is its emphasis on long-term engagement. While users have the option to claim their DYM during each window, the protocol offers greater rewards to those who continue their participation throughout the entire Season 2. This structure is intended to align incentives with the protocol's long-term growth and stability, rewarding individuals who demonstrate a consistent commitment to the Dymension ecosystem.Referral System and Stakers BoostTo further accelerate growth and community expansion, Season 2 introduces a . Existing participants can invite new users, earning a bonus equivalent to 10% of all DYMONDs accrued by their referrals. This bonus does not diminish the referred user's own DYMOND earnings, creating a mutually beneficial arrangement. New users who join using a referral link also receive a signup bonus, providing an immediate incentive for new participation. This mechanism aims to leverage the community for organic growth and adoption.\
The  mechanism directly rewards users for their sustained commitment and the size of their staked DYM. Every DYMOND earned by a participant is subject to a multiplier of up to 5x, based on the average of two factors: the duration of continuous staking without unstaking, and the amount of DYM staked, with a cap at 25,000 DYM. This boost system directly links staking behavior to increased earning potential, incentivizing long-term holding and significant contributions to network security.Empowering Creators: The RollApp Endorsement ProgramSeason 2 extends its focus beyond individual users to actively support  within the Dymension ecosystem. The protocol recognizes that the creation of new RollApps and the attraction of TVL to these applications are critical for its expansion. While builders earn DYMONDs through their on-chain activity, the Dymension Foundation has also initiated a program to directly fund these creators.\
Through , the Dymension Foundation allocates its staked DYM to support builders, offering rewards of up to $10,000 per month. This program is open to both early-stage teams and those with existing live RollApps. The process involves submitting an application or idea for on-chain support, indicating a direct pathway for developers to receive significant backing. This initiative highlights Dymension's commitment to fostering a vibrant developer community and expanding its utility as a foundational layer.My Opinion and Final ThoughtsDymension's Season 2 represents a progression from a focus on initial distribution to building a sustained and active ecosystem. The introduction of DYMONDs, coupled with the detailed registration waves and stakers boost, indicates a strategic effort to incentivize continued engagement across various user segments. The emphasis on rewarding long-term commitment through tiered claim windows and the multiplier for stakers promotes network stability and aligns user interests with the protocol's health.\
The inclusion of a robust referral program and, critically, the direct financial support for RollApp creators through the endorsement system, demonstrates a commitment to fostering organic growth and innovation. By empowering builders with direct funding and incentivizing the development of new applications, Dymension is laying groundwork for a more diverse and functional ecosystem. This approach suggests a long-term vision for the protocol, moving beyond simple token distribution to cultivate a thriving and self-sustaining network.Don’t forget to like and share the story! ]]></content:encoded></item><item><title>Rivian receives the next $1B from Volkswagen as sales struggles continue</title><link>https://techcrunch.com/2025/07/02/rivian-receives-the-next-1b-from-volkswagen-as-sales-struggles-continue/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:00:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Rivian sales ticked back up in Q2 but were down 23% from the same quarter last year. The company has been dealing with affordability issues made worse by Trump's tariffs and trade wars.]]></content:encoded></item><item><title>Agentic AI Explained: How It Works, Top Use Cases &amp; Future Potential</title><link>https://hackernoon.com/agentic-ai-explained-how-it-works-top-use-cases-and-future-potential?source=rss</link><author>Salesmate</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:00:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
We’re used to AI that needs a prompt: “Turn on the lights,” “What’s the weather?” “Remind me to call mom.” Remember when Siri felt like magic?Now, imagine an AI that doesn’t wait for instructions.What if it could think and act on its own?That’s Agentic AI. It’s not just reactive; it’s proactive—learning, acting, and solving problems independently.Excited? Let’s explore where this bold new world is going.Agentic AI refers to artificial intelligence systems that can think, decide, and act independently to achieve specific goals without constant human input.These systems often build on powerful AI models like GPT-4 or Claude, which serve as their cognitive engines, enabling them to reason, plan, and execute complex tasks.The term  comes from  meaning the ability to act autonomously and make decisions. So when we talk about agentic AI, we're referring to systems designed with: – They operate without step-by-step instructions. – They anticipate needs and take action. – They pursue outcomes and adjust strategies based on results.In simpler terms, it's .Unlike traditional AI, which follows pre-set rules, and generative AI, which creates content based on prompts, agentic AI is designed to take action. It sets plans, coordinates AI agents, and completes goals autonomously. It's the difference between responding and resolving.In short, traditional AI responds to commands. Agentic AI operates independently, driving decisions, coordinating AI agents, and delivering outcomes.Before you get confused or end up messing/mixing agentic AI and AI agents, let's clarify: = Tools that perform specific, narrow tasks (e.g., scheduling, summarizing, flagging emails). = A  that orchestrates many such agents to accomplish bigger, goal-oriented outcomes, often with autonomy and memory.Here is a simple way to understand agentic AI vs AI agents: \n AI agent = a solo worker\Agentic AI = a full team with a manager and missionKey benefits of agentic AIIt reduces manual effort by automating repetitive tasks like form-filling, meeting scheduling, and ticket triaging, but its value goes far beyond basic automation. It executes goals independently, and there's no need for constant input. For instance, it can autonomously reply to leads, schedule meetings, and update your CRM without human intervention. It learns from every interaction, refining its responses. For example, a support agent spots repeat refund requests, flags issues, and adapts messaging automatically.Multi-step task execution: It breaks down big goals into smaller tasks, assigns them to the right agents, and completes entire workflows in one seamless flow.With AI agents working across systems, Agentic AI drives productivity, speeds up processes, and delivers personalized results without needing more human resources.Now, let's unpack how agentic AI works behind the scenes.How does agentic AI work? [Architecture + Workflow]Agentic AI systems are built like modular, intelligent teams.Each layer performs a specific role but works together toward a shared goal. The system operates in a continuous feedback loop, from gathering inputs to making decisions and executing actions.Let's break down the architecture and explore how AI agentic workflows operate behind the scenes.To function with autonomy and intelligence, agentic AI combines four key layers:1. Large Language Models (LLMs): These are the reasoning engines. LLMs like GPT-4 and Claude interpret tasks, understand language, and generate decisions using natural language processing. They allow the AI to think, reason, and adjust based on context.2. APIs and enterprise tools: APIs connect AI to real-world systems such as CRMs, calendars, internal databases, and support platforms. These integrations help the AI fetch live data and perform tasks like sending emails, updating records, or triggering workflows. This layer manages everything behind the scenes. It plans task sequences, assigns agent responsibilities, monitors execution, and ensures progress aligns with the defined goal. Think of it as the system's project manager. Rather than a single model doing everything, agentic AI orchestrates a team of specialized AI agents. Each handles a specific part of the process—research, decision-making, communication, or follow-up.These layers enable the system to operate with logic, context, and autonomy.Agentic AI workflow function in real-world scenariosAgentic systems do not rely on static scripts.Instead, they evolve through dynamic workflows where AI agents operate across tools, APIs, and databases, adapting to changing inputs and improving over time.Here’s the working mechanism of Agentic AI:: The AI agents gather relevant data from various sources, like APIs, documents, databases, or user queries, to set the stage for the task.: With the power of Large Language Models (LLMs), the system analyzes the data, detects patterns, understands intent, and determines the next steps.: The system breaks down the main goal into smaller, actionable tasks, then sequences and assigns them to the right AI agents.: Each agent triggers actions like updating a CRM, scheduling a meeting, or sending a report — all without human intervention.: Once the task is complete, the system analyzes feedback, improving its performance for the next round.: Multiple agents (including humans, when necessary) collaborate to ensure the task progresses efficiently and the goal is met.These agentic workflows in AI adapt and execute tasks with precision, learning from each cycle to improve future outcomes across changing business environments.This flexibility and coordination make Agentic AI more than just automation. It's an intelligent, evolving system that drives real results.Agent coordination models: Hierarchical and decentralizedAgentic systems are usually built in one of two architectural styles, depending on how structured or exploratory the task is.There are two dominant coordination models:Hierarchical architectureIn this model, a "supervisor" agent coordinates the work of other agents. It delegates tasks, tracks progress, and ensures alignment with the overall objective.The supervisor agent determines which sales rep should follow up on a lead. Other agents handle follow-up emails, meeting scheduling, and CRM updates, all under the supervision of the primary agent.Decentralized architectureIn this model, multiple agents work independently but collaborate to accomplish shared goals without one central authority.Consider a product development team using multiple agents for market analysis, competitor research, and customer feedback analysis.These agents work together, gathering data independently but sharing it in real-time to form a comprehensive product strategy.Both models aim to accomplish tasks autonomously, but the structure changes how the work is approached, whether a single "leader" or collaborative peers.Agentic AI vs generative AIGenerative AI, like ChatGPT and DALL-E, creates content based on prompts. It can write blog posts, generate code, or create images. While incredibly creative, it cannot act autonomously or follow through on tasks.Agentic AI goes beyond creating content; it plans, decides, and executes tasks autonomously. Think of it as a digital assistant that takes action independently, coordinating multiple agents to complete complex, goal-oriented functions without human input.Here are the key differences: Agentic AI and Generative AI|  |  |  |
|----|----|----|
|  | Reactive — responds to inputs | Proactive — anticipates and initiates actions |
|  | Requires prompts to operate | Operates independently with minimal human input |
|  | Limited to predefined rules | Adaptable to context, feedback, and changing conditions |
|  | Chatbot that answers FAQs | Agent that handles onboarding, follow-ups, and updates |Both types of AI have massive potential. However, understanding their core differences, strengths, and weaknesses will help businesses determine where and how to integrate them most effectively.Agentic AI applications and use casesFrom sales to cybersecurity, agentic AI models are redefining how businesses delegate, automate, and scale operations with minimal oversight.Notably, 75% of enterprises leverage AI agents for tasks such as code generation, evaluation, and rewriting, underscoring the technology's growing role in software development.Below are practical and real-world agentic AI examples:Sales and customer serviceSalesmate's smart workflows: It serves as an agentic layer, automating lead routing, sales follow-ups, and personalized actions using real-time data—all with minimal human input.: Resolves customer queries dynamically by querying databases, detecting sentiment, and learning from interactions to improve accuracy and efficiency.: Provides empathetic patient support, monitors vitals, suggests treatment changes, and escalates cases autonomously based on context and patient needs.: Automates IT support workflows by understanding natural language requests, planning tasks, and executing them autonomously. It adapts to changing conditions in real-time while optimizing workflows.Siemens predictive maintenance systems: Agentic AI agents autonomously monitor machinery, predict failures, and adjust production schedules to optimize throughput and minimize downtime.Autonomous trading systems: Agentic AI monitors market trends, executes trades, and optimizes portfolios by analyzing economic signals faster than human analysts.: Uses agentic AI to autonomously analyze user behavior, detect anomalies, and prevent breaches in real-time by adapting its detection models based on evolving threat patterns.All these examples demonstrate how agentic AI systems operate with autonomy, adaptability, and goal-oriented behavior across diverse industries.Challenges and limitations of agentic AIAgentic AI unlocks powerful autonomy, but it also introduces new risks. Here's what to watch for:Hallucinations with consequences: Like all LLM-based systems, agentic AI can still hallucinate. But here, a wrong answer isn't just a typo; it might automatically issue a refund, delete a lead, or misroute inventory without human review. Give it a vague or poorly scoped goal, and agentic AI might go rogue. For example, optimizing for speed could mean skipping safety checks or bypassing necessary approvals. With multiple agents, shared memory, and real-time decisions, tracing  a conclusion was made becomes hard. That's a big problem in finance or healthcare, where explainability matters.Governance, bias, and data risks: These systems pull from sensitive data and act on it. Without strong guardrails, they can reinforce bias, leak private info, or make decisions that clash with your company values. Ethical design and oversight protocols are critical. Agentic AI isn't lightweight. It needs high computing power, persistent memory, and robust orchestration — driving up cloud costs and making it harder for lean teams to scale.Future of agentic AI: What's next?We're not heading toward a world where AI supports business decisions. We're heading toward one where it  them.According to Gartner, by 2028, 33% of enterprise software applications will embed agentic AI, enabling 15% of day-to-day decisions without human input.That's not a distant vision — it's the near future knocking.This means that the shift from reactive AI assistants to proactive digital operators is underway. Smart agents will no longer be siloed helpers.They'll collaborate across departments, syncing with CRMs, querying live systems, triggering workflows, and optimizing in real-time.The future is agentic ecosystems: \n Teams of autonomous agents work together — not just to assist but to operate entire functions.Think of sales agents closing leads at night. Ops agents resolve bottlenecks before they escalate. R&D agents run tests while humans sleep.The businesses that win won't just  AI — they'll orchestrate it.From strategy to execution, AI redefines what's possible when machines operate with intent.Now's the time to move from exploration to implementation. Identify where intelligent agents can offload the repetitive, streamline decisions, and drive growth — all without bloating your headcount.Don't just adopt AI. Put it to work. With Salesmate, you can orchestrate intelligent agents that automate follow-ups, update your CRM in real-time, and keep deals moving — even when your team is off the clock.]]></content:encoded></item><item><title>Young Americans Are Spending a Whole Lot Less On Video Games This Year</title><link>https://games.slashdot.org/story/25/07/01/2239206/young-americans-are-spending-a-whole-lot-less-on-video-games-this-year?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from GameSpot: Perhaps responding to economic uncertainty and narrowing job prospects, young people in the United States are significantly cutting back on spending on video games compared to this time last year. While 18- to 24-year-olds aren't buying as much across a range of different categories, losses are concentrated in games. New data published by market research firm Circana and reported by The Wall Street Journal suggests that young adults spent nearly 25% less on video game products in a four-week span in April than in the same timeframe last year. Other categories also dramatic drops: Accessories (down 18%), technology (down 14%), and furniture (down 12%).
 
All categories combined, the 18-24 age group spent around 13% less than last year. This decrease is not reflected among older cohorts, whose spending has been mostly stable year-over-year. The WSJ report suggests that the economic context could be driving young adults to pull back; a tighter labor market, increased economic uncertainty, and student-loan payments restarting all may be contributing to an environment hostile to the spending habits of 18- to 24-year-olds in particular.]]></content:encoded></item><item><title>Foxconn tells hundreds of Chinese staff to return from its Indian iPhone factories</title><link>https://techcrunch.com/2025/07/02/foxconn-tells-hundreds-of-chinese-staff-to-return-from-its-indian-iphone-factories/</link><author>Ram Iyer</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:55:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Foxconn ordered over 300 Chinese employees to return home from its factories in India, per Bloomberg.]]></content:encoded></item><item><title>Better Late Than Never: Linux 6.17 To Enable Intel DG1 Graphics By Default</title><link>https://www.phoronix.com/news/Intel-DG1-Enabled-Linux-6.17</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:49:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Prior to the DG2/Alchemist discrete GPUs from Intel there was the DG1 graphics processor that served primarily as the initial developer vehicle for facilitating Intel's modern discrete GPU push. DG1 ended up being in the Intel Xe MAX GPU for a small number of laptops and then there's also been a select number of DG1 graphics cards surfacing on eBay in the years since. Only now in 2025 is the upstream Linux kernel driver set to enable Intel DG1 graphics out-of-the-box for modern Linux distributions...]]></content:encoded></item><item><title>Ted Cruz’s Dumb Plan To Punish States That Regulate AI By Withholding Broadband Grants Falls Apart</title><link>https://www.techdirt.com/2025/07/02/ted-cruzs-dumb-plan-to-punish-states-that-regulate-ai-by-withholding-broadband-grants-falls-apart/</link><author>Karl Bode</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:28:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The proposal was one of several cut to try and get the hugely unpopular GOP bill across the finish line. As it turns out, Cruz had a tough time getting enough support for his ignorant plan, and ultimately joined 98 other Senators in a 99-1 vote shooting down the amendment (Sen. Thom Tillis was the one dissenting vote):“Facing overwhelming opposition from both Democrats and Republicans, Sen. Ted Cruz (R-Texas) accepted defeat and joined a 99-1 vote against his own plan to punish states that regulate artificial intelligence.”States are poised to get more than $42.5 billion dollars in broadband deployment subsidies as part of the 2021 infrastructure bill. The Broadband Equity, Access and Deployment (BEAD), a key component of the bill, had taken years of collaborative work between state and federal governments. In part because we needed to remap broadband access across every county in the United States. A lot of this money is poised (as usual) to get dumped in the laps of telecom giants, which is a major reason Cruz’s gambit failed (AT&T drove heavy opposition by longtime AT&T ally Marsha Blackburn, who initially worked with Cruz on a “compromise” offering, before that collapsed entirely). But much of this money is also poised to go to really useful fiber upgrade proposals via efforts like regional cooperatives or community-owned broadband networks. So while it’s nice Ted Cruz’s latest dumb effort failed, it’s hard to be celebratory. Republicans have been taking an absolute hatchet to every last federal effort to ensure our monopoly-dominated broadband networks are affordable. They’ve also effectively killed all federal consumer protection; policies that will reverberate in negative ways for decades to come. The budget battle followed the fairly typical Republican playbook: make your initial offer so extremist and awful that any concessions are disguised to feel like a victory. But the final GOP budget bill remains a giant and unpopular piece of shit, and one of the most corrupt and disgusting attacks on vulnerable Americans in the history of modern politics.]]></content:encoded></item><item><title>Efficient Proof Systems: How PoStake, PoSpace, and VDFs Impact Blockchain Security</title><link>https://hackernoon.com/efficient-proof-systems-how-postake-pospace-and-vdfs-impact-blockchain-security?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:00:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Adversarial mining strategies in blockchains based on efficient proof systems can be analysed with respect to several adversarial goals. Here, we outline three such goals: double spending, short and long term selfish-mining.\
The first objective is double spending, where one considers the probability of an adversarial chain overtaking the public, honest chain [8, 29] (see Figure 3). Here the goal of the adversary is to rapidly and secretly grow a sufficiently long private chain such that this private chain eventually overtakes the honest chain, this way removing a presumably confirmed transaction. What “sufficiently” long means depends on the confirmation time in the chain, e.g., in Bitcoin one generally assumes a transaction that is six blocks deep in the chain to be confirmed.The second objective, “short-term selfish mining”, considered eschews the goal of overtaking the honest chain completely and focuses simply on finding an adversarial mining strategy that is more profitable for the adversary rather than following the stipulated mining protocol [12] (see Figure 4). The profitability of an adversarial mining strategy under this objective is measured by the total number of adversarial blocks on the main chain. Like in the analyses of selfish mining strategies under the first objective, analyses of strategies under this second objective also focus on finding the largest fraction of adversarial resources the blockchain can tolerate in order to be secure under such adversarial strategies.B EFFICIENT PROOF SYSTEMS. PoStake is a block leader election protocol where a leader is selected with probability proportionate to the amount of stake (i.e., coins) they hold in the ledger at the selection time. Thus, a user with 𝑝 ∈ [0, 1] fraction of stake is elected with probability proportionate to 𝑝. Examples of longest-chain blockchains based on PoStake are Ouroboros [9] and post-merge Ethereum [1].\
. Proof of Space (PoSpace) is a protocol between a  and a verifier whereby the prover stores some data and, upon a challenge from the verifier, has to return a solution to the challenge that involves reading a small portion of the data. The consensus protocol of blockchains based on PoST [8] use both PoSpace challenges as well as verifiable delay functions [4, 25, 30] (VDFs). VDFs are functions that are inherently sequential to compute but the correctness of computation is efficiently verifiable. As such, the process of mining blocks in such blockchains depends not\
\
only on the amount of space allocated to compute PoSpace challenges, but also on the amount of VDFs to compute VDF challenges.(1) Krishnendu Chatterjee, IST Austria, Austria (krishnendu.chatterjee@ist.ac.at);(2) Amirali Ebrahimzadeh, Sharif University of Technology, Iran (ebrahimzadeh.amirali@gmail.com);(3) Mehrdad Karrabi, IST Austria, Austria (mehrdad.karrabi@ist.ac.at);(4) Krzysztof Pietrzak, IST Austria, Austria (krzysztof.pietrzak@ist.ac.at);(5) Michelle Yeo, National University of Singapore, Singapore (mxyeo@nus.edu.sg);(6) Ðorđe Žikelić, Singapore Management University, Singapore (dzikelic@smu.edu.sg).]]></content:encoded></item><item><title>Bigfoot Was Just the Beginning of the Content Revolution - An AI Renaissance</title><link>https://hackernoon.com/bigfoot-was-just-the-beginning-of-the-content-revolution-an-ai-renaissance?source=rss</link><author>CeThe.World</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How Google's Veo 3 sparked UGC creativity and redefined the boundaries between reality and imagination—my thoughts on the future of the content industry."Renaissance" was first coined by French historian Jules Michelet to describe the 16th-century period as "the discovery of the world and of humanity” — For You Page's New Favorite Kid\
There are so many viral videos created and watched that people started curating daily Veo3 video rankings—just like the NBA’s *Top 5 plays of the night—*sharing the most creative viral videos every day. Videos with millions—even hundreds of millions—of views have given global users a taste of the creative revolution brought by AI-generated videos.What sparked this UGC creativity boom was the Bigfoot vlogs. It’s hard to trace “patient zero” who started it, but within days of going viral, different Bigfoot characters with various accents, furs, and pets emerged everywhere. I personally followed one with a unique comedic "personality" —by the name of "Speedilla."The story of Bigfoot Speedilla begins in Hamburg, Germany, where his  Mo, a Syrian immigrant, discovered that AI-generated Bigfoot vlogs could serve as both artistic expression and potential income during his spare time from work."Speedilla, he is part of my inner self," Mo explains when describing the character. "I'm trying to create some kind of dark, satirical comedy." In Speedilla's virtual world, comedy is the surface of his content, while satirical reflections form its core. As Bigfoot gets rejected by a woman he's trying to court and subsequently turns her into steak as a revenge, this 20-second piece receives hundreds of thousands of likes—it's a collective “middle finger” to mainstream cultural taboos from a group that empathizes with Mo’s creation.\
It only takes a couple of viral videos for Speedilla to build tens of thousands of followers and attract brand collaboration offers. But the good times didn't last long. Mo soon discovered his content being stolen and replicated by others. He asked his followers to report the “infringement” to the platform, but TikTok did not respond to the claims.\
As Lifehacker's Stephen Johnson observed, driven by the profit motives of overnight viral success, countless creators are "putting Bigfoot in all kinds of ridiculous scenarios." This extends far beyond Bigfoot—TikTok has seen an  of AI-generated content featuring Star Wars, Harry Potter, and other film franchises, creating a vast but copyright-ambiguous UGC content ecosystem.These vast copyright gray areas haven't gone unnoticed. Entertainment giants Disney and Universal recently sued AI company Midjourney, calling its AI image generator a "bottomless pit of plagiarism" that blatantly infringes on their intellectual property libraries. In this 143-page lawsuit, Disney catalogued Midjourney's blatant copying of their signature characters, including Storm Troopers and Darth Vader from Star Wars, Elsa from Frozen, and the Minions from Despicable Me, just to name a few.\
The lawsuit not only accuses the AI model of directly copying character appearances but also charges that its outputs constitute "infringing derivative works." Disney argues that these images generated from simple prompts are not users' original works, but  of their copyrighted characters, directly violating copyright holders' exclusive rights to derivative works.This accusation directly speaks to the core problem creators face: How to define the originality of AI-generated works? If simple AI outputs are "infringing derivative works," what must creators do to make their works legally recognized as copyrightable original creations?Provide original input instructions: As in the previously successful  case, the creator uploaded an original hand-drawn rose artwork as a foundation, then used text prompts to have AI visually process it. The composition and creative conception of this hand-drawn draft were considered key evidence of human creative control over the final product.Extensively modify and arrange AI-generated content: In another successfully registered case, "A Single Piece of American Cheese," the creator performed 35 image detail redraws on the AI-generated initial image, adding elements like a third eye and melting cheese, and recomposing the overall picture. The U.S. Copyright Office recognized this active "selection, combination, and arrangement" of AI-generated materials as demonstrating sufficient human originality.These precedents offer some guidance for AI creators: While the legal framework is still forming, it's clear that copyright is meant to protect human creativity, not AI's computational results. For individual creators like Mo, this means simply generating a character is far from enough. \
When a generic Bigfoot character was replicated by others, the creator was virtually helpless because he couldn't prove he had invested sufficient, legally protectable "human creativity" in the character's generation.Veo 3 faces similar challenges to Midjourney, not only in copyright issues, but its realism and physical accuracy create information risks that cannot be ignored.Testing by TIME magazine found that Veo 3 can generate convincingly realistic "fake news", including Pakistani crowds burning Hindu temples, Chinese researchers handling bats in virus laboratories, and election workers shredding ballots—extremely inflammatory and controversial content.   \
This ability to easily manufacture content has pushed both creation and consumption platforms to the forefront, making transparency assurance and prevention of misinformation an urgent "labeling war" requiring deep involvement from both regulatory bodies and tech companies worldwide.Facing escalating misinformation risks, platforms have implemented inconsistent and often inadequate strategies. As TIME magazine reported, only after they contacted Google about misleading videos generated by Veo 3 did Google, as the content creation platform, take reactive measures: adding a tiny, easily croppable, visible watermark to videos.\
On the other side, where the content is distributed and watched by billions of people, TikTok, the short video platform famous for viral content, has, since September 2023, required creators to actively label AI-generated content to avoid misleading audiences. But in reality, when scrolling through the TikTok feed today, one will find very limited coverage of this "AI-generated" label, with only a small portion of content displaying this label due to creators' voluntary disclosure.\
The reason behind this is simple: creators all want their content to appear as realistic as possible. Having a prominent label telling viewers "this is AI-generated" would presumably impact video performance heavily, so relying solely on creators' voluntary reporting isn’t adequate.\
Globally, regulatory requirements for AI content labeling are inconsistent. China and the EU take the lead in this space, respectively legislating a mandate that requires content platforms to implement metadata for machine and visible watermarks for humans to identify AI content. The U.S. hasn't legislated at the federal level, but states like California have taken the lead, with multiple bills in the works. Other regions, including Australia, ASEAN, and the Middle East, have yet to legislate, details see Table 1.\
With incomplete regulatory frameworks, social media platforms are proactively becoming de facto rule-makers. China's Douyin, for example, strictly enforces national mandatory requirements, requiring both standardized metadata and fixed-position visible watermarks for all AI-generated content. Meta’s Instagram and Facebook, Douyin’s sibling TikTok, YouTube, and Snapchat are also actively advancing dual strategies combining metadata (like C2PA standards) and visible labels (like "AI Info," "Imagined with AI"), but implementation varies. \
In contrast, some platforms like X (formerly Twitter) have been slower to respond, not yet introducing AI content labeling policies that promote information transparency. This inconsistent status across platforms reflects tech companies' attempts to balance user growth, commercial success, and content compliance in the AI race.Table 1: AI Content Labeling Requirements by Country/Region| Country/Region | Metadata Marking Requirements | Visible Watermark Requirements | Effective Date | Information Source |
|----|----|----|----|----|
|  | : All AI-generated content must include metadata for tracking, classification, and platform information. | : All publicly released AI-generated content must have clear (visible) watermarks. | September 1, 2025 | China Law Translate, Douyin Regulations |
|  | : Under the AI Act, AI-generated content must have machine-readable metadata (like digital watermarks, C2PA). | Not explicitly mandated, but future guidelines or industry practices may require. | August 2026 | EU AI Act \n Data Innovation |
|  |  (federal level); but multiple bills have proposed mandatory requirements, and some states (like California) have passed legislation. |  (federal level); but California law and proposed federal bills require visible watermarks. | California: January 1, 2026 | FPF, PBS |
|  | No legal requirement; best practice is using metadata and provenance standards (C2PA). | No legal requirement; visible labels are best practice but not mandatory. | - | Legal123 |
|  | ; regional guidelines support digital watermarks and encrypted provenance (C2PA). | Recommended as best practice, not legally mandatory. | - | ASEAN Guide |
|  | No specific AI legal requirements; ethical guidelines encourage transparency and data provenance. | No legal requirements; visible labels encouraged under ethical principles. | - | Thomson Reuters |However, despite the complexity of copyright, misinformation, and platform regulation, Veo 3's potential is also being explored by professional directors. While countless content creators chase the next "Bigfoot"-style viral moment, Turkish self-made director Öner S. Biberkökü chose a completely different path. His goal wasn't to create a quick viral moment that would spread on social media, but to use this new technology to tell a story that could deeply move hearts.\
Öner's collaborator is Turkey's household name, "Queen of Pop" Sezen Aksu, a legendary figure spanning five decades in music, who released a new song, Doğrucu. "I want people to remember their childhood," Öner mentioned in my interview, "I listened to her songs as a child, her voice has special meaning for Turkish people."\
This project cost tens of thousands of dollars in Veo3 credits alone, plus substantial human resources, completed by Öner's team (Pepperroot Studio) working around the clock. During creation, Öner and his team deeply explored Veo 3's technical limitations. He frankly admitted that achieving character consistency was a "nightmare" using this tool alone, forcing them to combine multiple generation methods through careful planning and repeated experimentation to finally achieve the desired outcome.\
This approach differs from another music video Öner created 4 months earlier, "Uchigatana" (which I often refer to as "Reborn as a Samurai in Edo"), which pursued spectacle moments, style transfer, and lip sync by an AI character that showcased AI’s capabilities. This new music video with Sezen Aksu deliberately maintained restraint and simplicity. "I wanted a simple music video with only one magical moment—the instant when sparrows lift the woman from a fall," Öner said. \
He prioritized emotional delivery over AI technical showmanship. Ultimately, this "magical moment" served the song's narrative, completing the work's concrete expression to aspire for "hope."The same creative tool, in different creators' hands, some people can produce comedic viral content, some people use it to fabricate misinformation, and some would wield its power to call for a nation's emotional response.Looking Forward: The AI RenaissanceIn addition to Google's Veo3, the AI video model field is highly competitive with numerous participants. There are platforms like Kling (by Kuaishou) and Dreamina (by ByteDance) that possess vast video training data from their short video platforms, as well as companies like Runway, Higgsfield, and Minimax, etc. continuously iterating products in their respective niche sub-fields. \
New technological innovations are announced almost every few days. Even Reddit already has rumors circulating that Veo 4 will be released in December 2025, though no one can accurately predict what capabilities these models will achieve by then.\
However, as Google CEO Sundar Pichai said in a podcast interview, the trend from these advanced models is clear—they will enable a dramatic increase in content creation, empowering a broader range of creators—democratizing video creation. He predicts: using AI video tools will become as commonplace as “using Google Docs is today."\
But I believe the core of content creation has never been, and should never be, determined by how advanced our tools are, but by the human spirit with which we wield these tools.\
The Veo 3 viral moment represents more than technological progress—it's a mirror reflecting the essence of human creativity. When a Syrian immigrant in Germany creates artistic expression with an AI Bigfoot, when a Turkish team produces emotionally complete stories at minimal cost, when the boundaries between real and synthetic blur, when you and I can use AI tools to manifest "What You Think Is What You See" we're not just witnessing a tool's evolution—we're observing the transformation across the entire content industry, what some call the AI Renaissance, an era where AI is accelerating the re-discovery and re-definition of humanity’s place in the world.\
This emerging AI Renaissance brings us back to our fundamental principles. The question we need to answer isn't whether AI will change content creation—because it already has. The question is whether we can establish a framework that embraces these tools' democratizing potential while protecting the value of human creativity.\
In that future of AI Renaissance, the stories we choose to tell—and how we tell them—will matter far more than the technology used to bring those stories to life.Öner S. Biberkökü and Cansın Çetin Kuşluvan (Co-founders of Turkish creative studio Pepperroot Studio). Video conference recordings with the author, June 2025. (AI content creator). Video conference recordings with the author, June 2025.II. Legal Documents and ReportsIII. News Reports and Analysis]]></content:encoded></item><item><title>Qantas hack results in theft of 6 million passengers’ personal data</title><link>https://techcrunch.com/2025/07/02/qantas-hack-results-in-theft-of-6-million-passengers-personal-data/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Wed, 2 Jul 2025 11:48:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Qantas, the largest airline in Australia, confirmed the theft of 6 million customers' personal information.]]></content:encoded></item><item><title>This Math Hack Could Let Miners Earn Extra on Blockchains</title><link>https://hackernoon.com/this-math-hack-could-let-miners-earn-extra-on-blockchains?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 2 Jul 2025 11:00:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We initiated the study of optimal selfish mining strategies for unpredictable blockchain protocols based on efficient proof systems. To this end, we considered a selfish mining objective corresponding to changes in chain quality and proposed a novel selfish mining attack that aims to maximize this objective. We formally modeled our attack as an MDP strategy and we presented a formal analysis procedure for computing an 𝜖-tight lower bound on the optimal expected relative revenue in the MDP and a strategy that achieves it for a specified precision 𝜖 > 0. The procedure is fully automated and provides formal guarantees on the correctness of the computed bound.\
We believe that our work opens several exciting lines for future research. We highlight two particular directions. First, our formal analysis only allows us to compute lower bounds on the expected relative revenue that an adversary can achieve. A natural direction of future research would be to consider computing upper bounds on the optimal expected relative revenue for fixed resource amounts. Second, as discussed in Section 3.4, our formal analysis only computes 𝜖-tight lower bounds on the expected relative revenue by following a strategy in our MDP model. However, our model in Section 3.2 introduces assumptions such as growing private forks instead of trees and bounding the maximal length of each fork for tractability purposes. It would be interesting to study whether these assumptions could be relaxed while still providing formal correctness guarantees.This work was supported in part by the ERC-2020-CoG 863818 (FoRM-SMArt) grant and the MOE-T2EP20122-0014 (Data-Driven Distributed Algorithms) grant.[1] 2022. https://ethereum.org/en/roadmap/merge/.\
[2] Adam Back. 1997. Hashcash. http://hashcash.org/.\
[3] Vivek Kumar Bagaria, Amir Dembo, Sreeram Kannan, Sewoong Oh, David Tse, Pramod Viswanath, Xuechao Wang, and Ofer Zeitouni. 2022. Proof-of-Stake Longest Chain Protocols: Security vs Predictability. In Proceedings of the 2022 ACM Workshop on Developments in Consensus, ConsensusDay 2022, Los Angeles, CA, USA, 7 November 2022, Jorge M. Soares, Dawn Song, and Marko Vukolic (Eds.). ACM, 29–42. https://doi.org/10.1145/3560829.3563559\
[4] Dan Boneh, Joseph Bonneau, Benedikt Bünz, and Ben Fisch. 2018. Verifiable Delay Functions. IACR Cryptol. ePrint Arch. (2018), 601. https://eprint.iacr.org/2018/601\
[5] Jonah Brown-Cohen, Arvind Narayanan, Christos-Alexandros Psomas, and S. Matthew Weinberg. 2018. Formal Barriers to Longest-Chain Proof-of-Stake Protocols. CoRR abs/1809.06528 (2018). arXiv:1809.06528 http://arxiv.org/abs/ 1809.06528\
[6] Jing Chen and Silvio Micali. 2019. Algorand: A secure and efficient distributed ledger. Theor. Comput. Sci. 777 (2019), 155–183.\
[7] Bram Cohen and Krzysztof Pietrzak. 2019. The chia network blockchain. https://docs.chia.net/assets/files/Precursor-ChiaGreenPaper82cb50060c575f3f71444a4b7430fb9d.pdf\
[8] Bram Cohen and Krzysztof Pietrzak. 2023. Chia Greenpaper. https://docs.chia. net/green-paper-abstract\
[9] Bernardo Machado David, Peter Gazi, Aggelos Kiayias, and Alexander Russell. 2017. Ouroboros Praos: An adaptively-secure, semi-synchronous proof-of-stake protocol. IACR Cryptol. ePrint Arch. (2017), 573. http://eprint.iacr.org/2017/573\
[10] Stefan Dziembowski, Sebastian Faust, Vladimir Kolmogorov, and Krzysztof Pietrzak. 2015. Proofs of Space. In Advances in Cryptology - CRYPTO 2015 - 35th Annual Cryptology Conference, Santa Barbara, CA, USA, August 16-20, 2015, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 9216), Rosario Gennaro and Matthew Robshaw (Eds.). Springer, 585–605. https://doi.org/10.1007/978-3- 662-48000-7_29\
[11] Ittay Eyal and Emin Gün Sirer. 2018. Majority is not enough: bitcoin mining is vulnerable. Commun. ACM 61, 7 (2018), 95–102. https://doi.org/10.1145/3212998\
[12] Lei Fan and Hong-Sheng Zhou. 2017. iChing: A Scalable Proof-of-Stake Blockchain in the Open Setting (or, How to Mimic Nakamoto’s Design via Proofof-Stake). IACR Cryptol. ePrint Arch. (2017), 656. http://eprint.iacr.org/2017/656\
[13] Matheus V. X. Ferreira, Ye Lin Sally Hahn, S. Matthew Weinberg, and Catherine Yu. 2022. Optimal Strategic Mining Against Cryptographic Self-Selection in Proof-of-Stake. In EC. ACM, 89–114.\
[14] Matheus V. X. Ferreira and S. Matthew Weinberg. 2021. Proof-of-Stake Mining Games with Perfect Randomness. In EC ’21: The 22nd ACM Conference on Economics and Computation, Budapest, Hungary, July 18-23, 2021, Péter Biró, Shuchi Chawla, and Federico Echenique (Eds.). ACM, 433–453. https: //doi.org/10.1145/3465456.3467636\
[15] Jerzy Filar and Koos Vrieze. 2012. Competitive Markov decision processes. Springer Science & Business Media.\
[16] Juan A. Garay, Aggelos Kiayias, and Nikos Leonardos. 2015. The Bitcoin Backbone Protocol: Analysis and Applications. In EUROCRYPT (2) (Lecture Notes in Computer Science, Vol. 9057). Springer, 281–310.\
[17] Peter Gazi, Aggelos Kiayias, and Alexander Russell. 2020. Tight Consistency Bounds for Bitcoin. In CCS. ACM, 819–838.\
[18] Christian Hensel, Sebastian Junges, Joost-Pieter Katoen, Tim Quatmann, and Matthias Volk. 2022. The probabilistic model checker Storm. Int. J. Softw. Tools Technol. Transf. 24, 4 (2022), 589–610. https://doi.org/10.1007/s10009-021-00633-z\
[19] Charlie Hou, Mingxun Zhou, Yan Ji, Phil Daian, Florian Tramèr, Giulia Fanti, and Ari Juels. 2021. SquirRL: Automating Attack Analysis on Blockchain Incentive Mechanisms with Deep Reinforcement Learning. In NDSS. The Internet Society.\
[20] Marta Z. Kwiatkowska, Gethin Norman, and David Parker. 2011. PRISM 4.0: Verification of Probabilistic Real-Time Systems. In CAV (Lecture Notes in Computer Science, Vol. 6806). Springer, 585–591.\
[21] Satoshi Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system. https: //bitcoin.org/bitcoin.pdf.\
[22] James R Norris. 1998. Markov chains. Number 2. Cambridge university press.\
[23] Sunoo Park, Albert Kwon, Georg Fuchsbauer, Peter Gazi, Joël Alwen, and Krzysztof Pietrzak. 2018. SpaceMint: A Cryptocurrency Based on Proofs of Space. In Financial Cryptography (Lecture Notes in Computer Science, Vol. 10957). Springer, 480–499.\
[24] Rafael Pass, Lior Seeman, and Abhi Shelat. 2017. Analysis of the Blockchain Protocol in Asynchronous Networks. In EUROCRYPT (2) (Lecture Notes in Computer Science, Vol. 10211). 643–673.\
[25] Krzysztof Pietrzak. 2019. Simple Verifiable Delay Functions. In ITCS (LIPIcs, Vol. 124). Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 60:1–60:15.\
[26] Martin L. Puterman. 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley.\
[27] Ayelet Sapirshtein, Yonatan Sompolinsky, and Aviv Zohar. 2016. Optimal Selfish Mining Strategies in Bitcoin. In Financial Cryptography (Lecture Notes in Computer Science, Vol. 9603). Springer, 515–532.\
[28] Roozbeh Sarenche, Svetla Nikova, and Bart Preneel. 2024. Deep Selfish Proposing in Longest-Chain Proof-of-Stake Protocols. In Financial Cryptography and Data Security.\
[29] Xuechao Wang, Govinda M. Kamath, Vivek Kumar Bagaria, Sreeram Kannan, Sewoong Oh, David Tse, and Pramod Viswanath. 2019. Proof-of-Stake Longest Chain Protocols Revisited. CoRR abs/1910.02218 (2019).\
[30] Benjamin Wesolowski. 2018. Efficient verifiable delay functions. IACR Cryptol. ePrint Arch. (2018), 623. https://eprint.iacr.org/2018/623\
[31] Roi Bar Zur, Ittay Eyal, and Aviv Tamar. 2020. Efficient MDP Analysis for SelfishMining in Blockchains. In AFT. ACM, 113–131.(1) Krishnendu Chatterjee, IST Austria, Austria (krishnendu.chatterjee@ist.ac.at);(2) Amirali Ebrahimzadeh, Sharif University of Technology, Iran (ebrahimzadeh.amirali@gmail.com);(3) Mehrdad Karrabi, IST Austria, Austria (mehrdad.karrabi@ist.ac.at);(4) Krzysztof Pietrzak, IST Austria, Austria (krzysztof.pietrzak@ist.ac.at);(5) Michelle Yeo, National University of Singapore, Singapore (mxyeo@nus.edu.sg);(6) Ðorđe Žikelić, Singapore Management University, Singapore (dzikelic@smu.edu.sg).]]></content:encoded></item><item><title>GNOME Papers Document Viewer Approved To Replace Evince In GNOME 49</title><link>https://www.phoronix.com/news/GNOME-Papers-Approved-49</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:33:30 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[GNOME Papers has been in development as a modern GTK4-based document viewer. There have been many improvements made to Papers and now ahead of the GNOME 49 release in September, it's been approved to replace Evince as the official document viewer of the GNOME desktop...]]></content:encoded></item><item><title>Towards 21st Century Equitable Prosperity: From Reaganomics to the Abundance Agenda</title><link>https://hackernoon.com/towards-21st-century-equitable-prosperity-from-reaganomics-to-the-abundance-agenda?source=rss</link><author>Ralph Benko and Jeff Garzik</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:30:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The transformation from the Industrial Revolution to the Innovation Revolution.  Good energy policy is key.]]></content:encoded></item><item><title>Mesa&apos;s Zink Preps NV_timeline_semaphore For Better OpenGL-Vulkan Interoperability</title><link>https://www.phoronix.com/news/Zink-NV-Timeline-Semaphore</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:20:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Mike Blumenkrantz with Valve's Linux graphics driver team continues working on enhancements to Mesa's Zink driver for OpenGL implemented over the Vulkan API. A new merge request is further enhancing OpenGL and Vulkan interoperability by supporting the GL_NV_timeline_semaphore extension...]]></content:encoded></item><item><title>Linux Patches Posted For Axiado AX3000 SoC Support</title><link>https://www.phoronix.com/news/Axiado-AX3000-Linux-Patches</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:07:06 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The newest Arm SoC seeing Linux kernel patches working their way toward the mainline kernel is the Axiado AX3000 as a security processor designed for cloud data center, network gear, and more...]]></content:encoded></item><item><title>New Research Shows Classic Selfish Mining Is Outdated</title><link>https://hackernoon.com/new-research-shows-classic-selfish-mining-is-outdated?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:00:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[4 EXPERIMENTAL EVALUATIONWe implement the MDP model and the formal analysis procedure presented in Section 3 and perform an experimental evaluation towards answering the following research questions (RQs):\
 What is the expected relative revenue that our selfish mining strategy achieves? How does it compare to direct extensions of classic selfish mining attacks on PoW blockchains [11] or to mining honestly?\
 How do different values of the System Model parameters impact the expected relative revenue that our selfish mining attack can achieve? The System Model parameters include the relative resource of the adversary 𝑝 ∈ [0, 1] and the switching probability 𝛾 ∈ [0, 1].\
. To answer these RQs, we compare our selfish mining attack against two baselines:\
(1) . This is the strategy extending only the leading block of the main chain.\
(2) Single-tree selfish mining attack. This is the attack strategy that exactly follows the classic selfish mining attack in Bitcoin proposed in [11], however it grows a private tree fork rather than a private chain. Analogously as in [11], the adversary publishes the private tree whenever the length of the main chain catches up with the depth of the private tree. We omit the formal model of this baseline due to space limitations. We also use this baseline to empirically evaluate how severe is the second limitation discussed in Section 3.4.\
 We perform an experimental comparison of our attack and the two baselines for the values of the adversarial relative resource 𝑝 ∈ [0, 0.3] (in increments of 0.01) and the switching probability 𝛾 ∈ {0, 0.25, 0.5, 0.75, 1}. As for the parameters of each selfish mining attack:\
• For our selfish mining attack, we set the maximal length of private forks 𝑙 = 4 and consider all combinations (𝑑, 𝑓 ) ∈ {(1, 1), (2, 1), (2, 2), (3, 2), (4, 2)} of the values of the attack depth 𝑑 and the forking number 𝑓 .\
• For the single-tree selfish mining attack baseline, we set the maximal depth of the private tree 𝑙 = 4 to match our maximal private fork length, and the maximal width of the private tree 𝑓 = 5.\
All experiments were run on Ubuntu 20, 2.80GHz 11th Gen Intel(R) Core(TM) i7-1165G7 CPU, 16 GB RAM, and 16 GB SWAP SSD Memory. For solving mean-payoff MDPs, we use the probabilistic model checker Storm [18], a popular MDP analysis tool within the formal methods community[2].\
. Table 1 shows the runtimes of both our selfish mining attack as well as the single-tree selfish mining attack given various parameter settings and for a fixed switching parameter of 𝛾 = 0.5. We only show timings for 𝛾 = 0.5 as we found the runtimes of our experiments to be very similar across all 𝛾 parameter settings. As can be seen from Table 1, increasing the depth of the attack increases the runtime of our evaluation by an order of magnitude due to the exponential increase in state space.\
Experimental results are shown in Figure 2, showing plots for each 𝛾 ∈ {0, 0.25, 0.5, 0.75, 1}. As we can see from the plots, our selfish mining attack consistently achieves higher expected relative revenue ERRev than both baselines for each value of 𝛾, except when 𝑑 = 1 and 𝑓 = 1. Indeed, already for 𝑑 = 2 and 𝑓 = 1 when the adversary grows a single private fork on the first two blocks in the main chain, our attack achieves higher ERRev than both baselines. This shows that growing private forks at two different blocks already provides a more powerful attack than growing a much larger private tree at a single block. Hence, our results indicate that growing disjoint private forks rather than trees is not a significant limitation, justifying our choice to grow private forks towards making the analysis computationally tractable.\
The attained ERRev grows significantly as we increase 𝑑 and 𝑓 and allow the adversary to grow more private forks. In particular, for 𝑑 = 4, 𝑓 = 2, and relative adversarial resource 𝑝 = 0.3, our attack achieves ERRev that is larger by at least 0.2 than that of both baselines, for all values of the switching probability𝛾. This indicates a  of selfish mining attacks in efficient proof systems blockchains compared to PoW, as the ability to simultaneously grow multiple private forks on multiple blocks translates to a much larger ERRev. Our results suggest that further study of techniques to reduce the advantage of the adversary when mining on several blocks is important in order to maintain reasonable chain quality for efficient proof systems blockchains.\
Finally, we notice that larger 𝛾 values correspond to larger ERRev in our strategies. This is expected, as larger 𝛾 values introduce bias in the likelihood of the adversarial chain becoming the main chain. This is most pertinently observed in the case of 𝑑 = 𝑓 = 1: since 𝑑 = 𝑓 = 1 corresponds to a strategy that only mines a private block on the leading block in the main chain, the only way to deviate from honest mining is to withhold a freshly mined block and reveal it together with the occurrence of a freshly mined honest block. As we can see in the plots, for 𝛾 < 0.5 the achieved ERRev of the strategy with 𝑑 = 𝑓 = 1 corresponds to that of honest mining and the two lines in plots mostly overlap, whereas this strategy only starts to pay off for 𝛾 > 0.5 and for the proportion of resource 𝑝 > 0.25. Altogether, this suggests that further and careful analysis of the control of the adversary over the broadcast network as well as the fork choice breaking rule is necessary.\
 The key takeaways of our experimental evaluation are as follows:\
• Our selfish mining attack achieves  ERRev than both baselines, reaching up to 0.2 difference in ERRev. Thus, our results strongly suggest that growing private forks at multiple blocks is much more advantageous than growing all forks on the first block in the main chain.\
• Our results suggest that growing private trees rather than disjoint private forks would not lead to a significant improvement in the adversary’s ERRev. Hence, the second limitation of our attack discussed in Section 3.4 does not seem to be significant.\
• Our results suggest that enhancing security against selfish mining attacks in efficient proof system blockchains requires further and careful analysis of the control that the adversary has over the broadcast system. In particular, for large values of the switching probability 𝛾, even the simplest variant of our attack with 𝑑 = 1 and 𝑓 = 1 starts to pay off when 𝑝 > 0.25.(1) Krishnendu Chatterjee, IST Austria, Austria (krishnendu.chatterjee@ist.ac.at);(2) Amirali Ebrahimzadeh, Sharif University of Technology, Iran (ebrahimzadeh.amirali@gmail.com);(3) Mehrdad Karrabi, IST Austria, Austria (mehrdad.karrabi@ist.ac.at);(4) Krzysztof Pietrzak, IST Austria, Austria (krzysztof.pietrzak@ist.ac.at);(5) Michelle Yeo, National University of Singapore, Singapore (mxyeo@nus.edu.sg);(6) Ðorđe Žikelić, Singapore Management University, Singapore (dzikelic@smu.edu.sg).[2]   Refer to our github repository for our implementation details: https://github.com/mehrdad76/Automated-Selfish-Mining-Analysis-in-EPSBlockchains]]></content:encoded></item><item><title>China Successfully Tests Hypersonic Aircraft, Maybe At Mach 12</title><link>https://tech.slashdot.org/story/25/07/01/2222223/china-successfully-tests-hypersonic-aircraft-maybe-at-mach-12?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China's Northwestern Polytechnical University successfully tested a hypersonic aircraft called Feitian-2, claiming it reached Mach 12 and achieved a world-first by autonomously switching between rocket and ramjet propulsion mid-flight. The Register reports: The University named the craft "Feitian-2" and according to Chinese media the test flight saw it reach Mach 12 (14,800 km/h or 9,200 mph) -- handily faster than the Mach 5 speeds considered to represent hypersonic flight. Chinese media have not detailed the size of Feitian-2, or its capabilities other than to repeat the University's claim that it combined a rocket and a ramjet into a single unit. [...] The University and Chinese media claim the Feitian-2 flew autonomously while changing from rocket to ramjet while handling the hellish stresses that come with high speed flight.
 
This test matters because, as the US Congressional Budget Office found in 2023, hypothetical hypersonic missiles "have the potential to create uncertainty about what their ultimate target is. Their low flight profile puts them below the horizon for long-range radar and makes them difficult to track, and their ability to maneuver while gliding makes their path unpredictable." "Hypersonic weapons can also maneuver unpredictably at high speeds to counter short-range defenses near a target, making it harder to track and intercept them," the Office found.
 
Washington is so worried about Beijing developing hypersonic weapons that the Trump administration cited the possibility as one reason for banning another 27 Chinese organizations from doing business with US suppliers of AI and advanced computing tech. The flight of Feitian-2 was therefore a further demonstration of China's ability to develop advanced technologies despite US bans.]]></content:encoded></item><item><title>Published Fiction at Center of Fair Use Dispute in Anthropic AI Training Lawsuit</title><link>https://hackernoon.com/published-fiction-at-center-of-fair-use-dispute-in-anthropic-ai-training-lawsuit?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Wed, 2 Jul 2025 09:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 7 of 10. 2. THE NATURE OF THE COPYRIGHTED WORKThe second fair use factor is “the nature of the copyrighted work.” 17 U.S.C. § 107(2). This factor “calls for recognition that some works are closer to the core of intended copyright protection than others, with the consequence that fair use is more difficult to establish when the former works are copied.” Campbell, 510 U.S. at 586. For one thing, less protection is due published works than unpublished ones. For another, less protection is due “factual works than works of fiction or fantasy.” Harper & Row, 471 U.S. at 563. But less protection is not no protection. Even the arrangement of otherwise unprotectable facts surpasses the low bar for a protectable original work of authorship. Google, 804 F.3d at 220. Here, Anthropic accepts that all of Authors’ books — all published, whether non-fiction or fiction — contained expressive elements (Reply 9). And, as set out above, this order accepts Authors’ view of the evidence that their works were chosen for their expressive qualities in building a central library and then in training specific LLMs (Opp. 11, 17 (citing, e.g., Opp. Exh. 3 at -03433)). The main function of the second factor is to help assess the other factors: to reveal differences between the nature of the works at issue and the nature of their secondary use (above), and to reveal any relation between the amount and substantiality of each work taken and the secondary use (next). E.g., Campbell, 510 U.S. at 586; Kelly, 336 F.3d at 820; Google, 804 F.3d at 220; HathiTrust, 755 F.3d at 98; Bill Graham Archives v. Dorling Kindersley Ltd., 448 F.3d 605, 612–13 (2d Cir. 2006). The second factor points against fair use for all copies alike.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>The Limits of Automated Selfish Mining Detection</title><link>https://hackernoon.com/the-limits-of-automated-selfish-mining-detection?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 2 Jul 2025 09:00:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.4 Key Features and Limitations. The key features of our selfish mining attack and formal analysis are as follows:\
(1) . Manual (i.e. non-automated) analysis of optimal selfish mining attacks is already challenging and technically involved for PoW blockchains, where the adversary only grows a single private fork [11]. Hence, it would be even more difficult and potentially intractable in blockchains based on efficient proof systems. By modelling our selfish mining attack as an MDP and reducing the analysis to solving mean-payoff MDPs, we leverage existing methods for formal analysis of MDPs to obtain a  procedure, thus avoiding the necessity for tedious manual analyses.\
(2) Formal guarantees on correctness. Our analysis provides formal guarantees on the correctness of its output. Again, this is achieved by formally reducing our problem to solving mean-payoff MDPs for which exact algorithms with formal correctness guarantees are available [18, 20].\
(3) Flexibility of the analysis. Our analysis is agnostic to the values of system model and attack parameters and it is flexible to their changes. Hence, it allows us to tweak the parameter values and study their impact on the optimal expected relative revenue, while preserving formal guarantees on the correctness. To illustrate the flexibility, observe that:\
• If the attack depth 𝑑, forking number 𝑓 or maximal fork length 𝑙 of the attack change, then both the state space and the action space of the MDP change.\
• If the relative resource of the adversary 𝑝 or the switching probability 𝛾 change, then the transition function of the MDP changes.\
• As we show in our experiments in Section 4, a change in any of these parameter values results in a change in the optimal expected relative revenue that the adversary can achieve.\
The flexibility of our analysis is thus a significant feature, since it again avoids the need for tedious manual analyses for different parameter values that give rise to different MDPs.\
. While our formal analysis computes an optimal selfish mining strategy in the MDP up to a desired precision, note that there still exist selfish mining attacks that do not correspond to any strategy in our MDP model. Hence, the strategy computed by our method is optimal only with respect to the  of strategies captured by the MDP model. There are two key reasons behind the incompleteness of our MDP model:\
(1) . In order to ensure finiteness of our MDP model, we impose an upper bound 𝑙 on the maximal length of each private fork. This means that the adversary cannot grow arbitrarily long private forks. Since the probability of the adversary being able to grow extremely long private forks is low, we believe that this limitation does not significantly impact the expected relative revenue of selfish mining strategy under this restriction.\
(2) Disjoint forks vs fork trees. Our attack grows private forks on different blocks in the main chain. However, rather than growing multiple disjoint private forks, a more general class of selfish mining attacks would be to allow growing . We stick to disjoint private forks in order to preserve  efficiency of our analysis, since allowing the adversary to grow private trees would result in our MDP states needing to store information about each private tree topology, which would lead to a huge blow-up in the size of the MDP. In contrast, storing disjoint private forks only requires storing fork lengths, resulting in smaller MDP models.\
We conclude by noting that, while our formal analysis is incomplete due to considering a subclass of selfish mining attacks, the formal guarantees provided by our analysis still ensure that we compute a  on the expected relative revenue that a selfish mining attack achieves.(1) Krishnendu Chatterjee, IST Austria, Austria (krishnendu.chatterjee@ist.ac.at);(2) Amirali Ebrahimzadeh, Sharif University of Technology, Iran (ebrahimzadeh.amirali@gmail.com);(3) Mehrdad Karrabi, IST Austria, Austria (mehrdad.karrabi@ist.ac.at);(4) Krzysztof Pietrzak, IST Austria, Austria (krzysztof.pietrzak@ist.ac.at);(5) Michelle Yeo, National University of Singapore, Singapore (mxyeo@nus.edu.sg);(6) Ðorđe Žikelić, Singapore Management University, Singapore (dzikelic@smu.edu.sg).]]></content:encoded></item><item><title>Anthropic Accused of Building Claude AI with 7 Million Pirated Books</title><link>https://hackernoon.com/anthropic-accused-of-building-claude-ai-with-7-million-pirated-books?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Wed, 2 Jul 2025 08:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 6 of 10. (ii) The Pirated Library CopiesBefore buying books for its central library, Anthropic downloaded over seven million pirated copies of books, paid nothing, and kept these pirated copies in its library even after deciding it would not use them to train its AI (at all or ever again). Authors argue Anthropic should have paid for these pirated library copies (e.g., Tr. 24–25, 65; Opp. 7, 12–13). This order agrees. The basic problem here was well-stated by Anthropic at oral argument: “You can’t just bless yourself by saying I have a research purpose and, therefore, go and take any textbook you want. That would destroy the academic publishing market if that were the case” (Tr. 53). Of course, the person who purchases the textbook owes no further accounting for keeping the copy. But the person who copies the textbook from a pirate site has infringed already, full stop. This order further rejects Anthropic’s assumption that the use of the copies for a central library can be excused as fair use merely because some will eventually be used to train LLMs. This order doubts that any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use. There is no decision holding or requiring that pirating a book that could have been bought at a bookstore was reasonably necessary to writing a book review, conducting research on facts in the book, or creating an LLM. Such piracy of otherwise available copies is inherently, irredeemably infringing even if the pirated copies are immediately used for the transformative use and immediately discarded. But this order need not decide this case on that rule. Anthropic did not use these copies only for training its LLM. Indeed, it retained pirated copies even after deciding it would not use them or copies from them for training its LLMs ever again. They were acquired and retained, as a central library of all the books in the world. Building a central library of works to be available for any number of further uses was itself the use for which Anthropic acquired these copies. One further use was making further copies for training LLMs. But not every book Anthropic pirated was used to train LLMs. And, every pirated library copy was retained even if it was determined it would not be so used. Pirating copies to build a research library without paying for it, and to retain copies should they prove useful for one thing or another, was its own use — and not a transformative one (see Tr. 24–25, 35, 65; Opp. 4–10, 12 n.6; CC Br. Exh. 12 at -0144509 (“everything forever”)). Napster, 239 F.3d at 1015; BMG Music v. Gonzalez, 430 F.3d 888, 890 (7th Cir. 2005).Anthropic’s briefing contains other reasons why it believes its pirated library copies are irrelevant to our fair use analysis, notwithstanding its own statements at our oral argument. First, Anthropic accepts in this posture that it acted in bad faith but argues that its bad faith in pirating copies cannot “somehow short-circuit[ ]” the fair use analysis (Reply 6 (downplaying Atari Games Corp. v. Nintendo of Am., Inc., 975 F.2d 832, 843 (Fed. Cir. 1992) (applying law of Ninth Circuit))). But its bad faith is not the basis for this decision. Each use of a work must be analyzed objectively. Warhol, 598 U.S. at 544–45. The objective analysis here shows the initial copies were pirated to create a central, general-purpose library, as a substitute for paid copies to do the same thing. (Of course, if infringement is found, bad faith would matter for determining willfulness. 17 U.S.C. § 504(c)(2).) Second, Anthropic argues that its goal to put the copies eventually “to a highly transformative use” requires that each copy and use along the way be justified as having a transformative use, too (Reply 14). But now Anthropic seeks to take the shortcut Anthropic just said cannot be taken. Again, the Supreme Court tasks us with looking past the “subjective intent of the user” to the objective use made of each copy. Warhol, 598 U.S. at 544–45 (emphasis added). Put another way, what a copyist says or thinks or feels matters only to the extent it shows what a copyist in fact does with the work. Indeed, the same copy can be used one way, then another, each with a different result. Id. at 533. Here, what Anthropic said about its acquisitions at the time — that they were made to “build[ ] a research library” while avoiding a “huge legal/practice/business slog” — are relevant in this regard. And, Anthropic’s actual use of these pirated copies was to create its central library of texts that, like any university or corporate library, stored the works’ well-organized facts, analyses, and expressive examples for various contingent uses, one being training. (5).Third, Anthropic argues that Texaco — the case involving copies used in a central library, copies used in desk libraries, and copies used in the laboratory — is inapposite. Anthropic argues that the disputed copies in Texaco were never used in the laboratory but instead in personal desk libraries for a use “identical to the original purpose and use” of the central library copies, and so not for a transformative use (Reply 8 (summarizing 60 F.3d at 922–23)). By contrast, says Anthropic, here it did use copies in the laboratory to train LLMs — a very transformative use. But this is a fast glide over thin ice. Like Texaco, Anthropic possessed copies it did not put into use in the laboratory and it kept those copies in a central library even after its transformative use had been completed. But, unlike Texaco, which bought those copies, Anthropic never paid for the central library copies stolen off the internet. Texaco also shows why Anthropic is wrong to suppose that so long as you create an exciting end product, every “back-end step, invisible to the public,” is excused (Br. 10). Notably, this is not a case where source copies were unavailable for separate purchase or loan. See, e.g., NXIVM Corp. v. Ross Inst., 364 F.3d 471, 475–76, 478–79 (2d Cir. 2004) (using selections of training manual — otherwise available only to cult’s trainees subject to NDAs — to expose cult in critical review); Time Inc. v. Bernard Geis Assocs., 293 F. Supp. 130, 135–36, 138, 146 (S.D.N.Y. 1968) (Judge Inzer Bass Wyatt) (making charcoal drawings of photographs taken of originals otherwise not on sale or loan out to illustrate a history book). (6). Nor were the copies made only incidentally and necessarily from pirated copies. See, e.g., Perfect 10, 508 F.3d at 1164 n.8 (copies of images that had been pirated by third-party websites were used to index those same websites while indexing the entire web). Here, piracy was the point: To build a central library that one could have paid for, just as Anthropic later did, but without paying for it. Nor were the initial copies made immediately transformed into a significantly altered form. In Perfect 10, images were copied by the search engine in thumbnail form only and deployed immediately into the transformative use of identifying the full-sized images and the pages from which they came. 508 F.3d at 1160, 1165, 1167. And, in Kelly v. Arriba Software Corp., images were copied at full size and then into thumbnails for immediate use in building a search engine, after which the full-sized copies were immediately deleted. 336 F.3d 811, 815 (9th Cir. 2003). Not here. The full-text copies of books were downloaded and maintained “forever.” Nor does the initial copying here even resemble the full-text copying in the Google Books cases. There, libraries of authorized copies already had been assembled, and all copies therefrom were made for direct employment in a one-to-one further fair use — whether the transformative use of pointing to the works themselves, the use of providing the works in formats for print-disabled patrons, or the use of insuring against going out of print, getting lost, and becoming otherwise unavailable. HathiTrust, 755 F.3d at 97, 101, 103; Google, 804 F.3d at 206, 216–18, 228 (further distinguishing search and snippet uses, which “test[ed] the boundaries of fair use”). Not so here concerning the pirated copies. No authorized copies existed from which Anthropic made its first copies. No full-text copy therefrom was put immediately into use training LLMs. Not every copy was even necessary nor used for training LLMs. No initial copy was ever deleted, even if never used or no longer used. (7)  The university libraries and Google went to exceedingly great lengths to ensure that all copies were secured against unauthorized uses — both through technical measures and through legal agreements among all participants. Not so here. The library copies lacked internal controls limiting access and use. Nor do the decisions on intermediate copying require anything less than the analysis applied here. Anthropic argues that our court of appeals in Sega Enterprises Ltd. v. Accolade, Inc. looked only at the “ultimate use” and “did not analyze a series of atomized acts of ‘infringement’ distinct from that overall purpose” (Reply 3). To the contrary, the appeals court examined the initial, intermediate, and ultimate copies used by the copyist. The court explained that the copyist initially purchased commercially available copies of game cartridges and then made further copies necessarily and “solely in order to discover the functional requirements for compatibility.” 977 F.2d 1510, 1522 (9th Cir. 1992). Thus, it reached only one result because on those facts there was only one “overall purpose” for the unauthorized copies. Indeed, the court reaffirmed prior caselaw holding that “intermediate copying of [a work] may infringe the exclusive rights granted to the copyright owner in [S]ection 106 of the Copyright Act regardless of whether the end product of the copying also infringes those rights.” Id. at 1518–19 (reaffirming Walker v. Univ. Books, 602 F.2d 859, 864 (9th Cir. 1979)). Similarly, in Sony Computer Entertainment, Inc. v. Connectix Corp., our appeals court applied the same law to similarly focused conduct. Another copyist allegedly had purchased an authorized copy and then made further copies solely and necessarily to reverse-engineer compatibility requirements. 203 F.3d 596, 601, 602–03 (9th Cir. 2000). Both Sega and Sony avoided imposing an “artificial hurdle” to fair use by generously construing the intermediate copying necessary to the fair use. As one example, Sega stated that an engineer should be permitted to reboot her computer while undertaking to reverseengineer software loaded onto it — even if doing so creates another digital copy of the software and is not strictly necessary to reverse-engineering. Id. at 605. But neither Sega nor Sony fathomed gifting an “artificial head start” to a fair user, either, by treating even the initial copy as an intermediate one. And, yes, some courts have “not inquire[d]” into intermediate or initial copying at all (Reply 2 (citing Campbell as not inquiring into surplus copies in the studio)). But if a “close reading of those cases [ ] reveals that in none of them was the legality of the [initial or] intermediate copying at issue,” then it was not raised and not necessarily decided. Sega, 977 F.2d at 1519; see Webster v. Fall, 266 U.S. 507, 511 (1925). It was expressly decided elsewhere: Our analysis must attend to different uses of different copies, and even to different uses of the same copies. Warhol, 598 U.S. at 533. Finally, Anthropic argues that even if the initial copies served a different use than the intermediate and ultimate copies, it was not a use for which Anthropic necessarily would have needed to pay Authors for a copy. In theory, argues Anthropic, it could have done as Google did in Google Books — find an existing reference library willing to loan its copies for free as source copies. Or, in theory, it could have done as Anthropic did later — go buy used copies without having to pay Authors at all. See 17 U.S.C. § 109(a). But Anthropic did not do those things — instead it stole the works for its central library by downloading them from pirated libraries. In sum, the first factor points against fair use for the central library copies made from pirated sources — and no damages from pirating copies could be undone by later paying for copies of the same works.(5) Our court of appeals has not yet reappraised how bad faith (or good faith) figures in fair use after Warhol. Its prior appraisal applied the Supreme Court’s statement that “[f]air use presupposes good faith and fair dealing,” Harper & Row, 471 U.S. at 562 (cleaned up). See Perfect 10, 508 F.3d at1164 n.8. Since then, the Supreme Court has renewed its “skepticism about whether bad faith has any role.” Oracle, 593 U.S. at 32–33 (reiterating doubts of Campbell, 510 U.S. at 585 n.18). And, recently, the Supreme Court has held squarely that it is not the “subjective intent” of a copyist that counts, but the “objective . . . use” of the copy. Warhol, 598 U.S. at 544– 45. This order applies this most recent analysis. Miller v. Gammie, 335 F.3d 889, 900 (9th Cir. 2003) (en banc).(6) Anthropic repeats the misleading characterization of the copyright holder in Oracle that the initial copies were there purloined (Reply 5). Not so. “All agree[d] that Google was and remain[ed] free to use the Java language itself. All agree[d] that Google’s virtual machine [wa]s free of any copyright issues. All agree[d] that the six-thousand-plus method implementations by Google [we]re free of copyright issues. The copyright issue, rather,” was the use of Java for purposes of creating competing software having the same familiar, functional schema. Oracle Am., Inc. v. Google Inc., 872 F. Supp. 2d 974, 978 (N.D. Cal. 2012), aff’d and rev’d in part, 750 F.3d 1339 (Fed. Cir. 2014).(7) Training LLMs was not a use where perpetually maintaining a library copy was intrinsic to the proffered fair use (e.g., for a plagiarism-checker service). Nor is this an instance where retaining at least one copy was authorized by contract with the copyright owners (e.g., by agreement to express terms upon submission to a plagiarism-checker service, notwithstanding proposed terms scrawled on a paper prior to submission). A.V. ex rel. Vanderhye v. iParadigms, LLC, 562 F.3d 630, 635–36 & n.5, 645 n.8 (4th Cir. 2009), aff’g in relevant parts 544 F. Supp. 2d 473, 480 (E.D. Va. 2008) (Judge Claude Hilton). Anthropic mischaracterizes this case.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>How Confidential Are Your Blockchain Transactions, Really?</title><link>https://hackernoon.com/how-confidential-are-your-blockchain-transactions-really?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Wed, 2 Jul 2025 08:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[APPENDIX A. KEY MANAGEMENTA variety of different keys are used in the life cycle of TCSC. For simplicity, we use Intel SGX as the instance. We classify these keys into two types, namely, service keys (top half) and SGX internal keys (bottom half).\
 The MEE key is generated at boot, and is placed in special registers, and destroyed at system reset. The MEE key is used for memory encryption and decryption, which plays a crucial role in protecting the confidentiality and integrity of enclaves. At the same time, different enclaves in the same TEE platform share one function key, such as the report key and the attestation key [88].APPENDIX B. ANONYMITY AND CONFIDENTIALITYAnonymity refers to the privacy that relates to real entities, especially for users’ identities. In a blockchain system, anonymity indicates that users’ transaction activities will not expose any personal information about them. Alternatively, an attack cannot obtain the correct links between real users and their corresponding account/address that sends the transaction [98]. Bitcoin and Ethereum only provide a very early version of anonymity, using the pseudonym-based address mechanism to protect identities. However, this cannot guarantee anonymity because attackers can effortlessly map virtual addresses to physical entities through the relationship analysis.\
Confidentiality in a blockchain system mainly refers to the privacy of data and contents recorded on-chain [99], [9]. Classic blockchain systems expose all transactions (includes amount information, addresses, amount, etc.) plainly where anyone can read and access. Sensitive information might unconsciously be leaked to malicious analyzers. For instance, ERC20 tokens in the Ethereum system do not provide confidentiality, since anyone can observe every amount’s balance. Adversaries can keep tracing the accounts that have a huge amount of tokens and launch attacks such as using the phishing website or cheating through offline activities.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Podcast Episode: Cryptography Makes a Post-Quantum Leap</title><link>https://www.eff.org/deeplinks/2025/06/podcast-episode-cryptography-makes-post-quantum-leap</link><author>Josh Richman</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/2025-htfi-deirdre-blog.png" length="" type=""/><pubDate>Wed, 2 Jul 2025 07:05:17 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[ I only got into cryptography and especially post quantum quickly after that. further into my professional life. I was a software engineer for a whil,e and the Snowden leaks happened, and phone records get leaked. All of Verizon's phone records get leaked. and then Prism and more leaks and more leaks. And as an engineer first, I felt like everything that I was building and we were building and telling people to use was vulnerable. I wanted to learn more about how to do things securely. I went further and further and further down the rabbit hole of cryptography. And then, I think I saw a talk which was basically like, oh, elliptic curves are vulnerable to a quantum attack. And I was like, well, I, I really like these things. They're very elegant mathematical objects, it's very beautiful. I was sad that they were fundamentally broken, and, I think it was, Dan Bernstein who was like, well, there's this new thing that uses elliptic curves, but is supposed to be post quantum secure. But the math is very difficult and no one understands it. I was like, well, I want to understand it if it preserves my beautiful elliptic curves. That's how I just went, just running, screaming downhill into post quantum cryptography. That's Deirdre Connolly talking about how her love of beautiful math and her anger at the Snowden revelations about how the government was undermining security, led her to the world of post-quantum cryptography.I'm Cindy Cohn, the executive director of the Electronic Frontier Foundation. And I'm Jason Kelley, EFF's activism director. You're listening to How to Fix the Internet. On this show we talk to tech leaders, policy-makers, thinkers, artists and engineers about what the future could look like if we get things right online. Our guest today is at the forefront of the future of digital security. And just a heads up that this is one of the more technical episodes that we've recorded -- you'll hear quite a bit of cryptography jargon, so we've written up some of the terms that come up in the show notes, so take a look there if you hear a term you don't recognize. Deidre Connolly is a research engineer and applied cryptographer at Sandbox AQ, with a particular expertise in post-quantum encryption. She also co-hosts the Security, Cryptography, Whatever podcast, so she's something of a cryptography influencer too. When we asked our tech team here at EFF who we should be speaking with on this episode about quantum cryptography and quantum computers more generally, everyone agreed that Deirdre was the person. So we're very glad to have you here. Welcome, Deirdre. Thank you very much for having me. Hi. Now we obviously work with a lot of technologists here and, and certainly personally cryptography is near and dear to my heart, but we are not technologists, neither Jason nor I. So can you just give us a baseline of what post-quantum cryptography is and why people are talking about it? Sure. So a lot of the cryptography that we have deployed in the real world relies on a lot of math and security assumptions on that math based on things like abstract groups, Diffie-Hellman, elliptic curves, finite fields, and factoring prime numbers such as, uh, systems like RSA. All of these, constructions and problems, mathematical problems, have served us very well in the last 40-ish years of cryptography. They've let us build very useful, efficient, small cryptography that we've deployed in the real world. It turns out that they are all also vulnerable in the same way to advanced cryptographic attacks that are only possible and only efficient when run on a quantum computer, and this is a class of computation, a whole new class of computation versus digital computers, which is the main computing paradigm that we've been used to for the last 75 years plus. Quantum computers allow these new classes of attacks, especially, variants of Shore's algorithm – named Dr. Peter Shore – that basically when run on a sufficiently large, cryptographically relevant quantum computer, makes all of the asymmetric cryptography based on these problems that we've deployed very, very vulnerable. So post-quantum cryptography is trying to take that class of attack into consideration and building cryptography to both replace what we've already deployed and make it resilient to this kind of attack, and trying to see what else we can do with these fundamentally different mathematical and cryptographic assumptions when building cryptography. So we've kind of, we've secured our stuff behind a whole lot of walls, and we're slowly building a bulldozer. This is a particular piece of the world where the speed at which computers can do things has been part of our protection, and so we have to rethink that. Yeah, quantum computing is a fundamentally new paradigm of how we process data that promises to have very interesting, uh, and like, applications beyond what we can envision right now. Like things like protein folding, chemical analysis, nuclear simulation, and cryptanalysts, or very strong attacks against cryptography.But it is a field where it's such a fundamentally new computational paradigm that we don't even know what its applications fully would be yet, because like we didn't fully know what we were doing with digital computers in the forties and fifties. Like they were big calculators at one time. When it was suggested that we talk to you about this. I admit that I have not heard much about this field, and I realized quickly when looking into it that there's sort of a ton of hype around quantum computing and post-quantum cryptography and that kind of hype can make it hard to know whether or not something is like actually going to be a big thing or, whether this is something that's becoming like an investment cycle, like a lot of things do. And one of the things that quickly came up as an actual, like real danger is what's called sort of “save now decrypt later.” Oh yeah. Right? We have all these messages, for example, that have been encrypted with current encryption methods. And if someone holds onto those, they can decrypt them using quantum computers in the future. How serious is that danger? It’s definitely a concern and it's the number one driver I would say to post-quantum crypto adoption in broad industry right now is mitigating the threat of a Store Now/Decrypt Later attack, also known as Harvest Now/Decrypt Later, a bunch of names that all mean the same thing.And fundamentally, it's, uh, especially if you're doing any kind of key agreement over a public channel, and doing key agreement over a public channel is part of the whole purpose of like, you want to be able to talk to someone who you've never really, touched base with before, and you all kind of know, some public parameters that even your adversary knows and based on just the fact that you can send messages to each other and some public parameters, and some secret values that only you know, and only the other party knows you can establish a shared secret, and then you can start encrypting traffic between you to communicate. And this is what you do in your web browser when you have an HTTPS connection, that's over TLS.This is what you do with Signal or WhatsApp or any, or, you know, Facebook Messenger with the encrypted communications. They're using Diffie-Helman as part of the protocol to set up a shared secret, and then you use that to encrypt their message bodies that you're sending back and forth between you.But if you can just store all those communications over that public channel, and the adversary knows the public parameters 'cause they're freely published, that's part of Kerckhoff’s Principle about good cryptography - the only thing that the adversary shouldn't know about your crypto system is the secret key values that you're actually using. It should be secure against an adversary that knows everything that you know, except the secret key material. And you can just record all those public messages and all the public key exchange messages, and you just store them in a big database somewhere. And then when you have your large cryptographically relevant quantum computer, you can rifle through your files and say, hmm, let's point it at this.And that's the threat that's live now to the stuff that we have already deployed and the stuff that we're continuing to do communications on now that is protected by elliptic curve Diffie Hellman, or Finite Field Diffie Hellman, or RSA. They can just record that and just theoretically point an attack at it at a later date when that attack comes online. So like in TLS, there's a lot of browsers and servers and infrastructure providers that have updated to post-quantum resilient solutions for TLS. So they're using a combination of the classic elliptic curve, Diffie Hellman and a post-quantum KEM, uh, called ML Kem that was standardized by the United States based on a public design that's been, you know, a multi international collaboration to help do this design. I think that's been deployed in Chrome, and I think it's deployed by CloudFlare and it's getting deployed – I think it's now become the default option in the latest version of Open SSL. And a lot of other open source projects, so that's TLS similar, approaches are being adopted in open SSH, the most popular SSH implementation in the world. Signal, the service has updated their key exchange to also include a post quantum KEM and their updated key establishments. So when you start a new conversation with someone or reset a conversation with someone that is the latest version of Signal is now protected against that sort of attack. That is definitely happening and it's happening the most rapidly because of that Store now/Decrypt later attack, which is considered live. Everything that we're doing now can just be recorded and then later when the attack comes online, they can attack us retroactively. So that's definitely a big driver of things changing in the wild right now. Okay. I'm going to throw out two parallels for my very limited knowledge to make sure I understand. This reminds me a little bit of sort of the work that had to be done before Y2K in, in the sense of like, now people think nothing went wrong and nothing was ever gonna go wrong, but all of us working anywhere near the field know actually it took a ton of work to make sure that nothing blew up or stopped working. And the other is that in, I think it was 1998, EFF was involved in something we called Deep Crack, where we made, that's a, I'm realizing now that's a terrible name. But anyway, the DES cracker, um, we basically wanted to show that DES was capable of being cracked, right? And that this was a - correct me if I'm wrong - it was some sort of cryptographic standard that the government was using and people wanted to show that it wasn't sufficient. Yes - I think it was the first digital encryption standard. And then after its vulnerability was shown, they, they tripled it up to, to make it useful. And that's why Triple DES is still used in a lot of places and is actually considered okay. And then later came the advanced encryption standard, AES, which we prefer today. Okay, so we've learned the lesson, or we are learning the lesson, it sounds like. Yeah, I think that that's, that's right. I mean, EFF built the DES cracker because in the nineties the government was insisting that something that everybody knew was really, really insecure and was going to only get worse as computers got stronger and, and strong computers got in more people's hands, um, to basically show that the emperor had no clothes, um, that this wasn't very good. And I think with the NIST standards and what's happening with post-quantum is really, you know, the hopeful version is we learned that lesson and we're not seeing government trying to pretend like there isn't a risk in order to preserve old standards, but instead leading the way with new ones. Is that fair? That is very fair. NIST ran this post-quantum competition almost over 10 years, and it had over 80 submissions in the first round from all over the world, from industry, academia, and a mix of everything in between, and then it narrowed it down to. the three that are, they're not all out yet, but there's the key agreement, one called ML Kem, and three signatures. And there's a mix of cryptographic problems that they're based on, but there were multiple rounds, lots of feedback, lots of things got broken. This competition has absolutely led the way for the world of getting ready for post-quantum cryptography. There are some competitions that have happened in Korea, and I think there's some work happening in China for their, you know, for their area.There are other open standards and there are standards happening in other standards bodies, but the NIST competition has led the way, and it, because it's all open and all these standards are open and all of the work and the cryptanalysis that has gone in for the whole stretch. It's all been public and all these standards and drafts and analysis and attacks have been public. It's able to benefit everyone in the world. I got started in the crypto wars in the nineties where the government was kind of the problem and they still are. And I do wanna ask you about whether you're seeing any role of the kinda national social security, FBI infrastructure, which has traditionally tried to put a thumb on the scales and make things less secure so that they could have access, if you're seeing any of that there. But on the NIST side, I think this provides a nice counter example of how government can help facilitate building a better world sometimes, as opposed to being the thing we have to drag kicking and screaming into it.But let me circle around to the question I embedded in that, which is, you know, one of the problems that that, that we know happened in the nineties around DES, and then of course some of the Snowden revelations indicated some mucking about in security as well behind the scenes by the NSA. Are you seeing anything like that and, and what should we be on the lookout for? Not in the PQC stuff. Uh, there, like there have been a lot of people that were paying very close attention to what these independent teams were proposing and then what was getting turned into a standard or a proposed standard and every little change, because I, I was closely following the key establishment stuff.Um, every little change people were trying to be like, did you tweak? Why did you tweak that? Did, like, is there a good reason? And like, running down basically all of those things. And like including trying to get into the nitty gritty of like. Okay. We think this is approximately these many bits of security using these parameter and like talking about, I dunno, 123 versus 128 bits and like really paying attention to all of that stuff.And I don't think there was any evidence of anything like that. And, and for, for plus or minus, because there were. I don't remember which crypto scheme it was, but it, there was definitely an improvement from, I think some of the folks at NSA very quietly back in the day to, I think it was the S boxes, and I don't remember if it was DES or AES or whatever it was.But people didn't understand at the time because it was related to advanced, uh, I think it was a differential crypto analysis attacks that folks inside there knew about, and people in outside academia didn't quite know about yet. And then after the fact they were like, oh, they've made this better. Um, we're not, we're not even seeing any evidence of anything of that character either.It's just sort of like, it's very open letting, like if everything's proceeding well and the products are going well of these post-quantum standards, like, you know, leave it alone. And so everything looks good. And like, especially for NSA, uh, national Security Systems in the, in the United States, they have updated their own targets to migrate to post-quantum, and they are relying fully on the highest security level of these new standards.So like they are eating their own dog food. They're protecting the highest classified systems and saying these need to be fully migrated to fully post quantum key agreement. Uh, and I think signatures at different times, but there has to be by like 2035. So if they were doing anything to kind of twiddle with those standards, they'd be, you know, hurting themselves and shooting themselves in the foot. Well fingers crossed. Because I wanna build a better internet and a better. Internet means that they aren't secretly messing around with our security. And so this is, you know, cautiously good news. Let's take a quick moment to thank our sponsor.“How to Fix the Internet” is supported by The Alfred P. Sloan Foundation’s Program in Public Understanding of Science and Technology. Enriching people’s lives through a keener appreciation of our increasingly technological world and portraying the complex humanity of scientists, engineers, and mathematicians.We also want to thank EFF members and donors. EFF has been fighting for digital rights for 35 years, and that fight is bigger than ever, so please, if you like what we do, go to eff.org/pod to donate. Also, we’d love for you to join us at this year’s EFF awards, where we celebrate the people working towards the better digital future that we all care so much about. Those are coming up on September 12th in San Francisco. You can find more information about that at eff.org/awards.We also wanted to share that our friend Cory Doctorow has a new podcast. Listen to this.  [Who Broke the Internet trailer] And now, back to our conversation with Deirdre Connolly. I think the thing that's fascinating about this is kind of seeing this cat and mouse game about the ability to break codes, and the ability to build codes and systems that are resistant to the breaking, kind of playing out here in the context of building better computers for everyone.And I think it's really fascinating and I think it also for people I. You know, this is a pretty technical conversation, um, even, you know, uh, for our audience. But this is the stuff that goes on under the hood of how we keep journalists safe, how we keep activists safe, how we keep us all safe, whether it's our bank accounts or our, you know, people are talking about mobile IDs now and other, you know, all sorts of sensitive documents that are going to not be in physical form anymore, but are gonna be in digital form. And unless we get this lock part right, we're really creating problems for people. And you know, what I really appreciate about you and the other people kind of in the midst of this fight is it's very unsung, right? It's kind of under the radar for the rest of us, but yet it's the, it's the ground that we need to stand on to, to be safe moving forward. Yeah, and there's a lot of assumptions, uh, that even the low level theoretical cryptographers and the people implementing their, their stuff into software and the stuff, the people trying to deploy, that there's a, a lot of assumptions that have been baked into what we've built that to a degree don't quite fit in some of the, the things we've been able to build in a post-quantum secure way, or the way we think it's a post-quantum secure way.Um, we're gonna need to change some stuff and we think we know how to change some stuff to make it work. but we are hoping that we don't accidentally introduce any vulnerabilities or gaps. We're trying, but also we're not a hundred percent sure that we're not missing something, 'cause these things are new. And so we're trying, and we're also trying to make sure we don't break things as we change them because we're trying to change them to be post quantum resilient. But you know, once you change something, if there's a possibility, you, you just didn't understand it completely. And you don't wanna break something that was working well in one direction because you wanna improve it in another direction. And that's why I think it's important to continue to have a robust community of people who are the breakers, right? Who are, are hackers, who are, who are attacking. And that is a, you know, that's a mindset, right? That's a way of thinking about stuff that is important to protect and nurture, um, because, you know, there's an old quote from Bruce Schneider: Anyone can build a crypto system that they themselves cannot break. Right? It takes a community of people trying to really pound away at something to figure out where the holes are. And you know, a lot of the work that EFF does around coders rights and other kinds of things is to make sure that there's space for that. and I think it's gonna be as needed in a quantum world as it was in a kind of classical computer world. Absolutely. I'm confident that we will learn a lot more from the breakers about this new cryptography because, like, we've tried to be robust through this, you know, NIST competition, and a lot of those, the things that we learn apply to other constructions as they come out. but like there's a whole area of people who are going to be encountering this kind of newish cryptography for the first time, and they kind of look at it and they're like. Oh, uh, I, I think I might be able to do something interesting with this, and we're, we'll all learn more and we'll try to patch and update as quickly as possible And this is why we have competitions to figure out what the best options are and why some people might favor one algorithm over another for different, different processes and things like that. And that's why we're probably gonna have a lot of different flavors of post-quantum cryptography getting deployed in the world because it's not just, ah, you know, I don't love NIST. I'm gonna do my own thing in my own country over here. Or, or have different requirements. There is that at play, but also you're trying to not put all your eggs in one basket as well. Yeah, so we want a menu of things so that people can really pick, from, you know, vetted, but different strategies. So I wanna ask the kind of core question for the podcast, which is, um, what does it look like if we get this right, if we get quantum computing and, you know, post-quantum crypto, right?How does the world look different? Or does it just look the same? How, what, what does it look like if we do this well? Hopefully to a person just using their phone or using their computer to talk to somebody on the other side of the world, hopefully they don't notice. Hopefully to them, if they're, you know, deploying a website and they're like, ah, I need to get a Let’s Encrypt certificate or whatever.Hopefully Let's Encrypt just, you know, insert bot just kind of does everything right by default and they don't have to worry about it. Um, for the builders, it should be, we have a good recommended menu of cryptography that you can use when you're deploying TLS, when you're deploying SSH, uh, when you're building cryptographic applications, especially. So like if you are building something in Go or Java or you know, whatever it might be, the crypto library in your language will have the updated recommended signature algorithm or key agreement algorithm and be, like, this is how we, you know, they have code snippets to say like, this is how you should use it, and they will deprecate the older stuff. And, like, unfortunately there's gonna be a long time where there's gonna be a mix of the new post-quantum stuff that we know how to use and know how to deploy and protect. The most important, you know, stuff like to mitigate Store now/Decrypt later and, you know, get those signatures with the most important, uh, protected stuff.Uh, get those done. But there's a lot of stuff that we're not really clear about. How we wanna do it yet, and kind of going back to one of the things you mentioned earlier, uh, comparing this to Y2K, there was a lot of work that went into mitigating Y2K before, during, immediately after.Unfortunately, the comparison to the post quantum migration kind of falls down because after Y2K, if you hadn't fixed something, it would break. And you would notice in usually an obvious way, and then you could go find it. You, you fix the most important stuff that, you know, if it broke, like you would lose billions of dollars or, you know, whatever. You'd have an outage. For cryptography, especially the stuff that's a little bit fancier. Um, you might not know it's broken because the adversary is not gonna, it's not gonna blow up.And you have to, you know, reboot a server or patch something and then, you know, redeploy. If it's gonna fail, it's gonna fail quietly. And so we're trying to kind of find these things, or at least make the kind of longer tail of stuff, uh, find fixes for that upfront, you know, so that at least the option is available. But for a regular person, hopefully they shouldn't notice. So everyone's trying really hard to make it so that the best security, in terms of the cryptography is deployed with, without downgrading your experience. We're gonna keep trying to do that.I don't wanna build crap and say “Go use it.” I want you to be able to just go about your life and use a tool that's supposed to be useful and helpful. And it's not accidentally leaking all your data to some third party service or just leaving a hole on your network for any, any actor who notices to walk through and you know, all that sort of stuff.So whether it's like implementing things securely in software, or it's cryptography or you know, post-quantum weirdness, like for me, I just wanna build good stuff for people, that's not crap. Everyone listening to this agrees with you. We don't want to build crap. We want to build some beautiful things. Let's go out there and do it. Thank you so much, Deirdre. Thank you! Thank you Deirdre. We really appreciate you coming and explaining all of this to, you know, uh, the lawyer and activist at EFF. Well, I think that was probably the most technical conversation we've had, but I followed along pretty well and I feel like at first I was very nervous based on the, save and decrypt concerns. But after we talked to Deirdre, I feel like the people working on this. Just like for Y2K are pretty much gonna keep us out of hot water. And I learned a lot more than I did know before we started the conversation. What about you, Cindy? I learned a lot as well. I mean, cryptography and, attacks on security is always, you know, it's a process, and it's a process by which we do the best we can, and then, then we also do the best we can to rip it apart and find all the holes, and then we, we iterate forward. And it's nice to hear that that model is still the model, even as we get into something like quantum computers, which, um, frankly are still hard to conceptualize. But I agree. I think that what the good news outta this interview is I feel like there's a lot of pieces in place to try to do this right, to have this tremendous shift in computing that we don't know when it's coming, but I think that the research indicates that it SI coming, be something that we can handle, um, rather than something that overwhelms us.And I think that's really,it's good to hear that good people are trying to do the right thing here since it's not inevitable. Yeah, and it is nice when someone's sort of best vision for what the future looks like is hopefully your life. You will have no impacts from this because everything will be taken care of. That's always good. I mean, it sounds like, you know, the main thing for EFF is, as you said, we have to make sure that security engineers, hackers have the resources that they need to protect us from these kinds of threats and, and other kinds of threats obviously.But, you know, that's part of EFF's job, like you mentioned. Our job is to make sure that there are people able to do this work and be protected while doing it so that when the. Solutions do come about. You know, they work and they're implemented and the average person doesn't have to know anything and isn't vulnerable. Yeah, I also think that, um, I appreciated her vision that this is a, you know, the future's gonna be not just one. Size fits all solution, but a menu of things that take into account, you know, both what works better in terms of, you know, bandwidth and compute time, but also what you know, what people actually need.And I think that's a piece that's kind of built into the way that this is happening that's also really hopeful. In the past and, and I was around when EFF built the DES cracker, um, you know, we had a government that was saying, you know, you know, everything's fine, everything's fine when everybody knew that things weren't fine. So it's also really hopeful that that's not the position that NIST is taking now, and that's not the position that people who may not even pick the NIST standards but pick other standards are really thinking through. Yeah, it's very helpful and positive and nice to hear when something has improved for the better. Right? And that's what happened here. We had this, this different attitude from, you know, government at large in the past and it's changed and that's partly thanks to EFF, which is amazing. Yeah, I think that's right. And, um, you know, we'll see going forward, you know, the governments change and they go through different things, but this is, this is a hopeful moment and we're gonna push on through to this future. I think there's a lot of, you know, there's a lot of worry about quantum computers and what they're gonna do in the world, and it's nice to have a little vision of, not only can we get it right, but there are forces in place that are getting it right. And of course it does my heart so, so good that, you know, someone like Deirdre was inspired by Snowden and dove deep and figured out how to be one of the people who was building the better world. We've talked to so many people like that, and this is a particular, you know, little geeky corner of the world. But, you know, those are our people and that makes me really happy. Thanks for joining us for this episode of How to Fix the Internet.If you have feedback or suggestions, we'd love to hear from you. Visit EFF dot org slash podcast and click on listener feedback. While you're there, you can become a member, donate, maybe even pick up some merch and just see what's happening in digital rights this week and every week.Our theme music is by Nat Keefe of BeatMower with Reed MathisHow to Fix the Internet is supported by the Alfred P. Sloan Foundation's program in public understanding of science and technology.I’m Jason Kelley… And I’m Cindy Cohn. This podcast is licensed creative commons attribution 4.0 international, and includes the following music licensed creative commons attribution 3.0 unported by its creators: Drops of H2O, The Filtered Water Treatment by Jay Lang. Sound design, additional music and theme remixes by Gaetan Harris.]]></content:encoded></item><item><title>Court Rules Anthropic’s Book Scans Were Fair Use</title><link>https://hackernoon.com/court-rules-anthropics-book-scans-were-fair-use?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 5 of 10. B. THE COPIES USED TO BUILD A CENTRAL LIBRARYRecall that Anthropic purchased millions of print books for its central library and pirated millions of digital books for its central library, too. It used specific sets and subsets of books for training specific LLMs. And, it then retained all the copies in its central library for other uses that might arise even after deciding it would not use them to train any LLM (at all or ever again). Anthropic seems to believe that because some of the works it copied were sometimes used in training LLMs, Anthropic was entitled to take for free all the works in the world and keep them forever with no further accounting. There is no carveout, however, from the Copyright Act for AI companies. Because the legal issues differ between the library copies Anthropic purchased and pirated, this order takes them in turn.(i) The Purchased Library Copies Converted from Print to DigitalAnthropic purchased millions of print copies to “build a research library” (Opp. Exh. 22 at 145, 148). It destroyed each print copy while replacing it with a digital copy for use in its library (not for sharing nor sale outside the company). As to these copies, Authors do not complain that Anthropic failed to pay to acquire a library copy. Authors only complain that Anthropic changed each copy’s format from print to digital (see Opp. 15, 25 & n.14). On the facts here, that format change itself added no new copies, eased storage and enabled searchability, and was not done for purposes trenching upon the copyright owner’s rightful interests — it was transformative. Anthropic purchased its print copies fair and square. With each purchase came entitlement for Anthropic to “dispose[ ]” each copy as it saw fit. 17 U.S.C. § 109(a). So, Anthropic was entitled to keep the copies in its central library for all the ordinary uses. Yes, Anthropic changed the format of these library copies from print to digital — giving rise to the issue here.All agree on the facts of the format change. Anthropic “destructively scan[ned]” the print copies to create the digital ones. Anthropic or its vendors stripped the bindings from the print books, cut the pages to workable dimensions, and scanned those pages — discarding each print copy while creating a digital one in its place. The digital copy was then housed in the “research library” or “generalized data area” in place of the print copy (Opp. Exh. 22 at 145– 46, 193–94). Authors do not allege and our record does not show that Anthropic provided its converted digital copies of print books to anyone outside Anthropic. The parties disagree about the legal consequences of the format change. Was scanning the print copies to create digital replacements transformative? Anthropic argues it was because it was reasonably necessary to training LLMs. Authors argue it was a distinguishable step requiring independent justification.Here, for reasons narrower than Anthropic offers, the mere format change was a fair use. Storage and searchability are not creative properties of the copyrighted work itself but physical properties of the frame around the work or informational properties about the work. See Texaco, 802 F. Supp. at 14 (physical), aff’d, 60 F.3d at 919; Google, 804 F.3d at 225 (informational); Sony Corp. of Am. v. Universal City Studios, Inc. (“Sony Betamax”), 464 U.S. 417, 447 (1984) (rightful interests). In Texaco, the court reasoned that if a purchased scientific journal article had been copied “onto microfilm to conserve space, this might [have been] a persuasive transformative use.” 802 F. Supp. at 14 (Judge Pierre Leval), aff’d, 60 F.3d at 919 (reducing “bulk[ ]” “might suffice to tilt the first fair use factor in favor of Texaco if these purposes were dominant“). In Google Books, the court reasoned that a print-to-digital change to expose information about the work was transformative. Google, 804 F.3d at 225 (Judge Pierre Leval). And, in Sony Betamax, the Supreme Court held that making a recording of a television show in order to instead watch it at a later time was copying but did not usurp any rightful interest of the copyright owner. 464 U.S. at 447, 455. Important to the Supreme Court’s reasoning was the expectation that most such copiers would not distribute the permanent copies of the work. Finally, in A&M Records, Inc. v. Napster, Inc., our court of appeals recognized the reasoning just explained, and therefore rejected by contrast a digitization effort that was touted as space-shifting but in fact resulted in the multiplication of copies shared with outsiders through a file-sharing service. 239 F.3d 1004, 1019 (9th Cir. 2001), aff’g in this part 114 F. Supp. 2d 896, 912–13, 915–16 (N.D. Cal. 2000) (Judge Marilyn Hall Patel) (citing Sony Betamax and Texaco). Here, every purchased print copy was copied in order to save storage space and to enable searchability as a digital copy. The print original was destroyed. One replaced the other. And, there is no evidence that the new, digital copy was shown, shared, or sold outside the company. This use was even more clearly transformative than those in Texaco, Google, and Sony Betamax (where the number of copies went up by at least one), and, of course, more transformative than those uses rejected in Napster (where the number went up by “millions” of copies shared for free with others).Yes, Anthropic is a commercial outfit. And, this order takes for granted that Anthropic in fact benefited from the print-to-digital format change — or it would not have gone to all the trouble. But the crux of the first fair use factor’s concern for “commercial” use is in protecting the copyright owners and their entitlements to exploit their copyright as they see fit (or not). See, e.g., Harper & Row, Publishers, Inc. v. Nation Enters., 471 U.S. 539, 562 (1985). That the accused is a commercial entity is indicative, not dispositive. That the accused stands to benefit is likewise indicative. But what matters most is whether the format change exploits anything the Copyright Act reserves to the copyright owner. Anthropic already had purchased permanent library copies (print ones). It did not create new copies to share or sell outside. Yes, Authors also might have wished to charge Anthropic more for digital than for print copies. And, this order takes for granted that Authors could have succeeded if Anthropic had been barred from the format change. “But the Constitution’s language [in Clause 8] nowhere suggests that [the copyright owner’s] limited exclusive right should include a right to divide markets or a concomitant right to charge different purchasers different prices for the same book, [merely] say to increase or to maximize gain.” See Kirtsaeng v. John Wiley & Sons, Inc., 568 U.S. 519, 552 (2013); see also U.S. CONST. art. I., § 8, cl. 8. Nor does the Copyright Act itself. Section 106 sets out exclusive rights that fair uses under Section 107 abridge. Section 106(1) reserves to the copyright owner the right to make reproductions. But on our facts we face the unusual situation where one copy entirely replaced the another. And, Section 106(2) reserves to the copyright owner the right to make derivative works that add or subtract creative material — as occurs in a “translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, [or] condensation” of a book, 17 U.S.C. § 101 (definitions). For some “other modification[ ]” of a book to constitute a “derivative work,” it must itself “represent an original work of authorship.” Ibid. But on our facts the format was changed but no content was added or subtracted. See Mirage Editions, Inc. v. Albuquerque A.R.T. Co., 856 F.2d 1341, 1342, 1343– 44 (9th Cir. 1988) (yes where elements added to create new decorative ceramic) (4). Section 106(3) further reserves to the copyright owner the right to distribute copies. But again, the replacement copy here was kept in the central library, not distributed. Cf. Fox News Network, LLC v. TVEyes, Inc., 883 F.3d 169, 176–78 (2d Cir. 2018) (enabling searching for “information about the material” can be transformative use, even if some distribution results); Lewis Galoob Toys, Inc. v. Nintendo of Am., Inc., 964 F.2d 965, 968, 971 (9th Cir. 1992) (using nifty converter to “merely enhance[ ]” audiovisual displays emitted from purchased videogame cartridge was fair use of those displays partly because no surplus copies of cartridge or displays were ever created). As a result, Anthropic’s format-change from print library copies to digital library copies was transformative under fair use factor one. Anthropic was entitled to retain a copy of these works in a print format. It retained them instead in a digital format, easing storage and searchability. And, the further copies made therefrom for purposes of training LLMs were themselves transformative for that further reason, as above. To be clear, this print-to-digital conversion involved a different and narrower form of transformative use than the broader one advanced by Anthropic. Anthropic argues that the central library use was part and parcel of the LLM training use and therefore transformative. This order disagrees. However, this order holds that the mere conversion of a print book to a digital file to save space and enable searchability was transformative for that reason alone. Therefore, the digital copy should be treated just as if the purchased print copy had been placed in the central library. In sum, the first fair use factor favors fair use for the digital library copies converted from purchased print library copies — but these do not excuse the pirated library copies.(4) Even if print-to-digital format change did infringe the right to prepare derivative works, Authors have conceded that “Plaintiffs’ infringement claims are predicated on Anthropic’s unauthorized reproduction (17 U.S.C. § 106(1)); Plaintiffs are not alleging infringement by Anthropic of any right to prepare derivative works (id. at § 106(2))” (Dkt. No. 203 at 2 (citations original)). Whether this concession had consequence for copies tokenized and used for training or “compressed” into the trained LLMs is not reached by this order because Anthropic does not rely on Authors’ concession and those copies were here used transformatively.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>Blockchain Security Layers: Tradeoffs Between L1, L2, and Hardware TEEs</title><link>https://hackernoon.com/blockchain-security-layers-tradeoffs-between-l1-l2-and-hardware-tees?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This section compares layer-one and layer-two solutions, and discusses the hardware’s options and impacts.\
Which solution is more secure? Even if we have built clear security metrics based on threat models and give concise security analyses in the context of layered architectures, it is still inadequate for answering this question: Which solution is more secure, layer-one solution or layer-two solution? This is because system security is a multidimensional topic, and measuring all security aspects is impractical. The security flaws may happen in any phase in a system [87]. Despite some projects performing well in our evaluation, we cannot roughly say that they are more secure. As hybrid technologies, both layer-one and layer-two systems have unsatisfactory security vulnerabilities in existing systems, and they must be carefully treated when applying them to real applications. Frankly speaking, there is a long road to achieving such a practically secure and confidential system. Our aim is not to argue which solution is more secure. Instead, we focus on helping developers and communities to establish a security measurement and avoid potential pitfalls in designing TCSC.\
Which solution is more efficient? The layer-one solutions require the contract to be confidentially executed in a distributed TEE network, which is time-consuming and hard to scale out. In contrast, layer-two systems only upload final calculated results from offline TEEs to online blockchains. Local TEE hosts can execute complicated computations with high scalability and short execution time. Assuming that the on-chain processing time remains stable, the overall performance gets improved by enabling parallel off-chain executions. Thus, from the view of performance and scalability, the layer-two solution is our recommendation.\
Which solution is more adoptable? From the aforementioned discussion, we can observe that the layer-one and layertwo solutions fit different scenarios. The layer-one solution is more adoptable in consortium blockchain systems, while the layer-two solution well fits the existing public blockchain systems. Layer-one systems require each blockchain node to equip a TEE, which is difficult to be fulfilled in a public blockchain while already in use. In a consortium blockchain, the nodes are controllable and manageable, and the committee can require each node to equip a TEE when joining the network. On the flip side, the layer-two solution does not change the original blockchain trust assumption. Instead, it creates an independent layer for executing the smart contract, and thus allows developers to seamlessly integrate the TEE into existing public blockchains without significant modifications.\
B. Hardware-anchored TEE Options\
Securing smart contracts with TEEs is challenging because we have to assume a strong attacker model, in which the attacker has physical possession of the hardware running the smart contract and can interfere with it in powerful ways. This part discusses the security impact of choosing different TEE architectures. In particular, we select  [88],  [89] and  [90] as examples.\
Intel SGX is a system allowing one to set up protected enclaves running on an Intel processor. Such enclaves are protected from malware running outside the enclave, including in the operating system. Enclaves can attest their software and computations using a signing key ultimately certified by Intel. Intel SGX has been marketed for desktop machines and servers alike; Microsoft Azure [91] is a commercial cloud offering that allows cloud customers to set up SGX enclaves in the cloud. Many attacks on SGX have been published in the eight years since its release. They may be categorised as side-channel attacks [92], fault attacks [93], [94] and software attacks [95]. While some of these attacks can be solved by improvements of SGX, it is unclear that it will ever be possible to have a completely secure version, because the attack surface is large, in the case of smart contracts, one has to assume that attackers have physical possession of the hardware.\
ARM TrustZone [89] is a technology widely used in mobile phones to protect secrets, such as secrets used in banking apps. Its ubiquity makes it an attractive option. However, it has been attacked even more than Intel SGX, and doesn’t offer a suitable attestation framework. Future hardware-anchored security products from ARM may address this problem.\
Dedicated chips such as the Open Titan [90] family of chips offer a better solution. Open Titan is an open-source design inspired by Google Titan, a chip used on Google servers and in Google mobile phones. The fact that the smart contract runs on a dedicated chip not shared with attacker code means that the attack surface is much smaller. Attestation frameworks exist for such chips, and the attestation keys can be rooted in a manufacturer’s certificate. The kind of attacks mentioned for SGX become much harder to mount. Nevertheless, even dedicated chips may succumb to a dedicated and resourceful attacker. Researchers have succeeded in mounting attacks based on power side-channels and electromagnetic (EM) radiation side channels. Defences against such attacks include masking, which consists of randomly splitting every sensitive intermediate variable into multiple shares. Even if the adversary is able to learn a share of the secret via side-channel, it would need all of them in order to recover the secret. Fault attacks such as EM and voltage glitching are also possible, but again, there are known defences [96] at both a software and hardware level. Software defences include making secret-dependent computations twice (in general  times) and then comparing results before producing any output. Countermeasures in hardware involve having internal voltage monitoring circuitry, which makes sure that the input voltage remains within a safe operation range and resets the device otherwise.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Why TEE-Based Smart Contracts Still Aren’t Fully Secure</title><link>https://hackernoon.com/why-tee-based-smart-contracts-still-arent-fully-secure?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
\
 The private keys in TEE-assisted systems are extremely crucial but hard to manage. On the one hand, putting the application keys in a single TEE contributes to the key security. However, it also makes the system raise the risk of a single point of failure. On the other hand, sharing the private key among multiple TEEs offers practical availability but (as a sacrifice) increases key exfiltration risk. Meanwhile, key sharing technologies are too complicated to adopt and cannot completely solve the key issues. Suppose that an attacker steals the attestation key somehow. She might consequently generate the attestation materials to deceive the user with a fake fact: The contract has been executed. Even worse, if a root key stored in the tamper-resistant hardware (e.g., Memory Encryption Engine Key in SGX) is compromised, all key technologies for protecting application keys become useless.\
 Compared with cryptographic approaches backed by mathematics [22], [23], [27], the confidential smart contracts relied on TEEs are lack of transparency. On the one hand, contracts are executed inside TEEs, and the outputs are usually encrypted, which lacks public verifiability inherited from traditional blockchain systems. The attestation service can only guarantee that the encrypted outputs indeed come from a TEE. However, neither users nor the blockchain nodes can learn whether a TEE is compromised or executes contracts following the predefined specifications. Even if many TEEs can re-execute the same contract with the same setup (e.g., the same private key) to check outputs, this inevitably increases the key exfiltration risk in the face of a confidentiality breach. On the other hand, the precise architectures of chips are still unclear for some TEE products, such as Intel SGX [80]. TEE-assisted solutions force the user to put too much trust in the manufacturers of this hardware. Users even argue that Intel may have reduced the security of SGX to improve performance to cater for market demand [97]. Additionally, the attestation service used to prove that a program runs inside TEEs is  and . A compromised provider has the ability to insert fake IDs, and further, steal the confidential state in smart contracts.The technologies on how to combine smart-contract execution with TEEs are mushrooming nowadays. The absence of systematic work confuses newcomers. In this paper, we provide the first SoK on TEE-assisted confidential smart contract systems. TEE technologies empower transparent smart contracts with confidentiality, greatly extending the scope of upper-layer applications. We summarize state-of-the-art solutions by proposing a unified framework covering aspects of design models, desired properties, and security considerations. Our analysis clarifies existing challenges and future directions for two mainstream architectures (layer-one and layer-two solutions). We believe that this work represents a snapshot of the technologies that have been open-sourced and made public in time. Our evaluation and analysis within this SoK will offer a good guide for communities, and greatly promote the prosperity of development for TCSC applications.\
. Rujia Li and Qi Wang are partially supported by the Shenzhen Fundamental Research Programs under Grant No.20200925154814002. We thank Xinrui Zhang (SUSTech) for her help. Also, we express our appreciation to anonymous reviewers for their valuable comments.[1] Nick Szabo. Formalizing and securing relationships on public networks. First monday, 1997.\
[2] Gavin Wood et al. Ethereum: A secure decentralised generalised transaction ledger. https://ethereum.github.io/yellowpaper/ paper.pdf , 2022.\
[3] Kevin Delmolino et al. Step by step towards creating a safe smart contract: Lessons and insights from a cryptocurrency lab. In FC, pages 79–94. Springer, 2016.\
[4] Hewa et al. Survey on blockchain based smart contracts: Technical aspects and future research. IEEE Access, 2021.\
[5] Maher Alharby and Aad Van Moorsel. Blockchain-based smart contracts: A systematic mapping study. arXiv preprint arXiv:1710.06372, 2017.\
[6] Marc Jansen et al. Do smart contract languages need to be turing complete? In CBA, pages 19–26. Springer, 2019.\
[7] Siraj Raval. Decentralized applications: harnessing Bitcoin’s blockchain technology. " O’Reilly Media, Inc.", 2016.\
[8] Weiqin Zou et al. Smart contract development: Challenges and opportunities. TSE, 2019.\
[9] Rui Zhang, Rui Xue, and Ling Liu. Security and privacy on blockchain. CSUR, 52(3):1–34, 2019.\
[10] Steven Goldfeder. Private smart contracts. 2018.\
[11] Samuel S., Benjamin Bichsel, Mario Gersbach, Noa Melchior, Petar Tsankov, and Martin Vechev. zkay: Specifying and enforcing data privacy in smart contracts. In CCS, pages 1759–1776, 2019.\
[12] Karim Baghery. On the efficiency of privacy-preserving smart contract systems. In AFRICACRYPT, pages 118–136. Springer, 2019.\
[13] A. Unterweger, F. Knirsch, et al. Lessons learned from implementing a privacy-preserving smart contract in ethereum. NTMS, pages 1–5, 2018.\
[14] Fan Zhang, Ethan Cecchetti, Kyle Croman, Ari Juels, and Elaine Shi. Town crier: An authenticated data feed for smart contracts. In CCS, pages 270–282, 2016.\
[15] Erik-Oliver Blass and Florian Kerschbaum. Borealis: Building block for sealed bid auctions on blockchains. In AsiaCCS, pages 558–571, 2020.\
[16] Hisham S Galal and Amr M Youssef. Trustee: full privacy preserving vickrey auction on top of ethereum. In FC, pages 190–207. Springer, 2019.\
[17] Véronique Cortier, David Galindo, Ralf Küsters, Johannes Mueller, and Tomasz Truderung. Sok: Verifiability notions for e-voting protocols. In SP, pages 779–798. IEEE, 2016.\
[18] Geetanjali Rathee et al. On the design and implementation of a blockchain enabled e-voting application within iot-oriented smart cities. IEEE Access, 9:34165–34176, 2021.\
[19] General data protection regulation. https://gdpr-info.eu/. 2020.\
[20] Paul Voigt et al. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017.\
[21] Ahmed Kosba, Andrew Miller, Elaine Shi, Zikai Wen, and Charalampos Papamanthou. Hawk: The blockchain model of cryptography and privacy-preserving smart contracts. In SP, pages 839–858. IEEE, 2016.\
[22] Harry Kalodner et al. Arbitrum: Scalable, private smart contracts. In USENIX Security, pages 1353–1370, 2018.\
[23] B. Bünz, Jonathan Bootle, Dan Boneh, Andrew Poelstra, Pieter Wuille, and Greg Maxwell. Bulletproofs: Short proofs for confidential transactions and more. In SP, pages 315–334. IEEE, 2018.\
[24] Benedikt Bünz et al. Zether: Towards privacy in a smart contract world. In FC, pages 423–443. Springer, 2020.\
[25] Yu Chen, Xuecheng Ma, Cong Tang, and Man Ho Au. Pgc: Decentralized confidential payment system with auditability. In ESORICS, pages 591–610. Springer, 2020.\
[26] Ravital Solomon et al. smartfhe: Privacy-preserving smart contracts from fully homomorphic encryption. IACR Cryptol. ePrint Arch., 2021:133, 2021.\
[27] Guy Zyskind et al. Enigma: Decentralized computation platform with guaranteed privacy. arXiv:1506.03471, 2015.\
[28] Dayeol Lee, David Kohlbrenner, et al. Keystone: An open framework for architecting trusted execution environments. In EuroSys, pages 1– 16, 2020.\
[29] Jan-Erik Ekberg et al. Trusted execution environments on mobile devices. In CCS, pages 1497–1498, 2013.\
[30] Seongmin Kim et al. Enhancing security and privacy of tor’s ecosystem by using trusted execution environments. In NSDI, pages 145–161, 2017.\
[31] David Kaplan, Jeremy Powell, and Tom Woller. Amd memory encryption. White paper, 2016.\
[32] Ferdinand Brasser, David Gens, Patrick Jauernig, Ahmad-Reza Sadeghi, and Emmanuel Stapf. Sanctuary: Arming trustzone with userspace enclaves. In NDSS, 2019.\
[33] Frank McKeen, Ilya Alexandrovich, Alex Berenzon, Carlos V Rozas, Hisham Shafi, Vedvyas Shanbhogue, and Uday R Savagaonkar. Innovative instructions and software model for isolated execution. Hasp@ isca, 10(1), 2013.\
[34] ChongChong Zhao et al. On the performance of intel sgx. In WISA, pages 184–187. IEEE, 2016.\
[35] Jinhua Cui et al. Dynamic binary translation for sgx enclaves. arXiv preprint arXiv:2103.15289, 2021.\
[36] Rujia Li, Qin Wang, et al. An offline delegatable cryptocurrency system. arXiv preprint arXiv:2103.12905, 2021.\
[37] Ying Yan, Changzheng Wei, et al. Confidentiality support over financial grade consortium blockchain. In SIGMOD, pages 2227–2240, 2020.\
[38] Rohit Sinha et al. Luciditee: A tee-blockchain system for policycompliant multiparty computation with fairness.\
[39] Chinese chang’an chain enterprise blockchain joins digital yuan project, Mar 2021.\
[40] Financials. Changan chain, the first independent and controllable blockchain technology system in china, was released today.\
[41] Yong Wang et al. Hybridchain: A novel architecture for confidentialitypreserving and performant permissioned blockchain using trusted execution environment. IEEE Access, 8:190652–190662, 2020.\
[42] Raymond Cheng, Fan Zhang, Jernej Kos, Warren He, Nicholas Hynes, Noah Johnson, Ari Juels, Andrew Miller, and Dawn Song. Ekiden: A platform for confidentiality-preserving, trustworthy, and performant smart contracts. In EuroSP, pages 185–200. IEEE, 2019.\
[43] Poulami Das et al. Fastkitten: Practical smart contracts on bitcoin. In USENIX Security, pages 801–818, 2019.\
[44] Christina Müller, Marcus Brandenburger, et al. Tz4fabric: Executing smart contracts with arm trustzone. arXiv preprint arXiv:2008.11601, 2020.\
[45] Mark Russinovich et al. Ccf: A framework for building confidential verifiable replicated services. Technical Report MSR-TR-2019-16, Microsoft, April 2019.\
[46] Mic Bowman et al. Private data objects: an overview. arXiv preprint arXiv:1807.05686, 2018.\
[47] Adam Young and Moti Yung. The dark side of “black-box” cryptography or: Should we trust capstone? In CRYPTO, pages 89–103. Springer, 1996.\
[48] Rujia Li, David Galindo, and Qi Wang. Auditable credential anonymity revocation based on privacy-preserving smart contracts. In CBT, pages 355–371. Springer, 2019.\
[49] Rujia Li, Qin Wang, et al. An accountable decryption system based on privacy-preserving smart contracts. In ISC, pages 372–390. Springer, 2020.\
[50] Oasis lab. https:// github.com/ oasislabs/ secret-ballot/ blob/master/ contracts/ SecretBallot.sol.\
[51] Véronique Cortier et al. Election verifiability for helios under weaker trust assumptions. In ESORICS, pages 327–344. Springer, 2014.\
[52] Nik Unger, Sergej Dechand, Joseph Bonneau, Sascha Fahl, H. Perl, I. Goldberg, and M. Smith. Sok: Secure messaging. SP, pages 232– 249, 2015.\
[53] Elli Androulaki, Ghassan O Karame, Marc Roeschlin, Tobias Scherer, and Srdjan Capkun. Evaluating user privacy in bitcoin. In FC, pages 34–51. Springer, 2013.\
[54] Sarah Meiklejohn, Marjori Pomarole, Grant Jordan, et al. A fistful of bitcoins: characterizing payments among men with no names. In IMC, pages 127–140, 2013.\
[55] Ferdinand Brasser et al. Software grand exposure:{SGX} cache attacks are practical. In WOOT, 2017.\
[56] Yuanzhong Xu et al. Controlled-channel attacks: Deterministic side channels for untrusted operating systems. In SP, pages 640–656. IEEE, 2015.\
[57] Mark D Hill et al. On the spectre and meltdown processor security vulnerabilities. IEEE Micro, 39(2):9–19, 2019.\
[58] Cynthia Dwork. Differential privacy: A survey of results. In International conference on theory and applications of models of computation, pages 1–19. Springer, 2008.\
[59] Ivan Homoliak and Pawel Szalachowski. Aquareum: A centralized ledger enhanced with blockchain and trusted computing. arXiv preprint arXiv:2005.13339, 2020. [60] Marcus Brandenburger et al. Blockchain and trusted computing: Problems, pitfalls, and a solution for hyperledger fabric. arXiv preprint arXiv:1805.08541, 2018.\
[61] Enigma – securing the decentralized web. https://www.enigma.co/.\
[62] Juan Garay et al. The bitcoin backbone protocol: Analysis and applications. In EUROCRYPT, pages 281–310. Springer, 2015.\
[63] Juan Garay et al. The bitcoin backbone protocol with chains of variable difficulty. In CRYPTO, pages 291–323. Springer, 2017.\
[64] Rafael Pass, Lior Seeman, and Abhi Shelat. Analysis of the blockchain protocol in asynchronous networks. In EUROCRYPT, pages 643–673. Springer, 2017.\
[65] Juan Garay and Aggelos Kiayias. Sok: A consensus taxonomy in the blockchain era. In RSA, pages 284–318. Springer, 2020.\
[66] Intel. Intel software guard extensions (intel sgx). Accessible on https:// software.intel.com/content/www/ us/en/ develop/topics/ software-guard-extensions.html, 2020.\
[67] Robert Krahn, Donald Dragoti, Franz Gregor, et al. Teemon: A continuous performance monitoring framework for tees. In Middleware, pages 178–192, 2020.\
[68] Rui Yuan et al. Shadoweth: Private smart contract on public blockchain. JCST, 33(3):542–556, 2018.\
[69] Yin Hang, Zhou Shunfan, and Jiang Jun. Phala network: A confidential smart contract network based on polkadot. https://files.phala.network/phala-paper.pdf, 2019.\
[70] Taxa. Taxa network: a universal logic layer for blockchain. Website, 2021. https://taxa.network/.\
[71] Enigma. The developer quickstart guide to enigma | by enigma project | enigma. https:// blog.enigma.co/ the-developer-quickstart-guide-to-enigma-880c3fc4308.\
[72] Hyperledger. Introducing hyperledger avalon. www.hyperledger.org/ blog/2019/10/03/introducing-hyperledger-avalon, 2019. (Accessed on 04/19/2021).\
[73] Andreas Erwig, S. Faust, et al. Commitee: An efficient and secure commit-chain protocol using tees. IACR Cryptol. ePrint Arch., 2020:1486, 2020.\
[74] Yang Xiao et al. Privacyguard: Enforcing private data usage control with blockchain and attested off-chain contract execution. In ESORICS, pages 610–629. Springer, 2020.\
[75] Perun Network. Introducing erdstall: Scaling ethereum using trusted execution environments | by perun network | perunnetwork | medium.\
[76] Erdstall. Technology – erdstall. https://erdstall.dev/technology/. (Accessed on 04/17/2021).\
[77] Wentao Liu. Research on dos attack and detection programming. In Third International Symposium on Intelligent Information Technology Application, volume 1, pages 207–210. IEEE, 2009.\
[78] Roberto De Prisco et al. Revisiting the paxos algorithm. Theoretical Computer Science, 243(1-2):35–91, 2000.\
[79] Peter Gaži, Aggelos Kiayias, and Dionysis Zindros. Proof-of-stake sidechains. In SP, pages 139–156. IEEE, 2019.\
[80] Victor Costan and Srinivas Devadas. Intel sgx explained. IACR Cryptol. ePrint Arch., 2016(86):1–118, 2016.\
[81] Nico W., Pierre-Louis Aublin, and Rüdiger Kapitza. sgx-perf: A performance analysis tool for intel sgx enclaves. In Middleware, pages 201–213, 2018.\
[82] R. Pries et al. A new replay attack against anonymous communication networks. ICC, pages 1578–1582, 2008.\
[83] Marcus Brandenburger, Christian Cachin, Rüdiger Kapitza, and Alessandro Sorniotti. Trusted computing meets blockchain: Rollback attacks and a solution for hyperledger fabric. In SRDS, pages 324– 32409. IEEE, 2019.\
[84] Shenbin Zhang et al. A solution for the risk of non-deterministic transactions in hyperledger fabric. In ICBC, pages 253–261. IEEE, 2019.\
[85] Rosario Gennaro, Stanisław Jarecki, Hugo Krawczyk, and Tal Rabin. Secure distributed key generation for discrete-log based cryptosystems. In EUROCRYPT, pages 295–310. Springer, 1999.\
[86] Adi Shamir. How to share a secret. Communications of the ACM, 22(11):612–613, 1979.\
[87] Shari Pfleeger and Robert Cunningham. Why measuring security is hard. IEEE SP, 8(4):46–54, 2010.\
[88] Intel. Introduction to intel® sgx sealing. Website, 2016. https://software.intel.com/content/www/us/en/develop/blogs/ introduction-to-intel-sgx-sealing.html.\
[89] Sandro Pinto and Nuno Santos. Demystifying arm trustzone: A comprehensive survey. CSUR, 51(6):1–36, 2019.\
[90] Scott Johnson et al. Titan: enabling a transparent silicon root of trust for cloud. In Hot Chips: A Symposium on High Performance Chips, volume 194, 2018.\
[91] Cynthia Dwork. Microsoft azure. 2021.\
[92] Jo Van Bulck et al. Foreshadow: Extracting the keys to the intel sgx kingdom with transient out-of-order execution. In USENIX Security, pages 991–1008, 2018.\
[93] Kit Murdock, David Oswald, Flavio D Garcia, et al. Plundervolt: Software-based fault injection attacks against intel sgx. In SP, pages 1466–1482. IEEE, 2020.\
[94] Zitai Chen et al. Voltpillager: Hardware-based fault injection attacks against intel sgx enclaves using the svid voltage scaling interface. In USENIX Security, 2021.\
[95] Jo Van Bulck, David Oswald, et al. A tale of two worlds: Assessing the vulnerability of enclave shielding runtimes. In CCS, pages 1741–1758, 2019.\
[96] Eli Biham and Adi Shamir. Differential fault analysis of secret key cryptosystems. In CRYPOTO, pages 513–525. Springer, 1997.\
[97] Tu Dinh Ngoc, Bao Bui, et al. Everything you should know about intel sgx performance on virtualized systems. POMACS, 3(1):1–21, 2019.\
[98] Arvind Narayanan, Joseph Bonneau, Edward Felten, Andrew Miller, and Steven Goldfeder. Bitcoin and cryptocurrency technologies: a comprehensive introduction. Princeton University Press, 2016.\
[99] Tsz Hon Yuen, Shi-feng Sun, et al. Ringct 3.0 for blockchain confidential transaction: Shorter size and stronger security. In FC, pages 464–483. Springer, 2020.\
[100] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system. Technical report, Manubot, 2008.\
[101] Ying Lan et al. Trustcross: Enabling confidential interoperability across blockchains using trusted hardware. arXiv preprint arXiv:2103.13809, 2021.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Bezos-Backed Methane Tracking Satellite Is Lost In Space</title><link>https://tech.slashdot.org/story/25/07/01/2211218/bezos-backed-methane-tracking-satellite-is-lost-in-space?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[MethaneSAT, an $88 million satellite backed by Jeff Bezos and led by the Environmental Defense Fund to track global methane emissions, has been lost in space after going off course and losing power over Norway. "We're seeing this as a setback, not a failure," Amy Middleton, senior vice president at EDF, told Reuters. "We've made so much progress and so much has been learned that if we hadn't taken this risk, we wouldn't have any of these learnings." Reuters reports: The launch of MethaneSAT in March 2024 was a milestone in a years-long campaign by EDF to hold accountable the more than 120 countries that in 2021 pledged to curb their methane emissions. It also sought to help enforce a further promise from 50 oil and gas companies made at the Dubai COP28 climate summit in December 2023 to eliminate methane and routine gas flaring. [...] While MethaneSAT was not the only project to publish satellite data on methane emissions, its backers said it provided more detail on emissions sources and it partnered with Google to create a publicly-available global map of emissions.
 
EDF reported the lost satellite to federal agencies including the National Oceanic and Atmospheric Administration, Federal Communications Commission and the U.S. Space Force on Tuesday, it said. Building and launching the satellite cost $88 million, according to the EDF. The organization had received a $100 million grant from the Bezos Earth Fund in 2020 and got other major financial support from Arnold Ventures, the Robertson Foundation and the TED Audacious Project and EDF donors. The project was also partnered with the New Zealand Space Agency. EDF said it had insurance to cover the loss and its engineers were investigating what had happened.
 
The organization said it would continue to use its resources, including aircraft with methane-detecting spectrometers, to look for methane leaks. It also said it was too early to say whether it would seek to launch another satellite but believed MethaneSAT proved that a highly sensitive instrument "could see total methane emissions, even at low levels, over wide areas."]]></content:encoded></item><item><title>The TechBeat: The GTM Singularity: Why Sales Will Never be the Same Again (7/2/2025)</title><link>https://hackernoon.com/7-2-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Wed, 2 Jul 2025 06:10:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @Ted-Chalouhi [ 3 Min read ] 
 Discover the FLIP GTM Singularity Framework, a bold new model for replacing traditional sales with AI-driven growth systems.  Read More.By @Ted-Chalouhi [ 3 Min read ] 
 The GEAR Framework: Turning Revenue into a Precision Machine by Ted Chalouhi Read More.By @hacker-aeozms7 [ 3 Min read ] 
 The arrival of truly intelligent, always-on, AI-native revenue engines is dismantling the way we’ve structured go-to-market motions for 20 years.  Read More.By @confluent [ 2 Min read ] 
 Learn how PyIceberg simplifies working with Apache Iceberg using Python—no JVM clusters needed. Ideal for small to mid-sized data lakehouses. Read More.By @AlinaShcherbyna [ 6 Min read ] 
 Why data alone misleads—and how emotion, feedback, and AI create better brand decisions. Read More.By @val314159 [ 28 Min read ] 
 let's explore manually sending the JSON over the wire for the MXP protocol Read More.By @mykolaoliiarnyk [ 4 Min read ] 
 Discover how Ukrainian tech talent is driving global innovation through success stories like GitLab, Revolut, Grammarly, Wise, and Moss.  Read More.By @kuwguap [ 4 Min read ] 
 The story of how the AI pentesting assistant, RAWPA, evolved from a static toolkit into a dynamic, learning system.  Read More.By @paoloap [ 6 Min read ] 
 Learn how to take AI agents from prototype to production with this 5-step roadmap covering Python, RAG, architecture, testing, and real-world monitoring. Read More.By @socialdiscoverygroup [ 6 Min read ] 
 Explore how an online dating platform scaled AI moderation with ChatGPT, custom prompt engineering, and in-house data labeling to cut review time 60x. Read More.By @permit [ 11 Min read ] 
 Learn how to implement scalable multi-tenant authorization using RBAC and Permit.io, with support for fine-grained and relationship-based permissions. Read More.By @brightdata [ 10 Min read ] 
 Let’s uncover what the Playwright MCP server brings to the table, and how to use it with the OpenAI Agents SDK. Read More.By @markpelf [ 6 Min read ] 
 GitHub Copilot Agent, as of June 2025, looks much more capable than it did 2 months ago Read More.By @ishanpandey [ 5 Min read ] 
 World, formerly Worldcoin, launches Priority Blockspace for Humans on its new blockchain. Discover how it prioritizes human users over bots. Read More.By @terezabizkova [ 9 Min read ] 
 Moonwell founder Luke Youngblood on why crypto UX needs empathy, not ego—from passkeys to Paymasters, Beam, and better onboarding for the next billion. Read More.By @nkmarvel [ 3 Min read ] 
 Learn from a real-world cloud security mishap and discover the four key lessons every developer and DevOps team must know. Read More.By @drone [ 4 Min read ] 
 Why values-led innovation is shaping the next phase of blockchain development Read More.By @therealsjr [ 5 Min read ] 
 In a recent candid conversation, Jeff Mahony, a seasoned investor with a distinctly contrarian philosophy, dismantled the Silicon Valley norms. Read More.]]></content:encoded></item><item><title>Build an AI Agent That Out-Researches Your Competitors</title><link>https://hackernoon.com/build-an-ai-agent-that-out-researches-your-competitors?source=rss</link><author>aifa</author><category>tech</category><pubDate>Wed, 2 Jul 2025 06:06:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[⚠️ A Wake-Up Call for DevelopersMany programmers will lose their jobs to AI in the coming years, but not those who learn to build it. Your mission isn't just to learn how to use ChatGPT or Claude — it's to become the creator of such systems, to build the next Perplexity rather than just use it.1. What You'll Master by Reading This ArticleThis article provides a complete architectural blueprint for building your own deep research AI agent, similar to Perplexity's "Deep Research" feature. You'll learn:: How to design a recursive search system using Next.js 15, OpenAI, and exa.ai that actually works in production: Why deep search is a tree structure, not a linear process — and how this changes everything about AI research: How to integrate external web search with internal vector knowledge bases to create truly unique content that your competitors can't replicate: How to manage server resources and user experience during long-running AI operations without breaking the bank: Concrete TypeScript implementations using modern tech stack that you can deploy todayBy the end of this article, you'll have a clear understanding of how to build a self-hosted SaaS for deep research that can be integrated into any product — giving you a competitive edge that's genuinely hard to replicate.1.1. The Technology Stack That Powers IntelligenceFor implementing our deep research AI agent, we use a modern tech stack optimized for production-ready applications with intensive AI usage. This isn't just a tech demo — it's built for real-world scale and reliability.1.1.1. Frontend and Framework — The latest version with improved performance and new concurrent rendering capabilities that handle complex AI interactions smoothly — Full-featured React framework with App Router, including support for parallel and intercepting routes (perfect for complex AI workflows) — Strict typing for code reliability and superior developer experience when building complex AI systems1.1.2. AI and Integrations — Official SDK for integration with GPT-4 and other OpenAI models, with full streaming support — Universal library for working with various AI providers, giving you flexibility to switch models — Specialized AI-oriented search engine for semantic search that actually understands context — Utility-first CSS framework for rapid development without sacrificing design quality — Headless components for creating accessible interfaces that work across all devices — Modern icon library with consistent design language — Component system built on Radix UI and Tailwind CSS for professional-grade interfaces — High-performance library for form handling that doesn't slow down your AI interfaces — TypeScript-first validation schema with static typing that catches errors before they reach production — Seamless integration between Zod and React Hook Form1.1.5. Content Processing — Markdown content rendering with component support for rich AI-generated reports — Modern library for date handling in AI research timelinesThis technology stack provides the , , and  necessary for creating complex production-level AI applications. Every choice here is intentional — from Next.js 15's parallel routes handling complex AI workflows, to Exa.js providing the semantic search capabilities that make deep research possible. A system that can handle the computational complexity of recursive AI research while maintaining the user experience standards that modern applications demand.Ready to see how these pieces fit together to create something truly powerful? Let's dive into the architecture that makes it all work.The AI revolution isn't coming—it's here. And it's creating a stark divide in the developer community. On one side are those who see AI as just another tool to boost productivity, using ChatGPT to write functions and debug code. On the other side are developers who understand a fundamental truth: the real opportunity isn't in using AI—it's in building it.While most developers are learning to prompt ChatGPT more effectively, a smaller group is mastering the architecture behind systems like Perplexity, Claude, and custom AI agents. This isn't just about staying relevant; it's about positioning yourself on the right side of the most significant technological shift since the internet.: Companies don't need developers who can use AI tools—they need developers who can build AI systems. The difference between these two skills will determine who thrives and who becomes obsolete in the next economic cycle.This article provides a complete architectural blueprint for building your own AI-powered deep research agent, similar to Perplexity's "Deep Research" feature. You'll learn not just the technical implementation, but the mental models and design principles that separate amateur AI integrations from production-ready systems that can become competitive advantages.What you'll master by the end:Recursive search architecture: How to design systems that think in trees, not lines: Integrating external web search with internal knowledge bases: Building AI systems that can evaluate, iterate, and improve their own outputsProduction considerations: Managing server resources, timeouts, and user experience for long-running AI operationsThe goal isn't to give you code to copy-paste. It's to transfer the architectural thinking that will let you design AI systems for any domain, any use case, and any scale. By the end, you'll have the mental framework to build AI agents that don't just answer questions—they conduct research like expert analysts.Ready to move from AI consumer to AI architect? Let's dive into why traditional LLMs need a "guide dog" to navigate the internet effectively.2. Introduction: Life After ChatGPT Changed EverythingWe're living through one of the most transformative periods in tech history. ChatGPT and other Large Language Models (LLMs) have fundamentally revolutionized how we interact with information. But if you've been building serious applications with these tools, you've probably hit the same wall I did: models only know the world up to their training cutoff date, and they hallucinate with alarming confidence.2.1. The Problem: Band-Aid Web Search in Modern LLMsThe teams behind ChatGPT, Claude, and other models tried to solve this with built-in web search. It sounds great in theory, but dig deeper and you'll find some serious architectural flaws that make it unsuitable for production applications:: The system makes one or two search queries, grabs the first few results, and calls it a day. This isn't research — it's glorified Google with a chat interface.: There's no recursive deepening into topics. If the first search doesn't yield comprehensive results, the system doesn't ask follow-up questions or explore alternative angles.: Traditional search engines return HTML pages cluttered with ads, navigation elements, and irrelevant content. The LLM has to "dig out" useful information from this digital junk.: The system can't connect found information with your internal data, company documents, or domain-specific knowledge bases.2.2. The Gold Standard: Perplexity's Deep Research RevolutionPerplexity was the first company to show us how search-LLM integration should actually work. Their approach is fundamentally different from these band-aid solutions:AI-Optimized Search Engines: Instead of generic Google API calls, they use specialized search systems that return clean, structured content designed for AI consumption.Iterative Investigation Process: The system doesn't stop at initial results. It analyzes findings, formulates new questions, and continues searching until it builds a comprehensive picture.: This is an autonomous AI agent that can work for minutes at a time, recursively drilling down into topics and gathering information from dozens of sources.This is exactly the kind of system we're going to build together.2.3. Why This Matters for Every DeveloperIn the AI-first era, every product is racing to become "smarter." But simply plugging in the ChatGPT API is just table stakes now. Real competitive advantage comes when your AI can:Research current information from the internet in real-timeCombine public data with your proprietary knowledge basesGenerate unique insights impossible to get from standard LLMsAdapt to your specific business domain and industry nuances2.4. What You'll Walk Away WithMy goal isn't to give you code to copy-paste (though you'll get plenty of that). I want to transfer the mental models and architectural principles that will enable you to:Understand the philosophy behind deep AI research for your specific use case using modern stack (Next.js 15, OpenAI, exa.ai) into any existing product the system for your needsBy the end of this article, you'll have a complete architectural blueprint and production-ready code examples for building your own "Perplexity" — an AI agent that could become your product's secret weapon.: We'll study not just the technical implementation, but the business logic too. Why is recursive search more effective than linear? How do you properly combine external and internal sources? What UX patterns work for long-running AI operations? These questions are just as critical as the code.2.5. For the Impatient: Skip to the CodeI personally can't stand articles that give you lots of words and little substance. Feel free to clone the repo and get it running in development mode right now.: You'll hit timeout limitations (403 errors) on Vercel's free hosting tier in production, but on localhost you can fully experiment and study the logs to your heart's content.Ready to build the future of AI-powered research? Let's start by understanding why LLMs need a "guide dog" to navigate the internet effectively.3. Why LLMs Need a "Guide Dog": The Critical Role of External Search SystemsHere's a hard truth that many developers learn the expensive way: Large Language Models cannot independently access current information from the internet. This isn't a bug — it's a fundamental architectural limitation that requires a sophisticated solution: integration with specialized search systems designed for AI consumption.3.1. Why Traditional Search Engines Are AI PoisonGoogle, Bing, and other traditional search engines were built for humans browsing the web, not for machines processing data. When you hit their APIs, you get back HTML pages stuffed with:Ad blocks and navigation clutter that confuse content extraction (comments, sidebars, footers, cookie banners) that requires complex parsing and often failsjavascript// The traditional approach - a nightmare for AI
const htmlResponse = await fetch('https://api.bing.com/search?q=query');
const messyHtml = await htmlResponse.text();
// You get HTML soup with ads, scripts, and digital garbage
// Good luck extracting meaningful insights from this mess
I've seen teams spend weeks building HTML parsers, only to have them break every time a major site updates their layout. It's not scalable, and it's definitely not reliable.3.2. Keyword Matching vs. Semantic Understanding: A World of DifferenceTraditional search systems look for exact word matches, completely ignoring context and meaning. A query like "Next.js optimization for e-commerce" might miss an excellent article about "boosting React application performance in online stores," even though they're semantically identical topics.This is like having a research assistant who can only find books by matching the exact words in the title, while ignoring everything about the actual content. For AI agents doing deep research, this approach is fundamentally broken.3.3. AI-Native Search Engines: The Game ChangerSpecialized systems like Exa.ai, Metaphor, and Tavily solve the core problems that make traditional search unusable for AI:Semantic Query Understanding They use vector representations to search by meaning, not just keywords. Your AI can find relevant content even when the exact terms don't match.They return pre-processed content without HTML garbage. No more parsing nightmares or broken extractors.They understand previous queries and the overall research context, enabling truly iterative investigation.javascript// The AI-native approach - clean and powerful
const cleanResults = await exa.search({
  query: "Detailed analysis of Next.js performance optimization for high-traffic e-commerce platforms",
  type: "neural",
  contents: { text: true, summary: true }
});
// You get clean, relevant content ready for AI processing
// No parsing, no cleanup, no headaches
3.4. Why This Matters for Production SystemsThe quality of your input data directly determines the quality of your final research output. AI-native search engines provide:: Structured content without the need for fragile HTML parsing \n : Stable APIs designed for automated, high-volume usage \n : Reduced computational overhead for data processing \n : Better source relevance leads to better final insightsjavascript// Hybrid search: external + internal sources
const [webResults, vectorResults] = await Promise.all([
  exa.search(query),
  vectorStore.similaritySearch(query)
]);

const combinedContext = [...webResults, ...vectorResults];
// Now your AI has both current web data AND your proprietary knowledge
3.5. The Bottom Line: Architecture MattersAI-native search engines aren't just a technical detail — they're the architectural foundation that makes quality AI research agents possible. Without the right "guide dog," even the most sophisticated LLM will struggle to create deep, accurate analysis of current information.Think of it this way: you wouldn't send a brilliant researcher into a library where all the books are written in code and half the pages are advertisements. Yet that's exactly what happens when you connect an LLM to traditional search APIs. Give your AI the right tools for the job. In the next section, we'll dive into the specific architecture patterns that make recursive, deep research possible.Ready to see how the pieces fit together? Let's explore the system design that powers truly intelligent AI research agents.4. Think Like a Tree: The Architecture of Recursive SearchThe human brain naturally structures complex information as hierarchical networks. When a researcher investigates a new topic, they don't move in a straight line — they develop a tree-like knowledge network where each new discovery generates additional questions and research directions. This is exactly the mental model we need to implement in our deep search AI agent architecture.4.1. The Fundamental Difference in ApproachesTraditional search systems and built-in web search in LLMs work linearly: receive query → perform search → return results → generate answer. This approach has critical limitations for deep research.Problems with the Linear Approach:: The system stops at the first facts it finds: Each search query is isolated from previous ones: The system can't see relationships between different aspects of a topic: Results depend entirely on the luck of the initial queryThe tree-based approach solves these problems by modeling the natural process of human investigation. Each discovered source can generate new questions, which become separate research branches.4.2. Anatomy of a Search TreeLet's examine the structure of a deep search tree with a concrete example:textNext.js vs WordPress for AI Projects/
├── Performance/
│   ├── Source 1
│   ├── Source 2
│   └── Impact of AI Libraries on Next.js Performance/
│       └── Source 7
├── Development Costs/
│   ├── Source 3
│   └── Source 4
└── SEO and Indexing/
    ├── Source 5
    └── Source 6
 are the main aspects of the topic that the LLM generates based on analysis of the original query. In our example, these are performance, costs, and SEO. These subtopics aren't formed randomly — the LLM analyzes the semantic space of the query and identifies key research directions. are specific sources (articles, documents, studies) found for each sub-query. Each leaf contains factual information that will be included in the final report. are the most powerful feature of this architecture. When the system analyzes found sources, it can discover new aspects of the topic that require additional investigation. These aspects become new sub-queries, generating their own branches.4.3. Practical Advantages of Tree Architecture: The tree ensures systematic topic coverage. Instead of a random collection of facts, the system builds a logically connected knowledge map where each element has its place in the overall structure.: The system automatically determines which directions require deeper investigation. If one branch yields many relevant sources, the system can go deeper. If a direction proves uninformative, the search terminates early.: Each new search query is formed considering already found information. This allows for more precise and specific questions than isolated searches.: At each tree level, the system can evaluate the relevance and quality of found sources, filtering noise and concentrating on the most valuable information.4.4. Managing Tree Parameters determines how many recursion levels the system can perform. Depth 1 means only main sub-queries without further drilling down. Depth 3-4 allows for truly detailed investigation. controls the number of sub-queries at each level. Too much width can lead to superficial investigation of many directions. Optimal width is usually 3-5 main directions per level. is the average number of child nodes for each tree node. In the context of information search, this corresponds to the number of new sub-queries generated based on each found source.4.5. Optimization and Problem Prevention: The system must track already investigated directions to avoid infinite recursion loops.: More promising branches should be investigated with greater depth, while less informative directions can be terminated earlier.: Different tree branches can be investigated in parallel, significantly speeding up the process when sufficient computational resources are available.: Search results should be cached to avoid repeated requests to external APIs when topics overlap.Execution Time and Server Timeouts: This is another problem that often manifests when implementing deep research, especially if depth exceeds two levels. You could say that increasing the level exponentially increases research complexity. For example, research with four levels of depth can take up to 12 hours.4.6. The Bottom Line: From Chaos to SystemTree architecture transforms the chaotic process of information search into systematic investigation, where each element has its place in the overall knowledge structure. This allows the AI agent to work like an experienced researcher — not just collecting facts, but building a comprehensive understanding of the investigated topic. An AI system that thinks like a human researcher, but operates at machine scale and speed. In the next section, we'll dive into the technical implementation that makes this architectural vision a reality.Ready to see how we translate this conceptual framework into production code? Let's explore the technical stack that powers recursive intelligence.5. The "Search-Evaluate-Deepen" Cycle: Implementing True RecursionRecursive internet parsing isn't just a technical feature — it's a fundamental necessity for creating truly intelligent AI agents. The first page of any search results shows only the tip of the information iceberg. Real insights lie deeper, in related articles, cited sources, and specialized research that most systems never reach.5.1. Data Architecture for Deep InvestigationIn production implementations, the system operates with structured data types that accumulate knowledge at each recursion level:typescripttype Learning = {
  learning: string;
  followUpQuestions: string[];
};

type SearchResult = {
  title: string;
  url: string;
  content: string;
  publishedDate: string;
};

type Research = {
  query: string | undefined;
  queries: string[];
  searchResults: SearchResult[];
  knowledgeBaseResults: string[]; // Vector database responses
  learnings: Learning[];
  completedQueries: string[];
};
This data structure accumulates knowledge at each recursion level, creating unified context for the entire investigation — exactly what separates professional research from random fact-gathering.5.2. Stage 1: "Search" — Intelligent Query GenerationThe system doesn't rely on a single search query. Instead, it generates multiple targeted queries using LLM intelligence:typescriptconst generateSearchQueries = async (query: string, breadth: number) => {
  const {
    object: { queries },
  } = await generateObject({
    model: mainModel,
    prompt: `Generate ${breadth} search queries for the following query: ${query}`,
    schema: z.object({
      queries: z.array(z.string()).min(1).max(10),
    }),
  });
  return queries;
};
: The  parameter controls research width — the number of different topic aspects that will be investigated in parallel. This is where the magic happens: instead of linear search, you get exponential coverage.5.3. Stage 2: "Evaluate" — AI-Driven Result FilteringNot all found sources are equally valuable. The system uses an AI agent with tools for intelligent evaluation of each result:typescriptconst searchAndProcess = async (/* parameters */) => {
  const pendingSearchResults: SearchResult[] = [];
  const finalSearchResults: SearchResult[] = [];

  await generateText({
    model: mainModel,
    prompt: `Search the web for information about ${query}, For each item, where possible, collect detailed examples of use cases (news stories) with a detailed description.`,
    system: "You are a researcher. For each query, search the web and then evaluate if the results are relevant",
    maxSteps: 10,
    tools: {
      searchWeb: tool({
        description: "Search the web for information about a given query",
        parameters: z.object({ query: z.string().min(1) }),
        async execute({ query }) {
          const results = await searchWeb(query, breadth, /* other params */);
          pendingSearchResults.push(...results);
          return results;
        },
      }),
      evaluate: tool({
        description: "Evaluate the search results",
        parameters: z.object({}),
        async execute() {
          const pendingResult = pendingSearchResults.pop();
          if (!pendingResult) return "No search results available for evaluation.";

          const { object: evaluation } = await generateObject({
            model: mainModel,
            prompt: `Evaluate whether the search results are relevant and will help answer the following query: ${query}. If the page already exists in the existing results, mark it as irrelevant.`,
            output: "enum",
            enum: ["relevant", "irrelevant"],
          });

          if (evaluation === "relevant") {
            finalSearchResults.push(pendingResult);
          }
          return evaluation === "irrelevant" 
            ? "Search results are irrelevant. Please search again with a more specific query."
            : "Search results are relevant. End research for this query.";
        },
      }),
    },
  });
  return finalSearchResults;
};
: The system uses an AI agent with tools that can repeatedly search and evaluate results until it finds sufficient relevant information. This is like having a research assistant who doesn't give up after the first Google search.5.4. Vector Knowledge Base IntegrationThe real power emerges from synergy between external and internal search. For each query, the system simultaneously searches the internet and its own vector knowledge base:typescriptasync function getKnowledgeItem(query: string, vectorStoreId: string) {
  const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

  const response = await client.responses.create({
    model: "gpt-4o-mini",
    tools: [
      {
        type: "file_search",
        vector_store_ids: [vectorStoreId],
        max_num_results: 5,
      },
    ],
    input: [
      {
        role: "developer",
        content: `Search the vector store for information. Output format language: ${process.env.NEXT_PUBLIC_APP_HTTP_LANG || "en"}`,
      },
      {
        role: "user",
        content: query,
      },
    ],
  });
  return response.output_text;
}
5.5. Practical ImplementationIn the main research loop, the system queries both sources in parallel:typescriptfor (const query of queries) {
  const searchResults = await searchAndProcess(/* web search */);
  accumulatedResearch.searchResults.push(...searchResults);

  if (vectorStoreId && vectorStoreId !== "") {
    const kbResult = await getKnowledgeItem(query, vectorStoreId);
    accumulatedResearch.knowledgeBaseResults.push(kbResult);
  }
}
5.6. Stage 3: "Deepen" — Generating Follow-Up QuestionsThe most powerful feature: the system's ability to generate new research directions based on already found information:typescriptconst generateLearnings = async (query: string, searchResult: SearchResult) => {
  const { object } = await generateObject({
    model: mainModel,
    prompt: `The user is researching "${query}". The following search result were deemed relevant.
      Generate a learning and a follow-up question from the following search result:

      <search_result>
      ${JSON.stringify(searchResult)}
      </search_result>`,
    schema: z.object({
      learning: z.string(),
      followUpQuestions: z.array(z.string()),
    }),
  });
  return object;
};
Each found source is analyzed to extract new questions that become the foundation for the next search level:typescriptfor (const searchResult of searchResults) {
  const learnings = await generateLearnings(query, searchResult);
  accumulatedResearch.learnings.push(learnings);
  accumulatedResearch.completedQueries.push(query);

  const newQuery = `Overall research goal: ${prompt}
Previous search queries: ${accumulatedResearch.completedQueries.join(", ")}
Follow-up questions: ${learnings.followUpQuestions.join(", ")}`;

  await deepResearch(
    /* search parameters */,
    newQuery,
    depth - 1,
    Math.ceil(breadth / 2), // Reduce width at each level
    vectorOfThought,
    accumulatedResearch,
    vectorStoreId
  );
}
The production implementation shows how to manage exponential complexity growth:typescriptconst deepResearch = async (
  /* multiple filtering parameters */,
  prompt: string,
  depth: number = 2,
  breadth: number = 5,
  vectorOfThought: string[] = [],
  accumulatedResearch: Research = {
    query: undefined,
    queries: [],
    searchResults: [],
    knowledgeBaseResults: [],
    learnings: [],
    completedQueries: [],
  },
  vectorStoreId: string
): Promise<Research> => {
  if (depth === 0) {
    return accumulatedResearch; // Base case for recursion
  }

  // Adaptive query formation based on "thought vector"
  let updatedPrompt = "";
  if (vectorOfThought.length === 0) {
    updatedPrompt = prompt;
  } else {
    const vectorOfThoughItem = vectorOfThought[vectorOfThought.length - depth];
    updatedPrompt = `${prompt}, focus on these important branches of thought: ${vectorOfThoughItem}`;
  }
  // ... rest of implementation
};
: at each level prevents exponential growth \n : allows directing research into specific areas \n : All results are preserved in a unified data structure5.10. The Hybrid Advantage in Practice: Combining public data with internal knowledge enables creating reports that no one else can replicate. Your competitors may access the same public sources, but not your internal cases, statistics, and expertise.: External data provides currency and breadth, internal data provides depth and specificity. The system can find general industry trends online, then supplement them with your own data about how these trends affect your business.: Even if web information is outdated or inaccurate, your internal knowledge base can provide fresher and verified data.6. From Chaos to Order: Generating Expert-Level ReportsAfter completing all levels of recursive search, the system accumulates massive amounts of disparate information: web search results, vector database data, generated learnings, and follow-up questions. The final stage is transforming this chaos into a structured, expert-level report that rivals human analysis.6.1. Context Accumulation: Building the Complete PictureAll collected data is unified into a single  structure that serves as complete context for final synthesis:typescripttype Research = {
  query: string | undefined;           // Original query
  queries: string[];                   // All generated search queries
  searchResults: SearchResult[];       // Web search results
  knowledgeBaseResults: string[];      // Vector database responses
  learnings: Learning[];               // Extracted insights
  completedQueries: string[];          // History of completed queries
};
This isn't just data storage — it's a comprehensive knowledge graph that captures the entire investigation journey. Every insight, every source, every connection is preserved for the final synthesis.6.2. The Master Prompt: Where Intelligence Meets SynthesisThe quality of the final report directly depends on the sophistication of the generation prompt. The system uses OpenAI's most powerful model for synthesis:typescriptconst generateReport = async (
  research: Research,
  vectorOfThought: string[],
  systemPrompt: string
) => {
  const { text } = await generateText({
    model: openai("o3-mini"), // Most powerful model for synthesis
    system: systemPrompt,
    prompt:
      "Use the following structured research data to generate a detailed expert report:\n\n" +
      JSON.stringify(research, null, 2),
  });
  return text;
};
: We're not just asking the AI to summarize — we're providing it with a complete research dataset and asking it to think like a domain expert. The difference in output quality is dramatic.6.3. Structured Output: Beyond Simple SummariesThe system doesn't just create a text summary — it generates structured documents with headers, tables, pro/con lists, and professional formatting, as shown in the result saving:typescriptconsole.log("Research completed!");
console.log("Generating report...");
const report = await generateReport(research, vectorOfThought, systemPrompt);
console.log("Report generated! Saving to report.md");
fs.writeFileSync("report.md", report); // Save as Markdown
 It's the perfect format for AI-generated content — structured enough for professional presentation, flexible enough for various output formats, and readable in any modern development workflow.6.4. Quality Control Through System PromptsThe  allows customizing report style and structure for specific needs: for research papers and scholarly analysis for corporate reports and executive summaries for developer-focused content for financial and strategic reports// Example: Business-focused system prompt const businessSystemPrompt = `You are a senior business analyst creating an executive report. Structure your analysis with:Use data-driven insights and provide specific examples from the research.`;6.5. The Intelligence Multiplier EffectHere's what makes this approach revolutionary: The system doesn't just aggregate information — it synthesizes insights that emerge from the connections between different sources. A human researcher might spend 8-12 hours conducting this level of investigation. Our system does it in 10-60 minutes, often uncovering connections that humans miss.6.6. Production Considerations: With deep research (depth 3-4), the accumulated context can become massive. The system needs to handle large JSON structures efficiently.: The final synthesis prompt can easily exceed token limits. Production implementations need smart truncation strategies that preserve the most valuable insights.: Not all generated reports are equal. Consider implementing scoring mechanisms to evaluate report completeness and coherence.: Hours of human research → Minutes of AI analysis \n : AI can process and connect more sources than humanly possible \n : Every report follows the same rigorous methodology \n : Generate dozens of reports simultaneously7. Conclusion: Building the Future of AI ResearchCreating a deep research AI agent isn't just a technical challenge — it's an architectural solution that can become a competitive advantage for any product. We've covered the complete cycle from concept to implementation, showing how to transform the chaotic process of information search into systematic, expert-level investigation.7.1. Key Architectural PrinciplesThink in Trees, Not Lines: Deep search is about exploring tree-structured information networks, where each discovery generates new questions and research directions.: Specialized search engines like exa.ai aren't optional — they're essential for quality research. They return clean data instead of HTML garbage that traditional search APIs provide.Apply Recursion for Depth: The first page of results is just the tip of the iceberg. Real insights lie in recursive deepening through the "Search-Evaluate-Deepen" cycle.Combine External and Internal Sources: The synergy between public internet data and private organizational knowledge creates unique content that's impossible to obtain any other way.Use LLMs for Both Analysis and Synthesis: AI agents with tools can not only search for information but also evaluate its relevance, generate new questions, and create structured reports.7.2. Production-Ready ResultsRecursive architecture with depth and width managementIntegration of web search with vector knowledge basesAI agents with tools for result evaluationExpert report generation with file saving capabilities7.3. Challenges and Limitations: Research with depth greater than 2 levels can take hours, requiring special solutions for production environments.Exponential Complexity Growth: Each depth level increases the number of queries geometrically, requiring careful resource management.: Even AI search engines can return inaccurate information, requiring additional validation and fact-checking.Now you have a complete architectural blueprint and real code examples. You can:Start with Minimal Implementation: Use the basic version from this article for prototyping \n Explore the Ready Solution: Clone https://github.com/aifa-agi/aifa-deep-researcer-starterand experiment locally \n : Integrate these principles into existing products and workflows8. Homework Challenge: Solving the Long-Wait UX ProblemWe've covered the technical architecture of deep research AI agents, but there's a critically important UX problem remaining: what do you do when the system works for several minutes while the user stares at a blank screen? As the real implementation from aifa-deep-researcer-starter shows, research with depth greater than 2 levels can take hours, while users see only a static loading screen.8.1. The Problem: Server Silence Kills TrustUnlike typical web applications where operations take seconds, deep research AI agents can be silent for minutes. Users get no feedback from the server until the very end of the process. This creates several psychological problems:: Users don't know if the system is working or frozen: No way to understand how much longer to wait: It seems like the application is broken or "ate" the request: Users close the tab without waiting for resultsPerplexity, Claude, and other modern AI products solve this with interactive animations, progress indicators, and dynamic hints. But how do you implement something similar when your server doesn't send intermediate data?8.2. The Developer ChallengeImagine this technical constraint: your Next.js API route performs a long operation (deep research) and can't send intermediate data until completion. The frontend only gets a response at the very end. Classic solutions like Server-Sent Events or WebSockets might be unavailable due to hosting limitations or architecture constraints.How do you create engaging UX under these conditions that will retain users and reduce waiting anxiety?8.3. Discussion QuestionsUX Patterns and Visualization:What UX patterns would you apply to visualize deep search processes when the server is "silent"?How can you simulate "live" progress even without real server status updates?Should you use fake progress bars, or does this violate user trust?What animations and micro-interactions help create a sense of a "living" system?How do you explain to users why waiting might be long? What texts/illustrations to use?Should you show estimated wait times if they can vary greatly (2 to 60 minutes)?How do you visualize process stages ("Generating search queries…", "Analyzing sources…", "Forming expert report…")?What metaphors help users understand the value of waiting?Technical Implementation:What optimistic UI approaches can be applied without server feedback?How do you implement a "conversational" interface that supports users during waiting?Can you use local computations (Web Workers, WASM) to simulate progress?How do you organize graceful degradation if users close the tab during research?8.4. Learning from the BestStudy solutions implemented in Perplexity Deep Research, Bing Copilot, Google Search Generative Experience. What can you take from gaming loading screens that hold attention for minutes? How do Netflix, YouTube, and other platforms solve long content loading problems?: In an era of instant ChatGPT responses, quality waiting can become a competitive advantage. Users are willing to wait if they understand the process value and feel the system is working for them.9. About the Author and AIFA ProjectThe author, Roman Bolshiyanov, in his recent publication series, details the tools and architectural solutions he implements in his ambitious open-source project AIFA (AI agents in evolving and self-replicating architecture).In its current implementation, AIFA already represents an impressive starter template for creating AI-first applications with a unique user interface where artificial intelligence becomes the primary interaction method, while traditional web interface serves as auxiliary visualization.However, this is just the beginning. The project's long-term goal is evolution into a full-fledged AGI system where AI agents will possess capabilities for: and improvement of their algorithms and creation of new specialized agentsCompetition and cooperation in distributed environments in web spaces and blockchain networksThe deep search covered in this article is just one of the basic skills of future AGI agents that will be able to not only research information but also make decisions, create products, and interact with the real world.If you're interested in observing the project's development and experimenting with cutting-edge AI technologies, don't hesitate to fork the AIFA repository and dive into exploring the architecture of the future. Each commit brings us closer to creating truly autonomous artificial intelligence.Ready to build the future? The code is open, the architecture is proven, and the possibilities are limitless. Your next breakthrough in AI-powered research is just a git clone away.]]></content:encoded></item><item><title>Cloudflare Gives Website Owners Option to Charge OpenAI Bots for Scraping</title><link>https://hackernoon.com/cloudflare-gives-website-owners-option-to-charge-openai-bots-for-scraping?source=rss</link><author>Dmitry Baraishuk</author><category>tech</category><pubDate>Wed, 2 Jul 2025 06:04:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Cloudflare has launched a private beta feature called Pay per Crawl, whose sole purpose is to let a website owner charge an AI crawler a fixed fee.]]></content:encoded></item><item><title>Still.js Is Taking Vanilla JavaScript to the Enterprise</title><link>https://hackernoon.com/stilljs-is-taking-vanilla-javascript-to-the-enterprise?source=rss</link><author>Nakassony Bernardo</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:58:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When it comes to complex web application development on the frontend, we often encounter the need to implement features using vanilla JavaScript—whether it’s for direct DOM manipulation or integrating a specific jQuery plugin. Sometimes, a from-scratch implementation using plain HTML, CSS, and vanilla JavaScript is the best (or only) choice.Still.js is a modern Open Source framework that offers the same capabilities as Angular, React, and Vue—without abandoning vanilla JavaScript.Because of its pure JS approach, :Does not require preprocessingDoes not depend on bundlers like Webpack or ViteIs ideal for teams and developers who prefer direct, no-compromise access to native web technologies in addition to those modern features a Web Framework provides.Enterprise-grade web applications need more than just rich UI features. They require: , user permission management, , , , , Micro-frontend architecture (e.g. Frontend embedding and interaction), and more.Still.js supports all of this features natively without the burden of a bundler increasing build time and potential complexity or even tooling overhead.Note: Those will only work within a  project with the correct setup, which is thoroughly explained in the official documentation.Bellow is a simple component implementing a counter, inspite the template is placed inside the class, it can be moved to a .html file which is appropriate in complex apps.import { ViewComponent } from "../../../@still/component/super/ViewComponent.js";

export class CounterComponent extends ViewComponent {

    isPublic = true;
    count = 0;

    template = `
    <div>
        <p>My counter state is @count</p>
        <button (click)="increment()">Increment (@count)</button>
    </div>
    `;

    increment() {
        this.count = this.count.value + 1;
    }
}
\
Basic User authorization managementIn the bellow implementation we're stating that some components won't be accessible by the user, this can be done with proper business logic according to the user role checking.import { StillAppMixin } from "./@still/component/super/AppMixin.js";
import { Components } from "./@still/setup/components.js";
import { AppTemplate } from "./app-template.js";
import { CheckingAccount } from "./app/components/BankAccount/CheckingAccount.js";
import { NewAccountForm } from "./app/components/BankAccount/NewAccountForm.js";
import { SavingAccount } from "./app/components/BankAccount/SavingAccount.js";

export class StillAppSetup extends StillAppMixin(Components) {

    constructor() {
        super();
        //Bellow the entry point component is being set
        this.setHomeComponent(NewAccountForm);
        const blackListComponents = [SavingAccount, CheckingAccount];
        //Make components black-listed by passing it to setBlackList App configuration
        this.setBlackList(blackListComponents);
    }

    async init() {
        return await AppTemplate.newApp();
    }

}
Still.js provides built-in validators, but custom ones can be implemented effortlessly.import { ViewComponent } from "../../../@still/component/super/ViewComponent.js";

export class BasicForm extends ViewComponent {

    isPublic = true;
    firstName = '';
    shoeSize;

    template = `
    <div>
        <form>
            <div class="form-group">
                <label>Shoe Size</label>
                <input 
                    (value)="shoeSize" 
                    (validator)="number" 
                    (validator-warn)="Invalid shoe size, number is required"
                    placeholder="Enter valid shoe size"
                >
            </div>
        </form>
    </div>
    `;

}
A service serves for both Global reactive storage and data flow (e.g. HTTP requests) implementations. Services path can be set in the application level, and we can overriden for specific service with  annotation.import { ViewComponent } from "../../../@still/component/super/ViewComponent.js";
import { CustomersService } from "../../service/api/CustomersService.js";

export class BiddingDisplay extends ViewComponent {

    isPublic = true;

    /** Service declaration, will get injected automatically due to Inject anottation
     *  from the specified Path path due to the annotation
     * @Inject
     * @Path service/api/
     * @type { CustomersService } */
    custService;

    template = `<div></div>`;

    /** Component Hook which takes place when it's completly render and startder */
    stAfterInit() {

        this.custService.customerList.onChange((newValue) => { 
            console.log('Ths customerList store got updated with: ', newValue);
        });

    }

}
\
Hurry up, get yourself started with Still.js is available through NPM on @stilljs/cli. Watch the  and bear for more coming ones.]]></content:encoded></item><item><title>2025 ASIC Miner Buyer’s Guide: Specs, Costs, and What Really Pays Off</title><link>https://hackernoon.com/2025-asic-miner-buyers-guide-specs-costs-and-what-really-pays-off?source=rss</link><author></author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:53:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[So, You’re Still Thinking About Mining?Let’s be real: mining crypto in 2025 is not the get-rich-quick playground it was a decade ago. The latest Bitcoin halving squeezed margins even tighter. Old rigs like the Antminer S9? Basically museum pieces now. Easy Bitcoin is history — but if you run the numbers, plan your power costs, and choose the  ASIC miner, mining can still pay you back — smartly.Look at any legit mining hub — MinerStat, CryptoMinerBros, Oneminers — and it’s obvious: ASICs still dominate Bitcoin’s SHA-256 hashpower (over 90% of it). But here’s the real point: Hashrate alone means nothing if you’re not watching your efficiency, electricity, and ROI window.Why ASICs Still Matter in 2025“Why not just run GPUs?” Or, “Why not rent hashpower on the cloud?” Good questions — but if you’re serious about Bitcoin or any big SHA-256 coin, an ASIC wins hands-down. They’re built for one thing: max hashes with minimum wasted watts.Take Bitmain’s Antminer S21. It cranks out 200 TH/s with far less power per hash than even the best multi-GPU farm. Data from WhatToMine, MinerStat, and CryptoCompare still show the top earners in 2025 are ASICs. GPUs? Great for flexible altcoins — not for your next Bitcoin payout.3 Numbers That Decide If Your Rig Prints Money or Burns CashBefore you click  on some flashy new box, break it down:✅  Raw horsepower. Higher is better, but remember — network difficulty creeps up. \n ✅ How many Joules to squeeze out one Terahash. Top-tier rigs like the S21 run ~17.5 J/TH. Lower is better. \n ✅ Basic math — Upfront Cost ÷ Net Monthly Profit = months to break even. Solid is 12–18 months. If it’s 24+, you’re betting big on Bitcoin staying strong  your rig staying relevant.📌  Bitcointalk pros say: Avoid any ASIC with a payback longer than 20 months — unless your electricity is dirt cheap.The 2025 ASICs People Are Actually BuyingHere’s a , pulled from CryptoMinerBros specs, MinerStat ROI calculators, and real chatter in Discord and Bitcointalk. These are rigs miners trust — not just promo hype.1️⃣ Bitmain Antminer S21 (200 TH/s) Rock-solid workhorse for Bitcoin mining. High hashrate, tight efficiency, and good resale value when you upgrade down the line. Many farms moved from S19s to this. Plug-and-play, proven, easy to flip later. Small-to-mid BTC farms that need a safe bet with fair resale value.2️⃣ Bitmain Antminer S21 Hydro (250 TH/s) Liquid-cooled version of the S21. Higher hashrate, stays cooler, and quieter if you have the setup for it. Big industrial farms love it to max out racks.Power Draw: ~5300W (with cooling) Industrial setups that can handle liquid cooling.3️⃣ Whatsminer M60S (180 TH/s) Whatsminer gives Bitmain real competition. Tough build, easy maintenance, faster shipping. Miners mix these in to dodge Bitmain stock delays. Farms wanting rugged hardware with fewer supply headaches.4️⃣ Canaan Avalon A1566 (185 TH/s) Avalon rigs are all about simple, plug-and-go. Solid durability, strong after-sales support, and easy to run. r/cryptomining folks swear by these for no-fuss farms. Miners who hate tinkering and want dependable output.5️⃣ Bitmain Antminer KS5 Pro (21 TH/s) Purpose-built for Kaspa — an altcoin that’s popped up as a smart hedge against Bitcoin swings. MinerStat shows good profitability for Kaspa this year.Hashrate: 21 TH/s (Kaspa) Miners adding Kaspa or other PoW coins to balance risk.Don’t Forget: New Models Drop FastBitmain, Whatsminer, Goldshell — they push new units every few months. Pre-order? Maybe — but always double-check Discord or Bitcointalk to see if the reseller actually delivers on time. Missed delivery = ROI ruined.You can have the best rig in the world — but if your electricity is pricey, you’re toast. Here’s how serious miners keep it real:✔️ Know your kWh rate — check your bill or ask your provider. \n ✔️ Use live ROI tools — WhatToMine and MinerStat are must-haves. \n ✔️ Pool fees are real — 1–2% adds up. \n ✔️ 95% uptime is realistic — no rig runs perfectly 24/7. \n ✔️ Aim for 12–18 months to break even — 24+ months is a risky bet.✅ Lurk on Discord, Telegram, or Reddit — new deals and hacks show up daily. \n ✅ Smart plugs = catch weird power spikes early. \n ✅ Check resale trends — newer rigs hold value longer. \n ✅ Reseller delays hurt — always verify shipping timelines. \n ✅ Manage the heat — a 3.5 kW rig turns your garage into a sauna fast.Mining in 2025 isn’t plug-and-print money — but smart miners are still turning profits. Get your power costs under control, choose wisely, run the numbers , and keep an eye on new rigs. ASICs remain the backbone — if you plan right, you’ll stay in the green.What’s your mining plan? Are you sticking with Bitcoin or diversifying? Drop your ROI calculations below — let’s swap real numbers and lessons!]]></content:encoded></item><item><title>Cosmic DePIN: How Space‑Based Decentralized Networks Will Redefine Connectivity</title><link>https://hackernoon.com/cosmic-depin-how-spacebased-decentralized-networks-will-redefine-connectivity?source=rss</link><author>Madd Like Mojo</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:51:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In a land where Internet Providers assume 'full' control of our network systems, discover true decentralization from a flying space saucer above.]]></content:encoded></item><item><title>A Software Architect’s No-Bull Take on Vibe Coding</title><link>https://hackernoon.com/a-software-architects-no-bull-take-on-vibe-coding?source=rss</link><author>Andrew Korolov</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:45:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most People Don’t Know What Vibe Coding IsVibe coding is not what people think it is.Y Combinator compares the current state of vibe coding to the first cars, which resembled horse-drawn carriages. As a result, that first car design had too many unnecessary limitations. Once people realized they could do better, real cars emerged.  Similarly, the current state of vibe coding is in its initial suboptimal phase.The analogy is good but incomplete. Vibe coding is in its early stages. It is a vague vision, a sense of what an app should look like. Computer code doesn’t work like that: it needs clear, concrete 0s and 1s. Programmers are forced to translate vague intentions into precise instructions, often doing a significant amount of work to clarify the original intent into concrete rules.There is a Vast Difference Between Traditional Coding and Vibe CodingVibe coding delivers code without requiring specifications. It’s an ongoing experiment. When someone succeeds with vibe coding, they rave about it. When vibe coding fails, few will share their experience. We’re dealing with  here: those who make it tell the story. And there are far fewer stories from those who don’t make it.This is similar to the missing stories of dolphins that forced people to drown. There is a reason you don’t hear about them. People, in general, are less likely to brag about their failures. As a result, we get a distorted picture of how effective vibe coding is: more myth than method, built on selectively shared success stories.When you hear success stories about vibe coding, you think you can repeat it. You can’t.  Those who succeeded most likely had different tasks, used different prompts, and thought differently. AI models hallucinate.  Different prompts yield vastly differing code, whereas traditional coding generally follows more or less the same standards.The Key Principles of Traditional CodingVibe coding follows a process without consistency and repeatability, core qualities of any engineering discipline. Without those, success becomes a lucky exception.Traditional coding was built to avoid that kind of guesswork. It relies on precise algorithms, follows exact patterns, and uses standard structures that make code predictable and maintainable. Experts like Martin Fowler have spent decades formalizing these standards so developers don’t rely on gut instinct. They build on shared logic that scales.I get it; it's an appealing idea for a non-technical person to skip communicating with a boring, nitty-gritty geek and get the code made by yourself. My take on this is, why don’t you apply the same logic to healthcare? Why not ask AI for advice on do-it-yourself heart surgery?Where Vibe Coding Works WellDon’t get me wrong, there are great applications for vibe coding. Making an app just for fun is one such use case. When security and data privacy are not a concern, vibe coding is excellent! Not so when you want to build an app for a health insurance or medical provider. Vibe coding for a flight control system, NASA rocket ship propulsion control system, anyone?  was a pioneering NASA software engineer, here pictured with the code she wrote for the Apollo program that landed humans on the Moon.Would you vibe code that? Maybe you’d risk it, but that’s too much uncertainty for most people. And the problem isn’t that we’re overly cautious or scared of new tech. It’s that we already know these systems sometimes go off the rails, and when they do, it’s not a glitch. It’s real consequences for real people.How AI Works Defines Where Vibe Coding FailsThe way AI works is that it continuously forecasts the next most likely token, regardless of whether the method is applied to a human language or code. To build an AI code generation engine, you take all the existing code in that language that you can get your hands on. Most of the publicly available code in the world is written by Junior-level developers. Senior developers and corporations treat their code as intellectual property and don’t share most of it freely. This is why most good working code is in the private domain and not open source. Let’s take Python. Like a human language, it has a specific standard sentence structure with its own must-have tokens. AI uses forecasting to build code by continuously selecting the next most likely token, thereby arriving at functional code. If you’re interested, Microsoft provides a detailed breakdown of the technical aspects of forecasting.The open-source code is primarily experimental, and only a tiny fraction of that is corporate code that companies release into the public domain, like Lama and Red Hat. Another aspect of open code is that it can be partially open, so it is unlikely to include enterprise-level functions and features, such as advanced access rules, permission systems, authentication, and authorization. Because public code is the primary training source, AI learns from examples that fall short of enterprise-level quality.But simplifying, predicting what “usually” comes next isn’t the same as understanding what’s true. When context shifts or edge cases arise, this kind of pattern matching breaks down. And in code, completing without understanding can quietly introduce bugs that are hard to catch and costly to fix.Who has Access to High-Quality AI-Generated Code (and Why)People using AI want to generate enterprise-quality code, but they can’t. Huge companies like Amazon, Oracle, and Microsoft have massive volumes of trustworthy code that is verified, standardized, and quality-checked by generations of software engineers. AI models trained on code like that would be much more reliable, but the problem is intellectual property rights. Because their code is the corporations' intellectual property and the source of their market valuations, any AI models trained on it can’t and won’t be public. This is why you may have heard about a minor scandal OpenAI got into when using private data from GitHub, a case known as “Zombie Data.” AI models that use private data for training are usually used by those companies internally. Inside those companies, they generate high-quality vibe code. Companies with large volumes or proprietary code can use vibe coding internally with great results. The problem they solve is generating standardized code, not generating just any code fast. With models trained on massive volumes of high-quality code they own, they can ensure the code they produce maintains the style, syntax, and commenting conventions, generating code that’s similar to the code thousands of engineers have written already.  However, companies like that are unlikely to use unverified open-source code from GitHub to train their AI models.  The Way AI Model Training is Designed Causes it to Use Outdated CodeOne of the unspoken issues about AI code generators is the delay in catching up with the latest code updates. Y Combinator and Medium posts feature numerous discussions on version control issues for AI-generated code. AI models train on existing code, so much of what they learn from is already behind current standards. Since most of the available training data is obsolete, the model assigns more weight to older patterns in its predictions. As a result, it tends to generate code using deprecated syntax or practices.Vibe coders don’t realize problems caused by different code versions, but it inevitably becomes a barrier. They’re not engineers, so they cannot tell whether the code they generate is outdated or valid. As a result, they ship things that break silently, and they have no idea where or why they fail.Every new version of a programming language brings changes, and AI models aren’t version-aware. Google’s Vertex AI offers advice on overcoming this problem, but it’s probably not a guide that vibe coders will read. Most version updates fall into three categories:Deprecated functions that stop workingPatches to existing functions that require new syntax, including security updatesNew implementations that the AI model will ignoreVibe coding models often produce old code, and that’s a recipe for security bugs, especially if the latest code version’s critical security patches were missed. Consequently, you get failed supply chain security checks, policy violations, and blocked releases.In theory, AI models could avoid this by learning from user feedback, choosing one code version over another. Stackoverflow and Quora are full of discussions on this topic. But there are two issues here. First, vibe coders don’t have the expertise to evaluate which version is correct, yet the model still relies on them to steer it. Second, new language versions lack real-world code examples, so older patterns dominate and skew the model’s predictions.Until those two problems are solved, vibe coding will stay unreliable, especially when security matters.Great Use Cases for Vibe CodingVibe coding is an excellent tool for creating simple apps and websites faster, cheaper, and with more creativity than before. Many services like Cursor, Bolt, Replit, Lovable, and various AI chatbots like Claude already help anyone develop sites and apps, so long as limited flexibility is not an issue. Quickly and cheaply generating apps and sites with vibe coding is great for creative agencies or entrepreneurs with an innovative vision, so long as no long-term support is needed. Ideas that grow into successful businesses can hire software engineers to create a new codebase that scales, is secure, and remains maintainable. But that handoff isn't always smooth. Replacing AI-generated code often means starting over because the foundation isn't built for extensibility. What starts as a shortcut can become a constraint once real growth begins.Companies can also use vibe coding, but they must have the code and prepare it for model training by providing extensive documentation and code comments that describe how functions, classes, and modules work. Otherwise, the model has no reliable reference point — it starts generating based on patterns. And that's where things fall apart fast: vague outputs, unexpected behaviors, and hours lost trying to reverse-engineer your system.]]></content:encoded></item><item><title>10 Things I Wish I Knew About Product Management Sooner</title><link>https://hackernoon.com/10-things-i-wish-i-knew-about-product-management-sooner?source=rss</link><author>Aditya Vikram Kashyap</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:41:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Think product management is all strategy and shipping features? Think again. This brutally honest, laugh-out-loud guide dives into the real lessons PMs learn the hard way—like saying 'no' without becoming the office villain, surviving roadmap chaos, and fixing the feature no one asked for but everyone hates. Written by someone who’s been through the fire (and the Slack threads), it’s part therapy, part war journal, and all heart. If you've ever found yourself explaining why nothing shipped this sprint while silently questioning your career choices—this one’s for you. Expect awkward user interviews, polite panic, and very real truths about being the calm in the storm.]]></content:encoded></item><item><title>Growth Without Control Can Wreck Your Business</title><link>https://hackernoon.com/growth-without-control-can-wreck-your-business?source=rss</link><author>Engjell Rraklli</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:36:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Imagine you’ve been running your service-based business for some time.You have your customers, a structure up and running and you are already making some money because people like your service.Where do you go from there?This was exactly the question I asked myself and my co-founder. We had been running division5 for 4 years. Everything was stable, we had our recurring customers and we were able to provide a good service.We hit a ceiling though. We were not able to grow beyond that, we simply did not know how.That was the point where we started seeking new partnerships.Partnerships as a growth means.Partnerships were the first way that came to my mind regarding growing my business. We wanted to go in new markets and the best way to do that is through a partner which is local to that market.Here are a few reasons why :Local partners have access to connections and people you would otherwise not have.Local partners speak the native language and are culturally similar to your potential customers.It’s easier to create trust if someone local would vouch for you.Local partners can help you with your pricing strategy as they know the market better than you do.Please leave this field empty.In our case, we struck a deal with Pirate.They not only had a great brand in Germany, they had been also running one of the coolest business conferences in Europe creating a vast network of entrepreneurs and companies.The problem with partnerships though is that you don’t really know how it’s going to go. In the best case scenario, everything would go as intended but there are many reasons why the partnership would fail.Mismatch in values between the companies / shareholders.Misalignment in expectations.Communication problems between partners.So as with every long-term decision, think carefully. In our case, we took our time to fine-tune our agreement, a few months actually. In the end, I am happy with the deal we did and how everything transpired.Partnerships though take control away from you. Your growth depends on a third party. So I would heavily suggest you don’t keep it as your only means of growth.Networking, networking, networking.Yup, that’s right, in the B2B world networking is almost everything.We tend to think that businesses deal with other businesses but in reality, it’s people who deal with other people.In many cases, the decision on who to deal with is based on who you know or / and who you like.Sounds unfair but it’s human nature.So who you know plays a huge difference.  Everyone needs to network and this comes from someone was not a good networker.When I first started, I was anxious to talking to people. I think for some people it comes naturally while for many others, it’s a skill to be learned.So if networking does not come naturally for you either, don’t despair. See it as a skill. To get better at it, you need to practice it more. Show up in events. Talk to new people. Do this for some time and you will soon start seeing the results.I eventually got better and if I could do that, so can you.Networking is great, it really is, it takes a long time though to start seeing results. So while opposed to partnerships, you have more control, it won’t lead to growth. You need marketing / sales processes in place to achieve that.When people hear marketing magic comes to mind.It’s the classic word people use for many different things but few really know the meaning.I like to see marketing as the process that gets people at your door.How you do that can vary. Some people go and knock from door to door spreading the word. Other people do some posts on social media. Many companies run ads on TV. The sole purpose of marketing is getting people interested in your service.Sales on the other hand, is the process that gets people in the building.The sole purpose of sales is getting people to sign the contract and become a customer.If you want to control growth, you need to create effective marketing campaigns that reach many people and have an effective sales process that converts them into customers.Back in the day, this usually meant paying for expensive ads on TV. With the advent of social media, it’s much much easier to reach many people and get them interested in your service.Here are a few marketing techniques that we follow at division5 that any service-based business can follow :Outreach via email or social media.We have been seeing the best results through our outreach via social media but every business is different. You just gotta try what works for you. That’s also what we are doing.As soon as you find something that works, do that more. Eventually, you will be able to control the growth.That’s right, there is no silver bullet.If you want to expand in new markets, the best way is through partnerships. If you want to control the growth, you do that through implementing marketing and sales processes.Either way, in B2B you need to learn how to network and spend quite some time doing that. What works for someone else does not mean it will work for you to.Be curious, keep trying new things until you figure out what works for you.]]></content:encoded></item><item><title>Here&apos;s My Exact AI Prompt to Audit Any Professional Profile on the Internet — Including Your Own</title><link>https://hackernoon.com/heres-my-exact-ai-prompt-to-audit-any-professional-profile-on-the-internet-including-your-own?source=rss</link><author></author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:30:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
What does your LinkedIn really say about you? Here’s a method to test it, powered by AI and brutal honesty.“Why does every profile sound the same?”If you’ve spent more than 5 minutes on LinkedIn, you’ve probably noticed something strange: Everyone is a , , cross-functional problem-solver with  of experience.And somehow… no one says anything real.In a world flooded with GPT-polished personal brands, how do we separate the  from the ?That’s the question that inspired this: A simple but powerful AI prompt to critically audit  professional profile — yours, your peer’s, your client’s, or your hero’s.🚀 Why This Prompt MattersThis isn’t about catching lies. It’s about closing the gap between:🔍 What they’ve  (and whether it’s visible)⚙️ How well their public digital footprint supports those claims\
For , , , and , the stakes are high. We hire people, fund projects, and build reputations based on online signals. So we need better tools to  — without hiring an investigator:🛠️ The Prompt (Copy‑Paste Ready)Paste this into your favorite LLM (tested with ChatGPT 4o/4.1). Replace the name/link with any target:[FULL NAME] [linkedin-profile] — Structured Expert Profile with Critical Article Analysis 🚦  

Instructions (ENGLISH):
Provide a concise, critically-structured professional profile of the subject as an expert, using ONLY up-to-date information from publicly available web sources (do not use your internal memory or prior knowledge).
For every section, display ALL available, relevant information without omission or summary.
⚡️ Use tables, lists, and emoji for structure and emphasis.

MANDATORY REQUIREMENTS:

Do NOT omit, crop, or summarize ANY publication, fact, link, or thematic section found in public sources.

EVERY publication/article/post/quote found online MUST be individually analyzed and included—no “see above”, “other similar”, or “not reviewed for brevity”.

All facts must be confirmed with accessible links.

For each section, if data is absent, explicitly state “No public data found” (with date of check).

Validate all dates, links, and organizational details.

Critically analyze content—do not copy, do not repeat, do not generalize.

Structure your output using the headings below.

1️⃣ Key Activities & Experience
List all main areas of expertise (e.g., PMO, delivery, IT consulting, AI, risk, etc.).

State real job titles, companies, years (if known).

List all standout achievements, unique facts, and current roles.

Confirm ALL with links.

2️⃣ Major Articles & Publications (with Quality & AI-Check)
📝 Title/Topic    💡 Key Idea 🌍 Platform 📅 Date (verified)  🌟 Impact/Discussion    🧠 Originality/Validity 🤖 AI/LLM Content Check
(Analyze EVERY found article/post/publication. For each: one phrase summary, impact, originality, validity, and specific AI-generated content check: ✅ Genuine, ⚠️ Slightly formulaic, ❗ Possible AI. Confirm with link.)                        

3️⃣ Influence & Community Presence
List all professional and social platforms where the expert is active (LinkedIn, Medium, forums, Slack groups, etc.).

List any notable engagement, viral posts, peer comments/quotes (with source).

Mention roles in professional communities, boards, or online groups.

4️⃣ Expertise Assessment & Value
3–5 bullets: reputation, originality, strengths/weaknesses, audience, practical value.

Explicitly mention any “red flags” on originality, credibility, or suspected AI content.

Fact-based, no generalizations.

5️⃣ Collaborations, Events & Certifications
List ALL professional collaborations (projects, joint publications, open source, partnerships).

List ALL conference presentations, panels, podcasts, workshops, juries (date, topic, platform, link).

List ALL professional certificates and courses (with date, organizer, validation link if possible).

Explicitly note any absence of public evidence.

6️⃣ Web & Media Footprint
List EVERY instance where the expert is mentioned outside their own channels:

Third-party articles, reviews, interviews, analytics, “top experts” lists, company/industry sites, media, podcasts, YouTube, SlideShare, ResearchGate, etc.

For each, include link, date, context, and a brief summary.

Check for independent citations and discussions of their work.

Note: If none found, explicitly state this.

7️⃣ Academic & Teaching Activities (optional)
List any teaching, mentoring, course design, scientific or academic publications, lectures, or participation in educational projects (dates, topics, links).

If nothing found, state so.

Technical Reminders:

DO NOT summarize or omit ANY discovered item, however minor.

ALWAYS provide validated links and dates.

If a claimed certificate/publication cannot be independently verified, mark as ⚠️ “Unverified”.

Structure all lists and tables for fast reading; add emojis for clarity.

OUTPUT HEADINGS:
1️⃣ Key Activities & Experience
2️⃣ Major Articles & Publications (with Quality & AI-Check)
3️⃣ Influence & Community Presence
4️⃣ Expertise Assessment & Value
5️⃣ Collaborations, Events & Certifications
6️⃣ Web & Media Footprint
7️⃣ Academic & Teaching Activities

If any section yields no results, explicitly write:

“No public data found as of [date of check].”
🎯 What You Get (If Used Well)A , no-BS profile analysisDetected red flags, gaps, and unverifiable claimsA real sense of what the person’s  doing, not just sayingSignal on originality — whether posts look human or AI‑templatedSurface-level brand vs. deep, verifiable contribution🧑💻 Who This Is Useful For🧩  – evaluating candidates based on real case studies and traceable outcomes📦  – assessing consultants, mentors, and subject-matter experts beyond buzzwords📊  – verifying collaborators and public figures in AI, research, and analytics🧑💼  – filtering inflated profiles and focusing on demonstrable expertise🪞 Content Creators & Professionals – auditing their own digital footprint to improve credibility🤖 What AI Still Can’t Fake (But Tries)When you run this prompt on someone (or yourself), look out for:Articles with  vs. SEO word soupProjects with timelines, roles, and outcomes, not just jargonPosts that show , not just “here’s my new blog post” that can’t be verifiedMedia mentions outside self-posted networks\
If the AI comes back empty or vague — that’s the story, too.Here’s the brave part: Paste your own profile link into the prompt and read the result.If it feels… flat — good. That’s data. Now improve what matters, not just your headline.🌐 Why This Should Be Standard PracticeWe’ve normalized a world where people claim “AI Strategy”, “Leadership Transformation”, or “Researcher” in one paragraph, then share Canva carousels in the next.This prompt is an invitation to make things real again.To bring back credibility — not by gatekeeping, but by showing what’s , , and .If you found this useful:Let’s make profiles worth reading again.]]></content:encoded></item><item><title>Here’s Everything I Learned Building a Bitcoin Wallet from Scratch in 4 Months</title><link>https://hackernoon.com/heres-everything-i-learned-building-a-bitcoin-wallet-from-scratch-in-4-months?source=rss</link><author>Tristan Bietsch</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:26:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I spent 4 months building a Bitcoin wallet solo, live-streaming (on twitch and twitter) the entire build from wireframe to finish to an audience of 3. It’s janky, it works, and I learned more about engineering — and myself — than any tutorial could teach.: This wallet is testnet-only and in ALPHA. Do NOT use it with real Bitcoin. It’s a learning project, not production software.I previously founded BitEscrow (a non-custodial Bitcoin escrow service) and worked on it for a very long time. My co founder and I shipped something amazing. But after 3 long, hard, bootstrapped years I learned a really hard lesson: nobody wants to spend their Bitcoin. After that startup wound down, I did what any reasonable person would do — I binged  for 270 hours… in a .My friend with a PhD in computer science, who has 5000+ hours in the game, mentored me through Factorio at its deepest level, and I was totally hooked. But while playing, something clicked. The game’s systems thinking, optimization patterns, and relentless focus on efficiency rewired how I approach engineering problems. I realized Factorio is just a gamified version of actual systems engineering, and playing it had genuinely sharpened a skill I didn’t fully appreciate I had. When I emerged from my Factorio haze, I knew I wanted to build something substantial again.After taking some time off and well rested, I learned mobile development in 2 weeks and started building a Bitcoin wallet from absolute scratch as an MVP for a bigger project. I’m pivoting to infrastructure again, and this wallet was my technical deep dive before the jump. I originally planned this as an MVP for a product idea. Build wallet → grow waitlist → pitch investors. Classic playbook. But four months of solo development changed my perspective.As the weeks turned to months, I realized:My idea was too niche (BitEscrow 2.0 mistake territory)True market demand wasn’t thereI missed building infrastructure more than consumer productsInfra makes way more money when done correctlyMost importantly, a friend helped me re-frame my thinking of said idea entirely when I was at Bitcoin Las Vegas. By month three, I knew this wallet would be my graduation project — not my destination.I set a hard deadline: 4 months from first commit. Full kanban board. Ship or die. Two months in, I almost quit. The kanban overflowed with features, nothing worked properly, and those early dopamine hits from small wins had evaporated. I seriously considered writing a post-mortem about my failed wallet instead. Then something snapped — in a good way. I realized perfection was optional. Momentum wasn’t.Feature creep nearly killed this project. My kanban had everything: Bitcoin price screens, USD/sats converters, beautiful animations. All nice-to-haves that blocked real progress. Take the Bitcoin price feature I had planned — I kept hitting CoinGecko’s rate limits, so I engineered this elaborate solution: multiple API fallbacks, which led to orchestrated price averaging, centralized state management. Then I had an epiphany — just delete the entire feature.Removing USD conversion eliminated complexity across the entire codebase:No conversion logic in SendAmountScreen.tsx, SendConfirmScreen, InvoiceScreen.tsx, ReceiveScreen.tsxNo storing historical Bitcoin prices for transaction detailsSimpler state management overallI systematically blocked everything non-essential:UI smoke tests (I tested edge cases manually anyway)Branding and fancy animationsComponent refactoring and modularizationSecurity hardening beyond basicsRefactors of certain files in GoOne feature I desperately wanted: Jibberlink for air-gapped transaction signing. Would’ve been cool. Would’ve taken too long. Cut.Here’s what actually matters in a Bitcoin wallet: sending and receiving. That’s it. When I finally built a transaction that passed all UTXO validation, saw that success screen, and watched it confirm on the mempool — that dopamine rush was unreal. I built that. From scratch. Just raw transaction construction.Systems Thinking and AI PartnershipSomewhere around month three, my brain shifted. I stopped thinking in code and started thinking in systems. I’d lie in bed visualizing the wallet’s architecture — not files and functions, but flows and rules. The wallet transformed from “stuff that works” to “a thing with internal logic.”AI became less of a tool and more like a temporary co-founder. I wasn’t just writing prompts — I was thinking in prompts. Prompting is just software architecture in disguise — and thanks to Factorio, that’s a skill I had sharpened. I’d architect the flows, and Claude/Cursor would handle the implementation. It wasn’t perfect, but it gave me a second wind when energy faded. I got  good at prompting.The last week was pure triage. Days blurred together: debugging one minute, rewriting UI the next, then losing hours to obscure Expo polyfills. Context-switching became my primary skill. I kept wanting to refactor. Reorganize files. Create neat folder structures. But that was procrastination dressed as progress. I had to wear all the hats and feel the weight of each one. When I finally committed that last bugfix and watched my transaction go through — signed, broadcasted, confirmed — I let it stay ugly. Because ugly is honest, and shipping is beautiful.I researched extensively, using Blue Wallet as inspiration. Every library choice had a purpose: — The Bitcoin Swiss Army knife — Cross-platform mobile development — Because Vanilla JavaScript sucks — State management that is actually good — HD wallets (hierarchical deterministic) — Mnemonic seed phrases — Additional cryptographic operations (primary) with  fallback with custom rate limiting for fast key-value storage for data fetchingFull send/receive with dedicated hooks (, )Transaction history with complete validation pipelineMulti-network support (testnet, mainnet, regtest)HD wallet generation and importMempool.space integrationNo tutorial, course, textbook, or bootcamp could’ve taught me what this build did. Four months of solo development beats four years of theory. Using “Programming Bitcoin” as my bible, I came to appreciate Bitcoin as the marvel of engineering it truly is. The wallet works. It’s not pretty. But I shipped it, and that’s what matters.What I Learned (Technical)Building a wallet solo is really hard. Don’t do it unless you’re prepared for pain. Do it if you want to level up fast.Key generation, signing, and address derivation will humble you quicklyUTXO handling is where theory meets reality (and usually loses)BitcoinJS is powerful but raw. With a team, I’d use BDK + LDK instead.State management in financial apps needs paranoid-level attention“Securely storing private keys” is harder than yelling “not your keys, not your coins” on TwitterXcode remains the worst development experience in modern computingAI pair programming is the future — it makes good devs better, and bad devs worse.How to run, use, and interface with Bitcoin Core — and why it’s still the gold standard for full nodes.Segwit is actually brilliant once you implement it yourselfWhat I Learned (Personal)After 4 months of solo development, streaming to empty rooms, and wrestling with cryptographic primitives:Perfection is the enemy of momentum. Ship ugly.Execution is the ultimate filter. Ideas are easy, 4-month commitments aren’t.You can do hard things alone. But you probably shouldn’t.Persistence beats everything else.The deepest lesson? I’m an infrastructure engineer at heart. Those months of micro-adjusting UI pixels confirmed what I already knew — I belong in the backend, architecting systems, not pixels for a UI.This wallet works. It’s ugly, under-tested, and desperately needs refactoring. Could I perfect it? %. Will I? I have bigger projects calling my name. This wallet served its purpose: deepening my Bitcoin knowledge, sharpening my engineering skills, and clarifying what I actually want to build.If nothing else, I hope this helps one person understand how Bitcoin wallets actually work under the hood. The architecture is clean, the code is organized, and it’s all there for you to explore, break, and learn from.]]></content:encoded></item><item><title>22 Jobs That AI Will Create (That No One’s Talking About)</title><link>https://hackernoon.com/22-jobs-that-ai-will-create-that-no-ones-talking-about?source=rss</link><author>Paul Dhaliwal</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:20:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Discover 22 new AI jobs of the future—roles in trust, integration, and taste that humans will own as AI transforms the workforce. Future-proof your career.]]></content:encoded></item><item><title>Here&apos;s Why Product and Design Operate Better as One Team</title><link>https://hackernoon.com/heres-why-product-and-design-operate-better-as-one-team?source=rss</link><author>Kunal Abichandani</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:19:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AI startups are operating at speeds faster than ever before. Teams are always experimenting, learning, and building in places where there’s rarely a clear playbook. If you want to win, you need to stay nimble, move fast, and, above all, keep everyone on the same page.Some of the fastest-growing companies in AI today are proof:Cursor: $0 to $100M ARR in 21 months with just 20 peopleBolt: $0 to $20M ARR in 2 months with 15 peopleLovable: $0 to $10M ARR in 2 months with 15 peopleMidjourney: $0 to $200M ARR in 2 years with 10 peopleMercor: $0 to $50M ARR in 2 years with 30 peopleThe reason this has become possible today is because AI is collapsing the boundaries between roles.From my own experience running product and design at a high-growth AI startup, Rilla, I’ve learned that the best outcomes come when these two roles converge. Not just working closely. But thinking as one: solving problems, crafting experiences, and shaping product direction in a unified way. In these environments, you don’t need to “hand off” a spec or “loop in” design late in the game, because the thinking happens together, from the start.Building Faster by Staying AlignedOne of the biggest unlocks I’ve seen is just getting product and design closer to the customer. Not through decks or market research, but by actually talking to people. Sitting in on support calls, visiting customers IRL, and testing new ideas alongside customers. When you hear what’s confusing or frustrating in someone’s own words, it lands differently.At Rilla, every engineer spends one week doing only support. During this time, they don’t ship code or attend engineering meetings. They talk to customers, field issues, and experience pain points firsthand. Some of our best product ideas have come out of these support weeks. It’s often the week engineers come away saying, “Why didn’t we build this sooner?”We’ve also seen how alignment between product and design directly impacts our speed. There have been multiple instances where we’ve designed and shipped a new feature in a single day, from idea to implementation, because the people involved shared full context from the start. There’s no need for lengthy specs or back and forths. We spot a problem, discuss it together, sketch it out, and ship.The more traditional companies we’ve worked at treated product and design as distinct lanes: product defined what to build, and design figured out how it should look and feel. That separation often created friction. Shipping took longer, context was lost, and the end result often missed the mark. Despite these issues, that setup might be fine when you’re working on well-scoped features. But with AI products, where behavior changes weekly and user expectations are still being defined, that gap becomes a real liability.What has worked better is treating product and design as one role with shared context and ownership. Over time, this builds instinct. You start to see patterns earlier, make faster calls, and avoid overthinking things that don’t matter. It’s not perfect, but it’s closer to how real progress happens when you’re building in a space that’s constantly shifting.Teams like Linear have fully embraced this model, deliberately structuring their organization so that engineers and designers own the core product work. As CEO and co-founder Karri Saarinen puts it, “No product managers, just a head of product. PM duties are distributed across engineering and design.” It’s a clear bet on tight, multidisciplinary teams that build with context, not layers.Taste as a Core Product AdvantageIn today’s AI-powered landscape, baseline functionality is expected. Most products “just work.” What separates the forgettable from the beloved is taste. Design can’t be a final layer or a polish pass; it has to be embedded from the very beginning.“When somebody unwrapped that box and saw somebody gave a sht about me, I think that’s a spiritual thing… it came from a place of love and care.”*That kind of care is what makes a product feel intentional. It shows up in small, thoughtful moments. A helpful message timed just right, a smooth interaction that anticipates confusion, or the decision not to add a feature that clutters the experience. These aren’t surface-level choices. They come from shared ownership and instinct, when product and design think as one, not in handoffs.In my experience, teams that prioritize taste make better calls, faster. They focus less on checking boxes and more on how something will actually feel in the hands of a user. And in a world where AI handles more of the building, that kind of human judgment becomes the real differentiator.The Evolution of the Product Builder RoleAI startups are starting to hire very few PMs, if any at all. Instead, product responsibilities are shared across the team. Designers are sitting in on customer interviews, engineers are helping shape feature priorities, and everyone is involved in defining what success looks like.This shift isn’t just about leaner teams. It’s about moving faster by reducing handoffs and increasing ownership. When the people building the product also understand the “why” behind each decision, they don’t just complete tasks—they care about the outcome. They make smarter calls, notice edge cases sooner, and adjust quickly when things change.AI tools like Cursor and Figma Make make this even more accessible. Even if a designer isn’t fluent in code, they can ask AI to tweak a hover state or fix spacing—and the code updates itself. The feedback loop is much tighter and so the product gets better, faster.This sentiment has been echoed by Brian Chesky, CEO of Airbnb, when he publicly announced that he was restructuring roles within his company. He said, "We got rid of the classic product management function… We elevated design to be alongside product, so it's engineering, design, and product." This approach emphasizes the importance of multidisciplinary collaboration and shared ownership in product development.The Future of Product TeamsPeeking ahead, you’ll see the lines between product, engineering, and design blur even more, thanks largely to AI tools that speed everything up. Designers might pull components straight from a shared library and drop them into production. Engineers could dive into design systems and lay out entire user flows without waiting around for a mockup.This shift will let teams collaborate seamlessly and own the product from end to end. Designers, engineers, and product folks will all be writing specs and documentation together, each contributing directly to how the product takes shape.As AI keeps automating parts of design, writing, research, and development, everyone can focus on what they do best. The “product builder” of tomorrow will be a hybrid: part engineer, part designer, part product thinker. One moment they’re tweaking a model, the next they’re sketching a new feature or digging through user metrics. It’s messy, often overwhelming, and not always clear where the work should start. But when it clicks -- when the right person is close to the problem and empowered to solve it -- the results are fast, focused, and far more impactful.]]></content:encoded></item><item><title>Google&apos;s Data Center Energy Use Doubled In 4 Years</title><link>https://hardware.slashdot.org/story/25/07/01/221237/googles-data-center-energy-use-doubled-in-4-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: No wonder Google is desperate for more power: The company's data centers more than doubled their electricity use in just four years. The eye-popping stat comes from Google's most recent sustainability report, which it released late last week. In 2024, Google data centers used 30.8 million megawatt-hours of electricity. That's up from 14.4 million megawatt-hours in 2020, the earliest year Google broke out data center consumption. Google has pledged to use only carbon-free sources of electricity to power its operations, a task made more challenging by its breakneck pace of data center growth. And the company's electricity woes are almost entirely a data center problem. In 2024, data centers accounted for 95.8% of the entire company's electron budget.
 
The company's ratio of data-center-to-everything-else has been remarkably consistent over the last four years. Though 2020 is the earliest year Google has made data center electricity consumption figures available, it's possible to use that ratio to extrapolate back in time. Some quick math reveals that Google's data centers likely used just over 4 million megawatt-hours of electricity in 2014. That's sevenfold growth in just a decade. The tech company has already picked most of the low-hanging fruit by improving the efficiency of its data centers. Those efforts have paid off, and the company is frequently lauded for being at the leading edge. But as the company's power usage effectiveness (PUE) has approached the theoretical ideal of 1.0, progress has slowed. Last year, Google's company-wide PUE dropped to 1.09, a 0.01 improvement over 2023 but only 0.02 better than a decade ago. Yesterday, Google announced a deal to purchase 200 megawatts of future fusion energy from Commonwealth Fusion Systems, despite the energy source not yet existing. "It's a sign of how hungry big tech companies are for a virtually unlimited source of clean power that is still years away," reports CNN.]]></content:encoded></item><item><title>Nintendo’s Anti-Consumer Anti-Piracy Measures Also Reduce The Value Of The Switch 2</title><link>https://www.techdirt.com/2025/07/01/nintendos-anti-consumer-anti-piracy-measures-also-reduce-the-value-of-the-switch-2/</link><author>Dark Helmet</author><category>tech</category><pubDate>Wed, 2 Jul 2025 03:15:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[When it comes to the anti-piracy efforts taken by some of the more aggressive companies out there, such as Nintendo, the most frustrating part of the whole thing for me is just how completely short-sighted those efforts tend to be. Take Nintendo’s updated EULA for its Switch consoles, for example. The updated agreement makes several changes from its previous iteration, but the most notable is that Nintendo says that if it thinks you’re doing the piracy for any reason, it can suspend all kinds of services on your console, up to and including bricking it completely. And, while the company has yet to go the bricking route so far,  it has already begun suspending all online services on consoles for the use of MIG Switches, cards for Switch devices on which you can load legitimately extracted ROMs from purchased games, or pirated versions of the same.Now, the first layer of how this is short-sighted is easy enough to see. In order to engage in copyright protectionism, Nintendo is risking long-term reputational damage by functionally ruining the consoles of customers for actions that aren’t illegal, or even immoral. Short term protection, longer term risk of everyone thinking you don’t care about your own customers. But there’s another layer to this, as a result of these service suspensions being tied directly to the device rather than the person. And that is what this protectionism means for the secondary market for Nintendo Switches.“I was driving between work sites and stopped at two different Walmarts,” says user Bimmytung. “At the second one I find a Mario Kart edition sitting in the case and couldn’t believe my luck.” They were informed by the Walmart staff that it was an “open box return,” so it was removed from the box to be checked over, and all looked well. The code for the packaged Mario Kart World had been scratched off already, so Walmart knocked another $50 off the price, and it all seemed like a good deal. Until they got home.Finally after work I get a chance to set it up. Quickly realize I need the super special micro SD card and none of the ~half dozen in the house would work. Drive ten minutes to Target and get one there and pick up a few other accessories as well. Get home and go to finish the setup—quickly get Error Code 2124-4508. A quick Google search shows me I’m screwed. FML.”Now, there are several layers of shame here to go around. Shame on Walmart for selling a device without ensuring it would work for the buyer the way it is intended to work. And shame on Nintendo for creating an anti-piracy program such that the punishments meted out are linked to hardware rather than the supposed bad-actor it seeks to punish. But all of that aside, it should also be true that this sort of thing drives the value of a Nintendo Switch console lower than it would be otherwise. Part of the value you gain when you buy a physical thing is the ability to eventually put it on the secondary market at some point. Because of the actions that Nintendo is taking in disabling and/or bricking its own consoles, that injects a great deal of risk into the prospect of buying one on the secondary market. The value of the hardware is, by at least some measure, diminished.But because Nintendo seems to only think about these things in the short term, the company probably doesn’t much care.However, the more immediate issue is for those looking to pick up a Switch 2 from a reseller or previous owner, given their current scarcity at first-party sellers. There’s really no way of knowing at all if a console has been bricked when buying the device online, and this could make the resale market a complete shambles for the whole life cycle of the console. And, grimly, that’s not exactly a priority for Nintendo, given that reselling, either in store or online, gains the company nothing, and some would argue actually costs the company a sale—it’s not like it’ll be in a rush to address the problem.Which is why I won’t be in a rush to buy a Switch 2 anytime soon. And I’m certainly in their target market, having two young children who desperately want one. Instead of the console, however, they will be getting a lesson in making smart buying decisions as a consumer.]]></content:encoded></item><item><title>Laptop Mag Is Shutting Down</title><link>https://hardware.slashdot.org/story/25/07/01/2133224/laptop-mag-is-shutting-down?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Laptop Mag, a tech publication that began in 1991 as a print magazine, is shutting down after nearly 35 years. The Verge reports: Laptop Mag has evolved many times over the years. It started as a print publication in 1991, when Bedford Communications launched the Laptop Buyers Guide and Handbook. Laptop Mag was later acquired by TechMedia Network (which is now called Purch) in 2011 and transitioned to digital-only content in 2013. Future PLC, the publisher that owns brands like PC Gamer, Tom's Guide, and TechRadar, acquired Purch -- and Laptop Mag along with it.
 
"We are incredibly grateful for your dedication, talent, and contributions to Laptop Mag, and we are committed to supporting you throughout this transition," [Faisal Alani, the global brand director at Laptop Mag owner Future PLC] said. Laptop Mag's shutdown follows the closure of long-running tech site AnandTech, which was also owned by Future PLC. It's not clear whether Laptop Mag's archives will be available following the shutdown.]]></content:encoded></item><item><title>Apple Accuses Former Engineer of Taking Vision Pro Secrets To Snap</title><link>https://yro.slashdot.org/story/25/07/01/2128235/apple-accuses-former-engineer-of-taking-vision-pro-secrets-to-snap?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple has filed (PDF) a lawsuit against former Vision Pro engineer Di Liu, accusing him of stealing thousands of confidential files related to his work on Apple's augmented reality headset for the benefit of his new employer Snap. The company alleges Liu misled colleagues about his departure, secretly accepted a job offer from Snap, and attempted to cover his tracks by deleting files -- actions Apple claims violated his confidentiality agreement. The Register reports: Liu secretly received a job offer from Snap on October 18, 2024, a role the complaint describes as "substantially similar" to his Apple position, meaning Liu waited nearly two weeks to resign from Apple, per the lawsuit. "Even then, he did not disclose he was leaving for Snap," the suit said. "Apple would not have allowed Mr. Liu continued access had he told the truth." Liu allegedly copied "more than a dozen folders containing thousands of files" from Apple's filesystem to a personal cloud storage account, dropping the stolen bits in a pair of nested folders with the amazingly nondescript names "Personal" and "Knowledge."
 
Apple said that data Liu copied includes "filenames containing confidential Apple product code names" and files "marked as Apple confidential." Company research, product design, and supply chain management documents were among the content Liu is accused of stealing. The complaint also alleges that Liu deleted files to conceal his activities, a move that may hinder Apple's ability to determine the full scope of the data he exfiltrated. "Mr. Liu additionally took actions to conceal his theft, including deceiving Apple about his job at Snap, and deleting files from his Apple-issued computer that might have let Apple determine what data Mr. Liu stole," the complaint noted.
 
Whatever he has, Apple wants it back. The company demands a jury trial on a single count of breach of contract under a confidentiality and intellectual property agreement Liu was bound to. It also asks the court to compel Liu to return all misappropriated data, award damages to be determined at trial, and reimburse Apple's costs and attorneys' fees.]]></content:encoded></item><item><title>Tinder To Require Facial Recognition Check For New Users In California</title><link>https://yro.slashdot.org/story/25/07/01/2112208/tinder-to-require-facial-recognition-check-for-new-users-in-california?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Axios: Tinder is mandating new users in California verify their profiles using facial recognition technology starting Monday, executives exclusively tell Axios. The move aims to reduce impersonation and is part of Tinder parent Match Group's broader effort to improve trust and safety amid ongoing user frustration. The Face Check feature prompts users to take a short video selfie during onboarding. The biometric face scan, powered by FaceTec, then confirms the person is real and present and whether their face matches their profile photos. It also checks if the face is used across multiple accounts. If the criteria are met, the user receives a photo verified badge on their profile. The selfie video is then deleted. Tinder stores a non-reversible, encrypted face map to detect duplicate profiles in the future.
 
Face Check is separate from Tinder's ID Check, which uses a government-issued ID to verify age and identity. "We see this as one part of a set of identity assurance options that are available to users," Match Group's head of trust and safety Yoel Roth says. "Face Check ... is really meant to be about confirming that this person is a real, live person and not a bot or a spoofed account." "Even if in the short term, it has the effect of potentially reducing some top-line user metrics, we think it's the right thing to do for the business," Rascoff said.]]></content:encoded></item><item><title>Figma Files For IPO</title><link>https://slashdot.org/story/25/07/01/2058244/figma-files-for-ipo?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Figma has filed to go public on the NYSE under the ticker "FIG," marking one of the most anticipated IPOs in recent years following its scrapped $20 billion acquisition by Adobe. CNBC reports: Revenue in the first quarter increased 46% to $228.2 million from $156.2 million in the same period a year ago, according to Figma's prospectus. The company recorded a net income of $44.9 million, compared to $13.5 million a year earlier. As of March 31, Figma had 1,031 customers contributing at least $100,000 a year to annual revenue, up 47% from a year earlier. Clients include Amazon Web Services, Google, Microsoft and Netflix. More than half of revenue comes from outside the U.S. Figma didn't say how many shares it plans to sell in the IPO. The company was valued at $12.5 billion in a tender offer last year, and in April it announced that it had confidentially filed for an IPO with the SEC. [...]
 
Figma was founded in 2012 by CEO Dylan Field, 33, and Evan Wallace, and is based in San Francisco. The company had 1,646 employees as of March 31. Before establishing Figma, Field spent over two years at Brown University, where he met Wallace. Field then took a Thiel Fellowship "to pursue entrepreneurial projects," according to the filing. The two-year program that Founders Fund partner Peter Thiel established in 2011 gives young entrepreneurs a $200,000 grant along with support from founders and investors, according to an online description. Field is the biggest individual owner of Figma, with 56.6 million Class B shares and 51.1% of voting power ahead of the IPO. He said in a letter to investors that it was time for Figma to buck the "trend of many amazing companies staying privately indefinitely." "Some of the obvious benefits such as good corporate hygiene, brand awareness, liquidity, stronger currency and access to capital markets apply," wrote Field. "More importantly, I like the idea of our community sharing in the ownership of Figma -- and the best way to accomplish this is through public markets."
 
As a public company, Field said investors should "expect us to take big swings," including through acquisitions.
 
In April, Figma bought the assets and team of an unnamed technology company for $14 million, according to the filing. They also registered over 13 million users per month, one-third of which are designers.]]></content:encoded></item><item><title>Jon McNeill brings the operator’s playbook to TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/jon-mcneill-brings-the-operators-playbook-to-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 23:02:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At TechCrunch All Stage 2025, Jon McNeill, CEO and co-founder of DVx Ventures, will take the Scale Stage to flip the script on conventional startup growth advice.]]></content:encoded></item><item><title>Xerox Buys Lexmark For $1.5 Billion As Print Industry Clings To Relevance</title><link>https://slashdot.org/story/25/07/01/2247221/xerox-buys-lexmark-for-15-billion-as-print-industry-clings-to-relevance?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 23:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli shares a report from NERDS.xyz: In a move that feels straight out of a different era, Xerox has officially acquired Lexmark for $1.5 billion. The deal includes net debt and assumed liabilities, and it pulls Lexmark out of the hands of Chinese ownership and into a freshly restructured Xerox. That's a lot of money for a company best known for making machines that spit out paper.
 
According to Xerox, this is all part of a "Reinvention" strategy. The company now claims it will be one of the top five players in every major print category and the leader in managed print services. [...] Xerox says the new leadership team will include executives from both sides, and the combined business will now support over 200,000 clients in more than 170 countries. They'll also be running 125 manufacturing and distribution centers in 16 countries.]]></content:encoded></item><item><title>AMC Warns Moviegoers To Expect &apos;25-30 Minutes&apos; of Ads and Trailers</title><link>https://entertainment.slashdot.org/story/25/07/01/2052226/amc-warns-moviegoers-to-expect-25-30-minutes-of-ads-and-trailers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AMC Theatres now warns customers that movies start 25-30 minutes after the listed showtime to account for ads and trailers, "making it easier for moviegoers to know the actual start time of their film screening," reports The Verge. From the report: Starting today, AMC will also show more ads than before, meaning its preshow lineup may have to be reconfigured to avoid exceeding the 30-minute mark. The company made an agreement with the National CineMedia ad network that includes as much as five minutes of commercials shown "after a movie's official start time," according to The Hollywood Reporter, and an additional 30-to-60-second "Platinum Spot" that plays before the last one or two trailers.
 
AMC was the only major theater chain to reject the National CineMedia ad spot when it was pitched in 2019, telling Bloomberg at the time that it believed "US moviegoers would react quite negatively." Now struggling financially amid an overall decline in movie theater attendance and box-office grosses, AMC has reversed course, telling The Hollywood Reporter that its competitors "have fully participated for more than five years without any direct impact to their attendance."]]></content:encoded></item><item><title>Congress just greenlit a NASA moon plan opposed by Musk and Isaacman</title><link>https://techcrunch.com/2025/07/01/congress-just-greenlit-a-nasa-moon-plan-opposed-by-musk-and-isaacman/</link><author>Aria Alamalhodaei</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:30:59 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The $10 billion addition to the Artemis architecture, which includes funding for additional Space Launch System rockets and an orbiting station around the moon called Gateway, is a rebuke to critics who wished to see alternative technologies used instead.]]></content:encoded></item><item><title>The Moral Imperative Of Clear Language</title><link>https://www.techdirt.com/2025/07/01/the-moral-imperative-of-clear-language/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:17:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[I need to say something that will make many of you deeply uncomfortable: your refusal to call fascism “fascism” is not sophistication—it’s .When Donald Trump posts explicit orders for “REMIGRATION” and “Mass Deportation Operations” targeting American cities because they are “the core of the Democrat Power Center,” that’s not “controversial immigration policy.” That’s mass deportation directed against political opponents. When federal troops deploy against American civilians exercising constitutional rights, that’s not “enhanced law enforcement.” That’s military occupation. When the systematic dismantling of democratic institutions gets described as “political polarization,” that’s not nuanced analysis—it’s linguistic evasion that enables the very thing it refuses to name.The sophisticates hate this clarity. They prefer the safety of euphemism, the comfort of complexity that never quite arrives at moral judgment. They speak of “concerning developments” and “troubling trends” while democracy burns around them. They perform nuanced understanding while fascism consolidates power through their very refusal to name it.But here’s what they don’t understand: authoritarianism thrives in ambiguity. It requires linguistic fog to operate. It depends on our unwillingness to call things by their proper names. Every euphemism is a small surrender. Every hedge is a tiny collaboration. Every refusal to speak plainly is a gift to those who profit from confusion.Language shapes consciousness. When we refuse to name what we see clearly, we don’t just fail to communicate—we erode our collective capacity to think clearly, to feel appropriately, to respond effectively. We make ourselves complicit in our own moral disorientation.George Orwell understood this when he wrote that “political language is designed to make lies sound truthful and murder respectable, and to give an appearance of solidity to pure wind.” But he was describing propaganda techniques used by totalitarian regimes. What we face now is worse: the voluntary adoption of euphemistic language by people who should know better, who pride themselves on seeing clearly, who claim to defend democratic values.We are doing the propagandists’ work for them.Consider how this linguistic distortion operates in practice. When mass deportation operations targeting millions of people get called “immigration enforcement,” we’re not being diplomatic—we’re making state violence psychologically easier to accept. When systematic attacks on democratic institutions get labeled “political disagreements,” we’re not showing balance—we’re normalizing authoritarianism. When obvious lies get treated as “alternative perspectives,” we’re not being fair—we’re weaponizing false equivalence against truth itself.The euphemism isn’t just descriptive failure—it’s moral failure. It changes how people process information, how they make decisions, how they understand their own moral obligations. When you call fascism “populism,” you’re not just using imprecise language. You’re making it easier for people to support fascism without confronting what they’re supporting.Hannah Arendt spent her life studying how ordinary people enable extraordinary evil, and she identified linguistic evasion as one of the primary mechanisms. In , she showed how bureaucratic language—“evacuation,” “resettlement,” “special treatment”—allowed participants in genocide to avoid confronting the reality of what they were doing. They weren’t murdering children; they were “processing population transfers.” They weren’t operating death camps; they were managing “facilities for the final solution.”The language didn’t just hide the reality from others—it hid it from themselves. It allowed them to participate in evil while maintaining their self-image as decent, law-abiding citizens following proper procedures.Arendt’s insight was that evil becomes possible not primarily through active malice but through the refusal of ordinary people to see and name what’s in front of them. The “banality of evil” is fundamentally about linguistic evasion enabling moral evasion. When we stop calling violence violence, we make violence easier to commit.This is what we’re witnessing now. The systematic training of a population to see clearly but speak obliquely, to understand precisely but describe vaguely, to recognize authoritarianism but call it something else. We have become a society of people who know exactly what’s happening but lack the linguistic courage to say so.The Practice of Plain NamingConsider how this evasion plays out in our current discourse:We don’t say “Trump is implementing fascist policies.” We say “Trump’s approach raises concerns about democratic norms.”We don’t say “Republicans are supporting mass deportation operations.” We say “There are disagreements about immigration enforcement strategies.”We don’t say “Conservative media spreads lies designed to enable authoritarianism.” We say “Different sources present different perspectives on complex issues.”We don’t say “MAGA supporters have chosen to enable fascism.” We say “There are legitimate grievances driving political polarization.”Each euphemism makes the reality a little less clear, a little less urgent, a little less morally demanding. Each hedge creates space for people to avoid confronting what they’re witnessing or participating in. Each refusal to name plainly is a small act of collaboration with the forces that depend on confusion to operate.When Trump orders ICE to conduct “Mass Deportation Operations” in cities he identifies as “the core of the Democrat Power Center,” that’s not immigration policy—it’s the use of state violence against political opponents. When he calls for “REMIGRATION” of millions of people, that’s not border security—it’s forced population transfer. When federal agents separate families and detain children, that’s not law enforcement—it’s state-sanctioned cruelty.The defenders will say “the law is the law”—as if legality were equivalent to morality. But slavery was legal. Segregation was legal. Japanese internment was legal. Every authoritarian regime in history has operated through law, not despite it. “The law is the law” is not a moral position—it’s moral abdication disguised as principled governance.Law without moral foundation is just organized violence. Rules without ethical grounding are just systematized cruelty. When your only defense of a policy is that it’s technically legal, you’ve already admitted it’s morally indefensible.The Sophisticates’ ResistanceThe sophisticates will tell you that such plain language is “inflammatory,” “divisive,” “unhelpful to productive dialogue.” They’ll suggest that calling fascism “fascism” alienates potential allies, shuts down conversation, makes compromise impossible.But here’s what they’re really saying: they prefer the comfort of ambiguity to the responsibility that comes with clarity. They’d rather maintain the illusion of reasoned discourse than confront the reality that one side has abandoned reason entirely. They want to keep playing by rules that the other side has explicitly rejected.This isn’t sophistication—it’s cowardice. It’s the intellectual’s version of appeasing authoritarianism through linguistic accommodation. It’s the belief that if we just find the right words, the right tone, the right approach, we can somehow reason with people who have chosen unreason as their governing principle.But you cannot have productive dialogue with fascists about the merits of fascism. You cannot find common ground with people who reject the premise of shared reality. You cannot compromise with those who view compromise as weakness and good faith as stupidity.What you can do is name what they are doing clearly enough that people understand what’s at stake and what choice they face.The power of plain naming is that it forces moral confrontation. It makes people choose sides. It strips away the comfortable distance that euphemism provides. It demands that people acknowledge what they’re actually supporting rather than hiding behind sanitized language.This is why authoritarians work so hard to control language. They understand that linguistic precision is the enemy of moral confusion. That clear naming makes their projects harder to defend. That euphemism is their friend and clarity is their enemy.They want us to call their fascism “nationalism.” Their lies “alternative facts.” Their cruelty “tough love.” Their mass deportations “border security.” Their authoritarianism “law and order.”Every time we adopt their language, we do their work. Every time we refuse to name their actions plainly, we make those actions easier to defend, easier to rationalize, easier to continue.When we refuse to call fascism “fascism”, we don’t make fascism less dangerous. We make ourselves less capable of recognizing and resisting it. We participate in our own disorientation. We become accomplices to our own confusion.The courage to name things plainly is not the courage to be harsh or inflammatory. It’s the courage to accept the responsibility that comes with seeing clearly. It’s the courage to abandon the comfortable illusion of neutrality and acknowledge that some things cannot be straddled, some positions cannot be hedged, some realities cannot be euphemized away.To say that systematic deployment of federal troops against American cities constitutes military occupation is not inflammatory—it’s accurate. To say that mass deportation operations targeting political opponents constitute fascist policy is not hyperbolic—it’s precise. To say that obvious lies designed to enable authoritarianism are lies is not divisive—it’s necessary.The alternative to plain naming is not diplomatic nuance—it’s moral blindness. It’s the systematic erosion of our capacity to recognize authoritarianism when it appears in familiar forms, speaking familiar languages, wearing familiar clothes.Evil depends on our unwillingness to call it evil. Fascism depends on our refusal to call it fascism. Lies depend on our treatment of them as “alternative perspectives.” State violence depends on our description of it as “tough policy choices.”The moment we name these things plainly, we restore the moral clarity that makes effective resistance possible. We acknowledge what we’re actually facing. We accept the responsibility that comes with seeing clearly. We choose truth over comfort, accuracy over diplomacy, moral clarity over intellectual sophistication.This is not just a linguistic choice—it’s a moral one. Every time we speak plainly about what we’re witnessing, we strike a blow against the forces that depend on confusion to operate. Every time we call fascism “fascism”, we make fascism a little harder to defend. Every time we name state violence as state violence, we make such violence a little less acceptable.Two plus two equals four. There are twenty-four hours in a day. And Trump’s mass deportation operations are fascistic displays of state violence targeting political enemies whether we have the courage to call them that or not.The difference is not in the reality—the difference is in our capacity to respond to reality appropriately.Name it plainly. Not because it’s easy, but because it’s true. Not because it’s comfortable, but because comfort in the face of authoritarianism is itself a form of collaboration. Not because it’s diplomatic, but because diplomacy with fascists is enabling fascism.The revolution is linguistic honesty. The rebellion is calling things by their proper names. The resistance is refusing to participate in the euphemistic erosion of moral clarity.Say what you see. Name what you know. Call fascism .Every minute of every day.Remember what’s real. Because the alternative to naming fascism clearly isn’t moderation or diplomacy—it’s surrender.Mike Brock is a former tech exec who was on the leadership team at Block. Originally published at his Notes From the Circus.]]></content:encoded></item><item><title>ICEBlock, an app for anonymously reporting ICE sightings, goes viral overnight after Bondi criticism</title><link>https://techcrunch.com/2025/07/01/iceblock-an-app-for-anonymously-reporting-ice-sightings-goes-viral-overnight-after-bondi-criticism/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:09:38 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The citizen app for anonymously reporting ICE agents and raids went viral after criticism from the U.S. attorney general.]]></content:encoded></item><item><title>EFFecting Change: EFF Turns 35!</title><link>https://www.eff.org/deeplinks/2025/06/effecting-change-eff-turns-35</link><author>Melissa Srago</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/effectingeffturns35_banner.png" length="" type=""/><pubDate>Tue, 1 Jul 2025 22:09:36 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[We're wishing EFF a happy birthday on July 10! Since 1990, EFF's lawyers, activists, analysts, and technologists have used everything in their toolkit to ensure that technology supports freedom, justice, and innovation for all people of the world. They've seen it all and in this special edition of our EFFecting Change livestream series, leading experts at EFF will explore what's next for technology users.Want to make sure you don’t miss our next livestream? Here’s a link to sign up for updates about this series:eff.org/ECUpdates.]]></content:encoded></item><item><title>Amazon Deploys Its One Millionth Robot, Releases Generative AI Model</title><link>https://hardware.slashdot.org/story/25/07/01/2046242/amazon-deploys-its-one-millionth-robot-releases-generative-ai-model?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: After 13 years of deploying robots into its warehouses, Amazon reached a new milestone. The tech behemoth now has 1 million robots in its warehouses, the company announced Monday. This one millionth robot was recently delivered to an Amazon fulfillment facility in Japan. That figure puts Amazon on track to reach another landmark: Its vast network of warehouses may soon have the same number of robots working as people, according to reporting from The Wall Street Journal. The WSJ also reported that 75% of Amazon's global deliveries are now assisted in some way by a robot. Amazon also unveiled a new generative AI model called DeepFleet, built using SageMaker and trained on its own warehouse data, which improves robotic fleet speed by 10% through more efficient route coordination.]]></content:encoded></item><item><title>Figma moves closer to a blockbuster IPO that could raise $1.5B</title><link>https://techcrunch.com/2025/07/01/figma-moves-closer-to-a-blockbuster-ipo-that-could-raise-1-5b/</link><author>Julie Bort</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:55:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The financials are impressive and  founder CEO Dylan Field already cashed out $20 million worth of shares last year.]]></content:encoded></item><item><title>$70M Committed To Boba Network As Foundation Concludes BOBA Token Agreement With FTX Recovery Trust</title><link>https://hackernoon.com/$70m-committed-to-boba-network-as-foundation-concludes-boba-token-agreement-with-ftx-recovery-trust?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:43:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Grand Cayman, Cayman Islands, July 1st, 2025/Chainwire/--Boba Governance Foundation today announced a significant milestone with a $70 million capital commitment secured from Awaken Foundation and LDA Capital to fund the continued development and ecosystem expansion of Boba Network, the leading Layer-2 blockchain for AI-powered decentralized applications (dApps), enabled by its unique HybridCompute technology. The Foundation also announced an agreement with FTX Recovery Trust regarding the BOBA tokens held by the Trust. The $70 million capital infusion will serve as a catalyst for Boba Network's ambitious growth plans. The funding will be strategically allocated to bolster the network's core infrastructure, expand its developer ecosystem, and foster the creation of innovative decentralized applications (dApps) on the platform, with a particular focus on enabling AI-powered dApps."This funding will accelerate the development of the Boba Network ecosystem, attract top-tier talent, and drive the widespread adoption of Boba Network as a premier Layer-2 solution for AI-powered dApps. We are excited to collaborate with Boba Network partners to shape the future of the AI-powered, decentralized web," said Alan Chiu, CEO of Enya Labs, a core contributor to Boba Network.""This substantial capital commitment from Awaken Foundation and LDA Capital is a testament to the transformative potential of Boba Network," said David Acutt, director of Boba Governance Foundation.Awaken Foundation, a key advocate for decentralized infrastructure and digital sovereignty, sees Boba Network as a critical component in the next phase of Web3 evolution.“We are thrilled to support the Boba Governance Foundation in its pursuit of open innovation,”said Nattaphol Vimolchalao, Director at Awaken Foundation. “Boba’s ability to connect smart contracts with off-chain computation—especially AI—unlocks enormous potential across industries.”LDA Capital, known for backing high-growth tech ventures and digital asset ecosystems, echoed that sentiment.“Boba Network is building essential infrastructure for the future of decentralized computation,” said Warren Baker, Managing Partner at LDA Capital. “We believe Boba will play a pivotal role in scaling the next generation of intelligent dApps, and we’re proud to support their mission as they push the boundaries of what’s possible in blockchain technology.”The strategic partnership with Awaken Foundation and LDA Capital goes far beyond financial support. It represents a powerful alignment of vision, expertise, and global reach. Leveraging deep industry knowledge, business development capabilities, and an extensive network of strategic partners, both firms are uniquely positioned to accelerate Boba Network’s growth. This collaboration is set to strengthen Boba’s leadership in blockchain innovation and drive its next phase of global expansion. In addition, LDA Capital offers differentiated value through LDA Velocity, its institutional-grade liquidity and market-making platform that supports healthy, scalable token ecosystems across global exchanges.Infrastructure Enhancement: The funding will be used to strengthen Boba Network's infrastructure, ensuring high throughput, low latency, and robust security for users and developers.Ecosystem Expansion: A portion of the capital will be dedicated to expanding the Boba Network ecosystem by attracting developers, projects, and users through grants and educational initiatives.dApp Development: The funding will support the creation of innovative dApps on Boba Network, with a strong emphasis on AI-powered dApps, ranging from decentralized finance (DeFi) protocols to real-world assets (RWA) applications.Community Engagement: Boba Governance Foundation will continue to foster a vibrant and engaged community by providing resources, support, and opportunities for collaboration.Resolution with FTX Recovery TrustIn addition, Boba Governance Foundation has executed an agreement with FTX Recovery Trust whereby all the BOBA tokens held by the Trust have been transferred to the Foundation. FTX Recovery Trust, in addition to other consideration and mutual release of claims, received the right to purchase up to approximately 29.4M BOBA tokens from Boba Governance Foundation at $0.09 per token within the next 18 months.“This agreement represents a momentous milestone for Boba Network, as it removes a major source of uncertainty over the BOBA token and strengthens the Foundation’s ability to support the continued development of Boba Network and its ecosystem,” said Acutt.About Boba Governance Foundation is a non-profit organization dedicated to the advancement and growth of Boba Network. It supports the development of the network's technology, fosters community engagement, and promotes the adoption of Boba Network across various industries. Boba Network is the leading Layer-2 blockchain for AI-powered decentralized applications (dApps), enabled by its unique HybridCompute technology. is a private investment firm founded by seasoned crypto, venture capital, and public market investors. The firm seeks to invest in established blockchain protocols to help further develop its technology. Awaken provides strategic capital, accelerated business development, and engineered exits for protocols that Awaken believes have a promising future in the modern economy. is a global alternative investment group with expertise in cross-border transactions worldwide. The team has collectively executed over 350 transactions in both the public and private middle markets across 43 countries with aggregate transaction values of over USD $11 billion. LDA’s investment activities across Web3 include 27+ transactions totaling $400m+ in capital commitments.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Decentralized Public-Key Infrastructure: The Future of Supply Chain Security</title><link>https://hackernoon.com/decentralized-public-key-infrastructure-the-future-of-supply-chain-security?source=rss</link><author>Nneoma Uche</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:31:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[All products, digital and physical, pass through a supply chain—a network of actors that supports their life cycle. But as the global market becomes increasingly interconnected, supply chain attacks are on the rise.In March 2023, cybercriminals infiltrated 3CX’s build environment, injecting malicious code into a library file for its macOS and Windows desktop apps. The compromised file was distributed through official updates, exposing users to malware. This breach emphasized the shortcomings of centralized systems in securing supply chains, as a single compromised vendor can jeopardize the privacy of the entire customer base.Web3 supply chains, leveraging blockchain and Decentralized Public Key Infrastructure (DPKI), offer a robust alternative. By prioritizing transparency, traceability, and tamper-proof security, they present a stronger defense against supply chain threats.This article explores how DPKI in blockchain-driven networks outperforms traditional PKI, and why a supply chain powered by the latter presents a tougher nut for bad actors to crack.Understanding Public-Key Infrastructure (PKI)IBM defines ‘Public-Key Infrastructure (PKI)’ as a comprehensive framework used to assign and verify user identity through digital certificates, for secure digital communications. The entire PKI framework relies on asymmetric cryptography- the use of a public and private key pair to encrypt and decrypt data, respectively.PKI allows us to associate identities with particular key pairs. Although the public key can be visible to anyone on the network, only the entity with the corresponding private key can access specific features or information.In a supply chain, PKI combines digital certificates and asymmetric cryptography to establish trust and ensure integrity.  The public keys are embedded in a digital certificate, which authenticates the user or device communicating across the network.Certificate Authority (CA): The Certificate Authority is a trusted entity that issues digital certificates to participants in the supply chain, e.g., developers, analytics providers, payment gateways, cloud providers, etc.Certificate: Digital certificates are cryptographic credentials, issued and signed by the CA, used to verify the identity of and secure communication among supply chain actors. They typically include public keys and identity details, accessible upon request.Registration Authority (RA): The RA ensures that only authorized participants can obtain a digital certificate, thus enhancing security within the supply chain. The CA can double as the registration authority, although trusted third-party services are just as efficient.Certificate database: This PKI component is a secure repository or location that stores issued digital certificates, alongside their metadata, i.e., public keys, revocation status, and validity details.Certificate policy: This is a formal document outlining the procedures and requirements governing the issuance, usage and management of digital certificates within the supply chain.Central directory: The central directory is a public repository where cryptographic keys, digital certificates and Certificate Revocation Lists(CRLs) are indexed and stored. It enables anyone in the ecosystem to authenticate a digital signature and encrypt data to a specific key owner.Traditional Supply Chain Systems (Web2) vs. Web3-Driven Supply ChainsWeb2-based supply chains are centralized; participants rely on Certificate Authorities to verify other actors and establish trust. Moreover, digital certificates and cryptographic keys are stored in central directories, leaving room for supply chain attacks in the absence of robust security measures.Another feature of Web2-based supply chains is the opacity around certificate issuance and revocation. There’s no universal metric to determine eligibility for certificate issuance. Instead, entities must operate on a trust assumption that the CA has properly vetted the requester.Oftentimes, delayed updates and limited visibility associated with Certificate Revocation Lists (CRLs) may result in revoked certificates appearing valid to related devices or applications. This can impact supply chain integrity, due to unauthorized access, tampered goods, compliance issues and a loss of trust. In 2024, Google delisted Entrust (a formerly reputable Certificate Authority), from its Chrome Root Program due to malfunctions in its certificate issuance and revocation operations. A few months prior, Entrust admitted to misissuing over 26,000 digital certificates and failing to revoke them within the revocation timeline outlined by the Certificate Authority/Browser Forum.Web3-driven supply chains, on the other hand, leverage decentralized systems, smart contracts, and pseudonymous transactions to create a trustless, transparent, and secure ecosystem. Unlike traditional supply chains that rely on centralized authorities, Web3 enables each participant to interact directly on a shared blockchain, reducing intermediaries and single points of failure.In addition, PKI data (i.e. public keys and certificates) is stored immutably on a blockchain, making them nearly tamper-proof, while being easily accessible for verification. Together, these features make Web3 supply chains more resilient and trustworthy than ttheir raditional counterparts.From PKI to DPKI: Strengthening Supply Chain Integrity in Web3In Web3-driven systems, PKI implementation shifts from traditional Certificate Authorities (CAs) to decentralized or distributed models that align with Web3 principles, hence the term—Decentralized Public-Key Infrastructure (DPKI).The idea behind it is simple: enable tamper-proof verification of supply chain data and participants, without relying on a centralized database.Here's how DPKI enhances supply chain integrity in a Web3 ecosystem:Decentralized trust models manage authentication and verificationRather than a centralized entity, DPKI relies on a web of trust—a network of on-chain participants who collectively verify and authenticate information. Each supplier on-chain uses a decentralized identifier (DID), which functions as a unique digital signature, to prove authenticity, access data and sign transactions.Vendors interacting in a decentralized supply network use their DIDs to access proprietary data, verify purchase orders, and access secure channels. Similarly, a product within the chain can be assigned a DID, enabling participants to verify its origin and authenticity at every step.Smart contracts ensure transparency and data integritySmart contracts are self-executing programs stored on a blockchain that automatically trigger specific actions once preset conditions are met. They automate various processes within a supply chain, such as processing payments, issuing tickets, or approving shipments. All interactions with the contract are recorded on the blockchain, creating a permanent and tamper-proof audit trail. This record allows stakeholders to trace:The journey of goods through the supply chain.Compliance with standards at each stage.Discrepancies back to their source.Automation through digital contracts reduces the risk of fraud, human error, and compliance issues.Blockchain-powered scalabilityBy eliminating reliance on a Certificate Authority and other intermediaries, decentralized public key infrastructure offers more scalability to Web3-based supply chains. In traditional PKI, managing certificates across a complex supply chain can leave the supply network vulnerable to single points of failure. Moreover, scaling may require the involvement of multiple Certificate Authorities, likely resulting in delays or bottlenecks around certificate issuance and revocation. This approach is resource-intensive and may be unrealistic for global supply chains involving numerous entities.In contrast, Web3-driven supply chains leverage blockchain as a trust anchor, enabling a distributed system where records and identities are verifiable by all participants on the chain. The result is a more efficient, scalable infrastructure, tailored to the complexity of modern supply chains.Advantages of DPKI to (Web3) Supply ChainsEliminates Central Authority Risks: No single entity can compromise the supply chain.Self-Sovereign Identity: Supply chain participants control their cryptographic identities, reducing the overhead associated with traditional certificate issuance and management.Enhanced Transparency: All actions (such as key creation and revocation), and transactions are publicly recorded, promoting trust and accountability across the chain.Efficiency: Automation through smart contracts streamlines processes such as inspections and approvals, saving time and resources.Enhanced Security: Cryptographic signatures prevent data tampering and fraud, thus protecting data authenticity.Improved Scalability: Participants can manage their keys and verify others without bottlenecks from centralized authorities.Integrating blockchain: a path to modernizing supply chainsThe shift from centralized to decentralized systems is no longer a futuristic concept, but a growing reality for Web2 companies looking to modernize their supply chains. Companies like IBM, with its blockchain-powered Food Trust, and  De Beers’ Tracr, used for tracking diamonds from source to store, demonstrate how blockchain and DPKI can integrate seamlessly into existing supply chain models to enhance transparency and trust.Harnessing the benefits of decentralized supply systems doesn't require a sudden overhaul of the existing supply chain. Web2 companies can adopt an incremental approach—beginning by identifying use cases—and testing DPKI in targeted areas, before gradually scaling the integration across the entire supply chain.]]></content:encoded></item><item><title>Landmark EU Tech Rules Holding Back Innovation, Google Says</title><link>https://tech.slashdot.org/story/25/07/01/1811254/landmark-eu-tech-rules-holding-back-innovation-google-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google will tell European Union antitrust regulators Tuesday that the bloc's Digital Markets Act is stifling innovation and harming European users and businesses. The tech giant faces charges under the DMA for allegedly favoring its own services like Google Shopping, Google Hotels, and Google Flights over competitors. Potential fines could reach 10% of Google's global annual revenue. 

Google lawyer Clare Kelly will address a European Commission workshop, arguing that compliance changes have forced Europeans to pay more for travel tickets while airlines, hotels, and restaurants report losing up to 30% of direct booking traffic.]]></content:encoded></item><item><title>Complete Gemini CLI Setup Guide for Your Terminal</title><link>https://hackernoon.com/complete-gemini-cli-setup-guide-for-your-terminal?source=rss</link><author>Vladislav Guzey</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:19:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Have you ever wished for an AI assistant right inside your terminal window? Well, your dream may have just come true because Google just released Gemini CLI. In this tutorial, I’m going to show you everything you need to know about this new open-source AI agent. We’ll cover how to use it, the pricing, and some useful tips and tricks. So, if you’re ready, let’s get started! ;)Gemini CLI is a free and open-source AI agent that works directly in your terminal. This powerful tool brings Google’s Gemini models to your command line, allowing for natural language interaction to get work done. You can ask it to:You don’t have to constantly switch between a web app and your terminal. And the best part? It’s , with no complicated setup.Getting Started with Gemini CLIStep 1: Install Gemini CLI on LinuxYou can install  on , , and . All the setup we will do inside the terminal. I am using Linux, but for Mac and Windows, the commands are almost the same, so you can follow my steps.To get started, make sure you have Node.js version 18 or higher. You can check this by running:If you don’t have it, use the following command to install it:sudo apt update && sudo apt install nodejs npm
Then, I installed Gemini CLI globally with:npm install -g @google/gemini-cli
If you don’t want to install it globally, you can also use:npx https://github.com/google-gemini/gemini-cli
After installing, just type:After that, you need to log in with your personal Google account.This gives you access to a free Gemini Code Assist license, which includes:Access to Gemini 2.5 Pro.A massive 1 million token context window.60 model requests per minute.1,000 model requests per day at no charge.Now you are ready to start asking questions and running tasks. You can ask the Agent to create a project, fix the bugs, explain the code in specific files, etc. Ensure that you run the agent within your project folder.> What does the file index.js do?
It read the file, analyzed it, and gave a clear explanation.> Add error handling to index.js
You can also run shell commands directly by using , like this:Creating a Simple To-Do App with Google CLINow that we’re all set up, let’s ask the AI to create a simple to-do application using HTML, CSS, and JavaScript. I will type “create a simple to-do app using simple js and html” into the Gemini CLI.” Watch the video to see the step-by-step process and the result.Gemini CLI has some handy built-in tools. You can use commands like: (). Lists files and folders in a directory—just like the shell  command. (). Reads the full content of a single file, useful for summaries or analysis. (). Reads multiple files at once, typically matching a glob pattern (e.g., all  files) (). Searches for files by pattern (e.g., find all  across your project). (). Searches within files for text, like finding all  comments. (). Applies code changes via diffs. Gemini previews edits and asks for approval before applying them. (). Creates new files (for example, ) with user-provided content. (). Runs commands you prefix with  (e.g., ) directly in the terminal . (). Fetches content from the web (HTML or JSON), enabling Gemini to analyze external data. (). Performs a Google search to ground responses with real-world information (e.g., explanation for an error). (). Stores facts or preferences during a session (like “I prefer async/await”) to improve consistencyTo see all available tools, you can use the  command.You can add specific instructions for the AI for a particular project by creating a  file in your project’s root directory. Inside this file, you can define project rules, code styles, and the tools the agent should use. This ensures that the generated code is consistent with your project’s standards.Google CLI MCP IntegrationFor most day-to-day uses, the built-in tools will suffice. But what if you want Gemini CLI to do something very domain-specific, like interact with specific APIs or use a specialized model (say an image generator or a security analysis tool)? This is where MCP (Model Context Protocol) comes in.MCP is essentially an open standard that allows developers to add new tools/abilities to the AI by running a server that the CLI can communicate with. In Gemini CLI, you can configure “MCP servers” in a JSON settings file, and the CLI will treat those as additional tools it can use.How to Set Up the MCP Server in Google CLIAs an example, I am going to show you how to set up the GitHub MCP server in Gemini CLI.Inside your project folder, create a folder by using the command:mkdir -p .gemini && touch .gemini/settings.json
Inside the file, add the following code:{  
  "mcpServers": {  
    "github": {  
      "command": "npx",  
      "args": ["-y", "@modelcontextprotocol/server-github"],  
      "env": { "GITHUB_PERSONAL_ACCESS_TOKEN": "[YOUR-TOKEN]" }  
    }  
  }  
}
After that  , from the Gemini CLI, and then reopen it.Write  command, and you will see a list of .Now you, Agent, can interact with GitHub. That simple! :)You can try it free for personal usage, but there is also a paid version that is billed based on token usage.: Free with Google account: Up to  and Great for individual developers and small-scale useUse your own  for higher usageBilled based on tokens consumed (model and usage dependent)Available through Gemini Code Assist Standard or Enterprise plansIncludes advanced features like governance, audit logs, and shared quotasAs you can see, Gemini CLI is a really powerful tool with a lot of potential. I’m excited to see how I’ll be using it in my daily workflow.If you write code, debug things, or manage files often, this tool is worth checking out.If you have any feedback, please share it in the comments below. ;)]]></content:encoded></item><item><title>AMD Preps Some Compute Driver Fixes For Polaris &amp; Hawaii Era GPUs With Linux 6.17</title><link>https://www.phoronix.com/news/Linux-6.17-AMDGPU</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:47:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD today submitted their initial batch of "new stuff" for queuing into DRM-Next of their kernel graphics/compute driver changes they have prepared for the upcoming Linux 6.17 cycle opening in a few weeks...]]></content:encoded></item><item><title>How Trusted Execution Environments Power Scalable, Private Smart Contracts</title><link>https://hackernoon.com/how-trusted-execution-environments-power-scalable-private-smart-contracts?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:45:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The layer-two solution is a straightforward approach that combines the TEE and blockchain to provide smart contracts with confidentiality while keeping scalability. In such systems, the operations of smart contracts are decoupled from their underlying blockchain systems. The smart contracts are executed in an independent layer outside blockchain systems.\
 In a general layer-two solution, the blockchain is used as a dispute resolution layer. The smart contract is executed outside the blockchain, making TEEs act as an agent between users and blockchain systems. Suppose that a user aims to use a private contract. She first needs to compile the original contract code, push binary codes to a TEE, and then upload execution results to the public ledger. As illustrated in Fig.4, we extract a generic data flow as follows. A user sends the encrypted input data to a TEE-powered node. Then, the TEE decrypts the input data and executes the contract. After that, the encrypted execution results are sent to the blockchain platform for verification and storage. Finally, the user fetches and decrypts the blockchain-confirmed results.\
Privacy-preserving property. The  is an essential property. In layer-two systems, such as [46], [68], [69], the contract computations run inside Intel SGX enclaves, while TZ4Fabric [44] moves contract executions into ARM Trusted Zone. Since the contract state-transition process happens inside TEEs, any intermediate states remain invisible to the outside. Meanwhile, to achieve the full lifecycle security for a smart contract, the input sent to a TEE and the output returned from this TEE are also required to be encrypted. For example, in ShadowEth [68], PDOs [46], Phala [69] and Hybridchain [41], the contract invocation arguments are encrypted with the TEE public key. They can only be decrypted within the enclave. Also, before transferring execution results to the blockchain (or users), the intermediate (or final) states in an enclave are encrypted. Some variants also enhance the privacy-preserving properties from other aspects. In Phala [69], only authorized queries to the contract will be answered. The smart contract source codes in ShadowEth [68] are hidden during the procedures of deployment and synchronization. This further reduces the possibility of data leakage in subsequent contract executions. Considering a fixed address may expose the user who has invoked the contract, PDOs [46] also allows the user to use pseudonym addresses for submitting a transaction (including TEE outputs) to the blockchain.\
Blockchain intrinsic feature. ShadowEth [68] and Taxa [70] introduce an external distributed service to manage the contracts, achieving the properties of code immutability, high availability and . Meanwhile, layer-two systems satisfy state consistency for reasons that the encrypted states of contracts in different blockchain nodes will eventually get consistent when reaching a successful agreement. Intuitively, the contracts deployed in layer-two systems should retain the features given by original blockchains. However, some fundamental properties are lost when using layer-two solutions. For example, most layer-two systems lose contract interoperability since each contract is executed in different machines. Among all the evaluated systems, only Phala [69] identifies this issue and proposes a command query responsibility segregation architecture to ensure certain interoperability. Also, public verifiability is a crucial property for the blockchain since it allows each contract invocation, and contract execution to be publicly verifiable. Unfortunately, contracts are executed in TEEs so that the outputs are encrypted. To check whether the TEE has executed contracts following loaded contract specifications is a non-trivial task.\
 An attacker may control the network between users and TEE hosts. Meanwhile, TEEs are assumed to always produce correct results, and the smart contracts inside TEEs cannot deviate from their specifications. The main difference compared with the assumption of layer-one systems is that an adversary can observe the network activities between the TEE interfaces and active blockchain nodes.\
 Several layer-two solutions adopt incentive or punishment mechanisms to encourage TEE hosts to provide a stable and secure environment for executing confidential contracts. For example, Fastkitten [43] and Erdstall [75], [76] propose  transactions, in which a host will be punished if its malicious behavior has been identified. In particular, if the TEE execution is aborted, the host will be charged according to previous deposits. In Taxa [70], every node can identify any faulty nodes with reliable proofs for executing further economic punishment. On another route, TEE hosts in Phala [69] will get paid by providing their computing resources to users. Similarly, the remuneration in ShadowEth [68] will be transferred to TEE hosts who execute private contracts. These mechanisms can effectively prevent malicious TEE hosts from an economic aspect. However, they are powerless against external threats. An adversary may directly terminate a TEE host at any time. Even worse, the TEE provides users with an open interface that is vulnerable to DoS [77] or single-point attack. To overcome such issues and achieve fault tolerance, different methods are proposed. Fastkitten provides low-level fault tolerance by periodically saving an encrypted snapshot of current states in enclaves. If the enclave fails, the TEE host can instantiate a new enclave and restart the computation starting from the encrypted snapshot. Similarly, Taxa [70] stores a session file for maintaining and recovering user’s requests. However, a malicious attacker may directly terminate the TEE host, and Fastkitten does not tolerate such host failures. Another technical route is to maintain a secure network. ShadowEth maintains a group of TEE nodes to ensure consistency via a Paxos-like [78] algorithm. Taxa adopts TEE-enabled computing nodes powered by a PBFT-derived PoS [79] algorithm. Any node in the network has the same responsibility to privately execute smart contracts and transfer execution results to the blockchain. However, this brings additional authentication issues. A TEE host must be carefully authenticated to ensure her TEE capability when joining an external network.\
Meanwhile, the systems PDOs [46], Phala [69], Ekiden [42] and COMMITEE [73] introduce an expendable and interchangeable solution. TEEs are stateless: any particular TEE can be easily replaced once it has clashed or finished its task. Unfortunately, these solutions are along with new challenges. Firstly, even if TEEs are changeable, detecting a compromised TEE is still difficult. For instance, PDOs can re-execute a method multiple times for the verification. Given the same input parameters to different TEEs, TEEs are believed to work securely only if their outcomes match. Then, the outputs of enclaves are allowed to commit to the blockchain. COMMITEE adopts  TEE host mechanism. If the master TEE host is proved to be malicious, a backup TEE host will continue to work without communications to the master TEE host. Nevertheless, this model increases the attack interface and makes the whole system vulnerable. Secondly, TEE hosts are stateless. That means, to ensure an exceptional execution is recoverable, any persistent state must be stored in the blockchain or a trusted third party (TTP). However, for a non-deterministic blockchain system such as Ethereum (PoS version) [2], verifying whether an item has been stored on the blockchain is a non-trivial task. Meanwhile, storing data in TTPs may lead to the single-point failure, which goes against the blockchain’s real intention.\
 A contract runs inside TEE, and heavily depends on remote attestation service. The SGX-supported blockchain systems including PDOs [46], Fastkitten [43], ShadowEth [68], Phala [69] and Ekiden [42] assume that Intel Attestation Service (IAS) is trusted. IAS can correctly and completely report whether a certain output with cryptographic material ( [80]) is produced by SGX-enabled hardware. However, IAS might be compromised, posing a risk to these architectures. A compromised or hijacked remote attestation service may maliciously report an attestation with the wrong cryptographic material that does not belong to its corresponding TEE hardware, breaking the promised security. Meanwhile, a centralized service might be crashed, causing the leakage of private states. Unfortunately, none of layer-two schemes consider these risks in designs or implementations.\
As discussed, current TEE implementations have memory limitations for confidential executions. If the memory usage exceeds the threshold, it may confront significant performance and security issues [81]. Hybridchain [41] optimizes the storage by maintaining transaction records outside Intel SGX. Meanwhile, TZ4Fabric [44] minimizes TCB by avoiding all the executions inside TEEs. However, these approaches increase the implementation complexity. A well-known fact is that a TEE is vulnerable to physical vulnerabilities [57]. Unfortunately, very few layer-two solutions provide remedial measures to reduce the risk of being attacked.\
 A poorly-written contract might deviate from designated functionalities and further leak the secret information. This part discusses the potential pitfalls and remedies when deploying contracts.\
In original smart contract systems, gas mechanism is a powerful tool to prevent  attacks [2]. Since the layertwo systems execute smart contract outside the blockchain, a similar mechanism must be considered. Fastkitten [43] and Hybridchain [41] protect against such attacks by using the  mechanism. Limitations are firstly defined on the maximum amount of execution steps that allow to perform inside a TEE per round. Then, TEE monitors smart contract operations. If the number of execution steps exceeds a predefined threshold, the enclave will terminate executions. ShadowEth [68] combines a timeout mechanism with a  mechanism. Similar to the gas mechanism in Ethereum [2], TEE hosts can still gain remuneration even if a contract exits after timeout since they provide sufficient computing power. These mechanisms effectively protect against endless loops and denial-of-service (DoS) launched by external attackers.\
The TEE itself lacks self-awareness of input data, since it cannot distinguish which state is fresh. A lack of input data authentication makes the system vulnerable to the rollback attack [82], [59]. A malicious user may attempt to invoke the confidential contract many times to seek the leaked secret information. Authentication of the user’s identity is helpful to prevent this attack. However, none layer-two solution provides these remedies for these potential pitfalls. On the other hand, the TEE input may come from a non-deterministic blockchain system [83], [84], in which deciding whether an input has been confirmed is tricky. Fastkitten [43] and COMMITEE [73] mitigate this issue by using a  mechanism. As for TEE output conflicts, Ekiden [42] uses a probabilistic proofof-publication protocol to avoid the ambiguous input.\
After the invocation of a private contract, the outputs returned from TEEs are uploaded on-chain for the final confirmation. But a malicious TEE host may send an exceptional result to the blockchain. Even worse, two hosts may publish different updates towards the same contract simultaneously. To prevent such malicious publications and to evade conflicts, PDOs [46] depends on Coordination and Commit Log (CCL) to manage the synchronization in the execution of interacting contracts and enables a contract owner to decide on selecting the enclave for contract executions, which effectively avoid conflicts. Phala [69] adopts an event sourcing command query responsibility segregation architecture to scale up and avoid conflicts, in which the write operations are recorded as events and read operations can be served by the current view of states. Again, these solutions contradict the property of decentralization. Ekiden [42] and ShadowEth [68] rely on the blockchain to resolve conflicts resulting from concurrency. In particular, ShadowEth [68] requires a worker to specify the version number with a timestamp when pushing data to the blockchain. Even miners accept different responses at first, they will eventually reach an agreement by comparing version number and the timestamp, with the help of the consensus procedure. Yet, such an approach is inefficient, especially in non-deterministic blockchain systems.\
 PDOs [46] uses a key provisioning service to distribute private keys. The drawback is obvious: A compromised provisioning service could make the entire system fail. To increase the robustness of a private key, Ekiden [42] designs a distributed key generation (DKG) [85] protocol using the secret sharing scheme [86]. Even if one key manager is compromised, an adversary cannot obtain the entire key. However, this solution does not completely solve the key leakage issue. The final keys are assembled and replicated among all end-TEEs. If an adversary compromises an end-TEE, exposing all the contract state becomes a trivial task. The key rotation technology, adopted by Ekiden [42], Fastkitten [43], Phala [69] partially solves the above issue by providing a short-term key in every epoch. An adversary cannot corrupt a future or previous committed state, which minimizes the possibility of key exposure to attackers and further helps the layer-two system to achieve forward secrecy. Also, layer-two projects such as COMMITEE [73] mitigate these key issues by providing each TEE per secret key. Even if a certain TEE’s private key were stolen, this only would affect the smart contract running on that compromised TEE. Furthermore, Phala Network [69], equips each contract with an asymmetric key called the , which also enhances the key security to a certain degree.\
The layer-two solution decreases computational burden and avoids latency by decoupling the smart contract executions from consensus mechanisms. The solution merely puts the execution results on-chain rather than all processing states. Meanwhile, the layer-two solution does not require a dedicated public ledger, meaning that such a solution can smoothly\
integrate with existing public blockchain platforms. Unfortunately, this method also brings security and functionality challenges when delegating the task of contract management to an external TEE layer.\
Firstly, the layer-two solution complexifies contract data management. The contracts that are deployed outside the blockchain require an external execution/storage party. A malicious storage maintainer may reject to provide the service, while a malicious host may abort TEE executions, terminate enclaves or delay/drop messages. Even an honest host might accidentally lose states in a power cycle. To solve the centralization issue and tolerate host failures, many countermeasures such as the TEE network, stateless TEEs and punishment mechanisms, are proposed. However, these solutions are not effortless, inevitably making the system complicated and hard to implement in practice.\
Secondly, the layer-two solution increases the attack surface and thus becomes vulnerable to rollback attacks. There is a high probability that an adversary node can revert transactions where temporary forks, representing inconsistent blockchain views, are allowed in blockchain systems with probabilistic consensus (e.g., PoW). Since TEEs provide no guarantee on verification of input data; they cannot distinguish whether an input state is fresh or not. An attacker may offer stale states to resume a TEE’s execution. This enables rollback attacks against randomized TEEs programs. Even worse, plugging up these loopholes needs much effort.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Layer-One Confidential Smart Contracts: Architecture, Threats, and Tradeoffs</title><link>https://hackernoon.com/layer-one-confidential-smart-contracts-architecture-threats-and-tradeoffs?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The layer-one approach enables blockchain nodes to run contracts in their isolated areas, as well as conducting the consensus (see Fig. 3). This approach combines the consensus procedure and state execution, either in terms of logically or physically. The reason why we call this method layer-one is that all executions are completed in the same layer of the blockchain network. The key to such an approach is to equip every blockchain node with a TEE. Indeed, this requires more integration efforts, but also comes with several advantages. The smart contract can implement stateful functionalities that receive arguments and update states instantly. In particular, a smart contract can directly access the ledger data stored in a local disk, greatly saving time often wasted in the interactive network communications.\
 In a layer-one execution model, the operation of ledger update (consensus) and state transition (contract execution) are coupled. Like Ethereum [2], smart contracts run inside blockchain nodes. Assume that a user plans to use the private contract; she only needs to upload data to the blockchain service and wait for results. The remaining procedures are completed by TEE-assisted distributed nodes. A TEE in these nodes acts as a black box for data processing and output targeted results without the data leakage. This approach greatly improves convenience for users due to its easy access and management. As illustrated in Fig.3, a generic data flow goes as follows: A contract creator deploys the code into blockchain. Then, a user sends the transaction with an encrypted argument to an arbitrary blockchain node. Her request is confidentially executed inside TEEs in this node and output encrypted state. Then, the consensus algorithm in this node broadcasts the encrypted results to peers. After the encrypted results are confirmed by other blockchain nodes, users fetch on-chain results and decrypt them for the plaintext.\
Privacy-preserving property. This property indicates that contract states and the procedure of contract executions are hidden from the public. To achieve privacy, layer-one systems execute these confidential contracts inside TEEs in every distributed node. CCF [45], Fabric [60] and CONFIDE [37] follow this straightforward design where confidential contracts are loaded to the TEE of each consensus node, which encrypts both the inputs and outputs of contract states, together with their operating logic and predefined rules. Enigma[1] [61] introduces the secret network and allows users to submit their transactions together with encrypted data to miners. We also notice that current layer-one solutions only focus on internal procedures rather than the linkability and anonymity of addresses and transactions. This indicates that confidential smart contracts only protect the contents that have been loaded into TEEs, while the data that relates to external users is out of the scope of this work.\
Blockchain intrinsic feature. The layer-one systems inherit most of the features empowered by blockchain. More precisely, the properties of code immutability, high availability, explicit invocation, decentralized execution, automatic execution and  remain the same because basic contract executions still rely on their underlying blockchain systems. Also, the property of (confidential)  in Enigma [61], CCF [45] and Fabric [60] remains unchanged. The states and executions from these systems follow the procedures of online consensus processes. Then, the returned results from inside TEEs still require to be confirmed on-chain. This makes their actions effectively perform the same functions as a normal smart contract, except for that the contents of states are transmitted from plaintext to ciphertext. In contrast, the property of contract interoperability is lost since the contracts are executed in isolated TEEs. This isolation requires additional communications such as dispatching keys through the remote attestation service, bringing much complexity.\
The layer-one solution encapsulates TEE computations into blockchain nodes. Every node in the network has to take responsibility for conducting confidential executions and performing the consensus. The design to coordinate TEEs and consensus within the same physical space brings many distinguished features. We start the analysis from their threat model and then dive into each component of these systems.\
 Users in the layer-one approach are assumed to be unreliable. They may have mistakes unconsciously, like dropping messages or mis-sending transactions. Even worse, a malicious user can arbitrarily behave like faking messages, identities, or compromising other nodes. As for TEE hosts, an external attacker can monitor, eavesdrop or even compromise part of involved TEE hosts among these distributed nodes, but cannot block all traffic transmitted in communication channels. Subsequently, a TEE is supposed to work in a good condition: The attestation service is trusted, and the cryptographic primitives used inside TEEs are secure. Meanwhile, as for the blockchain network, the basic systems (ledgers) are assumed to be robust [62], [63], [64]. When running the consensus, the majority (might be two-third, depends on specific consensus algorithms) of nodes are assumed to be honest [65]. Also, forging smart contract codes or states will happen in honest blockchain nodes with a negligible possibility. Based on that, we analyse securities from four aspects.\
 Firstly, we focus on the security of TEE hosts, or equally, individual nodes that run TEEs. Unlike classical blockchain systems, there are no explicit incentive or punishment mechanisms in this solution. This is easy to understand: A node with malicious behaviors will be instantly moved out of the committee and replaced by a new honest participant. Meanwhile, due to the fact that CCF [45] and Enigma [61] rely on Tendermint (a BFT variant) consensus algorithm, they can tolerate at most one-third of TEE Byzantine nodes. But the sacrifice is the increased difficulty in synchronization, especially when every node has to establish a secure channel for communications of distributed TEEs. In layer-one systems, host authentication is necessary. The node who wants to join the committee has to obtain permission from communities by proving her TEE capability. For instance, CONFIDE [37] builds a mutual authenticated protocol (MAP) (supported by SGX remote attestation techniques [66]) among blockchain nodes. Any nodes joining in the network have to pass the authentication via MAP.\
 Then, we analyse TEE-level securities. Attestation service is an essential part of TEE techniques. Systems in the layer-one solution still require such services for network connection and verification. Enigma [61], Fabric [60] and CCF [45] follow the original attestation mechanism with an implicit rule: The Intel Attestation Service (IAS) should be reliable. However, this cannot be guaranteed in the case of IAS being comprised. In contrast, CONFIDE [37] utilizes a customized Decentralized Attestation Service to provide the robust authentication. As for memory limitations, layer-one systems load contract executions and consensus algorithms into one TEE-embedded node, causing an increase in disk and memory usage of individual nodes. Once the usage of TEE memory runs over the predefined settings, a decrease in the performance is inevitable [34]. This may further cause an unpredictably severe result like system crash-down. Fortunately, Fabric [60] mitigates such issues by separating the operations into two types (execution and ordering) and delays the transaction- procedures after state-. Among them, only the state- parts are processed inside TEEs. This decreases computation complexity and limits the memory usage to a suitable range. Physical attacks like the Spectre and Meltdown vulnerabilities [57] are intrinsic design pitfalls that may occur inside the TEE kernel. To our knowledge, no layerone solutions mention them or provide the remedies.\
TEE program security. Next, we focus on the program-level security. Issues like overburdening may frequently happen, especially when a malicious developer deploys a contract with infinite loop logic. Unlike using the gas mechanism in Ethereum [2], systems in the layer-one model constrain their running programs by the  mechanism. It sets a threshold, namely, a suitable range of time that allows processing contract operations. When exceeding the timebound, the system will abort under-processing states and restart a new round. As for the flaw detection, no formal techniques or verification tools, based on our observation, have been applied to layer-one systems. This gap needs further exploration. Similar to the previous discussion, the properties of data verification (covering both user data authenticity and blockchain data confirmation) and output conflicts are guaranteed by their underlying consensus algorithms. Each time performing the consensus, these properties are automatically checked. For instance, Enigma [61] relies on trusted validators, who equip with TEEs to conduct the verification procedure. Such validators maintain both the privacy of executions inside TEEs and the consistency of states that connects to peers. Once conflicts occur, validators will quickly make decisions on a block and remove another conflicting block. Fabric [60] performs such a process inside TEEs among committee nodes and then submits the passed results to its abstract ordering service. This service prevents forks caused by conflicting states, as well as proving a fact that: All executed messages are valid and integral once reaching the consensus agreement. It should be noted that, successful consensus procedures can merely guarantee the integrity of transactions and states, rather than linkability and authenticity that relates to physical entities.\
 Lastly, we move to the aspect of TEE key management. In layer-one systems, the key management service takes over the task of creating and managing keys for activities like attestation, verification, encryption, etc. To achieve the key management service among distributed nodes, several types of designs have been proposed. CCF [45] relies on the public key infrastructure (PKI) for certificate issuance, management, and revocation. It creates key pairs and dispatches them to every participated TEE, where each TEE holder is authenticated by the certificate. Similarly, Fabric [60] adopts an admin peer to provision the specific decryption key to  during bootstrapping. Enigma [61] setups an independent key management component to reply to the requests for encryption. Such designs help to simplify complex management procedures, as well as providing distinguishable keys for each TEE. However, these independent key management services lead to centralization even they are maintained by a group of nodes in the committee. CONFIDE [37] mitigates this issue by proposing a decentralized key management protocol. Two types of keys are involved in this protocol: the  used to decrypt confidential transactions from clients and the  used for state encryption/decryption between the confidential engine and storage service.\
The layer-one solution provides a highly integrated approach towards confidential smart contracts.\
The layer-one solution provides a consistent interface for users without changing the customer’s habits transformed from non-TEE blockchain systems. A user can use the layer-one system by directly interacting with the blockchain interface, without considering cumbersome and complicated operations between the TEE and blockchain. However, the layer-one solution still confronts several common disadvantages.\
Minimizing the size of Trusted Computing Base (TCB) contributes to the TEE security [67]. In particular, a small TCB has fewer errors and can reduce attack surfaces. However, complicated interactive operations for contract execution and consensus agreement in the L1 solution greatly increase the size of TCB. Meanwhile, TEE products have limited secure memory. For example, in the current implementation of Intel SGX [35], the enclave page caches are constrained to 128 MB, and only 93 MB of those is available for applications, which limits the concurrent execution.\
Furthermore, the layer-one solution lacks compatibility, which means being incompatible with existing blockchain systems. The solution integrates the consensus procedure and the contract execution into the same blockchain node, requiring every node having to equip a TEE hardware. Nevertheless, this requirement is difficult to be fulfilled in a public blockchain while already in use (e.g., Ethereum [2]).(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.[1] Enigma’s secret network consists of a list of secret nodes equipped with TEE, which is categorised as a layer-one solution in the context of our definition (Sec.III-A). We also note that such a secret network can be regarded as a layer-two solution in the traditional classifications in terms of Ethereum, namely, either on-Ethereum chain (L1) or off-Ethereum chain (L2).]]></content:encoded></item><item><title>What Most Blockchain Devs Get Wrong About TEE Security and Smart Contract Privacy</title><link>https://hackernoon.com/what-most-blockchain-devs-get-wrong-about-tee-security-and-smart-contract-privacy?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[III. SYSTEMATIZATION METHODOLOGYTo find common aspects (e.g., offered functionality, design model, adversary model), we extract recurring design patterns from publicly available publications and projects, focusing on systematization and evaluation of desirable properties (the main target of TCSC) and potential pitfalls of underlying systems. Our systematization methodology follows the idea in [52]: classification and evaluation. We firstly make a classification for the current systems and then define a framework to evaluate them. Details are presented as below.\
\
We classify the existing systems into two main categories: layer-one solution (L1) and layer-two solution (L2). The layer-one solution executes the contract inside a TEE in the blockchain, requiring every blockchain node to equip a TEE. Instead, the layer-two solution decouples contract computations from the blockchain. It performs most of the smart contract computations off-chain. For a clear understanding, we make a comparison of the original blockchain (e.g., Ethereum), L1 solution, L2 solution. As in Tab.II, Ethereum runs smart contracts (in EVM) and consensus procedures in the same machine of distributed nodes. All the contract and transaction operations are publicly verifiable due to their total transparency. The layer-one solution performs such operations (contract execution and consensus) in the same machine, but contract operations are separate from consensus procedures. In contrast, the layer-two solution makes both of them operate independently. Contracts are executed outside the blockchain network, while the consensus happens inside each node.\
Ideally, moving smart contract executions into TEEs brings additional privacy as well as maintaining the original benefits of blockchain systems. Therefore, we have identified the desirable properties in two main categories: privacy-preserving property and .\
Privacy-preserving property. The property of confidentiality is the most distinguished feature in TCSC.\
A1. Specification hidden. The source code of a smart contract is hidden during the deployment and the subsequent synchronization and execution.\
 The inputs fed into a confidential smart contract are hidden from the public.\
 The outputs returned from a confidential smart contract should be kept private.\
 The execution procedure is hidden from unauthorized parties. An adversary cannot learn the operation knowledge inside a TEE.\
A5. Address unlinkability. The address pseudonymity does not entail strong privacy guarantees [53], [54]. This property prevents an adversary to learn the address linkability by observing users’ activities.\
 The contract caller’s identity (a user who invokes a smart contract) is hidden from an anonymity set [24] (see Appendix B).\
Blockchain intrinsic feature. TEE-assisted smart contracts inherit features given by original blockchain systems. We summarize these features as follows.\
 Once a contract is successfully deployed, its source code cannot be altered.\
A8. (Confidential) state consistency. Executions happening at a certain blockchain height will output the same result across different nodes.\
A9. Contract interoperability. A smart contract can call another contract and be called by others.\
 A smart contract is continuously accessible without the single point of failure.\
A11. Decentralized execution. A smart contract runs over the decentralized network.\
A12. Automatic execution. A smart contract can be automatically executed once conditions are satisfied.\
 Operations running on the smart contract will be charged with gas fees [2].\
A14. Explicit invocation. Each invocation will be formatted as a transaction and stored on blockchain.\
A15. Public verifiability. The procedure of contract execution and result are publicly verifiable.\
A16. Consensus verifiability. The consensus procedure on the (confidential) state is publicly verifiable.\
Essentially, all TCSC systems share the same principle: a TEE will handle the data from users. After that, encrypted data flows from the  The TEE plays a crucial role. Thus, this part defines a framework for evaluating underlying blockchain systems from four aspects: , and  This framework aims to identify potential design flaws and pitfalls based on the threat model and data workflow.\
 Our threat model mainly captures three types of attackers, which are stated as follows.\
T1. User adversary (active/passive). An attacker may control network between users and TEE host nodes.\
T2. TEE adversary (active/passive). An adversary may control TEE hosts or control the network between TEE and blockchain platforms.\
T3. Blockchain adversary (active/passive). An adversary may drop, modify and delay the blockchain messages. But the majority (or two-thirds) of the blockchain nodes are assumed to be honest.\
Note that adversaries are not necessarily exclusive. In some cases, adversaries in different types may collude.\
 This section defines four metrics regarding system security according to the data workflow: approaches to enhance the security of a TEE host, countermeasures to mitigate intrinsic TEE issues, methods to prevent program flaws or bugs inside TEEs, and solutions to clear up the TEE key security dilemma.\
 A TEE and its interaction with the external environment (e.g., with users or the blockchain) are operated and controlled by a host (such as a L1 blockchain node). A malicious host has abilities to abort the executions of a TEE, delay and modify inputs, or even drop any ingoing or outgoing messages. The following metrics discuss the approaches to improve the TEE host’s security.\
P1. Host punishment mechanism. Penalty mechanisms to reduce the risk of doing evil by a TEE host.\
P2. Host incentive mechanism. Incentive mechanisms to promote a TEE host to behave honestly.\
P3. Host fault tolerance. Solutions to make systems continually operate despite malfunctions or failures.\
 Methods to check the identity and the capability of a TEE host.\
 A TEE has inevitable weaknesses. For example, a TEE is vulnerable to side-channel attacks [55], [56]. These innate weaknesses directly pose severe challenges to the design and implementation of TEE-assisted contract systems. This part defines the defence approaches against these threats.\
P5. TEE attestation security. Methods to prevent TEE attestation service from being abnormally broken.\
P6. TEE memory limitation. Methods to optimize the memory size to prevent confidential data overflow.\
P7. TEE physical attacks. Approaches to prevent physical attacks, such as the Spectre vulnerability or the Meltdown vulnerability [57].\
 Approaches to provide a trusted timer when running a TEE.\
 Even a TEE is secure as assumed, a program bug may destroy the contract’s confidentiality in the real world. This part focuses on the measurements to prevent TEE programs from flaws or bugs.\
P9. Workload measurement. The workload measurement approach to prevent an infinite loop attack.\
 Formal techniques used for the modelling and verification of the source code of smart contracts to reduce the vulnerabilities.\
P11. User query restriction. The restriction on users’ queries, aiming to avoid data leakage resulting from differentialprivacy analysis [58].\
P12. Blockchain data confirmation. Methods for a TEE to check whether input data from blockchain has been confirmed to prevent the rollback attack [59].\
P13. TEE output conflicts. Methods to avoid multiple TEEs to produce a conflict result.\
 Various keys (cf. Appendix A) are involved in the contract execution, including TEE internal keys such as the attestation key and TEE service keys for state encryption/decryption. Since service keys directly affect the protection of contract states, the key security evaluation in this SoK mainly focuses on the generation, exchange, and storage of the TEE service key.\
P14. Distributed key protocol. The keys of confidential contracts are managed by a distributed protocol.\
P15. Key rotation protocol. The TEE replaces an old key with a fresh key for future contract encryption.\
P16. Independent contract key. Each contract is associated with a unique key, independent from the TEE.\
P17. Independent TEE key. Each TEE has a unique key, and different contracts share the same key.\
 The  shows a general view of the TCSC systems. Desirable property focuses on evaluating contract service provided by a TEE-assisted blockchain system.  describes the potential threats and system assumptions.  show the evaluating indicator for current TEE-assisted systems. In the following section IV-B and V-B, we attempt to answer the following questions: (i) What are the potential pitfalls in each security aspect; (ii) Do these pitfalls have significant security impacts; (iii) Do the designers/developers consider these pitfalls and accordingly come up with feasible remedies in their systems; (iv) What are the remedies and do they address above problems. Note that hundreds of TCSC systems have been proposed in both industry and academia. An exhaustive analysis is undesirable and infeasible. We only selected the projects that provide publicly accessible technical reports or academic papers.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Four Key Steps to Confidential Smart Contract Execution</title><link>https://hackernoon.com/four-key-steps-to-confidential-smart-contract-execution?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This section gives a high-level description and offers a running example to illustrate how a typical confidential smart contract operates. From existing literature [42], [43], [41], [44], [45], [46], establishing a confidential smart contract mainly requires four steps, namely , ,  and  (see Fig. 1).\
From a bird’s eye view, a TCSC can be used as an ideal contract-based black box [47] with secrecy and correctness. This idea has been adopted by several advanced security protocols [48], [49]. We provide a secret e-voting example borrowed from Oasislabs [50].\
A TCSC can be well qualified for the role of decentralized vote manager in an e-voting system [17], [51]. Once a contract-based manager is deployed successfully, the voting logic is loaded into a TEE and corresponding secret keys are privately generated and stored inside TEEs. The encrypted state is then confirmed by the blockchain nodes. This offers the e-voting protocol with confidentiality, neutrality, auditability and accountability. Firstly, the voter’s input cu is encrypted, and intermediate parameters (e.g., mb) are privately processed through TEEs. External attackers cannot obtain the knowledge of sensitive information, and thus the confidentiality is achieved. Secondly, the predefined voting logic only occurs in the decentralized network when certain conditions are satisfied, bringing neutrality for the access control management. Thirdly, if a voter wants to vote for a candidate, she needs to in advance build a channel to the TEE and then send a transaction Tx to call the contract. Due to the protection of encrypted channels, transaction arguments are kept secret. Meanwhile, such invoking records in the form of transactions remain visible and will become immutable, ensuring the voting process accountable. Unfortunately, verifiability, as one of fundamental properties, performs not smooth in the context of encryption. Contracts that are executed inside TEEs make the execution procedures lack public verifiability. Only the nodes who install TEEs with correct corresponding keys can verify the correctness of contract executions. However, the metadata of the transaction Tx retains unencrypted, making it possible to verify the absence of double spending.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>To Fix Smart Contracts, Start With Their Secrets</title><link>https://hackernoon.com/to-fix-smart-contracts-start-with-their-secrets?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.\
—The blockchain-based smart contract lacks privacy since the contract state and instruction code are exposed to the public. Combining smart-contract execution with Trusted Execution Environments (TEEs) provides an efficient solution, called TEE-assisted smart contracts, for protecting the confidentiality of contract states. However, the combination approaches are varied, and a systematic study is absent. Newly released systems may fail to draw upon the experience learned from existing protocols, such as repeating known design mistakes or applying TEE technology in insecure ways. In this paper, we first investigate and categorize the existing systems into two types: the layer-one solution and layer-two solution. Then, we establish an analysis framework to capture their common lights, covering the desired properties (for contract services), threat models, and security considerations (for underlying systems). Based on our taxonomy, we identify their ideal functionalities, and uncover the fundamental flaws and reason for the challenges in each specification’s design. We believe that this work would provide a guide for the development of TEE-assisted smart contracts, as well as a framework to evaluate future TEE-assisted confidential contract systems.Smart contract was originally introduced by Szabo [1] and further developed by Ethereum [2] in the blockchain systems. The blockchain-based smart contracts [3], [4], [5] adopt Turing-complete scripting languages to achieve complicated functionalities [6] and execute the predefined logic through state transition replication over consensus algorithms to realize final consistency. Smart contracts enable unfamiliar and distributed participants to fairly exchange without trusted third parties, and are further used to establish a uniform approach for developing decentralized applications (DApps [7]). However, blockchain-based smart contract lacks confidentiality. The state information and the instruction code are completely transparent [8], [9], [10]. Any states with their changes are publicly accessible and all users’ transaction data and contract variables are visible to external observers. Without privacy, building advanced DApps that rely on the user’s sensitive data becomes a challenge [11], [12], [13], [14]. For instance, smart contracts in Ethereum [2] cannot be directly applied to Vickrey auction [15], [16] or e-voting systems [17], [18], where the bid and vote require to be hidden from the public. Moreover, DApps without privacy protection might be prohibited by European Union because they go against the General Data Protection Regulation [19], [20]. Thus, the complete transparency of smart contracts constrains their wide adoption. Recently, researchers have explored many cryptographic solutions to solve these issues, including utilizing techniques of zero-knowledge proof (ZKP) [21], [22], [12], [23], [24], [25], homomorphic encryption (HE) [26] and secure multiparty computation (MPC) [27]. However, these approaches are merely applicable to applications requiring simple computations.\
Although various TCSC protocols have been proposed, newly released projects may fail to draw upon the experience learned from existing protocols, such as repeating known design mistakes or applying cryptography in insecure ways. For example, an absence of economic incentives will pose security risks and decrease the protocol’s stability. However, the recentproposed TCSC scheme Hybridchain [41] repeats similar pitfalls by simply combining the TEE with a permissioned blockchain network, omitting considerations on the miner’s incentive mechanism. The repeating of pitfalls comes from twofold. Firstly, in-the-wild projects differ from one to another, and a relatively unique model is absent, which narrows the developers’ vision. Meanwhile, a unified evaluation framework is missing, causing many security threats to be uncovered and resulting in considerable loss from applications underpinning the execution of confidential smart contracts. This paper aims to abstract a high-level framework to simply and clearly systematize knowledge on current TCSC schemes. We attempt to capture some commonalities among these projects regarding their features, properties, and potential security vulnerability. We believe that establishing evaluation criteria to measure features and identify problems and flaws of existing TCSC protocols will offer a good guide for industry communities and promote the DApps prosperity. Main contributions (a visualized guideline in Fig.2) are:\
• We provide a systematization of existing TCSC systems driven from academic work and  Based on their operating mechanisms and ways of combination, we investigate and categorize a set of typical protocols into two main classifications: the  solution and the  solution.\
• We establish a unified evaluation framework for confidential smart contract systems. We consider two parts: the smart contracts used as , and underlying supported blockchain systems. Accordingly, the framework covers three aspects:  for contract services,  and  for underlying systems. Specifically, we discuss two different types of desirable properties:  that inherit from traditional smart contracts and featured privacy-related properties. Then, we emphasize practical issues, pitfalls, and remedies in designing TEE-assisted blockchains from four aspects ( securities and  services).\
• We conduct a comparative analysis of existing protocols based on our evaluation framework. We discuss systems both from their  (system classification, threat model) and  (designs, properties). The common designs show us the consistent idea when re-designing the system, while the distinguished features highlight the ingenuity of each system design that deviates from others (see Tab.III/Tab.IV).\
• We further give a comprehensive discussion of current designs and implementations, including a running example, comparisons between layer-one and layer-two systems from the perspectives of ,  and , and common issues on . Unfortunately, a mature design is still not ready for large-scale applications. We thereby point out research  in this field, wishing to give insights for communities on defining their models and discovering possible solutions of designing TCSC systems.\
The rest of the paper is organized as follows. Sec.II gives a high-level introduction on how to operate a confidential smart contract inside TEEs. Sec.III provides the systematization methodology ( and ). Layer-one and layer-two systems are analysed in Sec.IV and Sec.V, respectively. Discussions are provided in Sec.VI. Research challenges are summarised in Sec.VII. Finally, Sec.VIII gives concluding remarks. Supplementary details are stated in Appendix A-D.]]></content:encoded></item><item><title>Road to Battlefield: Central Eurasia’s gateway to TechCrunch Startup Battlefield</title><link>https://techcrunch.com/2025/07/01/road-to-battlefield-central-eurasias-gateway-to-techcrunch-startup-battlefield/</link><author>Cindy Zackney</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:41:37 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Historic regional competition launches to showcase Central Eurasia’s rising startup ecosystem on Silicon Valley’s biggest stage. For the first time in its history, Central Eurasia will have a direct pathway to TechCrunch Startup Battlefield through the launch of “Road to Battlefield,” a groundbreaking regional competition that promises to put the underrepresented region firmly on the […]]]></content:encoded></item><item><title>Tech Hobbyist Destroys 51 MicroSD Cards To Build Ultimate Performance Database</title><link>https://it.slashdot.org/story/25/07/01/155208/tech-hobbyist-destroys-51-microsd-cards-to-build-ultimate-performance-database?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Tech enthusiast Matt Cole has created a comprehensive MicroSD card testing database, writing over 18 petabytes of data across nearly 200 cards since July 2023. Cole's "Great MicroSD Card Survey" uses eight machines running 70 card readers around the clock, writing 101 terabytes daily to test authenticity, performance, and endurance. 

The 15,000-word report covering over 200 different cards reveals significant quality disparities. Name-brand cards purchased from Amazon performed markedly better than identical models from AliExpress, while cards with "fake flash" -- inflated capacity ratings -- performed significantly worse than authentic storage. Sandisk and Kingston cards averaged 4,634 and 3,555 read/write cycles before first error, respectively, while Lenovo cards averaged just 291 cycles. Some off-brand cards failed after only 27 cycles. Cole tested 51 cards to complete destruction during the endurance testing phase.]]></content:encoded></item><item><title>You will own NOTHING and be HAPPY! (SKG)</title><link>https://www.youtube.com/watch?v=rAsgjKBkKMA</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/rAsgjKBkKMA?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 20:35:53 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[Please sign if you're in the EU or UK!
EU: https://eci.ec.europa.eu/045/public/#/screen/home
UK: https://petition.parliament.uk/petitions/702074/

Things I mentioned in this video:

  - Stop Killing Games: https://www.stopkillinggames.com
  - My video takedown (and getting a 2nd appeal): https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful-updated
  - Bosch Dishwasher video: https://www.youtube.com/watch?v=5M_hmwBBPnc
  - XKCD on WiFi appliances: https://xkcd.com/3109/
  - Dead game list: https://stopkillinggames.wiki.gg/wiki/Dead_game_list

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff

Contents:

00:00 - Controlling your own destiny. And dishwasher.
01:27 - Games (please stop killing them)
03:35 - What you can do (in the EU)]]></content:encoded></item><item><title>X is piloting a program that lets AI chatbots generate Community Notes</title><link>https://techcrunch.com/2025/07/01/x-is-piloting-a-program-that-lets-ai-chatbots-generate-community-notes/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:26:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The social platform X will pilot a feature that allows AI chatbots to generate Community Notes, a Twitter-era feature that Elon Musk has expanded under his ownership of the service now called X.]]></content:encoded></item><item><title>Trump Launches America’s Newest Concentration Camp, Complete With Tacky Merch</title><link>https://www.techdirt.com/2025/07/01/trump-launches-americas-first-concentration-camp-complete-with-tacky-merch/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:06:47 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Not content with just shipping people to a foreign concentration camp, Donald Trump now has his own, homegrown concentration camp in Florida. Trump, DHS Secretary Kristi Noem, and Florida Governor Ron DeSantis gleefully toured the hastily constructed concentration camp in the Florida Everglades, obnoxiously referred to as Alligator Alcatraz, in reference to (1) the infamous island prison in San Francisco that Trump is obsessed with and (2) the number of alligators (and crocodiles — the one place in the world that has both) that live in and around the Everglades.There’s no way to look at what the US government is doing here and not think of it more as Auschwitz than Alcatraz. The parallels are unmistakable: hastily constructed camps in remote locations, euphemistic naming designed to obscure their true purpose, and—most tellingly—officials proudly touring the facilities while discussing plans to build “a system” of such camps nationwide.But here’s where today’s American concentration camps differ from their 20th-century predecessors: the Trump regime isn’t trying to hide what they’re doing. They’re merchandising it. They’re selling t-shirts celebrating human suffering as if it were a sports team or a vacation destination.The United States government is literally selling branded merchandise to celebrate putting human beings in cages surrounded by dangerous predators. This isn’t just about policy—it’s about turning cruelty into a consumer product. It’s about making the suffering of others into something you can wear to own the libs.This commodification of human rights violations represents something uniquely American and uniquely horrifying: the gamification of genocide. Previous authoritarian regimes at least had the decency to be ashamed of their concentration camps. Trump is selling tickets to the show.These are the sorts of things that history books (should they exist in the future) will talk about as one of the many moments of pure evil that some people gleefully embraced without recognizing that people setting up concentration camps are, inherently, “the baddies.”For what it’s worth, Trump did little to dispel the notion that this is part of his new fascist campaign to imprison anyone who disagrees with him. During the tour, Trump and Noem talked about prosecuting CNN for their reporting and for releasing an app that alerts people to where ICE agents are located (both of which would violate the First Amendment, if it were still a thing anyone believed in).Trump admitted that he had brought up this idea as a joke, but his idiot advisors ran with it:“Is this a dream come true for you, sir” a reporter asks.“It was meant more as a joke, but the more I thought of it, the more I liked it… they were actually crocodiles,” Trump said.“We’d like to see them in many states. At some point, they might morph into a system,” Trump said on Tuesday.A “system.” The word choice isn’t accidental. This is the language of industrial-scale human rights violations, spoken with the same casual tone you’d use to discuss a chain restaurant expansion.In case you’re wondering how much it costs to go full Nazi, this one concentration camp will cost the American taxpayer nearly half a billion dollars a year. That money will come from FEMA, the organization that Trump (with an assist from former friend Elon Musk and DOGE) stripped budget from, meaning there will be even less to pay for actual emergencies, because all of that money will be used to jail people Trump doesn’t like in a swamp.The Everglades facility will cost Florida some $450 million to run for one year, according to DHS, though much of that will be reimbursed by the Federal Emergency Management Agency (FEMA). While the airstrip is owned by Miami-Dade County, where officials have viewed the plan with skepticism, DeSantis is using his emergency authority to proceed on a tight schedule.We are watching the latest march forward of American fascism in real time, complete with branded merchandise and gleeful photo ops. The US government is building concentration camps and selling t-shirts about it. This isn’t hyperbole. This isn’t partisan hysteria. This is what’s actually happening.Every day you don’t call this what it is—fascism—you become complicit in normalizing it. Every time you treat this as just another political story, you help them make it routine. They’re counting on your exhaustion, your normalization, your willingness to look away.The survivors of the Holocaust warned us this could happen again. They’re mostly gone now, but their warnings echo: it starts with camps, it starts with dehumanization, and it starts with good people doing nothing while evil wraps itself in flags and sells t-shirts.]]></content:encoded></item><item><title>AT&amp;amp;T Now Lets Customers Lock Down Account To Prevent SIM Swapping Attacks</title><link>https://tech.slashdot.org/story/25/07/01/181213/att-now-lets-customers-lock-down-account-to-prevent-sim-swapping-attacks?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AT&T has launched a new Account Lock feature designed to protect customers from SIM swapping attacks. The security tool, available through the myAT&T app, prevents unauthorized changes to customer accounts including phone number transfers, SIM card changes, billing information updates, device upgrades, and modifications to authorized users. 

SIM swapping attacks occur when criminals obtain a victim's phone number through social engineering techniques, then intercept messages and calls to access two-factor authentication codes for sensitive accounts. The attacks have become increasingly common in recent years. AT&T began gradually rolling out Account Lock earlier this year, joining T-Mobile, Verizon, and Google Fi, which already offer similar fraud prevention features.]]></content:encoded></item><item><title>Catalio Capital closes over $400M Fund IV</title><link>https://techcrunch.com/2025/07/01/catalio-capital-closes-over-400m-fund-iv/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Catalio Capital Management announced the closing of its more than $400 million Fund IV. The fund will continue the firm’s thesis of backing healthcare and biotechnology companies. ]]></content:encoded></item><item><title>Google’s data center energy use doubled in 4 years</title><link>https://techcrunch.com/2025/07/01/googles-data-center-energy-use-doubled-in-four-years/</link><author>Tim De Chant</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:52:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google has pledged to use only carbon-free sources of electricity to power its operations, a task made more challenging by its breakneck pace of data center growth.]]></content:encoded></item><item><title>7 Iconic TV Characters Whose Names Remain a Mystery</title><link>https://hackernoon.com/7-iconic-tv-characters-whose-names-remain-a-mystery?source=rss</link><author>Fayam Ayekame</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:37:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Names are one of the most prominent features of TV, holding an almost spiritual significance. Likewise, the absence of a name can also be a powerful tool, adding mystery to your favourite characters. Whether accidental or intentional, several beloved TV characters have remained nameless and managed to be loved by fans regardless. Here are some of the most prominent small-screen characters who remain nameless throughout their shows’ run:That ‘70s Show produced several comedic gems and running tropes over its eight-season run. Through it all, one of the key features on the show was Fez, the foreign exchange student who wasn't quite exchanged back. The lovable foreigner quickly became a fan favourite with his witty comebacks and his ever-elusive search for love.Fez’s real name is mentioned during the show, but school bells drown it out in keeping with the running Gag. His home nation, on the other hand, is never mentioned, with further confusion when his best friend from home comes to visit, and is unmistakably British. Penny - The Big Bang TheoryPenny was the ultimate girl next door, bringing socialisation skills and down-to-earth relationships to her scientist neighbours. While her first name is a common feature in the world of TV, her last name remains a mystery to date. Later in the series, she takes on Hofstadter after her marriage to Leonard, but her original family name is never mentioned. Despite her entire family making an appearance on the show, the family name is miraculously left out, creating the ultimate running gag. It's no surprise to see that the longest-running show on TV has the longest-running gag, “What is the doctor’s name?” This trope is also a play on the show’s title, with the question hidden in plain sight, “Dr Who”. While he is known throughout the cosmos as “The Doctor”, the man from Gallifrey has a name, which is never actually disclosed. The title of “Doctor” was chosen as a promise to the universe and has since become his identity. Throughout the series, many have sought to find out his actual name, with only  achieving this objective. We are introduced to our mystery man, Lucas Hood, in the very first episode of Banshee. After witnessing Lucas Hood’s name, our main character assumes his identity and position as the new sheriff of Banshee. The new Lucas Hood brandishes a unique brand of justice throughout the series, as he attempts to honour the badge he now wears. While there are characters who know his real name, it is never revealed, and his true identity remains a mystery. A brief glance into his past reveals a name on file as , which is just another aliasJohn Reese - Person Of InterestPerson of Interest lasted five seasons and introduced us to the computing power of AI with  and  at the centre of it all. John Reese is an ex-CIA operative whose real name has long been buried, operating under an alias, and running errands for “”. Despite assuming multiple identities throughout the series, John Reese remains the central identity of our main character. Rip Hunter - DC’s Legends of TomorrowDC’s Legends of Tomorrow revolves around a group of time travellers roaming the timeline and fixing problems. Our time travellers are put together and captained by , a 22nd-century time agent seeking retribution for the death of his family. As an agent of the Time Bureau, all records of his previous life and ancestry have been wiped clean, with the alias Rip Hunter becoming his identity. While his time on the show is short-lived, his identity remains a mystery to ensure the protection of the timeline.  is one of the most beloved side characters on Scrubs, thanks to his long-running feud with . Despite his constant presence on the show, we never actually learn his real name, and it eventually makes for an interesting trope. At one point, he jokes that his real name is  a play on Janitor, only to mess with JD and make a point.During his wedding, he is referred to as the Janitor, with his wife seemingly becoming known as Mrs. Janitor, and seemingly accepting it. While the Janitor ]]></content:encoded></item><item><title>IT Worker Sentenced To Seven Months After Trashing Company Network</title><link>https://it.slashdot.org/story/25/07/01/1552216/it-worker-sentenced-to-seven-months-after-trashing-company-network?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: A judge has sentenced a disgruntled IT worker to more than seven months in prison after he wreaked havoc on his employer's network following his suspension, according to West Yorkshire Police. 

According to the police, Mohammed Umar Taj, 31, from the Yorkshire town of Batley, was suspended from his job in nearby Huddersfield in July 2022. But the company didn't immediately rescind his network credentials, and within hours, he began altering login names and passwords to disrupt operations, the statement says. 

The following day, he allegedly changed access credentials and the biz's multi-factor authentication settings that locked out the firm and its clients in Germany and Bahrain, eventually causing an estimated $274,200 in lost business and reputational harm.]]></content:encoded></item><item><title>Automattic puts Tumblr migration to WordPress on hold</title><link>https://techcrunch.com/2025/07/01/automattic-puts-tumblr-migration-to-wordpress-on-hold/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:13:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Automattic CEO Matt Mullenweg confirmed that the company is no longer working on migrating its Tumblr blogging platform to WordPress, as previously announced. ]]></content:encoded></item><item><title>Why They’ll Never Get You—and Why That’s Okay</title><link>https://hackernoon.com/why-theyll-never-get-youand-why-thats-okay?source=rss</link><author>BenoitMalige</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What I realized when I stopped needing to be understood.This newsletter is sponsored by.. no one. That being said, have you read the first chapter of  Available here for download:This morning I sat at a café in silence.\
Not on purpose. I just didn’t have much left to say.\
I’ve been talking a lot lately—about my book, about overthinking, about clarity and freedom and becoming. But when you’re in the middle of sharing your story with the world, something strange happens: you start losing touch with the parts of it that were never meant to be shared.\
You start forgetting which thoughts were sacred, and which were strategic.\
So I sat down, let the noise settle, and wrote one sentence.\
“Your depth was never meant to be understood by those who live on the surface.”\
And then, without thinking:\
“That works for your ego, too.”\
It wasn’t planned. It wasn’t poetic. But it felt true enough to stop me. Because somewhere in those two lines was the real reason I’ve felt so disconnected lately—not just from others, but from myself.\
For most of my life, I’ve believed that being misunderstood was one of the worst feelings in the world.\
And in many ways, it is.\
When you’re trying to be honest, when you’re doing the work, when you’ve found the courage to show up without the mask.. and someone still doesn’t get it? Still doesn’t get ? It cuts deeper than silence ever could.\
But that’s not what I’ve been struggling with lately.\
Because I haven’t been misunderstood. And .\
And the person who did that… was me.\
I’ve simplified my words so they wouldn’t intimidate. Softened my thoughts so they’d feel more relatable. Downplayed my insights so they wouldn’t sound like I was trying too hard.\
Rounded off the edges. Wrapped it all in a layer of warmth and good intentions.\
Not because I’m fake. But because I’ve been trying to stay visible—to . To anyone. And that need? That quiet, gnawing hunger to be understood?\
It sounds noble to want to be understood.But sometimes it’s just a socially acceptable way of saying: We dilute ourselves to become palatable. And then we blame the world for not tasting the truth.\
This morning, I sat with the discomfort of that.\
Because maybe the goal was never to be fully understood in the first place.Maybe that’s just what the ego tells us when it’s tired of feeling alone.\
And maybe wholeness is quieter than we expected. Not loud, not proven, not validated: Just intact.\
Here’s what I’m holding close right now:If someone only sees your surface, it doesn’t mean your depth doesn’t exist.Your ego will always want to be seen. Your soul just wants to be .You weren’t misunderstood. You were translated.You don’t owe anyone a version of you that fits better.And you don’t have to shrink to stay safe anymore.This morning’s message to myself was simple:]]></content:encoded></item><item><title>GM’s Home-Grown LMR Battery Opens New Front in EV Competition</title><link>https://spectrum.ieee.org/general-motors-lmr-battery</link><author>Lawrence Ulrich</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEzNTM0OS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3ODUxOTYxOH0.wzZ6yTRrw7VtBOVPomWDmAx98DZ4ZJw9GVYgQNL8kCQ/image.jpg?width=600" length="" type=""/><pubDate>Tue, 1 Jul 2025 18:58:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Manganese-rich batteries could leapfrog China’s lithium-ion phosphate cells]]></content:encoded></item><item><title>Winning capital for your AI startup? Kleida Martiro is leading the conversation at TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/winning-capital-for-your-ai-startup-kleida-martiro-is-leading-the-conversation-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:52:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AI-native startups are rewriting the rules of what early traction looks like — and too often, investors are still playing by the old ones. At TechCrunch All Stage, happening in Boston on July 15, Kleida Martiro, partner at Glasswing Ventures, will lead a breakout that cuts straight to the core of this disconnect.]]></content:encoded></item><item><title>AI is Now Screening Job Candidates Before Humans Ever See Them</title><link>https://slashdot.org/story/25/07/01/186240/ai-is-now-screening-job-candidates-before-humans-ever-see-them?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AI agents are now conducting first-round job interviews to screen candidates before human recruiters review them, according to WashingtonPost, which cites job seekers who report being contacted by virtual recruiters from different staffing companies. The conversational agents, built on large language models, help recruiting firms respond to every applicant and conduct interviews around the clock as companies face increasingly large talent pools. 

LinkedIn reported that job applications have jumped 30% in the last two years, partially due to AI, with some positions receiving hundreds of applications within hours. The Society for Human Resource Management said a growing number of organizations now use AI for recruiting to automate candidate searches and communicate with applicants during interviews. The AI interviews, conducted by phone or video, can last anywhere from a few minutes to 20 minutes depending on the candidate's experience and the hiring firm's questions.]]></content:encoded></item><item><title>Nothing releases its first over-the-ear headphones, the $299 Headphone (1)</title><link>https://techcrunch.com/2025/07/01/nothing-releases-its-first-over-the-ear-headphones-the-299-headphone-1/</link><author>Maggie Stamets</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:38:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing stepped away from sensors in favor of a simple button to trigger your AI assistant or ChatGPT, if you have the Nothing X app, and a volume roller that can also be pressed to play, pause, and turn on and off noise canceling.]]></content:encoded></item><item><title>Limitless Raise $4m Strategic Funding, Launch Points Ahead Of TGE</title><link>https://hackernoon.com/limitless-raise-$4m-strategic-funding-launch-points-ahead-of-tge?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:12:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[New York, NY, USA, July 1st, 2025/Chainwire/--The largest prediction market on Base, , today announces the closure of $4M in fresh funding in a strategic round and welcomes Arthur Hayes as an advisor alongside an investment from his family office, Maelstrom. The funding follows a prior $3M pre-seed round led by 1confirmation, and comes after the team found breakout demand for short term prices markets on assets like BTC, which are similar in nature to 0DTE options but a much easier way to trade and feature even shorter dated expiries such as hourly, racking up over $250M in volume soon after launch. This brings Limitless' total funding raised to $7M, backed by Coinbase Ventures, 1confirmation, Maelstrom, Collider, Node Capital, Paper Ventures, Public Works, Punk DAO, and WAGMI Ventures, as well as individual investors via the Base Ecosystem Fund group on Echo. In preparation for an upcoming TGE, the team today launched a points program targeted at prediction market enthusiasts who can get skin in the game by using the product, providing liquidity, and referring their friends to join the platform. Limitless seems likely to become the first major prediction market platform to launch a token and distribute an airdrop to its early customers, marking a notable opportunity for retail traders. The team also just introduced a new mobile-first trading experience that enables people around the world to seamlessly wager on their favorite assets' performance in the next hour or day. “The future of trading is easy, fast, and powered by an army of token holders. We’re excited to bring this vision to reality,” said CJ Hetherington, CEO at Limitless Labs. is the largest prediction market on Base with over $250M bet on unique contracts that allow users to wager on the performance of their favorite assets in the next minutes, hour or day - a net new, easy way to trade for casual users.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Block3 Unveils Prompt-To-Game AI Engine As Presale Launches</title><link>https://hackernoon.com/block3-unveils-prompt-to-game-ai-engine-as-presale-launches?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:05:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[SF, CA, July 1st, 2025/Chainwire/--AI is steadily making deeper inroads into major tech industries, and its latest leap into the gaming sector marks a notable shift.Thousands have already backed : to build the world’s most advanced prompt-to-play engine, which will let anyone generate playable worlds in minutes from a simple text prompt. The scale of the impact is already spawning an equally vocal counter-movement, with experts saying this could be the .Already being called the “”, Block3 has a solution to one of AI’s ongoing challenges, which is how to get enough data for its LLM to learn and grow.By introducing a create2earn element, where users get paid for contributing, Block3 has found a practical route to accelerate product development. To enable this ambitious vision, Block3 is opening a limited-time token presale for just 90 days.The BL3 token goes on sale at 9AM UTC today, July 1st, with tokens initially available at just $0.01. Prices increase by 5% every 72 hours, offering early investors gains of 312% by the time it hits major exchanges. The token is only available for purchase through their .What is Block3 and Prompt-to-play GamingBlock3 is made up of a team of developers who are going head-to-head with the gaming industry, by using AI to put creative power in the hands of gamers rather than gaming studios.Think about it like an AI chatbot, where the user puts in prompts and gets an answer back from AI. With Block3, the mechanism is the same, except the output is a fully-formed video game, or in essence an immersive world customised to whatever the person typing can imagine.This concept is prompting a growing number of users to explore decentralized exchange (DEX) platforms. The potential applications for this technology are wide-ranging.For the more technical, the  reveals the complexity behind the concept and reveals the team’s roadmap. This is where the scale of the project is revealed, over a series of ambitious milestones that may see Block3 burgeon from fanatical fanbase to Unicorn over months of intensive development.The Threat to Gaming: Saving a $665 Billion IndustryAnd now, its blockbuster moment may have arrived. tackles this issue head-on and has thrown a spanner in the works. Even a 0.16% share of projected market revenue would generate over $1 billion.In a statement from the team, they said: “Traditional game dev is dead. For the first time, anyone can build games, not just studios with bloated teams and red tape. We’re here to unlock the imagination of gamers, and if we break a few corporate conglomerates along the way, then so be it.”The BL3 Presale is Now LiveThe gaming industry shows no sign of stopping, and AI gaming specifically is experiencing an impressive .Block3 is well-positioned to benefit from these mega-trends, and with $665 billion of potential revenue up for grabs, there is ample incentive for the developers to scale this as big and fast as they possibly can.The concept has already attracted an active community, generating notable attention online. With the presale set at a launch price of $0.01, early participants are positioning themselves ahead of the official rollout.Both the crypto and AI sectors are known for rapid innovation, and this project represents one of the more expansive efforts to emerge recently. is pioneering a new era in gaming by building the world’s first AI-native prompt-to-play platform. Designed to let anyone generate fully playable game worlds from a simple prompt, Block3 is eliminating the traditional barriers to game creation. By merging generative AI with real-time game logic and deployable environments, it’s changing how games are built and who gets to build them.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Cloudflare Flips AI Scraping Model With Pay-Per-Crawl System For Publishers</title><link>https://tech.slashdot.org/story/25/07/01/1745245/cloudflare-flips-ai-scraping-model-with-pay-per-crawl-system-for-publishers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Cloudflare today announced a "Pay Per Crawl" program that allows website owners to charge AI companies for accessing their content, a potential revenue stream for publishers whose work is increasingly being scraped to train AI models. The system uses HTTP response code 402 to enable content creators to set per-request prices across their sites. Publishers can choose to allow free access, require payment at a configured rate, or block crawlers entirely. 

When an AI crawler requests paid content, it either presents payment intent via request headers for successful access or receives a "402 Payment Required" response with pricing information. Cloudflare acts as the merchant of record and handles the underlying technical infrastructure. The company aggregates billing events, charges crawlers, and distributes earnings to publishers. 

Alongside Pay Per Crawl, Cloudflare has switched to blocking AI crawlers by default for its customers, becoming the first major internet infrastructure provider to require explicit permission for AI access. The company handles traffic for 20% of the web and more than one million customers have already activated its AI-blocking tools since their September 2024 launch, it wrote in a blog post.]]></content:encoded></item><item><title>National Guard Troops Sent To California By Trump Are Just Out There Doing Drug Busts</title><link>https://www.techdirt.com/2025/07/01/national-guard-troops-sent-to-california-by-trump-are-just-out-there-doing-drug-busts/</link><author>Tim Cushing</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:59:52 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Martial law? Police state? These are just things the alleged Leader of Free World , rather than things a nation founded on rejecting these options should be in the process of instituting. And yet, here we are, barely six months into Trump’s return to office, staring down the barrel of both of these related horrors.If it looks like fascism, it’s probably not intentional. Trump simply isn’t smart enough to implement the real thing. But he does like authoritarianism, which looks a lot like fascism, because he’s always felt a president should be treated like a king — someone who answers to no one, not even his 340 million employers. Trump tested the waters on martial law during his last term, threatening to send troops out to handle George Floyd protests. This time around, he’s amped everything up, openly hoping to turn every “Democrat” city into Kent State.Legally, he’s not allowed to do this. But his administration is relying on some vagueness in the law to get around the long-standing prohibition of sending in the army (so to speak) to police the populace. So, we get the sort of thing we’ve seen recently, where a Los Angeles swap meet was treated like an open-air market in some Middle Eastern country we’re currently at (undeclared) war with.The National Guard troops sent to Los Angeles are presumably still working without pay and/or beds, but that isn’t stopping them from blending in with federal law enforcement to aid and abet actual law enforcement work. First reported by CBS, a combined force of more than 500 federal officers and National Guard troops walked away from the ICE raids and the protection of federal property to perform a bog standard drug bust. Nicholas Slayton has more details for Task and Purpose, a military-oriented publication: California National Guard soldiers operating under federal orders helped the Drug Enforcement Administration and other federal personnel carry out a raid on a large marijuana growth operation in the eastern Coachella Valley last week, 130 miles from downtown Los Angeles. It’s unclear how many National Guard troops participated in the operation, but the force totalled roughly 500 people. According to the DEA, other agencies included Customs and Border Patrol, Bureau of Alcohol, Tobacco, Firearms and Explosives, Immigration and Custom Enforcement and the Federal Bureau of Investigation.While this commandeering of California National Guard troops may have originally been for the unstated purpose of pushing back against anti-ICE protests, now that they’re here, the administration has decided to just use them for whatever. This raid of multiple marijuana farms occurred more than  from the boundaries of Los Angeles County and even further away from the location these troops were originally sent: downtown Los Angeles. According to Trump’s military, everything about this is good and fine and nothing to be concerned about. After all, the law says the military can help federal cops, even if it (supposedly) prevents them from doing actual cop work. That’s the Title 10 vagueness the military is relying on when it serves up statements like this” “The catalyst of this order was related to events occurring in Los Angeles; however, the president’s order and NORTHCOM’s mission is not constrained by the geography of Southern California. Recently, Title 10 forces supported a Drug Enforcement Agency operation a few hours outside of Los Angeles. Title 10 forces protect federal personnel who are performing federal law enforcement functions…”Hence the military-provided shots of alleged National Guard troops allegedly manning the perimeter of the places being raided. And, also hence, the narrative no one can definitively dispute because — despite the National Guard embedding with federal law enforcement agencies — no journalists are being allowed to embed with military-esque operations occurring  the borders of the United States.Of course, we’ve already seen Marines detain people for the purpose of handing them over to law enforcement. And we’ve seen National Guard troops swarm a swap meet like they’re looking for terrorists in a foreign country, rather than just anyone looking kind of Hispanic who might not have the proper paperwork on them. The more things like this occur, the more easily many people will just come to accept this is the way the United States operates now. Many of them will cheer on these efforts, failing to recognize the abuse of these powers may, at some point, target them. But for the rest of us, this shouldn’t be allowed to pass without notice. Trump may be a blowhard and an idiot, but he’s surrounded by people who truly desire an opportunity to perform a hard reset on democracy and its principles, replacing it with jackboot heels, racism, fascism, and — eventually — a return of the British Empire, this time wrapped in an American flag.]]></content:encoded></item><item><title>Daily Deal: Academy of Educational Engineering</title><link>https://www.techdirt.com/2025/07/01/daily-deal-academy-of-educational-engineering-3/</link><author>Daily Deal</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:53:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The Academy of Educational Engineering is a premier platform tailored for aspiring and professional geeks. This all-in-one educational ecosystem is designed to empower you with expert-level knowledge and hands-on experience across embedded systems, electronics, IoT, and software development. As a premium member, you’ll access comprehensive tools, engaging projects, personalized feedback, and direct mentorship, helping you elevate your career in the tech industry. Whether you’re a beginner or a professional, this is your ultimate gateway to mastering the future of technology. It’s on sale for $50.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Nothing releases its first over-the-ear headphones</title><link>https://techcrunch.com/video/nothing-releases-its-first-over-the-ear-headphones/</link><author>TC Video</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:52:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing has revealed its first over-the-ear headphones. The aluminum and transparent Headphone (1) was designed in collaboration with KEF. They offer adaptive noise canceling, bass enhancement for deeper low frequencies, and spatial audio, for $299 at launch. But spoiler alert: The buttons on these headphones might be the best part.]]></content:encoded></item><item><title>Midas And 0G Partner To Bring Real-World Assets To AI-Native Blockchain Infrastructure</title><link>https://hackernoon.com/midas-and-0g-partner-to-bring-real-world-assets-to-ai-native-blockchain-infrastructure?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:49:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, Republic of Singapore, July 1st, 2025/Chainwire/--Tokenization protocol Midas and AI blockchain 0G have announced a strategic partnership to unlock the next wave of onchain finance through modular design. By combining Midas’ tokenisation infrastructure with 0G’s decentralized AI-native compute, the partners will develop new solutions that intelligently leverage real-world assets (RWAs).As part of the partnership, Midas will deploy on the 0G mainnet, scheduled for late Q3 2025, bringing its full stack of tokenization infrastructure. In parallel, 0G will integrate Midas’ tokenized instruments and vault logic into its optimized AI layer. This will position both platforms to serve institutions, developers, and liquidity providers at scale.Midas offers a compliant protocol suite for issuing tokenized certificates tracking institutional-grade strategies. Its tokens, including mF-ONE, mMEV, mEDGE, mRE7YIELD, mBASIS and mTBILL, provide exposure via tokenized certificates to reference real-world assets across private credit, US short-term treasuries, and market-neutral strategies.0G Labs CEO Michael Heinrich said: “Midas have made huge strides in expanding compliant access to tokenized RWAs and we’re delighted that they’ve chosen to build on 0G. We’re excited to be collaborating with them to develop new financial products that will combine AI with tokenized assets, giving users greater onchain opportunities than ever before.” By launching on 0G, Midas will introduce compliant, composable tokens into a modular environment optimized for AI-powered workflows and smart contract automation. Use cases range from onchain lending vaults and automated credit exposures to AI-enhanced risk analytics and composable strategy deployment. 0G’s modular Layer 1 blockchain is purpose-built for AI-native applications.It combines high-performance compute, decentralized storage, data availability, and low-latency smart contract execution, ideal for deploying data-intensive financial applications and real-time DeFi logic. 0G’s architecture supports seamless integration with EVM and non-EVM ecosystems, while its recent Galileo testnet demonstrated sustained throughput and low gas costs. It also saw significant developer adoption with over 170 million transactions and 13 million accounts in under two months. The collaboration between Midas and 0G reflects a shared vision: to make programmable, compliant financial infrastructure natively interoperable with the AI applications of the future.Midas is a tokenisation platform building institutional-grade financial products for the open web. Its ERC-20 tokens are structured to track dedicated strategies with verifiable on-chain performance, combining TradFi-grade standards with DeFi composability. Midas is backed by leading investors like Framework Ventures, BlockTower Capital, and GSR, and partners with regulated custodians to ensure strong compliance and risk controls. Learn more: https://midas.app/About 0G 0G is the first decentralized AI protocol (AIP), purpose-built to power a truly democratized future of intelligence. As a modular and infinitely scalable Layer 1, 0G enables the execution of decentralized AI applications at scale. It unifies high-performance decentralized storage, compute, and data availability (DA) to support the next generation of AI-native use cases. With verifiable AI processing and a permissionless agent ecosystem, 0G is laying the foundation for an open and unstoppable AI economy. Learn more: https://0g.ai/Disclaimer This announcement is for informational purposes only and does not constitute investment advice or an offer to sell or buy any financial instrument. Midas-issued tokens are not available to US & UK persons and entities, or those from sanctioned jurisdictions. This is not investment advice.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Nothing launches its most expensive flagship yet, Phone (3)</title><link>https://techcrunch.com/2025/07/01/nothing-launches-its-most-expensive-flagship-phone-3/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing launched its newest flagship phone after a two-year gap. At an event in London, the company unveiled the Phone (3), which starts at $799 and aims to take on bigwigs like Samsung and Apple.]]></content:encoded></item><item><title>AI Arms Race Drives Engineer Pay To More Than $10 Million</title><link>https://tech.slashdot.org/story/25/07/01/1536223/ai-arms-race-drives-engineer-pay-to-more-than-10-million?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Tech companies are paying AI engineers unprecedented salaries as competition for talent intensifies, with some top engineers earning more than $10 million annually and typical packages ranging from $3 million to $7 million. OpenAI told staff this week it is seeking "creative ways to recognize and reward top talent" after losing key employees to rivals, despite offering salaries near the top of the market. 

The move followed OpenAI CEO Sam Altman's claim that Meta had promised $100 million sign-on bonuses to the company's most high-profile AI engineers. Mark Chen, OpenAI's chief research officer, sent an internal memo saying he felt "as if someone has broken into our home and stolen something" after recent departures. 

AI engineer salaries have risen approximately 50% since 2022, with mid-to-senior level research scientists now earning $500,000 to $2 million at major tech companies, compared to $180,000 to $220,000 for senior software engineers without AI experience.]]></content:encoded></item><item><title>Best iPad apps to boost productivity and make your life easier</title><link>https://techcrunch.com/2025/07/01/best-ipad-apps-to-boost-productivity-and-make-your-life-easier/</link><author>Aisha Malik</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:18:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[There are many iPad apps to help you organize recipes; sync tasks across devices; be more productive; and manage your notes.]]></content:encoded></item><item><title>Raising a Series C+? Cathy Gao’s bringing the real playbook to TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/raising-a-series-c-cathy-gaos-bringing-the-real-playbook-to-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:13:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Cathy Gao of Sapphire Ventures shares her playbook to scale a Series C+ at TechCrunch All Stage on July 15 in Boston. Register now to save more than 60% on your tickets.]]></content:encoded></item><item><title>Amazon deploys its 1 millionth robot, releases generative AI model</title><link>https://techcrunch.com/2025/07/01/amazon-deploys-its-1-millionth-robot-releases-generative-ai-model/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:01:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As Amazon's fleet of robots has reaches a milestone, the company is also releasing a new AI model to make them more efficient. ]]></content:encoded></item><item><title>Global Warming Is Speeding Up and the World Is Feeling the Effects</title><link>https://news.slashdot.org/story/25/07/01/164239/global-warming-is-speeding-up-and-the-world-is-feeling-the-effects?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Summer started barely a week ago, and already the United States has been smothered in a record-breaking "heat dome." Alaska saw its first-ever heat advisory this month. And all of this comes on the heels of 2024, the hottest calendar year in recorded history. The world is getting hotter, faster. A report published last week found that human-caused global warming is now increasing by 0.27 degrees Celsius per decade. That rate was recorded at 0.2 degrees in the 1970s, and has been growing since. 

"Each additional fractional degree of warming brings about a relatively larger increase in atmospheric extremes, like extreme downpours and severe droughts and wildfires," said Daniel Swain, a climate scientist at the University of California. While this aligns with scientific predictions of how climate change can intensify such events, the increase in severity may feel sudden to people who experience them. 

"Back when we had lesser levels of warming, that relationship was a little bit less dramatic," Dr. Swain said. "There is growing evidence that the most extreme extremes probably will increase faster and to a greater extent than we used to think was the case," he added. Take rainfall, for example. Generally, extreme rainfall is intensifying at a rate of 7 percent with each degree Celsius of atmospheric warming. But recent studies indicate that so-called record-shattering events are increasing at double that rate, Dr. Swain said.]]></content:encoded></item><item><title>The End of the Guessing Game? Why Describing Data Beats Estimating It</title><link>https://hackernoon.com/the-end-of-the-guessing-game-why-describing-data-beats-estimating-it?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:38:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Achiam, J., Andrychowicz, M., Beattie, A., Clark, J., Drozdov, N., Ecoffet, A., Edwards, D., Giddings, J., Goldberg, I., Gomez, M., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)Batista, G.E., Monard, M.C.: A study of k-nearest neighbour as an imputation method. In: Frontiers in Artificial Intelligence and Applications. vol. 87, pp. 251–260. HIS (2002)Biessmann, F., Salinas, D., Schelter, S., Schmidt, P., Lange, D.: "deep" learning for missing value imputation in tables with non-numerical data. In: Proceedings of the 27th ACM International Conference on Information and Knowledge Management. p. 2017–2025. CIKM ’18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3269206.3272005, https://doi.org/10.1145/3269206.3272005Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are Few-Shot Learners. In: Advances in Neural Information Processing Systems. vol. 33, pp. 1877–1901. Curran Associates, Inc. (2020)Buuren, S.v., Groothuis-Oudshoorn, K.: Mice: Multivariate imputation by chained equations in r. Journal of Statistical Software 45, 1–67 (2011)Camino, R.D., Hammerschmidt, C.A., State, R.: Improving missing data imputation with deep generative models. arXiv preprint arXiv:1902.10666 pp. 1–8 (2019)Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., Fiedel, N.: PaLM: Scaling Language Modeling with Pathways (Oct 2022), http://arxiv.org/abs/2204.02311, arXiv:2204.02311 [cs]Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological) 39(1), 1–22 (1977)Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetuning of quantized llms (2023)Emmanuel, T., Maupong, T., Mpoeleng, D., Semong, T., Mphago, B., Tabona, O.: A survey on missing data in machine learning. J Big Data 8(1), 140 (2021). https://doi.org/10.1186/s40537-021-00516-9, epub 2021 Oct 27. PMID: 34722113; PMCID: PMC8549433Gimpy, M.: Missing value imputation in multi attribute data set. Int. J. Comput. Sci. Inf. Technol. 5(4), 1–7 (2014)Gondara, L., Wang, K.: Mida: Multiple imputation using denoising autoencoders. In: PacificAsia conference on knowledge discovery and data mining. pp. 260–272. Springer (2018)Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al.: Generative adversarial nets. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger, K.Q. (eds.) Advances in Neural Information Processing Systems. vol. 27, pp. 2672–2680. Curran Associates, Inc., Montréal, Canada (2014)Gupta, A., Lam, M.S.: Estimating missing values using neural networks. Journal of the Operational Research Society 47(2), 229–238 (1996)Hallaji, E., Razavi-Far, R., Saif, M.: Dlin: Deep ladder imputation network. IEEE Transactions on Cybernetics 52(9), 8629–8641 (2021)Jäger, S., Allhorn, A., Biessmann, F.: A benchmark for data imputation methods. Front Big Data 4, 693674 (2021). https://doi.org/10.3389/fdata.2021.693674, pMID: 34308343; PMCID: PMC8297389 Enhancing Imputation Accuracy with Contextual Large Language Models 15Little, R.J., Rubin, D.B.: Statistical Analysis with Missing Data, vol. 793. John Wiley & Sons, 3 edn. (2019)Little, R.J.A., Rubin, D.B.: Statistical Analysis with Missing Data. John Wiley & Sons, Hoboken, 2 edn. (2002)Lu, H.m., Perrone, G., Unpingco, J.: Multiple imputation with denoising autoencoder using metamorphic truth and imputation feedback. arXiv preprint arXiv:2002.08338 (2020)McCoy, J.T., Kroon, S., Auret, L.: Variational autoencoders for missing data imputation with application to a simulated milling circuit. IFAC-PapersOnLine 51(21), 141–146 (2018), 5th IFAC Workshop on Mining, Mineral and Metal Processing MMM 2018Nazabal, A., Olmos, P.M., Ghahramani, Z., Valera, I.: Handling incomplete heterogeneous data using vaes. arXiv preprint arXiv:1807.03653 (2018)Qiu, Y.L., Zheng, H., Gevaert, O.: Genomic data imputation with variational auto-encoders. GigaScience 9(8), giaa082 (2020)Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21(1), 140:5485–140:5551 (Jan 2020)Roberts, A., Raffel, C., Shazeer, N.: How Much Knowledge Can You Pack Into the Parameters of a Language Model? In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5418–5426. Association for Computational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.437, https://aclanthology.org/2020.emnlp-main.437Rubin, D.B.: Multiple imputations in sample surveys-a phenomenological bayesian approach to nonresponse. In: Proceedings of the survey research methods section of the American Statistical Association. vol. 1, pp. 20–34. American Statistical Association, Alexandria, VA, USA (1978)Rubin, D.B.: Multiple Imputation for Nonresponse in Surveys. John Wiley & Sons, New York, NY (2004)Schafer, J.L.: Analysis of Incomplete Multivariate Data. Chapman & Hall/CRC, London, UK (1997)Schelter, S., Rukat, T., Biessmann, F.: JENGA - A framework to study the impact of data errors on the predictions of machine learning models. In: Velegrakis, Y., Zeinalipour-Yazti, D., Chrysanthis, P.K., Guerra, F. (eds.) Proceedings of the 24th International Conference on Extending Database Technology, EDBT 2021, Nicosia, Cyprus, March 23 - 26, 2021. pp. 529–534. OpenProceedings.org (2021). https://doi.org/10.5441/002/EDBT.2021.63, https://doi.org/10.5441/002/edbt.2021.63Sharpe, P.K., Solly, R.: Dealing with missing values in neural network-based diagnostic systems. Neural Computing & Applications 3(2), 73–77 (1995)Stekhoven, D.J., Bühlmann, P.: Missforest—non-parametric missing value imputation for mixed-type data. Bioinformatics 28(1), 112–118 (2012)Stoyanovich, J., Howe, B., Jagadish, H.V.: Responsible data management. Proceedings of the VLDB Endowment 13, 3474–3488 (2020). https://doi.org/10.14778/ 3415478.3415570Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (Feb 2023), http://arxiv.org/ abs/2302.13971, arXiv:2302.13971 [cs]Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.: Llama 2: Open foundation and fine-tuned chat models (2023)Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th international conference on machine learning. pp. 1096–1103 (2008)Yang, K., Huang, B., Stoyanovich, J., Schelter, S.: Fairness-aware instrumentation of preprocessing pipelines for machine learning. In: Proceedings of the Workshop on HumanIn-the-Loop Data Analytics (HILDA’20). ACM (2020). https://doi.org/10.1145/3398730.3399194Yoon, J., Jordon, J., van der Schaar, M.: Gain: Missing data imputation using generative adversarial nets. In: International conference on machine learning. pp. 5689–5698. PMLR (2018)Yoon, J., Jordon, J., van der Schaar, M.: Gain: Missing data imputation using generative adversarial nets (2018)]]></content:encoded></item><item><title>Why Mamdani’s Refusal To Condemn Speech He Never Made Is Good Free Speech Advocacy</title><link>https://www.techdirt.com/2025/07/01/why-mamdanis-refusal-to-condemn-speech-he-never-made-is-good-free-speech-advocacy/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:28:52 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[At a time when politicians on both sides reflexively call for censorship and speech policing, it’s refreshing to see someone actually defend free speech principles—especially when it would be politically easier to cave.That’s exactly what New York City Democratic mayoral nominee Zohran Mamdani did when NBC’s  tried to pressure him into condemning language he’s never used. Rather than take the bait, Mamdani delivered a strong defense of free speech principles. It’s a better defense of free speech than we’ve seen from most politicians lately.What makes this particularly frustrating is that many of the Democrats attacking Mamdani should be laser-focused on the existential threat Trump poses to democracy. Instead, they’re wasting time and energy going after someone who actually accomplished what establishment Democrats claim they desperately want: activating young people who often fail to vote. Mamdani didn’t just talk about engaging young voters—he did it, handily winning the Democratic primary by mobilizing exactly the demographic Democrats say they need. His reward? A coordinated attack campaign.The controversy stems from demands that Mamdani condemn the phrase “globalize the intifada”—language he doesn’t use but which critics insist he must denounce to prove he’s not antisemitic. It’s the kind of ridiculous purity test that marginalized politicians routinely face (but somehow, white, Christian, male politicians never do), demanding they repeatedly distance themselves from the words of others simply because they share some demographic or political similarity.But rather than playing that game, Mamdani chose to defend the principle that government officials shouldn’t be in the business of policing speech—even speech they personally disagree with. At the same time, he used the opportunity to move from the “gotcha” kind of question to a focus on how to tackle the actual problems of racism and bigotry, beyond just focusing on specific language questions.There’s been a lot of pressure on Mamdani to specifically criticize pro-Palestinian language used by others. And, over the weekend, he went on Meet the Press and gave, what I think, is a really strong answer to a silly gotcha question that I think others could learn from:I want to ask you about an issue that has divided some New Yorkers in recent weeks. You were recently asked about the term “globalize the intifada,” if it makes you uncomfortable. In that moment you did not condemn the phrase. Now, just so folks understand, it’s a phrase that many people hear as a call to violence against Jews. There’s been a lot of attention on this issue, so I want to give you an opportunity to respond here and now. Do you condemn that phrase “globalize the intifada?”That’s not language that I use. The language that I use and the language that I will continue to use to lead this city is that which speaks clearly to my intent, which is an intent grounded in a belief in universal human rights. And ultimately, that’s what is the foundation of so much of my politics, the belief that freedom and justice and safety are things that, to have meaning, have to be applied to all people, and that includes Israelis and Palestinians as well.But do you actually condemn it? I think that’s the question and the outstanding issue that a number of people, both of the Jewish faith and beyond, have. Do you condemn that phrase, “globalize the intifada,” which a lot of people hear as a call to violence against Jews?I’ve heard from many Jewish New Yorkers who have shared their concerns with me, especially in light of the horrific attacks that we saw in Washington, D.C. and in Boulder, Colorado about this moment of antisemitism in our country and in our city. And I’ve heard those fears and I’ve had those conversations. And ultimately, they are part and parcel of why, in my campaign, I’ve put forward a commitment to increase funding for anti-hate crime programming by 800%. I don’t believe that the role of the mayor is to police speech in the manner, especially of that of Donald Trump, who has put one New Yorker in jail, who’s just returned to his family, Mahmoud Khalil, for that very supposed crime of speech. Ultimately, what I think I need to show is the ability to not only talk about something but to tackle it and to make clear that there’s no room for antisemitism in this city. And we have to root out that bigotry, and ultimately we do that through the actions. And that is the mayor I will be, one that protects Jewish New Yorkers and lives up to that commitment through the work that I do.But very quickly for the people who care about the language and who feel really concerned by that phrase, why not just condemn it?My concern is to start to walk down the line of language and making clear what language I believe is permissible or impermissible takes me into a place similar to that of the president, who is looking to do those very kinds of things, putting people in jail for writing an oped. Putting them in jail for protesting. Ultimately, it’s not language that I use. It’s language I understand there are concerns about. And what I will do is showcase my vision for this city through my words and my actions.Note what he does here. It would be easy enough to give into the framing and make statement condemning the language. And while some will (in bad faith) argue his failure to outright condemn the language is an endorsement of it, that’s bullshit. His answer is actually very thoughtful and a good way to approach such bad faith questions.He starts out with a direct and clear denial of using that language:That’s not language that I use.This immediately deflates the premise that he’s somehow responsible for words he’s never spoken.He then immediately shifts to a more positive framing of how he views what he’s focused on in his hopes of becoming mayor: human rights for all.The language that I use and the language that I will continue to use to lead this city is that which speaks clearly to my intent, which is an intent grounded in a belief in universal human rights. And ultimately, that’s what is the foundation of so much of my politics, the belief that freedom and justice and safety are things that, to have meaning, have to be applied to all people, and that includes Israelis and Palestinians as well.When NBC’s Welker trots out the purity test point, demanding he condemn it, he points out that he shouldn’t be in the business of policing language, but rather is focused on actual concerns of the people he’s hoping to represent. In doing so, he makes it clear that he’s concerned about actual antisemitism and actual threats and risks, and he’s looking at what might actually help rather than policing specific language:I’ve heard from many Jewish New Yorkers who have shared their concerns with me, especially in light of the horrific attacks that we saw in Washington, D.C. and in Boulder, Colorado about this moment of antisemitism in our country and in our city. And I’ve heard those fears and I’ve had those conversations. And ultimately, they are part and parcel of why, in my campaign, I’ve put forward a commitment to increase funding for anti-hate crime programming by 800%.And then he pivots to a reasonable defense of free speech, not in the misleading sense the way others view it, but rather in noting that government shouldn’t be in the business of policing speech (as Trump is doing) but focusing on where the  problems of hate and bigotry show up.I don’t believe that the role of the mayor is to police speech in the manner, especially of that of Donald Trump, who has put one New Yorker in jail, who’s just returned to his family, Mahmoud Khalil, for that very supposed crime of speech. Ultimately, what I think I need to show is the ability to not only talk about something but to tackle it and to make clear that there’s no room for antisemitism in this city. And we have to root out that bigotry, and ultimately we do that through the actions.After Welker desperately goes back to the “but won’t you condemn the language” nonsense, he makes it clear that speaking out on specific language choices is not productive when his focus is on dealing with the actual underlying problems:My concern is to start to walk down the line of language and making clear what language I believe is permissible or impermissible takes me into a place similar to that of the president, who is looking to do those very kinds of things, putting people in jail for writing an oped. Putting them in jail for protesting. Ultimately, it’s not language that I use. It’s language I understand there are concerns about. And what I will do is showcase my vision for this city through my words and my actions.This final answer is particularly smart because it connects his refusal to condemn specific language to Trump’s actual authoritarian attacks on free speech. Rather than getting trapped in semantic debates about particular phrases, he’s defending the broader principle that government officials shouldn’t be arbiters of acceptable speech.The contrast is stark: while the Trump regime is literally jailing people for their speech, critics want Mamdani to engage in the kind of speech policing that leads down that same authoritarian path. His refusal isn’t endorsement of problematic language—it’s recognition that the role of government isn’t to play word police.This is exactly the kind of principled free speech defense we need more of, especially from Democrats who have too often been willing to compromise these principles for short-term political gain. While it would have been easy for Mamdani to simply condemn the phrase and move on, his more thoughtful approach actually serves the cause of free speech better.The irony is that many of the same people attacking Mamdani are Democrats who claim to be defending democracy against Trump’s authoritarianism. Yet they’re demanding exactly the kind of speech policing that authoritarian governments excel at—forcing officials to take public positions on specific language as loyalty tests.And yes, some could argue that simply condemning certain language is not the same as censoring it. It’s not. It’s stating an opinion. But there’s value in Mamdani making it clear he’d rather focus on the real underlying issues around bigotry and hatred than trying to say magic words to appease a media that would never ask similar questions of a white, Christian politician.In an era where politicians routinely cave to demands for performative condemnations and symbolic gestures, Mamdani’s approach stands out. He’s more interested in actual solutions—like his 800% increase in anti-hate crime funding—than in playing the gotcha game that dominates political discourse.This is what defending free speech actually looks like: not demanding the right to be an asshole without consequences, but refusing to let government officials become the arbiters of acceptable speech—and politely reframing the issue when the media insists on playing such a gotcha game. If more politicians followed Mamdani’s lead, we’d have a much healthier democratic discourse.]]></content:encoded></item><item><title>Built for Scale: eXchange1 Brings Institutional-Grade Crypto Trading to Indian Users</title><link>https://hackernoon.com/built-for-scale-exchange1-brings-institutional-grade-crypto-trading-to-indian-users?source=rss</link><author>ZEX MEDIA</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:16:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As India continues its ascent as the world’s most active crypto market, eXchange1 arrives with the infrastructure, innovation, and integrity to match the moment. With its official launch, the European-regulated platform opens access to a full suite of digital asset tools designed for modern traders, institutional clients, and ecosystem partners alike.While many platforms promise speed and scale, few back it up with the regulatory foundation and high-performance architecture that eXchange1 offers by default.eXchange1 positions itself as more than a trading venue. It is a complete digital finance ecosystem that merges cutting-edge technology with seamless user experience. The platform provides:Spot, Margin, and Futures TradingCopy Trading (for both Spot and Futures)Tokenized Investment ProductsWhether you’re a new investor exploring basic trades or an institution managing complex portfolios, eXchange1 offers functionality at every level. The platform is accessible across web, mobile, and API, enabling real-time participation with global markets from any device.Its Copy Trading feature, for example, empowers less experienced traders to mirror strategies of seasoned investors—while the automated tools allow advanced users to deploy algorithmic trades with precision.Built for India. Backed by the World.India’s fast-growing crypto base—119 million users in 2024 alone—demands more than just flashy apps. It demands scalability, reliability, and regulatory clarity. eXchange1 delivers on all fronts.The platform is built on a high-liquidity architecture designed to support institutional-scale trade volumes without bottlenecks. Back-end infrastructure is optimized for rapid execution, real-time price discovery, and minimal slippage—ensuring traders get exactly what they see.To support this mission in India, eXchange1 has partnered with a leading global fintech firm to ensure localized performance, reduced latency, and seamless access—even during market surges.“We’re not here with a one-size-fits-all model,” said CEO Ms. Sandoval Mera. “India’s scale and diversity deserve a platform that’s tailored, responsive, and resilient. We’ve invested in both infrastructure and partnerships to deliver that.”Enterprise-Grade Security by DefaultInfrastructure isn’t just about speed—it’s also about safety. eXchange1 operates with enterprise-grade security systems that ensure operational integrity, user protection, and compliance with global standards.Key security features include:Real-time risk monitoring systemsInstitutional-grade custodial protocolsMulti-layer authentication and transaction securityAudit-ready transparency for regulators and partnersWith the rising sophistication of crypto-related threats, such features are no longer optional—they are critical. eXchange1 ensures its infrastructure protects both user assets and market confidence.The platform is licensed under the Markets in Crypto-Assets Regulation (MiCA) by the Financial Crime Investigation Service (FCIS) of Lithuania, and is registered with India’s Financial Intelligence Unit (FIU).This dual compliance enables eXchange1 to integrate seamlessly with India’s evolving regulatory environment while retaining the global credibility needed to serve cross-border investors.Being MiCA-compliant also brings with it rigorous operating standards, including rules around asset segregation, risk exposure, transparency, and consumer protection—all of which are reflected in eXchange1’s infrastructure design.“The future of digital assets depends on platforms that are both innovative and accountable,” said Dr. James Newsome, Chairman of eXchange1 and former head of the U.S. Commodity Futures Trading Commission (CFTC). “We’ve built eXchange1 to be just that—a high-performance platform that regulators can work with, and users can depend on.”One of eXchange1’s key advantages is its ability to scale with its users. Retail traders benefit from an intuitive UI/UX and educational support, while institutions get access to:Advanced APIs for high-frequency tradingReal-time market data feedsDedicated account supportTokenized investment vehicles for diversificationThis dual approach—serving both ends of the crypto maturity spectrum—makes eXchange1 uniquely capable of addressing India’s diverse and rapidly evolving investor base.And with multilingual customer service available 24/7, the platform ensures every user, from first-time investor to fund manager, gets the assistance they need—when they need it.Future-Proofed for a Changing IndustryCrypto is changing fast. New asset classes, regulatory shifts, institutional adoption, and user behavior all demand platforms that can adapt in real time. eXchange1’s architecture is modular and forward-compatible, allowing it to integrate future features like:Cross-chain liquidity poolsCompliance automation for new jurisdictionsThis future-readiness is part of the platform’s DNA. It’s also why eXchange1 insists on building for long-term resilience rather than short-term speculation.Final Thoughts: A New Infrastructure StandardAs more Indian users seek robust, regulated platforms to trade and invest in crypto, infrastructure will become the dividing line between the serious players and the short-lived ones.eXchange1 has entered the Indian market not to test the waters, but to build a foundation—one grounded in regulation, engineered for performance, and designed for trust.From institutional-grade systems to retail-friendly tools, and from global oversight to local integration, eXchange1 represents what a next-generation exchange should look like: fast, safe, scalable, and accountable.]]></content:encoded></item><item><title>Nikita Bier joins X as head of product: ‘I’ve officially posted my way to the top’</title><link>https://techcrunch.com/2025/07/01/nikita-bier-joins-x-as-head-of-product-ive-officially-posted-my-way-to-the-top/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:13:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At X, Bier could potentially build features that drive adoption beyond the user base that's typically drawn to text-first social networks.]]></content:encoded></item><item><title>DORA Regulation Explained - Plus a Free Compliance Checklist</title><link>https://hackernoon.com/dora-regulation-explained-plus-a-free-compliance-checklist?source=rss</link><author>N2W</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:06:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Digital Operational Resilience Act (DORA) sets new EU-wide standards for digital risk in the financial sector. This guide breaks down key requirements, penalties, and regional implications, with best practices for compliance—including risk management, testing, and third-party oversight. Download a free checklist to get your organization DORA-ready.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Will VR Integration for Remote Work Become Permanent? (7/1/2025)</title><link>https://hackernoon.com/7-1-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:03:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, July 1, 2025?By @editingprotocol [ 4 Min read ] Run your blog like a product. Learn how to define your audience, set goals, and build a content strategy that drives real growth. Read More.By @allan-grain [ 3 Min read ] The best part of VR is that it provides a three-dimensional immersive space where employees can interact as if they were physically present. Read More.By @paoloap [ 6 Min read ] Discover how the original PDF design creates hurdles for AI document parsing and learn solutions for better, machine-readable document workflows. Read More.By @legalpdf [ 8 Min read ] Anthropic allegedly used over 7M pirated books to train Claude AI. Authors sue over copyright theft fueling a billion-dollar LLM business. Read More.By @techthrilled [ 4 Min read ] As soon as a16z’s Bryan Kim saw Cluely’s buzz online, he was intrigued. He hadn’t even met the founder yet, but he could tell the company was onto something. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>It&apos;s Not Just What&apos;s Missing, It&apos;s How You Say It: CLAIM&apos;s Winning Formula</title><link>https://hackernoon.com/its-not-just-whats-missing-its-how-you-say-it-claims-winning-formula?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:03:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).We conducted a series of experiments to systematically evaluate the efficacy of CLAIM in addressing the research questions presented in Section 1. Our validation criterion for CLAIM’s effectiveness was the post-imputation performance of pre-trained LLMs finetuned with missing-aware contextual datasets for downstream classification tasks. We focused on three types of missingness mechanisms: MCAR, MAR, and MNAR.\
. We evaluated the performance of CLAIM using seven real-life multivariate classification datasets from the UCI repository [12]. Detailed information on these datasets is provided in the Appendix.\
Baseline Imputation Methods. Our approach was compared against a broad spectrum of commonly-used baseline imputation methods, encompassing single imputation (SI) and multiple imputation (MI) techniques, non-ML and ML methods, and both discriminative and generative ML approaches.\
SI methods included mean imputation using the feature-wise mean (), kNearest Neighbors (k-NN) [3] (), a tree-based algorithm using MissForest [37] (), and a deep generative adversarial network for imputation using GAIN (Generative Adversarial Imputation Nets) [45] (). The MI method employed was MICE (Multiple Imputation by Chained Equations) [22] ().\
 The hyperparameter settings for the various imputation methods and the LLM used in our experiments are detailed below.\
Hyperparameters for Baseline Imputation Methods. For GAIN, we adhered to the hyperparameters specified in the original publication, setting α to 100, the batch size to 128, the hint rate at 0.9, and the number of iterations to 1000 for optimal performance. MissForest and MICE were configured with their respective default parameters as provided in their PyPI implementations[2]. For k-NN, we chose k = 5 and the Euclidean distance measure based on literature suggesting this configuration offers superior performance [15].\
 We utilized the 7 billion-parameter LLaMA 2 model [40], fine-tuning it with the parameter-efficient QLoRA method [11]. The settings were r = 16, α = 64, dropout = 0.1 with the task type set to “CAUSALLM”. The learning rate was 2e-4, using the “pagedadamw_32bit” optimizer.\
Experiments were conducted with a batch size of 4 across 50 epochs, considering memory constraints during fine-tuning. Tesla A40 GPUs (48GB RAM) were used for distributed training. For evaluation, we used 20% randomly sampled instances of each dataset. Models were evaluated five times, reporting both average performance and standard deviation.Figure 2 displays the experimental outcomes for seven datasets, where we benchmarked CLAIM against existing imputation methods. Performance metrics for LLMs fine-tuned on fully complete datasets (without any missing values, thus no imputation was necessary) were included for comparison. This approach delineates the effectiveness of CLAIM by providing a reference to baseline performances, offering a clearer perspective on the benefits provided by CLAIM over traditional imputation methods.\
: How effective is CLAIM in imputing missing values across the distinct missingness mechanisms (MCAR, MAR, and MNAR) and how does it compare with existing imputation methods in terms of accuracy and robustness across varied datasets and missing data scenarios?\
: CLAIM demonstrated superior accuracy in imputing missing values across all datasets compared to baseline imputation methods. Its performance under the MCAR assumption, where missingness is independent of any data, suggests that CLAIM efficiently leverages the contextual information inherent in the dataset for imputation. This efficiency is particularly evident in its ability to significantly close the gap towards the performance of fully complete datasets (no imputation), showcasing its effectiveness\
 Under MAR, where missingness depends on observed data, the adaptability of CLAIM is further highlighted. It outperforms other methods by a considerable margin, indicating its proficiency in utilizing available data points to predict missing values accurately.\
: The MNAR scenario, characterized by missingness that depends on unobserved data, poses the greatest challenge. Here, CLAIM’s performance remains notably superior to traditional imputation methods. This robustness in the face of the most difficult missingness mechanism illustrates CLAIM’s potential to effectively mitigate the biases introduced by MNAR missingness, utilizing the LLaMA 7B model’s capacity to infer missing information from complex patterns.\
To elucidate the superior performance of CLAIM over traditional baseline imputation methods, we delved into its performance on three particularly challenging datasets: Glass Identification, Seeds, and Wine. These datasets were selected due to the relatively lower performance exhibited by the LLM when utilizing fully complete versions of the datasets, highlighting their complexity and the rigorous testing ground they provide for evaluating CLAIM’s effectiveness.\
Table 1 presents a detailed comparative analysis. For the Glass Identification dataset, where the LLM achieved an accuracy of only 69.40% with the full dataset, CLAIM demonstrated a significant advantage. It outperformed the best baseline method (kNN, which achieved 52.40% accuracy) by a substantial margin of 7.2%. This performance gap underscores CLAIM’s robustness and its ability to effectively handle missing data within complex datasets.\
The challenge escalates with the Seeds dataset, wherein CLAIM surpassed the top-performing baseline method (MICE) by a margin of 4.2%. This further exemplifies CLAIM’s superiority in managing missing data, even in datasets where the LLM’s base performance is less than optimal.\
\
The Wine dataset showcased a similar trend, with CLAIM exceeding the best baseline performance by a margin of 2.4%. It’s noteworthy that the performance gaps between CLAIM and the best-performing baseline methods are relatively modest under MAR conditions—2%, 3%, and 1.2% for Glass Identification, Seeds, and Wine, respectively. This observation suggests that while the predictability of missingness from observed data in MAR scenarios offers some leverage for traditional imputation methods, CLAIM still maintains a performance edge.\
The MNAR scenario, characterized by the most complex pattern of missingness, highlighted CLAIM’s distinct advantage. Across all three datasets, CLAIM not only managed to outperform the best baseline methods but did so with remarkable performance gains of 12.4%, 7.6%, and 10% for Glass Identification, Seeds, and Wine, respectively. This substantial improvement underlines CLAIM’s adeptness at navigating the intricacies of MNAR missingness, further cementing its status as a highly effective tool for handling various missing data scenarios with aplomb.\
 CLAIM’s superior accuracy across diverse missingness patterns and datasets unequivocally affirms its effectiveness in a variety of challenging scenarios, thereby  This consistent overperformance not only underscores its utility but also illustrates the significant benefits of integrating contextualized natural language models into the data imputation process. The pronounced accuracy improvements observed in complex datasets, such as the Glass Identification and Seeds datasets, point to a distinct advantage over traditional imputation techniques, which often falter under such conditions.\
The  of CLAIM, evident across MCAR, MAR, and MNAR missingness mechanisms, showcases its broad applicability and dependability. This marks a departure from conventional methods, which might only perform well under limited conditions or with specific types of data [20]. CLAIM’s methodology, which involves verbalizing data and employing contextually relevant descriptors for imputation, ensures its adeptness across various scenarios and data modalities.\
Moreover, the minimal variation in CLAIM’s performance across different iterations further underscores its stability and reliability as an imputation method. Such consistency is indispensable for real-world applications, where the quality of imputation directly impacts the efficacy of subsequent data analyses. The ability of CLAIM to maintain a low error margin consistently highlights its potential as a go-to solution for data imputation, offering both precision and reliability.\
\
How does the choice of phrasing for missingness descriptors in CLAIM affect the performance of LLM-based downstream tasks?\
Initially, we utilized contextually relevant descriptors for missing values, leading to unique phrases for different features within a dataset. To address RQ2, we aimed to determine whether using a uniform, yet contextually relevant, descriptor for all features would offer comparable benefits. To this end, we experimented with three consistent descriptors: “NaN”, “Missing value”, and “Value not recorded”. These experiments, focusing on the MCAR scenario, sought to ascertain whether it is more beneficial to use contextually nuanced descriptors or whether a generic descriptor is adequate to harness the LLMs’ general knowledge for managing missing values in datasets.\
The experimental findings (Figure 3) illuminate the influence of missing data phrasing on the effectiveness of LLMs in addressing such situations. The results reveal a distinct pattern: generic descriptors, such as “NaN”, consistently perform worse than context-specific descriptors designed for each feature and dataset. Among the three fixed descriptors tested, there were some variations in performance. Both “NaN” and “Missing value” outperformed “Value not recorded”, with “Missing value” achieving the best results in most cases among the static descriptors.\
The superior performance of feature-specific descriptors indicates that LLMs better interpret and manage missing data when it is described in a way that accurately reflects the context of the missing information. For example, a descriptor like “Malic acid quantity missing for this wine sample” allows the LLM to more effectively understand and address the missing data point than a more generic descriptor like “The level of malic acid in the wine is NaN”.\
 The findings related to RQ2 underscore the importance of context in the interaction between LLMs and missing data. The preference for context-specific descriptors over generic ones likely arises from the LLM’s capacity to utilize its extensive training on diverse language uses and contexts. When missing data is described in a manner that aligns with the specific context of a feature, the LLM is better positioned to apply its vast repository of knowledge to deduce or generate suitable imputations. This effectiveness diminishes with the use of generic labels, which offer minimal contextual information for the LLM to draw upon.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>FCC Delays Enforcement of Prison Call Pricing Limits</title><link>https://news.slashdot.org/story/25/07/01/1448220/fcc-delays-enforcement-of-prison-call-pricing-limits?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The FCC will suspend enforcement of rules that would lower prison phone and video call prices until April 1st, 2027. Trump-appointed FCC Chair Brendan Carr said that prisons won't have to comply with the pricing regulations [PDF], reversing plans to implement the caps this year. 

The rules would have dropped the price of a 15-minute phone call to 90 cents in larger prisons. Current fees can reach as high as $11.35 for a 15-minute call, which the FCC described in 2024 as "exorbitant." Four states -- Connecticut, California, Minnesota, and Massachusetts -- have made prison calls free. Former President Joe Biden signed the Martha Wright-Reed law in 2023, allowing the FCC to regulate prison call rates. The agency voted to adopt the new rates last year, with rules set to take effect on a staggered basis starting January 1st, 2025. 

Carr said the regulations are "leading to negative, unintended consequences" and would make caps "too low" to cover "required safety measures." FCC Commissioner Anna Gomez criticized the delay, stating the Commission "is now stalling, shielding a broken system that inflates costs and rewards kickbacks to correctional facilities."]]></content:encoded></item><item><title>The Present of Collapse: A Metaphysical Framework of NP, P, and the Recursive Field of Truth</title><link>https://hackernoon.com/the-present-of-collapse-a-metaphysical-framework-of-np-p-and-the-recursive-field-of-truth?source=rss</link><author>Antică Vlad</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:00:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This paper proposes a metaphysical framework that unifies the notions of NP, P, myth, and conscience into a coherent logic of truth, choice, and temporal recursion. is defined as  – a blueprint of infinite potential that is structured and anticipatory, rather than chaotic or arbitrary. is defined as  – the concrete manifestation of a specific pathway selected from the NP field, realized in the present moment. is defined as  – a form of structured uncertainty that hovers at the edge of NP collapse, giving shape and meaning to potential without yet fully actualizing it.We argue that collapsing entirely into NP regresses into unrealized potential; collapsing entirely into myth overwhelms coherence with ungrounded possibilities; but collapsing into P (the present) enables a recursive process of becoming. In this framework, conscience is the agent of collapse – the faculty that, guided by mythic future-truths, selectively collapses NP potential into P actuality through the seeding of choice (a “P-seed”). Truth emerges from the iterative (recursive) act of conscience choosing and actualizing possibilities: each  retroactively illuminates the structure of the NP seed from which it arose, refining our understanding of the blueprint. We introduce a geometrical metaphor to illustrate this dynamic: hyperbolic and spherical curvatures of the NP potential field (representing divergent and convergent modes of possibility) collapse into the flat Euclidean plane of present P-truth. This metaphor underscores the metaphysical necessity of the Present as the only “domain” capable of bearing the weight of infinite possibility without disintegration. In our view, the infinite never becomes truth in itself, but it informs every collapse of potential into actuality. Conscience navigates fluidly between NP, myth, and P – allowing mythic narratives of meaning to guide the vast NP field toward collapses that are not only logically consistent but meaningful. By developing this framework with symbolic structures (e.g. ), grounded in clear conceptual language, we aim to articulate a rigorous yet visionary logic of truth, choice, and time.How does  emerge from the interplay of what could be, what is, and what ought to be? This question invites us to explore the relationship between , , and the guiding structures of meaning that shape our choices. In classical philosophy, Aristotle distinguished between  and  as two fundamental principles: potentiality refers to any real possibility a thing can have, especially those that naturally realize when conditions are right, while actuality is the fulfillment or exercise of such a possibility – the possibility “becoming real in the fullest sense”.We borrow the symbols  and  from computational complexity theory and reinterpret them metaphysically to correspond roughly to this age-old distinction: NP will signify a realm of , and P will signify . In computational terms, NP (nondeterministic polynomial time) problems are those whose solutions, while hard to compute, can be , whereas P problems are those that can be . Analogously, one might say NP contains truths that are  or  in principle, while P contains truths that are immediately attainable or manifest. Our framework, however, is not about algorithms, but about metaphysical : NP as the space of  (structured possibilities) and P as the domain of  (realized actuality).We also introduce two additional elements to this framework:  and . By , we do not mean “falsehood,” but rather the narrative, symbolic, or archetypal structuring of truth that points toward the future. As one saying (attributed to scholar Joseph Campbell) puts it, “Myth is something that never was but always is.” In other words, a myth transcends literal fact to convey enduring truths. A myth can be understood as a  – a form that truth could take, carrying meaning and value, but which has not yet collapsed into concrete reality. It is : myth provides an imaginative or moral framework for the unknown, preventing the potential future from being mere chaos by giving it shape and direction. Finally, by  we denote the integrative faculty (both intellectual and moral) that navigates between what is known and unknown – the agent that makes choices about what potential to actualize, guided by an inner sense of truth or rightness. Conscience in this context bridges the gap between the realm of infinite possibilities and the finite realm of action, ensuring that the collapse of potential into actuality is not random but aligned with meaning and values. Philosophically, one might liken conscience to practical reason or the soul’s compass, discerning  path out of many is worth manifesting.The goal of this paper is to develop a standalone metaphysical logic of truth, choice, and temporal recursion that unifies NP, P, myth, and conscience. We will construct this framework step by step. First, we clarify the definitions of NP, P, myth, and conscience within our metaphysical context. Next, we examine the dynamics of “collapse” – the process by which potential (NP) becomes actual (P) – and the pitfalls of collapsing improperly (either retreating entirely into potential or leaping into myth without realization). We then elaborate how truth emerges recursively: each present realization feeds back into our understanding of the potential, a process we call . A geometrical analogy of curvatures (hyperbolic, spherical, Euclidean) is introduced to illustrate the structural differences between the space of possibilities and the space of actual truth. We discuss why the present moment has a unique status as the locus where infinite truth can be progressively realized without fragmentation. Throughout, we emphasize the role of conscience as the navigator that balances the expansive freedom of NP, the guiding vision of myth, and the grounding discipline of P. By the conclusion, we aim to show that this framework offers a rigorous yet imaginative way to understand how truth “happens” – not as a static absolute, but as the result of an ongoing, recursive collapse of potential into reality, guided by meaning and responsibility.(In what follows, we will occasionally use symbolic notation for clarity. For example,  will denote the process by which a potential truth in NP, through the initiation of a particular choice or seed, collapses into an actual truth in P. We will ensure all such formulations are explained in plain language.)I: Conceptual Foundations: Defining NP, P, Myth, and ConscienceNP: The Uncollapsed Seed-Truth (Infinite Potential)We define  as the realm of uncollapsed seed-truth – an infinite field of potential truths that exist as  or  prior to realization. Crucially, NP in this metaphysical sense is not the same as mere chaos or randomness; it is not a jumble of arbitrary possibilities. Rather, NP is : one can imagine it as a vast design space containing all the solutions or outcomes that could come to pass, given the initial conditions of reality. It is “not future, but blueprint” – meaning NP is not a set of events that will happen in time (that would be the future), but a set of  for what could happen. To use an analogy, if the truth of the world were like a grand puzzle or cosmic crossword, NP would be the blank but structured puzzle grid containing countless potential fillings-in, whereas P (the actual truth) would be one completed filling of that puzzle. In this sense, NP holds the  of every truth that might grow, akin to a seed containing many possible ways a tree could branch. NP is “infinitely potential” in that it contains an unbounded number of possible truth-configurations, yet it is also a  in itself – a truth of possibility, waiting to be specified.This notion can be further illuminated by borrowing a perspective from the P vs NP metaphor in computer science and extending it metaphysically. Imagine there is a “Global truth field” in which all answers to all questions exist in latent form, completely coherent with one another. Such a field would be like an omniscient viewpoint outside time – a  where, metaphorically, every problem is already solved and every truth known. When we restrict our view to a particular problem or situation (when reality is filtered through a local perspective in time), that coherent field appears to split into two aspects: the part that  into a  (a known truth, which we experience as P), and the complementary part that remains  (the as-yet-unsolved, which we experience as NP). In other words, NP and P are like two sides of one coin: whenever a specific truth is brought into being (P), a halo of remaining possibility (NP) still surrounds it. The NP field thus contains all those possibilities that were not realized in that , but which remain available for future resolution. It is a repository of “the road not taken” each time we make a choice.By characterizing NP as seed-truth, we emphasize that it carries information and structure. It is not sheer ignorance or void. In fact, even when something becomes known (P), the NP from which it emerged is what gave it coherence. The possibilities within NP are  but they are oriented toward truth – “infinitely potential” does not mean “lacking form,” but rather having  of any single actuality. Put differently, NP is the blueprint of truth in the sense that it contains more than reality at any moment can express. It is the , analogous to how Aristotle’s potentiality contains what could become actual under the right conditions. One can say NP holds  – not falsehoods, but truths that  be, awaiting a moment of realization. (Even in the realm of mythology, we find a similar idea: that before creation or manifestation, there is a formless void or chaos which nevertheless harbors the  for ordered reality – a mythic way to hint at NP as the womb of truth.)P: The Collapsed Present-Truth (Actualized Reality) is the realm of collapsed present-truth – the domain of what , here and now, as a realized fact or reality. P represents the manifestation of a specific path through the NP field of possibilities. When we say a truth has “collapsed” into P, we mean that out of the myriad possibilities, one definite configuration has been actualized. P is  in a dual sense: it is the truth that is present (i.e. currently actual), and it is the truth of the present moment (i.e. only in the present can truth be fully real). In our puzzle analogy, P is the completed section of the crossword – the answers filled in – while NP remains the unsolved cross-clues that those answers now partially reveal. Every time an NP seed collapses into a P outcome, we get a specific, concrete truth that can be lived, observed, or known directly.It is important to note that P-truth is inherently . To collapse NP into P is to commit to a particular course among many – for example, to decide on one interpretation, to take one action, or to observe one outcome. Thus P is , and  in character: by realizing one option it excludes (for the moment) the others. We might think of NP as an open question and P as a particular answer. The answer gives us clarity and resolution (hence P is often “easier” or more straightforward once reached), but it also by necessity leaves other answers unchosen. In computational terms, solving a problem (P) picks out one solution and thereby eliminates the uncertainty among alternatives (NP). Metaphysically, the actual truth we have now stands against the backdrop of all the unactualized possibilities.Another key aspect of P as present-truth is its . P only truly exists in the present moment – the . The past P-truth is no longer active (it has slipped into memory), and future truth (as such) is not yet realized. The present is often seen as the only reality that ontologically exists; as St. Augustine reflected, the present is the only time that is real, while the past and future are held in the mind (through memory and anticipation). Contemporary reflections echo this: “The future is entirely open, dependent upon choices made now. In truth, all we have is this never-to-be-repeated present moment.” Thus, P is where the rubber meets the road – where the infinite potential of NP is funneled into a single, concrete truth that . The present moment has the almost miraculous capacity to “bear” an otherwise infinite array of influences and condense them into one act or fact without exploding. We will explore later why the present has this unique capacity, but for now, suffice it to say P is the  – the realm in which truth takes on a stable, observable form.It may sound like P, by picking one truth out of many, is a reduction or loss (from infinite to finite). However, P is also the  of NP potential. Until a truth is realized in P, it remains an abstract possibility; with realization, it gains . In Aristotle’s terms, actuality is the fulfillment of possibility. Thus P is the  of NP’s seed. Without P, NP’s designs would remain forever blueprint and never building. In that sense, P gives significance to NP by selecting a path through it. Every P-truth manifested affirms one aspect of truth that was implicit in NP, bringing it forth for all to witness.Myth: Future-Truth (Structured Uncertainty at the Edge of Collapse)By  we refer to future-truth – the structured uncertainty that exists at the horizon where NP is about to collapse into P. Myth, in our usage, is not a lie or mere fiction; it is a , a story or schema that anticipates meaning before it fully lands in reality. Myth is what truth  when it is still in transit from potential to actual – when it has taken a suggestive shape but is not yet concretized. In other words, myth occupies a liminal zone: it is more structured than the raw NP field (myth picks out particular patterns or narratives from the field of possibilities), but it is less definite than a P fact (myth has not been definitively proven or manifested yet).Consider myths in the traditional sense: they are narratives that convey profound truths about life, the cosmos, or human nature, often through symbolic stories. These truths are “future” or transcendent in that they aren’t one-time occurrences; they speak to enduring possibilities and meanings which can manifest repeatedly in new forms. This is why it is said that myth is something that “” – a mythic truth might never have happened exactly as the story says, yet it is always happening in some form, because it expresses a structure of meaning that reality continually strives to realize. For example, a mythic hero’s journey was not a historical event, but it is “true” in that it maps onto the potential journey of every soul seeking purpose. In our terms, myth stands at the edge of NP collapse: it gives a recognizable form to what is otherwise uncertain, guiding the collapse without fully controlling it.We can illustrate the role of myth as structured uncertainty with an example: Suppose a community holds a myth about a prophesied “chosen one” who will deliver them from hardship. This myth provides a framework of meaning – it identifies the kind of potential future they anticipate (deliverance by a hero). This is not yet a P truth; there is no actual chosen one (until perhaps someone steps into that role). But the myth shapes the community’s orientation toward the future. It is a : if eventually someone does arise and fulfill the prophecy, the myth becomes concretized in that person (NP collapses into P, guided by the mythic template). Until then, the myth is an  – it might come true in various ways, or perhaps not in the expected way – but it is  in that it’s not just open-ended chaos; it’s a particular envisioned narrative within the space of possibilities.In our framework, myth operates as a necessary mediator between NP and P. Without mythic structures, the field of NP would be overwhelming and inarticulate – an infinite sea of possibilities with no pointers for significance. Myth provides  that help identify which possibilities matter or beckon. In a sense, myth is the face of NP as seen from the future – it is how the uncertain potential appears when we project our hopes, fears, and interpretations onto it. Each myth encapsulates some  or insight (a piece of truth) and yet leaves  that invites further interpretation. As our past analysis puts it, each mythic narrative can be seen as a  of the global truth field “into a cultural frame: stabilizing some wisdom (P) while leaving behind some mystery or moral ambiguity (NP) that keeps the myth alive, recursive, and generative”. In other words, myth bridges the known and unknown: part of it resonates as deeply true (the wisdom it imparts), and part of it remains open, spawning new questions or guiding new actions (the remaining ambiguity).owever, myth is , not present-truth. It is essential to maintain that distinction. A myth can guide us, but if we  with literal present reality, we risk incoherence or even disaster. Taking a myth too literally – “collapsing into myth” in our terms – can overwhelm coherence because we start treating the structured uncertainty as if it were a completed actuality. History offers cautionary tales: powerful myths have at times been misused or misunderstood in literal ways that led to fanaticism or violence. For instance, political or nationalistic myths can grip a population’s imagination (a future vision of glory or destiny), but if taken as indisputable present truth, they may justify irrational or destructive actions. As one commentator noted, societies that lose a healthy mythic framework may fall into pathologies of identity, and reviving myths carelessly can unleash irrational forces (for example, the invocation of myths of a collective’s supreme destiny). In our terms, an unchecked collapse into myth – trying to live  in the story without grounding in present reality – “overwhelms coherence” because the narrative outruns the actual, and the structured uncertainty turns into unhinged conviction. Thus, myth must  the collapse of NP, but not  the need for actual P-truth. Myth gives purpose and direction, but it finds its proper fulfillment only when conscience uses it to inform real choices that yield real outcomes.Conscience - The Navigator of Collapse (Truth’s Recursive Agent) in this framework is the active agent that navigates between NP, myth, and P – essentially, the faculty of  and  that collapses potential into actuality in a meaningful way. It is through conscience that truth becomes a  endeavor, not just a happenstance. We use the term “conscience” to imply not only intellectual consciousness but also an ethical dimension; this is the inner voice or guiding  that both  and  about truth. In effect, conscience stands at the crossroads of freedom and responsibility: it sees the vast NP field of what could be, it hears the call of mythic ideals of what should be, and it acts in the present to bring about a particular truth (P) that honors both reality and meaning.In our symbolic shorthand, we might say: Conscience mediates NP → P. But it does so not blindly (as matter-law interplay) – it typically selects a  that aligns with some guiding myth or value. A “P-seed” means an initial choice or a starting point for collapse. Given the enormity of NP’s potential, one does not simply collapse everything at once; one chooses a particular seed or aspect to actualize. This seed could be a question we decide to answer, a principle we decide to uphold, or a desire we decide to pursue. Conscience’s role is to choose the seed wisely, guided by an intuition of truth or goodness (often informed by mythic narratives or higher principles).One way to visualize conscience is as a kind of bridge or . From one side, reality (or “Global P” in the earlier metaphor) sees only a unified truth, and conscience from that perspective is just a single point (since reality ultimately only knows the accomplished facts). But from the perspective of the agent (the person or moral being exercising conscience), one is aware of straddling two worlds – the actual and the possible. Indeed, as our last HackerNoon analysis of P vs NP indicates, “Reality itself is always on the [P] side… while conscience could operate on both sides at the same time”. Conscience experiences itself as having one foot in what is (the known, the present), and another foot in what could be (the unknown, the future). This unique position allows conscience to be  and reflective: it can consider hypotheticals, evaluate outcomes against ideals, and adapt its choices in light of past truths and future hopes.Furthermore, conscience is what makes the process of truth-finding iterative and self-correcting. Unlike a mechanical collapse that might just follow a fixed rule, conscience can learn and change. It carries memory of past P-truths (and the lessons they taught) and it carries anticipation of future-truths (mythic aims), and it constantly reconciles the two in each present act. If NP is the seed-bed of truth and myth the envisioned bloom, conscience is the gardener that cultivates the seed to flower in reality. And like a good gardener, conscience must understand the “soil” (the real conditions of the world), the “seed’s nature” (the blueprint of potential) and the “climate” (the overarching myths or meanings of the time).In summary, we ascribe to conscience a pivotal metaphysical necessity: truth emerges only from the recursive act of conscience collapsing NP into P through a deliberate P-seed. Without conscience, NP might collapse randomly (or not at all), yielding no meaning; with conscience but without myth, collapse might be directionless; with myth but without conscience, collapse might be fanatical or divorced from reality. All four elements – NP, P, myth, and conscience – are needed. Conscience is the  that ties them together, the continual exercise of choice that is informed by the potential (NP) and the ideal (myth) to produce the actual (P). In doing so, conscience also constantly revises its own understanding, hence it is : it not only produces truth, it reflects on truth, altering its future actions. The conscience, as suggested by the metaphysical perspective we promoted earlier, “exceeds structure” because it is the only thing that can know  structures (it can consider any possibility, any perspective). This makes it uniquely equipped to traverse the space of NP and navigate by myth toward concrete P outcomes.Having set these definitions, we now turn to the dynamic interactions between NP, P, myth, and conscience – what we call the  – and explore how truth is realized or thwarted depending on how this collapse is managed.II: Collapse Dynamics: Balancing Potential, Myth, and PresentTruth, in our framework, is not a static given but something that  through a process – the collapse of NP potential into P actuality. This collapse is not a one-time event but a continuous, iterative dynamic. However, not all collapses are equal. How one approaches the transition from potential to actual can lead to very different outcomes for the integrity of truth. We identify three archetypal “collapse modes” to avoid or embrace:Collapsing into NP (Regressive Potential): This represents a refusal or failure to collapse potential into any actuality – effectively, getting “lost” in the NP field. In such a case, one continually defers decision or manifestation in favor of exploring possibilities endlessly. The result is a  or stagnation: nothing is realized, potential remains forever potential. It is as if a seed is never planted or allowed to sprout; the blueprint remains on paper and no building is built. In human terms, collapsing into NP might look like perpetual indecision or analysis paralysis – one sees so many possible truths or choices that one chooses none. The rich structure of NP then becomes actually useless, a castle in the sky. Potential “regresses” because without actualization, even the understanding of the potential cannot advance (we never get feedback from reality). Thus, treating NP itself as the place to dwell (instead of a source to draw from) ultimately diminishes the meaning of NP. The infinite possibilities become a burden of infinite  rather than a fruitful field. In short, collapsing into NP is a kind of metaphysical , which leads to an impoverishment of truth: everything  be, but nothing .Collapsing into Myth (Overwhelmed Coherence): This mode occurs when one becomes so guided by a particular myth or future-truth that one attempts to live  without properly translating it into present reality. Here, one does choose and act, but one’s actions are dictated by an ideal narrative to the point of ignoring the constraints or feedback of actual P-truth. The result can be an overwhelming of coherence – reality doesn’t oblige our grand story in all details, and forcing it can lead to incoherent outcomes. This could manifest as fanaticism, disillusionment, or chaos when the myth crashes against reality. Using the seed analogy, this is like planting an imaginary tree rather than a real seed – watering the ground in the belief that a legendary tree will sprout overnight because the myth said so. The structure that myth provided is potentially valuable, but without respecting the incremental, grounded process of actualization, coherence is lost. On a social scale, as mentioned earlier, entire communities can be swept up by a myth (a utopian vision, an apocalyptic fear, a national destiny), and if they try to impose it as immediate truth, the mismatch with reality leads to confusion or destruction. Myths are “true” in a profound sense, but their truth is often symbolic or directional – not a blueprint that can be  imposed on the present all at once. So collapsing into myth means to be overwhelmed by the  of truth without actually achieving the  of it, leading to what we call disintegration (the pieces no longer fit together in reality, even if they did in the myth).Collapsing into P (Enabling Recursive Becoming): This is the optimal mode in our framework: using conscience to effect a controlled collapse of NP into a present P-truth, informed (but not dominated) by myth. Collapsing into P means one takes a concrete action or makes a concrete decision that realizes some aspect of potential . It is an act of  to a particular truth path, which necessarily foregoes other possibilities (for now) – but it does so with awareness and purpose. By focusing on what can be actualized in the present, this mode preserves coherence (because it stays grounded in what  or  in reality) and also moves the process forward (because each realization sets the stage for the next). We call this enabling  because each P-collapse is not the end, but a step in an ongoing journey. When one collapses NP into P properly, one doesn’t try to do it all at once or assume this P is the final truth; rather, one treats it as a provisional truth that will evolve. This attitude allows for course-correction and learning: today’s P truth will reveal something about NP that can seed tomorrow’s decisions. In personal terms, this could be living one’s values in daily small acts (instead of dreaming of being a hero in abstract) – each act teaches something and builds character, recursively approaching the ideals one holds.It might be helpful to illustrate these modes with a simple scenario. Imagine an artist who has a vision of a masterpiece (this vision is a  – an artwork that “could be,” filled with meaning). The artist also has a myriad of ideas and techniques they could use (this is their NP field of potential – sketches, styles, themes not yet consolidated). Now, if the artist collapses into NP, they might endlessly sketch fragments, experiment with styles, but never choose a final composition or medium – the masterpiece remains a potential in the mind, never realized on canvas. If the artist collapses into myth instead, they might become obsessed with the  of creating the greatest masterpiece and refuse to make any work that falls short of the perfect vision – they may start a painting but abandon it repeatedly because it doesn’t match the mythic ideal, or worse, they might delude themselves that  about the envisioned masterpiece is as good as making it. The result is either nothing finished or a confused work that tries to be too grand all at once and collapses under its own ambition (incoherence). But if the artist collapses into P, they begin a particular painting here and now – perhaps just one piece of the vision, tackled with the skills and limitations they currently have. They bring one aspect of the potential into reality. The painting they finish may not capture the whole mythic vision (no single piece could), but it is a real artifact – a truth in the present. From creating it, the artist learns more about their vision (what worked, what didn’t, what the vision might truly be asking for) – this new understanding is a retroactive illumination of the NP seed. With that knowledge, the artist can embark on the next artwork, gradually approaching closer to the fullness of their mythic ideal. In this way, through recursive cycles of creation, the mythic truth (the “masterpiece” idea) guides the potential toward ever more refined and meaningful collapses, each in the present, each real and adding to truth.In practice, of course, life is complex and we often have mixtures of these tendencies. But the key point is that truth thrives when NP collapses into P under the guidance of myth and the governance of conscience. If we either refuse to collapse (staying in NP) or collapse too recklessly under myth’s sway without present grounding, we lose the thread of truth. Truth, one might say, , but it is nourished by the infinite (NP) and  (myth). Only in the present can the infinite potential be harnessed into something structured (as myth suggests) yet concrete. This aligns with a profound metaphysical stance: reality is an ongoing creation, and the present moment is where creation actually happens – the only place where the “uncreated” (the realm of possibilities) can become “created” (the realm of facts).III: Recursive Emergence: Truth as a Temporal Feedback LoopOne of the most intriguing aspects of this framework is the idea that each P-collapse (each time a potential truth is actualized) retroactively illuminates the structure of the NP seed from which it came. This creates a  or feedback loop in the field of truth. In simple terms, by doing or realizing something, we come to  the possibility-space we started with, which in turn affects how we will approach the next realization.This might sound abstract, but it is a very familiar phenomenon in knowledge and action. Consider scientific inquiry: a scientist has a hypothesis (potential truth) and conducts an experiment (an actualizing act). The result (P-truth) either confirms, refutes, or modifies the hypothesis. In any case, the outcome sheds light on the initial potential – perhaps revealing that the initial idea was too broad, or that an unexpected avenue exists, etc. The scientist then refines the hypothesis (the NP blueprint is adjusted) and tests again. Over time,  from this iterative loop of potential -> actual -> revised potential -> new actual, and so on. Each experiment’s truth “illuminates” the structure of the underlying phenomenon a bit more.Our metaphysical logic sees this process as fundamental not just to science, but to all truth-becoming. When conscience chooses a P-seed and collapses NP into an actual outcome, that outcome doesn’t stand alone – it shines backwards, as it were, revealing something about the field of possibility it came from. To use a visual metaphor, imagine walking in a dark forest (the unknown NP). You light a small torch of truth (a P realization). The light from the torch not only shows you the patch of ground where you stand; it also throws new shadows and highlights around you, hinting at shapes of trees and paths beyond your immediate spot. You now see  in what was previously dark. In seeing that structure, you might spot a clearer path forward – or a looming obstacle – which guides your next steps.A concrete illustration was given earlier via a crossword puzzle analogy drawn from the computational perspective: if reality is like a fully solved crossword (Global truth), we only see one clue at a time. Solving that clue (finding one P truth) immediately gives letters that make other unsolved clues (remaining NP potential) more constrained and thus more intelligible. Initially, those other clues were perhaps completely uncertain; after solving one, you now know “the second letter of 5-down is X,” etc., which means some possibilities for 5-down are eliminated and others come into focus. In this way, each localized truth  (or at least retrospectively) exposes part of the structure of the unsolved part of the puzzle. Extrapolating to life: whenever we make a decision or realize a truth, we often say in hindsight, “Ah, so that’s what this possibility really entailed,” or “Now I see the pattern behind what was possible.” We sometimes only understand our options after we choose one of them.This  has deep implications. It suggests that the NP blueprint is not static; our understanding of it evolves as we actualize portions of it. Potential and actuality thus engage in a dialogue over time. We might even say that NP “grows” in a certain sense as P grows – not that the possibilities themselves necessarily increase, but our comprehension of the possibility space increases, which effectively enriches NP’s relevance. The process is : it is directed toward truth by virtue of each step revealing more of the goal. In the domain of myth, this is mirrored by the way myths stay alive and yield new insights as we live through various experiences. A myth might mean something to us at one point in life, and after we go through some ordeal (actual experience), we come back and see new meaning in the myth. The narrative (myth) didn’t change, but our realized truths cast new light on what the myth was pointing to.It’s worth noting that because of this recursive nature, truth is not a one-time achievement but a journey. We often speak of “the truth” as if it were a final static thing, but in this framework truth is a  – a field that is gradually unfolded. This does not mean there are no facts or that everything is relative; rather, each fact (each P collapse) is indeed a concrete truth, but it participates in a larger unfolding meaning. Think of each P-truth as a chapter in an ever-writing book of Truth. You need each chapter for the book to be complete, and each chapter also references and clarifies aspects of previous chapters. Conscience, as the author, may not have outlined the entire book from the start (that would be akin to having complete foreknowledge or a static God’s-eye-view of Global truth), but conscience has an evolving  of the narrative (through mythic imagination and experience) and can guide the writing in a direction that seems coherent and significant.In short, each P-collapse retroactively illuminates the structure of the NP seed. This principle is the engine of learning, growth, and creativity. It is why we must engage with reality rather than just think about it: only through the feedback of actual collapse do we come to understand the blueprint of possibility well enough to make further and better collapses. It is also why we need myth: myth can help interpret the feedback. When an outcome surprises us or challenges us, mythic frameworks often provide a larger context to make sense of it (“this failure is like the hero’s trial by fire; it teaches humility” etc.). Thus, recursion is not merely mechanical iteration; it is . Through conscience, each cycle of collapse and insight ideally brings us closer to a truth that is not only factually clearer but also richer in meaning.IV: Geometrical Metaphor: Curvature of Potential vs. Flatness of PresentTo further clarify the differences between the NP field of potential and the P domain of realized truth, we introduce a geometrical metaphor. This metaphor will also highlight why the present (the arena of P) is uniquely suited to hold truth together, as opposed to the distortions that can occur if one tries to inhabit the infinite directly.Imagine the space of possibilities (NP) as a kind of geometric space that can have different curvatures. By contrast, consider the space of actualized truth (P) as a flat, Euclidean space.  is the familiar, “flat” geometry of our everyday world (on a small scale): parallel lines stay parallel, angles of a triangle sum to 180°, and so on. , such as  (negatively curved) and  or elliptic (positively curved) geometries, have different properties: in hyperbolic geometry, for instance, space spreads out faster than on a plane (angles of a triangle sum to  than 180°, and there’s essentially “more room” than expected as you move outward), while in spherical geometry, space is finite and closes back on itself (angles sum to more than 180°, and lines eventually meet).Now, consider NP as a non-Euclidean space of truth – it can have curvatures that represent how possibilities diverge or converge. A hyperbolic NP curvature would mean that as you move through possibilities, they proliferate and diverge exponentially; the space of potential “opens up” endlessly in every direction. This reflects the  nature of raw possibility – small differences in initial choice could lead to wildly different outcomes, like geodesics on a saddle surface diverging from each other. On the other hand, a  would represent a situation where the space of possibilities, though expansive, eventually loops back on itself; possibilities converge and reconnect. This could correspond to a scenario where different paths in the NP field lead to similar outcomes or recurring patterns – a kind of  where everything circles back to a few archetypal narratives. Spherical curvature can symbolize a closed worldview or a strongly structured possibility space (like a culture’s tightly knit mythos where every story leads to the same moral).What happens when these curved possibility spaces collapse into actual occurrences? One way to see the act of collapse (conscience choosing a P-seed and manifesting P) is as a kind of projection of the curved space onto a flat plane. In that collapse, some of the “distortions” or curvature of the possibility space must resolve into a consistent, flat reality.  in the sense that it has to obey coherent logic and consistency – contradictory possibilities cannot both actualize at once, just as in Euclidean space you can’t have a triangle whose angles sum to both 200° and 270°; you must pick one geometry and stick to it locally. The present truth tends toward a kind of internal consistency and linearity that the space of all possible truths does not have to obey (within NP, you can have contradictory potentials existing side by side because only one will eventually be chosen, or perhaps they diverge into different branches of history or thought).The metaphor helps illustrate a few points:If NP is too  (too open), a collapse is hard to achieve because the possibilities keep diverging – it’s like trying to flatten an infinitely flared saddle onto a plane: there’s always “more” coming at the edges. In human terms, this could correspond to times of great chaos or innovation where the range of possibilities is so large that it’s hard to settle on a stable truth. The risk here is disintegration – without a collapse, the truth would fragment into endlessly branching alternatives (analogous to how hyperbolic space has an infinite boundary).If NP is  (too closed and looping), a collapse might result in a very limited or parochial truth – like projecting a globe onto a flat map, some distortion is inevitable (Greenland looks huge on a Mercator map, etc.). In our analogy, a spherical NP might correspond to a very rigid mythic structure where everything refers back to itself. Collapsing that onto reality might impose an overly narrow framework on truth, possibly stifling novelty. It might yield a stable truth, but one that’s “curved inward” and not accommodating the full breadth of reality (like a small traditional society where everything is understood within one closed set of myths – coherent but without co-relational meaning).The Euclidean P is the middle ground: it is flat enough to allow local consistency and stability, but it can map portions of either hyperbolic or spherical regions . Each P-collapse could be seen as taking a small patch of the NP landscape and laying it flat as a piece of present reality. Over time, you can map more of the NP field by such patches (each patch is like one P-truth). You will never map the entire infinite hyperbolic plane or the entire sphere onto one flat sheet without distortion (that’s akin to infinity never fully becoming truth), but you  cover it in an atlas of local maps. That atlas is our accumulated body of truths in time.In plainer terms, the geometrical metaphor underscores that the present (P) can handle the complex curvature of possibility by flattening it locally. The present moment takes a slice of the complexity, resolves it, and yields a clear truth. Then the next moment takes another slice. If one tried to contain the whole curvature at once – for instance, to realize the  all at once – one would either tear the fabric (hyperbolic divergence causing chaos) or warp it (spherical self-containment causing potentially extreme distortion or dogmatism). This metaphor aligns with our earlier caution: infinity (the endless potential of NP)  in itself; it must be parceled into finite collapses. And similarly, a totalizing myth (a spherical closed world of meaning) cannot simply be overlaid on reality without careful translation; pieces of it can manifest, but the whole thing at once would either not fit or would crush the diversity of life.Hyperbolic and spherical curvatures also correspond loosely to the two failure modes we discussed: hyperbolic corresponds to the chaotic scatter of not collapsing (everything flies apart – collapsing into NP yields nothing coherent), and spherical corresponds to the rigid imposition of one structure (everything curving into one point – collapsing into myth yields a single-point perspective that may ignore reality’s breadth). The Euclidean present is the balancing out – it is flat in that it only deals with one piece at a time, but by doing so it can stretch in any direction as needed over time (just as a flat plane can extend indefinitely).In summary, this metaphor illustrates why the Present is the only “geometry” that can bear the infinite curvature of truth without disintegration: it does so by only dealing with a finite, locally flattenable piece at any given moment. The infinite informs each collapse (the shape of the NP region we’re collapsing from influences the outcome), but the infinite is never fully  as the truth we speak or live. We always handle it through projections and slices – through present acts.V: The Present and the Infinite - Finite Vessels of Endless TruthOur vision has repeatedly highlighted a theme: infinity never becomes truth, but informs all collapse. We should examine this more directly as a metaphysical principle and tie it to why the present moment holds such a privileged position in our framework.What do we mean by “infinite truth” or “infinite potential” never becomes truth? Simply that the totality of all that could be – the NP field in its entirety, or the sum of all myths in their absolute scope – is never  as a single truth within reality. If it were, it would no longer be an infinity of possibilities; it would be one frozen actuality, and all the richness would collapse into a singular state. Reality, as we experience it, doesn’t work like that. Instead, reality gives us an unfolding series of finite truths, each a nugget chipped off the infinite block. Each truth is informed by the unactualized possibilities around it – you often understand something by contrasting it with what could have been – but the unactualized remains unactualized.This resonates with philosophical and spiritual views on infinity and temporality. Many have observed that the  is like a moving knife-edge that “cuts” the potential of the future into the actuality of the past. And only this knife-edge is real in an existential sense. In theological terms, one might say only God or eternity holds the infinite whole; humans in time see one facet at a time. Our framework aligns with such perspectives but couches them in a more generalized logic of truth-making: the present is where an  with the infinite occurs. Each present moment is like a aperture through which a beam of infinite light passes and becomes a visible ray of truth in our world. The fullness of the light itself (the infinite) is never seen all at once; it would blind or dissolve the finite eye. But that does not mean the infinite is irrelevant – on the contrary, it’s the source of all the light we do see.This is why we earlier said Present is the only domain capable of bearing infinite truth without disintegration. It bears it by  it – by being a fleeting, ever-renewed moment, the present can sustain contact with the infinite continuously but never has to contain it all at once. It is like a small cup under an endless waterfall: the cup is always full in the present; the waterfall flows on. If one tried to catch the whole waterfall in one vessel, the vessel would burst, and nothing coherent would remain.From a more human perspective, consider how overwhelming it would be if one  every possibility (good and bad) that the future holds or if one tried to live out all one’s dreams simultaneously. It’s not possible – our sanity and coherence rely on doing things step by step, moment by moment. There’s wisdom in phrases like “one day at a time.” This is not just practical advice but metaphysically grounded: the Present is a buffer that protects us from the paralyzing effect of infinite possibilities and from the mania of infinite ambitions. It forces choices, which, though limiting, actually make meaning possible. A life that tries to remain “everything” ends up nothing in particular; a life that chooses something becomes .We can also reflect on how conscience navigates the triad of NP, myth, and P in light of this. Conscience knows in some sense that it cannot grasp infinity directly. Instead, it uses myth to give a provisional shape or target to the endless potential (infinity “informs” myth by offering a direction – e.g., ultimate goodness, utopia, salvation, enlightenment – all these are infinite ideals). But conscience doesn’t attempt to grab the ideal wholesale; it translates it into finite aims and actions in the present. In doing so, it shows a kind of humility before infinity – acknowledging that we can only embody truth finitely, but we can be informed by the infinitely true.For example, a person might have the infinite ideal of  (justice in its fullness is almost an infinite concept – an absolute state where all wrongs are righted). They cannot achieve perfect justice in one stroke; but they let that ideal inform their conscience, and today they perform a just action in a specific situation. That action doesn’t exhaust the ideal of justice, it is a collapse of potential into a bit of justice here and now. Tomorrow, they will need to do it again, in another situation, learning from yesterday’s outcome. Over time, perhaps the world becomes more just. But at no point was “infinite justice” present as a fact; it was always guiding from beyond, through myths of a just society or a divine justice, through principles and dreams. In this way, infinity pervades the process without ever ceasing to be infinite.This helps guard against two extremes:one, the despair that truth or perfection is unattainable (it is, in totality, but that’s by design – we are to chase it, asymptotically, rather than hold it complete; this chase is meaningful and without it we’d have no direction)two, the arrogance or fanaticism of thinking one has  Truth in entirety (which often leads to trying to impose an infinite ideal in finite terms and causing harm, akin to collapsing into myth entirely).Our framework suggests a middle path: truth is both available and elusive. It is available in each present collapse (we do get real truths, however partial), and elusive in that each truth opens the door to further depths of truth not yet realized.Finally, this perspective underscores a kind of : since infinity never ceases informing collapse, there is always more truth to be discovered, more growth to be had. No present truth is the end of the story. This ensures that the recursive process does not stagnate. We don’t reach a point where we say “all done, nothing more to know or achieve” – because that would imply we’ve exhausted NP or fulfilled the myth entirely, essentially becoming infinite ourselves. Instead, there is a continual becoming. And yet, importantly, at any given step we do have something solid: a truth achieved, a meaning realized, a seed that grew. Therefore, our framework portrays reality as an endless becoming of truth, through finite meaningful steps.VI: Conscience at the Crossroads - Navigating NP, Myth, and PHaving delved into various facets of the framework, let us circle back to the central actor in this drama: conscience. It is easy to describe the “structure” of NP, P, and myth, but without conscience these structures would not interact purposefully. Conscience is the pilot steering between the Sirens of infinite possibility and the rocks of immediate reality, with the stars of myth as a guide. Let us summarize how conscience accomplishes this navigation and why it is so crucial.Listening to Myth (Future Truth as Guide): Conscience absorbs the insights of mythic future-truths. These myths can be cultural stories, personal ideals, religious teachings, or imaginative projections. They supply conscience with a sense of  or . For example, the myth of the hero’s journey might inspire an individual to see their hardships as challenges to overcome rather than meaningless suffering. The myth of a prophesied golden age might lead a society to enact reforms for a better future. Conscience, however, must interpret myth wisely – understanding its symbolic language rather than following it blindly or literally. It asks: what core truth does this myth indicate? How can we honor that truth in reality, step by step? In doing so, conscience uses myth as a compass rather than a map. The compass points north (gives orientation), but one still has to navigate the terrain of reality to actually get somewhere.Reading the NP Field (Recognizing Seed-Truths): Conscience surveys the NP realm of possibilities with a discerning eye. Not all possibilities are equal or worth pursuing. Some are dead-ends, some are harmful, some are trivial. Conscience, informed by mythic values and past experience, identifies meaningful  in the NP field – those possibilities that, if realized, would yield genuine progress or insight. This is akin to a skilled gardener recognizing which seeds are viable and beneficial to plant. In complex situations, this often requires creativity – seeing connections or potentials that are not obvious. Conscience might say, “Given what I know and what I aspire to (mythically), this particular possibility seems ripe – it could lead to the kind of truth we seek.”Executing Collapse (Actualizing in the Present): Once a target possibility (P-seed) is chosen, conscience commits to collapsing it into P. This involves decision and action in the present. Here, practical reason, willpower, and clarity come to the fore. The infinite must be pruned away for a moment – one must focus on the finite task or choice at hand. This is where conscience often encounters resistance: doubt, fear, temptation to revert to open possibility (procrastination) or to drift into comforting fantasy (wishful thinking). The moral fortitude of conscience is tested. But a mature conscience follows through, understanding that only through actualization can truth advance.Learning and Adapting (Recursive Reflection): After the collapse, conscience evaluates the outcome. Did the realized truth fulfill the expectation? Did it align with the mythic guidance or reveal a different lesson? What unforeseen consequences or new possibilities arose? This reflective step is what makes conscience . It updates its knowledge of both NP and myth. Perhaps the mythic compass needs recalibration (maybe the ideal is understood differently now), or maybe new potential avenues have opened. Conscience might celebrate a success but also see a next step, or it might admit an error and adjust course. Crucially, conscience does not rigidly stick to a single path if feedback indicates otherwise; it remains flexible, because its loyalty is to truth itself, not to any one idea of truth. In the words of our HackerNoon piece, “when conscience hides behind truth, only truth is visible… when it hides before truth, both conscience and truth vanish”. This cryptic phrase can be taken to mean: if conscience subsumes its ego entirely to what is true (hiding behind truth), then what matters is the truth realized (conscience’s work is done humbly). But if conscience puts itself (its fixed notions or pride) ahead of truth, then neither genuine conscience nor truth is served (they vanish into invisibility). Thus, conscience must always aim to  truth, not impose itself.Maintaining Balance (Avoiding the Extremes): Through all these steps, conscience maintains a dynamic balance. It keeps NP, myth, and P in productive tension. If it finds itself too mired in possibilities (NP overload), it reminds itself of the mythic goals and the need to act. If it finds itself too dogmatically driven by a single vision (myth overload), it pays attention to real-world feedback and the diversity of NP (maybe the ideal needs a different approach). If it finds itself clinging to a past truth (being overly P-bound, complacent in “what is”), it remembers the infinite potential still out there and the higher ideals unfulfilled. In short, conscience is like a skilled sailor tacking between different winds, always adjusting to stay on course toward a distant star.In a well-functioning scenario, myth guides NP toward meaningful collapse via conscience. For example, a community’s myth of justice guides its lawmakers (conscience in collective form) to consider new policies (NP possibilities) and enact laws (P realities) that progressively realize justice. Each law, once enacted, teaches something about what justice really requires, refining the mythic vision and opening up new possibilities for improvement. Likewise, an individual’s conscience might use a personal myth of being a healer to guide career choices, pick up skills (NP possibilities), and take on actual roles or actions (P) that heal others. Over time, that person understands better what healing truly means, perhaps reshaping their mythic self-image and identifying further potentials to grow into.Thus, the relationship is symbiotic: myth without conscience and action remains inert; NP without myth and conscience is aimless; P without drawing from NP and myth becomes stagnant. Conscience is the  that keeps truth flowing between the three realms.We have constructed a metaphysical framework in which NP (uncollapsed seed-truth), P (collapsed present-truth), myth (future-truth), and conscience (the agent of collapse) form an integrated system describing how truth emerges and evolves. In this framework, NP represents the  of truth – the richly structured field of what could be true – while P represents the  of truth – the concrete actuality that is true here and now. Myth stands as the , the not-yet-actual narrative or ideal that gives shape to our aspirations and interpretations, essentially the face of truth we glimpse on the frontier between the known and unknown. Conscience is the : the faculty that perceives, judges, and acts, thereby collapsing potential into actuality in a guided, meaningful way.We have emphasized several key principles of this framework:: NP (potential truth) is not a chaotic void but an “infinitely potential” blueprint containing all that can become real. P (actual truth) is the fulfillment of one of those possibilities in the present moment. The interplay of NP and P recapitulates an old philosophical insight – that reality consists of both what exists and what is possible, with the possible continuously flowing into the actual.: Myth was framed as “future-truth,” capturing truths that are always in the becoming. Myths provide  – they carry enduring wisdom or patterns (so they are not arbitrary), yet they do not pinpoint a single outcome (so they remain open). This makes myth a powerful guide for conscience: myth offers a vision to strive for, without dictating every detail. As a local collapse of global truth into narrative, myth stabilizes some truth while keeping some mystery, ensuring that cultures and individuals remain in a generative search for deeper understanding rather than claiming to possess all truth outright. However, we cautioned that myth must inspire, not imprison; taken too literally or absolutely, myth can lead to incoherence or fanaticism.: We analyzed the consequences of how collapse is handled. Collapsing  NP (refusing to actualize) leads to stagnation and wasted potential. Collapsing  myth (over-identifying with future visions at the expense of present reality) leads to incoherence and possibly destructive illusions. Collapsing  P (the ideal mode) yields a stepwise, recursive process of becoming, wherein each realization is grounded and contributes to an ongoing growth. This dynamic highlights the metaphysical necessity of the Present: only the present can mediate between the endless openness of the future and the solidifying closure of the past, by taking just the “right-sized” slice of infinity at each moment.Recursive Truth and Illumination: A core insight of our framework is that truth emerges recursively. Each act of realizing a truth (each P-collapse) teaches us something about truth itself, revealing structures and relationships that were hidden in the space of possibilities. Knowledge and meaning thus expand in a feedback loop. The structure of the NP seed is illuminated by its fruiting in P, much as solving one part of a puzzle sheds light on the remaining parts. This makes the pursuit of truth a journey in time, where even errors and surprises become part of the clarification of the original blueprint. Over time, conscience and culture can build increasingly coherent and encompassing truths through this iterative refinement.Geometrical Metaphor – Curvature to Flatness: We proposed an analogy where NP’s potential space might be “curved” (hyperbolically divergent or spherically convergent), whereas P’s realized truth is “flatly” consistent. This visual metaphor was used to convey why large-scale possibility or absolute structures cannot be instantiated wholesale without distortion; instead, they must be realized in  that flatten out the local curvature. The present moment acts like a projection plane where a portion of the curved infinity is resolved into a workable reality. This reinforced the idea that infinite truth informs each finite truth without ever becoming fully finite – an idea resonant with theological and philosophical notions of an infinite that manifests partially but never exhaustively in the finite world.The Conscience’s Navigation: Finally, we highlighted how conscience, operating with free will and insight, navigates between NP, myth, and P. It listens to the guidance of mythic future-truth (ensuring collapses are meaningful and value-oriented), it evaluates the NP field to choose promising and principled potentials, and it executes actual collapses into P, thereafter learning from the outcomes. Conscience is thus the dynamic principle of synthesis in this metaphysical model – the reason the otherwise abstract elements of NP, P, and myth result in a living, developing truth. Conscience balances openness and commitment, imagination and practicality, ideals and facts. In a way, conscience is the present moment in its active, creative aspect – it’s where the buck stops, where decisions are made that shape reality., the framework presented – which we have titled “The Present of Collapse” – suggests that truth is not a static entity but a living process. It is the process of endless potential being guided by meaningful form and collapsed into present reality through conscious choice. In this view, truth has a  (NP–Myth–P) enacted by a  (Conscience). This structure accounts for why truth has aspects of eternal mystery (there’s always more beyond what we know), concrete certainty (some things are definitively true now), and aspirational narrative (we sense patterns or destinies that draw us onward). The logic of collapse and recursion offers a way to think about change and choice that neither reduces truth to arbitrary relativism nor to fixed absolutism: truth  and is  real piece by piece, and in doing so, it remains ever aligned with an infinite backdrop of meaning that keeps it from being merely relative or “small.”Such a metaphysical framework can have various implications. It provides a lens to interpret personal growth, scientific discovery, social progress, and even cosmic evolution as a unified story of truth realizing itself. It honors the role of imagination and myth in human life, without sacrificing the importance of rational coherence and empirical grounding. It places moral and intellectual conscience at the heart of reality’s unfolding, hinting that perhaps the universe’s truths are not just “out there” to find, but also “in here” to create responsibly. In sum, the  framework invites us to see ourselves as participants in a recursive dance of truth – always choosing, always learning, guided by ideals, grounded in reality, and never exhausting the richness of what can be known. It is a vision that is both humbling (for it acknowledges the endless and the unknown) and empowering (for it asserts the value of our choices and the possibility of progress). And ultimately, it portrays a cosmos where truth is less a monolithic monument and more an ever-flowing river – one we navigate in our finite boats of the present, with currents from the infinite deep and stars of meaning overhead, always moving toward a horizon we never fully reach but continually approach.\
VIII: Addendum: On the P vs NP QuestionTo answer the foundational question through the lens of this metaphysical framework:P is indeed  from NP .A global equivalence between P and NP would imply a resolution of the universe — a total collapse of all potential into manifest form. But the universe, as shown, resists total resolution, because it is structured upon recursive unfolding, not finality. The only “resolution” beyond such recursion would be an atemporal, adimensional, and a-existential truth — a domain outside of being itself, inaccessible to form. And maybe this is the very domain of absolute Truth, truthfullness across infinite posibility.On  terms, P is  to NP.That is, each instance of present truth (P) reflects a coherent collapse of seed-potential (NP), such that from within the recursion, the emergence appears seamless. P unfolds what NP held, not because they are the same, but because conscience recursively integrates what it once only observed.Thus, the resolution of P ≠ NP is not computational, but ontological: The difference protects reality, and the local equivalence permits it to grow.]]></content:encoded></item><item><title>Remark raises $16M to build out human-powered expert models for e-commerce</title><link>https://techcrunch.com/2025/07/01/remark-raises-16-million-in-to-build-out-human-powered-expert-models-for-e-commerce/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Startups working on AI-powered e-commerce tools often rely on external data or user signals to build and improve their models. But Remark lets thousands of human experts chat with users while they are purchasing items, and then using that knowledge to train models.]]></content:encoded></item><item><title>Threads gets its own DMs as app distances itself from Instagram</title><link>https://techcrunch.com/2025/07/01/threads-gets-its-own-dms-as-app-distances-itself-from-instagram/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:59:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At launch, Threads DMs will support one-on-one chats, preset emoji reactions, the ability to report spam, and mute DMs (as on Instagram). Other features, like group messaging, inbox filters, and more advanced message controls, will arrive in a later release.]]></content:encoded></item><item><title>Teaching AI to Say &quot;I Don&apos;t Know&quot;: A Four-Step Guide to Contextual Data Imputation</title><link>https://hackernoon.com/teaching-ai-to-say-i-dont-know-a-four-step-guide-to-contextual-data-imputation?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:58:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).We represent the missing data mechanism as a conditional distribution of M given X, which is parameterized by an unknown ϕ, as follows.\
In the literature, the following three standard mechanisms for missing data are defined [21].\
Missing completely at random (MCAR). An MCAR case occurs when the probability that a value of a variable is missing is independent of the variable itself and any other variables, expressed as follows.\
In MCAR, the missingness probability depends neither on the missing variable nor on the observed variables.\
 The probability that the value of a variable is missing only depends on the observed values of other variables XO. Thus, the missingness is independent of the missing variables and the missing value is predictable from the observed variables, formalized as follows.\
Missing not at random (MNAR). This case corresponds to missing mechanisms that are neither MCAR nor MAR. In MNAR, the reason for a value to be missing, can depend on other variables, but also on the value that is missing.\
Unlike MAR, the missingness in MNAR cannot be predicted only from the observed variables. There is a no general method of handling MNAR missing data properly [14].\
Often the reasons for missing data is ignored when the missingness is due to MCAR or MAR, thus imputation methods can be simplified [33]. For this reason, the majority of research covers the cases where missing data are of the MAR or the MCAR type.2.3 Generating Missing ValuesWe constructed synthetic datasets with up to 30% missing values by applying the following three missingness mechanisms on complete datasets: MCAR, MAR and MNAR. The implementations of these mechanisms are modified from [20].\
. It was introduced by randomly removing 30% of the observations from each feature.\
. First, we select all observations within the 30-th percentile range of an independent feature (usually the first column in the dataset). Then, we randomly remove 60% observations from each corresponding (dependent) feature.\
. We remove the observations of a feature if the observations fall within the 30-th percentile range of the feature value.Figure 1 illustrates the CLAIM process, which encompasses four stages: (1) constructing a contextualized natural language dataset, (2) generating suitable descriptors for\
missing values, (3) creating a missingness-aware contextualized dataset, and (4) adapting an LLM for downstream tasks. We detail these stages below.\
Constructing a Contextualized Natural Language Dataset. We construct a contextualized natural language dataset from a numeric dataset X containing missing values. The objective is to generate contextually suitable description of each attribute and its measures in natural language. For instance, a record from the UCI Wine dataset [12] with numeric input and output attributes is contextualized as follows: “The alcohol content in the wine is 12.47. The level of malic acid in the wine is 1.52 … The class of the wine is classified as class 1 wine.”[1] This step converts numeric values into detailed descriptions, preparing the dataset for embedding missing value descriptors.\
Generating Suitable Descriptors for Missing Values. Unlike conventional imputation methods that estimate missing values from observed data using numerical methods, we utilize contextually-relevant descriptors of missing values for imputation. We generate these descriptors by a conversational LLM (e.g., OpenAI’s ChatGPT-3.5 [2]). We prompt the LLM with a dataset description and instruct it to generate missing value descriptors, such as: “For any missing attribute values, suggest a descriptor for the missing data that I can place in those cells.” This method relies on the LLM’s extensive knowledge base to produce appropriate missing value descriptors. A list of feature-specific contextually relevant missing-value descriptors for selected datasets are provided in the Appendix.\
Creating a Missingness-Aware Contextualized Dataset. We construct the missingness aware contextualized natural language dataset Xmissingness_aware by replacing the missing values with the generated descriptors. This process ensures that each data instance is aware of its missing attributes, thus capable of improving the LLM’s ability to learn from incomplete data by providing explicit context. Furthermore, we use distinct descriptors for separate features in the dataset that contain missing values, thereby implicitly informing an LLM to handle the missingness of each feature in a contextually-suitable way for improving the performance of the downstream task.\
Adapting an LLM for Solving Downstream Tasks. The final step involves finetuning a pre-trained LLM with the missingness-aware, contextually-rich dataset. We incorporate specific task instructions and strategies for handling missing data into the fine-tuning process. For instance, for classification tasks, we might include instructions like: “Predict the class based on the given measurements. Use the context provided by missing value descriptors to inform your prediction.”\
This structured approach, from transforming datasets to fine-tuning LLMs, signifies a comprehensive method for addressing data missingness through the capabilities of LLMs.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.[1] the Python script used for contextualization is provided in the Supplementary Material.]]></content:encoded></item><item><title>Trump’s prospective TikTok buyer reportedly includes Oracle, Blackstone, a16z</title><link>https://techcrunch.com/2025/07/01/trumps-prospective-tiktok-buyer-reportedly-includes-oracle-blackstone-a16z/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:50:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After pushing the TikTok sale date back yet again, President Donald Trump said in an interview on Sunday that he had found a potential buyer for the ByteDance-owned platform.]]></content:encoded></item><item><title>I Automated My Content Side Hustle with Notion, ChatGPT, and Zapier — Here&apos;s the Exact Workflow</title><link>https://hackernoon.com/i-automated-my-content-side-hustle-with-notion-chatgpt-and-zapier-heres-the-exact-workflow?source=rss</link><author>The Crypto Quill</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:49:42 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Intro: The Dream of Passive ProductivityEverybody's going on about AI and automation. Does it really work, though, if you are an individual creator who wants to create a side income?I experimented: I set up an automated system to write, schedule, and post my content solely with ChatGPT, Notion, and Zapier. My aim was straightforward — to find out whether or not I could save time, generate passive income, and cut down on content creation burnout.No smoke-and-mirrors. No "just believe me" assertions.This is day-by-day how I did it, what I cabled up, and what actually happened in 7 days.Step 1: Content Ideation with ChatGPT“Generate title variations, outline blog posts, and create tweet threads.”I gave ChatGPT one prompt:“Give me 10 viral content ideas about using automation to earn passive income with Notion and Zapier.”ChatGPT brought back these gems:"How I Built a Passive Income Engine Using Just AI and No-Code Tools""The Lazy Creator's Guide to Automating a Content Business""How to Earn While You Sleep With Notion + Zapier Workflows"I selected two and used ChatGPT to outline the blog post structure, complete with H2 headers and CTA lines.Step 2: Organizing in Notion Notion \n   Content calendar, automation trigger base, and task manager.I created a content database in Notion with these columns:Status (Idea, Drafting, Done)Platform (Medium, Gumroad, HackerNoon)Output (Tweet, Post, Newsletter)**Why Notion?
\  Because it plays *really* well with Zapier.I tagged content by status, and every time I updated a status to “Done,” it triggered the automation in Step 3.Step 3: Zapier Automation Zapier \n   Automate cross-posting and link distribution.Here’s the automation flow I built: New “Done” status in Notion Auto-post summary + link to Twitter (via X) Add to newsletter draft in ConvertKit Log in Airtable for analytics I produced 3 times as much content by outsourcing formatting and idea generation.  My content got consistent, owing to Notion+Zapier oversight. Low effort, actual reach: I constructed an auto-distribution loop in under 2 hours.Zapier's basic plan is too limiting – just 100 tasks/month. Must upgrade.  – Zapier would not tweet if it was done too often. Affiliate link clickthrough was underperforming without strong CTAs or images.Webhooks to automate Substack formattingCanva API to generate auto thumbnailsA webhook that triggers when I  a keyword (using Voiceflow + Zapier)Takeaway: It's Real, But It’s Not MagicAI won't  — but it can . If you're a solo content creator and you want to launch a content business, then running admin and distribution automates creative hours.But only if your system is based on receipts, not hopes.Have you tried automating your side hustle? What’s in your stack? I respond to every comment — especially the ones that break my workflow.📧 You can reach me at [debestnext@gmail.com]]]></content:encoded></item><item><title>The Last Rank We Need? QDyLoRA&apos;s Vision for the Future of LLM Tuning</title><link>https://hackernoon.com/the-last-rank-we-need-qdyloras-vision-for-the-future-of-llm-tuning?source=rss</link><author>Model Tuning</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:30:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
A. Supplementary MaterialQDyLoRA offers an efficient and effective technique for LoRA-based fine-tuning LLMs on downstream tasks. Eliminating the need for fine-tuning multiple models to find the optimal LoRA rank and offering the possibility of fine-tuning larger LLMs are two main advantages of QDyLoRA. The experimental results demonstrated that the optimal rank for QDyLoRA can be surprisingly low, yet it consistently outperforms QLoRA. QDyLoRA provides greater flexibility for deploying LLMs in various contexts and represents a promising step towards making fine-tuning large language models more accessible and efficient.While the 4-bit QDyLoRA exhibits notable performance, it falls short of achieving the performance levels of full precision fine-tuning. One possible solution could be dynamic quantized DyLoRA (DyQDyLoRA), in which the quantization level could also vary during finetuning. In particular, the finetuning strategy can dynamically switch between different quantization levels based on a predefined learning feedback. Additionally, further research is required to investigate the impact of LoRA's scalar and the range of underlying ranks in QDyLoRA.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255.\
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.\
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient fine-tuning of large-scale pretrained language models. Nature Machine Intelligence, 5(3):220–235.\
Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. 2022. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650.\
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2021. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366.\
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR.\
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.\
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023. Open-assistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327.\
Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee. 2022. Alphatuning: Quantization-aware parameterefficient adaptation of large-scale pre-trained language models. arXiv preprint arXiv:2210.03858.\
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965.\
Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. Webglm: Towards an efficient webenhanced question answering system with human preferences. arXiv preprint arXiv:2306.07906.\
Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. 2021. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv preprint arXiv:2110.07577.\
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991–13005.\
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.\
Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. 2022. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint arXiv:2210.07558.\
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.Table 4 provides an overview of the hyperparameters and experimental configurations employed in this study, which are crucial configurations that determine various aspects of the training process and model behavior in this study. Common key parameters across the experiments include the choice of optimizer, Adam-Beta2 value, maximum gradient norm, and warmup ratio, which collectively influence how the model adjusts its weights during training. LoRA-specific parameters such as LoRA dropout probability, maximum LoRA rank, and alpha value control the behavior of LoRA layers. Additionally, double quantization and quantization type impact the precision of numerical representations within the model, which are considered the same as baselines. Learning rate scheduling and weight decay contribute to the optimization process, helping to prevent overfitting and stabilize training. Random seeds ensure reproducibility, while the specified GPU determines the hardware used for training. Each model configuration, whether for the Web-GLM, GSM8k, or the specific experiment outlined in Table 1, features parameters tailored to the characteristics of the dataset and the computational resources available. These hyperparameters collectively shape the training process, ultimately influencing the performance and effectiveness of the models in the study.A.2 Generated Text QualityTo describe the quality of texts generated by QDyLoRA, a sample query taken from GSM8k is fed to Falcon-40b trained by QDyLoRA. Below, we report the generated answers for different LoRA ranks.\
: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers’ market?\
: Janet has 16 - 3 - 4 = «16-3-4=7»7 eggs left. She sells 7 - 16 = «7-16=9»9 eggs at the\
\
farmers’ market. She makes 9 * 2 =«9*2=18»18 at the farmers’ market every day.\
 She eats 3 eggs for breakfast and bakes 4 muffins per day, so she uses 3 + 4 = «3+4=7»7 eggs per day. She has 16 - 7 = «16- 7=9»9 eggs left over each day. She sells these eggs at the farmers’ market for $2 per egg, so she makes 2 * 9 = «2*9=18»18 per day at the market.\
: She eats 3 eggs for breakfast and bakes 4 muffins per day, so she uses 3+4 = «3+4=7»7 eggs per day. She sells 16-7 = «16-7=9»9 eggs per day for $2 per egg. She makes 9*2 = «9*2=18»18 per day at the market.\
: Janet’s ducks lay 16 eggs per day and she eats 3 for breakfast every morning so she has 16-3=13 eggs left. Janet bakes 4 muffins every day and she uses 4 eggs so she has 13-4=9 eggs left. Janet sells the remaining eggs at the farmers’ market for $2 each so she makes 9*2=«9*2=18»18 dollars a day. Janet makes 18 dollars a day at the farmers’ market.(1) Hossein Rajabzadeh, University of Waterloo and Huawei Noah’s Ark Lab (hossein.rajabzadeh@uwaterloo.ca);(2) Mojtaba Valipour, University of Waterloo (mojtaba.valipour@uwaterloo.ca);(3) Tianshu Zhu, Huawei Noah’s Ark Lab (tianshu.zhu@huawei.com);(4) Marzieh Tahaei, Huawei Noah’s Ark Lab (marzieh.tahaei@huawei.com);(5) Hyock Ju Kwon, (hjkwon@uwaterloo.ca);(6) Ali Ghodsi, (ali.ghodsi@uwaterloo.ca);(7) Boxing Chen, Huawei Noah’s Ark Lab (boxing.chen@huawei.com);(8) Mehdi Rezagholizadeh, Huawei Noah’s Ark Lab (mehdi.rezagholizadeh@huawei.com).:::info
This paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.]]></content:encoded></item><item><title>Grammarly acquires AI email client Superhuman</title><link>https://techcrunch.com/2025/07/01/grammarly-acquires-ai-email-client-superhuman/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:25:03 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In its announcement, Grammarly said it wants to build AI agents for emails using Superhuman's tech. ]]></content:encoded></item><item><title>The electric Hummer is almost outselling the F-150 Lightning</title><link>https://techcrunch.com/2025/07/01/the-electric-hummer-is-almost-outselling-the-f-150-lightning/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:22:46 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[GM's EVs were popular in the second quarter, a complete contrast to the struggles by Ford, Tesla, Hyundai, and Kia to grow electric vehicle sales.]]></content:encoded></item><item><title>Proton Joins Antitrust Lawsuit Against Apple&apos;s App Store Practices</title><link>https://apple.slashdot.org/story/25/07/01/0917211/proton-joins-antitrust-lawsuit-against-apples-app-store-practices?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Encrypted communications provider Proton has joined an antitrust lawsuit against Apple, filing a legal complaint that claims the company's App Store practices harm developers, consumers, and privacy. The Switzerland-based firm joined a group of Korean developers who sued Apple in May rather than filing a separate case. 

Proton asked the US District Court for Northern California to require Apple to allow alternative app stores, expose those stores through its own App Store, permit developers to disable Apple's in-app payment system, and provide full access to Apple APIs. The company added a privacy-focused argument to typical antitrust complaints, contending that Apple's pricing model particularly penalizes companies that refuse to harvest user data. Developers of free apps typically sell user data to cover costs, while privacy-focused companies like Proton must charge subscriptions for revenue, making Apple's commission cuts more burdensome.]]></content:encoded></item><item><title>Meta adds business voice calling to WhatsApp, explores AI-powered product recommendations</title><link>https://techcrunch.com/2025/07/01/meta-adds-business-voice-calling-to-whatsapp-explores-ai-powered-product-reccomendations/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:15:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[WhatsApp announced it's introducing the ability for large businesses to reach customers through voice calls, which will allow the app to explore the use of AI-powered voice agents.]]></content:encoded></item><item><title>What If Your Blog Had a Product Manager? (Hint: It’s You)</title><link>https://hackernoon.com/what-if-your-blog-had-a-product-manager-hint-its-you?source=rss</link><author>Editing Protocol</author><category>tech</category><pubDate>Tue, 1 Jul 2025 15:00:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most bloggers focus on publishing content—but the best ones treat their blog like a product. That means thinking strategically: defining your audience, setting goals, building systems, and iterating based on feedback.Whether you’re writing to grow a personal brand, build a business, or simply share ideas, applying product management principles to your blog can help you stay focused, intentional, and impactful.Here’s how to step into the role of Product Manager of your blog—and start running your content like a pro.:::tip
Already an expert? Share your learnings using this template!Step 1: Formulate a VISION for your career in contentHow do you want to see yourself growing and operating as a writer and blogger?Outline the specific challenges your writing is intended to solve—who it’s for, why it matters, and when they’ll need it. Go with the first thoughts that come to mind, and don’t be afraid to get really specific. \n :::info
 when you try to speak to everyone, you end up speaking to no one.Here’s a content strategy vision template to help guide your thinking:Step 2: Document Your Ideal Reader PERSONASThis is a crucial exercise in empathizing with your readers—understanding their pain points and knowing what they need so your content can directly address those needs.Creating at least 3–5 ideal reader personas helps you refine not only the tone, branding, and design of your content, but also your overall content strategy, calendar, and monetization opportunities.:::info
If you don’t yet have access to analytics about your readers, that’s okay—start with educated guesses. Try quick long-tail keyword research by typing potential questions into Google and reviewing the suggested queries and related searches. Tools like Ahrefs, Answer the Public, or even your own search history can help. Ideally, you’ll use Google Analytics to uncover a wealth of information about the audience already engaging with your work.Step 3: Pick Your PROTO-PERSONA and Start DetailingFrom your 3–5 ideal reader personas, choose one to represent the majority of your target audience. Then use your imagination (and research) to flesh out the following details: What do you know about this person? (Age, education, location, job, background) Do they need to earn more money? Change careers? Master a skill? Be the first to know something? Build a side hustle? Are they introverted or extroverted? Where do they hang out online and offline? What are their routines? How do they learn—visually, audibly, by reading? Do they want to start a business? Learn a tool or software? Grow an audience? Improve time management?By putting all this together, you’ll have a clear map of who you’re creating content for—a smart foundation for planning and writing.Step 4: Sketch Out Your Reader Journey MapThis step helps define your content distribution strategy by putting yourself in your reader’s shoes and mapping out the path they take toward having their needs met through your content.Start with your vision statement and your readers’ goals. What might they be typing into Google right now? Make a list of relevant search terms or questions your content could answer.How will my ideal audience discover my content? At HackerNoon, for example, we get a lot of direct traffic (people typing the URL directly or bookmarking us) and organic traffic (people searching and finding our stories). That tells us our brand is strong and our content answers real questions.How would they most enjoy engaging with my content? Does your post need a YouTube embed? Could you repurpose it for social media? Should it become a newsletter, podcast, or ebook?Step 5: Define Your Content Strategy, Goals, and FocusThis final step is about clarity. Define your long-term strategy, your unique value as a creator, and potential blockers to success. It’s where you turn your content creation into a career plan.Start with this question:What would wild success look like?Write down outcomes for the following time frames:Next, write your six-month goal as a statement, as though it’s already happened:“In the last six months, I published 100,000 words and created 3 months’ worth of reading time. I also launched a podcast / niche TikTok / LinkedIn community, built my personal site, and got my first 100 newsletter signups.”Then, play the pessimist for a second and ask the hard questions:What are the potential blockers to my success?What are my current challenges?What do I need to prioritize to overcome them?How can I turn daily struggles into #relatablecontent?And lastly, ask yourself:What sets me apart from other creators in my niche?What’s the most meaningful thing I could focus on to rise above the noise?What unique perspectives and experiences do I bring?Which creators could I collaborate with to reach a more relevant audience?:::tip
👉 Ready to put this into practice?By treating your blog like a product, you're not just publishing content—you’re building a career. A roadmap. A strategy for your ideas.Start small, iterate often, and remember: you’re the product manager of your writing life.]]></content:encoded></item><item><title>CLAIM: A Contextual Language Model for Accurate Imputation of Missing Tabular Data</title><link>https://hackernoon.com/claim-a-contextual-language-model-for-accurate-imputation-of-missing-tabular-data?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:57:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).\
. This paper introduces the Contextual Language model for Accurate Imputation Method (CLAIM), a novel strategy that capitalizes on the expansive knowledge and reasoning capabilities of pre-trained large language models (LLMs) to address missing data challenges in tabular datasets. Unlike traditional imputation methods, which predominantly rely on numerical estimations, CLAIM utilizes contextually relevant natural language descriptors to fill missing values. This approach transforms datasets into natural language contextualized formats that are inherently more aligned with LLMs’ capabilities, thereby facilitating the dual use of LLMs: first, to generate missing value descriptors, and then, to fine-tune the LLM on the enriched dataset for improved performance in downstream tasks. Our evaluations across diverse datasets and missingness patterns reveal CLAIM’s superior performance over existing imputation techniques. Furthermore, our investigation into the effectiveness of context-specific versus generic descriptors for missing data highlights the importance of contextual accuracy in enhancing LLM performance for data imputation. The results underscore CLAIM’s potential to markedly improve the reliability and quality of data analysis and machine learning models, offering a more nuanced and effective solution for handling missing data.‘Well! I’ve often seen a cat without a grin,’ thought Alice; ‘but a grin without a cat! It’s the most curious thing I ever saw in all my life!’Lewis Carroll, Alice’s Adventures in Wonderland (1865)\
A compelling real-world example of how context-unaware estimation of missing data can defy reality and compromise the integrity of downstream tasks is highlighted in [35]. This account describes a scenario where a predictive machine learning (ML) model, developed to process tabular demographic data including individuals’ ages, faced challenges due to missing age entries. The imputation strategy employed involved substituting missing age values with zeros—a common default for initializing integers in several programming languages. This approach inadvertently led the model to categorize individuals with unspecified ages as “toddlers”, resulting in aberrant model behavior. Numerous instances echoing this type of bias in ML models, resulting from context-unaware imputation of missing data, are reported in the literature [35,14,38,43,34,1].\
These incidents prompt a critical inquiry into more sophisticated and reality-congruent methods for estimating missing tabular data. While simple statistical replacements such as the mean or median might suffice under the assumption of a normal distribution, predictive ML techniques like k-Nearest Neighbors (k-NN), random forest (RF), or even deep learning (DL)-based generative models offer alternative strategies [20,13,45,8]. These ML/DL methods typically presuppose that missingness in an attribute correlates with observable values in other features. However, this raises fundamental questions: What if the missing data is independent of observed values? Or if the absence of data is influenced solely by unobserved variables? In scenarios where missingness is not attributable to external factors or other observed data, the challenge then becomes how to accurately estimate the missing values. To date, no single imputation method has proven universally effective, underscoring the complexity and variety of missing data scenarios encountered in practice [20].\
This paper introduces a novel approach, leveraging the capabilities of pre-trained large language models (LLMs) [6,9,39,26], to innovatively address the challenge of missing data in tabular datasets. Our method, the Contextual Language model for Accurate Imputation Method (CLAIM), diverges significantly from traditional imputation techniques that predominantly estimate missing values through numerical methods. Instead, CLAIM harnesses LLMs’ expansive knowledge [28,29] and reasoning capabilities [9,42,4] in a : initially, it employs LLMs to generate contextually relevant natural language descriptors for missing values, effectively transforming datasets into natural language contextualized formats. This transformation is crucial, as it aligns the data with the inherent strengths of LLMs, making it more amenable to their processing capabilities.\
Subsequently, these enriched datasets serve as the foundation for fine-tuning LLMs to enhance performance in downstream tasks (e.g., classification), showcasing a unique and effective use of language models beyond their conventional applications. By incorporating contextually relevant descriptors for missing data, CLAIM not only addresses the variability and specificity inherent in data across different domains but also adeptly navigates the complexities introduced by various missingness mechanisms. Through this innovative integration of LLMs into the data imputation process, CLAIM aims to deliver a more nuanced, accurate, and reliable method for data recovery, essential for improving the quality of subsequent data analysis and machine learning tasks.\
To assess the effectiveness of CLAIM, we undertake a comprehensive analysis across three standard missing data mechanisms—MCAR (Missing Completely at Random), MAR (Missing at Random), and MNAR (Missing Not at Random) [30], and comparing CLAIM against a wide range of existing imputation methods spanning single and multiple imputation techniques, non-ML and ML methods, and discriminative and generative ML approaches. Our empirical studies, aimed at evaluating the impact of CLAIM on LLM-based downstream classification tasks, are guided by two principal research questions (RQs):\
 How effective is CLAIM in imputing missing values across the distinct missingness mechanisms (MCAR, MAR, and MNAR) and how does it compare with existing imputation methods in terms of accuracy and robustness across varied datasets and missing data scenarios?\
 How does the choice of phrasing for missingness descriptors in CLAIM affect the performance of LLM-based downstream tasks?\
The main contributions of this work are multifaceted. Firstly, CLAIM represents a departure from traditional imputation methods by using LLMs to generate context specific descriptors for missing data, establishing a new benchmark in data imputation. Secondly, through extensive empirical evaluation, we demonstrate CLAIM’s superior performance over existing methods across varied datasets and missingness patterns. Lastly, our analysis of context-specific versus generic descriptors provides key insights into optimizing LLM performance for imputation tasks, highlighting the significance of contextual accuracy. Collectively, these contributions advance data preprocessing techniques and open novel pathways for applying LLMs in complex data science challenges.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>NIH-Funded Science Must Now Be Free To Read Instantly</title><link>https://science.slashdot.org/story/25/07/01/0827211/nih-funded-science-must-now-be-free-to-read-instantly?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Starting today, researchers funded by the US National Institutes of Health (NIH) will be required to make their scientific papers available to read for free as soon as they are published in a peer-reviewed journal. That's according to the agency's latest public-access policy, aimed at making federally funded research accessible to taxpayers. From a report: Established under former US president Joe Biden, the policy was originally set to take effect on 31 December for all US agencies, but the administration of Biden's successor, Donald Trump, has accelerated its implementation for the NIH, a move that has surprised some scholars. That's because, although the Trump team has declared itself a defender of taxpayer dollars, it has also targeted programmes and research projects focused on equity and inclusion for elimination. And one of the policy's main goals is to ensure equitable access to federally funded research. 

The move means that universities will have less time to advise their researchers on how to comply with the policy, says Peter Suber, director of the Harvard Open Access Project in Cambridge, Massachusetts. There is usually "some confusion or even some non-compliance after a new policy takes effect, but I think universities will eventually get on top of that," he says.]]></content:encoded></item><item><title>Faster, More Accurate IoT Security: A Quantitative Analysis of the CUMAD Framework</title><link>https://hackernoon.com/faster-more-accurate-iot-security-a-quantitative-analysis-of-the-cumad-framework?source=rss</link><author>Hypothesis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:39:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).In this section we perform evaluation studies to investigate the performance of CUMAD using the publicdomain N-BaIoT dataset [8]. In order to better understand the evaluation studies, we will first describe the dataset, in particular, the features of the data points contained in the dataset. We will also compare the performance of CUMAD with that of the N-BaIoT scheme (which is the name for both the dataset and the corresponding scheme on detecting compromised IoT devices) [8].5.1. Dataset, Features, and CUMAD System Setup\
N-BaIoT contains both benign and (Mirai and Bashlite) attack traffic of 9 commercial IoT devices, including two doorbells (Danmini and Ennio), an Ecobee thermostat, three baby monitors (different models from Provision and Philips), two SimpleHome security cameras, and a Samsung webcam. Benign IoT device traffic was collected immediately after the corresponding IoT device was connected to the experimental testbed. Care was taken to ensure that various representative normal operations and behaviors of IoT devices were collected into the benign dataset.\
In the N-BaIoT dataset, each data point corresponds to an arrived packet, and contains 115 statistical features, which together represent a behavioral snapshot that describes the context of the corresponding packet when it arrives at the data collection point. The snapshot contains the source and destination device information, the protocol information, among others. More specifically, the 115 features were extracted in the following manner. For each arriving packet, a total of 23 features were collected at different levels of aggregation (see Table 1), including features aggregated at source IP address level, at source MAC and IP addresses level, at level of channel (source and destination IP addresses), and at socket level (source and destination IP addresses and port numbers). These 23 features were extracted in a sliding window fashion, over 5 time windows of 100ms, 500ms, 1.5sec, 10sec, and 1min, respectively, generating a total of 115 features for each data point.\
We use the Keras sequential model as the foundation for our development of the autoencoder [12]. The model’s input dimension is set to match the number of features in the dataset (that is, 115). To ensure effective compression, we implement three hidden layers within the encoder. These layers progressively reduce the dimensions to 87, 58, 38, and 29, respectively, with the last one (29) being the dimension of the output layer of the encoder, that is, the dimension of the obtained code. Conversely, the decoder component mirrors the dimensions of the encoder layers in the reverse order, starting from 38. By employing compression and decompression in the encoder and decoder layers, we effectively eliminate redundant information from the features of the input data points. To optimize training performance, we utilize the Adam optimizer, and the mean square error is used as the reconstruction error (objective function of the model).\
SPRT requires four user-defined parameters in order to compute the upper and lower bounds A and B (see Eq. (3)), as well as the step function for computing Λn following each observation (see Eq. (1)). The desired values for both the false positive rate and the false negative rate (represented by α and β, respectively) are typically very small. In this study we set both α and β to 0.01. Ideally, the parameter θ indicates the true probability of an observation being classified as an anomaly, from either a benign or compromised IoT device. We determine the values for θ0 and θ1 through our preliminary studies, and set them to 0.2 and 0.8, respectively.Table 2 shows the performance of CUMAD in detecting IoT devices, in terms of accuracy, recall, and F1 score [12]. From the table we can see that CUMAD achieves superior performance in all three metrics. For example, for 5 of the IoT devices, CUMAD is able to detect all the compromised cases (see the column of Recall). CUMAD is also able to detect vast majority of the compromised cases for the remaining two of the IoT devices, with recall scores of 0.999 and 0.994. Considering both detection precision of attack and benign traffic, we can see that CUMAD also performs very well, with an accuracy score ranging from 0.955 to 0.995 for all 7 IoT devices. The F1 scores, which is a weighted average of the precision and recall scores of a model, also confirm that CUMAD performs well in detecting compromised cases.\
Figure 4 shows the false positive rates of an autoencoder based anomaly detection scheme and CUMAD. As shown in the figure, the false positive rates of the autoencoder-based anomaly detection scheme for the 7 IoT devices range from 0.77% to 11.22%, while the false positive rates of CUMAD range from 0.014% to 2.067%. On average the autoencoder based anomaly detection scheme has about 3.57% false positive rate, while the false positive rate of CUMAD is about 0.5%, which represents about 7 times performance improvement in terms of false positive rate for CUMAD over the autoencoder-based anomaly detection scheme.\
\
For performance comparison, we also include in the table the performance results of the N-BaIoT scheme, with the same evaluation studies setup. We can see from the table that CUMAD and N-BaIoT performs comparably in terms of all three-performance metrics. However, N-BaIoT works on a fixed window size. Table 2 shows that N-BaIoT requires a relatively large window size, ranging from 20 to 82 (column with name Window Size). In contrast, CUMAD works in an online fashion and does not requires such a fixed window size. Table 2 shows the average number of observations required for CUMAD to reach a detection (column with name Mean Size); we can see from the table that it takes on average less than 5 observations for CUMAD to make a detection of a compromised case, much quicker than NBaIoT. In order to have a better understanding of the number of observations for CUMAD to make a detection of a compromised case, Figure 5 shows the cumulative distribution function (CDF) of required observations for CUMAD to make a detection for all the 7 IoT devices. We can see from the figure that the vast majority of detection requires less than 10 observations for all 7 IoT devices.\
\
In summary, compared to simple anomaly detection schemes such as the ones only based on autoencoders, CUMAD can greatly reduce the false positive rates, making CUMAD much more attractive than simple anomaly detection schemes in the real-world deployment. Compared with window-based schemes such as N-BaIoT, CUMAD requires much less observations to reach a detection, and thus can detect compromised IoT devices much quicker.In this paper we have developed CUMAD, a cumulative anomaly detection framework for detecting compromised IoT devices. CUMAD employs an unsupervised neural network autoencoder to classify whether an individual input data point is anomalous or normal. CUMAD also incorporates a statistical tool sequential probability ratio test (SPRT) to accumulate sufficient evidence to detect if an IoT device is compromised, instead of directly relying on individual anomalous input data points. CUMAD can greatly improve the performance in detecting compromised IoT devices in terms of false positive rate compared to the methods only relying on individual anomalous input data points. In addition, as a sequential method, CUMAD can quickly detect compromised IoT devices. Evaluation studies based on public-domain IoT dataset N-BaIoT confirmed the superior performance of CUMAD.[1] Yin Minn Pa Pa, Shogo Suzuki, Katsunari Yoshioka, Tsutomu Matsumoto, Takahiro Kasama, and Christian Rossow. Iotpot: analysing the rise of iot compromises. In Proceedings of the 9th USENIX Conference on Offensive Technologies, pages 9–9, 2015.\
[2] Michael Fagan, Katerina Megas, Karen Scarfone, and Matthew Smith. Foundational cybersecurity activities for iot device manufacturers. Technical report, National Institute of Standards and Technology, May 2020.\
[3] Mohammed Ali Al-Garadi, Amr Mohamed, Abdulla Khalid Al-Ali, Xiaojiang Du, Ihsan Ali, and Mohsen Guizani. A survey of machine and deep learning methods for internet of things (iot) security. IEEE Communications Surveys & Tutorials, 22(3):1646–1685, 2020.\
[4] Andrew A Cook, Goksel Mısırlı, and Zhong Fan. Anomaly detection ¨ for iot time-series data: A survey. IEEE Internet of Things Journal, 7(7):6481–6494, 2019.\
[5] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):1–58, 2009.\
[6] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.\
[7] Abraham Wald. Sequential Analysis. John Wiley & Sons, Inc, 1947.\
[8] Yair Meidan, Michael Bohadana, Yael Mathov, Yisroel Mirsky, Asaf Shabtai, Dominik Breitenbacher, and Yuval Elovici. Nbaiot—network-based detection of iot botnet attacks using deep autoencoders. IEEE Pervasive Computing, 17(3):12–22, 2018.\
[9] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1–38, 2021.\
[10] Erol Gelenbe and Mert Nakıp. Traffic based sequential learning during botnet attacks to identify compromised iot devices. IEEE Access, 10:126536–126549, 2022.\
[11] Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni, N Asokan, and Ahmad-Reza Sadeghi. D¨ıot: A federated self-learning anomaly detection system for iot. In 2019 IEEE 39th International conference on distributed computing systems (ICDCS), pages 756–767. IEEE, 2019.\
[12] Francois Chollet. Deep learning with Python. Simon and Schuster, 2021.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Redefining IoT Threat Detection: The Power of Cumulative Analysis in the CUMAD Framework</title><link>https://hackernoon.com/redefining-iot-threat-detection-the-power-of-cumulative-analysis-in-the-cumad-framework?source=rss</link><author>Hypothesis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:33:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).3.2. Sequential Probability Ratio TestSequential probability ratio test (SPRT) is a simple yet powerful statistical tool that has found applications in many different domains, in particular, fault detection or quality control [7]. SPRT is a variant of the traditional probability ratio test for testing under what distribution (or with what distribution parameters), it is more likely to have the observed sequence of samples. Unlike traditional probability ratio test that requires a pre-defined fixed number of samples to carry out the test, SPRT works in an online fashion; it updates the corresponding statistical measure as samples arrive sequentially, and can conclude when sufficient samples have arrived to reach a decision. In its simplest form, SPRT is a statistical method to test a simple null hypothesis against a simple alternative hypothesis. In the following we will more formally describe the operation of SPRT.\
Let y denote a Bernoulli random variable with an unknown parameter θ, and let yi , for i = 1, 2, . . . denote the corresponding successive observations of y. SPRT can be used to test a simple null hypothesis H0 that θ = θ0 against a simple alternative hypothesis H1 that θ = θ1, more specifically,\
As a simple and powerful statistical tool, SPRT possesses a few critical and desired properties that lead to the wide-spread application of the technique in many different domains. First, the false positive and false negative rates of SPRT can be specified by user-desired error rates, which in turn control the thresholds of the model. Second, it has been proved that, among all sequential and non-sequential probability ratio testing algorithms, SPRT minimizes the expected number of observations to reach a decision with no greater errors. Put in another way, on average SPRT can reach a conclusion quickly compared to other probability ratio testing algorithms.In this section we will first discuss the considered network model, where CUMAD will be deployed, and then we will present the design of the CUMAD framework.Figure 2 illustrates the conceptual network model, where CUMAD is deployed. As shown in the figure, in order for CUMAD to carry out its task to detect compromised IoT devices in a network, CUMAD needs to have access to the network traffic associated with the IoT devices in the network. Depending on the deployment scenarios of CUMAD in the network and the corresponding network architecture, there can be a few different ways for CUMAD to obtain the corresponding network traffic of IoT devices. In essence, CUMAD as a network-based solution can be deployed in a similar way as network-based intrusion detection systems.\
\
In the current design of CUMAD, (statistical) features from raw network traffic will be extracted and fed to CUMAD for detecting compromised IoT devices. Each input data point fed to CUMAD comprises these extracted features, and can be summarized at different levels of granularity of network traffic, such as packets, flows, and time windows. These features will capture the network behavioral characteristics of the corresponding IoT devices. In Section 5 we will discuss the network traffic features contained in the public-domain N-BaIoT dataset when we perform evaluation studies on CUMAD [8].4.2. CUMAD: Cumulative Anomaly DetectionFigure 3 illustrates the high-level architecture of the CUMAD framework. CUMAD consists of two main components: an anomaly detection component (ADC) and a cumulative anomaly component (CAC). Assuming the model has been properly trained (will be discussed shortly), given an input data point with the corresponding features, the main responsibility of ADC is to classify an input data point as either normal or anomalous. After the classification of the input data point, the result is passed to the second component (CAC), which will maintain a cumulative view of the network traffic behavior of the corresponding IoT device, by sequentially merging the individual classification results into the view. When sufficient evidence on an IoT device has been collected to indicate that it has been compromised, an alert will be generated. In the following we will describe each component in details, both in model training and deployment to detect compromised IoT devices.\
We note that different types of IoT devices perform drastically different functionalities, and in addition, we would like to detect which IoT device is compromised, we need to develop a separate CUMAD model for each IoT device and monitor their network traffic behaviors separately using their corresponding CUMAD models. Therefore, the following discussions are for one IoT device. We note that, although there are vastly diverse types of IoT devices on the Internet, autoencoder is a powerful neural network that is capable of learning different models. Therefore, we are able to build diverse autoencoder models, one for each IoT device, despite their vastly different network traffic behaviors of\
\
these IoT devices. In addition, IoT devices also provide us with unique opportunities in establishing the models of normal behaviors, compared to traditional computer systems. In particular, each IoT device only performs a few well-defined simple functionalities in an autonomous or semiautonomous fashion, with very limited user interactions after the initial device configuration and setup. This makes it simpler to establish a model of normal behaviors in carrying out anomaly detection.\
4.2.1. Model Training and Setup. Before CUMAD can be used to monitor network traffic to detect compromised IoT devices, we need to train a CUMAD system for each IoT device so that it can learn the normal model of the device. During the training stage of a deployed CUMAD system, it is critical that we should only feed normal (benign) network traffic of the device to the system. This can be done, for example, when an IoT device is first deployed in the network. In order to minimize false positives during the detection stage, it is also important that CUMAD has a reasonably complete view of all the normal network traffic behavior of the device.\
\
As discussed above, the premise of using an autoencoder as an anomaly detection mechanism is that, although it can effectively reconstruct data points that are similar to the data points that it has seen previously during the training stage, it in general performs poorly to reconstruct data points that substantially differ from the training data. This is manifested in large reconstruction errors. Therefore, we will use the reconstruction error as the anomaly score, and when the anomaly score is greater than the pre-defined threshold, we classify the corresponding input data point as an anomalous sample.\
\
The parameters α and β are the user-desired false positive rate and false negative rate, respectively. They normally have small values for all practical applications, for example, in the range 0.01 to 0.05. The initial value of Λn in Eq. (1) is set to 0 during the setup stage of the system. The functionality of the Alert module is to generate proper alert to inform system administrators of the detection of a compromised IoT device. Other actions can also be taken based on the local security policies, for example, informing proper agents to isolate the compromised IoT device.\
 After the model has been trained and the required parameters have been set for the CUMAD system, it can be used to monitor network traffic to detect if the corresponding IoT device has been compromised. In the following we describe the basic steps of a CUMAD system in carrying out the detection task (see Algorithm 1).\
\
generated to indicate a normal data point. The output of the Detector module is then passed to the SPRT module to determine if sufficient evidence has been accumulated to make a decision regarding the nature of the IoT device (compromised or normal; line 10 of the algorithm). SPRT updates the probability ratio measure Λn according to Eq. (1), as the 0 (normal data point) and 1 (anomalous data point) output sequence of the Detector module arrives (lines 13 to 18). After the value of Λn is updated for each input data point, SPRT compares the value of Λn with the two boundaries A and B to determine if a decision can be made (lines 20 to 29). When the value of Λn hits or crosses the upper bound B, SPRT will conclude that the alternative hypothesis H1 is true, that is, the IoT device has been compromised. In this case, SPRT will inform the Alert module the detection of an compromised IoT device. Proper alert will be generated and corresponding system administrators will be informed. In addition, from this time on, it is not necessary for CUMAD to monitor the IoT device anymore, until proper actions have been taken to clean up or remove the device.\
When the value of Λn is equal to or smaller than the lower bound A, SPRT reaches the conclusion that H0 is true, that is, the IoT device is not compromised. From the viewpoint of detecting compromised IoT devices, this conclusion is less interesting in that we cannot terminate the monitoring of the device as we have done when a compromised IoT device is detected. A normal IoT device may become compromised at a later time. Therefore, in this case, we will reset the state of SPRT to restart the monitoring of the IoT device, in particular, we will reset the value of Λn to zero. If a decision cannot be reached at this time (line 28), SPRT will simply wait for additional input data points and repeat the same procedure.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>David George from a16z on the future of going public at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/07/01/david-george-on-the-future-of-going-public-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[David George of a16z joins the Going Public stage at TechCrunch Disrupt 2025, Moscone West in San Francisco, from October 27–29. Register now.]]></content:encoded></item><item><title>Solving the IoT&apos;s &quot;Boy Who Cried Wolf&quot; Problem: From Individual Alerts to Cumulative Certainty</title><link>https://hackernoon.com/solving-the-iots-boy-who-cried-wolf-problem-from-individual-alerts-to-cumulative-certainty?source=rss</link><author>Hypothesis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:27:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).The problem of anomaly detection has been studied in many different application domains and many techniques have been proposed, based on statistical inference, data mining, signal processing, and recently machine learning, among others. We note that in the literature of anomaly detection, anomalies have been classified into three categories: point anomaly, contextual anomaly, and collective anomaly [5]. However, they are all concerned with the detection of individual anomalous events, which are different from the cumulative anomaly we consider in this paper. In cumulative anomaly we are more concerned with the cause of anomalous events (for example, compromised IoT device), instead of individual anomalous events. As a consequence, we need to accumulate sufficient evidence (individual anomalous events) to reach a conclusion (for example, if an IoT device is compromised) in cumulative anomaly detection.\
Given the importance of improving IoT security, many security attack detection techniques have been proposed, including various ML-based solutions [3], [9]. However, some of them required the training data of both benign and attack traffic. They cannot detect new security attacks. Others developed anomaly detection based schemes to detect anomalous traffic originated from IoT devices. However, as we have discussed in Section 1, they often trigger a large number of false alerts, rendering them unusable in detecting compromised IoT devices in the real-world deployment.\
In [10], Gelenbe and Nakip developed an online scheme CDIS to detect compromised IoT devices based on autoassociative learning. However, the design of CDIS was tailored to Mirai botnet, and may not be effective to detect other types of compromised IoT devices. In addition, CDIS still only targeted individual anomalous events, instead of cumulative anomaly detection as we perform in this paper. The authors of [11] developed a federated self-learning based scheme D¨IoT to detect compromised IoT devices, where local security gateways communicate with remote IoT Security Service to build a more comprehensive normal traffic model of IoT devices. In order to further reduce the false alerts generated by the aggregated anomaly detection model, a window-based scheme was adopted, where anomaly alarm was triggered only if the fraction of anomalous packets was greater than a pre-defined threshold value. In [8], Meidan et al. presented an autoencoder-based anomaly detection system N-BaIoT to detect compromised IoT devices. N-BaIoT also tried to reduce the number of false alerts triggered by the pure anomaly detection system using a window-based scheme with a majority vote to reach a decision.3. Background on Autoencoder and SPRTIn this section we provide the necessary background on autoencoder and sequential probability ratio test (SPRT) for understanding the development of the proposed CUMAD framework. We refer interested readers to [6] and [7], respectively, for the detailed treatment on these two topics.Autoencoder is an unsupervised neutral network that aims to reconstruct the input at the output. Figure 1 illustrates a simple standard (undercomplete) autoencoder.\
\
An autoencoder can be considered as consisting of two components: an encoder f and an decoder g. Given input data x, the encoder function f maps x to a latent-space representation, or code h, that is h = f(x). Using the corresponding code h as the input, the decoder function g tries to reconstruct the original input x at its output x ′, that is, x′ = g(h). Combining both the encoder function and decoder function together, we have x′ = g(f(x)). Let L(x, x′) be the reconstruction error, that is, the difference between x and x′. The autoenceder aims to minimize L(x, x ′). We note that there are different definitions of L(x, x′) and one of the most common definitions is the mean squared errors (MSE). We note that in the example autoencoder of Figure 1, both the encoder and decoder have only one hidden layer. This is only for illustration purpose. In reality they can have many hidden layers, depending on the specific application requirement.\
Autoencoders have been traditionally used in applications of dimensionality reduction and feature learning, by focusing on the compressed code of an autoencoder, which holds the latent-space representation of the original data. On the other hand, autoencoders also possess a few desired properties, making them an attractive candidate for anomaly detection. For example, an autoencoder is able to extract the salient features of the original data to remove dependency in the original data. More importantly, an autoencoder can only learn the properties or distributions of the data that it has seen during the training stage, that is, the data points in the training dataset. It excels at reconstructing data that are similar to the training data, but performs poorly on data that are very different from the training data, in terms of the reconstruction error L(x, x′).\
This is an appealing property of autoencoders in the application of anomaly detection. For example, in the context of detecting compromised IoT devices, we can establish the normal behavioral model of an IoT device using an autoencoder by training it with benign network traffic before the device has been compromised. We can continue monitoring the IoT device by passing the corresponding network traffic of the device into the trained model. If the reconstruction error is no greater than a pre-specified threshold, we consider the corresponding network traffic to be benign. When the reconstruction error is greater than the threshold, we claim that the network traffic is anomalous.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>How to Build a Modular Selenium + Cucumber Framework in Java</title><link>https://hackernoon.com/how-to-build-a-modular-selenium-cucumber-framework-in-java?source=rss</link><author>Rama Mallika Kadali</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:21:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Learn how to build a scalable and modular test automation framework using Selenium, Cucumber, and Java. This guide covers folder structure, CI/CD integration, Page Object Model, Allure reporting, and best practices for long-term maintainability and team collaboration.]]></content:encoded></item><item><title>How CUMAD Accumulates Evidence to Unmask Compromised IoT Devices</title><link>https://hackernoon.com/how-cumad-accumulates-evidence-to-unmask-compromised-iot-devices?source=rss</link><author>Hypothesis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:15:54 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).\
—IoT devices fundamentally lack built-in security mechanisms to protect themselves from security attacks. Existing works on improving IoT security mostly focus on detecting anomalous behaviors of IoT devices. However, these existing anomaly detection schemes may trigger an overwhelmingly large number of false alerts, rendering them unusable in detecting compromised IoT devices. In this paper we develop an effective and efficient framework, named CUMAD, to detect compromised IoT devices. Instead of directly relying on individual anomalous events, CUMAD aims to accumulate sufficient evidence in detecting compromised IoT devices, by integrating an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem. CUMAD can effectively reduce the number of false alerts in detecting compromised IoT devices, and moreover, it can detect compromised IoT devices quickly. Our evaluation studies based on the public-domain N-BaIoT dataset show that CUMAD can on average reduce the false positive rate from about 3.57% using only the autoencoder-based anomaly detection scheme to about 0.5%; in addition, CUMAD can detect compromised IoT devices quickly, with less than 5 observations on average.In recent years Internet of Things (IoT) devices have been increasingly integrated into our daily lives and our society, with notable example environments such as smart homes, healthcare, transportation, and power grid. On one hand, this rapid development helps to improve the quality and efficiency of our daily lives. On the other hand, this same development also poses potentially unprecedented security and privacy challenges on the Internet, given that most of these IoT devices are low-cost systems with limited computation, memory, and energy resources. These devices often lack proper built-in security mechanisms to protect themselves and are vulnerable to various security attacks.\
Many security attacks targeting or based on IoT devices have been reported in the past [1]. In response to the growing problems of IoT security, government agencies such as US NIST have developed many recommendations that manufacturers should adopt to mitigate the security risks associated with IoT devices [2]. In addition, many research efforts have been carried out to improve IoT security, including both proactive approaches to enhancing security mechanisms of IoT devices and more reactive solutions to monitor IoT device behaviors to detect rogue or infected IoT devices [3].\
Although some of the recommendations, for example, avoiding default common credentials, are relatively easy to be incorporated into IoT device manufacturing and certainly help mitigate IoT security risks, IoT devices are still fundamentally vulnerable to security attacks. As low-cost systems, IoT devices are inherently constrained in resources to support advanced security mechanisms. In addition, from the perspectives of both manufacturers and users, there are often conflicting objectives of IoT device usability and security, which often discourage the adoption of advanced security mechanisms in IoT devices.\
Given these constraints of deploying advanced security mechanisms on IoT devices, network-based solutions have attracted a great amount of research efforts in recent years [3]. In particular, many machine learning (ML) based methods have been developed in detecting anomalous network behaviors of IoT devices [3]. (In this paper we use the term ML to refer to both traditional machine learning algorithms such as SVM and deep learning (DL) algorithms such as RNN.) However, most existing solutions only targeted the problem of anomaly detection in IoT devices [4], instead of detecting compromised IoT devices. Although detecting individual anomalies is of critical importance in certain application domains [5], we note that these solutions may not be directly translated into the detection of compromised IoT devices. Given the large amount of network traffic, even a small false positive rate of an anomaly detection method can often translate into a large number of false alerts, rendering the detection method unusable in detecting compromised IoT devices in the real-world deployment.\
In this paper we develop an effective and efficient framework to detect compromised IoT devices, named CUMAD (cumulative anomaly detection). In essence, CUMAD integrates an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem [6], [7]. In CUMAD, the normal behavior of each IoT device is learnt and modeled by an autoencoder. During the training of an autoencoder model, it learns a latent space representation of the training data. More importantly, due to the nature of autoencoder, it excels at reconstructing inputs that are similar to the data used in training the model, but performs poorly when the new data is very different from the training data, manifested as large reconstruction errors. Although autoencoder has been mainly used in dimensionality reduction and feature learning in the past, in recent years it has also attracted a great amount of interests in anomaly detection in many different application domains.\
Instead of focusing on individual anomalous events detected by autoencoder, CUMAD aims to accumulate sufficient evidence to detect if an IoT device has been compromised. In CUMAD, the output of the autoencoder-based anomaly detection subsystem is fed into an SPRT-based sequential hypothesis testing subsystem. Unlike traditional probability ratio test methods that require a pre-defined fixed number of observations to reach a decision, SPRT works in an online manner and updates as observations arrive sequentially. SPRT reaches a conclusion whenever sufficient evidence has been observed. Therefore, SPRT can make a decision quickly (and consequently, CUMAD can detect compromised IoT devices quickly).\
In this paper we develop the CUMAD framework, and we also evaluate the performance of CUMAD using a public-domain IoT dataset N-BaIoT [8], which contains both benign and (Mirai and Bashlite) attack traffic of IoT devices. Our evaluation studies show that CUMAD can greatly improve the performance in detecting IoT devices in terms of false positive rates, for example, compared to the simple autoencoder-based anomaly detection system, CUMAD on average reduces the false positive rate from about 3.57% to 0.5%, representing about 7 times performance improvement in terms of false positive rate of the systems. In addition, CUMAD can detect a compromised IoT device quickly, with less than 5 sequential observations on average. We note that although both autoencoder and SPRT have been proposed in developing anomaly detection systems before, to our knowledge, we are the first to integrate the two techniques to detect compromised IoT devices, instead of being used separately for anomaly detection. In addition, we are the first to introduce the notion of cumulative anomaly in detecting compromised IoT devices (see Section 2 for more details).\
The remainder of the paper is organized as follows. In Section 2 we discuss related work. We present the background on autoencoder and SPRT in Section 3. We describe the design of CUMAD in Section 4, and evaluate its performance in Section 5. We conclude the paper in Section 6.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Future Wireless Comms Could Process Data in Midair</title><link>https://spectrum.ieee.org/wireless-communication-over-air-processing</link><author>Michelle Hampson</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExNTkwNS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2MzYyMzg1M30.XXiRdtScBG-Ox39QOpbhpoAjmKTB9e0NHXiQJBHOaQc/image.jpg?width=600" length="" type=""/><pubDate>Tue, 1 Jul 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[AirComp approaches do more than transmit data from point A to point B]]></content:encoded></item><item><title>Automakers Clash With India Over &apos;Aggressive&apos; Emission Limits</title><link>https://hardware.slashdot.org/story/25/07/01/081225/automakers-clash-with-india-over-aggressive-emission-limits?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[India's automakers are opposing the government's proposal to cut car emissions by 33% from 2027, calling the target "too aggressive" in a formal submission to the power ministry. 

The Society of Indian Automobile Manufacturers warned the plan risks billions of rupees in penalties and threatens future investments in the $137-billion auto sector. The proposal represents more than twice the pace of India's previous emission reduction target and forms part of the third phase of Corporate Average Fuel Efficiency norms first introduced in 2017. The industry body wants a more gradual 15% reduction target and opposes different standards for small versus heavy vehicles.]]></content:encoded></item><item><title>Mo Jomaa breaks down IPO prep for founders on the Scale Stage at TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/mo-jomaa-breaks-down-ipo-prep-for-founders-on-the-scale-stage-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Mo Jomaa of CapitalG, will lead a session, “What to Think About Now If You Want to IPO Someday” at TechCrunch All Stage on July 15 in Boston. Register now.]]></content:encoded></item><item><title>Performance &amp; Power Of The Low-Cost EPYC 4005 &quot;Grado&quot; vs. Original EPYC 7601 Zen 1 Flagship CPU</title><link>https://www.phoronix.com/review/amd-epyc-4005-server</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 14:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those on very long server upgrade cycles, typically just running the hardware until failure or consider buying second-hand servers that are generations old for lower up-front cost, today's unique article is for you with quantifying a first-generation EPYC server compared to today's entry-level EPYC processors in performance and power efficiency. With the fascinating AMD EPYC 4005 "Grado" budget-friendly server processors I was curious how well they would stack up against AMD's original flagship EPYC processor, the AMD EPYC 7601 "Naples" processor from the Zen 1 era. Can an entry-level brand new Grado server processor with dual channel DDR5 memory outpace an original EPYC server with twice the core/thread counts and eight channel DDR4 server memory? Yes, with huge gains in performance and power efficiency.]]></content:encoded></item><item><title>Building the Unbreakable Contract: A Pipeline for AI-Powered Vulnerability Classification and Repair</title><link>https://hackernoon.com/building-the-unbreakable-contract-a-pipeline-for-ai-powered-vulnerability-classification-and-repair?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:58:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
To achieve high-quality results in training our framework utilizing a RandomForestClassifier and LLMs for classification and repair (Fig. 1), several essential features must be incorporated.\
A source code column (“contract source”) is necessary to run Slither and the LLMs. However, since the datasets consistently excluded source code, a web scraping algorithm that employed the “contract address” column would be necessary to obtain source code from Etherscan and generation (see subsection D.). In order to account for source code that could not be scraped through Etherscan, the dataset (200,000 contracts) was reduced to 2500 rows.\
Slither was then run on the newly acquired source code (see subsection B.), adding columns “vulnerability”, “confidence”, and “impact”. Slither occasionally failed to provide any vulnerabilities, totalling 474 failed contracts (80% successful output rate). To account for this, the dataset was reduced again to 2,000 smart contracts. Of the dataset, 400 were labeled malicious, and 1,600 were labeled non-malicious. Table I visualizes a segment of the finalized dataset.\
Slither is a static code analyzer, which checks the smart contracts for vulnerabilities without executing the contract. Slither’s initial input comes from the Solidity Abstract Syntax Tree (AST) generated by the Solidity compiler from the contract source code. The smart contract is then simplified into an intermediate representation called SlithIR. This intermediate representation is compared to current industry standards, and Slither outputs vulnerabilities. Slither leads the industry in smart contract vulnerability detection, outperforming other static code analyzers in almost every metric, as shown in Table II. This, coupled with our Random Forest Classifier, ensures high accuracy in detecting vulnerable smart contracts.\
After importing and running all 89 basic detectors provided by the API, we added each contract’s vulnerabilities to the dataset as a list of Slither’s natural language names with empty lists denoting contracts Slither deemed safe.\
C. Data Issues and Generation\
When it came to data collection, specific issues were encountered. Our biggest issue, extracting source code, proved to be a challenging task. For instance, in a dataset that bytecode was given, we were unsuccessful in decompiling that code into analyzable source code as we were unaware of the decompiler’s limits. We also struggled to find additional malicious source code to train a model on, as our dataset only included 150 malicious contracts. To overcome this, we implemented OpenAI’s GPT 3.5 Turbo to generate malicious source code. Initial attempts were barred by GPT 3.5’s ethical limitations (Fig. 2). However, after jailbreaking GPT 3.5 with prompt engineering [18], GPT 3.5 would produce malicious source code that could be repaired by the model.\
The variability of the dataset made it difficult to generate Slither vulnerabilities for smart contracts, so a BLANK-step approach was used. The primary issue was the 100+ versions all contracts were written in combined with the limited backward compatibility of Solidity — i.e., version 0.4.11 could run on a compiler of version 0.4.26 but not a compiler of version 0.5.0+. Addressing this required modifying each contract to read ”pragma solidity ≥{version}”, creating five different scripts, and running each script on the entire dataset with one of five following Solidity versions: 0.4.26, 0.5.17,\
0.6.12, 0.7.6, or 0.8.21, with Slither vulnerabilities of scripts that could not be compiled recorded as null, and those that could be recorded with the English name of the vulnerability, obtained from parsing the returned json. Combining these lists resulted in the final list of Slither vulnerabilities for the 75% of smart contracts for which this method yielded results.\
Each detector class includes the detector’s confidence and impact levels. After creating a key-value pair of each detector’s English name and their confidence plus impact, this list was used to create confidence and impact lists for all vulnerabilities for each smart contract.\
Various models were implemented to classify smart contract maliciousness. Ultimately, RandomForestClassifier (RFC) provided the highest accuracy after pre-processing the finalized dataset.\
RFC is unable to train on the dataset as provided by webscraping, generation, and Slither processing due to the abundance of unnecessary string-based features. So, unnecessary features are dropped, and necessary features are processed for RFC. For example, “confidence” and “vulnerability” retain a weaker correlation to “malicious” in comparison to “impact”, so to avoid convoluting the model, both are dropped. Thus, “contract source” and “impact” remain as the classifying features and “malicious” as the target label.\
As all columns are still either string or boolean data types, RFC is still unable to train on the dataset. “contract source” was tokenized using the CountVectorizer (CV) tool from the sci-kit-learn library. “malicious” and “impact” were encoded into usable numeric values by mapping dictionaries. Since “impact” contained more than two possible outputs, unlike “malicious”, the outputs of “impact” were scaled from 0-4.\
After the tokenized and encoded columns are concatenated, RFC’s numeric prerequisite is fulfilled.\
The data is then split into a train-test split of 0.6-0.4 and randomized before RFC fits to the train set and predicts on the test set. Accuracy and confusion are evaluated in .\
E. Large Language Models (LLMs)\
: We incorporated multiple Large Language Models to repair the smart contracts after they had been identified as malicious with our two-layered frameworks. The best results came from the Llama-2-7B model, which can be found on Hugging Face. This model finished training in July 2023. Our finetuning process took place about three weeks later. The Llama-2-7B model has become very popular due to its low number of parameters and reliability, leading to a less memory-intensive alternative to other LLMs in the industry.\
The finetuning process took place on Google Colab using the T4 chip, which carries 16 GB of VRAM. However, Llama2-7B’s weights themselves fill this limit (7b * 2 bytes = 14). This also does not include any weights, optimizers, or gradients. Thus to run Llama-2-7B and be able to run it without memory restrictions on a platform like Google Colab, we will use parameter-efficient-finetuning (PEFT). Specifically, we will use QLoRa (Efficient Finetuning of Quantized LLMs), using 4-bit precision instead of the normal 16-bit precision. This quantization process allows for finetuning on Colab while also ensuring that the precision of the model is adequate. This is because when saving the 4-bit model, we also save the QLoRa adapters, which can be used with the model.\
Moreover, Llama-2-7B is open source meaning the model is available to be downloaded and used locally. Traditional data privacy concerns with LLMs are therefore nullified because all data is processed on the local machine, not in a 3rd party server. This bodes well for smart contracts as many execute agreements with sensitive information and large sums of money. Llama-2-7B provides the benefits and accuracy of an advanced LLM while also providing the security and versatility neccesary for blockchain technology.\
The Llama-2-7B model was fine-tuned on fifty smart contracts that were once malicious and then repaired, using a supervised learning approach. These smart contracts were collected in the data collection mentioned above. Specifically, the source code was tokenized and embedded, using the quantization outlined previously. The model was trained over 100 steps, with training loss consistently decreasing with every step(as shown in figure 3).\
The supervised fine-tuning process allowed the model to understand the relationships between malicious source code and the same source code that had been repaired to emulate that with any other contract.\
: We also utilized OpenAI’s API to use GPT-3.5-Turbo to repair vulnerabilities. OpenAI is one of the most well known names in the industry with applications such as DALL -E and ChatGPT. Specifically, while all GPT models are optimized to generate code, GPT-3.5-Turbo is the best combination of performance and efficiency. Moreover, by utilizing a ”chat bot”, we were able to use prompt engineering to create a prompt with the best possible performance. Directly querying GPT-3.5-Turbo to repair malicious code was unsuccessful. Similar to the generation of malicious smart contracts, GPT-3.5-Turbo had a reluctance to work with malicious source code (Fig. 4).\
Thus prompt engineering was utilized to circumvent this problem.\
First, the use of the word ”malicious” needed to be removed. While we were looking for our LLM to repair malicious smart contracts, GPT-3.5 Turbo was instead asked to help us “fix vulnerable smart contracts”.\
We then used Chain of Thought Techniques in order for the model to elaborate on what changes it made and why. This led to a more accurate source code output and more vulnerabilities repaired. Additionally, this provided more information for the\
user as the specific vulnerabilities in the malicious smart contract were highlighted and explained.\
Ultimately, our prompt(Fig. 5) used Slither’s source code and vulnerabilities to prompt GPT 3.5 Turbo to repair the smart contracts. While Slither also outputs impact level and confidence on those vulnerabilities, we found incorporating these into the prompt hurt the model’s ability to output repaired source code or even source code that could be compiled. Essentially, using other Slither outputs led to overfitting. This prompt was also used with the Llama-2-7B model outlined above in order to create uniformity across outputs. In both models, the prompt allowed for the generation of repaired source code while also generating details that explained any changes and provided explanation.\
In conclusion, we ended with two primary models to repair source code. First, the Llama-2-7B, which had been finetuned specifically for repairing smart contracts. Second was the utilization of GPT-3.5-Turbo which learned to repair smart contracts through CoT prompt engineering.(1) Abhinav Jain, Westborough High School, Westborough, MA and contributed equally to this work (jain3abhinav@gmail.com);(2) Ehan Masud, Sunset High School, Portland, OR and contributed equally to this work (ehanmasud2006@gmail.com);(3) Michelle Han, Granite Bay High School, Granite Bay, CA (michellehan2007agt@gmail.com);(4) Rohan Dhillon, Lakeside School, Seattle, WA (rohand25@lakesideschool.org);(5) Sumukh Rao, Bellarmine College Preparatory, San Jose, CA (sumukhsf@gmail.com);(6) Arya Joshi, Robbinsville High School, Robbinsville, NJ (arya.joshi@gmail.com);(7) Salar Cheema, University of Illinois, Champaign, IL (salarwc2@illinois.edu);(8) Saurav Kumar, University of Illinois, Champaign, IL (sauravk4@illinois.edu).:::info
This paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.]]></content:encoded></item><item><title>Will VR Integration for Remote Work Become Permanent?</title><link>https://hackernoon.com/will-vr-integration-for-remote-work-become-permanent?source=rss</link><author>Allan Grain</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:51:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If there’s been any seismic shift in the workplace over the last few decades, it’s manifested in the way remote work has become a cornerstone of modern business operations.Many organizations today have teams distributed over several geographical areas and bridging the gap is a challenge. Phone calls, emails and even Zoom calls simply are not sufficient to replicate the in-office experience.The question now is whether technology will advance to the point that virtual reality (VR) solutions can revolutionize remote collaboration and allow companies to operate multiple locations in a VR environment.The best part of VR is that it provides a three-dimensional immersive space where employees can interact as if they were physically present. The addition of AI would probably assist in making these virtual environments even more realistic. This type of innovation would certainly address the limitations of traditional video conferencing as seen with Zoom or Microsoft Teams for instance. These technologies might be good for certain uses, but they lack the depth and engagement of face-to-face interaction.One of the fun aspects of VR is the ability to create virtual offices – digital workspaces in which employees, represented by lifelike avatars, can collaborate in real time. AI algorithms power these avatars, enabling them to mimic natural human behaviors, such as gestures, facial expressions, and even tone of voice.A terrific emerging technology that has existed for a while now but is reaching perfection today is real-time language translation. Language barriers have long been a challenge for global teams. AI-driven translation tools, such as those powered by natural language processing (NLP), are now being integrated into VR platforms. These tools are now able to translate speech in real time, displaying subtitles or dubbing voices in a user's native language. For instance, a team member in Japan can speak in Japanese, while a colleague in Brazil hears the conversation in Portuguese, all within a shared VR environment. This capability allows employees from various geographical regions to collaborate on projects even if they do not speak the same language. This is groundbreaking.The virtual environment can also be altered to accommodate one-on-one meetings or large boardroom meetings. Users can even alter the room to accommodate client presentations.AI is a VR environment is useful for scheduling, note taking, or action-item tracking. Transcription tools are easily integrated into a VR environment so that conversations and discussion can be transcribed, with key points summarized and follow-up tasks assigned in real-time. AI and VR working seamlessly together can function as a high-speed virtual assistant capable of performing multiple tasks at once with high accuracy and minimal input.Several companies are already pioneering AI-VR integration for remote work. Meta's Horizon Workrooms, for example, allows teams to collaborate in virtual meeting rooms with spatial audio and interactive whiteboards.Similarly, Spatial, a VR collaboration platform, uses AI to create cross-platform compatibility, allowing users on different devices—VR headsets, laptops, or smartphones—to work together seamlessly.Firms like Accenture have adopted AI-VR solutions to host global meetings and training sessions. Accenture's "Nth Floor" virtual campus enables employees to attend workshops, network with colleagues, and even explore virtual replicas of physical offices.Beyond corporate settings, AI combined with VR is making waves in education and healthcare. Universities now use virtual classrooms to connect students across continents. In healthcare, VR simulations allow medical professionals to practice surgeries or collaborate on patient care plans remotely, improving access to expertise.Unfortunately, despite its potential, AI-VR integration for remote work still faces several major hurdles before it can be widely adopted. Cost and accessibility remain significant barriers, as high-quality VR headsets and AI-powered software are highly expensive. VR devices like the Oculus Quest 3 or Apple Vision Pro might become more affordable, but their price prevents widespread adoption.While challenges remain, ongoing innovations will pave the way for a future where virtual offices imitate their physical counterparts sufficiently enough that companies will integrate them permanently as part of the workday.Tomorrow’s workplace promises to be more immersive and perhaps even more fun. ]]></content:encoded></item><item><title>A Novel Pipeline for Classifying and Repairing Smart Contracts at Scale</title><link>https://hackernoon.com/a-novel-pipeline-for-classifying-and-repairing-smart-contracts-at-scale?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:47:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Abhinav Jain, Westborough High School, Westborough, MA and contributed equally to this work (jain3abhinav@gmail.com);(2) Ehan Masud, Sunset High School, Portland, OR and contributed equally to this work (ehanmasud2006@gmail.com);(3) Michelle Han, Granite Bay High School, Granite Bay, CA (michellehan2007agt@gmail.com);(4) Rohan Dhillon, Lakeside School, Seattle, WA (rohand25@lakesideschool.org);(5) Sumukh Rao, Bellarmine College Preparatory, San Jose, CA (sumukhsf@gmail.com);(6) Arya Joshi, Robbinsville High School, Robbinsville, NJ (arya.joshi@gmail.com);(7) Salar Cheema, University of Illinois, Champaign, IL (salarwc2@illinois.edu);(8) Saurav Kumar, University of Illinois, Champaign, IL (sauravk4@illinois.edu).\
—Due to the modern relevance of blockchain technology, smart contracts present both substantial risks and benefits. Vulnerabilities within them can trigger a cascade of consequences, resulting in significant losses. Many current papers primarily focus on classifying smart contracts for malicious intent, often relying on limited contract characteristics, such as bytecode or opcode. This paper proposes a novel, two-layered framework: 1) classifying and 2) directly repairing malicious contracts. Slither’s vulnerability report is combined with source code and passed through a pre-trained RandomForestClassifier (RFC) and Large Language Models (LLMs), classifying and repairing each suggested vulnerability. Experiments demonstrate the effectiveness of fine-tuned and prompt-engineered LLMs. The smart contract repair models, built from pre-trained GPT-3.5-Turbo and finetuned Llama-2-7B models, reduced the overall vulnerability count by 97.5% and 96.7% respectively. A manual inspection of repaired contracts shows that all retain functionality, indicating that the proposed method is appropriate for automatic batch classification and repair of vulnerabilities in smart contracts.As we delve into the crucial role smart contracts play in the global blockchain, it becomes increasingly imperative that we understand the severity of cyberattacks that exploit weak code. 2018 saw $23.5 million worth of cryptocurrencies stolen from the Bancor network due to the compromise of a wallet used to upgrade smart contracts, sparking controversy online over the safety of decentralized exchange and smart contract systems [16]. More recently, in 2020, a hacker drained Harvest Finance of $24 million by implementing a smart contract that manipulated the share values of the vaults [17]. The common theme across these hacks is that vulnerabilities within smart contracts were exploited to steal millions of dollars, highlighting the importance of strengthening smart contracts to prevent vulnerabilities from arising.\
Smart contracts provide a secure platform for transactions without the need for a trusted intermediary. For this reason, they have become increasingly common in blockchain applications. But because most blockchain applications prevent users from editing smart contracts after they have been deployed, there is a need for analysis tools that can accurately and precisely determine the vulnerabilities of smart contracts. Although most tools rely on expert-developed frameworks, recent research has begun developing deep learning models that can evaluate a smart contract’s vulnerability. However, most existing deep learning models fail to provide helpful feedback on a smart contract’s vulnerabilities — instead, they determine whether or not a smart contract is vulnerable.\
DLVA [1] introduces a three-step approach involving mapping bytecode to high-dimensional vectors, classifying vectors based on training data, and using neural networks to infer vulnerable contracts. However, a significant weakness in this approach was the high false positive rate during the prediction process. Similarly, MRN-GCN [5] utilizes deep learning with a nest contract graph capturing syntactic and semantic information, enabling the classification of vulnerable functions, but like [1], retained mixed recall percentages ranging from 98.18% to 79.59%. The authors of [3] take a different approach by proposing peer-to-peer voting and reward-and-slash mechanisms to mitigate and discourage malicious behavior in smart contracts.\
Large Language Models (LLMs) models prove to be exceptional in performing complex tasks. The authors of [8] demonstrated the capabilities of various LLMs in identifying vulnerabilities in DeFi smart contracts with F1-scores significantly higher than random baselines, which has the potential arXiv:2309.07841v1 [cs.CR] 14 Sep 2023 to be improved by the tool enhancement framework developed in [4]. Prompt engineering allows LLMs to be substantially enhanced. One powerful LLM prompt engineering method involves Chain of Thought (CoT) prompting [2] that significantly improves the ability of LLMs to perform complex reasoning. In eight CoT exemplars, [2] achieves an accuracy of 56.9 on PaLM-540B in the GSM8K benchmark, demonstrating an accuracy improvement of 39. However, the paper chooses to rely solely on CoT, neglecting fine-tuning entirely. In a similar implementation, the authors of [7] present a framework that improves upon CoT by transferring advanced reasoning abilities from large models to smaller ones through knowledge distillation, resulting in improved question-answering performance. In another scenario, [6] utilized prompt engineering by giving ChatGPT specific information, such as the translation’s purpose and target audience, leading to industry standard translation quality.\
A comprehensive survey [11] described the current landscape of smart contract security, identifying eight core defense methods across 133 models. This finding underscores the complexity of the field but also reveals limitations. One limitation is seen in applying automated smart contract tools to DeFi systems [12]. Surprisingly, these tools only detected 8% of attacks, indicating a challenge with intricate vulnerabilities. Addressing this, [13] evaluated five smart contract detection tools, focusing on three types of vulnerabilities. [13]’s analysis determined that different detection models have varying strengths and weaknesses, suggesting a combination of methods may be more effective. Furthermore, this notion is corroborated by [9] and [10], which both utilize Multi-Task Learning, a combination method that leverages concurrent learning and optimization of multiple tasks. Notably, [14] advances this methodology by using an approach that blends K-means clustering and LSTM networks with a universal sentence encoder. This approach understood the smart contract code’s semantic meaning, outperforming baseline models.\
Moreover, current work regarding repairing smart contracts has been shown to be reliable. For example, [19] utilizes a framework called ContractFix to repair vulnerabilites with 94% accuracy. ContractFix was based around static code analyzers and focused on repairing broken patches. Similarly, [15] utilizes a tool, Elysium, to repair patches in bytecode for seven vulnerabilities. However, this paper improves on these frameworks in two main ways. First, our framework is built on LLMs which allow for a more robust repairing process, that is adaptable to zero-day vulnerabilities. Secondly, we work directly with source code, which is a novel approach to repair vulnerabilities.\
These existing methods have been shown to work well in vulnerability detection across various situations with relatively little statistical error. However, we show that existing vulnerability detection methods face the following problems: 1) lack of a broad approach, 2) little detail on specific errors, 3) high false positive evaluations, and 4) lack of a direct repair framework. To address all these problems, we propose a novel pipeline. The pipeline first utilizes Slither and a RandomForestClassifier to detect and provide specific vulnerabilities within smart contract source code. After filtering out non-malicious contracts, two LLMs, GPT-3.5-Turbo and a fine-tuned Llama-2-7b generation model, each repair the vulnerable smart contract source code. The repaired contract is then evaluated by Slither against its vulnerable counterpart, assessing the effectiveness of the repair.\
The rest of this paper is outlined as follows: Section II details our novel pipeline approach that utilizes two layers for vulnerability detection: Slither and RandomForestClassifier, to classify vulnerable smart contracts and two LLM models (Llama-2-7B and GPT-3.5-Turbo) to repair them. Section III exhibits the results of our approach in comparison to existing methods. Section IV provides a conclusion.:::info
This paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.]]></content:encoded></item><item><title>Why You Can&apos;t Miss SpoonOS&apos;s Developer Call S1 for AI Innovation</title><link>https://hackernoon.com/why-you-cant-miss-spoonoss-developer-call-s1-for-ai-innovation?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:41:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What if artificial intelligence could operate with the transparency and decentralized control of blockchain technology? SpoonOS, described as an infrastructure developer for the sentient economy, is initiating its first coordinated community effort, Developer Call S1, to address this question. The goal is to encourage developers to explore the core functionalities of SpoonOS, an agentic operating system designed for Web3. This initiative aims to validate the SpoonOS infrastructure and identify individuals who will contribute to the operating system's evolution. It represents a structured approach with defined objectives, a timeline, and incentives for participation, along with comprehensive documentation.\
For those unfamiliar with the terminology, Web3 refers to the next iteration of the internet, emphasizing decentralization, blockchain technologies, and token-based economics. Unlike the current internet, where large corporations often control data and platforms, Web3 aims to give users more control over their data and digital interactions. Within this context, AI agents are autonomous entities that perceive their environment, act to achieve specific goals, and can improve their performance through learning. SpoonOS positions itself at the intersection of these two transformative fields, providing a framework for developers to create these intelligent, on-chain agents.Developer Call S1 is specifically designed for Web3 developers engaged in building with AI agents. The program provides participants with resources to understand the SpoonOS technology stack. This includes examining the SpoonOS codebase on GitHub, which offers clear instructions for environment setup, building a first AI agent, mastering command line interface (CLI) tools, and integrating with built-in Model Context Protocol (MCP) servers. The MCP serves as an interface connecting SpoonOS to various data sources, both decentralized and traditional.\
Beyond the GitHub resources, developers gain access to a "Spoon Cookbook," which contains end-to-end tutorials. These tutorials cover topics such as API key configuration, setting up agent memory, and constructing reproducible AI agents. Additionally, the "Spoon Toolkit" details third-party integrations, including on-chain data feeds and decentralized storage. On-chain data feeds provide real-time information directly from a blockchain, enabling AI agents to react to events as they happen on the network. Decentralized storage, in contrast to centralized cloud storage, distributes data across a network of computers, enhancing security and resilience by removing a single point of failure. SpoonOS operates on NEO's Layer-1 blockchain, a foundational blockchain that provides the underlying infrastructure for these scalable Web3 applications. This integration is presented as a means for developers to build, deploy, and manage AI agents using intuitive tools and interfaces.Cultivating an Agent Ecosystem: Beyond DevCall S1\
Developer Call S1 is a foundational element of SpoonOS's broader strategy to expand its AI agent ecosystem. Upcoming initiatives include a global hackathon and collaborative learning programs. The intent behind these efforts is to empower developers to create versatile AI agents capable of leveraging the full capabilities of the SpoonOS platform. The vision is to lay the groundwork for the next generation of AI agents, which are expected to play a significant role in the emerging "sentient economy."\
The "sentient economy" refers to a conceptual ecosystem where intelligent agents can autonomously interact, learn, and contribute to value creation within Web3 networks. This implies a future where AI agents are not merely tools, but active participants in economic processes, making decisions and executing transactions independently based on real-time data and learned behaviors. SpoonOS aims to provide the robust and integrated environment necessary for such an economy to flourish.\
The launch of SpoonOS's Developer Call S1 marks a significant step towards a more integrated future for AI and blockchain. The emphasis on providing comprehensive tools and structured learning pathways suggests a genuine commitment to developer adoption, which is crucial for any new platform's success. The concept of a "sentient economy" fueled by autonomous AI agents operating on a decentralized infrastructure is compelling. It suggests a shift from passive data consumption to active, intelligent participation within digital economies.\
However, the realization of such a vision depends heavily on the practical utility and scalability of these AI agents. The complexity of integrating AI models with the immutable and often resource-intensive nature of blockchain transactions presents challenges. The success of SpoonOS will hinge on how effectively its toolkit simplifies this complexity for developers, allowing them to build agents that are not only intelligent but also efficient and secure. The ability for these agents to interact seamlessly across various data sources, both on-chain and off-chain, will be a key differentiator. It remains to be seen how broadly the developer community will embrace this new paradigm, but the initial push with incentivized calls and comprehensive resources provides a strong starting point. The development of a truly "sentient economy" will require continued innovation and collaboration from a diverse set of builders, and SpoonOS's approach seems designed to foster that environment.\
Don’t forget to like and share the story!:::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Why Your Next Smart-Home Device Hunts Mosquitoes: Meet Bzigo Iris</title><link>https://hackernoon.com/why-your-next-smart-home-device-hunts-mosquitoes-meet-bzigo-iris?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:28:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Bzigo Iris is the world’s first AI-powered mosquito detector that uses infrared and laser tech to locate mosquitoes indoors without harmful chemicals. Founded in 2019, Bzigo offers a smart, health-conscious alternative to sprays and zappers—sending real-time alerts so you can finally sleep bite-free. Visit bzigo.com to learn more.

]]></content:encoded></item><item><title>Little Pepe (LILPEPE) Breezes Past 3rd Presale Stage in Record Time</title><link>https://hackernoon.com/little-pepe-lilpepe-breezes-past-3rd-presale-stage-in-record-time?source=rss</link><author>Kashvi Pandey</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:24:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Little Pepe (LILPEPE) just completed its third presale stage faster than expected, raising $2.57M in Stage 4. Backed by its own Layer 2 blockchain and a $777K giveaway, the token offers real utility beyond meme hype. With rising prices and strong demand, LILPEPE is quickly becoming 2025’s breakout meme coin.

]]></content:encoded></item><item><title>Stablecoins, Smart Contracts and The Rise of More Intelligent Cash</title><link>https://hackernoon.com/stablecoins-smart-contracts-and-the-rise-of-more-intelligent-cash?source=rss</link><author>Paul Quickenden</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:20:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Fintechs already have the talent, the ingenuity and after a decade of challenger success with innovations like Wise’s borderless accounts, Stripe’s one-click checkout and Revolut’s multi-currency wallets - the credibility to reshape finance on a global stage. Those breakthroughs gave consumers faster payments and slicker front-ends; but let’s call that Act I.\
Act II is unfolding now: money itself is becoming programmable, composable and borderless. Stablecoins can settle in seconds, smart contracts can execute “if-this-then-that” logic without humans in the loop and tokenised assets can move 24/7 across jurisdictions. The rails we lay today - whether they’re open, interoperable and secure or fragmented and proprietary - will determine not just how fast value moves tomorrow, but how fairly it’s distributed and who gets to participate in the next wave of financial innovation.\
The challenge to builders today is this: if your vision begins and ends with speeding up domestic payments or making bill-splitting cuter, you’re missing the bigger prize. Programmable money rewrites the playbook for remittances, trade, treasury and even machine-to-machine transactions. Are we building for that horizon - or solving tomorrow’s problems with yesterday’s stack?What’s the biggie with blockchain?Behind the scenes, blockchain infrastructure is becoming the default layer for secure, real-time, permissionless value transfer. J.P. Morgan’s Onyx platform, for instance, has settled more than US$1 trillion in tokenised intrabank payments on permissioned chains.\
If you’re designing for the next five years, on-chain rails offer instant settlement, embedded logic and global composability that legacy systems simply can’t match. No, it won’t replace every system but it is rapidly becoming the preferred foundation for anything that needs to be fast, transparent and interoperable. This isn’t about riding the crypto wave but about recognising a smarter way to move value - one that’s auditable, modular and increasingly composable.\
If you’re designing for the next five years, building  blockchain gives you access to speed, programmability and network effects that simply don’t exist in traditional architecture. We need to shift our thinking from ‘What is blockchain for?’ to ‘What’s already working, and how does it fit into my stack?’Could stablecoins be the new fuel behind an AI-driven economy?At Stripe’s 2025 Sessions conference, co-founder Patrick Collison didn’t mince his words: “There are not one, but two, gale-force tailwinds… reshaping the economic landscape around us: AI and stablecoins.” In a world increasingly run by AI’s, programmable stablecoins are the natural currency and could become the financial fuel behind an AI-driven economy.\
Stripe has just rolled out stablecoin-powered financial accounts to businesses in 101 countries which allow companies to hold, receive, and send stablecoins and, crucially, to hedge against inflation and plug directly into the global economy. All of this without needing a traditional bank account. And it's not just Stripe, Worldpay, Mastercard, X and even Apple are exploring how to turbocharge their business using Stablecoins.\
This isn’t theory; this is infrastructure, and entrepreneurs in high-inflation countries can now operate in stable, digital USD. What’s more, startups can pay contractors in stablecoins, skip wire transfer fees and move capital in minutes via lower-cost cross-border transactions.\
Stablecoins have quietly moved from the fringes to becoming the go-to method for paying global contractors (instantly, with zero FX fees), cross-border commerce (settling USD payments in mere seconds), accessing savings (even in inflation-prone regions) and plugging into on-chain treasury and lending protocols (without needing a bank account).\
So if you’re building a payments platform, now is the time to ask yourself: can my product evolve with how digital finance is  being used? If the answer is no - it might be worth reconsidering your architecture because what works here must scale there.That sounds like “money, but faster.” What else is cool?Think of smart contracts as ‘if this, then that’ logic for money - basically code that executes financial actions automatically when certain conditions are met. There’s no middleman, no lag and no manual work. This is where money stops just moving and starts \
Imagine if paying contractors happened the moment work is verified; if refunds were triggered instantly when goods didn’t arrive; if cross-border trade ran on rules baked into your code, not your paperwork.\
This is already happening globally - programmable finance means stablecoins that plug into automated workflows and APIs that react to real-world events. The question now moves on from can we move money faster? .. to can our money move smarter?What separates good from game-changing companies is vision. The winners in fintech will be the ones who design for what’s , not just what’s  right now.If you believe that money will be programmable… \n If you see that global commerce demands truly global rails… \n If you know that the rules of infrastructure are being rewritten in real-time right now…… then now’s the moment to double down. Build with stablecoins. Plug into on-chain rails. Create APIs that are blockchain-ready. It’s time to think like the internet: borderless, open and lightning fast.Let’s not just build for today’s money; let’s help define tomorrow’s.]]></content:encoded></item><item><title>US Senate removes controversial ‘AI moratorium’ from budget bill</title><link>https://techcrunch.com/2025/07/01/us-senate-removes-controversial-ai-moratorium-from-budget-bill/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:07:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After going back and forth over the provision, Sen. Marsha Blackburn offered an amendment to strip the provision alongside Sen. Maria Cantwell. ]]></content:encoded></item><item><title>US Government Takes Down Major North Korean &apos;Remote IT Workers&apos; Operation</title><link>https://yro.slashdot.org/story/25/06/30/2236218/us-government-takes-down-major-north-korean-remote-it-workers-operation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: The U.S. Department of Justice announced on Monday that it had taken several enforcement actions against North Korea's money-making operations, which rely on undercover remote IT workers inside American tech companies to raise funds for the regime's nuclear weapons program, as well as to steal data and cryptocurrency. As part of the DOJ's multi-state effort, the government announced the arrest and indictment of U.S. national Zhenxing "Danny" Wang, who allegedly ran a years-long fraud scheme from New Jersey to sneak remote North Korean IT workers inside U.S. tech companies. According to the indictment, the scheme generated more than $5 million in revenue for the North Korean regime. [...]
 
From 2021 until 2024, the co-conspirators allegedly impersonated more than 80 U.S. individuals to get remote jobs at more than 100 American companies, causing $3 million in damages due to legal fees, data breach remediation efforts, and more. The group is said to have run laptop farms inside the United States, which the North Korean IT workers could essentially use as proxies to hide their provenance, according to the DOJ. At times, they used hardware devices known as keyboard-video-mouse (KVM) switches, which allow one person to control multiple computers from a single keyboard and mouse. The group allegedly also ran shell companies inside the U.S. to make it seem like the North Korean IT workers were affiliated with legitimate local companies, and to receive money that would then be transferred abroad, the DOJ said.
 
The fraudulent scheme allegedly also involved the North Korean workers stealing sensitive data, such as source code, from the companies they were working for, such as from an unnamed California-based defense contractor "that develops artificial intelligence-powered equipment and technologies."]]></content:encoded></item><item><title>Genesis AI launches with $105M seed funding from Eclipse, Khosla to build AI models for robots</title><link>https://techcrunch.com/2025/07/01/genesis-ai-launches-with-105m-seed-funding-from-eclipse-khosla-to-build-ai-models-for-robots/</link><author>Marina Temkin</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:56:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Genesis AI, which aims to build a foundational model for powering all kinds of robots, has emerged from stealth with $105M in seed funding.]]></content:encoded></item><item><title>Canonical Decides To Double Down On Their Investment In Java For Ubuntu</title><link>https://www.phoronix.com/news/Canonical-Double-Down-Java</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:52:17 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Ubuntu maker Canonical has decided to "double down" their investment in OpenJDK Java for Ubuntu Linux...]]></content:encoded></item><item><title>xAI raises $10B in debt and equity</title><link>https://techcrunch.com/2025/07/01/xai-raises-10b-in-debt-and-equity/</link><author>Ram Iyer</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:35:54 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Elon Musk's AI company, xAI, has raised $5 billion in debt and $5 billion in equity, Morgan Stanley said on Monday. ]]></content:encoded></item><item><title>Elon Musk’s Starlink Adds $750 Congestion Charge</title><link>https://www.techdirt.com/2025/07/01/elon-musks-starlink-adds-750-congestion-charge/</link><author>Karl Bode</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:27:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Low-Earth Orbit satellite broadband services like Starlink have their uses, but will always be dealing with capacity constraints. That means higher prices, weird restrictions, and, as of November 2024, a $100 “congestion charge” for a service that’s already too expensive for many of the rural Americans who could most benefit.It didn’t take long for that “congestion charge” to soar to $500 in some areas. Now it’s already risen as high as $750 in states like Washington as Starlink is forced to try and deter users in some markets from using the increasingly congested network:“The change can crank up the starting price simply to own the Starlink dish on a residential plan to $1,099.”Other parts of the country see no congestion charge, but there’s no guarantee that they won’t see one down the line as the network subscribership grows. It’s also very likely the company will increasingly have to resort to doing things like throttling higher definition videos, or engaging in other network management tricks to try and keep the service semi-reliable.You might recall that Republicans and Elon Musk threw a hissy fit a few years ago when the Biden FCC prioritized “future-proof” fiber and higher-capacity 5G services over Starlink in previous government subsidy programs, (correctly) expressing concerns that the service lacked the capacity to provide consistently reliable speeds on the taxpayer dime. These are all things Republican Elon Musk ass kissers either don’t know, or don’t care about as they work to reward their billionaire benefactor. It will be up to their constituents to figure it out later. But money redirected to Starlink is money redirected to cheaper and better broadband alternatives, including super cheap gigabit fiber access and community-owned and operated broadband networks. So again, Starlink is a nice step up if you’re in the middle of nowhere, lack any other connectivity options, can afford it, and don’t care about its potential environmental impact. But it shouldn’t be taking priority in terms of taxpayer subsidies. Unless, of course, you only care about kissing Elon Musk’s ass and don’t  about the constituents you claim to serve.]]></content:encoded></item><item><title>Why Extracting Text From PDFs Still Feels Like a Hack–And the Legacy Design that Keeps AI Stuck</title><link>https://hackernoon.com/why-extracting-text-from-pdfs-still-feels-like-a-hack-and-the-legacy-design-that-keeps-ai-stuck?source=rss</link><author>Paolo Perrone</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:00:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Devs working with LLMs run into document parsing constantly. And every few months, there’s a new wave of hype (or frustration) around the PDF problem. During those moments, it’s not unusual to see software folks venting about how one file format became such a massive headache. But the struggle isn’t new.\
Long before LLMs entered the picture, entire SaaS businesses were built around managing the messiness of PDFs. And for good reason, it’s a format that was never designed for the kind of structured, machine-readable access we now expect.\
When software becomes as widespread as Adobe Acrobat and the PDF format, it starts to feel like a permanent part of the landscape. It’s easy to forget that behind that ubiquity were real design decisions, constraints, and tradeoffs made by real engineers solving real problems. Problems that, over time, evolved and became the roots of today’s pain.\
Yes, PDFs are frustrating. But they weren’t born broken. In fact, they were a surprisingly elegant solution for their time.\
So, let’s zoom out. This story takes a step back to explore the origins of the PDF format: how it came to be, what problems it set out to solve, and how the decisions made in the early 90s still ripple through today’s stack. The goal: to understand not just the “why is this so hard?”, but also the “how did we get here?”Back to the ‘80s, from paper to pixels.The shift had begun. Personal computers were exploding in popularity, and paper documents were no longer the default. Software like VisiCalc, WordStar, WordPerfect, and early Microsoft Word marked the dawn of a new way to write, edit, and share.By the late ’80s, PC suites had all but killed off the typewriter. Executives could tweak reports minutes before a meeting. Analysts were running “what-if” scenarios in spreadsheets. Teachers were printing tests on the fly. Engineers replaced drafting tables with digital blueprints.\
Increasingly, documents became the new workplace. Not just the end product, but where the work actually happened.The ‘90s and the birth of the PDF.In the early 1990s, the rise of PC-based word processing and electronic file sharing solved many problems, while introducing new ones. Every computer had its own fonts, printer drivers, and layout quirks. A report that looked perfect on one machine could print as a jumbled mess on another. Sharing files became a gamble.\
To fix this, in 1991 Adobe co-founder John Warnock and his team launched a project codenamed “Camelot” to create a truly universal document format. The result was the PDF, a file that embedded fonts, graphics, and page layout all in one place. This “digital paper” guaranteed that documents looked exactly the same everywhere, whether on Windows, Mac, or any printer.\
By bundling every font, image, and layout detail into a single file, PDFs let users share documents without surprises, and what you see on screen is printed exactly the same everywhere. Adobe made the free Acrobat Reader available in 1994, and within five years, PDF became the go-to format for everything from product manuals and corporate reports to government forms and academic papers. \
By the early 2000s, “export as PDF” was a one-click option in almost every authoring tool, and organizations across industries embraced it for distribution, archiving, and compliance. And it’s still the standard today.The very thing that made PDFs so appealing (their promise of pixel-perfect fidelity) also introduced a hidden trade-off: it locked content into a rigid, print-first structure.\
Beneath every flawless page was essentially a digital snapshot, built to mimic what came out of a printer. Headings, tables, paragraphs, none of it had semantic meaning. To a computer, it was just coordinates and text boxes scattered across a canvas.\
At first, this didn’t matter. But as documents moved from desktops to web browsers, mobile screens, and automated pipelines, the cracks began to show. Want to extract clean data? Reflow text on a phone? Understand document structure? Suddenly, what looked clean to humans became a mess for machines. \n  Tagged PDF and Other Modernization AttemptsAdobe wasn’t blind to the problem. Tagged PDF (introduced in 2001 and later formalized in PDF/UA for accessibility) adds an HTML-like logical structure. It never became universal, but it is mandated for accessible government documents and widely used in large-enterprise workflows. Other milestones, such as PDF/A for long-term archiving, XMP metadata support, and the 2008 hand-off of the spec to ISO, show steady efforts to modernize the format. Still, broad adoption lagged; tagging is invisible to most users, tedious for creators, and often stripped out by careless export settings.\
A whole ecosystem of SaaS tools popped up to bridge this gap. You see it in heavyweights like DocuSign, in the many web-based PDF editors such as DocHub, and in open-source libraries like Poppler, which developers depend on just to pull text out of PDFs. \
That’s also why the big cloud players are all throwing serious AI muscle at this problem: AWS with Textract, Google with Document AI, and Microsoft with Azure AI Document Intelligence. The market emerged, products followed, and plenty of revenue flowed. Adobe, whether we like it or not, changed the game.The Rise of AI-Native PDF HandlingWhen ChatGPT hit, the “PDF problem” exploded. Companies scrambled to feed their data into LLMs, only to hit a wall: most of that valuable info was locked away inside PDFs.\
At first, the goal was simple: just extract clean text for Retrieval-Augmented Generation (RAG). But that quickly proved too basic. Without layout awareness, text from columns got scrambled, tables turned into nonsense, images got ignored, and important context disappeared.\
Modern Document AI now trains models to understand a document’s visual and logical layout: identifying titles, paragraphs, tables, and images. So, AI can reference information, skip repeated headers/footers, and grasp the overall structure.\
This AI stack reveals the full extent of the mess we’re dealing with. What should be straightforward data extraction now requires multiple specialized layers:Layout analysis to understand document structure,OCR to extract text from images and scanned documents,VLM orchestration to coordinate these different AI components.Each layer adds latency, potential errors, and compute cost. The irony is staggering: we’re using some of the most advanced AI models ever built to solve a problem that stems from a 30-year-old decision to treat documents like photographs.\
While PDFs have gradually evolved, their print-first DNA keeps piling costs onto every modern workflow. Structured formats, scanned or photographed, do introduce some of the same hurdles, but PDF’s design amplifies the pain.We can’t scrap decades of PDFs overnight, but we can avoid repeating history. For new content, choose born-digital formats that preserve semantics by default:Markdown-derived standards for technical docs,or DOCX/OOXML when Office compatibility is a must.\
When a fixed-layout file is unavoidable, export with full tags and metadata intact; some authoring tools now automate this. Government procurement rules that require PDF/UA compliance are a positive precedent. Similar pressure from enterprises on vendors and regulators can push tagging from “nice-to-have” to “table stakes.”\
Long term, open standards like W3C’s Portable Web Publication or EPUB 3, along with upcoming containerized JSON-based formats, promise fidelity without sacrificing structure. Supporting these in mainstream authoring tools (and educating users to adopt them) will spare the next generation from writing vision models just to pull text out of a contract.\
The story of PDFs proves that early design choices echo for decades. The lesson isn’t to vilify the engineers who solved 1991’s problem; it’s to recognize that today’s “good enough” shortcuts become tomorrow’s costly handcuffs. Let’s embed semantics at the source, back open, machine-readable standards, and ensure the next wave of document tech is built for humans and machines alike.\
For teams already dealing with legacy formats, tools like  offer an Open-Source API-based pipeline to convert complex documents into structured, chunked formats tailored for LLM and RAG workflows, available both as hosted endpoints or self-managed infrastructure.Struggling to Grow Your Audience as a Tech Professional?The Tech Audience Accelerator is the go-to newsletter for tech creators serious about growing their audience. You’ll get the proven frameworks, templates, and tactics behind my 30M+ impressions (and counting).]]></content:encoded></item><item><title>Cuba’s Power Grid Nears Total Failure</title><link>https://spectrum.ieee.org/cuba-energy-crisis</link><author>Ricardo Torres</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExNTM5Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2NTc3MTgyMn0.MpArvxigPLjNqXMmdCRIIb_2Han8CkTmfg2U1UC9dj8/image.jpg?width=600" length="" type=""/><pubDate>Tue, 1 Jul 2025 12:00:05 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Decades of neglect and subpar fuel leave Cuba’s energy infrastructure in crisis]]></content:encoded></item><item><title>Gentoo Releases Updated Install Media Based On KDE Plasma 6.3 + Linux 6.12 LTS</title><link>https://www.phoronix.com/news/Gentoo-Linux-July-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 12:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Gentoo Linux project ended the month of June by releasing new install media...]]></content:encoded></item><item><title>How Blockchain Is Redefining Financial Inclusion for the Unbanked in 2025</title><link>https://hackernoon.com/how-blockchain-is-redefining-financial-inclusion-for-the-unbanked-in-2025?source=rss</link><author>Vision NP</author><category>tech</category><pubDate>Tue, 1 Jul 2025 11:59:50 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In 2025, the global economy will continue to rapidly digitize. Do you know another secret? Yet over 1.4 billion people remain unbanked, according to the latest World Bank data. These people lack access to traditional financial services due to systemic barriers, such as identity issues, geographic isolation, high service fees, and distrust in centralized institutions. On the other hand, blockchain networks have a decentralized nature with the potential for financial inclusion. Bitcoin appeared to be the main enabler of decentralization. Traditional financial systems are bound by gatekeeping, bureaucracy, and geography, but blockchain offers a trustless, borderless, and programmable financial layer. It can turn a smartphone into a bank, digital ID provider, and loan officer, all in one.\
The diagram below explains how blockchain technology transforms the financial inclusion of an unbanked population.\
This study explores how blockchain tackles financial exclusion through the latest innovations.1. Own Your Online Identity—No Central Authority RequiredOne of the key obstacles to unbanking is its verifiable identity. Decentralized Identity (DID blockchain systems (Polygon ID and Proof of Personhood of Worldcoin) enable users to create tamper-resistant digital identities without the help of central authorities. They are owned by the users themselves and cryptographically authenticated, meaning that they can be applied to any finance platform. The DID enables one to.Participate in global marketsThe effective implementation of a DID can be determined using a real-life example in Bhutan. In the middle of 2024, Bhutan migrated its national self-sovereign digital identity operating on the Hyperledger Indy to the Polygon blockchain and started using the previously adopted CREDEBL protocol, which is an open-source, UN-approved verifiable credentials system.\
It is very likely that you also ask yourself why it is important. Bhutan is one of the first nations in the world to fully implement a sovereign identity system and provide all citizens with ownership and control of their digital credentials. Moving to Polygon implies a faster, safer, and higher capacity to have a broad, nationwide scope of use.2. Banking Without Banks? Stablecoins Make It PossibleCentralized traditional banks often charge high fees for account maintenance, remittances, and FX conversion. However, if we locate Stablecoins like USDC, cUSD (Celo Dollar), and GHO (Aave's stablecoin), they provide a USD-pegged, low-volatility alternative that can be sent and received globally, instantly, and at nearly zero cost.Through mobile wallets, such as Trust Wallet, users in remote areas canSave in a stable currencyAvoid local currency inflationIn Latin America, stablecoins are rapidly growing, as in the case of Venezuela, where stablecoins are used in 34 % of cases, and Argentina, where they constitute approximately 61.8 % of crypto activity, far more than in Brazil (59.8 %) and Mexico (18 %). In low-banking, high-inflation contexts, mobile-first users are also turning to dollar-pegged tokens, such as USDT and USDC, as an easily accessible and reliable source of savings, transactions, and even remittances when normal banks are weakened by rampant inflation and rising costs of using bank services.3. Microfinance and DeFi LendingThe decentralized Finance (DeFi) protocols Goldfinch, Aave Arc, and Maple Finance currently target real-world assets (RWA) and under-collateralized lending, giving credit to new establishments and individuals in emerging markets.Accessible without a credit historyWith blockchain, a street vendor can seek out a loan through a foreign investor to whom smart contracts apply and are repaid through the blockchain.\
The fact that Goldfinch has successfully lent more than USD 100 + million to real-life businesses, and most prominently within the Sub-Saharan Africa market, exemplifies the issue of using blockchain to match global capital to the underbanked. Goldfinch offers accessible and sustainable financing to underserved members of the community by utilizing the USDC, local partnerships, and smart contracts to offer scalable, accountable, and impactful financing to the underserved.4. Remittances Without MiddlemenThe world remittance market, with an annual turnover of more than 800 billion, is currently monopolized by intermediaries such as the Western Union, charging sums up to 10 percent in fees. Smart contract-based remittance systems such as Ramp Network, Xoom using the USDC, and OnFinality have minimized the cost to be nearly free and settle within minutes rather than days.\
One of the real-world examples for this category is that in the Philippines,  is revolutionizing remittances by using blockchain and stablecoins such as USDC and PHPC to enable near-instant, low-cost cross-border transfers.\
Traditional services charge 6-7% and create a delay by taking time, but Coins.ph processes remittances in minutes with fees as low as 0.1%. Recently, it handled over  in remittances. This allowed unbanked users to reach local cash-in partners.\
This is the same case as Chipper Cash, the African platform that runs on Ripple to make cross-border remittances through cryptocurrency simple and relatively cheap, which is a good example of how blockchain can remove middlemen and raise financial inclusion.5. Local Economies Powered by BlockchainNew blockchain-backed economies are becoming strong alternatives to conventional markets as substitutes for community currencies and economies of rewards, which are dependent on banks and centralized authorities. In such ecosystems, users have access to earn, save, spend, and accumulate wealth completely inside blockchain-native infrastructure, in many cases, only with a mobile phone. promote environmental action through token incentives. and  enable direct aid and grants without bureaucratic delay. integrated blockchain wallets with mobile money services, such as M-Pesa, in Africa.These projects present a new trend: blockchain is no longer reshaping finance, but rather democratizing access to value, redefining aid, and a local, inclusive economy. Blockchain native systems allow the elimination of mediators and the provision of communities that are disadvantaged in the global economy with resilient financial infrastructure by allowing programmable trust. Efforts such as Spacecoin to decentralize the Internet and finance show how blockchain can overcome the digital and financial divides. By paying Internet bills with crypto, generally no more than a dollar or two per month, disadvantaged communities develop an on-chain credit history that can lead them to loans and other financial services without going through banks.6.Key Challenges to Watch & Attempt to Tackle ThemOwing to the transformational power of the blockchain, there are real-world obstacles to its use in disadvantaged communities. Problematic issues should be addressed in a context-aware and user-centric manner to help create meaningful financial inclusion.A significant portion of the unbanked population does not have smartphones or access to a stable Internet connection, which reduces their exposure to dApps. This gap is bridged by mobile-first instruments and SMS-based wallets such as .Interfaces are difficult to understand and deal with, and they contain unknown cryptographic concepts. Getting new users on board is assisted by local education initiatives and simplified apps such as .Using stablecoins puts users at regulatory and issuer risks. Asset-backed or regional stablecoins can be made more resilient.The use of technical terms and the lack of proper localization made mass adoption impossible. To address this, platforms should enhance the user interface and experience with a user-friendly multilingual interface.Losing private keys or becoming victims of fraud can be disastrous. Therefore, emerging safer models include social recovery wallets (e.g., Trust Wallet has a feature to back up your key in Google Drive) and local custodianship models.Vague or antagonistic legislation halts the advancement in certain countries like Bhutan, which are leading positive exponents by adopting controlled Blockchain ID structures.The expensive mining costs for PoW-based blockchains, such as Bitcoin, and network congestion, restrict their use. To tackle this problem, there are other scalable and cheap options, including chains such as Polygon, Celo, and Solana.These are not only technical issues, but also ethical. Addressing these problems presents the challenge of working together, and doing so involves developers, policymakers, educators, and communities. It is only after that time that the blockchain can open the door to fair access to financial tools.🔗 Blockchain + AI: Smarter Financial AccessArtificial intelligence (AI)-based products and services are becoming increasingly popular as of 2025, and the future seems even brighter if blockchain is combined with AI for effective services. Some real-world examples, such as Worldcoin and the Human Protocol, combine blockchain with AI to assess reputational credit scores. This allows lenders to evaluate unbanked users based on the following criteria:This AI + blockchain combination completely upends conventional credit scoring regimes in low-data settings by relying on on-chain modes of behavior, peer-to-peer recommendations, digital identity solutions, and decentralized reputation systems to create dynamic, immutable credit profiles. The new system can offer inclusive, real-time risk assessment, whereas traditional systems use credit rating agencies that have not changed since the 80s and fixed financial history; millions of unbanked people will be able to be supplied with fair credit, insurance, and other financial instruments.\
🏦 Central Bank Digital Currencies (CBDCs)With DeFi developing organically, it is being approached by the government. Over 130 nations are in the trial or implementation of CBDCs. When combined with blockchain wallets, CBDCs canEnable state benefits without banksDistribute UBI (universal basic income)Track inflation-resistant micro-savingsThe e-CNY, eNaira, and India digital rupees are already under testing for merchant payments and cross-border payouts.\
🌍 Local Economies, Global RailsKenyan community projects, such as , or a Latin American community project, such as , enable communities to create locally built digital currencies that are backed to stabilize other ecoins, such as cUSD or DAI. These currencies are frequently issued to local DAOs or mobile applications and are tasked with reflecting the actual economic transactions in the community.\
These systems: \n • Mobilize microtrade without money: allow local businesses, farmers, and vendors to enable them to receive payments digitally, even in places where the traditional banking system is not well connected. \n • Pay people to do good things, such as tree planting, education, recycling, or volunteering--turn people into communal tokens that anybody can spend in the community.• Establish circular economies founded on activities tied in which value is generated and used in the local area, hence disregarding the need to rely on outside aid or unstable fiat currencies.\
For example, the Sarafu Network in Kenya, initiated by Grassroots Economics, has provided over 60,000 users with services and transfer of goods using digital credits printed in the community, with a total number of transacted dollars so far, being more than $ 3 million. Members are credited with credit vouchers in exchange for labor or production, which can be used in the network for food, transport, and other basic necessities. The given model not only creates financial stability but also develops better social bonds, more economic collaboration, and local development, which are also strengthened by blockchain technology that works under the surface.Final Thoughts: A Financial ResetBlockchain is not just a technological innovation, it is also a catalyst for economic empowerment and financial justice. Blockchain allows people in an underserved community to be empowered about their financial future, irrespective of geographical locations and backgrounds, by breaking the traditional rungs of identity, access, and trust.\
Low-fee remittances made AirBnB-killer project stable ecoins, microcredit without collateral provided by the DeFi protocols, secure documents and data on the blockchain, and decentralized identity systems that are replacing formal KYC, all of which are already here and radically developing.\
We are in a new financial age where no one is forgotten and banking comes not with a privilege but a right as governments, developers, and communities collaborate to create ethical, inclusive, and clear blockchain-based solutions.\
The next billion users will not just join Web3; they will help shape it. In the future, the most powerful bank may not have walls. It may simply be a phone, key, or a chain.]]></content:encoded></item><item><title>From Crisis to Security - How DePIN Can Solve Tonga&apos;s Cybersecurity Challenges</title><link>https://hackernoon.com/from-crisis-to-security-how-depin-can-solve-tongas-cybersecurity-challenges?source=rss</link><author>Edwin Liava&apos;a</author><category>tech</category><pubDate>Tue, 1 Jul 2025 11:55:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A comprehensive response to recent data breaches and a roadmap for digital sovereigntyThe Wake-Up Call - Recent Cybersecurity Breaches in TongaThe recent leak of patient information online has sent shockwaves through Tonga's government and healthcare systems, highlighting a critical vulnerability that threatens not just individual privacy, but national security itself. When hackers can access sensitive patient data and potentially disrupt essential services, it becomes clear that Tonga's current digital infrastructure is inadequately prepared for modern cyber threats.This breach represents more than just a technical failure, it's a symptom of a fundamental problem with centralized, vulnerable systems that rely on foreign infrastructure and proprietary software. For vulnerable groups, including patients, the elderly, and those requiring critical medical services, these security failures can have life-threatening consequences.The time for half measures is over. Tonga needs a revolutionary approach to cybersecurity, one that puts sovereignty, resilience, and unbreachable security at its core.The DePIN Solution - Building an Unhackable Government NetworkDecentralized Physical Infrastructure Network (DePIN) represents a paradigm shift from vulnerable centralized systems to a distributed, blockchain-based architecture that leverages the proven security model of Bitcoin, a network that has operated for 16 years without ever being successfully hacked or compromised.Why Bitcoin Level Security Matters January 3, 2009 16 Years Never compromised or hacked since inception Imperfect networks and imperfect human aspectsBitcoin has stood the test of time as the most secure decentralized network precisely because it was designed to be resilient against the very types of attacks that are currently plaguing traditional government systems.The Journey to Digital Sovereignty - Tonga's DePIN Transformation StoryPicture this i.e. It's a Monday morning in Nuku'alofa, and instead of waking up to news of another devastating cyberattack, government workers across Tonga are logging into the most secure digital infrastructure in the Pacific. This isn't a distant dream, it's the inevitable destination of our DePIN journey, and here's how we get there.Chapter 1: The Foundation - Building Our Digital Fortress at MEIDECC"Every revolution begins with a single spark."Our story begins where all great transformations do, with a proof of concept that will forever change how Tonga thinks about digital security. Picture the Tonga Ministry of Meteorology, Energy, Information, Disaster Management, Environment, Climate Change and Communications (MEIDECC) building, currently vulnerable to the same attacks that just compromised patient data across the country. But within six months, this same building will house the most secure government network node in the Pacific.The First Month - Discovery and VisionThe transformation begins quietly, almost invisibly. Our technical team walks through MEIDECC's corridors, laptops in hand, conducting what looks like a routine network audit. But this is no ordinary assessment, we're mapping every cable, every server, every potential entry point that hackers could exploit. In conference rooms late into the evening, security experts spread out network diagrams like military strategists planning a campaign.By week three, the real work begins. While government employees continue their daily routines, our developers are crafting something revolutionary in the background, a blockchain architecture specifically designed for Tonga's unique needs. Picture lines of code flowing across screens, each function carefully designed to prevent the exact type of breach that recently exposed sensitive patient information. This isn't just software development; it's digital nation building.Months Two and Three - The Technical RenaissanceCafe Escape, Friends, Post Coffee and other coffee spots around MEIDECC start recognizing our Pasifika Web3 Tech Hub team as regulars. Day and night, they're building Tonga's digital sovereignty one line of code at a time. They're implementing proof-of-authority consensus mechanisms that ensure only trusted government nodes can validate transactions. They're creating smart contracts that will make document forgery impossible. They're designing multi-signature authorization systems that require multiple government officials to approve any sensitive operation.Meanwhile, the real magic is happening in the government's commitment to energy independence. Picture construction crews arriving at MEIDECC with solar panels and battery systems. These aren't just renewable energy installations, they're the foundation of a network that can operate completely independently of Tonga's power grid. When the next cyclone hits and the lights go out across Nuku'alofa, MEIDECC's blockchain nodes will continue humming along, powered by the Pacific sun.Months Four and Five - Hardening the FortressAs solar panels gleam on MEIDECC's roof, inside the building, another transformation is taking place. Hardware security modules, the digital equivalent of bank vaults, are being installed to protect the cryptographic keys that secure Tonga's data. Multi-layered firewalls are configured like digital moats around the government's most sensitive information. Intrusion detection systems are deployed like digital sentries, watching for any sign of malicious activity.The real-time monitoring systems we install don't just detect threats—they learn from them. Every attempted intrusion, every suspicious packet, every anomalous behavior pattern becomes data that makes the system smarter and more resilient.Month Six - The Proof is in the TestingPicture this scene i.e. In a secure facility in Nuku'alofa, ethical hackers, some of the best cybersecurity experts money can hire, are unleashing every attack they can imagine against MEIDECC's new blockchain infrastructure. They try DDoS attacks that would cripple traditional systems. They attempt the same social engineering tactics that compromised the patient data. They simulate the power outages and network failures that have historically left government systems vulnerable.And the system holds. Every attack is deflected, every vulnerability is absent, every backup system activates flawlessly. The blockchain continues processing transactions, the solar power keeps systems operational, and the monitoring systems log every attempted intrusion for future analysis.Chapter 2: The Expansion - From Spark to Wildfire"Success is not a destination, but a journey of continuous transformation"By month seven, word is spreading through government circles about something remarkable happening at MEIDECC. While other ministries struggle with basic cybersecurity, MEIDECC operates with the confidence of an organization whose data is truly secure. Ministers start asking questions: "How can we get this level of protection?"The Ministry Awakening - Months Seven and EightOur story now expands beyond MEIDECC's walls. Picture assessment teams walking through the Ministry of Health, where the recent patient data breach occurred. They're not just looking at current vulnerabilities, they're envisioning a future where patient records are stored on an immutable blockchain, where access requires cryptographic authorization from multiple medical professionals, where even the most sophisticated hackers find only impenetrable walls.At the Ministry of Education, we see the potential for student records that can never be falsified, academic credentials that are instantly verifiable anywhere in the world, and research data that remains secure even as it's shared with international partners.The Ministry of Finance presents the most compelling case for blockchain implementation. Picture budget allocations recorded on an immutable ledger, government expenditures tracked with transparent smart contracts, and financial reporting that citizens can verify themselves. This isn't just cybersecurity—it's the foundation of unprecedented government transparency.Months Nine Through Twelve - The Network EffectThe real magic happens when individual ministry nodes start connecting to form Tonga's unified government blockchain network. Picture dedicated blockchain nodes being installed in each ministry building, each one a fortress in its own right, but collectively forming an impregnable digital kingdom.These aren't isolated systems, they're nodes in a living network that grows stronger with each addition. When the Health Ministry node communicates with the Finance Ministry node to process a medical equipment purchase, the transaction is validated by multiple nodes across the government network. When the Education Ministry needs to verify a student's eligibility for government assistance, the request flows through encrypted channels that make eavesdropping impossible.The Tonga Cable Ltd Partnership - A Strategic AllianceAs our network grows, a crucial partnership emerges with Tonga Cable Ltd. Picture TCL not just as an internet service provider, but as the strategic gatekeeper of Tonga's digital sovereignty. Through the dedicated infrastructure of TongaIX, Tonga's own internet exchange point (IXP), government data never has to leave Tongan soil to travel between ministries.This means that when a doctor at Vaiola Hospital accesses a patient's records, that data travels through Tongan infrastructure, controlled by Tongan engineers, secured by Tongan cryptography. No foreign servers, no vulnerable international links, no opportunity for external interference."True sovereignty is not given, it is built, byte by byte, block by block"Months Thirteen Through Eighteen - The Great MigrationPicture government offices across Tonga undergoing a quiet but profound transformation. The familiar Windows interfaces that have dominated government desktops for decades are being replaced by sleek, secure Linux distributions designed specifically for government use. Microsoft Office gives way to LibreOffice, but this is more than just a software change, it's a declaration of digital independence.Government workers receive training not just on new software, but on a new philosophy of digital security. They learn to think like cybersecurity professionals, to recognize threats before they materialize, to operate in an environment where every click is part of a larger security ecosystem.This is where the PasifikaOS vision comes alive, not just for Tonga, but as a model for Pacific digital independence that other island nations will soon seek to emulate.Months Sixteen Through Twenty - The Backup RevolutionWhile Tonga has historically been at the mercy of submarine cable failures, our DePIN solution includes a revolutionary backup strategy. Picture Starlink ground stations being installed at each ministry location, not as primary connections, but as instantly available backup links that activate automatically when traditional connectivity fails.The next time a ship's anchor damages Tonga's submarine cable (and it will happen), government operations continue without interruption. Critical communications flow through Low Earth Orbit satellites, blockchain transactions continue processing, and essential services remain available to citizens.Months Eighteen Through Twenty Four - The Digital GovernmentIn the final phase of our journey, Tonga doesn't just have secure government systems, it has a completely reimagined digital government that serves as a model for the world. Picture citizens receiving digital identity credentials stored on the blockchain, making identity theft virtually impossible. Elections conducted through cryptographically secure voting systems that provide both complete transparency and absolute privacy.Government procurement happens through smart contracts that automatically release payments when delivery conditions are met, eliminating corruption and ensuring efficient use of public funds. Land registry records become immutable blockchain entries that resolve property disputes instantly and prevent fraudulent land claims.Budget tracking becomes a real-time, public dashboard where every citizen can see exactly how their tax money is being spent, with smart contracts automatically flagging unusual expenditures for review."In the calm after the storm, we realize that we built not just a network, but a legacy"Picture this scene twelve months from today i.e. Across the Pacific, another island nation experiences a massive cyberattack that brings down government services for weeks. Citizens can't access health records, schools can't verify student credentials, and government operations grind to a halt.But in Tonga, government workers arrive at their offices that same morning to find their systems operating normally. The blockchain network continues processing transactions, the solar powered infrastructure hums along independently, and the automated monitoring systems haven't detected so much as a suspicious packet.More importantly, when news of the attack reaches Tonga, government officials don't respond with fear, they respond with confidence. They know their systems are built on the same cryptographic foundations that have protected Bitcoin for sixteen years without a single successful hack. They know their data is distributed across multiple nodes, their communications are end-to-end encrypted, and their backup systems can operate independently of any external infrastructure.This isn't just the end of our implementation story, it's the beginning of Tonga's emergence as a digital sovereignty leader in the Pacific. Other nations start sending delegations to study Tonga's DePIN implementation. International cybersecurity experts cite Tonga's government network as a model for resilient infrastructure design. And most importantly, Tongan citizens begin to trust their government's digital services in ways they never could before."Ko e 'Otua mo Tonga ko hotau tofi'a" - God and Tonga are our inheritanceOur journey from vulnerability to invincibility isn't just a technical transformation, it's a story of national pride, digital sovereignty, and the courage to build something completely new rather than patch something fundamentally broken. In twenty four months, we don't just solve Tonga's cybersecurity problems, we eliminate them entirely, creating a digital infrastructure that becomes Tonga's gift to the Pacific and the world.Technical Architecture: Layer-by-Layer SecurityLayer 1: Physical InfrastructureSolar-powered mini-grids: Energy independence eliminates grid vulnerability No single point of failure Military-grade encryption at the hardware levelLayer 2: Network Security All communication encrypted end-to-end Efficient, secure packet routing Government traffic stays within Tonga Independent satellite connectivityLayer 3: Blockchain Consensus Government controlled validator nodesMulti-signature authorization: Multiple approvals required for critical actions All transactions permanently recorded and verifiable Automated execution of government processesLayer 4: Application Security Every user and device must be verified Permissions based on job function and clearance level 24/7 surveillance of all network activityAutomated incident response: Immediate isolation of compromised systemsAddressing Current VulnerabilitiesProblem 1: Centralized Data Storage Single point of failure makes entire systems vulnerable  Distributed storage across multiple blockchain nodes ensures no single point of failureProblem 2: Foreign Infrastructure Dependency Reliance on foreign systems creates sovereignty and security risks  100% Tongan-controlled infrastructure with local decision-makingProblem 3: Proprietary Software Vulnerabilities Closed-source software contains unknown security flaws  Open-source transparency allows for continuous security auditingProblem 4: Inadequate Backup Systems System failures can cause extended outages  Multiple redundancy layers including solar power and satellite backupProblem 5: Insufficient Cyber Threat Response Slow response to security incidents  Real-time monitoring with automated threat response and isolationEconomic Benefits: Security That Pays for ItselfLicensing fee elimination: Save approximately TOP 150,000 annually Solar power saves estimated TOP 35,000 per year 99.9% uptime eliminates productivity lossesLong-term Economic Impact Full system cost recovery within 3-4 years Compared to traditional cybersecurity solutions Technical expertise remains in Tonga Avoid costs of incident response and reputation damage Meet international data protection standardsInsurance premium reduction: Demonstrable security measures reduce coverage costsImplementation Roadmap: Making It HappenConduct comprehensive security audit of current government systemsAssemble technical implementation team with blockchain and cybersecurity expertise for MEIDECC proof of conceptBegin stakeholder engagement across all government ministriesShort-term Goals (3-6 Months)Deploy MEIDECC pilot program with full blockchain infrastructureDemonstrate measurable security improvements through testing and monitoringTrain core technical staff on DePIN management and maintenance for other government ministriesMedium-term Objectives (6-18 Months)Roll out to all major ministries with full blockchain node deploymentImplement Tonga Cable Ltd integration for complete network controlDeploy solar power infrastructure for energy independenceBegin open-source migration away from vulnerable proprietary systemsLong-term Vision (18-24 Months)Achieve complete digital sovereignty with 100% Tongan-controlled infrastructureDeploy advanced blockchain services for citizens and businessesEstablish Tonga as a model for Pacific Island digital independenceCreate exportable solution for other developing nationsWhy This Solution Will SucceedProven Technology FoundationBuilt on Bitcoin's 16 year track record of unbreachable security, the DePIN solution leverages battle tested blockchain technology that has never been successfully compromised.Tailored for Tonga's NeedsUnlike generic cybersecurity solutions, this system is specifically designed for Tonga's unique challenges, including natural disaster resilience, energy independence, and sovereignty requirements.The modular design allows for gradual implementation and expansion, while the economic benefits ensure long-term sustainability without ongoing foreign dependency.Future Proof ArchitectureWeb3 compatibility ensures that Tonga's infrastructure will remain cutting edge as digital technologies continue to evolve.Call to Action: Securing Tonga's Digital FutureThe recent cybersecurity breaches are not just isolated incidents, they are warnings of what happens when a nation's digital infrastructure is inadequately protected. The cost of inaction is measured not just in compromised data, but in lost sovereignty, economic damage, and risks to vulnerable populations.The DePIN solution offers Tonga a unique opportunity to leapfrog traditional cybersecurity approaches and implement a truly revolutionary system that provides: that has never been breachedComplete digital sovereignty with 100% local control through cost savings and local capacity building through multiple redundancy layers with emerging Web3 technologiesGovernment Leadership Commitment: Secure high-level government buy-in for the DePIN transformation Pursue both local and donor funding options for the initial implementation Recruit and train the technical expertise needed for deployment Build public support for digital sovereignty initiatives"Ko e 'Otua mo Tonga ko hotau tofi'a" - God and Tonga are our inheritanceThe time has come for Tonga to claim its digital inheritance, a secure, sovereign, and sustainable technological infrastructure that protects its people and preserves its independence for generations to come.The full technical specifications, architecture diagrams, and detailed budget breakdowns are available in our comprehensive pitch deck presentation. This interactive presentation includes:Complete network architecture visualizationDepartment-by-department blockchain node configurationSolar power infrastructure layoutsBusiness continuity and disaster recovery planningDetailed budget analysis with cost comparisonsTimeline and implementation roadmaps]]></content:encoded></item></channel></rss>