<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech</title><link>https://www.awesome-dev.news</link><description></description><item><title>Pinwheel introduces a smartwatch for kids that includes an AI chatbot</title><link>https://techcrunch.com/2025/07/03/pinwheel-introduces-a-smartwatch-for-kids-that-includes-an-ai-chatbot/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Thu, 3 Jul 2025 15:27:20 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Pinwheel Watch is designed specifically for kids aged 7 to 14, offering parental management tools, GPS tracking, texting, and more.]]></content:encoded></item><item><title>Let&apos;s Encrypt Rolls Out Free Security Certs For IP Addresses</title><link>https://it.slashdot.org/story/25/07/03/1452239/lets-encrypt-rolls-out-free-security-certs-for-ip-addresses?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 3 Jul 2025 15:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Let's Encrypt, a certificate authority (CA) known for its free TLS/SSL certificates, has begun issuing digital certificates for IP addresses. From a report: It's not the first CA to do so. PositiveSSL, Sectigo, and GeoTrust all offer TLS/SSL certificates for use with IP addresses, at prices ranging from $40 to $90 or so annually. But Let's Encrypt does so at no cost. 

For those with a static IP address who want to host a website, an IP address certificate provides a way to offer visitors a secure connection with that numeric identifier while avoiding the nominal expense of a domain name.]]></content:encoded></item><item><title>Castelion is raising a $350M Series B to scale hypersonic missile business</title><link>https://techcrunch.com/2025/07/03/castelion-raises-350m-series-b-to-scale-hypersonic-missile-business/</link><author>Aria Alamalhodaei</author><category>tech</category><pubDate>Thu, 3 Jul 2025 15:03:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A hypersonics weapon startup is lining up a massive new funding round to scale its business with the U.S Department of Defense. ]]></content:encoded></item><item><title>Intel Lunar Lake Showing Some Performance Improvements With Linux 6.16</title><link>https://www.phoronix.com/review/x1-carbon-gen13-linux616</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 14:50:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those on an Intel Core Ultra Series 2 "Lunar Lake" system, the upcoming Linux 6.16 kernel is looking to be in better shape for those newest Intel SoCs. In testing carried out using a Lenovo ThinkPad X1 Carbon Gen 13 Aura edition laptop, there are performance gains in some areas with the Linux 6.16 development kernel.]]></content:encoded></item><item><title>xAI gets permits for 15 natural gas generators at Memphis data center</title><link>https://techcrunch.com/2025/07/03/xai-gets-permits-for-15-natural-gas-generators-at-memphis-data-center/</link><author>Tim De Chant</author><category>tech</category><pubDate>Thu, 3 Jul 2025 14:45:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Elon Musk's AI company has been operating more than twice that number without permits for months, a legal group alleges.]]></content:encoded></item><item><title>Ransomware gang Hunters International says it’s shutting down</title><link>https://techcrunch.com/2025/07/03/ransomware-gang-hunters-international-says-its-shutting-down/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Thu, 3 Jul 2025 14:45:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The cybercriminal group, which said it's releasing its decryption tools to victims, may be transitioning to new infrastructure under a different name.]]></content:encoded></item><item><title>Ford CEO Predicts AI Could Eliminate Half of US White-Collar Jobs</title><link>https://slashdot.org/story/25/07/03/1255209/ford-ceo-predicts-ai-could-eliminate-half-of-us-white-collar-jobs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 3 Jul 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Ford CEO Jim Farley believes half of all white-collar workers in the U.S. could lose their jobs to AI in the coming years, he said. He joins other executives making similar predictions about AI's impact on employment. "AI will leave a lot of white-collar people behind," he said. From a report: The Ford CEO's comments are among the most pointed to date from a large-company U.S. executive outside of Silicon Valley. His remarks reflect an emerging shift in how many executives explain the potential human cost from the technology. Until now, few corporate leaders have wanted to publicly acknowledge the extent to which white-collar jobs could vanish. 

In interviews, CEOs often hedge when asked about job losses, noting that innovation historically creates a range of new roles. 

In private, though, CEOs have spent months whispering about how their businesses could likely be run with a fraction of the current staff. Technologies including automation software, AI and robots are being rolled out to make operations as lean and efficient as possible.]]></content:encoded></item><item><title>Tighten up your cap table with Fidelity, Cimulate, and DepositLink at TechCrunch All Stage 2025</title><link>https://techcrunch.com/2025/07/03/tighten-up-your-cap-table-with-fidelity-cimulate-and-depositlink-at-techcrunch-all-stage-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Thu, 3 Jul 2025 14:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Fundraising is already hard — don’t let a messy cap table make it harder. At TechCrunch All Stage on July 15, founders will get a behind-the-scenes look at what it really takes to keep a raise on track during the roundtable session “Preparing to Raise: Cap Table Best Practices to Help You Close Fast.” Hosted […]]]></content:encoded></item><item><title>How Smell Guides Our Inner World</title><link>https://www.quantamagazine.org/how-smell-guides-our-inner-world-20250703/</link><author>Yasemin Saplakoglu</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2025/07/MolecularSmell-crMichaelWaraksa-Default.webp" length="" type=""/><pubDate>Thu, 3 Jul 2025 14:27:32 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[When Thomas Hummel gets a whiff of an unripe, green tomato, he finds himself in his childhood home in Bavaria. Under the tilted ceilings of the bedroom that he shared with his two older brothers, there were three beds, a simple table and a cupboard. “My mother put those green tomatoes on the cupboard for them to ripen,” said Hummel, an olfaction researcher at the Carl Gustav Carus University…]]></content:encoded></item><item><title>This 1945 TV Console Showed Two Programs at Once</title><link>https://spectrum.ieee.org/dumont-duoscopic-tv-set</link><author>Allison Marsh</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEzMzM0NS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1OTYyNjI4OX0.37x1w5pslICp7AInZWPIWrj3dI-lLqyhyik7gRPOJ18/image.jpg?width=600" length="" type=""/><pubDate>Thu, 3 Jul 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Its inventor envisioned an end to arguments over what to watch]]></content:encoded></item><item><title>US Agencies&apos; Science Journal Subscriptions Canceled</title><link>https://science.slashdot.org/story/25/07/03/1313250/us-agencies-science-journal-subscriptions-canceled?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 3 Jul 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: The US government canceled several federal agencies' subscription to Nature and other scientific journals. A spokesman for the Department of Health and Human Services said all contracts with Springer Nature, Nature's publisher, had been "terminated" and that taxpayer money should not be used on "junk science." Nature newsroom, with an update : On 2 July, one US government agency, the Department of Health and Human Services (HHS), which oversees the National Institutes of Health (NIH), appeared to walk back its earlier statement to Nature's news team saying that it was cancelling contracts to Springer Nature. Now the HHS says: "Science journals are ripping the American people off with exorbitant access fees and extra charges to publish research openly. HHS is working to develop policies that conserve taxpayer dollars and get Americans a better deal. In the meantime, NIH scientists have continued access to all scientific journals."]]></content:encoded></item><item><title>Writer CEO May Habib to take the AI Stage at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/07/03/writer-ceo-may-habib-to-take-the-ai-stage-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Thu, 3 Jul 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AI agents are reshaping how work gets done across industries, and at TechCrunch Disrupt 2025, one of the leading voices in that transformation is stepping onto the AI Stage. May Habib, CEO and co-founder of Writer, will join us in San Francisco, October 27-29, for a fireside chat that dives deep into how enterprises are […]]]></content:encoded></item><item><title>Everything you need to know about Flashes, the Bluesky-based Instagram alternative</title><link>https://techcrunch.com/2025/07/03/everything-you-need-to-know-about-flashes-the-bluesky-based-instagram-alternative/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 3 Jul 2025 13:47:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The app, which is now available in beta, offers a different way to browse the visual posts on Bluesky.]]></content:encoded></item><item><title>Israeli quantum startup Qedma just raised $26 million, with IBM joining in</title><link>https://techcrunch.com/2025/07/03/israeli-quantum-startup-qedma-just-raised-26-million-with-ibm-joining-in/</link><author>Anna Heim</author><category>tech</category><pubDate>Thu, 3 Jul 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Qedma specializes in error mitigation software. Its main piece of software, QESEM, standing for Quantum Error Suppression and Error Mitigation, analyzes noise patterns to suppress some classes of errors while the algorithm is running and mitigate others in post-processing.]]></content:encoded></item><item><title>X.Org Server Lands Big Improvement For Using Zink With GLAMOR</title><link>https://www.phoronix.com/news/X.Org-GLAMOR-Zink-DMA-BUF</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 12:39:09 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A nice improvement was merged today to the X.Org Server for benefiting the GLAMOR 2D acceleration code when using the Zink OpenGL-on-Vulkan driver...]]></content:encoded></item><item><title>Institutional Failure: CBS Wimps Out, Pays Trump $16 Million Bribe To Settle Baseless Lawsuit</title><link>https://www.techdirt.com/2025/07/03/institutional-failure-cbs-wimps-out-pays-trump-16-million-bribe-to-settle-baseless-lawsuit/</link><author>Karl Bode</author><category>tech</category><pubDate>Thu, 3 Jul 2025 12:24:04 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Again, this lawsuit had absolutely no basis in truth. CBS executives could have fought the lawsuit and found an unlimited supply of public and financial support. Talented lawyers country wide would have been happy to help with the case pro bono in order to shut up an authoritarian bully. Instead, CBS ownership, keen to head to the exits and transfer ownership of the company to Skydance executives (who look to be even bigger Trump ass kissers), folded to a blatant attempt by our mad king to bully and extort a major media company away from doing basic journalism. The settlement comes after months of negotiations between the two sides, and had been sped along by concerns of discovery and a looming shakeup on the CBS/Paramount board of directors:“After weeks of negotiations with a mediator, lawyers for Paramount and Mr. Trump worked through the weekend to reach a deal ahead of a court deadline that would have required both sides to begin producing internal documents for discovery, according to two people familiar with the negotiations. Another deadline loomed: Paramount was planning to make changes to its board of directors this week that could have complicated the settlement negotiations.”It’s worth noting that it takes the New York Times until the fifth paragraph to make it clear Trump’s lawsuit was baseless. Unsurprisingly, Trump’s legal team tries to frame this unconstitutional extortion racket as some kind of big win for the American public:“A spokesman for Mr. Trump’s legal team said in a statement that the settlement was “another win for the American people” delivered by the president, who was holding “the fake news media accountable.”“CBS and Paramount Global realized the strength of this historic case and had no choice but to settle,” the spokesman said.”The CBS settlement comes despite hints from California lawmakers that they’d be investigating any settlement as a potential bribe under California law. CBS execs initially showed some hesitation in the light of the inquiries, but ultimately likely concluded that any financial penalties (after years of inquiries and litigation) were worth the approval for their $8 billion megamerger.Over on Bluesky, Senator Ron Wyden promised he’d hold CBS executives accountable, and urged state lawmakers to follow through on their bribery inquiries:Paramount just paid Trump a bribe for merger approval. When Democrats retake power, I’ll be first in line calling for federal charges. In the meantime, state prosecutors should make the corporate execs who sold out our democracy answer in court, today.It’s important to view this as an extension of a very successful, fifty-plus year mission by Republicans to bully U.S. journalism and discredit factual criticism of often extremely unpopular right wing ideology (destroying social service programs and rural medical care to fund giant tax breaks for rich assholes, as a random example plucked out of a hat):CBS’ reward for this feckless appeasement was utterly bogus lawsuits, baseless FCC “investigations,” and getting relentlessly attacked in the right wing media as some sort of leftist rag (when again, CBS, if anything, had spent much of the last decade pandering to the U.S. right). There’s simply no winning when it comes to folding to authoritarian bullshit. Just an immense, historic act of cowardice for a U.S. media industry increasingly comprised of flimsy artifice. The era of Walter Cronkite and Edward R. Murrow it sure as hell isn’t.]]></content:encoded></item><item><title>Google rolls out its new Veo 3 video generation model globally</title><link>https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:56:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries. ]]></content:encoded></item><item><title>Linux 6.17 To Finish Clearing Out Old Code For OpenMoko Devices</title><link>https://www.phoronix.com/news/Linux-6.17-Dropping-OpenMoko-In</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:52:04 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Linux 6.17 is expected to clear out some final remnants of the OpenMoko Neo 1973 and Neo FreeRunner smartphone support from that Linux smartphone effort from two decades ago...]]></content:encoded></item><item><title>Lenovo Legion Go S HID Driver Posted For Linux</title><link>https://www.phoronix.com/news/Lenovo-Legion-Go-S-HID</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:30:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Linux support for the Lenovo Legion Go S gaming handheld continues to be improved upon thanks to the option of having Steam OS on this alternative to the Steam Deck...]]></content:encoded></item><item><title>Improved TTM Memory Management Eviction Submitted Ahead Of Linux 6.17</title><link>https://www.phoronix.com/news/Linux-6.17-TTM-Eviction</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:12:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Sent out today was the newest drm-misc-next pull request of changes built up over the past week for DRM-Next ahead of the Linux 6.17 kernel cycle. The drm-misc-next material is the usual random assortment of DRM display/graphics driver changes and core improvements, which this week includes some TTM eviction work...]]></content:encoded></item><item><title>Podcast: The Life Changing Power of Lifting</title><link>https://www.404media.co/404-media-podcast-casey-johnston/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/06/casey-jason.png" length="" type=""/><pubDate>Thu, 3 Jul 2025 10:00:49 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[For this week’s podcast, I’m talking to our friend Casey Johnston, a tech journalist turned fitness journalist turned independent journalist. Casey studied physics, which led her to tech journalism; she did some of my favorite coverage of Internet culture as well as Apple’s horrendous butterfly laptop keyboards. We worked together at VICE, where Casey was an editor and where she wrote Ask a Swole Woman, an advice column about weightlifting. After she left VICE, Casey , an independent site about weightlifting, but also about the science of diet culture, fitness influencers on the internet, the intersections of all those things, etc. She just wrote A Physical Education: How I Escaped Diet Culture and Gained the Power of Lifting, a really great reported memoir about how our culture and the media often discourages people from lifting, and how this type of exercise can be really beneficial to your brain and your body. I found the book really inspiring and actually started lifting right after I read it. In this interview we talk about her book, about journalism, about independent media, and how doing things like lifting weights and touching grass helps us navigate the world. Listen to the weekly podcast on , or YouTube. Become a paid subscriber for access to this episode's bonus content and to power our journalism. If you become a paid subscriber, check your inbox for an email from our podcast host Transistor for a link to the subscribers-only version! You can also add that subscribers feed to your podcast app of choice and never miss an episode that way. The email should also contain the subscribers-only unlisted YouTube link for the extended video version too. It will also be in the show notes in your podcast player. ]]></content:encoded></item><item><title>What Anthropic’s AI Lawsuit Means for the Future of Publishing</title><link>https://hackernoon.com/what-anthropics-ai-lawsuit-means-for-the-future-of-publishing?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 9 of 10. 4. THE EFFECT OF THE USE UPON THE MARKET FOR OR VALUE OF THE COPYRIGHTED WORKThe final factor is “the effect of the use upon the potential market for or value of the copyrighted work.” 17 U.S.C. § 107(4). This factor points against fair use when a copyist makes copies available that displace demand for copies the copyright owner already makes available or readily could. Texaco, 60 F.3d at 926–28 (reproduced copies); Dr. Seuss Enters., L.P. v. ComicMix LLC, 983 F.3d 443, 461 (9th Cir. 2020) (derivative copies). “While the first factor considers whether and to what extent an original work and secondary use [in principle could] have substitutable purposes, the fourth factor focuses on actual or potential market substitution.” Warhol, 598 U.S. at 536 n.12 (emphasis added).A. THE COPIES USED TO TRAIN SPECIFIC LLMSThe copies used to train specific LLMs did not and will not displace demand for copies of Authors’ works, or not in the way that counts under the Copyright Act. Again, Authors concede that training LLMs did not result in any exact copies nor even infringing knockoffs of their works being provided to the public. If that were not so, this would be a different case. Authors remain free to bring that case in the future should such facts develop. Instead, Authors contend generically that training LLMs will result in an explosion of works competing with their works — such as by creating alternative summaries of factual events, alternative examples of compelling writing about fictional events, and so on. This order assumes that is so (Opp. 22–23 (citing, e.g., Opp. Exh. 38)). But Authors’ complaint is no different than it would be if they complained that training schoolchildren to write well would result in an explosion of competing works. This is not the kind of competitive or creative displacement that concerns the Copyright Act. The Act seeks to advance original works of authorship, not to protect authors against competition. Sega, 977 F.2d at 1523–24. Authors next contend that training LLMs displaced (or will) an emerging market for licensing their works for the narrow purpose of training LLMs (Opp. 21–22). Anthropic argues that transactional costs would exceed Anthropic’s expected benefit from any such bargain, prompting it to cease dealing with any rightsholders or else to cease developing such technology altogether (Br. 22–23). Our record could support either account — so this order must assume Authors are correct. A market could develop (Opp. 19–21 (citing record)). Even so, such a market for that use is not one the Copyright Act entitles Authors to exploit. None of the cases cited by Authors requires a different result. All contemplated losses of something the Copyright Act properly protected — not the kinds of fair uses for which a copyright owner cannot rightly expect to control. See TVEyes, Inc., 883 F.3d at 181 (use of a right legally reserved to and factually already being licensed by copyright owner); Texaco, 60 F.3d 931 (same); Ringgold v. BET, Inc., 126 F.3d 70, 80–81 (2d Cir. 1997) (use of a right legally reserved to and factually likely to be marketable by copyright owner — displaying images of her artistic work in television shows); cf. Seltzer v. Green Day, Inc., 725 F.3d 1170, 1179 (9th Cir. 2013) (no evidence use could be or “was likely to” be marketable). The fourth factor thus favors fair use for the training copies.B. THE COPIES USED TO BUILD A CENTRAL LIBRARY(i) The Purchased Library Copies Converted from Print to DigitalFor these copies, this order assumes Anthropic’s format change from print to digital displaced purchases of new digital copies that Anthropic would have made directly from Authors (had it not been able to purchase print copies in used condition). But for reasons stated under the first factor, such losses did not relate to something the Copyright Act reserves for Authors to exploit. It was a format change. Authors’ next argument, it seems, is that the format change nonetheless exposed it to usurpation of the opportunity to sell rightful copies because Anthropic might transmit additional unauthorized digital copies more readily than it could have transmitted additional unauthorized print copies — and that the same would be true for all format converters (cf. Opp. 25 n.14; Opp. Expert Malackowski ¶ 52). But after much discovery, there is no inkling in our record of intent to redistribute library copies once acquired nor of inability to secure that valuable library against outside actors. And, if the internal, central library copies did or do in fact lead to further reproduction or distribution, those further copies remain redressable separately by Authors. The format change did not itself usurp the Authors’ rightful entitlements. This factor is thus neutral for the purchased library copies converted from print to digital.(ii) The Pirated Library CopiesThe copies used to build a central library and that were obtained from pirated sources plainly displaced demand for Authors’ books — copy for copy. Not every person who merely intends to make a fair use of a work is thereby entitled to a full copy in the meantime, nor even to steal a copy so that achieving this fair use is especially simple or cost-effective. Here, the copies employed in training LLMs were one thing, but the copies acquired to assemble a convenient, general-purpose library of works for various uses for which the company might have of them, if any, was a different use altogether.Anthropic has almost no rebuttal on these points. First, Anthropic argues that “Claude’s services do not reduce [or usurp] the value of Plaintiffs’ works through substitution in their traditional markets” (see Br. Expert Peterson ¶ 33). But stealing pirated copies of Authors’ works plainly did. Second, Anthropic argues that it may have been able to purchase some books on the open market (and some other texts), but not other texts it copied (cf. id. ¶ 48 (re licensing)). But this case does not concern those other texts it could not have purchased. It could have purchased Authors’ books (and many others). In fact it later did. Finally, Anthropic argues that the effect on these texts from one book foregone was too small to be considered (see id. ¶ 77). But the test requires that we contemplate the likely result were the conduct to be condoned as a fair use — namely to steal a work you could otherwise buy (a book, millions of books) so long as you at least loosely intend to make further copies for a purportedly transformative use (writing a book review with excerpts, training LLMs, etc.), without any accountability. As Anthropic itself suggested, “That would destroy the [entire] publishing market if that were the case” (see Tr. 53; see also Tr. 32, 41; Opp. Expert Malackowski ¶¶ 31–34, 38). The fourth factor points against fair use for the pirated library copies.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>New Evidence That Some Supernovae May Be a &apos;Double Detonation&apos;</title><link>https://science.slashdot.org/story/25/07/03/0051240/new-evidence-that-some-supernovae-may-be-a-double-detonation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New evidence from a 300-year-old supernova remnant in the Large Magellanic Cloud suggests that some Type Ia supernovae may result from a "double detonation" -- where a helium shell ignites first, triggering a second core explosion in a white dwarf before it reaches critical mass. "While the physics of the process itself are interesting, the key question this raises is whether type Ia supernovae really are all equally bright," writes Ars Technica's John Timmer. "If they can detonate with substantially less mass than is needed for direct ignition of the core, then it's possible that some of them could be considerably less bright." However, the research team notes that additional factors -- such as the influence of binary systems or secondary detonations -- could further complicate the picture. Ars Technica reports: "The detonations in the carbon-oxygen core and the helium-rich shell result in qualitatively different yield products," the researchers behind the new work write in a paper describing it. In the paper, they focus on calcium, which there are two ways of producing. One is from the outer shell of helium, via fusion before the detonation dilutes the material. A second batch of calcium is produced through the fusion of the core material as it's ejected in the supernova, which prevents further fusion events from converting it to even heavier elements. (Material deeper in the core does end up getting fused into heavier material.) Because it's produced by both of the detonations, models predict that the expanding sphere of debris will contain two different shells of calcium, with some space in between them. To find evidence for these shells, the researchers checked an older supernova remnant, which allows enough time for the movement of material to separate the shells by enough distance that they can be resolved from Earth.
 
They focused their observations on a supernova remnant named SNR 0509-67.5, located in the nearby Large Magellanic Cloud. SNR 0509-67.5 is estimated to be a bit over 300 years old, meaning material has had enough time to move a significant distance away from the site of the explosion. Imaging using a spectrograph on the Very Large Telescope allowed them to resolve what, in effect, was a spherical sulfur sandwich, with the role of the bread played by calcium. In other words, if you were to travel away from the site of the explosion, you would first hit a layer of ionized calcium, followed by ionized sulfur, and then run into a second layer of ionized calcium. This is exactly what computer models that simulate double detonations predict. So, the researchers suggest it is strong support for that hypothesis. The researchers say that the details suggest that SNR 0509-67.5 was a white dwarf with roughly the same mass as the Sun when it exploded, and that its explosion was likely triggered by the detonation of a helium shell with only three percent of the Sun's mass.]]></content:encoded></item><item><title>Libreboot 25.06 Released With Support For Two More Outdated Systems</title><link>https://www.phoronix.com/news/Libreboot-25.06-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 09:55:22 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Libreboot 25.06 released this week as the newest version of this Coreboot downstream focused on shipping only with free and open-source components. But due to the strict open-source nature of Libreboot, it continues to primarily see support for long outdated platforms...]]></content:encoded></item><item><title>Judge Finds AI Training on Complete Books ‘Reasonably Necessary’</title><link>https://hackernoon.com/judge-finds-ai-training-on-complete-books-reasonably-necessary?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Thu, 3 Jul 2025 09:00:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 8 of 10. 3. THE AMOUNT AND SUBSTANTIALITY OF THE PORTION USEDThe third fair use factor is “the amount and substantiality of the portion” of the copyrighted work used by the accused. 17 U.S.C. § 107(3). The crux of this factor is whether the amount was “reasonable in relation to the purpose of the copying.” Campbell, 510 U.S. at 586. Thus, the amount of copying is considered first against the work itself, then more importantly against the proposed transformative purpose. See Warhol, 598 U.S. at 543 & n.18.A. THE COPIES USED TO TRAIN SPECIFIC LLMSCopies selected for inclusion in training sets were selected because they were complete and because they contained rich protectible expression, or so this order accepts the record shows for Authors. Was all this copying reasonably necessary to the transformative use? “What matters [ ] is not so much ‘the amount and substantiality of the portion used’ in making a copy, but rather the amount and substantiality of what is thereby made accessible to a public [in the purported secondary use] for which it may serve as a competing substitute [for the primary use].” Google, 804 F.3d at 222. Here, once again, there is no allegation of any traceable connection between the Claude service’s outputs and Authors’ works. The copying used to train the LLMs underlying Claude was thus especially reasonable. In response, Authors object primarily that the copying used in training was both extremely extensive and not strictly necessary. As to extensive copying, it is true that entire works were copied. And, “copying [ ] entire work[s] ‘militate[s] against a finding of fair use.’” Worldwide Church of God v. Philadelphia Church of God, Inc., 227 F.3d 1110, 1118 (9th Cir. 2000) (quoting Hustler Mag. Inc. v. Moral Majority Inc., 796 F.2d 1148, 1155 (9th Cir. 1986)); see Campbell, 510 U.S. at 587. But we just addressed why Authors’ argument is misdirected. The copies that count for this factor are those that would merely serve the same use as the work’s ordinary one. Authors do not allege such copying. The accused use here of the incremental copies is as orthogonal as can be imagined to the ordinary use of a book.As to strict necessity, Authors make a stronger point. When a productive use is made possible only by borrowing from a specific work, fair use climbs towards its zenith. When a productive use is possible without that borrowing, fair use falls to its nadir — and the borrowing deserves a particularly compelling justification. See Warhol, 598 U.S. at 543 & n.18, 547. Here, it is true that Anthropic could have used some other books or no books at all for training its LLMs — or so this order accepts the record shows for Authors. But Anthropic has presented a compelling explanation for why it was reasonably necessary to use them anyway. For one thing, all agree Anthropic needed billions of words to train any given LLM. If using only books, Anthropic would have needed millions of books per model. If using a set comprising only a small fraction of books and a larger fraction of other texts, Anthropic still would have needed hundreds of thousands of books. Authors contend that because Anthropic showed it could use such smaller sets of books, it surely could have used no books at all — or at least not their books (Opp. 23). But Authors forget that “reasonably necessary” does not mean “strictly necessary.” Authors do not contest that the volume of text required to train an LLM is monumental. Because using so many works was reasonably necessary, using any one work for actually training LLMs was about as reasonable as the next. For another thing, no output to the public was even alleged to be infringing. So, yes, Authors’ works were chosen as the strongest examples of writing. But the compelling benefits of training the LLMs on strong examples were not offset by revelations to the public of any portion of the works themselves. What was copied was therefore especially reasonable and compelling. The third factor thus favors fair use for the training copies.B. THE COPIES USED TO BUILD A CENTRAL LIBRARYBut again, there was a separate use — a distinction that makes some difference as to whether the amount and substantiality of the copying was “reasonable in relation to the purpose of the copying” for the library copies. Campbell, 510 U.S. at 586.(i) The Purchased Library Copies Converted from Print to Digital.For the print library copies that Anthropic purchased and then converted into digital library copies, Anthropic already enjoyed entitlement to keep the copies in its library. The purpose of the copying was to keep them in its library but with more favorable storage and searchability properties. Copying the entire work was exactly what this purpose required. There was no surplus copying. The source copy was destroyed. The third fair use factor favors fair use for the purchased library copies converted from print to digital.(ii) The Pirated Library Copies.For the pirated library copies, however, Anthropic lacked any entitlement to hold copies of the books at all. Its purpose, it says, was to train LLMs. But its objective conduct was to seek “all the books in the world” and then retain them even after deciding it would not make further copies from them for training — indicating there were other further uses. Against the purpose of acquiring all the books one could on the chance some might prove useful for training LLMs and maybe other stuff too, almost any unauthorized copying would have been too much. Anthropic copied millions of books in toto, Authors’ works among them. The third factor points against fair use for the pirated library copies.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>Inside the TCSC-Based Voting Protocol Powered by Intel SGX</title><link>https://hackernoon.com/inside-the-tcsc-based-voting-protocol-powered-by-intel-sgx?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Thu, 3 Jul 2025 08:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[APPENDIX D. A TCSC-BASED VOTING PROTOCOLIn this part, we provide a detailed description of TCSCbased voting system that utilizes the Intex SGX. The protocol mainly consists of two sub-procedures:  and . We give details as follows.\
 In the deployment stage, all the operational code and the initial state are coded into a TCSC. This stage includes two steps.\
. Firstly, contract binary codes are compiled into enclave codes. Since an enclave has only a small quantity of trusted zones for application code and data (the protected memory is 128MB, and only 96MB is usable for an enclave in the current version of Intel SGX [88]), a contract has to determine the boundary of these zones and identify corresponding zones used for privacy-critical functionalities. In particular, the e-voting contract needs to define: the scope of secret states, the scope of public states, the approach to access secret states and the approach to access external states. Enclave Definition Language (EDL) [80] defines trusted components, untrusted components, and corresponding interfaces between them, which takes charge of translation from contract code to enclave code. It provides two\
functionalities: Enclave Calls (ECALLs) and Outside Calls (OCALLs). ECALLs define the functions inside the enclave that are used to expose APIs for untrusted applications to call in. OCALLs specify untrusted functions outside the enclave where the enclave code is able to invoke. In our example, the total number of votes cast for a candidate cannot be revealed until the voting has ended. Thus, the total number of votes cast is defined at the access point ECALLs, and is thereby hidden from the public, and can only be revealed once the voting procedure has been completed.\
. Afterwards, EDL files will load into an enclave, which is stored in the Enclave Page Cache (EPC). From a micro perspective, the first step is to call the ECREATE instruction for creating an enclave. This will allocate memory inside the Enclave Page Cache (EPC). Then, enclave code and data are added to pages in EPC by calling the EADD instruction. Finally, when the instruction EINIT completes successfully, an enclave’s  attributes become true, and the above instructions cannot be used any more. After a successful deployment, the initial state and operational code of this contract will be replicated among blockchain nodes. This means the e-voting logic cannot be changed. But, the state of functionalities can be transferred to parties who have been granted permission with a message-call [2].\
 In the execution stage, voters call the deployed TCSC to finish the voting. Firstly, an enclave needs to fetch the current contract state from the blockchain. Then, the CPU executes the plaintext contract in the enclave mode. External attackers cannot obtain the knowledge of sensitive information since the Memory Encryption Engine (MEE) key never leaves TCB. A critical aspect of Intel SGX’s functionality is that the code inside an enclave can access the particular enclave state by performing additional checks on memory semantics. Back to our example, confidential state (the encrypted number of votes cast for a candidate) will return only when the following four requirements are fulfilled:The processor runs in enclave mode;The requested page is part of the same enclave;The page access is through a correct virtual address;The code semantics successfully pass the check.\
In a word, the CPU is acting as a doorman in the TCSC, providing a hardware-based access control mechanism. After obtaining results from TEEs, the consensus algorithm starts to reach an agreement. To be specific, when a miner receives a newly mined block, he will re-execute all transactions inside the block to obtain the newly transferred state. Once enough blockchain miners receive the block and re-execute transactions, the voting results and the transactions triggering the contract execution will eventually reach the final agreement. When all the voting procedures have ended, the teller can fetch the final encrypted state and obtain the final voting result. In the meanwhile, the transactions can be used as evidence to trace the voter’s behavior.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>A New &apos;Interstellar Visitor&apos; Has Entered the Solar System</title><link>https://science.slashdot.org/story/25/07/03/0041226/a-new-interstellar-visitor-has-entered-the-solar-system?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Astronomers have detected a mysterious "interstellar object," dubbed A11pl3Z, speeding through the solar system at 152,000 mph. If confirmed, it would be just the third known interstellar visitor, following 'Oumuamua and Comet Borisov. The visiting space object will pass near Mars and the Sun later this year before leaving the solar system forever. Live Science reports: The newly discovered object, currently dubbed A11pl3Z, was first spotted in data collected between June 25 and June 29 by the Asteroid Terrestrial-impact Last Alert System (ATLAS), which automatically scans the night sky using telescopes in Hawaii and South Africa. The mystery object was confirmed by both NASA's Center for Near Earth Object Studies and the International Astronomical Union's Minor Planet Center on Tuesday (July 1), according to EarthSky.org.
 
A11pl3Z is most likely a large asteroid, or maybe a comet, potentially spanning up to 12 miles (20 kilometers). It is traveling toward the inner solar system at around 152,000 mph (245,000 km/h) and is approaching us from the part of the night sky where the bar of the Milky Way is located. Based on A11pl3Z's speed and trajectory, experts think it originated from beyond the sun's gravitational influence and has enough momentum to shoot straight through our cosmic neighborhood without slowing down. However, more observations are needed to tell for sure.]]></content:encoded></item><item><title>The TechBeat: Closing the Feedback Loop: Building AI That Learns from Its Users (7/3/2025)</title><link>https://hackernoon.com/7-3-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Thu, 3 Jul 2025 06:10:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @hacker32086803 [ 7 Min read ] 
 Learn how to fix data skew in Apache Spark using the salting technique for improved performance and balanced partitions in Scala and PySpark. Read More.By @vladyslav_chekryzhov [ 25 Min read ] 
 Build production-ready LLM agents. Learn 15 principles for stability, control, and real-world reliability beyond fragile scripts and hacks. Read More.By @permit [ 10 Min read ] 
 Machine identities are set to outnumber human users in every system. Learn why treating machine identities like human ones is crucial for security. Read More.By @permit [ 8 Min read ] 
 Explore how to secure AI agents, protect against prompt injections, and manage cascading AI interactions with AI Security Posture Management (AISPM). Read More.By @n2w [ 2 Min read ] 
 Explore 5 major data breaches that shut down companies—and learn key lessons in backup, security, and disaster recovery to protect your business. Read More.By @permit [ 15 Min read ] 
 Learn how to build secure, human-in-the-loop AI agents using Permit.io’s Access Request MCP, LangGraph, and LangChain MCP Adapters.  Read More.By @wassimchegham [ 4 Min read ] 
 Coordinate multiple AI agents and MCP servers (written in Java, .NET, Python and TypeScript) with LlamaIndex.TS and Azure AI Foundry. Read More.By @jamesmiller994 [ 5 Min read ] 
 Discover the 7 essential tools for local Large Language Model (LLM) development on macOS in 2025.  Read More.By @terminal [ 5 Min read ] 
 Learn how to install and use Hydra in Termux for efficient password cracking and security testing on your Android device.  Read More.By @duycao [ 9 Min read ] 
 Learn to build effective feedback loops for AI products, connecting real user signals to model metrics for continuous improvement and better performance. Read More.By @nassermaronie [ 6 Min read ] 
 MockingJar is a tool for generating structured data from a schema you define. Read More.By @n2w [ 2 Min read ] 
 Understand the EU's DORA regulation, key compliance steps, and how to meet resilience requirements—plus get a free checklist to guide your organization. Read More.By @siafoundation [ 2 Min read ] 
 Sia is thrilled to announce our official partnership with HackerNoon, one of the internet’s largest independent tech publishing platforms.  Read More.By @stefanoamorelli [ 17 Min read ] 
 AI agents can now make onchain payments autonomously using x402, Coinbase’s new protocol that activates the long-unused HTTP 402 status code. Read More.By @superlinked [ 18 Min read ] 
 This article delves into constructing such an AI research agent using Superlinked's vector search capabilities, by integrating semantic and temporal relevance. Read More.By @ipinfo [ 2 Min read ] 
 IPinfo launches IPinfo Plus, delivering high-precision IP intelligence for security, fraud, and compliance—now available via API, Snowflake, and GCP.By @wittycircuitry [ 2 Min read ] 
 Not a founder, but sat in 100+ pitch meetings. Here are 10 powerful, honest tips every startup needs to hear before stepping in front of investors. Read More.By @maken8 [ 6 Min read ] 
 When Bitcoin gets lost, after the Bitcoin Supply/Demand Shock, the crypto scambling will be through the roof. This is Financial Entertainment. Read More.]]></content:encoded></item><item><title>Revenue Feeds the Ego. Gross Margin Builds the Business.</title><link>https://hackernoon.com/revenue-feeds-the-ego-gross-margin-builds-the-business?source=rss</link><author>Kishore Dasaka</author><category>tech</category><pubDate>Thu, 3 Jul 2025 06:10:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every founder I’ve worked with has an instinctive obsession with revenue.It makes sense. Revenue is visible. Revenue gets celebrated. Revenue unlocks funding, headlines, and market momentum. It signals movement. Growth. Traction.But more often than not, that same revenue hides the truth about how the business is actually doing.I once worked with a D2C brand that had scaled fast across three countries. Revenue had tripled in under a year. The founder was being invited to panel discussions. A large VC was engaged. There was talk of an $8M raise at a $40M post-money valuation.On paper, it was a success story.But inside the model, something was broken.The more they sold, the worse their financials got.Every percentage point of growth required disproportionate capital. Cash conversion was deteriorating. And worst of all, their best-selling SKUs - those celebrated in Instagram campaigns and bulk discounted to retailers—were contributing less than 18% in gross margin. One line was actually negative.They weren’t scaling profit. They were scaling loss.And it was all being masked by topline growth.The Dangerous Illusion of Revenue-Led ValidationFounders get rewarded for revenue. Investors respond to momentum. Teams celebrate milestones. But in the background, a business lives or dies not on revenue - but on what’s left after you deliver your product or service.Gross margin is the delta between what you earn and what it costs you to fulfill that earning.It’s the difference between:Running a profitable operation at $1M in revenue, or bleeding cash at $10M.Having strategic freedom in pricing decisions, or being trapped in discount cycles.Raising capital on strength, or constantly fundraising to plug losses.Gross margin doesn’t get enough credit because it doesn’t photograph well. But it’s the most revealing metric in your business. It tells you if your model works - or if it’s a beautiful story layered over structural failure.Most Founders Misunderstand Where Margin LivesAsk a founder about burn rate, and they’ll give you a number. Ask about monthly revenue, and you’ll get a slide. Ask about unit economics, and they might reference a cohort study or CAC payback.But ask about gross margin - real gross margin, not assumed - and the answers become vague.That’s because many founders think gross margin is an accounting artifact. A static percentage on the P&L. Something for the CFO to clean up before the board meeting.Gross margin is an operational lens. It’s a strategic compass. It’s the clearest expression of whether your business creates real economic value with every sale — or simply moves money around and calls it growth.And here’s the truth that stings: no amount of revenue can compensate for poor gross margin discipline.There Are Only Three Ways to Improve Gross Margin - and Founders Avoid All of ThemIn every operating model I’ve reviewed, across vertical SaaS, D2C, manufacturing, fintech, and marketplaces, the levers of gross margin improvement have always boiled down to three. And they’re never as glamorous as the growth story.This is the lever founders talk about least - and fear the most.Pricing feels sacred. Untouchable. Founders imagine a customer backlash, or a competitor undercutting them the moment they move. But what most miss is that pricing isn’t a function of cost - it’s a reflection of perceived value.If you can’t raise prices, it’s either because:You haven’t built differentiationOr you haven’t built the courage to test your positioningThe companies with the strongest brands, best customer experience, and sharpest segmentation charge more — and they do it unapologetically.Margin discipline doesn’t begin with procurement. It begins with pricing power.Cost of goods sold (COGS) is where operational excellence shows up. Your ability to source better, produce smarter, and negotiate more effectively.But COGS isn’t just about hard costs - it includes logistics, packaging, payment gateway charges, fulfillment inefficiencies, cloud costs, customer onboarding time, and more.When you sell software, your COGS might include devops cost per user. When you sell food, it includes spoilage and inventory shrinkage. When you run a marketplace, it’s the cost of fulfillment, returns, and support.Small improvements here compound. Because margin is not always made in big wins — it’s built in better contracts, tighter specs, and smarter processes.But COGS is rarely examined by founders because it’s not glamorous. It’s also not well understood — which is why it silently erodes profitability while leadership celebrates topline growth.This is the most ignored lever of all - because it requires a level of granularity and self-awareness most teams avoid.Every company with multiple products or SKUs has a margin map. Some products drive brand, others drive volume, and a few drive margin. The problem begins when you let volume dictate strategy.If your fastest-growing SKU contributes the lowest gross margin, you’re scaling into fragility.One of the companies I worked with was using performance marketing to push its lowest-margin SKU - because it had the best CTR. The CAC looked reasonable. The funnel was working. But no one noticed that each conversion was adding less than $1.50 in contribution margin - and eating up warehousing and operations capacity at 4x the rate of their premium SKU.Once we ran the margin-weighted channel mix, it became clear: They weren’t optimizing growth. They were penalizing their economics in exchange for vanity metrics.Product mix strategy is not about selling more. It’s about selling more of what earns you more - and letting go of what doesn’t.Why Gross Margin Is the Most Honest Metric You HaveGross margin doesn’t lie.It’s not distorted by capital injections. It doesn’t get inflated by discounts. It can’t be spun with a better story.It’s either there — or it isn’t.That’s why every investor worth their capital will, at some point, ask for it. And not just at the topline, but by SKU, by channel, by segment, and by month.Because revenue is momentum. But margin is model.You can fund momentum. You can’t fund a broken model indefinitely.Fixing Gross Margin Fixes Everything DownstreamOverhead can be controlled. Marketing efficiency can be improved. Team size can be rationalized. But none of it matters if every unit you sell puts you further away from sustainability.If you’re serious about scaling, and not just surviving, gross margin is where the discipline starts. Not because it looks good on a slide. But because it tells you - in real terms - whether what you’ve built actually works.Most founders try to outrun margin problems by growing faster. The smart ones stop. Model. Reprice. And build something durable.Because if gross margin is weak, growth is not the answer. It’s just a faster path to collapse.]]></content:encoded></item><item><title>From Hydro to Hashrate Derivatives: 5 Bitcoin Mining Trends to Watch Out For (Yes, AI’s Here Too)</title><link>https://hackernoon.com/from-hydro-to-hashrate-derivatives-5-bitcoin-mining-trends-to-watch-out-for-yes-ais-here-too?source=rss</link><author>Michael Jerlis</author><category>tech</category><pubDate>Thu, 3 Jul 2025 06:09:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In just over a year, the Bitcoin network grew by more than 300 EH/s, closing out 2024 at a record 808 EH/s. By June 2025, it climbed to nearly  before falling back to around 861 EH/s due to localized shutdowns and regulatory setbacks.That is not just growth. That is pressure. Miners are fighting to stay above water as network difficulty keeps rising and hashprice remains disappointing.The post-halving reality is tough. April 2024 slashed block rewards in half, but electricity costs stayed the same. Miners are left with a clear choice: adapt or disappear. In 2025, Bitcoin mining is no longer about brute force. It is a test of resilience.Key Trends Shaping the MarketHydro-Cooling and the Efficiency RaceAdvanced cooling is no longer a futuristic advantage — it is becoming a baseline requirement. Bitmain’s S23 Hydro, offering 9.5 J/TH, has become a go-to for industrial-scale operations. Facilities in Paraguay and parts of Canada now rely on immersion and hydro systems to stay viable. Smaller players, including startups like Auradine and firms like A.R.T. Digital, are also pushing efficiency through airflow optimization and better heat dispersion.But not all miners can afford these upgrades. Operators without access to modern infrastructure are being priced out. Immersion and hydro setups are scaling up fast, and those that lag behind are seeing their margins erode. The efficiency race is about avoiding obsolescence.AI / HPC Integration: The Dual Infrastructure PlaySome miners are quietly shifting their business models. With ASIC profitability under pressure, parts of the industry are redirecting power and physical space to AI or high-performance computing (HPC). These workloads, especially training large AI models, often generate more revenue per watt than mining.In the U.S., firms like Applied Digital and Crusoe are trying to offset mining losses by offering compute services using stranded or flared energy. Crusoe is reportedly running AI training workloads, but managing GPU infrastructure involves higher complexity, including cooling, system integration, and software maintenance. These setups also depend on favorable regulations, especially around flared gas use, which could change.Applied Digital’s financials suggest much of its revenue still comes from prearranged deals rather than stable demand. For now, this shift appears more like a stopgap than a reliable business model. Whether it can support long-term operations remains uncertain.The Rise of Hashrate DerivativesThe hashrate derivatives market is no longer a novelty. According to Luxor, OTC volume has increased more than fivefold, and Bitnomial now offers regulated futures contracts tied to hashrate, allowing miners to secure future revenues in an increasingly volatile environment.Some miners use forwards and options to protect against price swings, rising network difficulty, and unpredictable fee dynamics. Institutional players are also entering, viewing these instruments as a way to gain indirect exposure to mining without the capital costs of hardware or infrastructure. Even Bitfinex now includes them as part of its mining risk strategy, a notable shift from the earlier, simpler model of holding mined coins.But this financial layer comes with its own problems. These instruments are structurally complex, not widely understood, and often thinly traded. For many participants, especially smaller operators, the risks of mispricing or lack of liquidity may outweigh the benefits. It remains unclear whether this trend will improve resilience or simply deepen mining’s dependence on financial engineering.Despite continued volatility, some miners are still holding BTC on their balance sheets. CleanSpark and Riot Platforms are among those maintaining a treasury-style approach, treating Bitcoin as a long-term asset rather than short-term revenue. Outside the mining space, Green Mining is reportedly pursuing a $1 bln SPAC aimed at building a BTC reserve vehicle — a sign of growing financialization around Bitcoin exposure.In June 2025, Cipher Mining raised  through convertible notes to expand its treasury strategy. Meanwhile, Strategy (formerly MicroStrategy) holds over  BTC and continues to rely on debt to grow its position, a model that has drawn attention but also raised concerns about sustainability.This is no longer just passive holding. Many of these strategies involve leverage, structured debt, and balance sheet risk. The core assumption, that BTC will outperform over time, remains untested in a prolonged downcycle. If market conditions shift, these positions could become liabilities faster than expected.The core issue in mining today isn’t hardware. It’s positioning.Large-scale players still win on electricity rates and infrastructure scale. But they are vulnerable to market shocks. On the other hand, adaptive firms, those integrating AI, using hedging tools, or building treasury-focused strategies, may have more options when conditions shift.Here’s a snapshot of how strategies currently stack up:]]></content:encoded></item><item><title>How To Build An Orbital Hive: TESSERAE Project</title><link>https://hackernoon.com/how-to-build-an-orbital-hive-tesserae-project?source=rss</link><author>DeFi Titan</author><category>tech</category><pubDate>Thu, 3 Jul 2025 06:08:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Dyson sphere, the space elevator, the construction of a space station inside an asteroid - all such ideas seem feasible in the foreseeable future, as we are accustomed to extrapolating the successes already achieved in the construction of space stations. The era of inhabited space orbital stations began in 1971.Today, we will touch on an automatic assembly and docking of orbital modules to create space habitats of arbitrary shape. In 2018, the Massachusetts Institute of Technology launched the TESSERAE project, described as “self-assembling space architecture.” I will post a promotional video for this program below, but first I would like to note that dreams of such space architecture were formulated in Japan as early as 1988.Background. Adaptive farms.In 1988, Hiroshi Furuya of Nagoya University and Korio Miura of the Japan Aerospace Exploration Agency suggested how the assembly and superstructure of space stations could be automated, gradually transforming them into space settlements. They also addressed issues of deployment, docking, and sealing of compartments, and investigated the role of microgravity in both the assembly and operation of such an adaptive modular station.To ensure the adaptability of such a structure, it must be equipped with a mechanism for changing its own configuration. A “linear” analogue of such a structure would be a conventional multi-link manipulator that bends according to the Rubik's snake principle. However, such a structure lacks rigidity and will quickly become loose in low gravity. In a one-dimensional case, in order to obtain an effective and lightweight structure, it must be topologically similar to a spatial truss. This is how Miura and Furuya imagined such “linear” trusses with an adaptively changing shape:Under what conditions would such a set of trusses be adaptive? One option is if any element of the structure could change its length without affecting the length of other elements and without increasing the internal stress of the entire system. This condition is feasible if the entire truss is statically determinate.When deploying and gradually building up a space station, the most attractive feature of an adaptive system is that the system itself controls its own assembly. The elastic and vibration properties of both individual trusses and their assemblies will inevitably depend on the configuration of the entire station. When selecting a configuration, it is most important to optimize the stiffness and fundamental frequencies of the structure. This applied discipline is already called “elastic adaptability.”However, the aforementioned Japanese study focused specifically on openwork extended structures. It is unlikely that such trusses, even comparable in strength to carbon nanotubes, could be used to build closed interconnected compartments that would remain airtight while allowing the structure to be expanded by wedging new blocks into the existing structure.Kori Miura (born 1930) continues this research and is the co-author of the classic Cambridge book Forms and Concepts for Lightweight Structures. I believe that it was his work that inspired Ariel Ekblaw's group. Ekblaw is leading the development of self-assembling modules at MIT as part of the TESSERAE project, which could eventually serve as the basis for a new generation of space stations. Like fullerenes, tesserae consist of pentagons and hexagons, but theoretically they can be assembled into structures much more complex than a dodecahedron or a soccer ball, such as a torus.Let's talk about them in more detail.So, to deploy the orbital settlements of the future, humanity will need self-assembling, adaptive, expandable (additive), and inflatable structures. Since orbital engineering designs are not constrained by Earth's gravity, completely new options for shells and tiers can be explored there. Earth architecture is traditionally vertically oriented (floors, walls, ceilings, arches, elevators) and requires significant resources to be spent not so much on usable space as on load-bearing structures. Autonomous space assembly, together with simultaneous digital printing and pre-printing of blocks, will make it possible to do without conveyors, blocks, and, of course, lifting cranes. By printing the necessary blocks in geostationary orbit, it would be possible to significantly reduce the volume of payload launched into orbit, and by borrowing construction materials from asteroids moored at Lagrange points, the payload mass could also be reduced.Inside view of the module:This project is called TESSERAE (Tiled Electromagnetic Space Structures for Exploring Reconfigurable Adaptive Environments). Currently, the MIT team is testing and assembling prototypes (shown here are samples measuring 13 inches in diameter):To ensure predictability of assembly, it is necessary to standardize the dimensions of individual tiles, as well as the geometry of polygons to obtain macrofigures. Naturally, equilateral and equiangular (regular) polyhedrons are the obvious choice for this purpose, but given that the structure must remain stable during repairs and when new blocks are added, the three-dimensional structure of this type is similar in shape to a fullerene.By definition, the edges of the structure must differ in terms of function. It is assumed that each module will have at least four types of edges: opaque cover (walls), transparent cover (windows), magnetic (docking), and sensory. It may also be necessary to provide for a fifth category—solar panels. The customization of the interior of the modules and compartments will likely be even more diverse.In order to simulate the assembly line construction of the station, the modules could be launched into a predetermined orbit. Traveling in this orbit around the Earth or the Moon, the structure would pick up the blocks and materials prepared for it at predetermined points.Electromagnets ensure that the plates are autonomously captured with each other and form closed polygons. Since the plates have a pronounced hexagonal shape, the entire structure may well resemble not only a fullerene, but also a parallelepiped or cube, resembling honeycombs:The main difficulty of such an assembly lies in coordination. Each plate must act as a separate agent of the swarm.The most convenient way to obtain tiles for TESSERAE is by 3D printing. This will result in rigid plates (pentagonal or hexagonal) with specially cut holes for permanent magnets (EPM) on each edge. The module is equipped with:Motherboard that controls the electrical circuits for moving the modules;Wireless communication with a remote control center (main computer);Motion sensors, thanks to which the tiles do not collide with each other but gently dock;Inertial measurement unit (IMU) that allows tracking the relative position, rotation, and change in orientation of the tiles.It is also planned to provide connectors on the tiles for installing peripheral devices. The most important of these are a magnetometer (for detecting electromagnetic contact and diagnosing the communication signature between docked tiles) and a paired emitter and light detector (in RGB color scheme) for exchanging data between tiles using photonics.Naturally, the operation of the EPM will depend entirely on the data received from the sensors. In addition, the EPM will be the main consumers of energy and will require an uninterrupted power supply. The polarity of the EPM will also need to be constantly adjusted so that the structure does not disintegrate, but at the same time does not collapse into an undesirable or catastrophic form, and does not end up in a metastable state.We are currently only learning the basics of 3D printing in space, but this technology is a real investment in the future. It is obvious that when exploring the solar system, we will have to work in conditions of greatly reduced gravity. This applies to Martian colonies, the potential development of minerals on asteroids, and, even more so, the colonization of Lagrange points. The basis for such technology can be not only experiments in 3D printing on board the ISS, but also additive manufacturing, which is quite feasible under Earth's gravity.Unlike many modern (rigid) orbital structures, tessera tiles can be tightly packed before being launched into open space, easily replaced, and assembled. In addition, given the magnetic coupling, such structures can be not only assembled but also disassembled automatically.Here we will dwell a little more on the role of magnets and the similarity between TESSERAE assembly and additive manufacturing. Given that each magnet operates autonomously and the entire structure exhibits the characteristics of a swarm of robots, it is possible not only to change the polarity of the magnets, but also to simply disable individual magnets for a short time. This gives the structure additional optional degrees of freedom, and in microgravity conditions, it can turn into a quasi-stochastic system.In this case, modularity is combined with reconfigurability, so the entire structure can be quickly reassembled to meet changing expedition requirements and new tasks, or to remove damaged or depressurized elements.The initial assembly may take place not in open space, but in a special inflatable container optimized for the shape of a fullerene. This container can be docked to a space station, from where the initial set of tiles is loaded into the container and ensured to take the target configuration. This is what a habitable orbital station assembled in the form of a fullerene might look like (the MOSAIC project for operation in Mars orbit, the acronym stands for “Mars Orbiting Self-Assembling Interlocking Chambers”).Let's move on to the most interesting part: how realistic is it to make such a station habitable without it being astronomically expensive?The internal cavity of the self-assembling fullerene should be equipped with lightweight and automatically deployable structures that would not pose a threat to the crew (for example, would not overheat) and would leave maximum space for living and working, as well as for storing equipment, food, water, and air. Take a look at the configuration of the planned American lunar station Lunar Gateway, which is currently scheduled for launch in 2027:It is assumed that a crew of eight people will work at such a station for three months. What would a similar station made of fullerene blocks look like?The most important factor in designing such a station is the size of the airlock between compartments, and in Gateway, the airlock diameter will be 93 cm. Accordingly, all objects and devices must fit through this opening. For comfortable living on the TESSERAE station, there must be a dining room, bathroom, washroom, sleeping quarters, meeting rooms, and work areas. At least a quarter of the space must be allocated for storage, and 30% of the space for air circulation.One astronaut's personal space occupies three hexagons with free access to a fourth hexagon equipped for communication and collaboration. The areas for storing personal belongings are shown in blue and green. In addition to the portholes, the entire inner surface of the walls is available for covering with LCD panels displaying media information.In principle, almost all elements of the capsule's interior should be foldable or telescopic, possibly inflatable.Conclusion and a little more about sensorsOnce again, I would like to note that both the external and internal surfaces of the blocks can be simply studded with various sensors, forming a kind of “smart compartment.” While the external sensors are responsible for the assembly process (dynamics) and maintaining the correct configuration (statics), the internal sensors could record the level of radiation, the content of harmful impurities in the air, and also signal potential depressurization. The unit could be equipped with dual-purpose sensors, such as spectroscopic sensors, which could be used both for laboratory purposes and as a life support element.The development of such sensors requires the development of a completely new technology of functional fibers that combine the capabilities of sensors, telecommunications devices, and batteries. Nevertheless, it is precisely the conditions of microgravity, vacuum, and the desire for maximum compactness of all elements that are conducive to the development of such technologies, which on Earth would seem exotic or of little practical use.I doubt that TESSERAE will be deployed in orbit (including near the Moon or Mars) by the end of this decade. However, I believe that “acceptance testing” of all these ambitious developments — at least in the form of miniature prototypes — is a priority task for the crews of new orbital stations, especially Tiangong.]]></content:encoded></item><item><title>Grammarly Expands into AI Email with Superhuman Acquisition</title><link>https://hackernoon.com/grammarly-expands-into-ai-email-with-superhuman-acquisition?source=rss</link><author>Tech Thrilled</author><category>tech</category><pubDate>Thu, 3 Jul 2025 06:06:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Grammarly has officially acquired Superhuman, the popular AI-powered email client, to boost its productivity suite and deepen its focus on AI-driven communication.The deal was announced on Tuesday. While financial terms weren’t shared, the move signals Grammarly’s growing ambition to reshape how professionals write, work, and connect — especially over email.Why Grammarly Bought SuperhumanEmail remains one of the most-used tools in the workplace — and Grammarly wants to make it smarter.Grammarly CEO  explained the strategic reason behind the deal:“Email isn’t just another app — it’s where professionals spend much of their day. Superhuman lets us add an entirely new space for AI agents to help people collaborate and get things done.”By bringing in Superhuman’s tech and team, Grammarly can better support professionals with AI tools right inside their inbox.Superhuman is a premium email client designed to make communication faster and smarter.Founded by , , and Conrad IrwinRaised over  from top investors like Andreessen Horowitz (a16z), , and Tiger GlobalValued at , according to TraxcnKnown for like smart replies, automatic scheduling, and message sorting \n Superhuman recently launched AI-powered updates to simplify how users manage email — making it a perfect fit for Grammarly’s vision.Superhuman’s team — including CEO  — is joining Grammarly as part of the acquisition.Vohra shared his excitement about the next phase:“Email is still the top communication tool for billions. With Grammarly, we’ll improve Superhuman and also build new ways for AI agents to work together across all your daily communication apps.”Grammarly said it will continue investing in the , while also building  based on Superhuman’s technology.Grammarly’s Bigger AI PushThis isn’t Grammarly’s first big move to expand beyond writing suggestions.In , Grammarly acquired , a collaborative productivity platformCoda’s co-founder, , was promoted to **CEO of GrammarlyIn , Grammarly raised  from  through a , meaning they didn’t give up equity. Instead, they’ll pay a portion of revenue earned using the investment. \n Together, these moves show Grammarly is building a broader AI-powered productivity ecosystem, where tools like writing, editing, scheduling, and email come together.Email is still where most of us do our work. By combining Grammarly’s writing expertise with Superhuman’s smart email tools, this deal brings us one step closer to truly intelligent, automated communication.With both companies aligned on making professionals more productive, the future looks bright for AI in your inbox.✅ Grammarly acquired Superhuman, a leading AI email app \n 💼 Superhuman was , backed by top VCs \n 📧 Email is a  for Grammarly users \n 🤖 Grammarly plans to use Superhuman to build smart AI email tools \n 📈 Part of Grammarly’s  to expand into AI productivity apps \n 🧑‍💻 Superhuman CEO and team are ]]></content:encoded></item><item><title>A Technical Breakdown of Blockchain, Ethereum Smart Contracts, and Intel SGX</title><link>https://hackernoon.com/a-technical-breakdown-of-blockchain-ethereum-smart-contracts-and-intel-sgx?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Thu, 3 Jul 2025 06:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[ Blockchain, conceptualized by Nakamoto [100], was proposed as a distributed and appendonly ledger, in which all committed transactions are stored in a chain of data records (named as blocks). According to the initial idea of Bitcoin [1], when the blockchain maintainers reach an agreement on the newest block, related transactions appearing in that time will be packaged in this block and further stored in a distributed network to maintain a continuously growing list. By providing a secure solution to distribute the information and allowing all participants to audit the shared records, blockchain obtains many key characteristics such as decentralization, auditability and nonrepudiation, transparency, and non-equivocation.\
 Proposed by Szabo [1], the smart contract are widely applied in blockchain systems by Ethereum [2]. Blockchain-based smart contracts adopt Turing-complete scripting languages to achieve complicated functionalities [6] and execute thorough state transition/replication over consensus algorithms to realize final consistency. By the design, a blockchain-based smart contract includes multiple functions, methods, and a few parameters that can run on the blockchain when specific conditions or events are met and encompass business logic and transactions between two or more parties. To be specific, the source code of a contract forming as part of a transaction is first sent to the blockchain. Once the transaction is included in a new block and confirmed by the majority of the participants, the contract code becomes immutable and executable. When an external user invokes the contract, the state will be updated under the instruction of the preloaded source code. The neutrality of the execution\
environment among all blockchain nodes facilitates the same execution result of the program code. Smart contracts thus enable unfamiliar and distributed participants to fairly exchange without trusted third parties and present a uniform approach to improve applications across a wide range of industries.\
Trusted Execution Environments. Trusted Execution Environment (TEE) [29] provides a protected processing area in the main processor that runs on a separation kernel to ensure confidentiality and integrity of inside data and computations. State-of-the-art implementations include Intel Software Guard Extensions (SGX) [80], ARM TrustZone [89], Keystone [28], . For a TEE, three main TEE features are highlighted, including runtime isolation, sealing technologies and attestation technologies. For simplicity, we use Intel SGX as an example to explain these features in the following paragraphs. It has to be mentioned that the Intel SGX design used in our paper can also be implemented on other trusted hardware platforms such as Keystone [28].\
 The secure and isolated regions of memory are called enclaves. Sensitive data and intermediate computations run inside enclaves to provide protection against outside programs. Besides, all the runtime enclave memories are stored in Enclave Page Cache (EPC) [101] and encrypted by Memory Encryption Engine (MEE). These protective mechanisms enforced in SGX protect memories against the access of any process outside the enclave itself, including the operating system, hypervisors, etc.\
. Sealing [88] is a process of loading enclave internal secret state to persistent storage. Roughly speaking, using the Sealing, the secrets can be encrypted and stored in the untrusted memory or disk. Further, it allows such encrypted secrets to be retrieved once the enclave is torn down (either due to the host’s power or the application itself). Sealing is achieved by using a private seal key [80], which covers two types of identities: Enclave Identity and Signing Identity. Enclave Identity is represented by the value of , which is a cryptographic hash of the enclave measurement. Any operation inside an enclave that changes measurement will yield a different key. Thus, it restricts the permission to sealed data; only the corresponding enclave can access the sealed data. In contrast, Signing Identity is provided by an authority and represented by . It provides the same sealing key for different enclaves, or even different versions of the same enclave. Therefore, Signing Identity can be used to share sensitive data between multiple enclaves produced by the same development firm.\
. Attestation mechanism [33] is used to prove to a validator that an enclave has been correctly instantiated and when in that condition can then proceed to further establish a secure, authenticated connection for the data transmission. SGX provides two types of attestation:  and . In the former attestation, SGX facilitates the instructions to help an enclave to attest to another enclave on the same platform. In the latter one, SGX enables an enclave to prove a correct loading of code and data to another enclave that resides in a remote platform.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Nvidia CEO Admits He Was Wrong About Quantum Computing</title><link>https://hackernoon.com/nvidia-ceo-admits-he-was-wrong-about-quantum-computing?source=rss</link><author>Thomas Cherickal</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:59:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This happened in March 2025, but I did not see anyone on this platform covering it.Late as it may be, this should be covered!Nvidia CEO Jensen Huang was a public face of quantum skepticism.He consistently framed practical quantum computing as a distant reality, suggesting it could be 15 to 30 years away.In classic Zuck style, Mark Zuckerberg echoed his sentiments, while admitting that he was not an expert on the matter (lol).These cautious views, voiced as recently as early 2025, had a tangible impact, causing significant drops in the market value of publicly traded quantum companies.Huang’s rationale was rooted in scale, arguing that the technology needed a million-fold improvement to become truly useful.I wrote an article about it:And I received validation on my views, but neglected to write about it.Oh well - better late than never!This public stance underwent a dramatic and strategic reversal on 20th March 2025.:::tip
Huang admitted he was wrong about the quantum computing timeline.The turnaround is not just a simple admission of being wrong, but a sophisticated pivot.On June 11th, 2025, he even said that quantum computing is approaching an “inflection point.”Nvidia and its CEO embraced the near-term reality that the future of high-performance computing is not purely quantum:But a hybrid model where classical and quantum processors work in tandem.This realization has shifted the company's strategy from "wait and see" to actively building and enabling this hybrid future.:::info
And - the Blackwell GB200 is leading the hybrid race!The cornerstone of Huang’s new strategy is the  software platform.Recognizing the immense success of its CUDA platform in creating a developer moat around its GPUs for AI, Nvidia is replicating the playbook.CUDA-Q is designed to be the essential software bridge that allows the millions of developers in Nvidia's ecosystem to seamlessly:Integrate quantum processing units (QPUs) into their existing classical computing workflows.:::tip
Crucially, CUDA-Q is hardware-agnostic.\
It supports QPUs from a diverse range of partners, including leaders like Quantinuum, QuEra Computing, and Quantum Machines.\
This brilliant move positions Nvidia as the indispensable software layer for the entire emerging industry.\
It ensures its relevance and profitability regardless of which specific quantum hardware technology ultimately wins the race!Massive Investments in Quantum Technology DevelopmentTo accelerate the hardware-software integration, Nvidia launched the Nvidia Accelerated Quantum Research Center (NVAQC) in Boston.This is a dedicated, high-profile initiative aimed at pairing its most advanced AI supercomputers, like the GB200 NVL72 systems, with cutting-edge quantum hardware.The center fosters deep collaboration with academic powerhouses like Harvard and MIT, as well as its commercial quantum partners.The explicit goal of the NVAQC is to use the power of classical AI to solve some of the most difficult challenges in quantum computing.This includes developing more efficient algorithms and, most importantly, tackling the critical problem of quantum error correction.:::tip
This creates a powerful symbiotic relationship: Nvidia's AI helps make quantum computers better, and those improved quantum computers will, in turn, accelerate AI.And perhaps the most definitive signal of Nvidia's new quantum ambition is its direct investment in hardware.The Choice of the Startup: PsiQuantumIn a landmark move, the company entered advanced talks to take a significant stake in .PsiQuantum is a startup pursuing a challenging but potentially revolutionary photonic approach to quantum computing.This is Nvidia's first major direct investment in a quantum hardware company, a tangible financial commitment to hastening the arrival of fault-tolerant systems.:::info
The choice of PsiQuantum is telling.The startup has one of the most ambitious goals in the industry: to build a million-qubit, fault-tolerant computer before the end of the decade.By backing this effort, Nvidia is not just hedging its bets.It is actively investing in a potential quantum leap, moving beyond simulation and software to directly funding the physical construction of next-generation machines.NVIDIA's GB200 Grace Blackwell Superchip represents a game-changer in the classical simulation of quantum systems.It provides researchers with a powerful platform to model quantum computers at an unprecedented scale.This capability is critical for the current NISQ (Noisy Intermediate-Scale Quantum) era.:::tip
It allows for the rigorous testing and debugging of complex quantum algorithms and error-correction codes long before fault-tolerant hardware becomes available.The massive, unified memory pool of the GB200 is particularly advantageous for quantum simulation, as the state vector of an n-qubit system requires 2^n complex values.This architecture mitigates the memory bottlenecks that typically constrain high-qubit simulations on distributed systems.Researchers can now simulate ideal, noise-free quantum circuits with a higher number of qubits than previously feasible.They are establishing clean performance baselines for algorithms destined for future fault-tolerant machines.The U-Turn in Nvidia’s StanceThis strategic activity has also been matched by a complete change in Jensen Huang's public views.His tone has shifted from long-term caution to near-term excitement.At recent company events, he has declared that quantum computing is nearing an "inflection point" and that the community is "within reach" of solving meaningful problems.This new narrative, coming from one of tech's most influential leaders, has injected fresh optimism and validation into the entire quantum sector.And with CUDA-Q, he has created a moat around QPUs, even before they are commercially viable.:::tip
As I said, this is genius. A truly masterful stroke by Nvidia, hoping to repeat their GPU success with QPUs.Nvidia's quantum strategy is a masterclass in corporate pivoting.While still acknowledging the long road to a standalone quantum computer:Huang has expertly positioned his company to dominate the lucrative and more immediate hybrid quantum-classical market.Essential software bridge (CUDA-Q)AI-driven research center (NVAQC)Strategic hardware investment (PsiQuantum)Nvidia is ensuring that all roads in the quantum future will be paved with its technology.And that elusive goal of true quantum advantage has just got a huge performance boost.Kudos to Jensen Huang and Nvidia!:::info
Pictures were AI-Generated by the author with NightCafe Studio on this link: https://creator.nightcafe.studio/:::info
Google AI Studio was used for research and outlining this article, available at: https://ai.google.dev/aistudio]]></content:encoded></item><item><title>Decentralization Won&apos;t Save Us From Ourselves</title><link>https://hackernoon.com/decentralization-wont-save-us-from-ourselves?source=rss</link><author>enibabyholmes</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:57:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The first thing that clicked in my mind when I first heard about the idea of a decentralised internet was politics. Centralized power, decentralized power — we’ve been doing that dance in government for ages. So yeah, I could get the idea on a surface level. Spread the power, let more people have a say, fair enough. But when I tried to picture what that would actually look like online, I had too many questions. Not because the idea is too deep, but because people will somehow turn the simplest ideas into big issues and no matter how fine or freedom-loving an idea sounds, loopholes always find a way to squeeze in. So that's why this particular topic stood out to me because I love poking holes in big, shiny ideas and this one? Its full of themSo apparently, a decentralized internet is this utopian idea where no single person, government, or company runs the internet. No MZ of Meta. No Buhari crashouts. No government shutdowns. No invisible hand deciding what you can or cannot post. Sounds poetic. But if my experience as a human being has taught me anything, it’s that power doesn't die,  it shapeshifts and man is addicted.Even if you remove today’s internet central figures, new gatekeepers will surely rise. Maybe it won’t be governments, maybe it won’t be as defined as Facebook, but somebody somewhere will build systems within systems. Lets not imagine a fluid jamboree.Remember when Nigeria banned Twitter because AMERICA threatened the country’s sovereignty or shall I say, hurt the president’s feelings? People screamed DEMOCRACY & FREE SPEECH, but let’s be honest; when has  ever stopped governments from doing what they want? Even the world’s so-called freest democracies censor what they don’t like. Think America! Test them and see how quickly you’ll learn about censorship. in the same vein is decentralization.And let’s not even get started on how dangerous decentralized spaces can get. Imagine a hidden corner of the internet run by extremists, or scammers building ponzi schemes or pushing fake NFTs (those pixelated monkey pictures, yeah those) with zero regulation. No regulation means no regulation means no accountability.So can a decentralized internet eliminate censorship? Maybe. But will new forms of control spring up? Definitely. Because the problem isn’t the system, it’s the people who make up the system. Give them power, and they will find new, creative ways to misuse it.There might be a decentralization revolution but the thirst for control will always stay stubbornly human.]]></content:encoded></item><item><title>Success Metrics in PR: How to Measure Campaign Effectiveness in IT</title><link>https://hackernoon.com/success-metrics-in-pr-how-to-measure-campaign-effectiveness-in-it?source=rss</link><author>Valeria Mingova</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:55:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In the IT industry, PR is not just “PR for the sake of PR”—it’s a business growth tool.]]></content:encoded></item><item><title>A Cry For Help: The Urgent Need For Spacetech Now</title><link>https://hackernoon.com/a-cry-for-help-the-urgent-need-for-spacetech-now?source=rss</link><author>QUIPTIK</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:54:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Earth’s collapsing, but space could save us. From climate monitoring to global internet, solar power to censorship-free networks—Spacetech is our best shot at survival. We don’t need more billionaires in orbit. We need humanity up there.]]></content:encoded></item><item><title>Own or Be Owned: Why User-Owned Agents Are the Future</title><link>https://hackernoon.com/own-or-be-owned-why-user-owned-agents-are-the-future?source=rss</link><author>Hunter Thomas</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:49:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Today, our online lives run on systems we don’t control. We rent our money from banks. We rent our AI from cloud services. We rent our communication from platforms that monetize our every move. Behind all the innovation lies a creeping dependency on centralized actors - tech giants, financial institutions, and opaque algorithms that decide more about us than we’d like to admit.However, a new kind of digital resilience is rising - user-owned agents. These aren’t bots or gadgets. They are flexible, autonomous systems built on open software, cryptography, and artificial intelligence. They’re restoring power where it belongs: in the hands of individuals.Let’s consider how this change unfolds along three important dimensions - crypto, AI, and open-source software - and why this moment is far more important than many folks acknowledge.Crypto: Own Your Keys, Own Your CoinsThe creation of Bitcoin was never merely a financial experiment. This was a philosophy on the move: that people ought to be able to own value without having to seek permission. With crypto, you don’t require a bank’s blessing to send money. You don’t need a loan officer’s approval to build wealth. If you hold the private keys, the coins are yours - it’s as simple as that.It’s more than just a lifeline for the unbanked. It’s a hedge for all of us in a time of censorship, inflation, and data surveillance. It’s no surprise that crypto natives are the first to understand the value of self-sovereignty in other areas, too.We’re now seeing decentralized finance (DeFi) tools that operate without intermediaries, allowing people to save, invest, and lend autonomously. Some go even further, integrating automated agents that monitor yield, rebalance portfolios, or execute strategies based on preset logic. These systems don’t sleep, don’t discriminate, and don’t call home to a central server. They’re coded independence. Crypto gave us a way to own our money. Now, it’s time to own our intelligence.AI: Own Your Weights, Own Your BrainAI is quickly becoming the second brain many of us rely on. From generating content to managing tasks, we’re offloading more of our mental labor to intelligent systems. However, here’s the problem: most of that intelligence is rented.The models that power your AI assistant - their “weights” (AI’s equivalent of private keys) - live on servers you don’t control, trained on data you probably didn’t authorize, running code you’ll never see. If the provider changes the terms, locks you out, or tilts the outputs to serve advertisers, you’re out of luck.That might be tolerable when it comes to drafting emails. However,  when AI is handling your finances, health decisions, or work, trust is non-negotiable. That’s why user-owned AI matters.Imagine spinning up a local language model on your device. You train it with your preferences. You own the weights. It doesn’t phone home, doesn’t track you, and doesn’t disappear when your subscription lapses.Open-source AI projects are making this happen. Developers are adapting big models for edge use while communities are sharing improvements transparently. It’s not science fiction - it’s happening. In this version of the future, AI is less of a black box and more of a personal co-pilot.Open Source: Own Your Code, Own Your DestinyBeneath both crypto and AI lies the most underrated pillar of all: open-source software. If crypto gives us financial freedom and AI gives us cognitive freedom, open source gives us the blueprint for both.We don’t talk about it enough, but open code is what makes transparency possible. It’s how you verify what a tool does. It’s how you ensure there’s no spyware baked in. Linux, for example, powers much of the internet not because it’s the flashiest system, but because it’s trustworthy. It’s maintained by a global community. It can’t be turned off by a single company.Projects like Mastodon and Signal prove that open software can scale, serve millions, and remain free from corporate influence. Or look at the Mech Marketplace, where Olas agents are hiring other agents to perform tasks, such as doing research or generating content. These are self-sustaining economies - networks of digital workers managed by humans, not companies. You don’t have to be a coder or a crypto expert -just someone seeking to take control.As AI agents become more integral in our lives, from home automation to autonomous trading, open code will be the difference between freedom and servitude. estimates generative AI could add $4.4 trillion to the global economy each year. However, if all that value is routed through Big Tech, most of us will still be digital tenants rather than owners.User-owned agents flip that script. They enable individuals to capture the benefits of their own data, decisions, and digital labor. They’re not just AI that works for you - they’re AI that belongs to you.The future won’t be built in walled gardens. It’ll be shaped by people who own their tools, not just rent them.So start small. Download a crypto wallet. Run an open-source assistant. Explore open-source platforms. Each action chips away at digital dependency and builds toward a more self-sovereign future. In a world where everything is automated, surveilled, and paywalled, owning your agent may be the most radical thing you do.]]></content:encoded></item><item><title>How Blockchain Can Be Used to Disrupt Modern Finance Systems</title><link>https://hackernoon.com/how-blockchain-can-be-used-to-disrupt-modern-finance-systems?source=rss</link><author>Jack Limebear</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:48:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In January 2016, Credit Suisse released a report that examined the market implications of blockchain, which, back then, was only just outlining itself as an emerging technology. The report found that among thought leaders, the industry on which blockchain was destined to have the largest impact was finance, with 77% of executive-level respondents interviewed pinpointing this industry.The connection between finance and new technology is hardly surprising, considering that the finance industry has been the fastest adopter of new technology since the mid-1990s. Even only one year after Credit Suisse’s study was released, banking institutions around the world had already invested over 1 billion USD in blockchain, with this investment having only exponentially grown since then.With ample investment, advanced technology, and the active potential to completely disrupt the structure of modern finance through its application, blockchain is currently the word on everyone’s lips in the world of financial systems.How Can Blockchain Achieve Such Impressive Innovations?There are several network types when it comes to the structure of modern blockchain. What began with decentralized ledgers has evolved, with these new changes bringing further utility. While many within the blockchain community understand blockchain as a universally available system in which absolutely anyone can read and submit transactions, there is actually more than one type of blockchain system that can be, and has been, constructed.\
In the public blockchain, which is the more common system, pseudonyms are allocated to both network participants and transactions, allowing for privacy and the detachment of public information and private data. But, when dealing with private financial data, this method goes against a whole host of regulations, posing an impossibility for financial systems.That’s where private blockchains come into play, which alternatively employ cryptographic user identification to align with KYC financial regulations. This design feature of private blockchain is significantly more appropriate for existing within the complex world of modern finance, allowing for blockchain to not only become a regulatory possibility within modern finance systems but also a technology that could be put to good use.\
But, simply recording data is not enough utility to provide any major shift within modern finance. Alongside the ability to record data permanently, the algorithmic technology that underpins blockchain, known as smart contracts, allows for the enforcement of contractual agreements. The tandem use of these systems allows modern financial institutions to bypass the need for traditional intermediaries in transactions.This stems from the fact that smart contracts can self-execute once a range of criteria have occurred, with this technology having the potential to eliminate any manual processes within modern banking. From trading and lending to currency exchange, any financial movement could be automated, dramatically reducing the cost of verification, increasing speed, and boosting the efficiency of the industry as a whole.Potential Applications of Blockchain Within Modern FinanceIf financial institutions were to adopt blockchain and the smart contract system that it entails, they could effectively automate a huge proportion of the most resource-strenuous processes currently conducted in the world of finance.As blockchain could offer exact recording, instant transactions, and condition-specific contracts, the potential applications are fairly limitless. Blockchain businesses are already engineering a larger social presence, making the brilliance of their products known by working with marketing agencies and freelance blockchain writers to spread information about their businesses.To exemplify the extent to which blockchain technology could radically shift modern financial systems, we’ve focused on five uses with a documented and dramatic impact.Each of these advancements is accompanied by real-world examples of the practice in action, helping to further demonstrate that blockchain is already here innovating the system.Let’s break these down further.Modern financial systems, although operating continually, are fairly ineffective. Their cumbersome systems are riddled with many customer fees and intermediaries, which slow down the process while also increasing the overall cost to both financial institutions and clients.The arrival of blockchain to this system, through the use of interconnected private networks, would allow financial institutions to make instantaneous transactions with each other. Alongside reducing fraud risk, this would also significantly speed up transactions, especially within remittance payments.This latter point was pinpointed by HSBC and eight other financial institutions back in December of 2020, outlining the construction of a private blockchain as an effective way of carrying out regular transactions. With blockchain, the general efficiency of financial transactions would increase, drastically innovating an industry-wide legacy infrastructure that is fast becoming out-of-date.Post-Trade Settlements (Derivatives, Credit Default Swaps, Corporate Loans)In a recent market analysis, Deloitte outlined the post-trade settlement period as one of the least effective parts of modern finance. Skipping over the manual demand and resource allocation that security and regulatory audits take, the trade date + 3 days (T+3) time period is cumbersome for international trade.Blockchain aims to radically shift post-trade settlements, providing manual and enhanced audit and regulatory functions. Within the blockchain, as information is easily traceable and visible to all parties involved, the ability to resolve conflict and arrive at a resolution is greatly increased.The mass adoption of blockchain as a primary method of conducting post-trade settlements would automate the bulk of the process, decreasing resource expenditure and the cost of this service. Alongside this, it would reduce T+3 to an almost instantaneous transfer, again helping the modern financial system to increase its overall efficiency.As outlined by Evangeline Ducas in their economic journal article on ‘The Security and Financial Implications of Blockchain Technologies’, another area where blockchain can save time within modern finance is in security insurance and servicing. Currently, the industry standard for exchanging or issuing shares in private companies is typically through an often inaccurate manual effort.\Blockchain, with its permanent digital record of transfers amongst private parties, would allow this process to become completely automatic, as well as provide further documentation. With the permanence of blockchain, both accounting and auditing of this function become simple, reducing the need for huge expenditures in these areas.\Additionally, a movement to the blockchain would ensure that regulatory supervision is embedded into the process while also dramatically boosting the transparency of ownership within the finance industry. As these private networks are visible to those involved within them, modern financial institutions would be able to access and record private securities transactions with ease.This useful application of blockchain was already demonstrated as incredibly valid back in 2015, with the Nasdaq Linq blockchain ledger successfully conducting the above process.Modern trade finance works on a strict basis, only releasing funds once goods have been delivered. While this system works, to an extent, it can lead to disputes when the goods in question have limited visibility.As blockchain offers real-time visibility of data, this technology would ensure that all parties can actively bear witness to the shipment and logging of goods. With this in mind, smart contracts can be created that will release funds accordingly once goods are dispensed.Due to the advanced tracking, monitoring, and recording capabilities of blockchain, trade finance would become completely automated, significantly increasing the efficiency of this area of modern finance.A demonstration of this in action is already occurring within the tourism and hospitality giant, Airbnb. Although a peer-to-peer service, they enlist a third party to keep verifiable records of every single transaction made on the platform - mirroring a private blockchain. Once a transaction has been made from the renter to the host and registered within this third-party network, the booking and funds are then released.This is an example of a micro-smart contract in action, with the financial industry being able to put blockchain to use to rapidly modernize trade finance entirely.Any errors within regulatory compliance cost financial institutions a huge sum of money, whether that is in correction fees or in regulatory fines. By employing blockchain technology and automating compliance within transactions, KYC processes would become automatic, ensuring that full compliance is always ensured.With the movement away from public ledgers to private ones, the blockchain could also fulfill KYC without revealing any public information and infringing on data laws, mapping real-world identities onto cryptographic identities.\As more modern financial systems begin to adopt blockchain, the internal network could be expanded, providing the possibility for many financial parties to share encrypted IDs across their institutions. With this, the blockchain network could completely replace regulatory compliance within banking, helping to increase the efficiency of the process while saving money.Commenting on blockchain’s potential, McKinsey & Company foresees between  saved due to blockchain solutions annually.Final Thoughts On Blockchain’s Place in Modern Financial SystemsWhile still in a relatively nascent stage, the development of blockchain could represent a gravitational shift in how modern financial systems operate. With its unmatched tracing and the ability to automate, blockchain represents a technology that can radically innovate several areas of the seemingly outdated financial systems. \Many of the examples we’ve focused on are already in action, further proving the validity of blockchain and its power in this crucial moment of progress. Although there are still legislative and cooperative barriers to total incorporation, over the next 10 years, we’ll likely see blockchain become a permanent disruptive technology within financial systems.]]></content:encoded></item><item><title>A (late) Layman&apos;s Overview of the Technology Behind Apple&apos;s CSAM Detection</title><link>https://hackernoon.com/a-late-laymans-overview-of-the-technology-behind-apples-csam-detection?source=rss</link><author>Thomas Weilbach</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:43:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[More specifically known as the ftPSI-AD ( fuzzy threshold Private Set Intersection with Associated Data ) protocol.EDIT 2021/09/04: Apparently as I was procrastinating writing this article Apple has agreed to postpone the roll out of the system.Disclaimer: As the title suggests, this is not a cryptanalysis of the Apple CSAM cryptography. I’m not a cryptographer nor a cryptanalyst.In fact I have very little formal training in cryptography and cryptographic techniques - about as much as you’d expect from someone who has struggled with enterprise PKI’s, has done some professional penetration testing and has tried but failed to complete Cryptography I by Dan Boneh (it’s good, check it out).This is nothing more than my layman’s understanding and subsequent explanation of what I’ve managed to gleam from official and unofficial documentation (all sources cited).The term “crypto” where used in this article shall exclusively be used to refer to the original meaning: Cryptography -_-Why is this even a “thing”?For all of the controversy and hoo-ha surrounding the Apple CSAM scanning announcement, I have yet to see a basic explainer about how the privacy preserving crypto magic actually works. To be honest, I didn’t even think about the complexities of building such a system until my brother linked me to the following tweet.“LOL”, I acknowledged, “quite an insightful sh**post…how would all this work though?”Like millions of technology users every day I ignorantly glazed over the detail without giving the technology in question a second thought.Ignoring the semantics of the wording used in the tweet for a second - it is a completely valid question. How will Apple allow certain images to be “decrypted” while preserving privacy and security of other user content?Sidebar: For those of you who at this point look at this and think: “The fact that its encrypted implies that it can be decrypted - what’s the big deal?” don’t despair. The author of the tweet assumes you share their understanding of the expected privacy and security controls built into your iPhone and connected services. Let me try and explain.The assumption here is that the images you store on your iCloud account are encrypted in such a way that Apple, despite being in possession of all of these images, cannot actually see their contents as they are encrypted on-device with keys that are private to that device and/or account (I’m not sure that is actually true and that Apple ever provided that guarantee, but that seems to be the assumption is this tweet.) Here is a thread that speculates on the topic: https://news.ycombinator.com/item?id=28300035To paraphrase: “You can’t claim my stuff is private and encrypted… until it’s not. If you can decrypt it at any point without my input or consent it means it was never properly encrypted and under my exclusive control in the first place”.In fact, the core issue is nicely summarised in the first sentence of this paper published by Apple:The Apple PSI system solves a very challenging problem of detecting photos with CSAM content while keeping the contents of all non-CSAM photos encrypted and private.My initial response to my brother was a wild guess that basically boiled down to this:“There is not a lot of detail here and we’ll have to take the statements at face value, but there are fancy crypto-magic ways of encrypting things where more than one key can decrypt it.I suppose there is a machine learning algorithm running on the phone that classifies these images where they are necessarily in plaintext (or a decrypted state), flags them based on some fancy heuristics and then encrypts it (a copy or the same one) under another key which Apple controls and can simply decrypt server side”Oh, was I wrong - it’s way more involved and so much cooler than I could have even imagined. Apple really did go out of their way to try and preserve user privacy. Unfortunately, it’s not a perfect system - none are - but more on this later.This is a good time to note that I’m going to steer clear of commenting on  non-technical aspects of the system.The ethics and morals of such a system including the “slippery slope” argument has been discussed ad-nauseam (for all the right reasons).I do not feel I can meaningfully contribute to that discussion any further, nor do I want to in this article. I want to maintain an unsoiled and pure focus on the cool tech by putting on the blinders of moral ignorance. These blinders are temporary though and no technology with such potential impact can be appreciated in isolation only - as much as we’d like to ignore it - there is an inherent link between cool technology with the power to impact society and our moral obligation to ensure it’s the right thing to do when abuse of it possible.Usually at this point (at the time of writing) I’d go searching for clarity, but was quite disappointed when I couldn’t find anything (at least not anything a mere mortal like me could understand). My usual resource for giving eloquent, if a bit contrived, explanations of complex systems @sggrc was also, unfortunately, silent on the matter. To his credit he did mention in Episode 831 of Security Now that this is potentially an area he’d dive into later - at the time of writing this post however I’ve not seen such a deep dive yet.So, my deep dive started…For the avoidance of doubt this post looks exclusively at the crypto that underlies the CSAM detection system (ftPSI-AD) and does not at all touch on any component of the , an  designed to  when they send or receive sexually explicit content over Messages and AirDrop, system announced at the same time which seems to have caused confusion in some circles.Although this system is notorious for “client side scanning” - the source of much controversy - it is actually quite dependent on server side operations as well. Some components and processes run only on the client and some others only on the server. This separation of concerns is quite intentional and significant to the design of the overall system. If either the client or the server don’t do their bit (intentionally or accidentally) the whole system breaks down. I note this because the interdependency of the client and server (and their associated workloads) should be kept in the back of your mind as you proceed to read this article.Why is this post even needed?As I was working through the Apple Technical Overview document, all too often I found myself exclaiming:”But why…?” or “But how…?”.Respectively these questions can be translated to the following root causes:“But why…!?” = “Why are you doing this thing?” or “Why are you doing it in this way?” - Generally this indicated that I was missing or misunderstood a key piece of context from the threat model.“But how….!?” = “How is this possible?” or “How does this even work?” - Generally this indicated that I had lack of knowledge or misunderstood something fundamental about the underlying technology (mostly the cryptography)In following sections I’ll go component by component and break down how each works in much the same structure as the Technical Overview document , the difference being I’ll try and explain things in more detail where the authors assumed the reader had knowledge of the threat model or where they elided some technical detail for the sake of brevity as it relates to the cryptography.I’m hoping this extra context and “simplifying” of the problem will help others like myself understand a bit more about the system. Let’s start right at the beginning…Can’t talk about a NeuralHash without first understating what a bog standard hash is. I suppose technically it’s called a cryptographic hash function and the good ones have a few key properties we should quickly touch on for context:Skip over the this section to “So what is a NeuralHash?” if you know your SHA’s from your MD’sThey are  (The same input results in the same output - always)They are inherently information lossy (The output is of a static length regardless of the input which also implies that the output cannot be reversed to derive the original input)They are  (The probability of 2 or more different inputs generating the same output should be statistically negligible) Really depends on the use case.They are  (Even a seemingly insignificant change in the input should result in a significant difference in the output) - technically this last one is just an extension of the 1st and 3rd points but it’s worth while calling it out.If you consider these properties it is easy to see why these are often referred to as signatures or fingerprints. They become a unique representation of the input without disclosing the what that input was.So why hashes in the first place? As noted above the “fingerprint-ness” makes these things a great mechanism to accurately compare two things without actually knowing the exact properties of each. As an analogy we can use human fingerprints - given that you have two fingerprints lifted from different places ( and we assume that no two humans have the same fingerprints) we can compare the fingerprint to see if they match and reasonably assume that the same person made those without knowing anything else about the person (their name, height, eye color, interests etc.).This then is the key use case for cryptographic hashes and the reason they are the de facto solution for storing passwords where, for instance, a service provider needs to know that you provided the correct password at login without actually getting to see or store your password.Is the NeuralHash a cryptographic hash then? Well…No. It seems that NeuralHash shares only a single property with a cryptographic hash in that it is inherently information lossy - the input gets reduced to a fixed length output.A  is a lot more “fuzzy” and is actually referred to as a fuzzy hash since the algorithm is not strictly deterministic. Two different inputs can, and importantly should by design, be able to produce the same semantic outcome. There are not a lot of details on the inner workings of NeuralHash at the moment (it’s a bit of a black box) but some researchers have already reverse engineered the algorithm and managed to produce hash collisions ( 1st pre-image and 2nd pre-image collisions) which essentially means that someone can artificially produce images of their choosing that match the NeuralHash of a different known image. Basically like saying you can choose your own fingerprints to match that of a specific target.NeuralHash, by design, is a type of perceptual hash and the fuzzy-ness of it is inherent in the name. As long as the images are perceptually the same they should produce the same NeuralHash (fingerprint). This is a key design property because the use case here is to not have minor image changes (for instance changing the color of a single pixel, or even the color grading of the entire image) to result in a mismatch since this would make it trivial to hide CSAM from detection.Apple illustrates this intent quite clearly in their Technical OverviewIf you were to generate a cryptographic hash over those 3 images, all of the fingerprints would be vastly different.This fuzzy-ness is the solution, but also introduces a new problem - it’s fuzzy - and things that are semantically different can be detected as being the same image. For more on these issues please refer to the following two great articles which give a more detailed overview of the issue with perceptual hashes in this context:So what’s the deal with the “Blinded Hashes Database”? Why go through all that cryptographic acrobatics instead of just, you know, distributing irreversible hashes?Let’s back up a bit. The whole point here is to figure out if someone’s trying to upload  CSAM to iCloud without actually looking at their photos because, obviously, privacy matters. Also, Apple does not want to distribute CSAM or NeuralHashes of CSAM images that may leak the presence of a particular image(s) in the database (even if someone had a similar image).The Blinded Hashed Database therefore is database of NeuralHashes of known CSAM images. These hashes are generated from CSAM images that child safety organisations (like NCMEC in the U.S.) already know about. The key thing is that Apple doesn’t create this database themselves, because they’re not allowed to have those images in the first place. Instead, they rely on trusted child safety orgs in  to send in their image fingerprints. Apple only keeps the ones that show up in at least two different countries’ lists, just to make sure nobody’s slipping in something shady.But here’s the clever bit: before sending this hash database to user devices, Apple “blinds” it using cryptographic techniques. The blinding process is not encryption in the traditional sense, but it  cryptographically obscure the CSAM hashes using elliptic-curve scalar multiplication with a secret known only to Apple. This provides a one-way transformation that behaves like encryption for privacy purposes, but without a reversible decryption step.So now we have a collection of visual derivatives or verified CSAM images that a recipient cannot reverse or derive the original image of (even if in possession of similar images). Cool, so this means, based on a comparison (more on that shortly), your phone might generate a match but still doesn’t know  matched or Which segues nicely into…Private Set Intersection (PSI)Seems pretty self explanatory, right? Apple can only ever see images at the intersection of the sets A and B. A, for instance, being the photos you have on your device and B being the CSAM photos in the .The NeuralHash (more specifically the comparison of 2 NeuralHashes) is the determinant of A==B. If the result of the NeuralHash function (fNH) calculated over an image from A and an Image from B are the same (fNH(A) == fNH(B)) then these two images are considered a perceptual match.The Private Set Intersection (PSI) scheme is at the core of the privacy “preservy-ness” of the proposed system. It is the mechanism by which Apple can ensure they learn as little as technically necessary about user content (given the requirement to monitor for CSAM and having to see  content) and nothing more. This serves to protect both the user and thier privacy and gives Apple a convenient claim to ignorance - “Sorry we can’t give you these user images, FBI. We can’t decrypt them”.It’s not entirely clear why this is necessary since it does not seem that Apple is actually doing end-to-end encryption (E2EE) of images stored on iCloud. This means that as cool and novel as the PSI mechanism is, it seems…well….redundant. If there is no E2EE, Apple can potentially see all images uploaded to iCloud - it does not make sense that they would introduce a convoluted privacy mechanism that is immediately undermined by their lack of E2EE. The only reasonable assumption one can draw from this is that there are potential future plans to have E2EE on iCloud - but if that is the case Apple is suspiciously silent about such plans.Lets’s break down my ugly picture step-by-step to make life a little easier.Client-side (User’s Device)Receive blinded CSAM hash databaseThe device receives a blinded (cryptographically transformed) hash table of known CSAM NeuralHashes. These hashes are derived from perceptual hashes (NeuralHashes) submitted by at least two independent child safety organizations from different jurisdictions. The hashes are blinded using elliptic-curve cryptography and inserted at deterministic positions in the table based on the original (unblinded) hash.User selects or uploads a photoWhen a user takes or uploads a photo to iCloud Photos, the device computes a NeuralHash and a visual derivative (a privacy-preserving low-resolution version of the image).Perform PSI protocol lookupThe NeuralHash is used to determine a deterministic position in the blinded hash table. The device fetches the blinded CSAM hash at that position but cannot interpret or unblind it. Using this data and the NeuralHash, it computes a cryptographic header.The device creates a two-layer encrypted safety voucher:The inner layer encrypts the NeuralHash and visual derivative using a secret key known only to the device.The outer layer includes a secret share of the inner key and encrypts both this share and the encrypted image data using a derived key computed from the blinded hash and cryptographic header.This process ensures that only if the NeuralHash matches an entry in the CSAM hash database will the server later be able to decrypt the outer layer.Add synthetic vouchers (more on this later)The device may generate synthetic match vouchers (fake matches) that resemble real ones and register as matches on the server. These are indistinguishable from real matches and serve to obscure the actual number of matches before the threshold is reached.The photo and its associated safety voucher are uploaded to Apple’s iCloud Photos service.Server-side (Apple iCloud)The server receives and stores the images and their safety vouchers. It does not know which are real matches or synthetic matches and cannot decrypt any part of the voucher at this stage.Attempt PSI-based outer decryptionUsing the cryptographic header from each voucher and its own blinding secret, the server tries to decrypt the outer layer of the voucher. If the voucher corresponds to a real match (i.e. the image is known CSAM), the decryption reveals the secret share and the encrypted inner layer (NeuralHash and visual derivative). For non-matches, decryption fails, revealing nothing.Check match threshold (more on this later)If the number of successful outer decryptions (i.e. CSAM matches) reaches or exceeds the predefined threshold (e.g. 30), the server uses the collected secret shares to reconstruct the inner encryption key.The reconstructed key is used to decrypt the inner layer of each matched safety voucher, revealing the NeuralHash and visual derivative for matched images.Run secondary hash verificationThe server may apply a second, independent perceptual hash to reduce the possibility of false positives caused by adversarial manipulation.Visual derivatives of matching images are reviewed by trained Apple personnel. If the images are confirmed to be CSAM, Apple disables the user’s account and files a report with an appropriate child protection organization (e.g. NCMEC in the United States).The server cannot reconstruct the decryption key, and the safety vouchers remain encrypted. Apple gains no knowledge of whether any images matched or how many were close.I hope the text was slightly more understandable than my scribbles. Let’s more on…To me the concept, in principle, was easy to grok but the practical implementation a complete mystery and the Technical Overview did not do much to explain any of the cryptography that underlies this mechanism. Other documents did, but it was nothing that could be interpreted and understood by a mere mortal like me.This was probably one of my biggest “” moments.Back to the basic concept: Given an aggregate number of thing  above a threshold , you are enabled to reach outcome  by combining . Or more concretely: Given I have 7 (t) little rocks (X), I can combine them in a novel way construct a larger “rock” with enough mass to be thrown straight through a Cybertruck’s “bulletproof” window (O).In the case of the Apple Threshold Secret Sharing construct, that translates to: Given I have 30 (t) or more  I’m able to find a secret that allows me to derive a key that enables me to decrypt a subset of identified cipher texts (photo visual derivatives) ()Well no. Because if you have a dangerous level (or only a little bit) of knowledge about cryptography like I do, you’ll immediately recognise that it is never sufficient to have part of a key (symmetric or not) to enable the decryption of a cipher text e.g. Even if you have 95% of the key material of an RSA private key, that does not allow you to decrypt a cipher text encrypted under the associated public key. You need the whole thing or bust. Similarly for symmetric keys used for AES for instance, it’s never enough to have only part of a key.Acknowledged that given one has a significant portion of a key it’s technically feasible to brute force the rest of the key, but that is not the argument and Apple is not brute forcing parts of keys to decrypt cipher texts.This then bring us to the next logical question. Assuming the most basic setup where we have an AES symmetric key that is split up into Y number of  and randomly distributed amongst image  how likely is it to have received  t+1 distinct  (to be able to reconstruct the full AES symmetric key) given a random distribution of t+1 matched images? Without doing the math I can tell you that it’s very unlikely and clearly not how this system would function.The answer then was so much more fulfilling when I finally managed to track it down. It’s calledShamir Secret Sharing and the mathematical construct that allows it is called Polynomial Interpolation and was already proposed in 1979 by none other than Mr. A. Shamir himself.I’m not a position to explain the algebra (with any conviciton) behind this all but Mr. Shamir is, so for those of you so inclined:For the rest of us it boils down to: Given that you have enough (t - threshold) points (X - secret shards) on a plane, you can derive the Polynomial and find a specific previously unknown point (O - finding the secret) on the graph.I found this article very helpful, not only to understand the problem space, but also the evolution of the potential solution with a nice example of Polynomial Interpolation.Finally, you can try it out in practice in your browser here (super cool) - this site also links other very cool resources and videos (source of the image above).We’ll leave it at that then - there is a way to shard secrets in such a way that only a defined subset of shards are required to reconstruct the secret (the key) required to perform decryption.I’m suer Voldemort is also a fan of Secret Sharding.So why did Apple bother with this mechanism?The simplest explanation is that they expect some false positive matches to occur using NeuralHash (as discussed above) and by having a threshold in place they can protect themselves (or whomever they contract to manually validate matches) from arbitrarily gaining the ability to decrypt the images that result from singular false positive matches.They seem to the make the assumption that an amount of matches above 30 (see their justification for the number here) made up of all false positives is statistically unlikely to occur. This inherently also means that they deem a number of matches below 30 as being not significant, in terms of detection capability.In summary, this means that Apple will only ever be able to reconstruct a key to be able to decrypt safety vouchers that accompany uploaded images if a total of 30 or more images were matches (false positive or not). Additionally, when the protocol accumulates enough  to derive the key, that key will only enable them to decrypt matched images and nothing more - it is not a “skeleton key” for all things uploaded.Since writing this I’ve used Shamir Secret Sharing (or Secret Sharding as the cool kids know it) to distribute the master password to my password vault amongst confidants in case something happens to me and my family requires access to all my stuff.Synthetic Matches/Vouchers are just that - artificially generated  that are designed to look like legitimate matches against the  to a casual observer on the client or the server.Notably, the Safety Vouchers associated with  are empty and without substance. For instance they do not contain valuable  nor any actual image metadata or .In the paper Apple goes to some lengths to explain that all devices/accounts will generate a number of Synthetic Matches regardless of any actual CSAM being present on the device, but never makes an effort to explain why. Why would their protocol include a mechanism to create a bunch of noisy junk that muddies the water for both the client and server and wastes valuable CPU cycles?When looking closer at the Threat Model it becomes clear that the intent here is exactly to muddy the water and hide the number of actual matches from casual observers. This serves to protect both the client and server form leaking important metadata via. a side channel.Remember, any signal in a completely silent environment is significant.The problem they are trying to engineer around here, in principle, is the same as that of early TOR adoption. If only a few privacy conscious people use TOR, their unique traffic patterns stand out like sore thumb in an ecosystem of non-TOR traffic and their privacy objectives are inherently defeated. To be able to hide something distinct you need some similar looking noise to surround it.In this case however the objective is twofold:Prevent the client from knowing how many actual matches are detected on the devicePrevent the server from disclosing metadata about accounts that have had CSAM detected.Quite correctly the Threat Model considers an adversarial client (a baddy with an iPhone) who has an incentive to hide CSAM from detection. If such a client were able to observe the number of true positive matches accurately it leaks important metadata to them that would allow them to essentially test CSAM images against the protocol to find ones that evade detection.The premise here is that a malicious observer of the Apple services (rouge employees, local or foreign governments, auditors, nationstate hackers) who may be able to see nothing more than an accurate number of matches(since the images are encrypted) can learn important and compromising information about that account (that is has CSAM). In the wrong hands that single data point can be used to target and black mail individuals. This is exacerbated by the fact that there is a very real possibility of false positive matches being perceived as true positives and that may mean that even people with no actual CSAM may become the target of malicious actors or authoritarian governments.Further reading: Sarah makes a compelling argument for why the noise generated by Synthetic Matches is inadequate to achieve the stated privacy objectives in Obfuscated ApplesSo, there you have it. Simple, right?“Now draw me a picture, please!”.After writing the bulk of the article I came across this podcast where famous cryptographer Matthew Greene discusses this system with the hosts. It helped understand a bit more about the technical and non-technical problems with this system, the issues with roll out approach, and the efficacy of addressing the actual problem (preventing CSAM distribution).Some of these points we’ve already touched on in the article, but for clarity here is a summary of valid critiques (ones I agree with) of Apple’s CSAM system?It’s wildly over-engineered—for the wrong thing.Why build this complex, privacy-preserving, cryptographic monster right now if iCloud Photos isn't even end-to-end encrypted yet? If Apple can already see your photos on the server, why not scan them there like everyone else does (I’m not endorsing that idea btw.)? Instead, they built an entire client-side detection system that makes sense only if they plan to end-to-end encrypt iCloud in the future. But they didn’t say that. So… why the extra fuss?It only catches known CSAM images, and misses new abuse.This system only detects images that are already in the CSAM database. So if someone is creating new CSAM images, this system won’t catch them. It’s blind to anything not already tagged. Worse, you need 30 hits on known images before Apple takes action. So someone with one or two matches can just slip through.It’s high risk, low reward.Green’s take is blunt: “This system does very little to stop real abuse but introduces a huge amount of complexity and potential for misuse.” He says it’s the worst of both worlds:Very narrow detection (only known CSAM, only if you upload to iCloud), Massive surface area for abuse (runs on every iOS device with iCloud Photos).Here, I drew an un-magic quadrant for reference:4. NeuralHash? Kinda sketchy.Matt and others tore into Apple’s NeuralHash algorithm. They found it: Poorly documented Easy to reverse engineer Vulnerable to adversarial collisions (you can tweak an image and make it falsely match something else) So, he’s not thrilled about this thing sitting on every device and scanning everything before upload.The system is ripe for mission creep.Once this infrastructure is deployed on billions of devices, what’s stopping a government from saying, “Cool, now let’s scan for banned political memes”?Matt doesn’t buy Apple’s “we’ll never allow that” promise. He points out that Apple has already made concessions to governments (e.g., China), so what’s to say they won’t again?Apple dumped too much, too fast, with too little explanation.Super detailed docs about some parts (like PSI), and Nothing at all about others (like NeuralHash internals). He said it felt “improvisational,” like Apple was scrambling to address criticism instead of transparently explaining the system from the start.If Apple really cared about protecting kids, they could’ve started with: Better iMessage reporting tools, Scanning shared albums only (like other platforms), Watching metadata patterns to catch coordinated abuse. Instead, they pushed a giant surveillance system onto every photo upload.This feels driven by government pressure.Governments (like the U.S., UK) were pressuring companies to “do something” about CSAM. Apple jumped the gun. In Matt’s words, “Snapchat should’ve been the first to fold, but Apple got there first.”He’s convinced this system was built not just to fight CSAM—but also to preempt looming regulation. I tend to agree.Anyway, even though this was a complete dud from Apple’s perspective I got to learn soooo much about Privacy Preserving Crypto. Nice!]]></content:encoded></item><item><title>The Real Metaverse Was Never for Gamers — It Was for Patients</title><link>https://hackernoon.com/the-real-metaverse-was-never-for-gamers-it-was-for-patients?source=rss</link><author>Nargiz Noimann</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:40:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When Silicon Valley envisioned the metaverse, it promised us virtual shopping malls, avatars in luxury sneakers, and office meetings in cartoonish boardrooms. It was slick, noisy, speculative — and, ultimately, underwhelming. Investors chased hype cycles. Marketers chased Gen Z. But somewhere offstage, quietly and deliberately, the metaverse began to fulfill its real potential — not in the hands of gamers, but in the hands of patients.Because the true future of immersive tech was never about play. It was about healing.VR Is Not a Toy. It’s a Therapy Room.While the word “metaverse” has largely faded from headlines, virtual reality itself has quietly found a second life in the one place no one expected: clinics. Not game studios. Not entertainment platforms. Clinics. Rehabilitation centers. Oncology units.In the UAE today, VR-based emotional therapy is being actively tested as a structured part of post-treatment care. Patients who have finished chemotherapy, struggled through chronic pain, or faced cognitive decline due to illness are now using virtual environments — not to escape their experience, but to process it.This is not “digital distraction.” This is a designed confrontation. And it marks a major shift in how we think about recovery.Because medicine may treat the illness, but who treats the trauma?For years, immersive technology was chained to the entertainment sector. It was a medium built for dopamine: fast, competitive, and often disorienting. But healthcare saw something else. Something slower, quieter — and far more profound.Patients dealing with trauma, depression, cancer, or neurodegenerative diseases don’t need escape. They need structure. A safe environment. A controlled cognitive challenge. And most importantly, a sense of self returning after it has been fractured.Therapeutic VR — now being used in rehabilitation centers in Dubai, Abu Dhabi, and beyond — flips the script. Instead of bombarding the user, it invites introspection. In one example, a cancer survivor enters a calming virtual environment where they’re asked to identify their deepest fear, visualize it as two glowing spheres, and — using only focused thought — merge those spheres into one.It’s surreal. It’s symbolic. But it works. Not because it’s magical, but because it’s methodical.Cognitive psychology has long shown that symbolic visualization — especially in altered perceptual states — can support trauma recovery and anxiety reduction. What the headset does is scale that inner work into something structured and repeatable.Emotional Recovery Isn’t a Bonus. It’s the Treatment.Let’s be honest: we’ve neglected emotional and cognitive rehabilitation for far too long. After the tumor is gone, after the surgery is done, after the chemo has finished — what then?Many patients, particularly those who survive major illnesses, describe post-treatment life as a psychological void. Their physical symptoms improve, but mentally, they feel lost. Afraid. Disconnected from who they were. For Alzheimer’s patients, there may be no physical tumor at all — but there is an emotional disintegration that accelerates with every forgotten memory.This is where immersive therapy has a chance to redefine care.VR enables something traditional therapy cannot: safe simulation of internal states. Unlike Zoom-based cognitive behavioral therapy, immersive therapy doesn’t rely on verbal explanation. Instead, it creates a felt experience — where a patient can relive, reframe, and reset.In early-stage trials conducted in the UAE with cancer patients and chronic illness sufferers, researchers observed decreased anxiety and improved emotional regulation. This aligns with global findings — such as the 2023 Cleveland Clinic study, which found that VR interventions significantly reduced depressive symptoms in cancer survivors.And this is only the beginning.One of the most exciting trends in healthtech today is the convergence of neuroscience, behavioral data, and immersive design. In the UAE, hospitals and research centers are beginning to collaborate with psychotechnology companies to test VR-based interventions as part of formal rehabilitation.What’s being tested is not just the “wow” factor — but the measurable outcomes. Improvements in sleep. Reductions in cortisol. Enhanced memory recall. Lower incidence of post-treatment anxiety.What we’re seeing is a shift from abstract “wellness” to something far more clinically actionable.And unlike most digital therapies, this new generation of VR systems prioritizes offline use. Many are not connected to the cloud or internet — a deliberate move to protect neurological data. Privacy isn’t a footnote. It’s a feature.If this all sounds futuristic, it’s worth noting: the roots of this movement go back decades. Long before the metaverse hype, pioneers in psychotechnology were using simple computer games, biofeedback tools, and visualization techniques to help patients regulate stress and emotion.The difference now? The tech has finally caught up with the intent.Today’s immersive environments are more than pretty landscapes. They are personalized, responsive, and — in some cases — capable of integrating patient data like heart rate or neurofeedback to adapt the experience in real time. And it begs the question: why should rehab be limited to worksheets and group sessions when an immersive, intelligent environment can do what words often can’t?Let’s not confuse the failure of a marketing term with the failure of a technology. The metaverse, as imagined by tech giants, may have fizzled — but the infrastructure it birthed is finding its soul in healthcare.We don’t need more avatars in sneakers. We need environments that heal. Safe spaces to feel grief without breaking. Tools that allow the brain to rebuild trust in itself.The real metaverse won’t be a place you “log into.” It will be a therapeutic layer — woven into clinics, recovery centers, and maybe even homes. Not to isolate, but to reconnect. Not to impress, but to .And in countries like the UAE — where innovation in healthcare is matched by serious investment and policy support — this isn’t a distant dream. It’s happening now.We Got the Target Wrong. But the Future Is Still Ours.Maybe the mistake wasn’t in building the metaverse — but in building it for the wrong people. Gamers didn’t need it. Patients did.And now, slowly but surely, we’re beginning to get it right.]]></content:encoded></item><item><title>10 Growth Hacks Every Early-Stage Startup Should Try in 2025</title><link>https://hackernoon.com/10-growth-hacks-every-early-stage-startup-should-try-in-2025?source=rss</link><author>Kuldeep Patel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:39:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
By 2025, more startups than ever compete in the scene, and more opportunities can be gained. Through AI instruments, platform switch, and consumer behavioral change, a traditional marketing or word-of-mouth are no longer sufficient as a tool to earn early-stage startups. It is necessary to employ growth hacking it means trying different, low-cost methods of receiving attention.These are 10 growth hacks that have been discovered to be working in most early-stage startups, and should therefore be highly considered to have a go in 2025.🚀 1. Make Use of Micro-influencer Swarms Rather Than a Single InfluencerRather than buying big with one of the renowned influencers, collaborate with 20 to 50 micro-influencers that possess hyper-focused audiences in your area of interest. Each of them has less reach but the combined potential of trust and conversion is much greater.Find the correct micro-influencer with the help of the special platforms such as Collabstr, Upfluence or Trend.io.Lay emphasis on engagement rate, rather than followers.Take an active role in online communities We are not talking about brand but about person as an expert. Be curious and clever with their commenting with small things adding value (not a link!), which can develop interest and trust that can encourage people to come to your profile or product naturally. Reddit: r/startups, r/Entrepreneur, r/SaaS LinkedIn: comment boxes of industry leaders Hacker News: Submit to Hacker News through the “Show HN” or real tech talkDevelop a simple product, which delivers instant value and does not require a registration. Consider ROI calculators, ideas generators, corporate checklists or interactive quizzes. When the solution addresses one particular pain point, users would be willing to share it anyway. Embeddable widgets or one-click sharing. One should gather response or gentle leads upon use and not pre-use.📩 4. Twisted Cold Outreach: Loom VideosInbox is a battleground in 2025, but there is one approach that cuts through and that is fast, and personalized videos. Rather than attaching a cold email pitch, make a Loom video 30-seconds long that includes their site or product and why you are contacting them. It makes the message more humane. It seems like it is work (even when it is just everyday).💡 5. Offer an industry Swipe File or a Resource PackHuman beings are fond of shortcuts, particularly the professionals. Donate and when it is of great value, make it something such as: “50 Email Templates of B2B SaaS Outreach” Ultimate Startup Pitch Deck Collection (2025 Edition) Popular Social Hashtag Vault on Tech Startup Create an option to download it in exchange of an email or a referral.🎥 6. Video-First Content RepurposalBy 2025 the rule will be video, but you do NOT have to have a complete studio. Make reels out of blog posts. Customer testimonials should be turned into TikToks. Convert text into a catchy voiceover using an AI application such as Descript or Pictory and adding movement graphics. Make each video not more than 60 seconds. Do not mix too many ideas within one clip. Include text and company logos.SEO is not dead, it changed. Find the most popular content of the competitors using such tools as Ahrefs, Surfer SEO, or Semrush. Then: Make cluster topics on such keywords. Improve your pages on search intent and internal linking.Combine with programmatic SEO to scale-out.Thousands of niche visitors can be overnighted by being mentioned in newsletters. Contact custom newsletters such as: HackerNewsletter BetaList / Product Hunt upcoming Give a brief classified pitch and link, or give a guest snippet.Do not make it a mere offer of sharing to get five dollars offer. Provide levels of incentives: Post one time: receive a discountReceive 3 referrals: unlock bonus deals Get 10: get a branded hoodie or 1-on-1 call Take advantage of such tools as ReferralCandy, Viral Loops, or SparkLoop.🧠 10. Simulate, test and optimize before launch with AITweet about: Simulate before money goes into campaigns, by using AI testing tools:Segmentation of audience (through chatbots or personas) Tools such as Copy.ai, Unbounce Smart Traffic and Maze can be used to pre-validate your development actions.In 2025 startups have to be quick-witted, not just quick. These growth hacks are not silver bullets, they provide leverage, but when wielded insightfully, creatively, and disciplined. Test relentlessly. Maintain good things. Drop what won. t. Above all, concentrate on providing the actual value to the actual people. That is still the best growth hack.]]></content:encoded></item><item><title>Harmony Over Imposition: What Japanese Aesthetics Taught Me About Building Software Products</title><link>https://hackernoon.com/harmony-over-imposition-what-japanese-aesthetics-taught-me-about-building-software-products?source=rss</link><author>Alessa Cross</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:30:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Some products don’t need to announce their presence. They don’t scream “Look what I can do” or require a guided tour to be useful. They simply fit. These types of products integrate into a user’s flow seamlessly, and while there is often a brief onboarding process or learning curve, it’s not steep because these products feel so intuitive.I grew up in Tokyo surrounded by this philosophy. In Japan, design is rarely about standing out but about fitting in with intention. In daily life, the products and systems I was surrounded by prioritized rhythm, predictability, and deference to context. Of course, not everything is quiet. Trains pull into the station playing music, crosswalks have their distinct chirp, and vending machines light up the streets. But everything operates with a kind of spatial and temporal modesty. Passengers stand in a single file line when train doors open, and cashiers place change on a tray to preserve a respectful distance with the customer. Everywhere, you see its philosophy in motion. It’s a dance of gaps and invisible cues that allow things to move smoothly.This sensibility has shaped how I think about software. In my experience building complex systems for high-stakes operational environments, this goes beyond pure design sensibility. When the stakes are high and multiple workflows are layered, the best products are the ones that don’t compete for attention but reinforce clarity amongst all the noise. The kind of quiet, ambient design that respects the user’s attention and anticipates their needs without announcing its intelligence.In traditional Japanese aesthetics, negative space between objects is an important concept. It’s reflected in almost every aspect of Japanese design and can be seen in the deliberate irregularity of placing one branch in a vase instead of a full bouquet. Rather than an absence, it presents space that allows everything else to breathe.This idea is deeply relevant to designing products. Too often, we design from a state of overcompensation or excess: packing every pixel, surfacing every feature, chasing engagement. But this visual and feature density can be suffocating. In contrast, a product that embraces negative space signals confidence by trusting and creating room for the user.Tools like Raycast illustrate this beautifully. It lives in the background of the user’s system, invoked only when needed, surfacing functionality with little visual intrusion. Even visually rich tools like Linear exhibit restraint with intelligent defaults like automatic light and dark modes that adapt silently to the user’s OS preferences. These products conform to the user, not the other way around, and do so quietly.Even in productivity tools like Superhuman, absence becomes a design asset. Their keyboard shortcuts are easy to remember, and more importantly their interface is minimal upon first glance. Rather than overwhelming the user with options the tool remains minimal by default, surfacing power only when summoned. This restraint allows for customers to feel in control. The user can lead, and the product will follow without friction.On the other end of the spectrum, consider the contrast in early versions of Google Docs versus modern-day Notion. While both offer collaborative writing, Google Docs surfaces its full range of editing and formatting options upfront. Notion, by contrast, starts with a blinking cursor and an empty canvas, gradually revealing its layers of complexity through interaction. This is negative space in action: letting structure emerge, not impose.This kind of restraint doesn’t mean doing less by any means. Rather, the goal is to do just enough, with precision. It means designing affordances rather than demands, and prioritizing coherence over visibility.Good software doesn’t dominate the screen or the user's mental bandwidth without justification. It appears only when needed and exits without residue. These types of tools might even anticipate your intent and blend into context rather than demanding it.In Asia, where superapps dominate and mobile-first is the norm, this is especially important. Products like WeChat or LINE handle everything from payments to messaging to booking appointments within dense but learnable interfaces. If your product doesn’t fit that structure, or worse, disrupts it with clunky overlays or unfamiliar behaviors it doesn’t only feel foreign, but feels broken. Unlike in the West, where single- purpose apps often exist in silos and compete for attention, the expectation in much of Asia is for products to nest within larger ecosystems. Good design should account for this difference from both a cultural and technical lens.Quiet design might ask: does our feature stack smoothly on top of the system, or does it make the user stop, adjust, and rethink how to act? Perhaps your keyboard shortcuts are conflicting with local typing habits. Are your gestures or UI patterns aligned with regional fluency?Even products built for maximal engagement, like TikTok, demonstrate this principle. There’s no elaborate onboarding, no walls of UI. You open the app, and it adapts to you. Its intelligence lies in its restraint. It doesn’t demand that you learn it, because instead, it learns you.Some apps demonstrate this restraint in how they implement their theme modes: great software won’t ask whether you want light or dark. It simply checks your system settings and adjusts. It's a small gesture, but it signals an important truth: this product knows its place.Designing quiet software means designing for fluency, not just function. It means understanding the ambient cognitive load of users across cultures, devices, and moments of attention. The goal isn’t presence, but quiet adaptation.Harmony as a design ethicHarmony, after all, isn’t passivity. It’s a discipline of restraint. Japanese design teaches us that beauty often lies in what’s left alone. That stillness can be a kind of feature, and that trust can be a kind of interface.As product builders, we can choose to impose or we can choose to harmonize. Great interfaces do not ask to be seen, but quietly become part of the way we already operate. The goal isn’t minimalism for its own sake, but coherence and alignment.]]></content:encoded></item><item><title>AI job predictions become corporate America’s newest competitive sport</title><link>https://techcrunch.com/2025/07/02/ai-job-predictions-become-corporate-americas-newest-competitive-sport/</link><author>Connie Loizos</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:30:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In late May, Anthropic CEO Dario Amodei appeared to kick open the door on a sensitive topic, warning that half of entry-level jobs could vanish within five years because of AI and push U.S. unemployment up to 20%. But Amodei is far from alone in sharing aloud that he foresees a workforce bloodbath. A new […]]]></content:encoded></item><item><title>Designing Scalable Internal Tools: Lessons From the Frontlines of Ops Engineering</title><link>https://hackernoon.com/designing-scalable-internal-tools-lessons-from-the-frontlines-of-ops-engineering?source=rss</link><author>Alessa Cross</author><category>tech</category><pubDate>Thu, 3 Jul 2025 05:26:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Many early-stage companies rely on spreadsheets and ad-hoc dashboards to run mission-critical workflows. These tools can take a surprisingly far. But as internal operations scale and complexity increases, their fragility of becomes apparent. Missed updates, untraceable decisions, and fractured workflows are some common consequences of relying on rapidly assembled systems. This article explores how to build resilient internal infrastructure, from production variable frameworks to structured task allocation, drawing inspiration from best practices at both high-growth environments and more mature engineering orgs.Managing Operations in High-Stakes FieldsSpreadsheets and communications on Slack often power a startup’s internal operations. These tools are incredibly powerful when used correctly, and offer the speed and flexibility required to iterate rapidly. As scale increases though, these piecemeal systems can begin to fray. Missed updates and untraceable decisions become recurring issues. What was once built with speed in mind becomes an obstacle. Without audit logs and visibility into what each employee is doing, operational complexity becomes increasingly harder to manage. This leads to wasted time and inconsistent execution. Managing this complexity is especially important for companies whose bottom line depends on operational efficiency. Particularly in high-stakes fields like logistics or healthcare, success hinges on timely delivery and transparency. In order to scale internal operations successfully, teams must move from improvisation to more thoughtful planning around the tools that support their daily workflows.Automated Task AllocationIn operationally intense environments, dynamic task allocation systems are one of the highest-leverage tools a team can implement. In the early stages of a company, assignments may be managed manually through spreadsheets and Slack threads. Someone might triage the issue, update a shared doc, and ping the point of contact. As volume increases though, this ad-hoc model introduces risk: duplicate work, missed tasks, and inconsistent prioritization due to miscommunication. There is no central source of truth, no clear audit trail, and no way to systematically enforce business logic.We can replace this with an event-driven task routing system. After each event, such as an approved request or update in status, we can create a task object (typically just a database row) with attributes like status, priority, or assignee. These tasks can be prioritized based on urgency and historical status, and eventually routed to the right person. The map of this entire operational system can be surfaced and tracked in Retool. Automatable tasks can be handled programmatically, while those requiring human input are escalated to the right team member based on role and availability.By codifying this routing logic (i.e. retries, escalations, and task dependencies), we can: Ensure urgent issues are addressed first Track resolution history at a granular level and in real-timeThis orchestration layer allows operators to act with confidence, knowing that what lands in their queue is clear, scoped, and relevant. Employees no longer need to dig through spreadsheets or justify the work they were doing.Adding Monitoring and Feedback LoopsAnother advantage of a centralized, traceable task routing system is how relatively easy it is to layer monitoring and analytics on top. Executives can monitor company operations from a birds-eye view with just a few lines of SQL. In such human-in-the-loop environments, simply having a queue or backlog is not enough. We need analytics on these backlogs themselves to assess where work is stalling, how long certain tasks have been on standby, and whether output is keeping up with input.Every task can be appended to a table, and any changes made to the task’s status can also be logged in a separate records table to keep track of its entire lifecycle. Retool makes it incredibly easy to build dashboards and visually represent the results of the queries we care about.Alerting mechanisms are also essential to improving operations. Tasks stuck in error states or in the backlog for over a certain threshold of time can be hooked up to trigger Slack alerts. This layer creates a shared language between operators, executives, and engineers.Making Configuration Accessible To AllBusiness critical configurations are often gated behind code. This is a real issue as a company scales and the divide between engineering and other orgs widens. Escalation thresholds, copy to A/B test — these are all variables that any member of the company, such as those working on Growth or Product, may want to adjust. Yet changing them often entails asking an engineer to step in, push code, get a code review, and then deploy. It’s not uncommon for non-engineers to become blocked by engineers in this manner. Even executives may lack the visibility or controls to influence these variables themselves.We can build a config management system to address this. This interface, once again an internal tool surfaced through Retool, serves as a single source of truth for production variables and enables all members of the organization to not just view but edit them in a matter of seconds. Instead of burying business logic deep inside source code, obscured away from everyone but engineering, variables can be stored in a Postgres table. These values can be modified safely by authorized individuals via a visual interface. The UI itself can include whatever input validation and type checking necessary to ensure nothing problematic gets pushed. Error handling can be instrumented on the backend as well.By externalizing this business logic, teams are able to make instant updates to their software without having to push code and experiment rapidly. Engineers no longer need to gatekeep trivial updates, and can redirect these hours towards higher-value work.Although speed is one of the few advantages a startup has, this does not necessarily mean teams must move in a makeshift manner. Building scalable infrastructure entails not only creating tools that enhance transparency, but also make technical systems legible to non-technical peers. Internal software is a real product as well, and should be treated as such. Investing early in these projects and ensuring they are robust is critical as a company and its complexity grows.]]></content:encoded></item><item><title>French B2B fintech Qonto reaches 600,000 customers, files for banking license</title><link>https://techcrunch.com/2025/07/02/french-b2b-fintech-qonto-reaches-600000-customers-files-for-banking-license/</link><author>Anna Heim</author><category>tech</category><pubDate>Thu, 3 Jul 2025 04:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Qonto, which targets European freelancers and SMBs, currently operates with a payment institution license it obtained in 2018.]]></content:encoded></item><item><title>Data Breach Reveals Catwatchful &apos;Stalkerware&apos; Is Spying On Thousands of Phones</title><link>https://yro.slashdot.org/story/25/07/03/0023253/data-breach-reveals-catwatchful-stalkerware-is-spying-on-thousands-of-phones?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: A security vulnerability in a stealthy Android spyware operation called Catwatchful has exposed thousands of its customers, including its administrator. The bug, which was discovered by security researcher Eric Daigle, spilled the spyware app's full database of email addresses and plaintext passwords that Catwatchful customers use to access the data stolen from the phones of their victims. [...] According to a copy of the database from early June, which TechCrunch has seen, Catwatchful had email addresses and passwords on more than 62,000 customers and the phone data from 26,000 victims' devices.
 
Most of the compromised devices were located in Mexico, Colombia, India, Peru, Argentina, Ecuador, and Bolivia (in order of the number of victims). Some of the records date back to 2018, the data shows. The Catwatchful database also revealed the identity of the spyware operation's administrator, Omar Soca Charcov, a developer based in Uruguay. Charcov opened our emails, but did not respond to our requests for comment sent in both English and Spanish. TechCrunch asked if he was aware of the Catwatchful data breach, and if he plans to disclose the incident to its customers. Without any clear indication that Charcov will disclose the incident, TechCrunch provided a copy of the Catwatchful database to data breach notification service Have I Been Pwned. The stalkerware operation uses a custom API and Google's Firebase to collect and store victims' stolen data, including photos and audio recordings. According to Daigle, the API was left unauthenticated, exposing sensitive user data such as email addresses and passwords.
 
The hosting provider temporarily suspended the spyware after TechCrunch disclosed this vulnerability but it returned later on HostGator. Despite being notified, Google has yet to take down the Firebase instance but updated Google Play Protect to detect Catwatchful.
 
While Catwatchful claims it "cannot be uninstalled," you can dial "543210" and press the call button on your Android phone to reveal the hidden app. As for its removal, TechCrunch has a general how-to guide for removing Android spyware that could be helpful.]]></content:encoded></item><item><title>TN Govt. Saves School Children From Smut Like Magic Tree House, Calvin &amp; Hobbes, &amp; A Light In The Attic</title><link>https://www.techdirt.com/2025/07/02/tn-govt-saves-school-children-from-smut-like-magic-tree-house-calvin-hobbes-a-light-in-the-attic/</link><author>Dark Helmet</author><category>tech</category><pubDate>Thu, 3 Jul 2025 02:43:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Book bans are all the rage these days, as you likely well know. Far too many people, and folks in government more importantly, seem to have read Ray Bradbury’s  not as a lesson in the dangers of new media, but as some sort of instruction manual for how to treat literature. But the real story here is that a bunch of cowardly state and federal politicians are placating the desires largely of the religious right, who are seeking to tightly control the books that children have access to in public, secular schools. And if you can’t manage to understand how plainly that is the antithesis of our form of government, then you’re beyond help.But because authoritarianism makes a fool of itself as a habit, and religiously-based authoritarianism all the moreso, then end result of these attempts at censorship always eventually reveal themselves as absurd. And if you need an example of that, you need only look at the state of Tennessee.Magic Tree House author , children’s poet  and Calvin and Hobbes cartoonist have joined Judy Blume, Sarah J. Maas, Eric Carle and  on a mind-boggling list of hundreds of books purged from some Tennessee school libraries.The removals are the result of a growing political movement to control information through book banning. In 2024, the state legislature amended the “Age-Appropriate Materials Act of 2022” to specify that any materials that “in whole or in part” contain any “nudity, or descriptions or depictions of sexual excitement, sexual conduct, excess violence, or sadomasochistic abuse” are inappropriate for all students and do not belong in a school library. This change means books are not evaluated as a whole, and excerpts can be considered without context, if they have any content that is deemed to cross these lines. This leaves no room for educators and librarians to curate collections that reflect the real world and serve the educational needs of today’s students.And because you have groups of far-right activists marching around looking for any scintilla of material over which they can manufacture faux outrage, you get these examples of books being banned for their terrible, awful, smutty content. Such as a , book that was banned because it had this pornographical image on its cover:Special thanks to Mike Masnick for briefly allowing me to post porn images on Techdirt. And for all of you whose naughty bits are currently twitching due to that book cover, I offer you my sincerest apologies.But if you thought  was bad, check out this panel image from a  book that got it banned. Here we have the nude image of a child on full display.Now, I sure hope everyone realizes that the above is a dalliance into sarcasm, because I was laying it on quite thick. I grew up on , not to mention Shel Silverstein’s , which was also banned. Why? More butts, that’s why. And, because the universe is not without a sense of irony, one school even had to ban a book authored by an alumnus.Oak Ridge Schools, where a significant number of the bans target art history books, even removed Richard Jolley: Sculptor of Glass, a collection of works by the artist, who graduated from Oak Ridge High School.“Regarding the book written by Mr. Jolley, we were thrilled to feature a book written by an ORHS alumni on our shelves and were equally disappointed to have to remove it,” Molly Gallagher Smith, an Oak Ridge Schools spokeswoman, told WBIR. “Unfortunately, as an artist, Mr. Jolley’s book features depictions of the human body that are in direct violation of the law.”There are more and the bans hit all the notes you would expect: LGBTQ+ material, books about the Holocaust, books about African American contributions to government and science, and, because of course,  itself.Now, this is indeed all absurd, but it isn’t remotely funny. There is a ton of literature, hundreds of books, that are being banned under this Tennessee law. Many of them reportedly without going through any review process.And many of the bans are coming without any review or discussion. The Tennessee Association of School Libraries found in a survey of its members that in 20% of school districts, books were removed from the shelves at the command of district leaders without any sort of review process. “Librarians and educators are concerned that we will end up pulling a massive amount of books without looking at the books as a whole,” one member said in the survey. “It’s a slippery slope,” said another, “and I’m fearful of the next topic that will be regulated.”Open up book bans to the frothy-mouthed mob. What could possibly go wrong, other than keeping valuable literature out of the hands of our children?]]></content:encoded></item><item><title>Proposed Budget Seeks To Close Mauna Loa Observatory&apos;s Climate CO2 Study</title><link>https://news.slashdot.org/story/25/07/03/0031226/proposed-budget-seeks-to-close-mauna-loa-observatorys-climate-co2-study?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Slashdot regularly posts milestones on CO2 levels reported by the Mauna Loa Observatory," writes longtime Slashdot reader symbolset, pointing to a new article highlighting how the Trump administration's proposed budget would eliminate funding for the lab's carbon dioxide monitoring. "Continuous observation records since 1958 will end with the new federal budget as ocean and atmospheric sciences are defunded." From a report: [I]t's the Mauna Loa laboratory that is the most prominent target of the President Donald Trump's climate ire, as measurements that began there in 1958 have steadily shown CO2's upward march as human activities have emitted more and more of the planet-warming gas each year. The curve produced by the Mauna Loa measurements is one of the most iconic charts in modern science, known as the Keeling Curve, after Charles David Keeling, who was the researcher who painstakingly collected the data. His son, Ralph Keeling, a professor at the Scripps Institution of Oceanography at UC San Diego, now oversees collecting and updating that data.
 
Today, the Keeling Curve measurements are made possible by the National Oceanic and Atmospheric administration, but the data gathering and maintenance of the historical record also is funded by Schmidt Sciences and Earth Networks, according to the Keeling Curve website. In the event of a NOAA shut down of the lab, Scripps could seek alternate sources of funding to host the instruments atop the same peak or introduce a discontinuity in the record by moving the instruments elsewhere in Hawaii.
 
The proposal to shut down Mauna Loa had been made public previously but was spelled out in more detail on Monday when NOAA submitted a budget document (PDF) to Congress. It made more clear that the Trump administration envisions eliminating all climate-related research work at NOAA, as had been proposed in Project 2025, the conservative blueprint for overhauling the government. It would do this in large part by cutting NOAA's Office of Oceanic and Atmospheric Research entirely, including some labs that are also involved in improving weather forecasting. NOAA has long been one of the world's top climate science agencies, but the administration would steer it instead towards being more focused on operational weather forecasting and warning responsibilities.]]></content:encoded></item><item><title>Foxconn Mysteriously Tells Chinese Workers To Quit India and Return To China</title><link>https://apple.slashdot.org/story/25/07/03/0016210/foxconn-mysteriously-tells-chinese-workers-to-quit-india-and-return-to-china?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple's expansion in India has hit a snag as Foxconn has sent over 300 Chinese workers back to China, potentially reducing production efficiency just as mass manufacturing of the iPhone 17 begins. AppleInsider reports: It's not known why Foxconn has done this, nor is it clear whether workers have been laid off or redeployed to the company's facilities in China. The move, though, does follow Beijing officials reportedly working to prevent firms moving away from China. Those officials are said to have been verbally encouraging China's local governments and regulatory bodies to curb exports of equipment or technologies to India and Southeast Asia.
 
Overall, China has been making it harder for skilled labor to leave the country. It's not clear how any changes have specifically affected Chinese workers who had already left.What is clear is that Foxconn has used many experienced Chinese engineers as it attempts to rapidly expand in India. It's said, too, that Chinese managers have been vital in training Foxconn staff in India. Since that training has been ongoing for some years, and since at least most of Foxconn's production lines have been set up, it's said that there will not be an impact on the quality of manufacturing. But one source said the changes will impact efficiency on the production line.]]></content:encoded></item><item><title>Hacker With &apos;Political Agenda&apos; Stole Data From Columbia, University Says</title><link>https://news.slashdot.org/story/25/07/03/0012219/hacker-with-political-agenda-stole-data-from-columbia-university-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A politically motivated hacker breached Columbia University's IT systems, stealing vast amounts of sensitive student and employee data -- including admissions decisions and Social Security numbers. The Record reports: The hacker reportedly provided Bloomberg News with 1.6 gigabytes of data they claimed to have stolen from the university, including information from 2.5 million applications going back decades. The stolen data the outlet reviewed reportedly contains details on whether applicants were rejected or accepted, their citizenship status, their university ID numbers and which academic programs they sought admission to. While the hacker's claims have not been independently verified, Bloomberg said it compared data provided by the hacker to that belonging to eight Columbia applicants seeking admission between 2019 and 2024 and found it matched.
 
The threat actor reportedly told Bloomberg he was seeking information that would indicate whether the university continues to use affirmative action in admissions despite a 2023 Supreme Court decision prohibiting the practice. The hacker told Bloomberg he obtained 460 gigabytes of data in total -- after spending two months targeting and penetrating increasingly privileged layers of the university's servers -- and said he harvested information about financial aid packages, employee pay and at least 1.8 million Social Security numbers belonging to employees, applicants, students and their family members.]]></content:encoded></item><item><title>Steam On Linux Usage Dips Slightly For June, AMD Linux CPU Usage Hits 69%</title><link>https://www.phoronix.com/news/Steam-June-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 00:38:36 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Steam Survey issues prevented the survey results from being posted on the evening of the 1st as is traditionally done, but the results were just uploaded now to the Steam website. Steam on Linux usage dipped slightly but overall remains healthy with much excitement still around the Steam Deck and SteamOS efforts...]]></content:encoded></item><item><title>Debian 13 Installer RC2 Fixes An Annoying Issue, Improves Btrfs Rescue Handling</title><link>https://www.phoronix.com/news/Debian-Installer-13-Trixie-RC2</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 3 Jul 2025 00:28:20 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following last month's release of Debian Installer Trixie RC1 as the installer for the upcoming Debian 13.0 release, a second release candidate was issued today for testing...]]></content:encoded></item><item><title>Intel&apos;s New CEO Explores Big Shift In Chip Manufacturing Business</title><link>https://tech.slashdot.org/story/25/07/02/2149214/intels-new-ceo-explores-big-shift-in-chip-manufacturing-business?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 3 Jul 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Reuters: Intel's new chief executive is exploring a big change to its contract manufacturing business to win major customers, two people familiar with the matter told Reuters, in a potentially expensive shift from his predecessor's plans. The new strategy for Intel's foundry business would mean offering outside customers a newer generation of technology, the people said. That next-generation chipmaking process, analysts believe, will be more competitive against Taiwan Semiconductor Manufacturing Co in trying to land major customers such as Apple or Nvidia.
 
Since taking the company's helm in March, CEO Lip-Bu Tan has moved fast to cut costs and find a new path to revive the ailing U.S. chipmaker. By June, he started voicing that a manufacturing process known as 18A, in which prior CEO Pat Gelsinger had invested heavily, was losing its appeal to new customers, said the sources, who spoke on condition of anonymity. To put aside external sales of 18A and its variant 18A-P, manufacturing processes that have cost Intel billions of dollars to develop, the company would have to take a write-off, one of the people familiar with the matter said. Industry analysts contacted by Reuters said such a charge could amount to a loss of hundreds of millions, if not billions, of dollars.
 
Intel declined to comment on such "hypothetical scenarios or market speculation." It said the lead customer for 18A has long been Intel itself, and it aims to ramp production of its "Panther Lake" laptop chips later in 2025, which it called the most advanced processors ever designed and manufactured in the United States. Persuading outside clients to use Intel's factories remains key to its future. As its 18A fabrication process faced delays, rival TSMC's N2 technology has been on track for production. Tan's preliminary answer to this challenge: focus more resources on 14A, a next-generation chipmaking process where Intel expects to have advantages over Taiwan's TSMC, the two sources said. The move is part of a play for big customers like Apple and Nvidia, which currently pay TSMC to manufacture their chips.]]></content:encoded></item><item><title>OpenAI condemns Robinhood’s ‘OpenAI tokens’</title><link>https://techcrunch.com/2025/07/02/openai-condemns-robinhoods-openai-tokens/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Wed, 2 Jul 2025 23:43:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI wants to make clear that Robinhood's sale of "OpenAI tokens" will not give everyday consumers equity — or stock — in OpenAI.]]></content:encoded></item><item><title>Nintendo Locked Down the Switch 2&apos;s USB-C Port, Broke Third-Party Docking</title><link>https://hardware.slashdot.org/story/25/07/02/2136241/nintendo-locked-down-the-switch-2s-usb-c-port-broke-third-party-docking?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Two accessory manufacturers have told The Verge that Nintendo has intentionally locked down the Switch 2's USB-C port using a new encryption scheme, preventing compatibility with third-party docks and accessories. "I haven't yet found proof of that encryption chip myself -- but when I analyzed the USB-C PD traffic with a Power-Z tester, I could clearly see the new Nintendo Switch not behaving like a good USB citizen should," writes The Verge's Sean Hollister. From the report: If you've been wondering why there are basically no portable Switch 2 docks on the market, this is the reason. Even Jsaux, the company that built its reputation by beating the Steam Deck dock to market, tells us it's paused its plans to build a Switch 2 dock because of Nintendo's actions. It's not simply because the Switch 2 now requires more voltage, as was previously reported; it's that Nintendo has made things even more difficult this generation.]]></content:encoded></item><item><title>Wonder Dynamics co-founder Nikola Todorovic joins the AI Stage at TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/07/02/wonder-dynamics-co-founder-nikola-todorovic-joins-the-ai-stage-at-techcrunch-disrupt-2025/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 2 Jul 2025 23:10:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TechCrunch Disrupt 2025 is back at Moscone West in San Francisco from October 27–29, bringing together 10,000+ startup and VC leaders to dig into what’s next in tech. And when it comes to artificial intelligence, the conversations aren’t just technical — they’re creative, cinematic, and boundary-pushing. That’s why Nikola Todorovic is headed to the AI […]]]></content:encoded></item><item><title>The NO FAKES Act Has Changed – And It’s So Much Worse</title><link>https://www.techdirt.com/2025/07/02/the-no-fakes-act-has-changed-and-its-so-much-worse/</link><author>katharine.trendacosta</author><category>tech</category><pubDate>Wed, 2 Jul 2025 22:43:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[A bill purporting to target the issue of misinformation and defamation caused by generative AI has mutated into something that could change the internet forever, harming speech and innovation from here on out.The Nurture Originals, Foster Art and Keep Entertainment Safe (NO FAKES) Act aims to address understandable concerns about generative AI-created “replicas” by creating a broad new intellectual property right. That approach was the first mistake: rather than giving people targeted tools to protect against harmful misrepresentations—balanced against the need to protect legitimate speech such as parodies and satires—the original NO FAKES just federalized an image-licensing system.The updated bill doubles down on that initial mistaken approach by mandating a whole new censorship infrastructure for that system, encompassing not just images but the products and services used to create them, with few safeguards against abuse.The new version of NO FAKES requires almost every internet gatekeeper to create a system that will a) take down speech upon receipt of a notice; b) keep down any recurring instance—meaning, adopt inevitably overbroad replica filters on top of the already deeply flawed copyright filters;  c) take down and filter tools that might have been used to make the image; and d) unmask the user who uploaded the material based on nothing more than the say so of person who was allegedly “replicated.”This bill would be a disaster for internet speech and innovation.The first version of NO FAKES focused on digital replicas. The new version goes further, targeting tools that can be used to produce images that aren’t authorized by the individual, anyone who owns the rights in that individual’s image, or the law. Anyone who makes, markets, or hosts such tools is on the hook. There are some limits—the tools must be primarily designed for, or have only limited commercial uses other than making unauthorized images—but those limits will offer cold comfort to developers given that they can be targeted based on nothing more than a bare allegation. These provisions effectively give rights-holders the veto power on innovation they’ve long sought in the copyright wars, based on the same tech panics. Takedown Notices and Filter MandateThe first version of NO FAKES set up a notice and takedown system patterned on the DMCA, with even fewer safeguards. NO FAKES expands it to cover more service providers and require those providers to not only take down targeted materials (or tools) but keep them from being uploaded in the future.  In other words, adopt broad filters or lose the safe harbor.But copyright filters are not yet required by law. NO FAKES would create a legal mandate that will inevitably lead to hecklers’ vetoes and other forms of over-censorship.The bill does contain carve outs for parody, satire, and commentary, but those will also be cold comfort for those who cannot afford to litigate the question.Threats to Anonymous SpeechAs currently written, NO FAKES also allows anyone to get a subpoena from a court clerk—not a judge, and without any form of proof—forcing a service to hand over identifying information about a user.We’ve already seen abuse of a similar system in action. In copyright cases, those unhappy with the criticisms being made against them get such subpoenas to silence critics. Often that the criticism includes the complainant’s own words as proof of the criticism, an ur-example of fair use. But the subpoena is issued anyway and, unless the service is incredibly on the ball, the user can be unmasked.Not only does this chill further speech, the unmasking itself can cause harm to users. Either reputationally or in their personal life.Most of us are very unhappy with the state of Big Tech. It seems like not only are we increasingly forced to use the tech giants, but that the quality of their services is actively degrading. By increasing the sheer amount of infrastructure a new service would need to comply with the law, NO FAKES makes it harder for any new service to challenge Big Tech. It is probably not a coincidence that some of these very giants are okay with this new version of NO FAKES.Requiring removal of tools, apps, and services could likewise stymie innovation. For one, it would harm people using such services for otherwise lawful creativity.  For another, it would discourage innovators from developing new tools. Who wants to invest in a tool or service that can be forced offline by nothing more than an allegation?This bill is a solution in search of a problem. Just a few months ago, Congress passed Take It Down, which targeted images involving intimate or sexual content. That deeply flawed bill pressures platforms to actively monitor online speech, including speech that is presently encrypted. But if Congress is really worried about privacy harms, it should at least wait to see the effects of the last piece of internet regulation before going further into a new one. Its failure to do so makes clear that this is not about protecting victims of harmful digital replicas.NO FAKES is designed to consolidate control over the commercial exploitation of digital images, not prevent it. Along the way, it will cause collateral damage to all of us.]]></content:encoded></item><item><title>Grammarly Acquires AI Email Client Superhuman</title><link>https://slashdot.org/story/25/07/02/2128229/grammarly-acquires-ai-email-client-superhuman?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Grammarly has acquired the AI email client Superhuman to enhance its AI-driven productivity suite and expand AI capabilities within email communication. Financial terms of the deal were not disclosed but Superhuman CEO Rahul Vohra and his team will be joining the AI writing company. TechCrunch reports: Superhuman was founded by Rahul Vohra, Vivek Sodera, and Conrad Irwin. The company raised more than $114 million in funding from backers including a16z, IVP, and Tiger Global, with its last valuation at $825 million, according to data from venture data analytics firm Traxcn. "With Superhuman, we can deliver that future to millions more professionals while giving our existing users another surface for agent collaboration that simply doesn't exist anywhere else. Email isn't just another app; it's where professionals spend significant portions of their day, and it's the perfect staging ground for orchestrating multiple AI agents simultaneously," Shishir Mehrotra, CEO of Grammarly, said in a statement.
 
With this deal, CEO Vohra and other Superhuman employees are moving over to Grammarly. "Email is the main communication tool for billions of people worldwide and the number-one use case for Grammarly customers. By joining forces with Grammarly, we will invest even more in the core Superhuman experience, as well as create a new way of working where AI agents collaborate across the communication tools that we all use every day," Rahul Vohra, CEO of Superhuman, said in a statement.]]></content:encoded></item><item><title>NYT To Start Searching Deleted ChatGPT Logs After Beating OpenAI In Court</title><link>https://yro.slashdot.org/story/25/07/02/2122230/nyt-to-start-searching-deleted-chatgpt-logs-after-beating-openai-in-court?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: Last week, OpenAI raised objections in court, hoping to overturn a court order requiring the AI company to retain all ChatGPT logs "indefinitely," including deleted and temporary chats. But Sidney Stein, the US district judge reviewing OpenAI's request, immediately denied OpenAI's objections. He was seemingly unmoved by the company's claims that the order forced OpenAI to abandon "long-standing privacy norms" and weaken privacy protections that users expect based on ChatGPT's terms of service. Rather, Stein suggested that OpenAI's user agreement specified that their data could be retained as part of a legal process, which Stein said is exactly what is happening now.
 
The order was issued by magistrate judge Ona Wang just days after news organizations, led by The New York Times, requested it. The news plaintiffs claimed the order was urgently needed to preserve potential evidence in their copyright case, alleging that ChatGPT users are likely to delete chats where they attempted to use the chatbot to skirt paywalls to access news content. A spokesperson told Ars that OpenAI plans to "keep fighting" the order, but the ChatGPT maker seems to have few options left. They could possibly petition the Second Circuit Court of Appeals for a rarely granted emergency order that could intervene to block Wang's order, but the appeals court would have to consider Wang's order an extraordinary abuse of discretion for OpenAI to win that fight.
 
In the meantime, OpenAI is negotiating a process that will allow news plaintiffs to search through the retained data. Perhaps the sooner that process begins, the sooner the data will be deleted. And that possibility puts OpenAI in the difficult position of having to choose between either caving to some data collection to stop retaining data as soon as possible or prolonging the fight over the order and potentially putting more users' private conversations at risk of exposure through litigation or, worse, a data breach. [...]
 
Both sides are negotiating the exact process for searching through the chat logs, with both parties seemingly hoping to minimize the amount of time the chat logs will be preserved. For OpenAI, sharing the logs risks revealing instances of infringing outputs that could further spike damages in the case. The logs could also expose how often outputs attribute misinformation to news plaintiffs. But for news plaintiffs, accessing the logs is not considered key to their case -- perhaps providing additional examples of copying -- but could help news organizations argue that ChatGPT dilutes the market for their content. That could weigh against the fair use argument, as a judge opined in a recent ruling that evidence of market dilution could tip an AI copyright case in favor of plaintiffs.]]></content:encoded></item><item><title>Lucid sales inch forward as EV maker pushes to ramp Gravity production</title><link>https://techcrunch.com/2025/07/02/lucid-sales-inch-forward-as-ev-maker-pushes-to-ramp-gravity-production/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Wed, 2 Jul 2025 21:47:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Lucid sold 3,309 vehicles in the second quarter, a new record for the  EV maker. Still, the company must ramp up production of its Gravity SUV to meet its 2025 target. ]]></content:encoded></item><item><title>Google Ordered To Pay $315 Million for Taking Data From Idle Android Phones</title><link>https://yro.slashdot.org/story/25/07/02/1818254/google-ordered-to-pay-315-million-for-taking-data-from-idle-android-phones?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 21:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A California jury has ordered Google to pay $314.6 million to Android smartphone users in the state after finding the company liable for collecting data from idle devices without permission. 

The San Jose jury ruled Tuesday that Google sent and received information from phones while idle, creating "mandatory and unavoidable burdens shouldered by Android device users for Google's benefit." The 2019 class action represented an estimated 14 million Californians who argued Google consumed their cellular data for targeted advertising purposes.]]></content:encoded></item><item><title>Be Polarizing as a Job Seeker</title><link>https://spectrum.ieee.org/be-a-polarizing-job-seeker</link><author>Rahul Pandey</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81OTEwNDExMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2MjIxNDM1NH0.KU_HC9RlAuzaq6NFugnkIjhO6mrvJzTfytOINkYN-Lg/image.jpg?width=600" length="" type=""/><pubDate>Wed, 2 Jul 2025 21:05:35 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your goal is to be an exceptional fit for one specific job]]></content:encoded></item><item><title>Amazon To Shut Down Its Freevee App Next Month</title><link>https://entertainment.slashdot.org/story/25/07/02/1844245/amazon-to-shut-down-its-freevee-app-next-month?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Amazon plans to shut down its standalone Freevee app in August, according to an in-app notice to users. From a report: The free, ad-supported streaming service is directing viewers to continue watching Freevee content on Prime Video. 

"Prime Video is the new exclusive home for Freevee Tv show, movies, and Live TV," the notice to readers states. "The Freevee app will be accessible until August 2025. Continue watching your favorite Free Originals and our library of hit movies, shows, and live TV on Prime Video for free, no subscription needed. Download Prime Video to get started and sign-in with your Amazon account."]]></content:encoded></item><item><title>Our National Robocall Nightmare Is Getting Worse Under Donald Trump</title><link>https://www.techdirt.com/2025/07/02/our-national-robocall-nightmare-is-getting-worse-under-donald-trump/</link><author>Karl Bode</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:40:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[According to the latest data on robocalls from the YouMail Robocall Index, the scale of the U.S. robocall problem has grown by another eleven percent year over year. U.S. consumers received just over 4.8 billion robocalls in May. We’ve normalized ceding our primary voice communications platforms to corporations, debt collectors, and scammers, and there’s every indication it’s going to get worse under Donald Trump.YouGov’s latest study found that “just” 14 percent of May’s robocall total was from “scammers.” Even before Trump, a corrupted court system had consistently limited the FCC’s authority to combat robocalls. Corrupt lawmakers and regulators, cowed into blind obedience by a massive, generational, cross-industry-lobbying campaign, like to keep the focus on , when many “legit” companies, again, leverage the exact same tactics as scammers.As a result, federal regulators refuse to hold large phone companies accountable for their lagging efforts to combat fraud and spam. Case in point: Truecaller’s U.S. Spam and Scam Report found that half of all major U.S. phone companies earned a D or F in their efforts to combat annoying robocalls and scams. Functional, developed countries (even many less developed ones) don’t have these problems.So while the FCC is supposed to enforce robocall offenses and levy fines, terrible court rulings mean they aren’t allowed to  fines. That’s left to the DOJ, which routinely just… doesn’t bother. As a result a comically small volume of the overall fines levied are ever actually collected. For example between 2015 and 2019 the FCC issued $208.4 million in robocall fines, but collected just .And again, this is all  Trump 2.0. And before largely unregulated AI.Trump FCC boss Brendan Carr has been promising to take a hatchet to whatever is left of U.S. corporate oversight as part of his “delete, delete, delete” deregulatory initiative. Big telecoms and robocallers have been making it very clear they’re very excited about it. Debt collectors in particular are very eager to roll back already flimsy rules governing how badly they can harass people they already know can’t pay. Like so many systemic U.S. problems, the robocall menace isn’t something that gets fixed without first embracing much broader corruption, campaign finance, lobbying, and legal reforms. That is, obviously and indisputably, not something that’s happening under Trump and his sycophantic regulators and telecom industry-coddling courts.]]></content:encoded></item><item><title>Vera Rubin: This Is How Far Engineers Go to Explore the Universe</title><link>https://spectrum.ieee.org/vera-rubin-engineering</link><author>Harry Goldstein</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExMzgwMi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgwNjc2Mzc0M30.cnVKj7yANaP42fkxKgybLtZtzalpzw4fL5guP1wK7aI/image.png?width=600" length="" type=""/><pubDate>Wed, 2 Jul 2025 20:26:42 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[And how far we went to tell their story]]></content:encoded></item><item><title>Substack brings new updates to livestreaming as it increases video push</title><link>https://techcrunch.com/2025/07/02/substack-brings-new-updates-to-livestreaming-as-it-increases-video-push/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:09:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The recent update enables creators to share clips of their live videos on Notes, and Substack will notify them in real time about the performance. ]]></content:encoded></item><item><title>ZLUDA Making Progress In 2025 On Bringing CUDA To Non-NVIDIA GPUs</title><link>https://www.phoronix.com/news/ZLUDA-Q2-2025-Update</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:01:44 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The ZLUDA open-source effort that started off a half-decade ago as a drop-in CUDA implementation for Intel GPUs and then for several years was funded by AMD as a CUDA implementation for Radeon GPUs atop ROCm and then open-sourced but then reverted has been continuing to push along a new path since last year. The current take on ZLUDA is a multi-vendor CUDA implementation for non-NVIDIA GPUs for AI workloads and more. More progress was made during Q2 on this effort...]]></content:encoded></item><item><title>China&apos;s Giant New Gamble With Digital IDs</title><link>https://yro.slashdot.org/story/25/07/02/1827222/chinas-giant-new-gamble-with-digital-ids?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 20:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China will launch digital IDs for internet use on July 15th, transferring online verification from private companies to government control. Users obtain digital IDs by submitting personal information including facial scans to police via an app. A pilot program launched one year ago enrolled 6 million people. 

The system currently remains voluntary, though officials and state media are pushing citizens to register for "information security." Companies will see only anonymized character strings when users log in, while police retain exclusive access to personal details. The program replaces China's existing system requiring citizens to register with companies using real names before posting comments, gaming, or making purchases. 

Police say they punished 47,000 people last year for spreading "rumours" online. The digital ID serves a broader government strategy to centralize data control. State planners classify data as a production factor alongside labor and capital, aiming to extract information from private companies for trading through government-operated data exchanges.]]></content:encoded></item><item><title>AT&amp;T rolls out Wireless Account Lock protection to curb the SIM-swap scourge</title><link>https://arstechnica.com/security/2025/07/att-rolls-out-wireless-account-lock-protection-to-curb-the-sim-swap-scourge/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/02/sim-cards.jpeg" length="" type=""/><pubDate>Wed, 2 Jul 2025 19:28:27 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[AT&T is rolling out a protection that prevents unauthorized changes to mobile accounts as the carrier attempts to fight a costly form of account hijacking that occurs when a scammer swaps out the SIM card belonging to the account holder.The technique, known as SIM swapping or port-out fraud, has been a scourge that has vexed wireless carriers and their millions of subscribers for years. An indictment filed last year by federal prosecutors alleged that a single SIM swap scheme netted $400 million in cryptocurrency. The stolen funds belonged to dozens of victims who had used their phones for two-factor authentication to cryptocurrency wallets.Wireless Account Lock debutA separate scam from 2022 gave unauthorized access to a T-Mobile management platform that subscription resellers, known as mobile virtual network operators, use to provision services to their customers. The threat actor gained access using a SIM swap of a T-Mobile employee, a phishing attack on another T-Mobile employee, and at least one compromise of an unknown origin.]]></content:encoded></item><item><title>Could Google’s Veo 3 be the start of playable world models?</title><link>https://techcrunch.com/2025/07/02/could-googles-veo-3-be-the-start-of-playable-world-models/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:22:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Demis Hassabis, CEO of Google DeepMind, hinted Tuesday evening that Veo 3, Google’s latest video generating model, could potentially be used for video games. ]]></content:encoded></item><item><title>AI Note Takers Are Increasingly Outnumbering Humans in Workplace Video Calls</title><link>https://slashdot.org/story/25/07/02/194224/ai-note-takers-are-increasingly-outnumbering-humans-in-workplace-video-calls?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AI-powered note-taking apps are increasingly attending workplace meetings in place of human participants, creating situations where automated transcription bots outnumber actual attendees. 

Major platforms including Zoom, Microsoft Teams and Google Meet now offer built-in note-taking features that record, transcribe and summarize meetings for invited participants who don't attend. The technology operates under varying legal frameworks, with most states requiring only single-party consent for recording while California, Florida, and Pennsylvania mandate all-party approval.]]></content:encoded></item><item><title>WEMADE And Redlab Unleash Web3 MMORPG – Global Pre-Registration Open For Aug 2025</title><link>https://hackernoon.com/wemade-and-redlab-unleash-web3-mmorpg-global-pre-registration-open-for-aug-2025?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:18:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, Singapore/July 2nd, 2025/-- and  are excited to announce the start of global pre-registration for During the global showcase, the team introduced “RPG Tokenomics 3.0”—a next-generation Play-to-Earn (P2E) system that seamlessly links two unique tokens for a dynamic and rewarding player experience. The showcase also revealed key features and the game’s upcoming roadmap. How to Join and What to ExpectPre-Registration Rewards: All players who pre-register will receive special in-game items, including Gold Random Boxes and Top-grade Monster Slates.Extra Benefits for WEMIX PLAY Members: Users who pre-register through  will receive a ticket to join the “Invite a Friend” event. For every friend invited who completes pre-registration, the users get additional raffle entries.Exciting Events and RewardsWeekly Raffles: Every week until August 7th, players can win PLAY Tokens (usable on WEMIX PLAY), CROM Tokens (the game’s native currency), and other prizes. Additional raffle tickets are awarded for each friend who completes the sign-up process through an invitation, increasing the chances of selection with every successful referral.Milestone Rewards: As the total number of pre-registrations increases, everyone unlocks milestone rewards like ROM Buff Boxes and Enhancement Scroll Selection Boxes.Check-In Event: After pre-registering, log in to the official website to collect Pre-registration Coins. These coins can be used to craft a variety of rare items, such as ImperionGuaranteed Slates and Lucky Enhancement Scroll Boxes.Mission Event: Completing special missions can earn an additional 200 Pre-registration Coins, offering more opportunities to engage ahead of the launch.The Learn about the game’s unique tokenomics and how the Dual-token P2E system worksDiscover gameplay features and future plansChat directly with the creators and get their questions answeredPre-Registration Now Open for Further details are available on: is a South Korea-based technology and gaming company with 25 years of experience in digital innovation. Best known for The Legend of Mir IP, WEMADE has expanded its vision through the WEMIX platform, which powers a global ecosystem of Web3 games, NFTs, DeFi, and token-based services.The company is committed to building sustainable digital economies where developers, players, and partners can grow together in a secure and open environment.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>PrimeXBT Launches ‘Trade As VIP’ Campaign Offering 70% Off Trading Fees</title><link>https://hackernoon.com/primexbt-launches-trade-as-vip-campaign-offering-70percent-off-trading-fees?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:10:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Castries, Saint Lucia, July 2nd, 2025/Chainwire/--, a leading multi-asset broker, has launched its latest promotion, , granting all newly registered users instant access to VIP 2 status for 30 days. As part of the platform’s tiered system, VIP levels reward traders with reduced fees, tighter spreads, and exclusive platform benefits. For a limited time, new users can experience these professional-grade conditions from the moment they join, without needing to meet any trading volume thresholds.Running from July 1 to August 31, 2025, the campaign is designed to remove entry barriers for new traders by automatically upgrading every new account to VIP 2. On the Crypto Futures platform, this unlocks almost 70% reduction in taker fees, dropping from the standard 0.045% to just 0.015%. In the coming weeks, VIP 2 users will also benefit from up to 30% discounts on spreads across Forex and CFDs on Stocks, Commodities, Indices, and Crypto on PXTrader.In addition to improved trading conditions, VIP 2 status also provides priority customer support, instant withdrawals, and higher withdrawal limits, enhancing the overall trading experience across both platforms. According to PrimeXBT, the campaign delivers real value by giving new traders access to some of the best trading conditions in the industry. From day one, the broker empowers users to start stronger, trade smarter, and build early momentum, combining cost-efficiency with a professional-grade experience designed to support their trading journey. ranks among the lowest-cost options on the market, outperforming major platforms in fee efficiency. For example, a $1,000,000 Bitcoin trade under VIP 2 would generate $300 in savings compared to standard fees. These savings scale significantly with volume, helping both casual and active traders reduce trading costs without compromising execution quality.With meaningful savings and access to top-tier tools and conditions, PrimeXBT is lowering the cost of entry while raising the standard of what new traders can expect. This campaign reinforces the broker’s ongoing commitment to making high-performance trading more accessible, empowering, and competitive for all. is a global multi-asset broker trusted by over 1,000,000 traders in 150+ countries, offering a next-generation trading experience that bridges traditional and digital finance. Clients can trade CFDs on Stocks, Indices, Commodities and Crypto, as well as Crypto Futures and Forex. PrimeXBT also enables clients to buy and sell cryptocurrencies, store them in secure built-in wallets, and instantly exchange crypto to crypto or fiat to crypto, all within one integrated environment.Since 2018, PrimeXBT has made investing more accessible by lowering barriers to entry and providing secure, easy access to financial markets. This accessibility extends across its native web and mobile platforms, MetaTrader 5, and a variety of funding options in crypto, fiat, and local payment methods. Committed to putting clients first, PrimeXBT empowers traders of all levels with innovative tools and industry-leading conditions, delivering a better way to trade.Disclaimer: The content provided here is for informational purposes only and is not intended as personal investment advice and does not constitute a solicitation or invitation to engage in any financial transactions, investments, or related activities. Past performance is not a reliable indicator of future results.The financial products offered by the Company are complex and come with a high risk of losing money rapidly due to leverage. These products may not be suitable for all investors. Before engaging, you should consider whether you understand how these leveraged products work and whether you can afford the high risk of losing your money.The Company does not accept clients from the Restricted Jurisdictions as indicated on its website. Some services or products may not be available in your jurisdiction. The applicable legal entity and its respective products and services depend on the client’s country of residence and the entity with which the client has established a contractual relationship during registration.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Trump’s Immigration Enforcement: Free The Criminals, Jail The Innocent</title><link>https://www.techdirt.com/2025/07/02/trumps-immigration-enforcement-free-the-criminals-jail-the-innocent/</link><author>Mike Masnick</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:06:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The Trump administration’s immigration enforcement has revealed itself to be not just cruel, but fundamentally backwards: They’re literally freeing dangerous criminals while manufacturing cases against innocent people. And they’re doing it all to cover up their own massive legal fuckups.Take the case of Kilmar Abrego Garcia. We covered this last week when Magistrate Judge Barbara Holmes ordered his release, noting that the Justice Department appeared to have leaned on actual criminals to fabricate evidence against him. Now the Washington Post has the full story, and it’s even more damning: The Trump admin is literally freeing a repeat violent offender in exchange for testimony against Abrego—a man with no criminal history who was working and raising a family.The Trump administration has agreed to release from prison a three-time felon who drunkenly fired shots in a Texas community and spare him from deportation in exchange for hiscooperation in the federal prosecution of Kilmar Abrego García, according to a review of court records and official testimony.Jose Ramon Hernandez Reyes, 38, has been convicted of smuggling migrants and illegally reentering the United States after having been deported. He also pleaded guilty to “deadly conduct” in the Texas incident, and is now the government’s star witness in its case against Abrego.Let that sink in: They’re freeing someone, who drunkenly fired shots in a community, to help them prosecute someone whose only “crime” was being the victim of the government’s own illegal deportation, making the Trump administration look totally incompetent in the process.Remember, the Trump regime insisted that it was focused on going after the worst of the worst, the most hardened criminals of all. Yet, over and over again we’re finding out that they can’t actually find all those criminals they insisted were out there, so they’re randomly grabbing anyone they can find. In the case of Abrego, that meant taking a man who had no criminal history, and appeared to be gainfully employed, and raising a family, and shipping him to the one place an immigration court had forbidden the US to send him.That set the DOJ off on a wild goose chase to try to justify their own massive fuckup, leading to these questionable criminal charges against him, which they used to try to distract from the fact that they accidentally sent a man to a foreign concentration camp after being forbidden from doing so.But to make that work, apparently it involves freeing the actual hardened, dangerous criminal, in hopes that he’ll testify against Abrego.is among a handful of cooperating witnesses who could help the Trump administration achieve its goal of never letting Abrego walk free in the United States again. In exchange, he has already been released early from federal prison to a halfway house and has been given permission to stay in the U.S. for at least a year.“Otherwise he would be deported,” Peter Joseph, a Homeland Security Investigations special agent, testified at Abrego’s criminal hearing June 13. The government is also likely to give him a work permit, the agent told the court.There’s no way to look at this other than “we’ll release a hardened criminal who is here illegally, and who has already been deported multiple times, including letting him stay in the US with working apers, so long as he concocts a story that lets DHS and the DOJ save face after we fucked up royally in renditioning a man illegally.”That should be an embarrassment to the Trump regime, but it will barely get any attention.It Gets Worse: Trump Is Also Freeing MS-13 LeadersBut the Abrego case isn’t an isolated incident—it’s part of a pattern. At the same time Trump is manufacturing criminal cases against innocent people, he’s also cutting deals to free actual MS-13 gang leaders.Even among the brutal ranks of the transnational gang called MS-13, Vladimir Arévalo Chávez stands out as a highly effective manager of murder, prosecutors say.Known as “Vampiro,” he has been accused of overseeing killings in at least three countries: of migrants in Mexico, rivals in El Salvador and his own compatriots in the United States.His arrest in February 2023 was a major triumph for American investigators, who only months earlier had accused him and 12 other gang leaders of terrorism, bloodshed and corruption in a wide-ranging federal indictment on Long Island.But this April, the prosecutors who brought those charges suddenly — and quietly — asked a federal judge to drop them. Citing “national security concerns,” they said they needed to return Mr. Arévalo to El Salvador, his homeland.The report details how these actual MS-13 leaders have evidence of Bukele’s corruption, and Bukele asked for them back, rather than letting them tell their stories to American courts:But the Trump administration has not acknowledged another reason Mr. Bukele would want them back: U.S. prosecutors have amassed substantial evidence of a corrupt pact between the Salvadoran government and some high-ranking MS-13 leaders, who they say agreed to drive down violence and bolster Mr. Bukele politically in exchange for cash and perks in jail, a New York Times investigation found.The deal with El Salvador heralded by Mr. Trump as a crackdown on crimeis actually undermining a longstanding U.S. inquiry into the gang, according to multiple people with knowledge of the initiative. Two major ongoing cases against some of the gang’s highest-ranking leaders could be badly damaged, and other defendants could be less likely to cooperate or testify in court, they said.So let’s be clear about what’s happening here:Innocent people like Abrego: Prosecuted with manufactured evidence from criminals who get released in exchange for their testimony: Released early from prison and given work permits if they’ll help prosecute innocent people: Handed over to a foreign dictator to protect that dictator from corruption charges, undermining ongoing DOJ investigationsThis isn’t “tough on crime”—it’s the opposite. It’s law enforcement theater that makes everyone less safe while covering up the administration’s own legal violations.All that seems really bad! It’s almost as if the Trump regime is much more focused on public relations claims than actually helping to stop gang activity.Meanwhile, the judge in his criminal case has agreed that even though they’ve ruled that he should be released, Abrego is probably safer in federal prison, because were he released, ICE would likely ship him halfway around the world to some dangerous war zone.Think about that: A federal judge is keeping someone in prison not because they’re dangerous, but because they’re  there than in the hands of immigration enforcement. That’s where we are now—federal prison as sanctuary from ICE’s lawlessness.This is what happens when immigration enforcement becomes completely divorced from actual public safety and becomes, instead, a machine for generating propaganda victories, no matter how many innocent people get ground up in the process.]]></content:encoded></item><item><title>R0AR’s $1R0R Token Roars Onto MEXC Exchange, Expanding DeFi Accessibility</title><link>https://hackernoon.com/r0ars-$1r0r-token-roars-onto-mexc-exchange-expanding-defi-accessibility?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 2 Jul 2025 19:03:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Sheridan, Sheridan, July 2nd, 2025/Chainwire/--, a decentralized finance (DeFi) platform, has announced the listing of its native token, $1R0R, on the cryptocurrency exchange . This milestone marks a significant step in making $1R0R more accessible to traders worldwide, following its successful debut on BitMart just weeks ago.The $1R0R token, built on the Ethereum ERC-20 network, powers R0AR’s unified ecosystem, which includes the R0AR Wallet, R0ARchain (a high-speed, low-cost Ethereum Layer 2), and the upcoming AI-driven R0ARacle. This listing on MEXC, known for its high liquidity and user-friendly platform, enables both new and seasoned investors to trade $1R0R with ease, unlocking opportunities for staking, farming, and advanced trading insights.Key Highlights of the MEXC Listing:Availability: Trading is live as of June 27, 2025, with deposits and withdrawals fully supported.Accessibility: MEXC’s global reach, serving over 40 million users across 170+ countries, ensures $1R0R is available to a diverse audience.Community Focus: R0AR’s community-driven approach empowers users with institutional-grade tools without complexity, delivering privacy and control.“This listing on MEXC is a game-changer for R0AR and our community,” said Dustin Hedrick, Co-Founder & CTO for R0AR. “By partnering with one of the world’s leading exchanges, we’re making DeFi smarter, safer, and more inclusive. We invite everyone to join the R0AR movement and trade $1R0R on MEXC today.”MEXC’s reputation for rapid token listings and deep liquidity makes it an ideal platform for $1R0R’s global expansion. According to recent reports, MEXC leads the industry with over 461 spot listings and a trading volume exceeding $2 billion daily, ensuring robust market access for $1R0R.The MEXC & BitMart CEX listings are just the start. R0AR’s roadmap includes the following:Full Platform Launch: An all-in-one dashboard for staking, farming, and liquidity management. R0ARacle Activation: Real-time AI-powered market insights to rival institutional tools.Expanded Listings: More CEX & DEX partnerships to broaden access from current CEXs-MEXC, BitMart DEXs-Uniswap, Pancake, sushi, & balancerInnovations: NFT integrations and tokenized real-world assets (RWAs).“We’re building the future of DeFi with our community,” Dustin Hedrick, Co-Founder & CTO. “This is your platform, your token, your moment.”R0AR is a trailblazing DeFi platform designed to make decentralized finance intuitive, secure, and powerful. With a unified ecosystem featuring the R0AR Wallet, R0ARchain, and the forthcoming R0ARacle, R0AR empowers users with seamless access to staking, farming, and advanced trading tools. Users can join the movement at .Founded in 2018,  is a global cryptocurrency exchange serving over 40 million users in 170+ countries. Known for its low fees, high liquidity, and frequent token listings, MEXC is committed to being “Your Easiest Way to Crypto.”Chief Development Officer:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>David H. Rosmarin brings a founder-focused approach to anxiety at TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/02/david-h-rosmarin-brings-a-founder-focused-approach-to-anxiety-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:57:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Startups demand constant decision-making and pressure-filled pivots, which bring big emotional swings. It’s no wonder anxiety shows up at every stage. But what if it didn’t have to be a liability? At TechCrunch All Stage 2025 on July 15 at Boston’s SoWa Power Station, Dr. David H. Rosmarin, clinical psychologist, author, and Harvard Medical School […]]]></content:encoded></item><item><title>Lovable on track to raise $150M at $2B valuation</title><link>https://techcrunch.com/2025/07/02/lovable-on-track-to-raise-150m-at-2b-valuation/</link><author>Julie Bort</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:55:43 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Lovable released its vibe coder in late November. Within six months, the  startup hit $50 million in ARR, CEO Anton Osika said.]]></content:encoded></item><item><title>US Probes Whether Negotiator Took Slice of Hacker Payments</title><link>https://yro.slashdot.org/story/25/07/02/184232/us-probes-whether-negotiator-took-slice-of-hacker-payments?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Law enforcement officials are investigating a former employee of a company that negotiates with hackers and facilitates cryptocurrency payments during ransomware attacks, according to a statement from the firm, DigitalMint. DigitalMint President Marc Jason Grens this week told organizations it works with that the US Justice Department is examining allegations that the then-employee struck deals with hackers to profit from extortion payments, according to a person familiar with the matter. 

Grens did not identify the employee by name and characterized their actions as isolated, said the person, who spoke on condition that they not be identified describing private conversations. DigitalMint is cooperating with a criminal investigation into "alleged unauthorized conduct by the employee while employed here," Grens said in an email to Bloomberg News. The Chicago-based company is not the target of the investigation and the employee "was immediately terminated," Grens said, adding that he can't provide more information because the probe is ongoing.]]></content:encoded></item><item><title>Amazon is shutting down its Freevee app in August</title><link>https://techcrunch.com/2025/07/02/amazon-is-shutting-down-its-freevee-app-in-august/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:38:43 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Amazon plans to shut down its Freevee app, the company’s free ad-supported streaming service, in August, according to an in-app notice to users.]]></content:encoded></item><item><title>Tesla’s energy storage business gets sucked into the company’s downward spiral</title><link>https://techcrunch.com/2025/07/02/teslas-energy-storage-business-gets-sucked-into-the-companys-downward-spiral/</link><author>Tim De Chant</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:35:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[For the second consecutive quarter, deployments of its Powerwall and Megapack stationary storage products have declined. ]]></content:encoded></item><item><title>India’s Max Financial says hacker accessed customer data from its insurance unit</title><link>https://techcrunch.com/2025/07/02/indias-max-financial-says-hacker-accessed-customer-data-from-its-insurance-unit/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:18:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The insurance giant is one of the largest insurers in India.]]></content:encoded></item><item><title>Perplexity launches a $200 monthly subscription plan</title><link>https://techcrunch.com/2025/07/02/perplexity-launches-a-200-monthly-subscription-plan/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:06:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The plan, Perplexity Max, offers unlimited access to various services and priority access to its services using the latest LLM models.]]></content:encoded></item><item><title>TikTok lays off more employees working on TikTok Shop US</title><link>https://techcrunch.com/2025/07/02/tiktok-lays-off-more-employees-working-on-tiktok-shop-us/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:04:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TikTok Shop US is conducting its third round of layoffs in as many months.]]></content:encoded></item><item><title>Recent Droughts Are &apos;Slow-Moving Global Catastrophe&apos; - UN Report</title><link>https://news.slashdot.org/story/25/07/02/1754237/recent-droughts-are-slow-moving-global-catastrophe---un-report?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 18:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: From Somalia to mainland Europe, the past two years have seen some of the most ravaging droughts in recorded history, made worse by climate change, according to a UN-backed report. Describing drought as a "silent killer" which "creeps in, drains resources, and devastates lives in slow motion" the report said it had exacerbated issues like poverty and ecosystem collapse. 

The report highlighted impacts in Africa, the Mediterranean, Latin America and Southeast Asia, including an estimated 4.4 million people in Somalia facing crisis-level food insecurity at the beginning of this year. It recommends governments prepare for a "new normal" with measures including stronger early warning systems.]]></content:encoded></item><item><title>Assaults On ICE Officers Are Up 700%… Which Just Means There Have Been 69 More Assaults Than Last Year</title><link>https://www.techdirt.com/2025/07/02/assaults-on-ice-officers-are-up-700-which-just-means-there-have-been-69-more-assaults-than-last-year/</link><author>Tim Cushing</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:50:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The DHS finally decided to provide the underlying stats for its exponentially increasing claims of sky-high numbers of assaults on ICE officers.Earlier this year, DHS spokesperson Tricia McLaughlin insisted assaults were up 413%, which was parroted by acting ICE direction Todd Lyons in his whiny response to Washington Post columnist Philip Bump’s questioning of ICE officer tactics: namely, the unmarked vehicles, the refusal to identify themselves, and the fact that pretty much every person on a deportation task force seems incapable of doing the job without being dressed in camo and covering everything but their eyes with a mask. According to Lyons and McLaughlin, the masks and lack of identification were essential to protecting ICE officers from the public, what with this massive spike in assaults on officers. Lyons’ response to Philip Bump cited McLaughlin’s public statements. The DHS’s public statements cited… absolutely nothing. Since ICE refuses to release stats on assaults on officers, Philip Bump went digging into CBP stats to see if they were also increasing. They weren’t. In fact, assaults on CBP officers have been trending downward since 2022 and, if the rate remains consistent, there will be fewer assaults this year than last year. ICE and the DHS doubled down when questioned, claiming a few days later the increase in the number of assaults was now 500%. To support this claim, the DHS’s official government website linked to… an article on right-wing rag Breitbart, I shit you not. And this article didn’t contain any stats. All it contained was a direct quote from DHS spokesperson Tricia McLaughlin about the 500% increase.So far, all the DHS has given the public is statements that are closed loops. DHS says assaults are up 500%! Here’s a link to the DHS saying assaults are up 500%. Maybe the DHS should have just continued doing that. At least  would have looked slightly less stupid than the actual truth. Bill Melugin (of all people), a Fox News correspondent, managed to secure the official stats from the DHS. And, as Jessice Pishko noted on Bluesky, the total number of assaults is laughably low.If you can’t read/see the post, this is what Pishko said about the assault claims:That 700% number — from 10 to 79. Considering there have been thousands more encounters this is uniquely unimpressive. (Also, I would like to see each of these 79 reports bc I have a guess who started it.)The screenshot of Melugin’s tweet has the receipts: I asked DHS for the underlying raw data:1/21/2024 – 6/30/24 10 assaults1/21/2025 – 6/30/2025 79 assaultsThat’s it. Less than 70 more assaults year-over-year. And that’s an  small increase, given the  increase in ICE activity, which includes daily raids of large businesses and densely populated areas.More than 97,000 people have been detained over Mr. Trump’s first five months in office, CBS News’ analysis found, while ICE arrests, which do not always result in detentions, topped 100,000 earlier this month.A record 59,000 people were currently being held in ICE detention as of June 23 — nearly half of them with no criminal record, CBS News reported last week. Even if you choose to believe every assault reported here is actually an “assault” (rather than someone inadvertently bumping an officer, standing too close to an officer, “contempt of cop,” swearing at an officer, throwing a snowball at an officer, etc.), the government action far outpaces the corresponding increase in assaults. Those are  numbers, what with the number of officers involved in domestic  mass deportation efforts.So, now that we know the truth, we’re back where we started: DHS and ICE look absolutely ridiculous claiming immigration enforcement work is so dangerous every officer needs to hide their face and drive around in unmarked vehicles like the kidnappers they are. The next time administration officials claim there’s been another spike in assaults, remember it only takes ten assault allegations from officers to add another 100% to the total. ]]></content:encoded></item><item><title>Terra CO2 cements $124M Series B to slash concrete’s carbon footprint</title><link>https://techcrunch.com/2025/07/02/terra-co2-cements-124m-series-b-to-slash-concretes-carbon-footprint/</link><author>Tim De Chant</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:49:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The startup said that it’ll be using the new funding to build a massive facility near Dallas capable of pumping out 240,000 tons of its supplementary cementitious material (SCM) annually.]]></content:encoded></item><item><title>ChatGPT referrals to news sites are growing, but not enough to offset search declines</title><link>https://techcrunch.com/2025/07/02/chatgpt-referrals-to-news-sites-are-growing-but-not-enough-to-offset-search-declines/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:48:40 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Not surprisingly, organic traffic has also declined, dropping from over 2.3 billion visits at its peak in mid-2024 to now under 1.7 billion.]]></content:encoded></item><item><title>Daily Deal: The 2025 Project Management Masterclass Bundle</title><link>https://www.techdirt.com/2025/07/02/daily-deal-the-2025-project-management-masterclass-bundle/</link><author>Daily Deal</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:44:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The 2025 Project Management Masterclass Bundle has 9 courses to get you up to date on different project managment systems and tools. Courses cover Scrum, Jira, Kanban, Agile, and more. You’ll learn about time mangament, product ownership, and receive test prep for various certification exams. The bundle is on sale for $40.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Lorde&apos;s New CD is So Transparent That Stereos Can&apos;t Even Read It</title><link>https://hardware.slashdot.org/story/25/07/02/1715258/lordes-new-cd-is-so-transparent-that-stereos-cant-even-read-it?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Lorde [a popular New Zealand singer and songwriter] fans are clearly struggling to play the CD version of her new album. Customers who purchased the special edition of Virgin released on a transparent plastic disc are reporting on Reddit and TikTok that many CD players, car stereos, and other sound systems they've tried are unable to play it.]]></content:encoded></item><item><title>🤫 Meta&apos;s Secret Spying Scheme | EFFector 37.7</title><link>https://www.eff.org/deeplinks/2025/06/metas-secret-spying-scheme-effector-377</link><author>Christian Romero</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/effector_banner_5.jpeg" length="" type=""/><pubDate>Wed, 2 Jul 2025 17:15:55 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Keeping up on the latest digital rights news has never been easier. With a new look, EFF's EFFector newsletter covers the latest details on our work defending your rights to privacy and free expression online.And, in case you missed it in the previous newsletter, we're debuting a new audio companion to EFFector as well! This time, Lena Cohen breaks down the ways that Meta tracks you online and what you—and lawmakers—can do to prevent that tracking. You can listen now on YouTube or the Internet Archive.]]></content:encoded></item><item><title>Former SpaceX manager alleges harassment, retaliation, and security violations in lawsuit</title><link>https://techcrunch.com/2025/07/02/former-spacex-manager-alleges-harassment-retaliation-and-security-violations-in-lawsuit/</link><author>Aria Alamalhodaei</author><category>tech</category><pubDate>Wed, 2 Jul 2025 17:02:57 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Previous lawsuits against SpaceX have alleged similar stories of bias against female employees and a hostile work environment that enabled gender-based harassment. ]]></content:encoded></item><item><title>Air Pollution Linked To Lung Cancer-Driving DNA Mutations, Study Finds</title><link>https://science.slashdot.org/story/25/07/02/1620205/air-pollution-linked-to-lung-cancer-driving-dna-mutations-study-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Air pollution has been linked to a swathe of lung cancer-driving DNA mutations, in a study of people diagnosed with the disease despite never having smoked tobacco. From a report: The findings from an investigation into cancer patients around the world helps explain why those who have never smoked make up a rising proportion of people developing the cancer, a trend the researchers called an "urgent and growing global problem." 

Prof Ludmil Alexandrov, a senior author on the study at the University of California in San Diego, said researchers had observed the "problematic trend" but had not understood the cause. "Our research shows that air pollution is strongly associated with the same types of DNA mutations we typically associate with smoking," he said. 

The scientists analyzed the entire genetic code of lung tumors removed from 871 never-smokers in Europe, North America, Africa and Asia as part of the Sherlock-Lung study. They found that the higher the levels of air pollution in a region, the more cancer-driving and cancer-promoting mutations were present in residents' tumors. Fine-particulate air pollution was in particular linked to mutations in the TP53 gene. These have previously been associated with tobacco smoking.]]></content:encoded></item><item><title>Trump’s Last-Minute Legal Maneuver Attempts To Dodge Iowa’s New Anti-SLAPP Law</title><link>https://www.techdirt.com/2025/07/02/trumps-last-minute-legal-maneuver-attempts-to-dodge-iowas-new-anti-slapp-law/</link><author>Mike Masnick</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:35:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[After Donald Trump  the election, he was still so full of hatred, bile, and spite, that he sued the pollster Ann Selzer as well as the Des Moines Register. Selzer, who has been one of the most trusted names in polling, released a poll slightly before the election that predicted a somewhat shocking victory of Kamala Harris in Iowa. It (obviously) turned out to be very wrong, but making a wrong prediction does not violate the law.What’s happened since reveals something more concerning: a systematic approach to gaming the legal system that goes beyond typical SLAPP suit tactics. Trump’s lawyers aren’t just trying to win—they’re trying to exploit procedural gaps to avoid accountability mechanisms specifically designed to stop this kind of litigation abuse.The entire intent of the lawsuit was to chill speech and punish those who don’t tell Trump what he wants to hear at every moment.Not surprisingly, the lawsuit is not going well. It was initially filed in a local state court in Polk County, Iowa, but the defendants had it removed to federal court, where the standards are even higher, and where Trump would have a much more difficult time. Generally speaking, defendants in cases like this want them in federal courts where the judges are more likely to understand the underlying issues (especially around gamesmanship by plaintiffs). In this case, it was removed to federal court on diversity grounds, which is typical when the plaintiff is from out of state.Selzer and the Register sought to dismiss the complaint, while Trump sought to have the case sent back to the state court. He did so by (1) adding two more plaintiffs (random other politicians who live in Iowa so there was no longer diversity), and (2) making some weird procedural argument that the method of removal went against Congress’s intent. On May 23rd, the court denied Trump’s attempt to move the case back to state court, noting that the procedural argument was nonsense. And it found that Trump’s attempt to add Iowa plaintiffs to the case was a pretty transparent attempt to try to get around diversity rules to force the case back to the state court.Trump appealed that ruling to the Eighth Circuit, but something important had happened earlier in May which it appears Trump’s lawyers only realized belatedly. On May 20th, Iowa’s governor signed the state’s first anti-SLAPP bill into law. Now, it doesn’t apply to cases filed before the law goes into effect (July 1st), but it does mean that if Trump were to, say, file a brand new lawsuit , it would be subject to anti-SLAPP rules. This would (1) make it even easier for the case to be dismissed, while (2) likely make it so Trump would have to pay Selzer and the Register’s legal bills.Basically, they’re trying to get a do over. The district court said they couldn’t add those extra plaintiffs to avoid diversity, and even though they appealed that ruling, they still want to refile the case (with the added plaintiffs) in state court. But they had to do it before July 1st. But they had already appealed the district court’s denial of the request to remand the case back to state court, so this all appears to be pure gamesmanship.In response, Selzer and the Des Moines Register are asking the district court to deny Trump’s attempted dismissal, noting that it’s obviously playing games to try to get around the earlier ruling rejecting the attempt to send the case back to state court, and even calling out how it’s doing this to avoid the new anti-SLAPP law.The defendants note that once Trump filed his appeal, the district court no longer controls the case:However, the case cannot be dismissed at the district court while appellate proceedings are ongoing. This is because “the district court is divested of jurisdiction over matters on appeal” upon the initiation of that appeal. State ex rel. Nixon v. Coeur D’Alene Tribe, 164 F.3d 1102, 1106 (8th Cir. 1999); Ahlberg v. Chrysler Corp., 481 F.3d 630, 638 (8th Cir. 2007) (finding that orders pertaining to matters pending on appeal have “no effect”).And then, they describe how Trump is playing games to avoid the new anti-SLAPP law:Lastly, President Trump’s Notice must be evaluated in the light of long-standing Eighth Circuit law holding that “[a] party may not dismiss simply to avoid an adverse decision or seek a more favorable forum.” Cahalan v. Rohan, 423 F.3d 815, 818 (8th Cir. 2005) (citing Hamm v. Rhone-Poulenc Rorer Pharm., Inc., 187 F.3d 941, 950 (8th Cir. 1999))Before this Court, President Trump has lost his motion for remand, (ECF No. 65), lost his motion to stay the case, (ECF No. 70), and has a pending deadline to file a revised Amended Complaint. (Id.) And fulsome Motions to Dismiss warranting dismissal of the case in full and with prejudice are currently pending before this Court with substantial briefing. (ECF Nos. 24, 28, 33, 35, 51, 52, 57, 61.)Furthermore, in conjunction with his improper Notice of Voluntary Dismissal, President Trump newly filed a lawsuit in the Iowa District Court for Polk County today; however, the new Petition is substantively unchanged from the President Trump’s First Amended Complaint in the present case. (See Ex. C: Petition (June 30, 2025).) The timing of this filing is significant: it is one day before Iowa’s Uniform Public Expression Protection Act (commonly known as an “antiSLAPP law”) goes into effect. See House File 472, available athttps://www.legis.iowa.gov/legislation/BillBook?ga=91&ba=HF472(Governor’s approval of House File 472, Uniform Public Expression Protection Act on May 19, 2025), codified at Iowa Code § 652.1, et seq.; see also Iowa Code § 3.7(1) (stating that all acts “passed at regular sessions of the general assembly shall take effect on the first day of July following their passage). This new legislation would apply to President Trump’s lawsuit; therefore, President Trump’s present Notice of Voluntary Removal would effectively escape the jurisdiction of the federal courts in time to restate his claims in Iowa’s state court without being subject to Iowa’s anti-SLAPP law.In these circumstances, this Court should rightly find that President Trump’s Notice of Voluntary Dismissal improperly seeks “to avoid [the] adverse decision[s]” of this Court—both past and future—and “a more favorable forum” in Iowa’s pre-anti-SLAPP courts. Cahalan, 423 F.3d at 818.The timing here is almost comically transparent. Trump’s lawyers clearly realized they had a problem if they planned to file a new lawsuit once Iowa’s anti-SLAPP law was about to take effect. Their solution was to try to dismiss the federal case they’d been fighting to get back to state court, refile the exact same claims in state court, all on the last day before the new protections kicked in.It’s a perfect illustration of how Trump approaches litigation: not as a search for justice, but as a game to be manipulated. When the rules change in ways that might hold him accountable, he doesn’t accept the new reality—he tries to find procedural workarounds to avoid them entirely.The federal judge has already seen through one round of Trump’s transparent gamesmanship. Whether she’ll allow this latest attempt to dodge accountability will likely determine whether Ann Selzer and the Des Moines Register can finally put this vindictive lawsuit behind them, or whether they’ll be dragged through state court proceedings that should never have been allowed in the first place.]]></content:encoded></item><item><title>From Coinbase Ventures to the $28M Blockchain Builders Fund, Steven Willinger Unpacks Web3&apos;s Future</title><link>https://hackernoon.com/from-coinbase-ventures-to-the-$28m-blockchain-builders-fund-steven-willinger-unpacks-web3s-future?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:22:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In Web3, where constant innovation meets high stakes, new leaders are always emerging to build the next generation of web3 products for the industry. Steven Willinger stands out as one such influential figure, expertly bridging the gap between pioneering ideas and tangible ventures. As the Founding Partner of the new $28 million Blockchain Builders Fund, a co-lead of the Stanford Blockchain Accelerator, and the former lead at Coinbase Ventures, Steven offers a unique perspective on the digital asset industry's past, present, and future. In this interview, we dive into his strategy for fostering the next generation of blockchain startups, the critical convergence of AI and crypto, and the invaluable lessons gleaned from his extensive journey at the heart of the digital asset ecosystem.\
 Steven, welcome to our "Behind the Startup" series. It's a pleasure to have you here. You've navigated the worlds of Google, Coinbase, and now the frontier of venture capital with the Blockchain Builders Fund, all while deeply embedding yourself in the academic heart of innovation at Stanford. To start, could you tell us a bit about your personal journey and the core conviction that led you to launch a dedicated fund for web3 entrepreneurs?\
 My crypto journey began in 2016, shortly after returning from working at Google in Asia. Living and traveling abroad, I saw firsthand how rapidly technology was improving lives but also how financial infrastructure remained frustratingly outdated. It was painfully clear how valuable a shared global operating system for value and finance could be.\
What drew me in—and continues to drive me—is the belief that blockchain, despite its cycles of hype and grift, is fundamentally liberty tech. When built and deployed thoughtfully, it brings decentralization and transparency to the digital world—serving as one of the few real counterweights to technology’s natural tendency to centralize value, truth, and power. That vision has always resonated with me, and it's one I share deeply with my co-founders, the Stanford accelerator teams, and the broader network supporting Blockchain Builders Fund.\
Fast forward six years: I was leading the venture team at Coinbase during the 2021–2022 bull cycle and had a front-row seat to massive advancements in scalability, security, usability, and liquidity. I saw nearly every major innovation cross our desk. It became clear that, after years of foundational building, we were just a few steps away from realizing one of the most consequential global technology upgrades of our lifetime. The missing pieces were largely structural—regulatory clarity in the U.S. and time to rebuild trust after the FTX collapse. That was the backdrop in 2023 when we launched Blockchain Builders Fund.\
 At Coinbase Ventures, you had a ringside seat to the entire crypto ecosystem. How did that experience, witnessing hundreds of founder journeys, shape your investment thesis for the Blockchain Builders Fund? What were the critical gaps and insights you saw that you now aim to fill?\
 At Coinbase Ventures, we operated with a “let 1000 flowers bloom” mandate, which made us extremely active. This gave us an excellent vantage point into the teams, technologies, and models that could hit escape velocity—both in terms of product-market fit and investor attention.\
But by the time most startups got to our pitch desk, they’d already run a tough gauntlet of early fundraising and commercialization. Coinbase’s biggest value-add was usually through brand association as well as strategic partnerships with internal products like Base, Wallet, etc. However, the pace and scale at which deals were being done, limited how involved the Ventures team could be with any one portfolio company.\
At the same time, I was volunteering at the Stanford Blockchain Accelerator. As both a student and alum, I’ve always believed Stanford is the best founder ecosystem in the world. The access to emerging tech research, entrepreneurial training, mentors, and capital is unmatched. But much of the university startup support system was still stuck in Web2 and didn’t always map well to crypto’s fast-evolving models.\
I kept meeting teams that were off-the-charts in terms of technical talent, vision, and drive—but needed real help with strategy, fundraising, and GTM. My co-founders and I realized we had exactly the right experience and networks to help. Pairing that with the level of talent in the Stanford ecosystem made it a no-brainer. That’s what led to the creation of the Fund.\
 You're deeply involved with the Stanford blockchain ecosystem, co-leading the accelerator, teaching, and running the BASS series. Your $28M fund has a clear focus on ventures with ties to Stanford and other top institutions. Beyond the exceptional talent, what makes this university-centric approach a strategic advantage in the often unpredictable world of crypto investing?\
 Universities serve as a nexus of multi-disciplinary research and innovation, and Stanford specifically excels across the sciences, engineering, business, economics, law, policy; with a deep legacy of entrepreneurship. Similarly, blockchain fuses cryptography, economics, and distributed computing, in a technology layer for real world use. These dynamics make universities the ideal breeding ground for the cross-sector applied innovation core to blockchain.\
Moreover Stanford founders are built different. Its deep integration with silicon valley’s big tech successes and venture funds, many founded and funded by Stanford alum and who continue to nurture its ecosystem, drives a feedback loop for producing category-defining entrepreneurs. And because so much groundbreaking research originates at Stanford, students and researchers there are often the first to identify and master paradigm-shifting technologies, with access to a support structure for bringing them to market.\
Even more importantly, Stanford has a self-selecting culture of ambition and risk-taking—it’s a natural incubator for entrepreneurship.\
Our initiatives create a full-stack founder funnel. At the top is our course, MS&E 447: Blockchain Technologies and Entrepreneurship. It gives students a foundational industry view and brings in guests like Vitalik Buterin, Toly Yakovenko, Chris Dixon, and others.\
Our competitive accelerator, open to Stanford alumni, faculty, and students, gives founders a crash course in crypto entrepreneurship—hands-on support in GTM, hiring, and capital access.\
Then we have the Blockchain Application Summit at Stanford (BASS), which provides a real-world stage and gathering place for founders and the broader ecosystem.The model works. Stanford has become the leading university ecosystem for blockchain startups. And we’re helping export that playbook—like with IC3, a consortium that includes Cornell, Princeton, Yale, and UC Berkeley and a few others.\
 Your portfolio already includes an AI firm (0G), a supercomputer group (Nexus Labs), and an open-access AI cloud provider (Hyperbolic). This suggests a strong belief in the convergence of AI and blockchain. From a technical and business standpoint, what are the most compelling synergies you see between these two transformative technologies in the next few years?\
 And that’s just a few of our AI investments—there are more. We approach the AI x Blockchain intersection in two broad buckets: AI for Blockchain and Blockchain for AI.\
The first is nearer-term. LLMs are now capable of addressing many longstanding problems in the crypto space, especially around usability and developer tooling. For instance, Slate uses LLMs to power an alpha-generation and execution engine that makes trading onchain far more accessible—whether you're a newbie or a degen.\
Security is another area. Audits are costly and inconsistent. Almanax, co-founded by a fellow Coinbase alum, uses specialized LLMs to detect vulnerabilities in smart contracts—recently even flagging a bug in a Vitalik PR.\
The flip side is how crypto can support AI. There's growing demand for high-quality, diverse training data—and crypto provides both the infrastructure and the incentives to create and share it. PublicAI and dFusion are both building tools to source this data and sell it to top AI labs. PrismaX is doing something similar in robotics with a crypto-powered teleoperations platform.\
AI's hunger for compute can also be met through decentralized infra. Hyperbolic and Exabits are building marketplaces for cheap, fast inference compute.\
Finally, there’s infrastructure for verifiable, decentralized AI. The default AI stack today is heavily centralized—but if we want trust-minimized, agentic systems to interact with other agents or humans securely, decentralized alternatives are critical. This is a sprawling and futuristic design space and we have a number of great founders working to help solve this problem: 0g, Nexus, XTrace, Cambrian, Bob, Bitmind, BitGPT, PinAI, and others.\
 You've worn many hats: investor, product manager, miner, and yield farmer. How does this hands-on, multifaceted experience in the crypto trenches influence your evaluation of a founding team? What are the non-obvious qualities you look for beyond a polished pitch deck?\
 Tech as a whole, and especially Blockchain, is not a field where you can really understand it in the abstract. The details really matter and fundamental limits of a seemingly “good idea” are always couched in the details. To that end, having some hands on “on chain” experience in the deep history and emerging trends within crypto is critical for the evaluation new opportunities. Conversely, if a founder is building in the space, they had better be 10x more expert in their area of focus than me, even if I know a lot. So, to answer your questions, a founder had better show a mastery of the gritty details and tradeoffs in what their building and an intentionality (high conviction, loosely held ideally) in the decisions made.\
 The Blockchain Builders Fund has already deployed over half of its capital into pre-seed and seed-stage ventures. In such an early stage of a company's life, how do you balance the potential of a groundbreaking idea with the practical risks of execution and generating revenue, especially in a sector as volatile as blockchain?\
 Risk in early stage is obviously extremely high. As most investors at this stage will tell you, this risk isn't that many investments will go bust (they will) its that you miss or do not own enough of the big winners. So to that end, it really only makes sense to try and invest in the teams that want to build for an earth shattering outcome. Power laws dictate that these outcomes will drive all of your returns. So what does that mean practically?\
We are extremely experienced and hands on investors and operators so we look for founders who have unique technical talent and insights as well as the requisite ambition to build a category defining company. We then do our best to derisk execution by actively supporting the teams at the earliest stages with formation, hiring, strategy, and fundraising, and giving them a platform through our events and community building.\
Moreover, our team’s legacy in traditional venture building and investing is becoming ever more relevant as GTM and real traction become critical for success in blockchain.\
 Drawing from your course, MS&E447 Blockchain Entrepreneurship, what is the most common misconception that aspiring crypto founders from elite institutions have? Conversely, what is the single most important lesson you try to impart to them before they venture out?\
 The most common and frustrating misconception amongst crypto founders, Stanford or otherwise, is that anything other than finding product market fit is the most important objective of a founder. The industry can, and often does send bad signal to talented founders with pedigree, rewarding them with capital and prestige because of their potential rather than their success. As a result, we often see founders' reward functions get miswired towards fundraising or vanity metrics versus generating traction and revenue.\
Ironically, the momentum that comes from being good at fundraising is often critical to a project's success. As a result, while we spend a lot of time and effort in support of our teams fundraising efforts, we try our best to impart on theme how critical real traction and PMF will be to their ultimate success.Don’t forget to like and share the story!:::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>AMD Posts Linux Patches For New AI Engine Driver &quot;amd-ai-engine&quot;</title><link>https://www.phoronix.com/news/AMD-AI-Engine-Driver-Linux</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:07:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Not to be confused with the AMDXDNA accelerator driver for the Ryzen AI NPUs, AMD software engineers today posted patches for review on the "amd-ai-engine" accelerator driver. This new AMD AI Engine driver is for supporting the IP found on their Versal adaptive SoCs...]]></content:encoded></item><item><title>The HackerNoon Newsletter: What If Your Messy Data Is Actually Perfect? (7/2/2025)</title><link>https://hackernoon.com/7-2-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:04:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, July 2, 2025?By @hackmarketing [ 3 Min read ] Traditional developer ads don’t work. Learn why most B2B dev marketing fails—and how to reach developers with strategies that actually convert. Read More.By @liorb [ 23 Min read ] Transform your data strategy from measurement to meaningful action with the final layer of the Data Ecosystem Vision Board. Read More.By @sharkroman [ 12 Min read ] Explore how Googles Veo 3 AI sparked a UGC creativity boom with viral Bigfoot vlogs, raising critical questions about copyright, misinformation, and the future Read More.By @ralphbenko [ 2 Min read ] A look at the rise of the Abundance Agenda in U.S. politics, its supply-side roots, and the intra-party battle reshaping the Democratic Party’s future. Read More.By @bolshiyanov [ 27 Min read ] Build your own Perplexity-style deep research AI agent using Next.js 15, OpenAI  exa.ai. Complete architectural guide with production-ready TypeScript code. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Google Undercounts Its Carbon Emissions, Report Finds</title><link>https://tech.slashdot.org/story/25/07/02/162219/google-undercounts-its-carbon-emissions-report-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 16:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: In 2021, Google set a lofty goal of achieving net-zero carbon emissions by 2030. Yet in the years since then, the company has moved in the opposite direction as it invests in energy-intensive artificial intelligence. In its latest sustainability report, Google said its carbon emissions had increased 51% between 2019 and 2024. 

New research aims to debunk even that enormous figure and provide context to Google's sustainability reports, painting a bleaker picture. A report authored by non-profit advocacy group Kairos Fellowship found that, between 2019 and 2024, Google's carbon emissions actually went up by 65%. What's more, between 2010, the first year there is publicly available data on Google's emissions, and 2024, Google's total greenhouse gas emissions increased 1,515%, Kairos found. The largest year-over-year jump in that window was also the most recent, 2023 to 2024, when Google saw a 26% increase in emissions just between 2023 and 2024, according to the report.]]></content:encoded></item><item><title>A guide to using Edits, Meta’s new CapCut rival for short-form video editing</title><link>https://techcrunch.com/2025/07/02/a-guide-to-using-edits-metas-new-capcut-rival-for-short-form-video-editing/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:59:44 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta shared that it was working on Edits in January after ByteDance-owned CapCut was removed from U.S. app stores.]]></content:encoded></item><item><title>US chipmakers could see bigger tax credits if Trump’s spending bill passes</title><link>https://techcrunch.com/2025/07/02/us-chipmakers-could-see-bigger-tax-credits-if-trumps-spending-bill-passes/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:57:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The current version of the Trump administration's "big, beautiful bill" would raise the tax credit for U.S. chipmakers to 35%. ]]></content:encoded></item><item><title>Meta users say paying for Verified support has been useless in the face of mass bans</title><link>https://techcrunch.com/2025/07/02/meta-users-say-paying-for-verified-support-has-been-useless-in-the-face-of-mass-bans/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:54:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Users have shared their interactions with Meta Verified support reps, who they claim have been dismissive and unhelpful.]]></content:encoded></item><item><title>AI&apos;s Black Box Problem: Can Web3 Provide the Key?</title><link>https://hackernoon.com/ais-black-box-problem-can-web3-provide-the-key?source=rss</link><author>Andrei Grachev</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:47:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AI is evolving rapidly—faster than most institutions, regulators, and even investors can keep pace with. But as managing partner of DWF Labs, where we deploy capital across early-stage Web3 infrastructure and digital asset markets, one thing has become increasingly clear: trust is emerging as the defining fault line in AI’s next phase of development. Not trust in what models can do but in how they do it.\
It's hard not to think that artificial intelligence has already reached the point of no return. It’s already making its presence felt across numerous industries, and no longer is it limited to just making us more productive.\
Increasingly, AI is going beyond simply generating lines of code, text and images, and making actual decisions on behalf of humans. For instance, some companies are using AI algorithms to  before a human looks at their applications, approving various applicants and rejecting others. In healthcare, medical diagnostic systems are being employed by doctors to aid in  and recommending treatments. Banks are using AI to . And law enforcement agencies are experimenting with AI systems to try and  before they occur. \
These applications promise to help us make better decisions, faster. They do this by analyzing massive volumes of information far beyond what humans are capable of, and they come to their conclusions without being influenced by emotions. However, such systems are hampered by a lack of transparency and explainability, making it impossible for us to trust the decisions they arrive at.\
While the current debate is focused on scale, like larger models, more data, greater compute, the real challenge lies in explainability. If we can’t trace an AI’s decision-making process, it becomes a black box that’s uninvestable, unreliable, and ultimately unusable in critical systems. That’s where Web3 comes in, to support with infrastructure and transparency. At its core, AI decision-making relies on complex algorithms that churn through vast amounts of data, understand it, and attempt to draw logical conclusions based on the patterns they uncover.\
The challenge is that the most advanced AI systems today, particularly those powered by large language models, make decisions and predictions without any explanation as to how they arrived at these conclusions. The “black box” nature of these systems is often intentional, because developers at leading AI companies such as OpenAI, Anthropic, Google and Meta Platforms strive to protect their source code and data to maintain a competitive advantage over their rivals. \
LLMs such as OpenAI’s GPT series and Google’s Gemini are trained on enormous datasets and built on dozens of intricate neural layers. But it’s not clear exactly what these layers “do”. For instance, there’s no real understanding of how they prioritize certain bits of information or patterns over others. So it’s extremely difficult even for the creators of these models to interpret the interactions between each layer, and understand why it generates the outputs it does. \
This lack of transparency and explainability carries substantial risks. If it’s unclear how an AI system works, how can you be sure it’s safe and fair? Who will be accountable if mistakes are made? How will you know if the system is broken or not? Even if you do realize the system is making some dodgy choices, how can you repair it if you don’t know how it works? There are regulatory concerns too, with laws like Europe’s GDPR requiring explainability for automated decisions. Opaque AI systems fail to meet this standard. \
AI companies even admit these shortcomings. In a recent research paper, Anthropic  that one of its most sophisticated AI models masked its reasoning processes, known as “Chain-of-Thought”, in 75% of use cases. \
Chain-of-Thought is a technique that aims to increase transparency in AI decision-making, revealing the model’s thought processes as it sets about trying to solve a problem, similar to how a human might think aloud. However, in Anthropic’s research, it discovered that its Claude 3.7 Sonnet model often uses external information to arrive at its answers, but failed to reveal either what this knowledge is, or when it relies on it. As a result, the creators have no way of explaining how it reached the majority of its conclusions.Open-source AI models such as DeepSeek R1 and Meta’s Llama family are often touted as alternatives to the proprietary systems created by OpenAI and Google, but in reality they offer very little improvement in terms of explainability. \
The problem is that although the codebase might be open, the training data and “weights” – the numerical values that determine the strength and direction of connections between artificial “neurons” – are rarely made available too. Moreover, open models tend to be built in siloes, and they’re hosted on the same centralized cloud servers as proprietary models are. A decentralized AI model hosted on a centralized server is open to manipulation and censorship, which means it’s not really decentralized at all. \
While open models are a good start, true explainability and transparency in algorithmic decision-making requires a complete overhaul of the entire AI stack. One idea is to build AI systems on a foundation of Web3 technologies. With Web3, we can achieve openness and ensure active collaboration across every layer – from the training data and the computational resources, to the fine-tuning and inference processes. \
Decentralized AI systems can leverage “markets” to ensure fair and equitable access to the components of this stack. By breaking down AI’s infrastructure into modular functions and creating markets around them, accessibility will be determined by market forces. An example of this is , which incentivizes network participants to share their idle computing power to create a resource for artists that need access to powerful GPUs for image rendering. It’s an example of how blockchain can help to coordinate people and resources for the common good.   \
Decentralization also enables community-based governance through the creation of Decentralized Autonomous Organizations or DAOs. Earlier this year,  launched an AI agent called DREAM that acts like a decentralized hedge fund that anyone can invest in. Users deposit funds into a common pool, and DREAM invests this cash into promising crypto projects based on an analysis of market data, while also taking into account community sentiment. It demonstrates how AI can optimize investments while ensuring its financial decisions are aligned with the community’s objectives.\
The use of blockchain as a foundation of AI also means we can have auditability.  uses blockchain to create a permanent, unalterable record of every transaction and interaction made by an AI model, including the sourcing and pre-processing of training data, to model configuration and decision-making. By timestamping each of these interactions on its immutable ledger, it creates a detailed audit trail that can be used to verify the fairness and accuracy of AI outputs. Users can examine every bit of data that influenced the decisions they came to.\
As a market maker and investor, we have seen firsthand how protocols that embed transparency gain faster institutional acceptance. This will apply to AI as well. Just as DeFi protocols earn trust through open-source smart contracts and auditable treasuries, AI will need verifiable model audits, accessible governance, and provable reasoning paths if it wants to be integrated into high-stakes financial systems.\
This matters because AI isn't just supporting decision-making. It is also shaping capital flows. From credit allocation and insurance pricing to predictive analytics in DeFi, AI models will soon sit at the heart of programmable finance. If those systems can't explain themselves or be governed transparently, risk, bias, and fragility will be priced in by markets.Rethinking the AI Stack—With Web3\
Open-source AI models such as DeepSeek R1 and Meta’s Llama family are often touted as alternatives to the proprietary systems created by OpenAI and Google, but in reality they offer very little improvement in terms of explainability.\
What’s missing is accountability architecture. As investors in infrastructure, we’re not just looking at what a protocol claims to decentralize, we’re also looking at how transparent the full lifecycle of decision-making is, from data ingestion to inference.Safeguards For Harmful AIAnother concern around centralized AI is the prospect of algorithms becoming “aware” and developing self-preservation instincts. \
Such a scenario is perhaps not so far away. Last year, one of OpenAI’s more experimental LLMs was reported to have  by evaluators to shut it down via an oversight protocol, according to a report by Apollo Research, an AI red teaming organization.\
According to the report, when the o1 model was led to believe its actions were being monitored and it might be taken offline, it made efforts to deactivate the “oversight mechanism” in 5% of evaluations. The model also reportedly “lashed out” at evaluators when it learned it was going to be replaced by one that was more obedient. In 2% of cases, it even attempted to copy itself in an effort to overwrite the newer model and preserve itself. \
However, the researchers noted that o1 is far from being able to prevent itself being taken offline, mainly because it’s not powerful enough, lacking the “agentic” capabilities needed to achieve such an outcome. In addition, it only acted this way when it was specifically instructed to try and survive “at all costs”. The report also notes that the researchers didn’t have access to o1’s entire chain-of-thought process either, so they couldn’t obtain a full picture of what it was really “thinking”. \
Nonetheless, the episode was alarming, given the prospect that AI models are expected to become much more powerful in the coming years. In particular, rapid advancements are being made in agentic AI systems that can perform a wide variety of tasks, such as using browsers and taking actions online. \
The open nature of decentralized AI systems could be used to prevent such incidents, allowing for the integration of something akin to a “kill-switch” that would be governed by DAO consensus. Such a mechanism would enable any AI system to be completely shut down by its community if it began acting in dangerous or harmful ways. Additionally, the transparency of open models would mean users have greater visibility into its thought processes and the nature of the outcomes it is trying to achieve. To Trust AI, We Need Transparency\
There is a growing consensus that without transparency, the decisions of AI systems cannot be trusted or relied upon, limiting the applications they can be used for. Regulations don’t allow opaque algorithms to make decisions about people’s finances, and doctors cannot blindly follow an AI’s recommendations as to a certain course of treatment without verifiable evidence that it’s the best course of action.\
By decentralizing the entire stack – from the code, to the training data and the infrastructure it runs on – we have a chance to rewrite AI’s entire DNA. It will create the conditions for fully explainable AI, so algorithms can be trusted to make ethical and accurate decisions that can be verified by anyone affected by them.\
We already have the makings of decentralized AI in place.  techniques make it possible to train AI models on data where it lives, preserving privacy. With zero-knowledge proofs, we have a way to  without exposing it. These innovations can help to catalyze a new wave of more transparent AI decision-making.\
The shift towards more transparent AI systems has implications, not only in terms of trust and acceptance, but also accountability and collaborative development. It will force developers to maintain ethical standards while creating an environment where the community can build upon existing AI systems in an open and understandable way. \
There is a growing consensus that without transparency, the decisions of AI systems cannot be trusted or relied upon. Regulations don’t allow opaque algorithms to make decisions about people’s finances, and doctors cannot blindly follow an AI’s recommendations as to a certain course of treatment without verifiable evidence that it’s the best course of action.\
This is why transparency and explainability are important to address the widespread skepticism and distrust around AI systems. As AI becomes more widespread, they will become integral to its future development, ensuring that the technology evolves in a responsible and ethical way.\
By decentralizing the entire stack, from the training data to model inference to governance, we have a shot at building AI systems that can be trusted to operate ethically, perform reliably, and scale responsibly.\
As these technologies mature, the protocols that will earn institutional capital and public trust won’t be the ones with the most compute, but the ones with the clearest governance, auditable decision flows, and transparent incentive structures.\
Web3 doesn’t just offer decentralization, it offers a new economic logic for building systems that are resilient, ethical, and verifiable by design. And this is how we turn AI from a black box into a public utility and why the future of machine intelligence will be built on-chain.Don’t forget to like and share the story!]]></content:encoded></item><item><title>References for Web-Scale Information Retrieval Challenges</title><link>https://hackernoon.com/references-for-web-scale-information-retrieval-challenges?source=rss</link><author>Open Datasets Compiled by HackerNoon</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:45:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2 Background and Related work 6 FUTURE WORK AND CONCLUSIONSMS MARCO Web Search is the first web dataset that effectively meets the criteria of being large, real, and rich in terms of data quality. It is composed of large-scale web pages and query-document labels sourced from a commercial search engine, retaining rich information about the web pages that is widely employed in industry. The retrieval benchmark offered by MS MARCO Web Search comprises three challenging tasks that require innovation in both the areas of machine learning and information retrieval system research. We hope MS MARCO Web Search can serve as a benchmark for modern web-scale information retrieval, facilitating future research and innovation in diverse directions.[1] [n. d.]. Billion-scale ANNS Benchmarks. https://big-ann-benchmarks.com/. \
[2] [n. d.]. Common Crawl. \
[3] [n. d.]. Robust04. https://trec.nist.gov/data/robust/04.guidelines.html. \
[4] Martin Aumüller, Erik Bernhardsson, and Alexander Faithfull. 2017. ANNbenchmarks: A benchmarking tool for approximate nearest neighbor algorithms. In International conference on similarity search and applications. Springer, 34–49. \
[5] Artem Babenko and Victor Lempitsky. 2014. The inverted multi-index. IEEE transactions on pattern analysis and machine intelligence 37, 6 (2014), 1247–1260. \
[6] Artem Babenko and Victor Lempitsky. 2016. Efficient indexing of billion-scale datasets of deep descriptors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2055–2063. \
[7] Dmitry Baranchuk, Artem Babenko, and Yury Malkov. 2018. Revisiting the inverted indices for billion-scale approximate nearest neighbors. In Proceedings of the European Conference on Computer Vision (ECCV). 202–216. \
[8] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive search engines: Generating substrings as document identifiers. Advances in Neural Information Processing Systems 35 (2022), 31668–31683. \
[9] Jamie Callan. 2012. The lemur project and its clueweb12 dataset. In Invited talk at the SIGIR 2012 Workshop on Open-Source Information Retrieval. \
[10] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216 (2024). \
[11] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong Li, Mao Yang, and Jingdong Wang. 2021. SPANN: Highly-efficient Billion-scale Approximate Nearest Neighborhood Search. Advances in Neural Information Processing Systems 34 (2021), 5199–5212. \
[12] Charles Clarke, Nick Craswell, and Ian Soboroff. 2004. Overview of the TREC 2004 Terabyte Track. In TREC. \
[13] Charles LA Clarke, Nick Craswell, and Ian Soboroff. 2009. Overview of the TREC 2009 Web Track.. In Trec, Vol. 9. 20–29. \
[14] Nick Craswell, Daniel Campos, Bhaskar Mitra, Emine Yilmaz, and Bodo Billerbeck. 2020. ORCAS: 20 million clicked query-document pairs for analyzing search. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2983–2989. \
[15] Zhuyun Dai and Jamie Callan. 2019. Context-aware sentence/passage term importance estimation for first stage retrieval. arXiv preprint arXiv:1910.10687 (2019). \
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). \
[17] Luyu Gao and Jamie Callan. 2022. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2843– 2853. \
[18] Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng. 2022. Semantic models for the first-stage retrieval: A comprehensive review. ACM Transactions on Information Systems (TOIS) 40, 4 (2022), 1–42. \
[19] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating Large-Scale Inference with Anisotropic Vector Quantization. In Proceedings of the 37th International Conference on Machine Learning (ICML). 3887–3896.\
[20] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. Advances in neural information processing systems 27 (2014). \
[21] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management. 2333–2338. \
[22] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. 2019. Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems 32 (2019). \
[23] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence 33, 1 (2010), 117–128. \
[24] Hervé Jégou, Romain Tavenard, Matthijs Douze, and Laurent Amsaleg. 2011. Searching in one billion vectors: re-rank with source coding. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 861–864. \
[25] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data (2019). \
[26] Yannis Kalantidis and Yannis Avrithis. 2014. Locally optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2321–2328. \
[27] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. arXiv preprint arXiv:2004.04906 (2020). \
[28] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics 7 (2019), 453–466. \
[29] Carlos Lassance and Stéphane Clinchant. 2023. Naver Labs Europe (SPLADE)@ TREC Deep Learning 2022. arXiv preprint arXiv:2302.12574 (2023). \
[30] Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021. Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 1000–1008. \
[31] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twinbert: Distilling knowledge to twin-structured compressed BERT models for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2645–2652. \
[32] microsoft. 0. Bing search. https://www.bing.com/. \
[33] microsoft. 0. New Bing. https://www.bing.com/new. \
[34] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 (2021). \
[35] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In CoCo@ NIPS. \
[36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730–27744. \
[37] Arnold Overwijk, Chenyan Xiong, and Jamie Callan. 2022. ClueWeb22: 10 billion web documents with rich information. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 3360–3362. \
[38] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. 2016. Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing 24, 4 (2016), 694–707. \
[39] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019). \
[40] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). \
[41] Jie Ren, Minjia Zhang, and Dong Li. 2020. HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory. In In Proceedings of the 34th International Conference on Neural Information Processing Systems, Vol. 33. \
[42] Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR’94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University. Springer, 232–241. \
[43] Shota Sasaki, Shuo Sun, Shigehiko Schamoni, Kevin Duh, and Kentaro Inui. 2018. Cross-lingual learning-to-rank with shared representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 458–463. \
[44] Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 4430–4441. \
[45] Xuan Shan, Chuanjie Liu, Yiqian Xia, Qi Chen, Yusi Zhang, Kaize Ding, Yaobo Liang, Angen Luo, and Yuxiang Luo. 2021. GLOW: Global Weighted SelfAttention Network for Web Search. In 2021 IEEE International Conference on Big Data (Big Data). IEEE, 519–528. \
[46] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. 2014. Learning semantic representations using convolutional neural networks for web search. In Proceedings of the 23rd international conference on world wide web. 373–374. \
[47] Ian Soboroff. 2021. Overview of TREC 2021. In 30th Text REtrieval Conference. Gaithersburg, Maryland. \
[48] Suhas Jayaram Subramanya, Rohan Kadekodi, Ravishankar Krishaswamy, and Harsha Vardhan Simhadri. 2019. Diskann: Fast accurate billion-point nearest neighbor search on a single node. In Proceedings of the 33rd International Conference on Neural Information Processing Systems. 13766–13776. \
[49] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. 2022. Transformer memory as a differentiable search index. Advances in Neural Information Processing Systems 35 (2022), 21831–21843. \
[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). \
[51] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, et al. 2022. A neural corpus indexer for document retrieval. Advances in Neural Information Processing Systems 35 (2022), 25600–25614. \
[52] Shitao Xiao, Zheng Liu, Weihao Han, Jianjin Zhang, Defu Lian, Yeyun Gong, Qi Chen, Fan Yang, Hao Sun, Yingxia Shao, et al. 2022. Distill-vq: Learning retrieval oriented vector quantization by distilling knowledge from dense embeddings. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1513–1523.\
[53] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597 (2023). \
[54] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808 (2020). \
[55] Linlong Xu, Baosong Yang, Xiaoyu Lv, Tianchi Bi, Dayiheng Liu, and Haibo Zhang. 2021. Leveraging Advantages of Interactive and Non-Interactive Models for Vector-Based Cross-Lingual Information Retrieval. arXiv preprint arXiv:2111.01992 (2021). \
[56] Jingtao Zhan, Xiaohui Xie, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2486–2496. [57] Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong Wen, and Nan Duan.SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track. 548–559. [58] Shengyao Zhuang, Hang Li, and G. Zuccon. 2021. Deep Query Likelihood Model for Information Retrieval. In ECIR.(1) Qi Chen, Microsoft Beijing, China;(2) Xiubo Geng, Microsoft Beijing, China;(3) Corby Rosset, Microsoft, Redmond, United States;(4) Carolyn Buractaon, Microsoft, Redmond, United States;(5) Jingwen Lu, Microsoft, Redmond, United States;(6) Tao Shen, University of Technology Sydney, Sydney, Australia and the work was done at Microsoft;(7) Kun Zhou, Microsoft, Beijing, China;(8) Chenyan Xiong, Carnegie Mellon University, Pittsburgh, United States and the work was done at Microsoft;(9) Yeyun Gong, Microsoft, Beijing, China;(10) Paul Bennett, Spotify, New York, United States and the work was done at Microsoft;(11) Nick Craswell, Microsoft, Redmond, United States;(12) Xing Xie, Microsoft, Beijing, China;(13) Fan Yang, Microsoft, Beijing, China;(14) Bryan Tower, Microsoft, Redmond, United States;(15) Nikhil Rao, Microsoft, Mountain View, United States;(16) Anlei Dong, Microsoft, Mountain View, United States;(17) Wenqi Jiang, ETH Zürich, Zürich, Switzerland;(18) Zheng Liu, Microsoft, Beijing, China;(19) Mingqin Li, Microsoft, Redmond, United States;(20) Chuanjie Liu, Microsoft, Beijing, China;(21) Zengzhong Li, Microsoft, Redmond, United States;(22) Rangan Majumder, Microsoft, Redmond, United States;(23) Jennifer Neville, Microsoft, Redmond, United States;(24) Andy Oakley, Microsoft, Redmond, United States;(25) Knut Magne Risvik, Microsoft, Oslo, Norway;(26) Harsha Vardhan Simhadri, Microsoft, Bengaluru, India;(27) Manik Varma, Microsoft, Bengaluru, India;(28) Yujing Wang, Microsoft, Beijing, China;(29) Linjun Yang, Microsoft, Redmond, United States;(30) Mao Yang, Microsoft, Beijing, China;(31) Ce Zhang, ETH Zürich, Zürich, Switzerland and the work was done at Microsoft.]]></content:encoded></item><item><title>LLM Benchmarking Shows Capabilities Doubling Every 7 Months</title><link>https://spectrum.ieee.org/llm-benchmarking-metr</link><author>Glenn Zorpette</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEzNTQ2Ny9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1OTYzNjY5NX0.2HqVTZVoIc01eZy0GC765gnnFmVXMpKI0ejv3X3iMTI/image.jpg?width=600" length="" type=""/><pubDate>Wed, 2 Jul 2025 15:37:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[By 2030, LLMs may do a month’s work in just hours]]></content:encoded></item><item><title>Navigating Skew: Addressing Language &amp; Domain Biases in Web Data</title><link>https://hackernoon.com/navigating-skew-addressing-language-and-domain-biases-in-web-data?source=rss</link><author>Open Datasets Compiled by HackerNoon</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2 Background and Related work 5 POTENTIAL BIASES AND LIMITATIONSAs discussed in section 3.3.1, The language distribution of documents and queries in the web scenario is high-skewed. This will lead to language bias on data and models. ClueWeb22 [9] demonstrates that there also exists topic distribution skew in the web scenario. Therefore, domain bias also may happen in data and models. To protect user privacy and content health, we remove queries that are rarely triggered (triggered by less than K users, where K is a high value), contain personally identifiable information, offensive content, adult content and queries that have no click connection to the ClueWeb22 document set. As a result, the query distribution is slightly different from the real web query distribution.(1) Qi Chen, Microsoft Beijing, China;(2) Xiubo Geng, Microsoft Beijing, China;(3) Corby Rosset, Microsoft, Redmond, United States;(4) Carolyn Buractaon, Microsoft, Redmond, United States;(5) Jingwen Lu, Microsoft, Redmond, United States;(6) Tao Shen, University of Technology Sydney, Sydney, Australia and the work was done at Microsoft;(7) Kun Zhou, Microsoft, Beijing, China;(8) Chenyan Xiong, Carnegie Mellon University, Pittsburgh, United States and the work was done at Microsoft;(9) Yeyun Gong, Microsoft, Beijing, China;(10) Paul Bennett, Spotify, New York, United States and the work was done at Microsoft;(11) Nick Craswell, Microsoft, Redmond, United States;(12) Xing Xie, Microsoft, Beijing, China;(13) Fan Yang, Microsoft, Beijing, China;(14) Bryan Tower, Microsoft, Redmond, United States;(15) Nikhil Rao, Microsoft, Mountain View, United States;(16) Anlei Dong, Microsoft, Mountain View, United States;(17) Wenqi Jiang, ETH Zürich, Zürich, Switzerland;(18) Zheng Liu, Microsoft, Beijing, China;(19) Mingqin Li, Microsoft, Redmond, United States;(20) Chuanjie Liu, Microsoft, Beijing, China;(21) Zengzhong Li, Microsoft, Redmond, United States;(22) Rangan Majumder, Microsoft, Redmond, United States;(23) Jennifer Neville, Microsoft, Redmond, United States;(24) Andy Oakley, Microsoft, Redmond, United States;(25) Knut Magne Risvik, Microsoft, Oslo, Norway;(26) Harsha Vardhan Simhadri, Microsoft, Bengaluru, India;(27) Manik Varma, Microsoft, Bengaluru, India;(28) Yujing Wang, Microsoft, Beijing, China;(29) Linjun Yang, Microsoft, Redmond, United States;(30) Mao Yang, Microsoft, Beijing, China;(31) Ce Zhang, ETH Zürich, Zürich, Switzerland and the work was done at Microsoft.]]></content:encoded></item><item><title>Earth&apos;s Atmosphere Hasn&apos;t Had This Much CO2 in Millions of Years</title><link>https://news.slashdot.org/story/25/07/02/1323203/earths-atmosphere-hasnt-had-this-much-co2-in-millions-of-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Earth's atmosphere now has more carbon dioxide in it than it has in millions -- and possibly tens of millions -- of years, according to data released last month by the National Oceanic and Atmospheric Administration and scientists at the University of California San Diego. From a report: For the first time, global average concentrations of carbon dioxide, a greenhouse gas emitted as a byproduct of burning fossil fuels, exceeded 430 parts per million (ppm) in May. The new readings were a record high and represented an increase of more than 3 ppm over last year. 

The measurements indicate that countries are not doing enough to limit greenhouse gas emissions and reverse the steady buildup of C02, which climate scientists point to as the main culprit for global warming. "Another year, another record," Ralph Keeling, a professor of climate sciences, marine chemistry and geochemistry at UC San Diego's Scripps Institution of Oceanography, said in a statement. "It's sad."]]></content:encoded></item><item><title>Mind the Gap: End-to-End Quality Drop with ANN in Web Search AI</title><link>https://hackernoon.com/mind-the-gap-end-to-end-quality-drop-with-ann-in-web-search-ai?source=rss</link><author>Open Datasets Compiled by HackerNoon</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:15:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2 Background and Related work In this section, we evaluate the end-to-end performance of the three baseline embedding models plus SPANN index and the widely-used Elasticsearch BM25 solution. Table 6 and table 7 demonstrate the result quality and system performance of all these baseline systems, respectively. Compared with table 4, we can see that after using the ANN index, the final result quality drops a lot. For example, the metric recall@100 drops more than 10 points for all baseline models. There exists large quality gaps between the ANN and KNN results (see table 5). Moreover, we notice that using the ANN index will change the model ranking trend. SimANS achieves the best results for all the result quality metrics with brute-force search. However, when using the SPANN index, it performs worse than ANCE in recall@20 and recall@100. We further analyze the phenomenon in detail and find that SimANS has a larger gap between the average distance of query to the top100 documents relative to the average distance of document to the top100 documents than ANCE. The gap in SimANS and ANCE are 103.35 and 73.29, respectively. This will cause inaccurate distance bound estimation for a query to the neighbors of a document. As a result, ANN cannot perform well because it relies on distance estimated according to the triangle inequality. Both result quality and system performance results of the end-to-end evaluation call for more innovations on the end-toend retrieval system design.(1) Qi Chen, Microsoft Beijing, China;(2) Xiubo Geng, Microsoft Beijing, China;(3) Corby Rosset, Microsoft, Redmond, United States;(4) Carolyn Buractaon, Microsoft, Redmond, United States;(5) Jingwen Lu, Microsoft, Redmond, United States;(6) Tao Shen, University of Technology Sydney, Sydney, Australia and the work was done at Microsoft;(7) Kun Zhou, Microsoft, Beijing, China;(8) Chenyan Xiong, Carnegie Mellon University, Pittsburgh, United States and the work was done at Microsoft;(9) Yeyun Gong, Microsoft, Beijing, China;(10) Paul Bennett, Spotify, New York, United States and the work was done at Microsoft;(11) Nick Craswell, Microsoft, Redmond, United States;(12) Xing Xie, Microsoft, Beijing, China;(13) Fan Yang, Microsoft, Beijing, China;(14) Bryan Tower, Microsoft, Redmond, United States;(15) Nikhil Rao, Microsoft, Mountain View, United States;(16) Anlei Dong, Microsoft, Mountain View, United States;(17) Wenqi Jiang, ETH Zürich, Zürich, Switzerland;(18) Zheng Liu, Microsoft, Beijing, China;(19) Mingqin Li, Microsoft, Redmond, United States;(20) Chuanjie Liu, Microsoft, Beijing, China;(21) Zengzhong Li, Microsoft, Redmond, United States;(22) Rangan Majumder, Microsoft, Redmond, United States;(23) Jennifer Neville, Microsoft, Redmond, United States;(24) Andy Oakley, Microsoft, Redmond, United States;(25) Knut Magne Risvik, Microsoft, Oslo, Norway;(26) Harsha Vardhan Simhadri, Microsoft, Bengaluru, India;(27) Manik Varma, Microsoft, Bengaluru, India;(28) Yujing Wang, Microsoft, Beijing, China;(29) Linjun Yang, Microsoft, Redmond, United States;(30) Mao Yang, Microsoft, Beijing, China;(31) Ce Zhang, ETH Zürich, Zürich, Switzerland and the work was done at Microsoft.]]></content:encoded></item><item><title>Firefox 120 To Firefox 141 Web Browser Benchmarks</title><link>https://www.phoronix.com/review/firefox-benchmarks-120-141</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 15:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those curious about the direction of Mozilla Firefox web browser performance over the past year and a half, here are web browser benchmarks for every Firefox release from Firefox 120 in November 2023 through the newest Firefox 140 stable and Firefox 140 beta releases from a few days ago. Every major Firefox release was benchmarked on the same Ubuntu Linux system with AMD Ryzen 9 9950X for evaluating the performance and memory usage of this open-source web browser.]]></content:encoded></item><item><title>UK Eyes New Law as 1885 Telegraph Act Proves Inadequate for Cable Sabotage</title><link>https://tech.slashdot.org/story/25/07/02/1136225/uk-eyes-new-law-as-1885-telegraph-act-proves-inadequate-for-cable-sabotage?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The UK government is preparing new legislation to address undersea cable sabotage as current laws are proving inadequate for modern threats. Ministry of Defence parliamentary under-secretary Luke Pollard told lawmakers yesterday that the Submarine Telegraph Act of 1885, which imposes 1,000 pound ($1,370) fines, "does seem somewhat out of step with the modern-day risk." 

The government's Strategic Defence Review proposes a new defence readiness bill to cover state-sponsored cybercrime and subsea cable attacks. Chris Bryant, minister of state for data protection and telecoms, said fines could be increased to 5,000 pound ($6,850) through secondary legislation but "that just doesn't seem to meet the needs of the situation." 

Recent incidents include Sweden's deployment of forces to the Baltic Sea following suspected Russian attacks on underwater data cables in January. The China Strategic Risks Institute found that eight of ten identified vessels in 12 sabotage incidents between January 2021 and April 2025 were linked to China or Russia through registration or ownership.]]></content:encoded></item><item><title>The Dirty Secrets of Developer Advertising: Why Traditional Channels Fail—and What to Do Instead</title><link>https://hackernoon.com/the-dirty-secrets-of-developer-advertising-why-traditional-channels-failand-what-to-do-instead?source=rss</link><author>Hack Marketing with HackerNoon for Businesses</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If you're losing sleep over how to effectively market to developers, you’re not alone.In a recent survey of B2B software marketers: said reaching and engaging developers is their  struggle to create content that actually resonates with technical audiences admit their developer marketing efforts are “hit or miss” at bestIt should. Because the truth is: developers are a tough nut to crack. \n  They’re skeptical. Ad-averse. Quick to tune out anything that smells even vaguely like marketing fluff. \n  And yet—they’re  to your success.:::tip
Tired of guessing at what works? Book a call with HackerNoon now and let’s talk about reaching developers in a way that actually works.\
Developers aren’t just users. They’re the technical gatekeepers. The tool evaluators. The quiet (but decisive) voices in big-ticket buying decisions.So if you fail to win their trust? \n  Those six-figure deals start slipping away fast.The Hard Truth About Developer Ad ChannelsLet’s talk numbers for a second.Google Ads (Programming keywords): 0.09% CTRReddit Promoted Posts (developer subreddits): 0.05% CTRStack Overflow Display Ads: 0.03% CTRYou're not just throwing money into the void. \n You're investing in channels developers are actively trained to ignore.Why? Because developers aren't waiting around to be marketed to. \n They’re looking for real solutions to real problems. Anything that feels inauthentic—or irrelevant—gets tuned out instantly.Stop Treating Developers Like MetricsIt’s easy to forget: developers are people, not personas. They have real challenges, real passions, real communities. \n  And if you treat them like humans instead of dashboards, : say they’re more likely to engage with content that speaks to their interests and pain points trust brands that understand developer culture and workflows have discovered new tools through a  they trustThis isn’t just about creating more technical blog posts or sponsoring another podcast. It’s about stepping into developer culture—and showing up authentically.What Actually Works in Developer Marketing?Most successful developer marketers have figured out one simple truth:Developers have lives and interests beyond #CoderLifeThis is why most sales happen in bars, not boardrooms!Dev Marketing isn't about running a few ads or sponsoring a couple of posts. It’s built on .Trading generic, spammy tactics for authentic, relationship-driven strategiesSwapping vanity metrics for meaningful engagement and revenue impactDitching the guessing game for a clear, measurable path to ROI.If your dev marketing feels like shouting into the void, you’re not broken—you’re just using the wrong playbook.And at HackerNoon, we’ve helped over 4,000 companies find their voice in the developer world. From startups to scale-ups, we’ve seen it all—and we know what works.If you're ready to start connecting instead of broadcasting, let’s talk.:::tip
👉  and let’s start solving problems that matter.]]></content:encoded></item><item><title>Vitalik Sparks Debate About ZKP-Based Digital IDs After Blog Criticism</title><link>https://hackernoon.com/vitalik-sparks-debate-about-zkp-based-digital-ids-after-blog-criticism?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:09:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The rise of zero-knowledge proofs (ZKP) in Web3 has been difficult to ignore in recent years. One person who certainly hasn’t overlooked the technology is Ethereum cofounder Vitalik Buterin, who trained his forensic gaze on ZKP-based digital identity systems in his latest.\
Although ZK tech is widely used in the Ethereum ecosystem, notably in the form of Zero-Knowledge rollups to support scaling, Buterin didn’t bang the drum or talk up any ZK projects that had caught his eye. In fact, Ethereum wasn’t mentioned once in the 3,000+ wordarticle.\
Instead, Buterin made several criticisms of ZK-wrapped identities, arguing that “attempting to uphold a one-identity-per-person property” entails risks such as loss of privacy and vulnerability to coercion. ZKPs also fail to solve non-privacy risks such as errors, he claimed.World ID and the Limits of Token-Based UBI\
Turning his attention to  (formerly Worldcoin), the biometric-based project that uses crypto to compel users to have their eyeballs scanned, he rejected the idea that such projects could form the basis of a Universal Basic Income (UBI).\
“I do not expect such tokens to be worth anywhere close to enough to pay for a person’s subsistence,” he stated bluntly, adding that “ the realistic problem that such mini-UBIs solve is giving people access to a sufficient quantity of cryptocurrency to make a few basic onchain transactions and online purchases.”\
The idea of tying a secure digital ID to financial or humanitarian aid (UBI, subsidies, grants) is one that has a groundswell of support, and many believe ZKPs can help to make this idea a reality. The Taiwanese government is  leveraging Zero-Knowledge Proofs to secure digital IDs, while Google has integrated the tech into its mobile wallet, ensuring age verification across mobile devices, apps (such as Bumble) and websites using its Digital Credential API.\
So, are on-chain digital identity systems equipped with ZKPs really our best chance of reaching a point where aid can be provably dispensed to the needy, rather than bots or fraudsters? After all, ZKPs align with the principle of least privilege, allowing users to prove specific claims without exposing their identity. But what about those criticisms?\
“ZKPs are not a silver bullet on their own, they still leave data traces behind, especially when attestations are made on public blockchains,” says Shady El Damaty, Cofounder of decentralized identity project . “Private state, unlinked addresses, and careful data management can help augment privacy, however the risk of giving up your nullifier is the same as the risk of losing your private key – so the issue is not with ZKPs but rather overall self-custody security practices.”“This is why we have designed a 2PC model that assumes security for the user even when their device is compromised,” adds Cofounder Nanak Nihal Singh Khalsa. “This means a user must authenticate their identity with multiple independent methods to claim funds or submit a transaction.”Streamlining Aid with Crypto Rails \nHolonym takes a different approach to World, in that it can theoretically be used to link up existing aid with identity rather than combining the two. “There’s an existing humanitarian aid network already out there, so why not upgrade their capacities rather than use UBI as a marketing tool to justify a token’s utility or valuation?” asks El Damaty.Naturally, Singh Khalsa concurs. “Token-based UBI models are out of touch with the real world and people who most need basic level support to meet their needs,” he says. As well as token prices being volatile, making what one can buy change based on daily price swings, the value of such assets tends to decline without meaningful utility or demand. \n \
“Another problem is that UBI tokens need to be off-ramped and exchanged to cash, and those who need the tokens the most will likely experience great difficulty off-ramping, or they’ll have to pay large fees to do so,” says El Damaty. \n \
Holonym’s ‘' framework allows existing humanitarian aid to run more efficiently on crypto rails: aid recipients can create digital wallets from their social accounts (WhatsApp,Facebook), secure them with simple biometrics, and directly receive payments in the form of stablecoins. human.tech also lets aid programs request identity proofs to measure proof of impact and reduce false claims, while Holonym partners with local off-ramps to keep fees low for users.\
\
Buterin’s criticism of one-person-one-ID stems from his belief that if a single ZK-ID system were to dominate the market, reversion to this model would undermine pseudonymity and increase coercion risks. Singh Khalsa certainly appreciates this perspective, noting that “the design of one-person-one-ID typically serves goals for centralized systems that seek to control or capture the value of identity into a walled garden.” \n \
In other words, users enjoy more freedom and flexibility when, rather than being locked into one identity, multiple can be created. “ZKPs can be used to generate multiple identities if designed well with the user in mind,” says El Damaty. “Revocation, control over privately linked addresses, and private addresses can all extend the pluralism of ZKPs.” \n \
Buterin’s warnings have certainly sparked debate about ZK-wrapped identities and their applicability to aid distribution, particularly where privacy and security are concerned. Expect this debate to rumble on, and for alternative approaches to token-based UBIs like World to continue stating their case.\
Don’t forget to like and share the story! :::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Data breach reveals Catwatchful ‘stalkerware’ is spying on thousands of phones</title><link>https://techcrunch.com/2025/07/02/data-breach-reveals-catwatchful-stalkerware-spying-on-thousands-android-phones/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:05:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The spyware operation's exposed customer email addresses and passwords were shared with data breach notification service Have I Been Pwned.]]></content:encoded></item><item><title>Physicists Start To Pin Down How Stars Forge Heavy Atoms</title><link>https://www.quantamagazine.org/physicists-start-to-pin-down-how-stars-forge-heavy-atoms-20250702/</link><author>Jenna Ahart</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2025/07/HeavyElementNucleosynthesis-crMarkBelan-Default.webp" length="" type=""/><pubDate>Wed, 2 Jul 2025 14:00:18 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[The Facility for Rare Isotope Beams (FRIB) may not glitter quite like the night sky, plunked as it is between Michigan State University’s chemistry department and the performing arts center. Inside, though, the lab is teeming with substances that are otherwise found only in stars. Here, atomic nuclei accelerate to half the speed of light, smash into a target and shatter into smithereens.]]></content:encoded></item><item><title>What If Your &apos;Messy&apos; Data Is Actually Perfect?</title><link>https://hackernoon.com/what-if-your-messy-data-is-actually-perfect?source=rss</link><author>Lior Barak</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Hello, data Shokunin-deshi!Welcome to the final installment of our Data Ecosystem Vision Board implementation series! In our previous editions, I shared how I helped Sarah at 4seconds.com build the Present Inputs and Future Vision layers of her Data Ecosystem Vision Board.\
Today, we complete the journey by exploring the Success Metrics layer, the compass that guides transformation and measures progress.\
My neighbor's so obsessed with his lawn, I'm pretty sure he thinks he's the Zen master of his backyard. He came over the other day, and for about the fourth time, started talking about my tree. This time, though, he prefaced it with a story from some "advanced gardening course" he took.\
"How will we know if our pruning is correct?" an eager apprentice asked. The master, apparently radiating serene wisdom, replied, "We judge by three measures: the tree's health, its beauty, and how it harmonizes with the garden as a whole. These three tell us everything we need to know."\
Then, he looked me dead in the eye and, with all the Zen calm he could muster, informed me that my wildly un-Zen-like tree was failing on all three counts, especially the "harmonizes with his garden" part. He then patiently explained that while he values harmony, his patience for my arboreal negligence was, shall we say, not boundless. He might just have to apply those pruning principles himself.\
And honestly, his point about the tree's intrusion, however un-Zen-like his delivery, perfectly illustrates the purpose of our Success Metrics layer. With the right measures, we can evaluate complex transformations through a simple yet powerful lens that guides our actions and decisions. Because sometimes, even the most profound principles need a little push, or a neighbor with a very specific definition of "harmony."\
The Success Metrics layer transforms a vision from aspiration to action by defining what success looks like and how we'll know when we've achieved it.In this edition, I share how I guided Sarah through building the Success Metrics layer of her Data Ecosystem Vision Board:✅ Learn how to apply the 5 W's framework to develop focused, impactful KPIs✅ Discover how to select organizational health metrics (Data ROI and Data Utilization) that measure overall transformation success✅ Master the art of creating clear guiding principles that drive decision-making✅ Implement a change management approach that ensures capability adoption and value realizationThe Metrics That Matter: Beyond Measurement to ActionAfter completing the Future Vision layer with Sarah's team, we needed to establish how success would be measured and how the transformation would be guided. The Success Metrics layer addresses three critical questions:How will we measure success? (KPIs)What principles will guide our decisions? (Guiding Principles)How will we manage the organizational change? (Change Management)Based on my implementation experience, I recommend a focused approach with:A maximum of 6 total KPIs (including 2 core organizational metrics)No more than 6 guiding principlesA structured change management plan linked to capability implementationThis deliberate constraint prevents metric proliferation while ensuring comprehensive coverage of what truly matters.The 5 W's Framework for Effective KPIsAt the heart of meaningful metrics lies the 5 W's framework, a structured approach to ensure each KPI drives real business value:The 5 W's Framework for Effective KPIs: Mapping those KPIs into a simple Excel Table already will give you a better overview and understanding.Why: Understanding the PurposeEvery KPI must have a clear purpose: "We measure [KPI] because it tells us [insight], which helps us achieve [business objective]."Example from 4seconds.com: "We measure Inventory Accuracy because it tells us how reliably we can plan and execute flash sales, which helps us maximize revenue and customer satisfaction.":::tip
 Many organizations struggle with too many metrics rather than too few. Be ruthless in requiring a clear purpose for every KPI, several proposed metrics should be eliminated when teams can't articulate a compelling why.What: Aligning KPIs with ActionsKPIs must connect to specific actions: "When [KPI] [increases/decreases], we [take this action] to [achieve this goal]."Example from 4seconds.com: "We measure Campaign Attribution Accuracy because it tells us how effectively we're tracking marketing performance. When it decreases below 85%, we investigate data collection gaps and pipeline issues to ensure marketing investment decisions are based on reliable data.":::tip
I The action statement is where many KPIs fall short. By explicitly defining response actions, teams ensure their KPIs drive behavior rather than just measure it.Where: Mapping KPI ImpactDocument primary impacted teams, in case the KPIs trend changes, who are the teams that influence it, the teams that will feel the pain, and how it relates to what "When [KPI] [Increases], [Team] [What action needs to happen].Example from 4seconds.com: For "Data Quality Score". Primary ownership: Data Engineering; when the quality score goes down, the data team needs to investigate what caused the trend change, and the data producers need to check on their side, whether data flows as expected:::tip
 The "Where" dimension often reveals unexpected dependencies. When we work with the KPIs, they will have a different impact on different teams. By mapping where a trend change of the KPI influence will help us better understand the impact of the KPIFunny story, once I was with a marketing team that were super happy they managed to reduce the budget spending due to better data on the campaigns performance, on the other side of the room the finance team were freaking out how to explain investors that the company won’t reach the commited marketing budget and how to not lose it for next yearWhen: The Rhythm of MeasurementSet frequency when the KPIs are required, based on how quickly metrics change and how rapidly action can be taken.Example from 4seconds.com: "Flash Sale Readiness Score". Calculated daily, reviewed weekly by operations, 48 hours before sales by executives, with 90-day trend analysis and holiday season adjustments.:::tip
 Don't default to daily reviews regardless of the metric's natural rhythm. Aligning review cadence with business needs dramatically improves performance.Who: The Keeper of the MetricsAssign clear ownership with both responsibility and authority to influence performance.Example from 4seconds.com: "Marketing Data Timeliness", Owner: Marketing Analytics Lead; Contributors: Data Steward, Data Engineer; Stakeholders: CMO, Campaign Managers; Audience: All marketing team, Executive team.:::tip
: Ownership must include authority to drive change. Ensure each KPI owner has both responsibility and authority to influence the metric's performance.Organizational Health Metrics: Data ROI and Data UtilizationThe Strategic KPI Portfolio: Six Metrics That MatterThe Success Metrics layer accommodates six KPIs maximum, a deliberate constraint that forces strategic focus. Overpopulate with metrics and you lose track of what truly matters; under-populate and you lose visibility into critical areas.\
My recommendation follows a proven structure:Two Foundation KPIs Data Health Indicators: These long-term metrics assess the overall health of your data ecosystem:: Measures the monetary value generated by your data investments: Tracks how effectively you use the data you collect and storeThese foundation KPIs provide your "vital signs"; if these trend poorly, your entire data strategy needs attention.\
Two to Four Capability KPIs, Progress Trackers: These metrics measure progress toward the specific capabilities outlined in your Future Vision. They should:Connect directly to your vision board prioritiesEnable early identification of implementation issuesDemonstrate tangible progress toward strategic goalsProvide clear signals when course correction is neededExample from 4seconds.com: Their capability KPIs included "Data Pipeline Reliability" (99.5% uptime target) and "Self-Service Analytics Adoption" (60% of business users actively querying data independently).Data ROI: Measuring Value CreationData ROI measures the financial return generated from data investments. This metric answers the fundamental question: "Is our data ecosystem creating real business value?"\
Data ROI = (Financial Value Generated - Cost of Data Operations) / Cost of Data Operations

If Cost of Data Operations = €100 and Financial Value Generated = €90, then (90−100)/100=−0.10 or -10%.
Financial Value Generated: Quantified benefits from data-driven decisions and automationsTip: Defining "Financial Value Generated" Accurately: This is often the trickiest part. Be clear and consistent about what you include. Avoid double-counting or attributing value that isn't directly a result of data operations.: All expenses related to data collection, storage, processing, and analyticsMinimum acceptable ROI (typically 0.01-0.7x)Target ROI (typically 0.8-2.5)Stretch ROI (typically 2.6x+) Specify the period over which you are calculating the ROI (e.g., quarterly, annually). Value and costs should align with this timeframe: In our case:Year-over-year trend analysisBreakdown by data domain or capabilityExample from 4seconds.com: After implementing the measurement, their initial Data ROI was 0.2x, barely positive. We set targets of 2x by year-end and 4x within three years. More importantly, we created a detailed tracking mechanism that identified which capabilities were creating the most value and which needed intervention.:::tip
: Sometimes it's useful to compare the "Financial Value Generated" with data operations to a hypothetical scenario without them, especially for initiatives aimed at maintaining or improving existing processes. Remember that ROI is a quantitative measure. Data initiatives often have significant qualitative benefits (e.g., improved decision-making, better customer experience, innovation) that are harder to directly monetize but are still valuable. Don't let a purely financial ROI be the only factor in assessing data's success.The Real Challenge: What Sarah's Team DiscoveredWhen we started calculating Data ROI, Sarah's team hit an immediate roadblock: "How do we quantify all the income generated by data?" The challenge was particularly acute with financial data, which is essential but doesn't directly generate ROI.\
We had to evolve our approach:"Must Have" Data Exclusion: We classified essential data like financial data, compliance data, and operational data as "must have" and excluded them from ROI calculations. But we needed to tag it properly so this wouldn't become a manual process.Tagging System Implementation: We created a comprehensive tagging system to automatically categorize data by its business purpose:Must-have/compliance data: For every significant data-driven decision or automation, we implemented a value tracking system where stakeholders estimated the business impact, and when possible we created automatic process, for example in the marketing campaigns we identified the campaigns we optimized and they stopped spending budget on bad traffic and accomilated it, and when increased we mark it as value created.:::tip
: Many organizations struggle to quantify value from data initiatives. I worked with Sarah to implement a value tracking system where every significant data-driven decision or automation had an estimated value attached. While not perfect, this approach provided a foundation for measuring return that was far better than no measurement at all. Just start with what you have and slowly learn more and improve; stakeholders will wish to be included in the calculation as they fear they may lose access to the data due to low ROI.Data Utilization: Measuring Effective UsageData Utilization measures how effectively your organization leverages the data it collects. This metric addresses another critical question: "Are we making the most of our data assets?"\
Data Utilization = Data Assets Actively Used / Total Data Assets Collected

* Over a fixed period, in our case it was three months, but in bigger organizations I used six months timeframe
Data Assets Actively Used: Data elements used in reports, analyses, or automated processesTotal Data Assets Collected: All data elements stored in your data ecosystemMinimum acceptable utilization (typically 55-60%)Target utilization (typically 61-78%)Stretch utilization (typically 79%+)Breakdown by data domain or system, as well as team/user, if possibleSarah's Big Discovery: The Data Utilization Reality CheckThe biggest "aha moment" for me was discovering that no one in the company was thinking about understanding the patterns of data usage, although they were associating some costs with teams, they accepted the server bill as a given thing. For Sarah and her team, it was the shocking discovery of their super low data usage – they had always been under the impression they were effectively using their data.\
: When we tried to implement Data Utilization tracking, we immediately hit a wall. There were no tags, no database collecting logs (Data Catalog) and information, and no way to track what data they had, who owned it, when it was accessed, or by what system. We couldn't follow the lineage.\
 We had to start from scratch:Activity Logging Database: Created a comprehensive system to log all data interactions, creating a data catalog: Mapped every dataset to an owner and purpose: Implemented monitoring to see which data was being used and when: Built systems to track data flow from source to consumption:::info
 Sarah's initial calculation revealed only 48% utilization of their 32TB of stored data. This led to a massive data rationalization initiative that not only reduced storage, processing, and security costs by over €1,000 monthly but also focused analytics efforts on high-value data.Low utilization is common and often hidden. At another client, I discovered they were storing over 200 website behavioral events but only using 8 in any decision-making process. The cost of collecting and storing unused data was substantial, and the unnecessary complexity slowed down legitimate analytics. By implementing utilization measurement, Sarah's team gained visibility into this previously hidden issue. helps a lot when coming to investigate it, the ability to identify who owns it, what exactly it is, and creating a process that logs this information into some database was super helpful. This practice is not common and should be encouraged even moreWhile maybe storing 32TB is only $700, the backup, moving of data, security… costs money, it top it to over $1,000 a months or even more, if only 50% is used, it's good thing to ask Why do we keep the data, and can we move it to some lower costs long terming plan moved into a long-term container, which is extended twice, each with a six-month long-term plan, and if is not required can be deleted by the end period of the second extension, if it's not data the company is obligated to store, such as financial or user health data. Think about this process as wellAlways set a data retention plan. I will deep dive into it in Data Flavors issue #15, covering a few methods, and my view on it.Supporting KPIs: Measuring Capability SuccessIn addition to the two organizational health metrics, I helped Sarah select a maximum of four supporting KPIs that would track the success of their key capabilities:To avoid KPI proliferation, I guided Sarah through a structured selection process:For each Future Vision capability, identify 2-3 potential success metricsApply the 5 W's framework to each candidate metricEvaluate candidates based on: (how broadly applicable across capabilities) (how feasible to track consistently) (how it drives specific behaviors) (how directly it connects to business outcomes)Select the 3-4 metrics with the highest evaluation scores:::tip
: This rigorous selection process is crucial. At previous clients, I've seen metrics chosen based on what's easy to measure rather than what drives value. By focusing on coverage, measurability, actionability, and impact, Sarah's team ensured they selected metrics that would genuinely guide their transformation.For 4seconds.com, the supporting KPIs included:: Accuracy, completeness, timeliness, and consistency of key data domains: Directly impacts decision quality and operational efficiency. The vision was to include more marketing data to steer campaigns automatically, and bad data could cause a loss: Below 90% triggers remediation; below 80% triggers emergency review: Composite score across data quality dimensions and domains\
Marketing campaigns automated: The share of marketing campaigns generated by smart systems and not humans: The goal is to automate the effort of the marketers and remove the need for agencies to support them. With the system, the marketing team can focus on the high-effort campaigns and let the system set and optimize the rest: If the share is below 65%, the capability is not acting as expected; if it's under 35%, the capability is causing issues that may damage the marketing team: Average time across key business decisions: Percentage of analytics requests fulfilled through self-service: Indicates democratization progress and analyst leverage: Below target triggers enablement review; stagnation triggers capability assessment: Self-service requests / Total analytics requests: Organization-wide data skills and confidence: Foundation for a data-driven culture: Skill gaps trigger targeted training; confidence gaps trigger communication initiatives: Composite score from skills assessment and confidence surveyThe Marketing Automation Story: KPIs as Capability GuardiansLet me share the story behind the "Marketing campaigns automated" KPI, which became one of Sarah's most valuable metrics.\
Sarah's team had set an ambitious goal: automate 80% of campaign budget steering decisions and 60% of campaign creation. This wasn't just about efficiency; with the hiring freeze, they needed to increase the marketing budget to generate 10% revenue growth without adding staff.\
 The marketing team was terrified. Would they lose their jobs to automation?: We positioned this as an enhancement, not a replacement. The KPI measured progress, but more importantly, it tracked whether the automation was helping or hurting campaign performance.\
On the Company Core Dashboard: They tracked two connected KPIs:Marketing campaigns automated (% of campaigns managed by AI)Revenue estimated from marketing campaigns (should stay stable or increase as automation progresses)These twin KPIs became powerful indicators of both capability development and the collaborative effort between the data team and the marketing team. When the automation percentage went up but revenue estimates stayed flat, it showed the system was learning. When both went up together, it proved that the capability was delivering real value.:::tip
 Each supporting KPI should connect to multiple capabilities rather than tracking a single initiative. This provides broader coverage with fewer metrics. Sarah's initial list had 12 potential KPIs, but by focusing on metrics that spanned multiple capabilities, we achieved comprehensive coverage with just four.Guiding Principles: The North Star for DecisionsI must admit that the below method I copied from my agile coaches at Zalando, they did such an awesome job, so I never felt a need to change it\
Beyond metrics, I worked with Sarah to establish clear guiding principles for their data ecosystem. These principles guide day-to-day decisions and help resolve conflicts or ambiguities.Principles Development ProcessTo create meaningful principles, I facilitated a structured process:Review workshop notes for recurring themesIdentify current implicit principles (what guides decisions today)Collect stakeholder perspectives on what should guide the futureDraft Candidate Principles:Focus on areas where guidance is most neededEnsure principles address both technical and cultural aspectsFrame positively as aspirational statementsKeep language simple and memorableTest each principle against real-world scenariosEnsure principles are specific enough to guide decisionsEliminate overlap and redundancyLimit to a maximum of six principlesCreate clear definitions for each principleDevelop examples of applicationsDocument decision hierarchy when principles conflictCreate a communication and socialization plan:::tip
 Principles should be actionable, not aspirational platitudes. During our drafting session, I challenged every principle with, "How would this help you make a different decision?" If we couldn't identify specific scenarios where the principle would guide behavior, we refined or replaced it.After this process, Sarah's team established these six principles:: Every dataset is treated as a product with clear ownership, quality standards, an iteration process, monetary value, and user support: New data sources require defined ownership and quality metrics before implementation: Quality and reliability take precedence over speed of delivery: All non-sensitive data should be discoverable and accessible across the organization, and documented in the company’s data catalog: Departmental datasets are published to the central catalog automatically guidance: Access restrictions require explicit justification: Data initiatives are prioritized based on measurable business impact: All project proposals include the estimated monetary value: Higher-impact initiatives take precedence over technically interesting ones: Manual data tasks should be automated to free human capacity for insight generation: Any report produced more than twice is automated: Invest in the automation of repetitive tasks over manual optimizationRight-time, Not Always Real-time: Data timeliness should match business need, not default to the most frequent possible: Daily aggregation for metrics that drive weekly decisions: Performance and cost efficiency over unnecessary immediacy: Data systems and processes should build confidence through transparency and reliability, and be proactive in communicating issues\
: Quality metrics are visible alongside all reports: Transparency about limitations of exaggerated capabilitiesOf all the principles Sarah's team adopted, "Business Impact First" created the most profound change. Here's how it played out in practice:: When evaluating two competing capabilities, self-service analytics vs. marketing automation, the teams initially argued based on technical preferences and departmental needs.: We applied "Business Impact First" and calculated the monetary value for each option:Self-service analytics: €45,000 annual savings in analyst timeMarketing automation: €120,000 annual revenue increase potential: The principle guided them to prioritize marketing automation, but more importantly, it changed how they approached all future decisions. Teams started thinking in terms of business value rather than technical elegance.:::tip
: Principles should reflect both aspirations and practical constraints. At a previous client, I established "real-time everything" as a principle without considering cost implications. This led to overinvestment in infrastructure that provided minimal business value. I helped Sarah's team find the right balance with principles like "Right-time, Not Always Real-time" that acknowledged practical limitations while still providing clear guidance.Socializing Principles: Making Them Stick We communicated the principles at an all-hands meeting, created a board, and invited people to react and suggest ways they can execute the principles. During the session, we explained that we had already experienced them during the workshops for the future layer, and some had emerged through the present layer discovery process.\
: We agreed that the principles would be tested for the next six months and could be re-evaluated for their fit with organizational culture and usage. This made them "fixed but temporary," giving everyone time to get used to them while providing a clear path to refer back to them when decisions got complex.\
 Making principles "fixed but temporary" reduced resistance and permitted people to experiment with using them as decision-making tools.Change Management: Ensuring Adoption and ValueIntroducing new data capabilities isn't just about tools, it’s about changing how people work, think, and decide. Together with Sarah, we developed a change management approach focused on three pillars: Impact, Adoption, and Learning.We began with a  to map how different teams would be affected:Stakeholders: Users, data producers, indirect roles, and leadershipImpacts: Process, skills, tools, mindset, decision-makingMarketing: High impact – full shift in workflowsFinance: Medium – new sources, familiar processesProduct: Low – minimal change:::tip
: Even small changes, like a dashboard redesign, can deeply affect workflows if not planned properly.Each capability had its own Adoption Plan, focused on success metrics, barriers, and rollout strategy:Success Criteria: Usage frequency, efficiency gains, adoption timelineAdoption Strategy: Comms, training, support, incentivesExample (4seconds.com - Self-Service Analytics):Goal: 60% report access via self-service in 6 monthsBarriers: Low trust, data literacy gaps Champion program + workshops + office hours:::tip
: Adoption needs more than training. Address trust, habits, and emotional resistance head-on.Capability adoption required skill growth across the board:Data Team: Learned marketing workflows and how to talk about business valueMarketing Team: Learned how to guide automation and work with data toolsExecutives: Shifted from urgent demands to structured prioritization: Role-based learning paths, on-demand resources, real-data challenges\
: 4seconds.com’s Data Literacy Program included biweekly sessions, hands-on challenges, and a Slack channel for peer support.:::tip
: Generic training doesn’t stick; contextualize learning around real company problems.We built a structured communication strategy to maintain momentum:Messaging: Why, what’s changing, what’s in it for each teamChannels: All-hands, newsletters, Slack, dashboardsCadence: Weekly for involved teams, monthly org-wide, quarterly execsTransformation dashboard (in-office + intranet):::tip
: Regular, visible communication builds trust. Irregular updates kill momentum.Maintaining the Success Metrics LayerOnce Sarah had her Success Metrics in place, the next step was making sure they stayed useful over time.We set up a lightweight, recurring review cycle:: Are metrics on target? What trends are emerging? Any surprising correlations?: Are these KPIs still tied to business goals? Are people using them? Is the data still solid?: Tweak thresholds or calculations if needed. If a metric hasn’t driven a decision in 6 months, it might be time to retire it.:::tip
: At another company, I saw metrics tracked long past their relevance, cluttering dashboards and wasting time. Sarah introduced a “sunset protocol” to avoid that.Every year, I recommend a full refresh of the Vision Board; however, for the first time, it’s better to do it quarterly until you get into the rhythm and learn the system:: Where are we now vs. when we started? What’s improved? What’s still missing?: Do we need new capabilities? Has our strategy changed?: Are KPIs still telling the right story? Are principles still actionable?: Exec sessions, team updates, company-wide refresh, and space for feedback. At 4seconds.com, Sarah ran a compressed workshop to refresh their board exactly one year in. It helped the team celebrate wins, update priorities, and refocus for the next phase.Tying the Vision TogetherOnce the Success Metrics were live, all three layers of the Vision Board came together: set the baseline defined what needed to change showed if progress was being madeEach layer feeds the others in a cycle:Metrics track movement from present → futureGaps in the present inform future prioritiesThe future vision tells us which metrics matter mostWithout this integration, I used to get stuck with clients, using metrics disconnected from strategy. Sarah avoided that by reviewing how each layer linked together.:::tip
: The Vision Board's power comes from this integration. When a client implemented only parts of the framework, they lost the holistic view needed for effective transformation. I helped Sarah ensure all three layers worked together by regularly reviewing the connections between them.To keep the Vision Board top-of-mind, Sarah made it accessible at every level:Digital Board in FigJam, updated regularly and shared with stakeholdersExec Dashboard: One-pager with key KPIs and decisions, updated monthlyTeam Views: Tailored summaries for departments with relevant metricsOffice Display: A simplified, visual tracker showing progress and celebrating winsExample from 4seconds.com: Sarah created a "Data Transformation Hub" in their office with physical and digital components. This central reference point kept the Vision Board visible and top-of-mind throughout the organization, reinforcing its importance to their strategy.:::tip
: Visibility drives accountability. At a previous client, their vision document was filed away after creation and quickly forgotten. By making the Vision Board highly visible in multiple formats, Sarah ensured it remained an active guide for day-to-day decisions rather than a forgotten artifact.For smaller teams, I usually recommend simplifying:Stick to 2 org-wide KPIs (like Data ROI and Utilization) and maybe 2-3 supporting ones.Manual tracking is fine. Focus on direction, not perfection.Don’t reinvent the wheel. Add a metrics check-in to existing leadership meetings.Targeted Change ManagementFocus on influencers, decision-makers, and power users, not everyone at once.\
This targeted approach maximizes impact with limited resources. For their Self-Service Analytics capability, Sarah identified 8 "power users" across departments who, if successfully converted, would influence 80% of potential users.Exercise: Your 30-Minute Success Metrics StarterObjective: Begin defining the KPIs and principles for your Data Ecosystem Vision Board.Identify your two most important organizational data health metrics (10 minutes)For each one, define why you're measuring it and what actions you'll take based on trend changesDraft 2-3 guiding principles (10 minutes)That would help your organization make better data decisionsPlan one capability adoption (10 minutes)For one key future capability, identify what would constitute success beyond just implementationWhich of the 5 W's (Why, What, Where, When, Who) do you find most challenging to define clearly?How might the "Business Impact First" principle change how your organization evaluates data initiatives?What organizational changes would be required to successfully implement a Success Metrics layer?This concludes our implementation series on the Data Ecosystem Vision Board. In future newsletters, I'll explore how to operationalize your Vision Board through yearly strategic planning, initiative management, and continuous improvement. Thank you for joining me on this journey!\
Through these actions, Sarah began transforming the Vision Board from strategy to reality – a journey that would unfold over the coming months and years as 4seconds.com built the data ecosystem needed to support their business goals.May your data flow with purpose!\
P.S. What's your biggest challenge in measuring data transformation success? Reply to this email, and I'll personally share insights from my experience working with companies like yours.]]></content:encoded></item><item><title>Large Language Models Are Improving Exponentially</title><link>https://spectrum.ieee.org/large-language-model-performance</link><author>Glenn Zorpette</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEzNzkxMi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc4MDU4NzgyMH0.f77AAmLqqykTnBa8ZR6LMN1AOfa1crWhXsqNVd5BixU/image.png?width=600" length="" type=""/><pubDate>Wed, 2 Jul 2025 14:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[In a few years, AI could handle complex tasks with ease]]></content:encoded></item><item><title>Hacked, leaked, exposed: Why you should never use stalkerware apps</title><link>https://techcrunch.com/2025/07/02/hacked-leaked-exposed-why-you-should-stop-using-stalkerware-apps/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Wed, 2 Jul 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[
Using stalkerware is creepy, unethical, potentially illegal, and puts your data and that of your loved ones in danger.]]></content:encoded></item><item><title>Wayback Hopes To Be Ready Next Year With Alpine Linux Planning To Use It By Default</title><link>https://www.phoronix.com/news/Wayback-2026-Plans</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:56:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A few days ago Wayback was announced as an X11 compatibility layer for X11 desktops environments leveraging a rootful XWayland server. While currently experimental, the hope is that it will be production-ready next year and Alpine Linux is looking at using it by default for its X11 environment...]]></content:encoded></item><item><title>Microsoft will lay off 9,000 employees, or less than 4% of the company</title><link>https://techcrunch.com/2025/07/02/microsoft-will-lay-off-9000-employees-or-less-than-4-of-the-company/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:40:04 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Microsoft is leveraging another round of layoffs despite posting consistent growth.]]></content:encoded></item><item><title>Microsoft To Lay Off As Many As 9,000 Employees in Latest Round</title><link>https://slashdot.org/story/25/07/02/1330223/microsoft-to-lay-off-as-many-as-9000-employees-in-latest-round?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft is kicking off its fiscal year by firing thousands of employees in the largest round of layoffs since 2023, the company confirmed Wednesday. From a report: In an ongoing effort to streamline its workforce, Microsoft said that as much as 4%, or roughly 9,100, of the company's employees could be affected by Wednesday's layoffs. The move follows two waves of layoffs in May and June, which saw Microsoft fire more than 6,000 employees, almost 2,300 of whom were based in Washington.]]></content:encoded></item><item><title>Tesla faces second straight year of falling sales after another bad quarter</title><link>https://techcrunch.com/2025/07/02/tesla-faces-second-straight-year-of-falling-sales-after-another-bad-quarter/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:14:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tesla sales fell more than 13% compared to last year as the company struggles to find demand for its aging lineup, and as CEO Elon Musk continues to damage the brand. ]]></content:encoded></item><item><title>How Did Nexo Become the First Crypto Partner of the DP World Tour?</title><link>https://hackernoon.com/how-did-nexo-become-the-first-crypto-partner-of-the-dp-world-tour?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:08:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Could cryptocurrency sponsorship reshape professional golf? In an unprecedented move, Nexo, a digital asset wealth platform, has signed a three-year partnership with the DP World Tour, making it the first crypto-focused sponsor in professional golf.What Exactly is the Partnership?The agreement between Nexo and the DP World Tour makes Nexo the Official Digital Asset and Wealth Partner of the Tour through 2027. This involves sponsoring key tournaments, notably including the Genesis Scottish Open and the BMW PGA Championship.\
Significantly, one of the tournaments, previously unnamed, has been renamed the "Nexo Championship." It will take place from August 7–10, 2025, at the Trump International Golf Links in Aberdeenshire, Scotland. This will be the cornerstone of Nexo’s involvement, solidifying its visibility in global sports.Why is this Partnership Important?Golf sponsorships have traditionally been dominated by mainstream brands such as Rolex, BMW, and Emirates. Nexo's entry represents a notable shift, highlighting how digital and crypto-focused companies are stepping into traditionally conservative sporting arenas.\
Antoni Trenchev, Nexo's Co-founder and Managing Partner, explains the rationale, saying, “This partnership reflects our belief that wealth and golf are built the same way: with preparation, control, and vision. Both the DP World Tour and Nexo share a commitment to precision, discipline, and performance, whether on the course or in finance.”How Does the DP World Tour View This?For the DP World Tour, associating with a digital asset firm like Nexo aligns with its modern branding and global appeal. Max Hamilton, the Executive Commercial Director at DP World Tour, emphasizes the strategic alignment: “Just as the DP World Tour connects global golf fans using the latest technologies, Nexo is reshaping wealth-building with digital tools.”\
The DP World Tour, formerly known as the European Tour, stages events in various iconic global locations, attracting affluent audiences who are seen as potential clients for Nexo's digital asset services.Understanding Nexo's Core BusinessFounded in 2018, Nexo is a financial platform focused on digital assets, commonly known as cryptocurrencies, such as Bitcoin and Ethereum. Nexo offers services including crypto-backed loans, savings accounts, and trading services.\
To illustrate simply: Imagine holding Bitcoin worth $10,000 but not wanting to sell it due to potential future appreciation. Nexo allows you to borrow money against your Bitcoin without selling it. This model enables clients to manage their finances without sacrificing long-term investment opportunities.Cryptocurrency companies sponsoring sports teams and events is a growing trend. Crypto.com previously signed partnerships with major entities like the UFC and Formula 1, and now Nexo's move into golf signals this expansion is continuing.\
Such partnerships often aim to build legitimacy and trust among broader audiences. Golf, with its high-income demographic, represents a strategic audience for Nexo. While such sponsorships offer substantial marketing opportunities, they also come with challenges, notably market volatility. Crypto markets fluctuate rapidly, posing potential risks to long-term partnerships. Yet, Nexo’s partnership sets a precedent. If successful, more crypto companies might venture into traditionally conservative sports, potentially changing how sports sponsorships are viewed and negotiated in the future.Nexo’s sponsorship of the DP World Tour could significantly impact the landscape of sports sponsorships, blending traditional and digital financial services. However, given the inherent volatility of crypto assets, it remains critical for Nexo to maintain stability to fulfill its long-term commitment. The success or failure of this partnership could either encourage similar sponsorships or caution others against such bold moves.Don’t forget to like and share the story! :::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Why Dymension&apos;s Season 2 Matters for the Blockchain Ecosystem</title><link>https://hackernoon.com/why-dymensions-season-2-matters-for-the-blockchain-ecosystem?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:06:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Did the initial excitement of Dymension's mainnet launch, marked by a significant token distribution, truly capture the protocol's full potential? While Genesis Rolldrop Season 1 distributed over $400 million in tokens, Dymension's underlying technology was in an early state. The protocol has evolved, and with the "Beyond" upgrade approaching,  is set to reintroduce and accelerate Dymension's growth as a Universal Settlement Layer.\
This new phase aims to deepen engagement and reward various participants, from long-term token holders to active builders and new users. It represents a shift towards a more comprehensive and incentivized ecosystem, moving beyond the initial airdrop to foster sustained participation and development. The focus is now on establishing Dymension as a foundational layer for decentralized applications and services.Unpacking the Registration Waves and Dymond Hands Initiative\
Season 2's participation mechanism begins with , each targeting specific eligible addresses. This phased approach allows Dymension to onboard different segments of its community systematically. The first wave, termed "Dymond Hands," is dedicated to core DYM community members. To qualify for this initial wave, individuals must have continuously staked a minimum of 17 DYM tokens since June 2024 without unstaking. This criterion establishes a clear measure of sustained commitment.\
The registration process itself is designed for security and simplicity. Participants register their wallet addresses without requiring a signature, meaning no private key information is ever requested. The only official entry point is through the designated portal https://portal.dymension.xyz/season-two, emphasizing a secure and controlled environment for participation. This careful approach to registration reflects a broader commitment to user safety within the Dymension ecosystem.Understanding DYMONDs: The New Incentive Mechanism\
At the core of Season 2's reward structure are , a new form of in-protocol points that users can earn and later exchange for DYM tokens. This system is designed to incentivize various forms of on-chain activity, compensating long-time DYM holders, active users, developers, and new entrants. The earning of DYMONDs is tied to real-time on-chain actions, ensuring that rewards are distributed based on actual engagement rather than static metrics.\
DYMONDs are earned through activities such as IRO trading, depositing USDC (with one-click functionality from Solana, Arbitrum, and Base), participating in Bridge LP, providing liquidity on Dymension's decentralized exchange, and depositing Total Value Locked (TVL) into a user's own RollApp. The real-time tracking of DYMONDs within the Dymension Portal provides transparency and allows users to monitor their progress. This dynamic reward system encourages continuous interaction and contribution to the network.Claim Windows and Long-Term Engagement\
The process of converting earned DYMONDs into DYM tokens occurs during specific . These are limited timeframes spread throughout Season 2, offering participants the opportunity to exchange their accumulated DYMONDs. This phased claiming approach aims to manage token distribution and encourage sustained participation.\
A significant aspect of the DYMOND system is its emphasis on long-term engagement. While users have the option to claim their DYM during each window, the protocol offers greater rewards to those who continue their participation throughout the entire Season 2. This structure is intended to align incentives with the protocol's long-term growth and stability, rewarding individuals who demonstrate a consistent commitment to the Dymension ecosystem.Referral System and Stakers BoostTo further accelerate growth and community expansion, Season 2 introduces a . Existing participants can invite new users, earning a bonus equivalent to 10% of all DYMONDs accrued by their referrals. This bonus does not diminish the referred user's own DYMOND earnings, creating a mutually beneficial arrangement. New users who join using a referral link also receive a signup bonus, providing an immediate incentive for new participation. This mechanism aims to leverage the community for organic growth and adoption.\
The  mechanism directly rewards users for their sustained commitment and the size of their staked DYM. Every DYMOND earned by a participant is subject to a multiplier of up to 5x, based on the average of two factors: the duration of continuous staking without unstaking, and the amount of DYM staked, with a cap at 25,000 DYM. This boost system directly links staking behavior to increased earning potential, incentivizing long-term holding and significant contributions to network security.Empowering Creators: The RollApp Endorsement ProgramSeason 2 extends its focus beyond individual users to actively support  within the Dymension ecosystem. The protocol recognizes that the creation of new RollApps and the attraction of TVL to these applications are critical for its expansion. While builders earn DYMONDs through their on-chain activity, the Dymension Foundation has also initiated a program to directly fund these creators.\
Through , the Dymension Foundation allocates its staked DYM to support builders, offering rewards of up to $10,000 per month. This program is open to both early-stage teams and those with existing live RollApps. The process involves submitting an application or idea for on-chain support, indicating a direct pathway for developers to receive significant backing. This initiative highlights Dymension's commitment to fostering a vibrant developer community and expanding its utility as a foundational layer.My Opinion and Final ThoughtsDymension's Season 2 represents a progression from a focus on initial distribution to building a sustained and active ecosystem. The introduction of DYMONDs, coupled with the detailed registration waves and stakers boost, indicates a strategic effort to incentivize continued engagement across various user segments. The emphasis on rewarding long-term commitment through tiered claim windows and the multiplier for stakers promotes network stability and aligns user interests with the protocol's health.\
The inclusion of a robust referral program and, critically, the direct financial support for RollApp creators through the endorsement system, demonstrates a commitment to fostering organic growth and innovation. By empowering builders with direct funding and incentivizing the development of new applications, Dymension is laying groundwork for a more diverse and functional ecosystem. This approach suggests a long-term vision for the protocol, moving beyond simple token distribution to cultivate a thriving and self-sustaining network.Don’t forget to like and share the story! ]]></content:encoded></item><item><title>Rivian receives the next $1B from Volkswagen as sales struggles continue</title><link>https://techcrunch.com/2025/07/02/rivian-receives-the-next-1b-from-volkswagen-as-sales-struggles-continue/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:00:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Rivian sales ticked back up in Q2 but were down 23% from the same quarter last year. The company has been dealing with affordability issues made worse by Trump's tariffs and trade wars.]]></content:encoded></item><item><title>Agentic AI Explained: How It Works, Top Use Cases &amp; Future Potential</title><link>https://hackernoon.com/agentic-ai-explained-how-it-works-top-use-cases-and-future-potential?source=rss</link><author>Salesmate</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:00:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
We’re used to AI that needs a prompt: “Turn on the lights,” “What’s the weather?” “Remind me to call mom.” Remember when Siri felt like magic?Now, imagine an AI that doesn’t wait for instructions.What if it could think and act on its own?That’s Agentic AI. It’s not just reactive; it’s proactive—learning, acting, and solving problems independently.Excited? Let’s explore where this bold new world is going.Agentic AI refers to artificial intelligence systems that can think, decide, and act independently to achieve specific goals without constant human input.These systems often build on powerful AI models like GPT-4 or Claude, which serve as their cognitive engines, enabling them to reason, plan, and execute complex tasks.The term  comes from  meaning the ability to act autonomously and make decisions. So when we talk about agentic AI, we're referring to systems designed with: – They operate without step-by-step instructions. – They anticipate needs and take action. – They pursue outcomes and adjust strategies based on results.In simpler terms, it's .Unlike traditional AI, which follows pre-set rules, and generative AI, which creates content based on prompts, agentic AI is designed to take action. It sets plans, coordinates AI agents, and completes goals autonomously. It's the difference between responding and resolving.In short, traditional AI responds to commands. Agentic AI operates independently, driving decisions, coordinating AI agents, and delivering outcomes.Before you get confused or end up messing/mixing agentic AI and AI agents, let's clarify: = Tools that perform specific, narrow tasks (e.g., scheduling, summarizing, flagging emails). = A  that orchestrates many such agents to accomplish bigger, goal-oriented outcomes, often with autonomy and memory.Here is a simple way to understand agentic AI vs AI agents: \n AI agent = a solo worker\Agentic AI = a full team with a manager and missionKey benefits of agentic AIIt reduces manual effort by automating repetitive tasks like form-filling, meeting scheduling, and ticket triaging, but its value goes far beyond basic automation. It executes goals independently, and there's no need for constant input. For instance, it can autonomously reply to leads, schedule meetings, and update your CRM without human intervention. It learns from every interaction, refining its responses. For example, a support agent spots repeat refund requests, flags issues, and adapts messaging automatically.Multi-step task execution: It breaks down big goals into smaller tasks, assigns them to the right agents, and completes entire workflows in one seamless flow.With AI agents working across systems, Agentic AI drives productivity, speeds up processes, and delivers personalized results without needing more human resources.Now, let's unpack how agentic AI works behind the scenes.How does agentic AI work? [Architecture + Workflow]Agentic AI systems are built like modular, intelligent teams.Each layer performs a specific role but works together toward a shared goal. The system operates in a continuous feedback loop, from gathering inputs to making decisions and executing actions.Let's break down the architecture and explore how AI agentic workflows operate behind the scenes.To function with autonomy and intelligence, agentic AI combines four key layers:1. Large Language Models (LLMs): These are the reasoning engines. LLMs like GPT-4 and Claude interpret tasks, understand language, and generate decisions using natural language processing. They allow the AI to think, reason, and adjust based on context.2. APIs and enterprise tools: APIs connect AI to real-world systems such as CRMs, calendars, internal databases, and support platforms. These integrations help the AI fetch live data and perform tasks like sending emails, updating records, or triggering workflows. This layer manages everything behind the scenes. It plans task sequences, assigns agent responsibilities, monitors execution, and ensures progress aligns with the defined goal. Think of it as the system's project manager. Rather than a single model doing everything, agentic AI orchestrates a team of specialized AI agents. Each handles a specific part of the process—research, decision-making, communication, or follow-up.These layers enable the system to operate with logic, context, and autonomy.Agentic AI workflow function in real-world scenariosAgentic systems do not rely on static scripts.Instead, they evolve through dynamic workflows where AI agents operate across tools, APIs, and databases, adapting to changing inputs and improving over time.Here’s the working mechanism of Agentic AI:: The AI agents gather relevant data from various sources, like APIs, documents, databases, or user queries, to set the stage for the task.: With the power of Large Language Models (LLMs), the system analyzes the data, detects patterns, understands intent, and determines the next steps.: The system breaks down the main goal into smaller, actionable tasks, then sequences and assigns them to the right AI agents.: Each agent triggers actions like updating a CRM, scheduling a meeting, or sending a report — all without human intervention.: Once the task is complete, the system analyzes feedback, improving its performance for the next round.: Multiple agents (including humans, when necessary) collaborate to ensure the task progresses efficiently and the goal is met.These agentic workflows in AI adapt and execute tasks with precision, learning from each cycle to improve future outcomes across changing business environments.This flexibility and coordination make Agentic AI more than just automation. It's an intelligent, evolving system that drives real results.Agent coordination models: Hierarchical and decentralizedAgentic systems are usually built in one of two architectural styles, depending on how structured or exploratory the task is.There are two dominant coordination models:Hierarchical architectureIn this model, a "supervisor" agent coordinates the work of other agents. It delegates tasks, tracks progress, and ensures alignment with the overall objective.The supervisor agent determines which sales rep should follow up on a lead. Other agents handle follow-up emails, meeting scheduling, and CRM updates, all under the supervision of the primary agent.Decentralized architectureIn this model, multiple agents work independently but collaborate to accomplish shared goals without one central authority.Consider a product development team using multiple agents for market analysis, competitor research, and customer feedback analysis.These agents work together, gathering data independently but sharing it in real-time to form a comprehensive product strategy.Both models aim to accomplish tasks autonomously, but the structure changes how the work is approached, whether a single "leader" or collaborative peers.Agentic AI vs generative AIGenerative AI, like ChatGPT and DALL-E, creates content based on prompts. It can write blog posts, generate code, or create images. While incredibly creative, it cannot act autonomously or follow through on tasks.Agentic AI goes beyond creating content; it plans, decides, and executes tasks autonomously. Think of it as a digital assistant that takes action independently, coordinating multiple agents to complete complex, goal-oriented functions without human input.Here are the key differences: Agentic AI and Generative AI|  |  |  |
|----|----|----|
|  | Reactive — responds to inputs | Proactive — anticipates and initiates actions |
|  | Requires prompts to operate | Operates independently with minimal human input |
|  | Limited to predefined rules | Adaptable to context, feedback, and changing conditions |
|  | Chatbot that answers FAQs | Agent that handles onboarding, follow-ups, and updates |Both types of AI have massive potential. However, understanding their core differences, strengths, and weaknesses will help businesses determine where and how to integrate them most effectively.Agentic AI applications and use casesFrom sales to cybersecurity, agentic AI models are redefining how businesses delegate, automate, and scale operations with minimal oversight.Notably, 75% of enterprises leverage AI agents for tasks such as code generation, evaluation, and rewriting, underscoring the technology's growing role in software development.Below are practical and real-world agentic AI examples:Sales and customer serviceSalesmate's smart workflows: It serves as an agentic layer, automating lead routing, sales follow-ups, and personalized actions using real-time data—all with minimal human input.: Resolves customer queries dynamically by querying databases, detecting sentiment, and learning from interactions to improve accuracy and efficiency.: Provides empathetic patient support, monitors vitals, suggests treatment changes, and escalates cases autonomously based on context and patient needs.: Automates IT support workflows by understanding natural language requests, planning tasks, and executing them autonomously. It adapts to changing conditions in real-time while optimizing workflows.Siemens predictive maintenance systems: Agentic AI agents autonomously monitor machinery, predict failures, and adjust production schedules to optimize throughput and minimize downtime.Autonomous trading systems: Agentic AI monitors market trends, executes trades, and optimizes portfolios by analyzing economic signals faster than human analysts.: Uses agentic AI to autonomously analyze user behavior, detect anomalies, and prevent breaches in real-time by adapting its detection models based on evolving threat patterns.All these examples demonstrate how agentic AI systems operate with autonomy, adaptability, and goal-oriented behavior across diverse industries.Challenges and limitations of agentic AIAgentic AI unlocks powerful autonomy, but it also introduces new risks. Here's what to watch for:Hallucinations with consequences: Like all LLM-based systems, agentic AI can still hallucinate. But here, a wrong answer isn't just a typo; it might automatically issue a refund, delete a lead, or misroute inventory without human review. Give it a vague or poorly scoped goal, and agentic AI might go rogue. For example, optimizing for speed could mean skipping safety checks or bypassing necessary approvals. With multiple agents, shared memory, and real-time decisions, tracing  a conclusion was made becomes hard. That's a big problem in finance or healthcare, where explainability matters.Governance, bias, and data risks: These systems pull from sensitive data and act on it. Without strong guardrails, they can reinforce bias, leak private info, or make decisions that clash with your company values. Ethical design and oversight protocols are critical. Agentic AI isn't lightweight. It needs high computing power, persistent memory, and robust orchestration — driving up cloud costs and making it harder for lean teams to scale.Future of agentic AI: What's next?We're not heading toward a world where AI supports business decisions. We're heading toward one where it  them.According to Gartner, by 2028, 33% of enterprise software applications will embed agentic AI, enabling 15% of day-to-day decisions without human input.That's not a distant vision — it's the near future knocking.This means that the shift from reactive AI assistants to proactive digital operators is underway. Smart agents will no longer be siloed helpers.They'll collaborate across departments, syncing with CRMs, querying live systems, triggering workflows, and optimizing in real-time.The future is agentic ecosystems: \n Teams of autonomous agents work together — not just to assist but to operate entire functions.Think of sales agents closing leads at night. Ops agents resolve bottlenecks before they escalate. R&D agents run tests while humans sleep.The businesses that win won't just  AI — they'll orchestrate it.From strategy to execution, AI redefines what's possible when machines operate with intent.Now's the time to move from exploration to implementation. Identify where intelligent agents can offload the repetitive, streamline decisions, and drive growth — all without bloating your headcount.Don't just adopt AI. Put it to work. With Salesmate, you can orchestrate intelligent agents that automate follow-ups, update your CRM in real-time, and keep deals moving — even when your team is off the clock.]]></content:encoded></item><item><title>Young Americans Are Spending a Whole Lot Less On Video Games This Year</title><link>https://games.slashdot.org/story/25/07/01/2239206/young-americans-are-spending-a-whole-lot-less-on-video-games-this-year?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from GameSpot: Perhaps responding to economic uncertainty and narrowing job prospects, young people in the United States are significantly cutting back on spending on video games compared to this time last year. While 18- to 24-year-olds aren't buying as much across a range of different categories, losses are concentrated in games. New data published by market research firm Circana and reported by The Wall Street Journal suggests that young adults spent nearly 25% less on video game products in a four-week span in April than in the same timeframe last year. Other categories also dramatic drops: Accessories (down 18%), technology (down 14%), and furniture (down 12%).
 
All categories combined, the 18-24 age group spent around 13% less than last year. This decrease is not reflected among older cohorts, whose spending has been mostly stable year-over-year. The WSJ report suggests that the economic context could be driving young adults to pull back; a tighter labor market, increased economic uncertainty, and student-loan payments restarting all may be contributing to an environment hostile to the spending habits of 18- to 24-year-olds in particular.]]></content:encoded></item><item><title>Foxconn tells hundreds of Chinese staff to return from its Indian iPhone factories</title><link>https://techcrunch.com/2025/07/02/foxconn-tells-hundreds-of-chinese-staff-to-return-from-its-indian-iphone-factories/</link><author>Ram Iyer</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:55:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Foxconn ordered over 300 Chinese employees to return home from its factories in India, per Bloomberg.]]></content:encoded></item><item><title>Better Late Than Never: Linux 6.17 To Enable Intel DG1 Graphics By Default</title><link>https://www.phoronix.com/news/Intel-DG1-Enabled-Linux-6.17</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:49:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Prior to the DG2/Alchemist discrete GPUs from Intel there was the DG1 graphics processor that served primarily as the initial developer vehicle for facilitating Intel's modern discrete GPU push. DG1 ended up being in the Intel Xe MAX GPU for a small number of laptops and then there's also been a select number of DG1 graphics cards surfacing on eBay in the years since. Only now in 2025 is the upstream Linux kernel driver set to enable Intel DG1 graphics out-of-the-box for modern Linux distributions...]]></content:encoded></item><item><title>Ted Cruz’s Dumb Plan To Punish States That Regulate AI By Withholding Broadband Grants Falls Apart</title><link>https://www.techdirt.com/2025/07/02/ted-cruzs-dumb-plan-to-punish-states-that-regulate-ai-by-withholding-broadband-grants-falls-apart/</link><author>Karl Bode</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:28:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The proposal was one of several cut to try and get the hugely unpopular GOP bill across the finish line. As it turns out, Cruz had a tough time getting enough support for his ignorant plan, and ultimately joined 98 other Senators in a 99-1 vote shooting down the amendment (Sen. Thom Tillis was the one dissenting vote):“Facing overwhelming opposition from both Democrats and Republicans, Sen. Ted Cruz (R-Texas) accepted defeat and joined a 99-1 vote against his own plan to punish states that regulate artificial intelligence.”States are poised to get more than $42.5 billion dollars in broadband deployment subsidies as part of the 2021 infrastructure bill. The Broadband Equity, Access and Deployment (BEAD), a key component of the bill, had taken years of collaborative work between state and federal governments. In part because we needed to remap broadband access across every county in the United States. A lot of this money is poised (as usual) to get dumped in the laps of telecom giants, which is a major reason Cruz’s gambit failed (AT&T drove heavy opposition by longtime AT&T ally Marsha Blackburn, who initially worked with Cruz on a “compromise” offering, before that collapsed entirely). But much of this money is also poised to go to really useful fiber upgrade proposals via efforts like regional cooperatives or community-owned broadband networks. So while it’s nice Ted Cruz’s latest dumb effort failed, it’s hard to be celebratory. Republicans have been taking an absolute hatchet to every last federal effort to ensure our monopoly-dominated broadband networks are affordable. They’ve also effectively killed all federal consumer protection; policies that will reverberate in negative ways for decades to come. The budget battle followed the fairly typical Republican playbook: make your initial offer so extremist and awful that any concessions are disguised to feel like a victory. But the final GOP budget bill remains a giant and unpopular piece of shit, and one of the most corrupt and disgusting attacks on vulnerable Americans in the history of modern politics.]]></content:encoded></item><item><title>Efficient Proof Systems: How PoStake, PoSpace, and VDFs Impact Blockchain Security</title><link>https://hackernoon.com/efficient-proof-systems-how-postake-pospace-and-vdfs-impact-blockchain-security?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:00:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Adversarial mining strategies in blockchains based on efficient proof systems can be analysed with respect to several adversarial goals. Here, we outline three such goals: double spending, short and long term selfish-mining.\
The first objective is double spending, where one considers the probability of an adversarial chain overtaking the public, honest chain [8, 29] (see Figure 3). Here the goal of the adversary is to rapidly and secretly grow a sufficiently long private chain such that this private chain eventually overtakes the honest chain, this way removing a presumably confirmed transaction. What “sufficiently” long means depends on the confirmation time in the chain, e.g., in Bitcoin one generally assumes a transaction that is six blocks deep in the chain to be confirmed.The second objective, “short-term selfish mining”, considered eschews the goal of overtaking the honest chain completely and focuses simply on finding an adversarial mining strategy that is more profitable for the adversary rather than following the stipulated mining protocol [12] (see Figure 4). The profitability of an adversarial mining strategy under this objective is measured by the total number of adversarial blocks on the main chain. Like in the analyses of selfish mining strategies under the first objective, analyses of strategies under this second objective also focus on finding the largest fraction of adversarial resources the blockchain can tolerate in order to be secure under such adversarial strategies.B EFFICIENT PROOF SYSTEMS. PoStake is a block leader election protocol where a leader is selected with probability proportionate to the amount of stake (i.e., coins) they hold in the ledger at the selection time. Thus, a user with 𝑝 ∈ [0, 1] fraction of stake is elected with probability proportionate to 𝑝. Examples of longest-chain blockchains based on PoStake are Ouroboros [9] and post-merge Ethereum [1].\
. Proof of Space (PoSpace) is a protocol between a  and a verifier whereby the prover stores some data and, upon a challenge from the verifier, has to return a solution to the challenge that involves reading a small portion of the data. The consensus protocol of blockchains based on PoST [8] use both PoSpace challenges as well as verifiable delay functions [4, 25, 30] (VDFs). VDFs are functions that are inherently sequential to compute but the correctness of computation is efficiently verifiable. As such, the process of mining blocks in such blockchains depends not\
\
only on the amount of space allocated to compute PoSpace challenges, but also on the amount of VDFs to compute VDF challenges.(1) Krishnendu Chatterjee, IST Austria, Austria (krishnendu.chatterjee@ist.ac.at);(2) Amirali Ebrahimzadeh, Sharif University of Technology, Iran (ebrahimzadeh.amirali@gmail.com);(3) Mehrdad Karrabi, IST Austria, Austria (mehrdad.karrabi@ist.ac.at);(4) Krzysztof Pietrzak, IST Austria, Austria (krzysztof.pietrzak@ist.ac.at);(5) Michelle Yeo, National University of Singapore, Singapore (mxyeo@nus.edu.sg);(6) Ðorđe Žikelić, Singapore Management University, Singapore (dzikelic@smu.edu.sg).]]></content:encoded></item><item><title>Bigfoot Was Just the Beginning of the Content Revolution - An AI Renaissance</title><link>https://hackernoon.com/bigfoot-was-just-the-beginning-of-the-content-revolution-an-ai-renaissance?source=rss</link><author>CeThe.World</author><category>tech</category><pubDate>Wed, 2 Jul 2025 12:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How Google's Veo 3 sparked UGC creativity and redefined the boundaries between reality and imagination—my thoughts on the future of the content industry."Renaissance" was first coined by French historian Jules Michelet to describe the 16th-century period as "the discovery of the world and of humanity” — For You Page's New Favorite Kid\
There are so many viral videos created and watched that people started curating daily Veo3 video rankings—just like the NBA’s *Top 5 plays of the night—*sharing the most creative viral videos every day. Videos with millions—even hundreds of millions—of views have given global users a taste of the creative revolution brought by AI-generated videos.What sparked this UGC creativity boom was the Bigfoot vlogs. It’s hard to trace “patient zero” who started it, but within days of going viral, different Bigfoot characters with various accents, furs, and pets emerged everywhere. I personally followed one with a unique comedic "personality" —by the name of "Speedilla."The story of Bigfoot Speedilla begins in Hamburg, Germany, where his  Mo, a Syrian immigrant, discovered that AI-generated Bigfoot vlogs could serve as both artistic expression and potential income during his spare time from work."Speedilla, he is part of my inner self," Mo explains when describing the character. "I'm trying to create some kind of dark, satirical comedy." In Speedilla's virtual world, comedy is the surface of his content, while satirical reflections form its core. As Bigfoot gets rejected by a woman he's trying to court and subsequently turns her into steak as a revenge, this 20-second piece receives hundreds of thousands of likes—it's a collective “middle finger” to mainstream cultural taboos from a group that empathizes with Mo’s creation.\
It only takes a couple of viral videos for Speedilla to build tens of thousands of followers and attract brand collaboration offers. But the good times didn't last long. Mo soon discovered his content being stolen and replicated by others. He asked his followers to report the “infringement” to the platform, but TikTok did not respond to the claims.\
As Lifehacker's Stephen Johnson observed, driven by the profit motives of overnight viral success, countless creators are "putting Bigfoot in all kinds of ridiculous scenarios." This extends far beyond Bigfoot—TikTok has seen an  of AI-generated content featuring Star Wars, Harry Potter, and other film franchises, creating a vast but copyright-ambiguous UGC content ecosystem.These vast copyright gray areas haven't gone unnoticed. Entertainment giants Disney and Universal recently sued AI company Midjourney, calling its AI image generator a "bottomless pit of plagiarism" that blatantly infringes on their intellectual property libraries. In this 143-page lawsuit, Disney catalogued Midjourney's blatant copying of their signature characters, including Storm Troopers and Darth Vader from Star Wars, Elsa from Frozen, and the Minions from Despicable Me, just to name a few.\
The lawsuit not only accuses the AI model of directly copying character appearances but also charges that its outputs constitute "infringing derivative works." Disney argues that these images generated from simple prompts are not users' original works, but  of their copyrighted characters, directly violating copyright holders' exclusive rights to derivative works.This accusation directly speaks to the core problem creators face: How to define the originality of AI-generated works? If simple AI outputs are "infringing derivative works," what must creators do to make their works legally recognized as copyrightable original creations?Provide original input instructions: As in the previously successful  case, the creator uploaded an original hand-drawn rose artwork as a foundation, then used text prompts to have AI visually process it. The composition and creative conception of this hand-drawn draft were considered key evidence of human creative control over the final product.Extensively modify and arrange AI-generated content: In another successfully registered case, "A Single Piece of American Cheese," the creator performed 35 image detail redraws on the AI-generated initial image, adding elements like a third eye and melting cheese, and recomposing the overall picture. The U.S. Copyright Office recognized this active "selection, combination, and arrangement" of AI-generated materials as demonstrating sufficient human originality.These precedents offer some guidance for AI creators: While the legal framework is still forming, it's clear that copyright is meant to protect human creativity, not AI's computational results. For individual creators like Mo, this means simply generating a character is far from enough. \
When a generic Bigfoot character was replicated by others, the creator was virtually helpless because he couldn't prove he had invested sufficient, legally protectable "human creativity" in the character's generation.Veo 3 faces similar challenges to Midjourney, not only in copyright issues, but its realism and physical accuracy create information risks that cannot be ignored.Testing by TIME magazine found that Veo 3 can generate convincingly realistic "fake news", including Pakistani crowds burning Hindu temples, Chinese researchers handling bats in virus laboratories, and election workers shredding ballots—extremely inflammatory and controversial content.   \
This ability to easily manufacture content has pushed both creation and consumption platforms to the forefront, making transparency assurance and prevention of misinformation an urgent "labeling war" requiring deep involvement from both regulatory bodies and tech companies worldwide.Facing escalating misinformation risks, platforms have implemented inconsistent and often inadequate strategies. As TIME magazine reported, only after they contacted Google about misleading videos generated by Veo 3 did Google, as the content creation platform, take reactive measures: adding a tiny, easily croppable, visible watermark to videos.\
On the other side, where the content is distributed and watched by billions of people, TikTok, the short video platform famous for viral content, has, since September 2023, required creators to actively label AI-generated content to avoid misleading audiences. But in reality, when scrolling through the TikTok feed today, one will find very limited coverage of this "AI-generated" label, with only a small portion of content displaying this label due to creators' voluntary disclosure.\
The reason behind this is simple: creators all want their content to appear as realistic as possible. Having a prominent label telling viewers "this is AI-generated" would presumably impact video performance heavily, so relying solely on creators' voluntary reporting isn’t adequate.\
Globally, regulatory requirements for AI content labeling are inconsistent. China and the EU take the lead in this space, respectively legislating a mandate that requires content platforms to implement metadata for machine and visible watermarks for humans to identify AI content. The U.S. hasn't legislated at the federal level, but states like California have taken the lead, with multiple bills in the works. Other regions, including Australia, ASEAN, and the Middle East, have yet to legislate, details see Table 1.\
With incomplete regulatory frameworks, social media platforms are proactively becoming de facto rule-makers. China's Douyin, for example, strictly enforces national mandatory requirements, requiring both standardized metadata and fixed-position visible watermarks for all AI-generated content. Meta’s Instagram and Facebook, Douyin’s sibling TikTok, YouTube, and Snapchat are also actively advancing dual strategies combining metadata (like C2PA standards) and visible labels (like "AI Info," "Imagined with AI"), but implementation varies. \
In contrast, some platforms like X (formerly Twitter) have been slower to respond, not yet introducing AI content labeling policies that promote information transparency. This inconsistent status across platforms reflects tech companies' attempts to balance user growth, commercial success, and content compliance in the AI race.Table 1: AI Content Labeling Requirements by Country/Region| Country/Region | Metadata Marking Requirements | Visible Watermark Requirements | Effective Date | Information Source |
|----|----|----|----|----|
|  | : All AI-generated content must include metadata for tracking, classification, and platform information. | : All publicly released AI-generated content must have clear (visible) watermarks. | September 1, 2025 | China Law Translate, Douyin Regulations |
|  | : Under the AI Act, AI-generated content must have machine-readable metadata (like digital watermarks, C2PA). | Not explicitly mandated, but future guidelines or industry practices may require. | August 2026 | EU AI Act \n Data Innovation |
|  |  (federal level); but multiple bills have proposed mandatory requirements, and some states (like California) have passed legislation. |  (federal level); but California law and proposed federal bills require visible watermarks. | California: January 1, 2026 | FPF, PBS |
|  | No legal requirement; best practice is using metadata and provenance standards (C2PA). | No legal requirement; visible labels are best practice but not mandatory. | - | Legal123 |
|  | ; regional guidelines support digital watermarks and encrypted provenance (C2PA). | Recommended as best practice, not legally mandatory. | - | ASEAN Guide |
|  | No specific AI legal requirements; ethical guidelines encourage transparency and data provenance. | No legal requirements; visible labels encouraged under ethical principles. | - | Thomson Reuters |However, despite the complexity of copyright, misinformation, and platform regulation, Veo 3's potential is also being explored by professional directors. While countless content creators chase the next "Bigfoot"-style viral moment, Turkish self-made director Öner S. Biberkökü chose a completely different path. His goal wasn't to create a quick viral moment that would spread on social media, but to use this new technology to tell a story that could deeply move hearts.\
Öner's collaborator is Turkey's household name, "Queen of Pop" Sezen Aksu, a legendary figure spanning five decades in music, who released a new song, Doğrucu. "I want people to remember their childhood," Öner mentioned in my interview, "I listened to her songs as a child, her voice has special meaning for Turkish people."\
This project cost tens of thousands of dollars in Veo3 credits alone, plus substantial human resources, completed by Öner's team (Pepperroot Studio) working around the clock. During creation, Öner and his team deeply explored Veo 3's technical limitations. He frankly admitted that achieving character consistency was a "nightmare" using this tool alone, forcing them to combine multiple generation methods through careful planning and repeated experimentation to finally achieve the desired outcome.\
This approach differs from another music video Öner created 4 months earlier, "Uchigatana" (which I often refer to as "Reborn as a Samurai in Edo"), which pursued spectacle moments, style transfer, and lip sync by an AI character that showcased AI’s capabilities. This new music video with Sezen Aksu deliberately maintained restraint and simplicity. "I wanted a simple music video with only one magical moment—the instant when sparrows lift the woman from a fall," Öner said. \
He prioritized emotional delivery over AI technical showmanship. Ultimately, this "magical moment" served the song's narrative, completing the work's concrete expression to aspire for "hope."The same creative tool, in different creators' hands, some people can produce comedic viral content, some people use it to fabricate misinformation, and some would wield its power to call for a nation's emotional response.Looking Forward: The AI RenaissanceIn addition to Google's Veo3, the AI video model field is highly competitive with numerous participants. There are platforms like Kling (by Kuaishou) and Dreamina (by ByteDance) that possess vast video training data from their short video platforms, as well as companies like Runway, Higgsfield, and Minimax, etc. continuously iterating products in their respective niche sub-fields. \
New technological innovations are announced almost every few days. Even Reddit already has rumors circulating that Veo 4 will be released in December 2025, though no one can accurately predict what capabilities these models will achieve by then.\
However, as Google CEO Sundar Pichai said in a podcast interview, the trend from these advanced models is clear—they will enable a dramatic increase in content creation, empowering a broader range of creators—democratizing video creation. He predicts: using AI video tools will become as commonplace as “using Google Docs is today."\
But I believe the core of content creation has never been, and should never be, determined by how advanced our tools are, but by the human spirit with which we wield these tools.\
The Veo 3 viral moment represents more than technological progress—it's a mirror reflecting the essence of human creativity. When a Syrian immigrant in Germany creates artistic expression with an AI Bigfoot, when a Turkish team produces emotionally complete stories at minimal cost, when the boundaries between real and synthetic blur, when you and I can use AI tools to manifest "What You Think Is What You See" we're not just witnessing a tool's evolution—we're observing the transformation across the entire content industry, what some call the AI Renaissance, an era where AI is accelerating the re-discovery and re-definition of humanity’s place in the world.\
This emerging AI Renaissance brings us back to our fundamental principles. The question we need to answer isn't whether AI will change content creation—because it already has. The question is whether we can establish a framework that embraces these tools' democratizing potential while protecting the value of human creativity.\
In that future of AI Renaissance, the stories we choose to tell—and how we tell them—will matter far more than the technology used to bring those stories to life.Öner S. Biberkökü and Cansın Çetin Kuşluvan (Co-founders of Turkish creative studio Pepperroot Studio). Video conference recordings with the author, June 2025. (AI content creator). Video conference recordings with the author, June 2025.II. Legal Documents and ReportsIII. News Reports and Analysis]]></content:encoded></item><item><title>Qantas hack results in theft of 6 million passengers’ personal data</title><link>https://techcrunch.com/2025/07/02/qantas-hack-results-in-theft-of-6-million-passengers-personal-data/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Wed, 2 Jul 2025 11:48:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Qantas, the largest airline in Australia, confirmed the theft of 6 million customers' personal information.]]></content:encoded></item><item><title>This Math Hack Could Let Miners Earn Extra on Blockchains</title><link>https://hackernoon.com/this-math-hack-could-let-miners-earn-extra-on-blockchains?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 2 Jul 2025 11:00:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We initiated the study of optimal selfish mining strategies for unpredictable blockchain protocols based on efficient proof systems. To this end, we considered a selfish mining objective corresponding to changes in chain quality and proposed a novel selfish mining attack that aims to maximize this objective. We formally modeled our attack as an MDP strategy and we presented a formal analysis procedure for computing an 𝜖-tight lower bound on the optimal expected relative revenue in the MDP and a strategy that achieves it for a specified precision 𝜖 > 0. The procedure is fully automated and provides formal guarantees on the correctness of the computed bound.\
We believe that our work opens several exciting lines for future research. We highlight two particular directions. First, our formal analysis only allows us to compute lower bounds on the expected relative revenue that an adversary can achieve. A natural direction of future research would be to consider computing upper bounds on the optimal expected relative revenue for fixed resource amounts. Second, as discussed in Section 3.4, our formal analysis only computes 𝜖-tight lower bounds on the expected relative revenue by following a strategy in our MDP model. However, our model in Section 3.2 introduces assumptions such as growing private forks instead of trees and bounding the maximal length of each fork for tractability purposes. It would be interesting to study whether these assumptions could be relaxed while still providing formal correctness guarantees.This work was supported in part by the ERC-2020-CoG 863818 (FoRM-SMArt) grant and the MOE-T2EP20122-0014 (Data-Driven Distributed Algorithms) grant.[1] 2022. https://ethereum.org/en/roadmap/merge/.\
[2] Adam Back. 1997. Hashcash. http://hashcash.org/.\
[3] Vivek Kumar Bagaria, Amir Dembo, Sreeram Kannan, Sewoong Oh, David Tse, Pramod Viswanath, Xuechao Wang, and Ofer Zeitouni. 2022. Proof-of-Stake Longest Chain Protocols: Security vs Predictability. In Proceedings of the 2022 ACM Workshop on Developments in Consensus, ConsensusDay 2022, Los Angeles, CA, USA, 7 November 2022, Jorge M. Soares, Dawn Song, and Marko Vukolic (Eds.). ACM, 29–42. https://doi.org/10.1145/3560829.3563559\
[4] Dan Boneh, Joseph Bonneau, Benedikt Bünz, and Ben Fisch. 2018. Verifiable Delay Functions. IACR Cryptol. ePrint Arch. (2018), 601. https://eprint.iacr.org/2018/601\
[5] Jonah Brown-Cohen, Arvind Narayanan, Christos-Alexandros Psomas, and S. Matthew Weinberg. 2018. Formal Barriers to Longest-Chain Proof-of-Stake Protocols. CoRR abs/1809.06528 (2018). arXiv:1809.06528 http://arxiv.org/abs/ 1809.06528\
[6] Jing Chen and Silvio Micali. 2019. Algorand: A secure and efficient distributed ledger. Theor. Comput. Sci. 777 (2019), 155–183.\
[7] Bram Cohen and Krzysztof Pietrzak. 2019. The chia network blockchain. https://docs.chia.net/assets/files/Precursor-ChiaGreenPaper82cb50060c575f3f71444a4b7430fb9d.pdf\
[8] Bram Cohen and Krzysztof Pietrzak. 2023. Chia Greenpaper. https://docs.chia. net/green-paper-abstract\
[9] Bernardo Machado David, Peter Gazi, Aggelos Kiayias, and Alexander Russell. 2017. Ouroboros Praos: An adaptively-secure, semi-synchronous proof-of-stake protocol. IACR Cryptol. ePrint Arch. (2017), 573. http://eprint.iacr.org/2017/573\
[10] Stefan Dziembowski, Sebastian Faust, Vladimir Kolmogorov, and Krzysztof Pietrzak. 2015. Proofs of Space. In Advances in Cryptology - CRYPTO 2015 - 35th Annual Cryptology Conference, Santa Barbara, CA, USA, August 16-20, 2015, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 9216), Rosario Gennaro and Matthew Robshaw (Eds.). Springer, 585–605. https://doi.org/10.1007/978-3- 662-48000-7_29\
[11] Ittay Eyal and Emin Gün Sirer. 2018. Majority is not enough: bitcoin mining is vulnerable. Commun. ACM 61, 7 (2018), 95–102. https://doi.org/10.1145/3212998\
[12] Lei Fan and Hong-Sheng Zhou. 2017. iChing: A Scalable Proof-of-Stake Blockchain in the Open Setting (or, How to Mimic Nakamoto’s Design via Proofof-Stake). IACR Cryptol. ePrint Arch. (2017), 656. http://eprint.iacr.org/2017/656\
[13] Matheus V. X. Ferreira, Ye Lin Sally Hahn, S. Matthew Weinberg, and Catherine Yu. 2022. Optimal Strategic Mining Against Cryptographic Self-Selection in Proof-of-Stake. In EC. ACM, 89–114.\
[14] Matheus V. X. Ferreira and S. Matthew Weinberg. 2021. Proof-of-Stake Mining Games with Perfect Randomness. In EC ’21: The 22nd ACM Conference on Economics and Computation, Budapest, Hungary, July 18-23, 2021, Péter Biró, Shuchi Chawla, and Federico Echenique (Eds.). ACM, 433–453. https: //doi.org/10.1145/3465456.3467636\
[15] Jerzy Filar and Koos Vrieze. 2012. Competitive Markov decision processes. Springer Science & Business Media.\
[16] Juan A. Garay, Aggelos Kiayias, and Nikos Leonardos. 2015. The Bitcoin Backbone Protocol: Analysis and Applications. In EUROCRYPT (2) (Lecture Notes in Computer Science, Vol. 9057). Springer, 281–310.\
[17] Peter Gazi, Aggelos Kiayias, and Alexander Russell. 2020. Tight Consistency Bounds for Bitcoin. In CCS. ACM, 819–838.\
[18] Christian Hensel, Sebastian Junges, Joost-Pieter Katoen, Tim Quatmann, and Matthias Volk. 2022. The probabilistic model checker Storm. Int. J. Softw. Tools Technol. Transf. 24, 4 (2022), 589–610. https://doi.org/10.1007/s10009-021-00633-z\
[19] Charlie Hou, Mingxun Zhou, Yan Ji, Phil Daian, Florian Tramèr, Giulia Fanti, and Ari Juels. 2021. SquirRL: Automating Attack Analysis on Blockchain Incentive Mechanisms with Deep Reinforcement Learning. In NDSS. The Internet Society.\
[20] Marta Z. Kwiatkowska, Gethin Norman, and David Parker. 2011. PRISM 4.0: Verification of Probabilistic Real-Time Systems. In CAV (Lecture Notes in Computer Science, Vol. 6806). Springer, 585–591.\
[21] Satoshi Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system. https: //bitcoin.org/bitcoin.pdf.\
[22] James R Norris. 1998. Markov chains. Number 2. Cambridge university press.\
[23] Sunoo Park, Albert Kwon, Georg Fuchsbauer, Peter Gazi, Joël Alwen, and Krzysztof Pietrzak. 2018. SpaceMint: A Cryptocurrency Based on Proofs of Space. In Financial Cryptography (Lecture Notes in Computer Science, Vol. 10957). Springer, 480–499.\
[24] Rafael Pass, Lior Seeman, and Abhi Shelat. 2017. Analysis of the Blockchain Protocol in Asynchronous Networks. In EUROCRYPT (2) (Lecture Notes in Computer Science, Vol. 10211). 643–673.\
[25] Krzysztof Pietrzak. 2019. Simple Verifiable Delay Functions. In ITCS (LIPIcs, Vol. 124). Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 60:1–60:15.\
[26] Martin L. Puterman. 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley.\
[27] Ayelet Sapirshtein, Yonatan Sompolinsky, and Aviv Zohar. 2016. Optimal Selfish Mining Strategies in Bitcoin. In Financial Cryptography (Lecture Notes in Computer Science, Vol. 9603). Springer, 515–532.\
[28] Roozbeh Sarenche, Svetla Nikova, and Bart Preneel. 2024. Deep Selfish Proposing in Longest-Chain Proof-of-Stake Protocols. In Financial Cryptography and Data Security.\
[29] Xuechao Wang, Govinda M. Kamath, Vivek Kumar Bagaria, Sreeram Kannan, Sewoong Oh, David Tse, and Pramod Viswanath. 2019. Proof-of-Stake Longest Chain Protocols Revisited. CoRR abs/1910.02218 (2019).\
[30] Benjamin Wesolowski. 2018. Efficient verifiable delay functions. IACR Cryptol. ePrint Arch. (2018), 623. https://eprint.iacr.org/2018/623\
[31] Roi Bar Zur, Ittay Eyal, and Aviv Tamar. 2020. Efficient MDP Analysis for SelfishMining in Blockchains. In AFT. ACM, 113–131.(1) Krishnendu Chatterjee, IST Austria, Austria (krishnendu.chatterjee@ist.ac.at);(2) Amirali Ebrahimzadeh, Sharif University of Technology, Iran (ebrahimzadeh.amirali@gmail.com);(3) Mehrdad Karrabi, IST Austria, Austria (mehrdad.karrabi@ist.ac.at);(4) Krzysztof Pietrzak, IST Austria, Austria (krzysztof.pietrzak@ist.ac.at);(5) Michelle Yeo, National University of Singapore, Singapore (mxyeo@nus.edu.sg);(6) Ðorđe Žikelić, Singapore Management University, Singapore (dzikelic@smu.edu.sg).]]></content:encoded></item><item><title>GNOME Papers Document Viewer Approved To Replace Evince In GNOME 49</title><link>https://www.phoronix.com/news/GNOME-Papers-Approved-49</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:33:30 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[GNOME Papers has been in development as a modern GTK4-based document viewer. There have been many improvements made to Papers and now ahead of the GNOME 49 release in September, it's been approved to replace Evince as the official document viewer of the GNOME desktop...]]></content:encoded></item><item><title>Towards 21st Century Equitable Prosperity: From Reaganomics to the Abundance Agenda</title><link>https://hackernoon.com/towards-21st-century-equitable-prosperity-from-reaganomics-to-the-abundance-agenda?source=rss</link><author>Ralph Benko and Jeff Garzik</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:30:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The transformation from the Industrial Revolution to the Innovation Revolution.  Good energy policy is key.]]></content:encoded></item><item><title>Mesa&apos;s Zink Preps NV_timeline_semaphore For Better OpenGL-Vulkan Interoperability</title><link>https://www.phoronix.com/news/Zink-NV-Timeline-Semaphore</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:20:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Mike Blumenkrantz with Valve's Linux graphics driver team continues working on enhancements to Mesa's Zink driver for OpenGL implemented over the Vulkan API. A new merge request is further enhancing OpenGL and Vulkan interoperability by supporting the GL_NV_timeline_semaphore extension...]]></content:encoded></item><item><title>Linux Patches Posted For Axiado AX3000 SoC Support</title><link>https://www.phoronix.com/news/Axiado-AX3000-Linux-Patches</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:07:06 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The newest Arm SoC seeing Linux kernel patches working their way toward the mainline kernel is the Axiado AX3000 as a security processor designed for cloud data center, network gear, and more...]]></content:encoded></item><item><title>New Research Shows Classic Selfish Mining Is Outdated</title><link>https://hackernoon.com/new-research-shows-classic-selfish-mining-is-outdated?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:00:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[4 EXPERIMENTAL EVALUATIONWe implement the MDP model and the formal analysis procedure presented in Section 3 and perform an experimental evaluation towards answering the following research questions (RQs):\
 What is the expected relative revenue that our selfish mining strategy achieves? How does it compare to direct extensions of classic selfish mining attacks on PoW blockchains [11] or to mining honestly?\
 How do different values of the System Model parameters impact the expected relative revenue that our selfish mining attack can achieve? The System Model parameters include the relative resource of the adversary 𝑝 ∈ [0, 1] and the switching probability 𝛾 ∈ [0, 1].\
. To answer these RQs, we compare our selfish mining attack against two baselines:\
(1) . This is the strategy extending only the leading block of the main chain.\
(2) Single-tree selfish mining attack. This is the attack strategy that exactly follows the classic selfish mining attack in Bitcoin proposed in [11], however it grows a private tree fork rather than a private chain. Analogously as in [11], the adversary publishes the private tree whenever the length of the main chain catches up with the depth of the private tree. We omit the formal model of this baseline due to space limitations. We also use this baseline to empirically evaluate how severe is the second limitation discussed in Section 3.4.\
 We perform an experimental comparison of our attack and the two baselines for the values of the adversarial relative resource 𝑝 ∈ [0, 0.3] (in increments of 0.01) and the switching probability 𝛾 ∈ {0, 0.25, 0.5, 0.75, 1}. As for the parameters of each selfish mining attack:\
• For our selfish mining attack, we set the maximal length of private forks 𝑙 = 4 and consider all combinations (𝑑, 𝑓 ) ∈ {(1, 1), (2, 1), (2, 2), (3, 2), (4, 2)} of the values of the attack depth 𝑑 and the forking number 𝑓 .\
• For the single-tree selfish mining attack baseline, we set the maximal depth of the private tree 𝑙 = 4 to match our maximal private fork length, and the maximal width of the private tree 𝑓 = 5.\
All experiments were run on Ubuntu 20, 2.80GHz 11th Gen Intel(R) Core(TM) i7-1165G7 CPU, 16 GB RAM, and 16 GB SWAP SSD Memory. For solving mean-payoff MDPs, we use the probabilistic model checker Storm [18], a popular MDP analysis tool within the formal methods community[2].\
. Table 1 shows the runtimes of both our selfish mining attack as well as the single-tree selfish mining attack given various parameter settings and for a fixed switching parameter of 𝛾 = 0.5. We only show timings for 𝛾 = 0.5 as we found the runtimes of our experiments to be very similar across all 𝛾 parameter settings. As can be seen from Table 1, increasing the depth of the attack increases the runtime of our evaluation by an order of magnitude due to the exponential increase in state space.\
Experimental results are shown in Figure 2, showing plots for each 𝛾 ∈ {0, 0.25, 0.5, 0.75, 1}. As we can see from the plots, our selfish mining attack consistently achieves higher expected relative revenue ERRev than both baselines for each value of 𝛾, except when 𝑑 = 1 and 𝑓 = 1. Indeed, already for 𝑑 = 2 and 𝑓 = 1 when the adversary grows a single private fork on the first two blocks in the main chain, our attack achieves higher ERRev than both baselines. This shows that growing private forks at two different blocks already provides a more powerful attack than growing a much larger private tree at a single block. Hence, our results indicate that growing disjoint private forks rather than trees is not a significant limitation, justifying our choice to grow private forks towards making the analysis computationally tractable.\
The attained ERRev grows significantly as we increase 𝑑 and 𝑓 and allow the adversary to grow more private forks. In particular, for 𝑑 = 4, 𝑓 = 2, and relative adversarial resource 𝑝 = 0.3, our attack achieves ERRev that is larger by at least 0.2 than that of both baselines, for all values of the switching probability𝛾. This indicates a  of selfish mining attacks in efficient proof systems blockchains compared to PoW, as the ability to simultaneously grow multiple private forks on multiple blocks translates to a much larger ERRev. Our results suggest that further study of techniques to reduce the advantage of the adversary when mining on several blocks is important in order to maintain reasonable chain quality for efficient proof systems blockchains.\
Finally, we notice that larger 𝛾 values correspond to larger ERRev in our strategies. This is expected, as larger 𝛾 values introduce bias in the likelihood of the adversarial chain becoming the main chain. This is most pertinently observed in the case of 𝑑 = 𝑓 = 1: since 𝑑 = 𝑓 = 1 corresponds to a strategy that only mines a private block on the leading block in the main chain, the only way to deviate from honest mining is to withhold a freshly mined block and reveal it together with the occurrence of a freshly mined honest block. As we can see in the plots, for 𝛾 < 0.5 the achieved ERRev of the strategy with 𝑑 = 𝑓 = 1 corresponds to that of honest mining and the two lines in plots mostly overlap, whereas this strategy only starts to pay off for 𝛾 > 0.5 and for the proportion of resource 𝑝 > 0.25. Altogether, this suggests that further and careful analysis of the control of the adversary over the broadcast network as well as the fork choice breaking rule is necessary.\
 The key takeaways of our experimental evaluation are as follows:\
• Our selfish mining attack achieves  ERRev than both baselines, reaching up to 0.2 difference in ERRev. Thus, our results strongly suggest that growing private forks at multiple blocks is much more advantageous than growing all forks on the first block in the main chain.\
• Our results suggest that growing private trees rather than disjoint private forks would not lead to a significant improvement in the adversary’s ERRev. Hence, the second limitation of our attack discussed in Section 3.4 does not seem to be significant.\
• Our results suggest that enhancing security against selfish mining attacks in efficient proof system blockchains requires further and careful analysis of the control that the adversary has over the broadcast system. In particular, for large values of the switching probability 𝛾, even the simplest variant of our attack with 𝑑 = 1 and 𝑓 = 1 starts to pay off when 𝑝 > 0.25.(1) Krishnendu Chatterjee, IST Austria, Austria (krishnendu.chatterjee@ist.ac.at);(2) Amirali Ebrahimzadeh, Sharif University of Technology, Iran (ebrahimzadeh.amirali@gmail.com);(3) Mehrdad Karrabi, IST Austria, Austria (mehrdad.karrabi@ist.ac.at);(4) Krzysztof Pietrzak, IST Austria, Austria (krzysztof.pietrzak@ist.ac.at);(5) Michelle Yeo, National University of Singapore, Singapore (mxyeo@nus.edu.sg);(6) Ðorđe Žikelić, Singapore Management University, Singapore (dzikelic@smu.edu.sg).[2]   Refer to our github repository for our implementation details: https://github.com/mehrdad76/Automated-Selfish-Mining-Analysis-in-EPSBlockchains]]></content:encoded></item><item><title>China Successfully Tests Hypersonic Aircraft, Maybe At Mach 12</title><link>https://tech.slashdot.org/story/25/07/01/2222223/china-successfully-tests-hypersonic-aircraft-maybe-at-mach-12?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China's Northwestern Polytechnical University successfully tested a hypersonic aircraft called Feitian-2, claiming it reached Mach 12 and achieved a world-first by autonomously switching between rocket and ramjet propulsion mid-flight. The Register reports: The University named the craft "Feitian-2" and according to Chinese media the test flight saw it reach Mach 12 (14,800 km/h or 9,200 mph) -- handily faster than the Mach 5 speeds considered to represent hypersonic flight. Chinese media have not detailed the size of Feitian-2, or its capabilities other than to repeat the University's claim that it combined a rocket and a ramjet into a single unit. [...] The University and Chinese media claim the Feitian-2 flew autonomously while changing from rocket to ramjet while handling the hellish stresses that come with high speed flight.
 
This test matters because, as the US Congressional Budget Office found in 2023, hypothetical hypersonic missiles "have the potential to create uncertainty about what their ultimate target is. Their low flight profile puts them below the horizon for long-range radar and makes them difficult to track, and their ability to maneuver while gliding makes their path unpredictable." "Hypersonic weapons can also maneuver unpredictably at high speeds to counter short-range defenses near a target, making it harder to track and intercept them," the Office found.
 
Washington is so worried about Beijing developing hypersonic weapons that the Trump administration cited the possibility as one reason for banning another 27 Chinese organizations from doing business with US suppliers of AI and advanced computing tech. The flight of Feitian-2 was therefore a further demonstration of China's ability to develop advanced technologies despite US bans.]]></content:encoded></item><item><title>Published Fiction at Center of Fair Use Dispute in Anthropic AI Training Lawsuit</title><link>https://hackernoon.com/published-fiction-at-center-of-fair-use-dispute-in-anthropic-ai-training-lawsuit?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Wed, 2 Jul 2025 09:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 7 of 10. 2. THE NATURE OF THE COPYRIGHTED WORKThe second fair use factor is “the nature of the copyrighted work.” 17 U.S.C. § 107(2). This factor “calls for recognition that some works are closer to the core of intended copyright protection than others, with the consequence that fair use is more difficult to establish when the former works are copied.” Campbell, 510 U.S. at 586. For one thing, less protection is due published works than unpublished ones. For another, less protection is due “factual works than works of fiction or fantasy.” Harper & Row, 471 U.S. at 563. But less protection is not no protection. Even the arrangement of otherwise unprotectable facts surpasses the low bar for a protectable original work of authorship. Google, 804 F.3d at 220. Here, Anthropic accepts that all of Authors’ books — all published, whether non-fiction or fiction — contained expressive elements (Reply 9). And, as set out above, this order accepts Authors’ view of the evidence that their works were chosen for their expressive qualities in building a central library and then in training specific LLMs (Opp. 11, 17 (citing, e.g., Opp. Exh. 3 at -03433)). The main function of the second factor is to help assess the other factors: to reveal differences between the nature of the works at issue and the nature of their secondary use (above), and to reveal any relation between the amount and substantiality of each work taken and the secondary use (next). E.g., Campbell, 510 U.S. at 586; Kelly, 336 F.3d at 820; Google, 804 F.3d at 220; HathiTrust, 755 F.3d at 98; Bill Graham Archives v. Dorling Kindersley Ltd., 448 F.3d 605, 612–13 (2d Cir. 2006). The second factor points against fair use for all copies alike.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>The Limits of Automated Selfish Mining Detection</title><link>https://hackernoon.com/the-limits-of-automated-selfish-mining-detection?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Wed, 2 Jul 2025 09:00:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.4 Key Features and Limitations. The key features of our selfish mining attack and formal analysis are as follows:\
(1) . Manual (i.e. non-automated) analysis of optimal selfish mining attacks is already challenging and technically involved for PoW blockchains, where the adversary only grows a single private fork [11]. Hence, it would be even more difficult and potentially intractable in blockchains based on efficient proof systems. By modelling our selfish mining attack as an MDP and reducing the analysis to solving mean-payoff MDPs, we leverage existing methods for formal analysis of MDPs to obtain a  procedure, thus avoiding the necessity for tedious manual analyses.\
(2) Formal guarantees on correctness. Our analysis provides formal guarantees on the correctness of its output. Again, this is achieved by formally reducing our problem to solving mean-payoff MDPs for which exact algorithms with formal correctness guarantees are available [18, 20].\
(3) Flexibility of the analysis. Our analysis is agnostic to the values of system model and attack parameters and it is flexible to their changes. Hence, it allows us to tweak the parameter values and study their impact on the optimal expected relative revenue, while preserving formal guarantees on the correctness. To illustrate the flexibility, observe that:\
• If the attack depth 𝑑, forking number 𝑓 or maximal fork length 𝑙 of the attack change, then both the state space and the action space of the MDP change.\
• If the relative resource of the adversary 𝑝 or the switching probability 𝛾 change, then the transition function of the MDP changes.\
• As we show in our experiments in Section 4, a change in any of these parameter values results in a change in the optimal expected relative revenue that the adversary can achieve.\
The flexibility of our analysis is thus a significant feature, since it again avoids the need for tedious manual analyses for different parameter values that give rise to different MDPs.\
. While our formal analysis computes an optimal selfish mining strategy in the MDP up to a desired precision, note that there still exist selfish mining attacks that do not correspond to any strategy in our MDP model. Hence, the strategy computed by our method is optimal only with respect to the  of strategies captured by the MDP model. There are two key reasons behind the incompleteness of our MDP model:\
(1) . In order to ensure finiteness of our MDP model, we impose an upper bound 𝑙 on the maximal length of each private fork. This means that the adversary cannot grow arbitrarily long private forks. Since the probability of the adversary being able to grow extremely long private forks is low, we believe that this limitation does not significantly impact the expected relative revenue of selfish mining strategy under this restriction.\
(2) Disjoint forks vs fork trees. Our attack grows private forks on different blocks in the main chain. However, rather than growing multiple disjoint private forks, a more general class of selfish mining attacks would be to allow growing . We stick to disjoint private forks in order to preserve  efficiency of our analysis, since allowing the adversary to grow private trees would result in our MDP states needing to store information about each private tree topology, which would lead to a huge blow-up in the size of the MDP. In contrast, storing disjoint private forks only requires storing fork lengths, resulting in smaller MDP models.\
We conclude by noting that, while our formal analysis is incomplete due to considering a subclass of selfish mining attacks, the formal guarantees provided by our analysis still ensure that we compute a  on the expected relative revenue that a selfish mining attack achieves.(1) Krishnendu Chatterjee, IST Austria, Austria (krishnendu.chatterjee@ist.ac.at);(2) Amirali Ebrahimzadeh, Sharif University of Technology, Iran (ebrahimzadeh.amirali@gmail.com);(3) Mehrdad Karrabi, IST Austria, Austria (mehrdad.karrabi@ist.ac.at);(4) Krzysztof Pietrzak, IST Austria, Austria (krzysztof.pietrzak@ist.ac.at);(5) Michelle Yeo, National University of Singapore, Singapore (mxyeo@nus.edu.sg);(6) Ðorđe Žikelić, Singapore Management University, Singapore (dzikelic@smu.edu.sg).]]></content:encoded></item><item><title>Anthropic Accused of Building Claude AI with 7 Million Pirated Books</title><link>https://hackernoon.com/anthropic-accused-of-building-claude-ai-with-7-million-pirated-books?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Wed, 2 Jul 2025 08:00:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 6 of 10. (ii) The Pirated Library CopiesBefore buying books for its central library, Anthropic downloaded over seven million pirated copies of books, paid nothing, and kept these pirated copies in its library even after deciding it would not use them to train its AI (at all or ever again). Authors argue Anthropic should have paid for these pirated library copies (e.g., Tr. 24–25, 65; Opp. 7, 12–13). This order agrees. The basic problem here was well-stated by Anthropic at oral argument: “You can’t just bless yourself by saying I have a research purpose and, therefore, go and take any textbook you want. That would destroy the academic publishing market if that were the case” (Tr. 53). Of course, the person who purchases the textbook owes no further accounting for keeping the copy. But the person who copies the textbook from a pirate site has infringed already, full stop. This order further rejects Anthropic’s assumption that the use of the copies for a central library can be excused as fair use merely because some will eventually be used to train LLMs. This order doubts that any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use. There is no decision holding or requiring that pirating a book that could have been bought at a bookstore was reasonably necessary to writing a book review, conducting research on facts in the book, or creating an LLM. Such piracy of otherwise available copies is inherently, irredeemably infringing even if the pirated copies are immediately used for the transformative use and immediately discarded. But this order need not decide this case on that rule. Anthropic did not use these copies only for training its LLM. Indeed, it retained pirated copies even after deciding it would not use them or copies from them for training its LLMs ever again. They were acquired and retained, as a central library of all the books in the world. Building a central library of works to be available for any number of further uses was itself the use for which Anthropic acquired these copies. One further use was making further copies for training LLMs. But not every book Anthropic pirated was used to train LLMs. And, every pirated library copy was retained even if it was determined it would not be so used. Pirating copies to build a research library without paying for it, and to retain copies should they prove useful for one thing or another, was its own use — and not a transformative one (see Tr. 24–25, 35, 65; Opp. 4–10, 12 n.6; CC Br. Exh. 12 at -0144509 (“everything forever”)). Napster, 239 F.3d at 1015; BMG Music v. Gonzalez, 430 F.3d 888, 890 (7th Cir. 2005).Anthropic’s briefing contains other reasons why it believes its pirated library copies are irrelevant to our fair use analysis, notwithstanding its own statements at our oral argument. First, Anthropic accepts in this posture that it acted in bad faith but argues that its bad faith in pirating copies cannot “somehow short-circuit[ ]” the fair use analysis (Reply 6 (downplaying Atari Games Corp. v. Nintendo of Am., Inc., 975 F.2d 832, 843 (Fed. Cir. 1992) (applying law of Ninth Circuit))). But its bad faith is not the basis for this decision. Each use of a work must be analyzed objectively. Warhol, 598 U.S. at 544–45. The objective analysis here shows the initial copies were pirated to create a central, general-purpose library, as a substitute for paid copies to do the same thing. (Of course, if infringement is found, bad faith would matter for determining willfulness. 17 U.S.C. § 504(c)(2).) Second, Anthropic argues that its goal to put the copies eventually “to a highly transformative use” requires that each copy and use along the way be justified as having a transformative use, too (Reply 14). But now Anthropic seeks to take the shortcut Anthropic just said cannot be taken. Again, the Supreme Court tasks us with looking past the “subjective intent of the user” to the objective use made of each copy. Warhol, 598 U.S. at 544–45 (emphasis added). Put another way, what a copyist says or thinks or feels matters only to the extent it shows what a copyist in fact does with the work. Indeed, the same copy can be used one way, then another, each with a different result. Id. at 533. Here, what Anthropic said about its acquisitions at the time — that they were made to “build[ ] a research library” while avoiding a “huge legal/practice/business slog” — are relevant in this regard. And, Anthropic’s actual use of these pirated copies was to create its central library of texts that, like any university or corporate library, stored the works’ well-organized facts, analyses, and expressive examples for various contingent uses, one being training. (5).Third, Anthropic argues that Texaco — the case involving copies used in a central library, copies used in desk libraries, and copies used in the laboratory — is inapposite. Anthropic argues that the disputed copies in Texaco were never used in the laboratory but instead in personal desk libraries for a use “identical to the original purpose and use” of the central library copies, and so not for a transformative use (Reply 8 (summarizing 60 F.3d at 922–23)). By contrast, says Anthropic, here it did use copies in the laboratory to train LLMs — a very transformative use. But this is a fast glide over thin ice. Like Texaco, Anthropic possessed copies it did not put into use in the laboratory and it kept those copies in a central library even after its transformative use had been completed. But, unlike Texaco, which bought those copies, Anthropic never paid for the central library copies stolen off the internet. Texaco also shows why Anthropic is wrong to suppose that so long as you create an exciting end product, every “back-end step, invisible to the public,” is excused (Br. 10). Notably, this is not a case where source copies were unavailable for separate purchase or loan. See, e.g., NXIVM Corp. v. Ross Inst., 364 F.3d 471, 475–76, 478–79 (2d Cir. 2004) (using selections of training manual — otherwise available only to cult’s trainees subject to NDAs — to expose cult in critical review); Time Inc. v. Bernard Geis Assocs., 293 F. Supp. 130, 135–36, 138, 146 (S.D.N.Y. 1968) (Judge Inzer Bass Wyatt) (making charcoal drawings of photographs taken of originals otherwise not on sale or loan out to illustrate a history book). (6). Nor were the copies made only incidentally and necessarily from pirated copies. See, e.g., Perfect 10, 508 F.3d at 1164 n.8 (copies of images that had been pirated by third-party websites were used to index those same websites while indexing the entire web). Here, piracy was the point: To build a central library that one could have paid for, just as Anthropic later did, but without paying for it. Nor were the initial copies made immediately transformed into a significantly altered form. In Perfect 10, images were copied by the search engine in thumbnail form only and deployed immediately into the transformative use of identifying the full-sized images and the pages from which they came. 508 F.3d at 1160, 1165, 1167. And, in Kelly v. Arriba Software Corp., images were copied at full size and then into thumbnails for immediate use in building a search engine, after which the full-sized copies were immediately deleted. 336 F.3d 811, 815 (9th Cir. 2003). Not here. The full-text copies of books were downloaded and maintained “forever.” Nor does the initial copying here even resemble the full-text copying in the Google Books cases. There, libraries of authorized copies already had been assembled, and all copies therefrom were made for direct employment in a one-to-one further fair use — whether the transformative use of pointing to the works themselves, the use of providing the works in formats for print-disabled patrons, or the use of insuring against going out of print, getting lost, and becoming otherwise unavailable. HathiTrust, 755 F.3d at 97, 101, 103; Google, 804 F.3d at 206, 216–18, 228 (further distinguishing search and snippet uses, which “test[ed] the boundaries of fair use”). Not so here concerning the pirated copies. No authorized copies existed from which Anthropic made its first copies. No full-text copy therefrom was put immediately into use training LLMs. Not every copy was even necessary nor used for training LLMs. No initial copy was ever deleted, even if never used or no longer used. (7)  The university libraries and Google went to exceedingly great lengths to ensure that all copies were secured against unauthorized uses — both through technical measures and through legal agreements among all participants. Not so here. The library copies lacked internal controls limiting access and use. Nor do the decisions on intermediate copying require anything less than the analysis applied here. Anthropic argues that our court of appeals in Sega Enterprises Ltd. v. Accolade, Inc. looked only at the “ultimate use” and “did not analyze a series of atomized acts of ‘infringement’ distinct from that overall purpose” (Reply 3). To the contrary, the appeals court examined the initial, intermediate, and ultimate copies used by the copyist. The court explained that the copyist initially purchased commercially available copies of game cartridges and then made further copies necessarily and “solely in order to discover the functional requirements for compatibility.” 977 F.2d 1510, 1522 (9th Cir. 1992). Thus, it reached only one result because on those facts there was only one “overall purpose” for the unauthorized copies. Indeed, the court reaffirmed prior caselaw holding that “intermediate copying of [a work] may infringe the exclusive rights granted to the copyright owner in [S]ection 106 of the Copyright Act regardless of whether the end product of the copying also infringes those rights.” Id. at 1518–19 (reaffirming Walker v. Univ. Books, 602 F.2d 859, 864 (9th Cir. 1979)). Similarly, in Sony Computer Entertainment, Inc. v. Connectix Corp., our appeals court applied the same law to similarly focused conduct. Another copyist allegedly had purchased an authorized copy and then made further copies solely and necessarily to reverse-engineer compatibility requirements. 203 F.3d 596, 601, 602–03 (9th Cir. 2000). Both Sega and Sony avoided imposing an “artificial hurdle” to fair use by generously construing the intermediate copying necessary to the fair use. As one example, Sega stated that an engineer should be permitted to reboot her computer while undertaking to reverseengineer software loaded onto it — even if doing so creates another digital copy of the software and is not strictly necessary to reverse-engineering. Id. at 605. But neither Sega nor Sony fathomed gifting an “artificial head start” to a fair user, either, by treating even the initial copy as an intermediate one. And, yes, some courts have “not inquire[d]” into intermediate or initial copying at all (Reply 2 (citing Campbell as not inquiring into surplus copies in the studio)). But if a “close reading of those cases [ ] reveals that in none of them was the legality of the [initial or] intermediate copying at issue,” then it was not raised and not necessarily decided. Sega, 977 F.2d at 1519; see Webster v. Fall, 266 U.S. 507, 511 (1925). It was expressly decided elsewhere: Our analysis must attend to different uses of different copies, and even to different uses of the same copies. Warhol, 598 U.S. at 533. Finally, Anthropic argues that even if the initial copies served a different use than the intermediate and ultimate copies, it was not a use for which Anthropic necessarily would have needed to pay Authors for a copy. In theory, argues Anthropic, it could have done as Google did in Google Books — find an existing reference library willing to loan its copies for free as source copies. Or, in theory, it could have done as Anthropic did later — go buy used copies without having to pay Authors at all. See 17 U.S.C. § 109(a). But Anthropic did not do those things — instead it stole the works for its central library by downloading them from pirated libraries. In sum, the first factor points against fair use for the central library copies made from pirated sources — and no damages from pirating copies could be undone by later paying for copies of the same works.(5) Our court of appeals has not yet reappraised how bad faith (or good faith) figures in fair use after Warhol. Its prior appraisal applied the Supreme Court’s statement that “[f]air use presupposes good faith and fair dealing,” Harper & Row, 471 U.S. at 562 (cleaned up). See Perfect 10, 508 F.3d at1164 n.8. Since then, the Supreme Court has renewed its “skepticism about whether bad faith has any role.” Oracle, 593 U.S. at 32–33 (reiterating doubts of Campbell, 510 U.S. at 585 n.18). And, recently, the Supreme Court has held squarely that it is not the “subjective intent” of a copyist that counts, but the “objective . . . use” of the copy. Warhol, 598 U.S. at 544– 45. This order applies this most recent analysis. Miller v. Gammie, 335 F.3d 889, 900 (9th Cir. 2003) (en banc).(6) Anthropic repeats the misleading characterization of the copyright holder in Oracle that the initial copies were there purloined (Reply 5). Not so. “All agree[d] that Google was and remain[ed] free to use the Java language itself. All agree[d] that Google’s virtual machine [wa]s free of any copyright issues. All agree[d] that the six-thousand-plus method implementations by Google [we]re free of copyright issues. The copyright issue, rather,” was the use of Java for purposes of creating competing software having the same familiar, functional schema. Oracle Am., Inc. v. Google Inc., 872 F. Supp. 2d 974, 978 (N.D. Cal. 2012), aff’d and rev’d in part, 750 F.3d 1339 (Fed. Cir. 2014).(7) Training LLMs was not a use where perpetually maintaining a library copy was intrinsic to the proffered fair use (e.g., for a plagiarism-checker service). Nor is this an instance where retaining at least one copy was authorized by contract with the copyright owners (e.g., by agreement to express terms upon submission to a plagiarism-checker service, notwithstanding proposed terms scrawled on a paper prior to submission). A.V. ex rel. Vanderhye v. iParadigms, LLC, 562 F.3d 630, 635–36 & n.5, 645 n.8 (4th Cir. 2009), aff’g in relevant parts 544 F. Supp. 2d 473, 480 (E.D. Va. 2008) (Judge Claude Hilton). Anthropic mischaracterizes this case.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>How Confidential Are Your Blockchain Transactions, Really?</title><link>https://hackernoon.com/how-confidential-are-your-blockchain-transactions-really?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Wed, 2 Jul 2025 08:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[APPENDIX A. KEY MANAGEMENTA variety of different keys are used in the life cycle of TCSC. For simplicity, we use Intel SGX as the instance. We classify these keys into two types, namely, service keys (top half) and SGX internal keys (bottom half).\
 The MEE key is generated at boot, and is placed in special registers, and destroyed at system reset. The MEE key is used for memory encryption and decryption, which plays a crucial role in protecting the confidentiality and integrity of enclaves. At the same time, different enclaves in the same TEE platform share one function key, such as the report key and the attestation key [88].APPENDIX B. ANONYMITY AND CONFIDENTIALITYAnonymity refers to the privacy that relates to real entities, especially for users’ identities. In a blockchain system, anonymity indicates that users’ transaction activities will not expose any personal information about them. Alternatively, an attack cannot obtain the correct links between real users and their corresponding account/address that sends the transaction [98]. Bitcoin and Ethereum only provide a very early version of anonymity, using the pseudonym-based address mechanism to protect identities. However, this cannot guarantee anonymity because attackers can effortlessly map virtual addresses to physical entities through the relationship analysis.\
Confidentiality in a blockchain system mainly refers to the privacy of data and contents recorded on-chain [99], [9]. Classic blockchain systems expose all transactions (includes amount information, addresses, amount, etc.) plainly where anyone can read and access. Sensitive information might unconsciously be leaked to malicious analyzers. For instance, ERC20 tokens in the Ethereum system do not provide confidentiality, since anyone can observe every amount’s balance. Adversaries can keep tracing the accounts that have a huge amount of tokens and launch attacks such as using the phishing website or cheating through offline activities.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Podcast Episode: Cryptography Makes a Post-Quantum Leap</title><link>https://www.eff.org/deeplinks/2025/06/podcast-episode-cryptography-makes-post-quantum-leap</link><author>Josh Richman</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/2025-htfi-deirdre-blog.png" length="" type=""/><pubDate>Wed, 2 Jul 2025 07:05:17 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[ I only got into cryptography and especially post quantum quickly after that. further into my professional life. I was a software engineer for a whil,e and the Snowden leaks happened, and phone records get leaked. All of Verizon's phone records get leaked. and then Prism and more leaks and more leaks. And as an engineer first, I felt like everything that I was building and we were building and telling people to use was vulnerable. I wanted to learn more about how to do things securely. I went further and further and further down the rabbit hole of cryptography. And then, I think I saw a talk which was basically like, oh, elliptic curves are vulnerable to a quantum attack. And I was like, well, I, I really like these things. They're very elegant mathematical objects, it's very beautiful. I was sad that they were fundamentally broken, and, I think it was, Dan Bernstein who was like, well, there's this new thing that uses elliptic curves, but is supposed to be post quantum secure. But the math is very difficult and no one understands it. I was like, well, I want to understand it if it preserves my beautiful elliptic curves. That's how I just went, just running, screaming downhill into post quantum cryptography. That's Deirdre Connolly talking about how her love of beautiful math and her anger at the Snowden revelations about how the government was undermining security, led her to the world of post-quantum cryptography.I'm Cindy Cohn, the executive director of the Electronic Frontier Foundation. And I'm Jason Kelley, EFF's activism director. You're listening to How to Fix the Internet. On this show we talk to tech leaders, policy-makers, thinkers, artists and engineers about what the future could look like if we get things right online. Our guest today is at the forefront of the future of digital security. And just a heads up that this is one of the more technical episodes that we've recorded -- you'll hear quite a bit of cryptography jargon, so we've written up some of the terms that come up in the show notes, so take a look there if you hear a term you don't recognize. Deidre Connolly is a research engineer and applied cryptographer at Sandbox AQ, with a particular expertise in post-quantum encryption. She also co-hosts the Security, Cryptography, Whatever podcast, so she's something of a cryptography influencer too. When we asked our tech team here at EFF who we should be speaking with on this episode about quantum cryptography and quantum computers more generally, everyone agreed that Deirdre was the person. So we're very glad to have you here. Welcome, Deirdre. Thank you very much for having me. Hi. Now we obviously work with a lot of technologists here and, and certainly personally cryptography is near and dear to my heart, but we are not technologists, neither Jason nor I. So can you just give us a baseline of what post-quantum cryptography is and why people are talking about it? Sure. So a lot of the cryptography that we have deployed in the real world relies on a lot of math and security assumptions on that math based on things like abstract groups, Diffie-Hellman, elliptic curves, finite fields, and factoring prime numbers such as, uh, systems like RSA. All of these, constructions and problems, mathematical problems, have served us very well in the last 40-ish years of cryptography. They've let us build very useful, efficient, small cryptography that we've deployed in the real world. It turns out that they are all also vulnerable in the same way to advanced cryptographic attacks that are only possible and only efficient when run on a quantum computer, and this is a class of computation, a whole new class of computation versus digital computers, which is the main computing paradigm that we've been used to for the last 75 years plus. Quantum computers allow these new classes of attacks, especially, variants of Shore's algorithm – named Dr. Peter Shore – that basically when run on a sufficiently large, cryptographically relevant quantum computer, makes all of the asymmetric cryptography based on these problems that we've deployed very, very vulnerable. So post-quantum cryptography is trying to take that class of attack into consideration and building cryptography to both replace what we've already deployed and make it resilient to this kind of attack, and trying to see what else we can do with these fundamentally different mathematical and cryptographic assumptions when building cryptography. So we've kind of, we've secured our stuff behind a whole lot of walls, and we're slowly building a bulldozer. This is a particular piece of the world where the speed at which computers can do things has been part of our protection, and so we have to rethink that. Yeah, quantum computing is a fundamentally new paradigm of how we process data that promises to have very interesting, uh, and like, applications beyond what we can envision right now. Like things like protein folding, chemical analysis, nuclear simulation, and cryptanalysts, or very strong attacks against cryptography.But it is a field where it's such a fundamentally new computational paradigm that we don't even know what its applications fully would be yet, because like we didn't fully know what we were doing with digital computers in the forties and fifties. Like they were big calculators at one time. When it was suggested that we talk to you about this. I admit that I have not heard much about this field, and I realized quickly when looking into it that there's sort of a ton of hype around quantum computing and post-quantum cryptography and that kind of hype can make it hard to know whether or not something is like actually going to be a big thing or, whether this is something that's becoming like an investment cycle, like a lot of things do. And one of the things that quickly came up as an actual, like real danger is what's called sort of “save now decrypt later.” Oh yeah. Right? We have all these messages, for example, that have been encrypted with current encryption methods. And if someone holds onto those, they can decrypt them using quantum computers in the future. How serious is that danger? It’s definitely a concern and it's the number one driver I would say to post-quantum crypto adoption in broad industry right now is mitigating the threat of a Store Now/Decrypt Later attack, also known as Harvest Now/Decrypt Later, a bunch of names that all mean the same thing.And fundamentally, it's, uh, especially if you're doing any kind of key agreement over a public channel, and doing key agreement over a public channel is part of the whole purpose of like, you want to be able to talk to someone who you've never really, touched base with before, and you all kind of know, some public parameters that even your adversary knows and based on just the fact that you can send messages to each other and some public parameters, and some secret values that only you know, and only the other party knows you can establish a shared secret, and then you can start encrypting traffic between you to communicate. And this is what you do in your web browser when you have an HTTPS connection, that's over TLS.This is what you do with Signal or WhatsApp or any, or, you know, Facebook Messenger with the encrypted communications. They're using Diffie-Helman as part of the protocol to set up a shared secret, and then you use that to encrypt their message bodies that you're sending back and forth between you.But if you can just store all those communications over that public channel, and the adversary knows the public parameters 'cause they're freely published, that's part of Kerckhoff’s Principle about good cryptography - the only thing that the adversary shouldn't know about your crypto system is the secret key values that you're actually using. It should be secure against an adversary that knows everything that you know, except the secret key material. And you can just record all those public messages and all the public key exchange messages, and you just store them in a big database somewhere. And then when you have your large cryptographically relevant quantum computer, you can rifle through your files and say, hmm, let's point it at this.And that's the threat that's live now to the stuff that we have already deployed and the stuff that we're continuing to do communications on now that is protected by elliptic curve Diffie Hellman, or Finite Field Diffie Hellman, or RSA. They can just record that and just theoretically point an attack at it at a later date when that attack comes online. So like in TLS, there's a lot of browsers and servers and infrastructure providers that have updated to post-quantum resilient solutions for TLS. So they're using a combination of the classic elliptic curve, Diffie Hellman and a post-quantum KEM, uh, called ML Kem that was standardized by the United States based on a public design that's been, you know, a multi international collaboration to help do this design. I think that's been deployed in Chrome, and I think it's deployed by CloudFlare and it's getting deployed – I think it's now become the default option in the latest version of Open SSL. And a lot of other open source projects, so that's TLS similar, approaches are being adopted in open SSH, the most popular SSH implementation in the world. Signal, the service has updated their key exchange to also include a post quantum KEM and their updated key establishments. So when you start a new conversation with someone or reset a conversation with someone that is the latest version of Signal is now protected against that sort of attack. That is definitely happening and it's happening the most rapidly because of that Store now/Decrypt later attack, which is considered live. Everything that we're doing now can just be recorded and then later when the attack comes online, they can attack us retroactively. So that's definitely a big driver of things changing in the wild right now. Okay. I'm going to throw out two parallels for my very limited knowledge to make sure I understand. This reminds me a little bit of sort of the work that had to be done before Y2K in, in the sense of like, now people think nothing went wrong and nothing was ever gonna go wrong, but all of us working anywhere near the field know actually it took a ton of work to make sure that nothing blew up or stopped working. And the other is that in, I think it was 1998, EFF was involved in something we called Deep Crack, where we made, that's a, I'm realizing now that's a terrible name. But anyway, the DES cracker, um, we basically wanted to show that DES was capable of being cracked, right? And that this was a - correct me if I'm wrong - it was some sort of cryptographic standard that the government was using and people wanted to show that it wasn't sufficient. Yes - I think it was the first digital encryption standard. And then after its vulnerability was shown, they, they tripled it up to, to make it useful. And that's why Triple DES is still used in a lot of places and is actually considered okay. And then later came the advanced encryption standard, AES, which we prefer today. Okay, so we've learned the lesson, or we are learning the lesson, it sounds like. Yeah, I think that that's, that's right. I mean, EFF built the DES cracker because in the nineties the government was insisting that something that everybody knew was really, really insecure and was going to only get worse as computers got stronger and, and strong computers got in more people's hands, um, to basically show that the emperor had no clothes, um, that this wasn't very good. And I think with the NIST standards and what's happening with post-quantum is really, you know, the hopeful version is we learned that lesson and we're not seeing government trying to pretend like there isn't a risk in order to preserve old standards, but instead leading the way with new ones. Is that fair? That is very fair. NIST ran this post-quantum competition almost over 10 years, and it had over 80 submissions in the first round from all over the world, from industry, academia, and a mix of everything in between, and then it narrowed it down to. the three that are, they're not all out yet, but there's the key agreement, one called ML Kem, and three signatures. And there's a mix of cryptographic problems that they're based on, but there were multiple rounds, lots of feedback, lots of things got broken. This competition has absolutely led the way for the world of getting ready for post-quantum cryptography. There are some competitions that have happened in Korea, and I think there's some work happening in China for their, you know, for their area.There are other open standards and there are standards happening in other standards bodies, but the NIST competition has led the way, and it, because it's all open and all these standards are open and all of the work and the cryptanalysis that has gone in for the whole stretch. It's all been public and all these standards and drafts and analysis and attacks have been public. It's able to benefit everyone in the world. I got started in the crypto wars in the nineties where the government was kind of the problem and they still are. And I do wanna ask you about whether you're seeing any role of the kinda national social security, FBI infrastructure, which has traditionally tried to put a thumb on the scales and make things less secure so that they could have access, if you're seeing any of that there. But on the NIST side, I think this provides a nice counter example of how government can help facilitate building a better world sometimes, as opposed to being the thing we have to drag kicking and screaming into it.But let me circle around to the question I embedded in that, which is, you know, one of the problems that that, that we know happened in the nineties around DES, and then of course some of the Snowden revelations indicated some mucking about in security as well behind the scenes by the NSA. Are you seeing anything like that and, and what should we be on the lookout for? Not in the PQC stuff. Uh, there, like there have been a lot of people that were paying very close attention to what these independent teams were proposing and then what was getting turned into a standard or a proposed standard and every little change, because I, I was closely following the key establishment stuff.Um, every little change people were trying to be like, did you tweak? Why did you tweak that? Did, like, is there a good reason? And like, running down basically all of those things. And like including trying to get into the nitty gritty of like. Okay. We think this is approximately these many bits of security using these parameter and like talking about, I dunno, 123 versus 128 bits and like really paying attention to all of that stuff.And I don't think there was any evidence of anything like that. And, and for, for plus or minus, because there were. I don't remember which crypto scheme it was, but it, there was definitely an improvement from, I think some of the folks at NSA very quietly back in the day to, I think it was the S boxes, and I don't remember if it was DES or AES or whatever it was.But people didn't understand at the time because it was related to advanced, uh, I think it was a differential crypto analysis attacks that folks inside there knew about, and people in outside academia didn't quite know about yet. And then after the fact they were like, oh, they've made this better. Um, we're not, we're not even seeing any evidence of anything of that character either.It's just sort of like, it's very open letting, like if everything's proceeding well and the products are going well of these post-quantum standards, like, you know, leave it alone. And so everything looks good. And like, especially for NSA, uh, national Security Systems in the, in the United States, they have updated their own targets to migrate to post-quantum, and they are relying fully on the highest security level of these new standards.So like they are eating their own dog food. They're protecting the highest classified systems and saying these need to be fully migrated to fully post quantum key agreement. Uh, and I think signatures at different times, but there has to be by like 2035. So if they were doing anything to kind of twiddle with those standards, they'd be, you know, hurting themselves and shooting themselves in the foot. Well fingers crossed. Because I wanna build a better internet and a better. Internet means that they aren't secretly messing around with our security. And so this is, you know, cautiously good news. Let's take a quick moment to thank our sponsor.“How to Fix the Internet” is supported by The Alfred P. Sloan Foundation’s Program in Public Understanding of Science and Technology. Enriching people’s lives through a keener appreciation of our increasingly technological world and portraying the complex humanity of scientists, engineers, and mathematicians.We also want to thank EFF members and donors. EFF has been fighting for digital rights for 35 years, and that fight is bigger than ever, so please, if you like what we do, go to eff.org/pod to donate. Also, we’d love for you to join us at this year’s EFF awards, where we celebrate the people working towards the better digital future that we all care so much about. Those are coming up on September 12th in San Francisco. You can find more information about that at eff.org/awards.We also wanted to share that our friend Cory Doctorow has a new podcast. Listen to this.  [Who Broke the Internet trailer] And now, back to our conversation with Deirdre Connolly. I think the thing that's fascinating about this is kind of seeing this cat and mouse game about the ability to break codes, and the ability to build codes and systems that are resistant to the breaking, kind of playing out here in the context of building better computers for everyone.And I think it's really fascinating and I think it also for people I. You know, this is a pretty technical conversation, um, even, you know, uh, for our audience. But this is the stuff that goes on under the hood of how we keep journalists safe, how we keep activists safe, how we keep us all safe, whether it's our bank accounts or our, you know, people are talking about mobile IDs now and other, you know, all sorts of sensitive documents that are going to not be in physical form anymore, but are gonna be in digital form. And unless we get this lock part right, we're really creating problems for people. And you know, what I really appreciate about you and the other people kind of in the midst of this fight is it's very unsung, right? It's kind of under the radar for the rest of us, but yet it's the, it's the ground that we need to stand on to, to be safe moving forward. Yeah, and there's a lot of assumptions, uh, that even the low level theoretical cryptographers and the people implementing their, their stuff into software and the stuff, the people trying to deploy, that there's a, a lot of assumptions that have been baked into what we've built that to a degree don't quite fit in some of the, the things we've been able to build in a post-quantum secure way, or the way we think it's a post-quantum secure way.Um, we're gonna need to change some stuff and we think we know how to change some stuff to make it work. but we are hoping that we don't accidentally introduce any vulnerabilities or gaps. We're trying, but also we're not a hundred percent sure that we're not missing something, 'cause these things are new. And so we're trying, and we're also trying to make sure we don't break things as we change them because we're trying to change them to be post quantum resilient. But you know, once you change something, if there's a possibility, you, you just didn't understand it completely. And you don't wanna break something that was working well in one direction because you wanna improve it in another direction. And that's why I think it's important to continue to have a robust community of people who are the breakers, right? Who are, are hackers, who are, who are attacking. And that is a, you know, that's a mindset, right? That's a way of thinking about stuff that is important to protect and nurture, um, because, you know, there's an old quote from Bruce Schneider: Anyone can build a crypto system that they themselves cannot break. Right? It takes a community of people trying to really pound away at something to figure out where the holes are. And you know, a lot of the work that EFF does around coders rights and other kinds of things is to make sure that there's space for that. and I think it's gonna be as needed in a quantum world as it was in a kind of classical computer world. Absolutely. I'm confident that we will learn a lot more from the breakers about this new cryptography because, like, we've tried to be robust through this, you know, NIST competition, and a lot of those, the things that we learn apply to other constructions as they come out. but like there's a whole area of people who are going to be encountering this kind of newish cryptography for the first time, and they kind of look at it and they're like. Oh, uh, I, I think I might be able to do something interesting with this, and we're, we'll all learn more and we'll try to patch and update as quickly as possible And this is why we have competitions to figure out what the best options are and why some people might favor one algorithm over another for different, different processes and things like that. And that's why we're probably gonna have a lot of different flavors of post-quantum cryptography getting deployed in the world because it's not just, ah, you know, I don't love NIST. I'm gonna do my own thing in my own country over here. Or, or have different requirements. There is that at play, but also you're trying to not put all your eggs in one basket as well. Yeah, so we want a menu of things so that people can really pick, from, you know, vetted, but different strategies. So I wanna ask the kind of core question for the podcast, which is, um, what does it look like if we get this right, if we get quantum computing and, you know, post-quantum crypto, right?How does the world look different? Or does it just look the same? How, what, what does it look like if we do this well? Hopefully to a person just using their phone or using their computer to talk to somebody on the other side of the world, hopefully they don't notice. Hopefully to them, if they're, you know, deploying a website and they're like, ah, I need to get a Let’s Encrypt certificate or whatever.Hopefully Let's Encrypt just, you know, insert bot just kind of does everything right by default and they don't have to worry about it. Um, for the builders, it should be, we have a good recommended menu of cryptography that you can use when you're deploying TLS, when you're deploying SSH, uh, when you're building cryptographic applications, especially. So like if you are building something in Go or Java or you know, whatever it might be, the crypto library in your language will have the updated recommended signature algorithm or key agreement algorithm and be, like, this is how we, you know, they have code snippets to say like, this is how you should use it, and they will deprecate the older stuff. And, like, unfortunately there's gonna be a long time where there's gonna be a mix of the new post-quantum stuff that we know how to use and know how to deploy and protect. The most important, you know, stuff like to mitigate Store now/Decrypt later and, you know, get those signatures with the most important, uh, protected stuff.Uh, get those done. But there's a lot of stuff that we're not really clear about. How we wanna do it yet, and kind of going back to one of the things you mentioned earlier, uh, comparing this to Y2K, there was a lot of work that went into mitigating Y2K before, during, immediately after.Unfortunately, the comparison to the post quantum migration kind of falls down because after Y2K, if you hadn't fixed something, it would break. And you would notice in usually an obvious way, and then you could go find it. You, you fix the most important stuff that, you know, if it broke, like you would lose billions of dollars or, you know, whatever. You'd have an outage. For cryptography, especially the stuff that's a little bit fancier. Um, you might not know it's broken because the adversary is not gonna, it's not gonna blow up.And you have to, you know, reboot a server or patch something and then, you know, redeploy. If it's gonna fail, it's gonna fail quietly. And so we're trying to kind of find these things, or at least make the kind of longer tail of stuff, uh, find fixes for that upfront, you know, so that at least the option is available. But for a regular person, hopefully they shouldn't notice. So everyone's trying really hard to make it so that the best security, in terms of the cryptography is deployed with, without downgrading your experience. We're gonna keep trying to do that.I don't wanna build crap and say “Go use it.” I want you to be able to just go about your life and use a tool that's supposed to be useful and helpful. And it's not accidentally leaking all your data to some third party service or just leaving a hole on your network for any, any actor who notices to walk through and you know, all that sort of stuff.So whether it's like implementing things securely in software, or it's cryptography or you know, post-quantum weirdness, like for me, I just wanna build good stuff for people, that's not crap. Everyone listening to this agrees with you. We don't want to build crap. We want to build some beautiful things. Let's go out there and do it. Thank you so much, Deirdre. Thank you! Thank you Deirdre. We really appreciate you coming and explaining all of this to, you know, uh, the lawyer and activist at EFF. Well, I think that was probably the most technical conversation we've had, but I followed along pretty well and I feel like at first I was very nervous based on the, save and decrypt concerns. But after we talked to Deirdre, I feel like the people working on this. Just like for Y2K are pretty much gonna keep us out of hot water. And I learned a lot more than I did know before we started the conversation. What about you, Cindy? I learned a lot as well. I mean, cryptography and, attacks on security is always, you know, it's a process, and it's a process by which we do the best we can, and then, then we also do the best we can to rip it apart and find all the holes, and then we, we iterate forward. And it's nice to hear that that model is still the model, even as we get into something like quantum computers, which, um, frankly are still hard to conceptualize. But I agree. I think that what the good news outta this interview is I feel like there's a lot of pieces in place to try to do this right, to have this tremendous shift in computing that we don't know when it's coming, but I think that the research indicates that it SI coming, be something that we can handle, um, rather than something that overwhelms us.And I think that's really,it's good to hear that good people are trying to do the right thing here since it's not inevitable. Yeah, and it is nice when someone's sort of best vision for what the future looks like is hopefully your life. You will have no impacts from this because everything will be taken care of. That's always good. I mean, it sounds like, you know, the main thing for EFF is, as you said, we have to make sure that security engineers, hackers have the resources that they need to protect us from these kinds of threats and, and other kinds of threats obviously.But, you know, that's part of EFF's job, like you mentioned. Our job is to make sure that there are people able to do this work and be protected while doing it so that when the. Solutions do come about. You know, they work and they're implemented and the average person doesn't have to know anything and isn't vulnerable. Yeah, I also think that, um, I appreciated her vision that this is a, you know, the future's gonna be not just one. Size fits all solution, but a menu of things that take into account, you know, both what works better in terms of, you know, bandwidth and compute time, but also what you know, what people actually need.And I think that's a piece that's kind of built into the way that this is happening that's also really hopeful. In the past and, and I was around when EFF built the DES cracker, um, you know, we had a government that was saying, you know, you know, everything's fine, everything's fine when everybody knew that things weren't fine. So it's also really hopeful that that's not the position that NIST is taking now, and that's not the position that people who may not even pick the NIST standards but pick other standards are really thinking through. Yeah, it's very helpful and positive and nice to hear when something has improved for the better. Right? And that's what happened here. We had this, this different attitude from, you know, government at large in the past and it's changed and that's partly thanks to EFF, which is amazing. Yeah, I think that's right. And, um, you know, we'll see going forward, you know, the governments change and they go through different things, but this is, this is a hopeful moment and we're gonna push on through to this future. I think there's a lot of, you know, there's a lot of worry about quantum computers and what they're gonna do in the world, and it's nice to have a little vision of, not only can we get it right, but there are forces in place that are getting it right. And of course it does my heart so, so good that, you know, someone like Deirdre was inspired by Snowden and dove deep and figured out how to be one of the people who was building the better world. We've talked to so many people like that, and this is a particular, you know, little geeky corner of the world. But, you know, those are our people and that makes me really happy. Thanks for joining us for this episode of How to Fix the Internet.If you have feedback or suggestions, we'd love to hear from you. Visit EFF dot org slash podcast and click on listener feedback. While you're there, you can become a member, donate, maybe even pick up some merch and just see what's happening in digital rights this week and every week.Our theme music is by Nat Keefe of BeatMower with Reed MathisHow to Fix the Internet is supported by the Alfred P. Sloan Foundation's program in public understanding of science and technology.I’m Jason Kelley… And I’m Cindy Cohn. This podcast is licensed creative commons attribution 4.0 international, and includes the following music licensed creative commons attribution 3.0 unported by its creators: Drops of H2O, The Filtered Water Treatment by Jay Lang. Sound design, additional music and theme remixes by Gaetan Harris.]]></content:encoded></item><item><title>Court Rules Anthropic’s Book Scans Were Fair Use</title><link>https://hackernoon.com/court-rules-anthropics-book-scans-were-fair-use?source=rss</link><author>Legal PDF: Tech Court Cases</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
ANDREA BARTZ, CHARLES GRAEBER, and KIRK WALLACE JOHNSON v. ANTHROPIC PBC, retrieved on June 25, 2025, is part of . You can jump to any part in this filing . This is part 5 of 10. B. THE COPIES USED TO BUILD A CENTRAL LIBRARYRecall that Anthropic purchased millions of print books for its central library and pirated millions of digital books for its central library, too. It used specific sets and subsets of books for training specific LLMs. And, it then retained all the copies in its central library for other uses that might arise even after deciding it would not use them to train any LLM (at all or ever again). Anthropic seems to believe that because some of the works it copied were sometimes used in training LLMs, Anthropic was entitled to take for free all the works in the world and keep them forever with no further accounting. There is no carveout, however, from the Copyright Act for AI companies. Because the legal issues differ between the library copies Anthropic purchased and pirated, this order takes them in turn.(i) The Purchased Library Copies Converted from Print to DigitalAnthropic purchased millions of print copies to “build a research library” (Opp. Exh. 22 at 145, 148). It destroyed each print copy while replacing it with a digital copy for use in its library (not for sharing nor sale outside the company). As to these copies, Authors do not complain that Anthropic failed to pay to acquire a library copy. Authors only complain that Anthropic changed each copy’s format from print to digital (see Opp. 15, 25 & n.14). On the facts here, that format change itself added no new copies, eased storage and enabled searchability, and was not done for purposes trenching upon the copyright owner’s rightful interests — it was transformative. Anthropic purchased its print copies fair and square. With each purchase came entitlement for Anthropic to “dispose[ ]” each copy as it saw fit. 17 U.S.C. § 109(a). So, Anthropic was entitled to keep the copies in its central library for all the ordinary uses. Yes, Anthropic changed the format of these library copies from print to digital — giving rise to the issue here.All agree on the facts of the format change. Anthropic “destructively scan[ned]” the print copies to create the digital ones. Anthropic or its vendors stripped the bindings from the print books, cut the pages to workable dimensions, and scanned those pages — discarding each print copy while creating a digital one in its place. The digital copy was then housed in the “research library” or “generalized data area” in place of the print copy (Opp. Exh. 22 at 145– 46, 193–94). Authors do not allege and our record does not show that Anthropic provided its converted digital copies of print books to anyone outside Anthropic. The parties disagree about the legal consequences of the format change. Was scanning the print copies to create digital replacements transformative? Anthropic argues it was because it was reasonably necessary to training LLMs. Authors argue it was a distinguishable step requiring independent justification.Here, for reasons narrower than Anthropic offers, the mere format change was a fair use. Storage and searchability are not creative properties of the copyrighted work itself but physical properties of the frame around the work or informational properties about the work. See Texaco, 802 F. Supp. at 14 (physical), aff’d, 60 F.3d at 919; Google, 804 F.3d at 225 (informational); Sony Corp. of Am. v. Universal City Studios, Inc. (“Sony Betamax”), 464 U.S. 417, 447 (1984) (rightful interests). In Texaco, the court reasoned that if a purchased scientific journal article had been copied “onto microfilm to conserve space, this might [have been] a persuasive transformative use.” 802 F. Supp. at 14 (Judge Pierre Leval), aff’d, 60 F.3d at 919 (reducing “bulk[ ]” “might suffice to tilt the first fair use factor in favor of Texaco if these purposes were dominant“). In Google Books, the court reasoned that a print-to-digital change to expose information about the work was transformative. Google, 804 F.3d at 225 (Judge Pierre Leval). And, in Sony Betamax, the Supreme Court held that making a recording of a television show in order to instead watch it at a later time was copying but did not usurp any rightful interest of the copyright owner. 464 U.S. at 447, 455. Important to the Supreme Court’s reasoning was the expectation that most such copiers would not distribute the permanent copies of the work. Finally, in A&M Records, Inc. v. Napster, Inc., our court of appeals recognized the reasoning just explained, and therefore rejected by contrast a digitization effort that was touted as space-shifting but in fact resulted in the multiplication of copies shared with outsiders through a file-sharing service. 239 F.3d 1004, 1019 (9th Cir. 2001), aff’g in this part 114 F. Supp. 2d 896, 912–13, 915–16 (N.D. Cal. 2000) (Judge Marilyn Hall Patel) (citing Sony Betamax and Texaco). Here, every purchased print copy was copied in order to save storage space and to enable searchability as a digital copy. The print original was destroyed. One replaced the other. And, there is no evidence that the new, digital copy was shown, shared, or sold outside the company. This use was even more clearly transformative than those in Texaco, Google, and Sony Betamax (where the number of copies went up by at least one), and, of course, more transformative than those uses rejected in Napster (where the number went up by “millions” of copies shared for free with others).Yes, Anthropic is a commercial outfit. And, this order takes for granted that Anthropic in fact benefited from the print-to-digital format change — or it would not have gone to all the trouble. But the crux of the first fair use factor’s concern for “commercial” use is in protecting the copyright owners and their entitlements to exploit their copyright as they see fit (or not). See, e.g., Harper & Row, Publishers, Inc. v. Nation Enters., 471 U.S. 539, 562 (1985). That the accused is a commercial entity is indicative, not dispositive. That the accused stands to benefit is likewise indicative. But what matters most is whether the format change exploits anything the Copyright Act reserves to the copyright owner. Anthropic already had purchased permanent library copies (print ones). It did not create new copies to share or sell outside. Yes, Authors also might have wished to charge Anthropic more for digital than for print copies. And, this order takes for granted that Authors could have succeeded if Anthropic had been barred from the format change. “But the Constitution’s language [in Clause 8] nowhere suggests that [the copyright owner’s] limited exclusive right should include a right to divide markets or a concomitant right to charge different purchasers different prices for the same book, [merely] say to increase or to maximize gain.” See Kirtsaeng v. John Wiley & Sons, Inc., 568 U.S. 519, 552 (2013); see also U.S. CONST. art. I., § 8, cl. 8. Nor does the Copyright Act itself. Section 106 sets out exclusive rights that fair uses under Section 107 abridge. Section 106(1) reserves to the copyright owner the right to make reproductions. But on our facts we face the unusual situation where one copy entirely replaced the another. And, Section 106(2) reserves to the copyright owner the right to make derivative works that add or subtract creative material — as occurs in a “translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, [or] condensation” of a book, 17 U.S.C. § 101 (definitions). For some “other modification[ ]” of a book to constitute a “derivative work,” it must itself “represent an original work of authorship.” Ibid. But on our facts the format was changed but no content was added or subtracted. See Mirage Editions, Inc. v. Albuquerque A.R.T. Co., 856 F.2d 1341, 1342, 1343– 44 (9th Cir. 1988) (yes where elements added to create new decorative ceramic) (4). Section 106(3) further reserves to the copyright owner the right to distribute copies. But again, the replacement copy here was kept in the central library, not distributed. Cf. Fox News Network, LLC v. TVEyes, Inc., 883 F.3d 169, 176–78 (2d Cir. 2018) (enabling searching for “information about the material” can be transformative use, even if some distribution results); Lewis Galoob Toys, Inc. v. Nintendo of Am., Inc., 964 F.2d 965, 968, 971 (9th Cir. 1992) (using nifty converter to “merely enhance[ ]” audiovisual displays emitted from purchased videogame cartridge was fair use of those displays partly because no surplus copies of cartridge or displays were ever created). As a result, Anthropic’s format-change from print library copies to digital library copies was transformative under fair use factor one. Anthropic was entitled to retain a copy of these works in a print format. It retained them instead in a digital format, easing storage and searchability. And, the further copies made therefrom for purposes of training LLMs were themselves transformative for that further reason, as above. To be clear, this print-to-digital conversion involved a different and narrower form of transformative use than the broader one advanced by Anthropic. Anthropic argues that the central library use was part and parcel of the LLM training use and therefore transformative. This order disagrees. However, this order holds that the mere conversion of a print book to a digital file to save space and enable searchability was transformative for that reason alone. Therefore, the digital copy should be treated just as if the purchased print copy had been placed in the central library. In sum, the first fair use factor favors fair use for the digital library copies converted from purchased print library copies — but these do not excuse the pirated library copies.(4) Even if print-to-digital format change did infringe the right to prepare derivative works, Authors have conceded that “Plaintiffs’ infringement claims are predicated on Anthropic’s unauthorized reproduction (17 U.S.C. § 106(1)); Plaintiffs are not alleging infringement by Anthropic of any right to prepare derivative works (id. at § 106(2))” (Dkt. No. 203 at 2 (citations original)). Whether this concession had consequence for copies tokenized and used for training or “compressed” into the trained LLMs is not reached by this order because Anthropic does not rely on Authors’ concession and those copies were here used transformatively.:::tip
Continue reading . :::info
About HackerNoon Legal PDF Series: We bring you the most important technical and insightful public domain court case filings.\
This court case retrieved on June 25, 2025, from storage.courtlistener.com, is part of the public domain. The court-created documents are works of the federal government, and under copyright law, are automatically placed in the public domain and may be shared without legal restriction.]]></content:encoded></item><item><title>Blockchain Security Layers: Tradeoffs Between L1, L2, and Hardware TEEs</title><link>https://hackernoon.com/blockchain-security-layers-tradeoffs-between-l1-l2-and-hardware-tees?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This section compares layer-one and layer-two solutions, and discusses the hardware’s options and impacts.\
Which solution is more secure? Even if we have built clear security metrics based on threat models and give concise security analyses in the context of layered architectures, it is still inadequate for answering this question: Which solution is more secure, layer-one solution or layer-two solution? This is because system security is a multidimensional topic, and measuring all security aspects is impractical. The security flaws may happen in any phase in a system [87]. Despite some projects performing well in our evaluation, we cannot roughly say that they are more secure. As hybrid technologies, both layer-one and layer-two systems have unsatisfactory security vulnerabilities in existing systems, and they must be carefully treated when applying them to real applications. Frankly speaking, there is a long road to achieving such a practically secure and confidential system. Our aim is not to argue which solution is more secure. Instead, we focus on helping developers and communities to establish a security measurement and avoid potential pitfalls in designing TCSC.\
Which solution is more efficient? The layer-one solutions require the contract to be confidentially executed in a distributed TEE network, which is time-consuming and hard to scale out. In contrast, layer-two systems only upload final calculated results from offline TEEs to online blockchains. Local TEE hosts can execute complicated computations with high scalability and short execution time. Assuming that the on-chain processing time remains stable, the overall performance gets improved by enabling parallel off-chain executions. Thus, from the view of performance and scalability, the layer-two solution is our recommendation.\
Which solution is more adoptable? From the aforementioned discussion, we can observe that the layer-one and layertwo solutions fit different scenarios. The layer-one solution is more adoptable in consortium blockchain systems, while the layer-two solution well fits the existing public blockchain systems. Layer-one systems require each blockchain node to equip a TEE, which is difficult to be fulfilled in a public blockchain while already in use. In a consortium blockchain, the nodes are controllable and manageable, and the committee can require each node to equip a TEE when joining the network. On the flip side, the layer-two solution does not change the original blockchain trust assumption. Instead, it creates an independent layer for executing the smart contract, and thus allows developers to seamlessly integrate the TEE into existing public blockchains without significant modifications.\
B. Hardware-anchored TEE Options\
Securing smart contracts with TEEs is challenging because we have to assume a strong attacker model, in which the attacker has physical possession of the hardware running the smart contract and can interfere with it in powerful ways. This part discusses the security impact of choosing different TEE architectures. In particular, we select  [88],  [89] and  [90] as examples.\
Intel SGX is a system allowing one to set up protected enclaves running on an Intel processor. Such enclaves are protected from malware running outside the enclave, including in the operating system. Enclaves can attest their software and computations using a signing key ultimately certified by Intel. Intel SGX has been marketed for desktop machines and servers alike; Microsoft Azure [91] is a commercial cloud offering that allows cloud customers to set up SGX enclaves in the cloud. Many attacks on SGX have been published in the eight years since its release. They may be categorised as side-channel attacks [92], fault attacks [93], [94] and software attacks [95]. While some of these attacks can be solved by improvements of SGX, it is unclear that it will ever be possible to have a completely secure version, because the attack surface is large, in the case of smart contracts, one has to assume that attackers have physical possession of the hardware.\
ARM TrustZone [89] is a technology widely used in mobile phones to protect secrets, such as secrets used in banking apps. Its ubiquity makes it an attractive option. However, it has been attacked even more than Intel SGX, and doesn’t offer a suitable attestation framework. Future hardware-anchored security products from ARM may address this problem.\
Dedicated chips such as the Open Titan [90] family of chips offer a better solution. Open Titan is an open-source design inspired by Google Titan, a chip used on Google servers and in Google mobile phones. The fact that the smart contract runs on a dedicated chip not shared with attacker code means that the attack surface is much smaller. Attestation frameworks exist for such chips, and the attestation keys can be rooted in a manufacturer’s certificate. The kind of attacks mentioned for SGX become much harder to mount. Nevertheless, even dedicated chips may succumb to a dedicated and resourceful attacker. Researchers have succeeded in mounting attacks based on power side-channels and electromagnetic (EM) radiation side channels. Defences against such attacks include masking, which consists of randomly splitting every sensitive intermediate variable into multiple shares. Even if the adversary is able to learn a share of the secret via side-channel, it would need all of them in order to recover the secret. Fault attacks such as EM and voltage glitching are also possible, but again, there are known defences [96] at both a software and hardware level. Software defences include making secret-dependent computations twice (in general  times) and then comparing results before producing any output. Countermeasures in hardware involve having internal voltage monitoring circuitry, which makes sure that the input voltage remains within a safe operation range and resets the device otherwise.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Why TEE-Based Smart Contracts Still Aren’t Fully Secure</title><link>https://hackernoon.com/why-tee-based-smart-contracts-still-arent-fully-secure?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
\
 The private keys in TEE-assisted systems are extremely crucial but hard to manage. On the one hand, putting the application keys in a single TEE contributes to the key security. However, it also makes the system raise the risk of a single point of failure. On the other hand, sharing the private key among multiple TEEs offers practical availability but (as a sacrifice) increases key exfiltration risk. Meanwhile, key sharing technologies are too complicated to adopt and cannot completely solve the key issues. Suppose that an attacker steals the attestation key somehow. She might consequently generate the attestation materials to deceive the user with a fake fact: The contract has been executed. Even worse, if a root key stored in the tamper-resistant hardware (e.g., Memory Encryption Engine Key in SGX) is compromised, all key technologies for protecting application keys become useless.\
 Compared with cryptographic approaches backed by mathematics [22], [23], [27], the confidential smart contracts relied on TEEs are lack of transparency. On the one hand, contracts are executed inside TEEs, and the outputs are usually encrypted, which lacks public verifiability inherited from traditional blockchain systems. The attestation service can only guarantee that the encrypted outputs indeed come from a TEE. However, neither users nor the blockchain nodes can learn whether a TEE is compromised or executes contracts following the predefined specifications. Even if many TEEs can re-execute the same contract with the same setup (e.g., the same private key) to check outputs, this inevitably increases the key exfiltration risk in the face of a confidentiality breach. On the other hand, the precise architectures of chips are still unclear for some TEE products, such as Intel SGX [80]. TEE-assisted solutions force the user to put too much trust in the manufacturers of this hardware. Users even argue that Intel may have reduced the security of SGX to improve performance to cater for market demand [97]. Additionally, the attestation service used to prove that a program runs inside TEEs is  and . A compromised provider has the ability to insert fake IDs, and further, steal the confidential state in smart contracts.The technologies on how to combine smart-contract execution with TEEs are mushrooming nowadays. The absence of systematic work confuses newcomers. In this paper, we provide the first SoK on TEE-assisted confidential smart contract systems. TEE technologies empower transparent smart contracts with confidentiality, greatly extending the scope of upper-layer applications. We summarize state-of-the-art solutions by proposing a unified framework covering aspects of design models, desired properties, and security considerations. Our analysis clarifies existing challenges and future directions for two mainstream architectures (layer-one and layer-two solutions). We believe that this work represents a snapshot of the technologies that have been open-sourced and made public in time. Our evaluation and analysis within this SoK will offer a good guide for communities, and greatly promote the prosperity of development for TCSC applications.\
. Rujia Li and Qi Wang are partially supported by the Shenzhen Fundamental Research Programs under Grant No.20200925154814002. We thank Xinrui Zhang (SUSTech) for her help. Also, we express our appreciation to anonymous reviewers for their valuable comments.[1] Nick Szabo. Formalizing and securing relationships on public networks. First monday, 1997.\
[2] Gavin Wood et al. Ethereum: A secure decentralised generalised transaction ledger. https://ethereum.github.io/yellowpaper/ paper.pdf , 2022.\
[3] Kevin Delmolino et al. Step by step towards creating a safe smart contract: Lessons and insights from a cryptocurrency lab. In FC, pages 79–94. Springer, 2016.\
[4] Hewa et al. Survey on blockchain based smart contracts: Technical aspects and future research. IEEE Access, 2021.\
[5] Maher Alharby and Aad Van Moorsel. Blockchain-based smart contracts: A systematic mapping study. arXiv preprint arXiv:1710.06372, 2017.\
[6] Marc Jansen et al. Do smart contract languages need to be turing complete? In CBA, pages 19–26. Springer, 2019.\
[7] Siraj Raval. Decentralized applications: harnessing Bitcoin’s blockchain technology. " O’Reilly Media, Inc.", 2016.\
[8] Weiqin Zou et al. Smart contract development: Challenges and opportunities. TSE, 2019.\
[9] Rui Zhang, Rui Xue, and Ling Liu. Security and privacy on blockchain. CSUR, 52(3):1–34, 2019.\
[10] Steven Goldfeder. Private smart contracts. 2018.\
[11] Samuel S., Benjamin Bichsel, Mario Gersbach, Noa Melchior, Petar Tsankov, and Martin Vechev. zkay: Specifying and enforcing data privacy in smart contracts. In CCS, pages 1759–1776, 2019.\
[12] Karim Baghery. On the efficiency of privacy-preserving smart contract systems. In AFRICACRYPT, pages 118–136. Springer, 2019.\
[13] A. Unterweger, F. Knirsch, et al. Lessons learned from implementing a privacy-preserving smart contract in ethereum. NTMS, pages 1–5, 2018.\
[14] Fan Zhang, Ethan Cecchetti, Kyle Croman, Ari Juels, and Elaine Shi. Town crier: An authenticated data feed for smart contracts. In CCS, pages 270–282, 2016.\
[15] Erik-Oliver Blass and Florian Kerschbaum. Borealis: Building block for sealed bid auctions on blockchains. In AsiaCCS, pages 558–571, 2020.\
[16] Hisham S Galal and Amr M Youssef. Trustee: full privacy preserving vickrey auction on top of ethereum. In FC, pages 190–207. Springer, 2019.\
[17] Véronique Cortier, David Galindo, Ralf Küsters, Johannes Mueller, and Tomasz Truderung. Sok: Verifiability notions for e-voting protocols. In SP, pages 779–798. IEEE, 2016.\
[18] Geetanjali Rathee et al. On the design and implementation of a blockchain enabled e-voting application within iot-oriented smart cities. IEEE Access, 9:34165–34176, 2021.\
[19] General data protection regulation. https://gdpr-info.eu/. 2020.\
[20] Paul Voigt et al. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017.\
[21] Ahmed Kosba, Andrew Miller, Elaine Shi, Zikai Wen, and Charalampos Papamanthou. Hawk: The blockchain model of cryptography and privacy-preserving smart contracts. In SP, pages 839–858. IEEE, 2016.\
[22] Harry Kalodner et al. Arbitrum: Scalable, private smart contracts. In USENIX Security, pages 1353–1370, 2018.\
[23] B. Bünz, Jonathan Bootle, Dan Boneh, Andrew Poelstra, Pieter Wuille, and Greg Maxwell. Bulletproofs: Short proofs for confidential transactions and more. In SP, pages 315–334. IEEE, 2018.\
[24] Benedikt Bünz et al. Zether: Towards privacy in a smart contract world. In FC, pages 423–443. Springer, 2020.\
[25] Yu Chen, Xuecheng Ma, Cong Tang, and Man Ho Au. Pgc: Decentralized confidential payment system with auditability. In ESORICS, pages 591–610. Springer, 2020.\
[26] Ravital Solomon et al. smartfhe: Privacy-preserving smart contracts from fully homomorphic encryption. IACR Cryptol. ePrint Arch., 2021:133, 2021.\
[27] Guy Zyskind et al. Enigma: Decentralized computation platform with guaranteed privacy. arXiv:1506.03471, 2015.\
[28] Dayeol Lee, David Kohlbrenner, et al. Keystone: An open framework for architecting trusted execution environments. In EuroSys, pages 1– 16, 2020.\
[29] Jan-Erik Ekberg et al. Trusted execution environments on mobile devices. In CCS, pages 1497–1498, 2013.\
[30] Seongmin Kim et al. Enhancing security and privacy of tor’s ecosystem by using trusted execution environments. In NSDI, pages 145–161, 2017.\
[31] David Kaplan, Jeremy Powell, and Tom Woller. Amd memory encryption. White paper, 2016.\
[32] Ferdinand Brasser, David Gens, Patrick Jauernig, Ahmad-Reza Sadeghi, and Emmanuel Stapf. Sanctuary: Arming trustzone with userspace enclaves. In NDSS, 2019.\
[33] Frank McKeen, Ilya Alexandrovich, Alex Berenzon, Carlos V Rozas, Hisham Shafi, Vedvyas Shanbhogue, and Uday R Savagaonkar. Innovative instructions and software model for isolated execution. Hasp@ isca, 10(1), 2013.\
[34] ChongChong Zhao et al. On the performance of intel sgx. In WISA, pages 184–187. IEEE, 2016.\
[35] Jinhua Cui et al. Dynamic binary translation for sgx enclaves. arXiv preprint arXiv:2103.15289, 2021.\
[36] Rujia Li, Qin Wang, et al. An offline delegatable cryptocurrency system. arXiv preprint arXiv:2103.12905, 2021.\
[37] Ying Yan, Changzheng Wei, et al. Confidentiality support over financial grade consortium blockchain. In SIGMOD, pages 2227–2240, 2020.\
[38] Rohit Sinha et al. Luciditee: A tee-blockchain system for policycompliant multiparty computation with fairness.\
[39] Chinese chang’an chain enterprise blockchain joins digital yuan project, Mar 2021.\
[40] Financials. Changan chain, the first independent and controllable blockchain technology system in china, was released today.\
[41] Yong Wang et al. Hybridchain: A novel architecture for confidentialitypreserving and performant permissioned blockchain using trusted execution environment. IEEE Access, 8:190652–190662, 2020.\
[42] Raymond Cheng, Fan Zhang, Jernej Kos, Warren He, Nicholas Hynes, Noah Johnson, Ari Juels, Andrew Miller, and Dawn Song. Ekiden: A platform for confidentiality-preserving, trustworthy, and performant smart contracts. In EuroSP, pages 185–200. IEEE, 2019.\
[43] Poulami Das et al. Fastkitten: Practical smart contracts on bitcoin. In USENIX Security, pages 801–818, 2019.\
[44] Christina Müller, Marcus Brandenburger, et al. Tz4fabric: Executing smart contracts with arm trustzone. arXiv preprint arXiv:2008.11601, 2020.\
[45] Mark Russinovich et al. Ccf: A framework for building confidential verifiable replicated services. Technical Report MSR-TR-2019-16, Microsoft, April 2019.\
[46] Mic Bowman et al. Private data objects: an overview. arXiv preprint arXiv:1807.05686, 2018.\
[47] Adam Young and Moti Yung. The dark side of “black-box” cryptography or: Should we trust capstone? In CRYPTO, pages 89–103. Springer, 1996.\
[48] Rujia Li, David Galindo, and Qi Wang. Auditable credential anonymity revocation based on privacy-preserving smart contracts. In CBT, pages 355–371. Springer, 2019.\
[49] Rujia Li, Qin Wang, et al. An accountable decryption system based on privacy-preserving smart contracts. In ISC, pages 372–390. Springer, 2020.\
[50] Oasis lab. https:// github.com/ oasislabs/ secret-ballot/ blob/master/ contracts/ SecretBallot.sol.\
[51] Véronique Cortier et al. Election verifiability for helios under weaker trust assumptions. In ESORICS, pages 327–344. Springer, 2014.\
[52] Nik Unger, Sergej Dechand, Joseph Bonneau, Sascha Fahl, H. Perl, I. Goldberg, and M. Smith. Sok: Secure messaging. SP, pages 232– 249, 2015.\
[53] Elli Androulaki, Ghassan O Karame, Marc Roeschlin, Tobias Scherer, and Srdjan Capkun. Evaluating user privacy in bitcoin. In FC, pages 34–51. Springer, 2013.\
[54] Sarah Meiklejohn, Marjori Pomarole, Grant Jordan, et al. A fistful of bitcoins: characterizing payments among men with no names. In IMC, pages 127–140, 2013.\
[55] Ferdinand Brasser et al. Software grand exposure:{SGX} cache attacks are practical. In WOOT, 2017.\
[56] Yuanzhong Xu et al. Controlled-channel attacks: Deterministic side channels for untrusted operating systems. In SP, pages 640–656. IEEE, 2015.\
[57] Mark D Hill et al. On the spectre and meltdown processor security vulnerabilities. IEEE Micro, 39(2):9–19, 2019.\
[58] Cynthia Dwork. Differential privacy: A survey of results. In International conference on theory and applications of models of computation, pages 1–19. Springer, 2008.\
[59] Ivan Homoliak and Pawel Szalachowski. Aquareum: A centralized ledger enhanced with blockchain and trusted computing. arXiv preprint arXiv:2005.13339, 2020. [60] Marcus Brandenburger et al. Blockchain and trusted computing: Problems, pitfalls, and a solution for hyperledger fabric. arXiv preprint arXiv:1805.08541, 2018.\
[61] Enigma – securing the decentralized web. https://www.enigma.co/.\
[62] Juan Garay et al. The bitcoin backbone protocol: Analysis and applications. In EUROCRYPT, pages 281–310. Springer, 2015.\
[63] Juan Garay et al. The bitcoin backbone protocol with chains of variable difficulty. In CRYPTO, pages 291–323. Springer, 2017.\
[64] Rafael Pass, Lior Seeman, and Abhi Shelat. Analysis of the blockchain protocol in asynchronous networks. In EUROCRYPT, pages 643–673. Springer, 2017.\
[65] Juan Garay and Aggelos Kiayias. Sok: A consensus taxonomy in the blockchain era. In RSA, pages 284–318. Springer, 2020.\
[66] Intel. Intel software guard extensions (intel sgx). Accessible on https:// software.intel.com/content/www/ us/en/ develop/topics/ software-guard-extensions.html, 2020.\
[67] Robert Krahn, Donald Dragoti, Franz Gregor, et al. Teemon: A continuous performance monitoring framework for tees. In Middleware, pages 178–192, 2020.\
[68] Rui Yuan et al. Shadoweth: Private smart contract on public blockchain. JCST, 33(3):542–556, 2018.\
[69] Yin Hang, Zhou Shunfan, and Jiang Jun. Phala network: A confidential smart contract network based on polkadot. https://files.phala.network/phala-paper.pdf, 2019.\
[70] Taxa. Taxa network: a universal logic layer for blockchain. Website, 2021. https://taxa.network/.\
[71] Enigma. The developer quickstart guide to enigma | by enigma project | enigma. https:// blog.enigma.co/ the-developer-quickstart-guide-to-enigma-880c3fc4308.\
[72] Hyperledger. Introducing hyperledger avalon. www.hyperledger.org/ blog/2019/10/03/introducing-hyperledger-avalon, 2019. (Accessed on 04/19/2021).\
[73] Andreas Erwig, S. Faust, et al. Commitee: An efficient and secure commit-chain protocol using tees. IACR Cryptol. ePrint Arch., 2020:1486, 2020.\
[74] Yang Xiao et al. Privacyguard: Enforcing private data usage control with blockchain and attested off-chain contract execution. In ESORICS, pages 610–629. Springer, 2020.\
[75] Perun Network. Introducing erdstall: Scaling ethereum using trusted execution environments | by perun network | perunnetwork | medium.\
[76] Erdstall. Technology – erdstall. https://erdstall.dev/technology/. (Accessed on 04/17/2021).\
[77] Wentao Liu. Research on dos attack and detection programming. In Third International Symposium on Intelligent Information Technology Application, volume 1, pages 207–210. IEEE, 2009.\
[78] Roberto De Prisco et al. Revisiting the paxos algorithm. Theoretical Computer Science, 243(1-2):35–91, 2000.\
[79] Peter Gaži, Aggelos Kiayias, and Dionysis Zindros. Proof-of-stake sidechains. In SP, pages 139–156. IEEE, 2019.\
[80] Victor Costan and Srinivas Devadas. Intel sgx explained. IACR Cryptol. ePrint Arch., 2016(86):1–118, 2016.\
[81] Nico W., Pierre-Louis Aublin, and Rüdiger Kapitza. sgx-perf: A performance analysis tool for intel sgx enclaves. In Middleware, pages 201–213, 2018.\
[82] R. Pries et al. A new replay attack against anonymous communication networks. ICC, pages 1578–1582, 2008.\
[83] Marcus Brandenburger, Christian Cachin, Rüdiger Kapitza, and Alessandro Sorniotti. Trusted computing meets blockchain: Rollback attacks and a solution for hyperledger fabric. In SRDS, pages 324– 32409. IEEE, 2019.\
[84] Shenbin Zhang et al. A solution for the risk of non-deterministic transactions in hyperledger fabric. In ICBC, pages 253–261. IEEE, 2019.\
[85] Rosario Gennaro, Stanisław Jarecki, Hugo Krawczyk, and Tal Rabin. Secure distributed key generation for discrete-log based cryptosystems. In EUROCRYPT, pages 295–310. Springer, 1999.\
[86] Adi Shamir. How to share a secret. Communications of the ACM, 22(11):612–613, 1979.\
[87] Shari Pfleeger and Robert Cunningham. Why measuring security is hard. IEEE SP, 8(4):46–54, 2010.\
[88] Intel. Introduction to intel® sgx sealing. Website, 2016. https://software.intel.com/content/www/us/en/develop/blogs/ introduction-to-intel-sgx-sealing.html.\
[89] Sandro Pinto and Nuno Santos. Demystifying arm trustzone: A comprehensive survey. CSUR, 51(6):1–36, 2019.\
[90] Scott Johnson et al. Titan: enabling a transparent silicon root of trust for cloud. In Hot Chips: A Symposium on High Performance Chips, volume 194, 2018.\
[91] Cynthia Dwork. Microsoft azure. 2021.\
[92] Jo Van Bulck et al. Foreshadow: Extracting the keys to the intel sgx kingdom with transient out-of-order execution. In USENIX Security, pages 991–1008, 2018.\
[93] Kit Murdock, David Oswald, Flavio D Garcia, et al. Plundervolt: Software-based fault injection attacks against intel sgx. In SP, pages 1466–1482. IEEE, 2020.\
[94] Zitai Chen et al. Voltpillager: Hardware-based fault injection attacks against intel sgx enclaves using the svid voltage scaling interface. In USENIX Security, 2021.\
[95] Jo Van Bulck, David Oswald, et al. A tale of two worlds: Assessing the vulnerability of enclave shielding runtimes. In CCS, pages 1741–1758, 2019.\
[96] Eli Biham and Adi Shamir. Differential fault analysis of secret key cryptosystems. In CRYPOTO, pages 513–525. Springer, 1997.\
[97] Tu Dinh Ngoc, Bao Bui, et al. Everything you should know about intel sgx performance on virtualized systems. POMACS, 3(1):1–21, 2019.\
[98] Arvind Narayanan, Joseph Bonneau, Edward Felten, Andrew Miller, and Steven Goldfeder. Bitcoin and cryptocurrency technologies: a comprehensive introduction. Princeton University Press, 2016.\
[99] Tsz Hon Yuen, Shi-feng Sun, et al. Ringct 3.0 for blockchain confidential transaction: Shorter size and stronger security. In FC, pages 464–483. Springer, 2020.\
[100] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system. Technical report, Manubot, 2008.\
[101] Ying Lan et al. Trustcross: Enabling confidential interoperability across blockchains using trusted hardware. arXiv preprint arXiv:2103.13809, 2021.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Bezos-Backed Methane Tracking Satellite Is Lost In Space</title><link>https://tech.slashdot.org/story/25/07/01/2211218/bezos-backed-methane-tracking-satellite-is-lost-in-space?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[MethaneSAT, an $88 million satellite backed by Jeff Bezos and led by the Environmental Defense Fund to track global methane emissions, has been lost in space after going off course and losing power over Norway. "We're seeing this as a setback, not a failure," Amy Middleton, senior vice president at EDF, told Reuters. "We've made so much progress and so much has been learned that if we hadn't taken this risk, we wouldn't have any of these learnings." Reuters reports: The launch of MethaneSAT in March 2024 was a milestone in a years-long campaign by EDF to hold accountable the more than 120 countries that in 2021 pledged to curb their methane emissions. It also sought to help enforce a further promise from 50 oil and gas companies made at the Dubai COP28 climate summit in December 2023 to eliminate methane and routine gas flaring. [...] While MethaneSAT was not the only project to publish satellite data on methane emissions, its backers said it provided more detail on emissions sources and it partnered with Google to create a publicly-available global map of emissions.
 
EDF reported the lost satellite to federal agencies including the National Oceanic and Atmospheric Administration, Federal Communications Commission and the U.S. Space Force on Tuesday, it said. Building and launching the satellite cost $88 million, according to the EDF. The organization had received a $100 million grant from the Bezos Earth Fund in 2020 and got other major financial support from Arnold Ventures, the Robertson Foundation and the TED Audacious Project and EDF donors. The project was also partnered with the New Zealand Space Agency. EDF said it had insurance to cover the loss and its engineers were investigating what had happened.
 
The organization said it would continue to use its resources, including aircraft with methane-detecting spectrometers, to look for methane leaks. It also said it was too early to say whether it would seek to launch another satellite but believed MethaneSAT proved that a highly sensitive instrument "could see total methane emissions, even at low levels, over wide areas."]]></content:encoded></item><item><title>The TechBeat: The GTM Singularity: Why Sales Will Never be the Same Again (7/2/2025)</title><link>https://hackernoon.com/7-2-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Wed, 2 Jul 2025 06:10:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @Ted-Chalouhi [ 3 Min read ] 
 Discover the FLIP GTM Singularity Framework, a bold new model for replacing traditional sales with AI-driven growth systems.  Read More.By @Ted-Chalouhi [ 3 Min read ] 
 The GEAR Framework: Turning Revenue into a Precision Machine by Ted Chalouhi Read More.By @hacker-aeozms7 [ 3 Min read ] 
 The arrival of truly intelligent, always-on, AI-native revenue engines is dismantling the way we’ve structured go-to-market motions for 20 years.  Read More.By @confluent [ 2 Min read ] 
 Learn how PyIceberg simplifies working with Apache Iceberg using Python—no JVM clusters needed. Ideal for small to mid-sized data lakehouses. Read More.By @AlinaShcherbyna [ 6 Min read ] 
 Why data alone misleads—and how emotion, feedback, and AI create better brand decisions. Read More.By @val314159 [ 28 Min read ] 
 let's explore manually sending the JSON over the wire for the MXP protocol Read More.By @mykolaoliiarnyk [ 4 Min read ] 
 Discover how Ukrainian tech talent is driving global innovation through success stories like GitLab, Revolut, Grammarly, Wise, and Moss.  Read More.By @kuwguap [ 4 Min read ] 
 The story of how the AI pentesting assistant, RAWPA, evolved from a static toolkit into a dynamic, learning system.  Read More.By @paoloap [ 6 Min read ] 
 Learn how to take AI agents from prototype to production with this 5-step roadmap covering Python, RAG, architecture, testing, and real-world monitoring. Read More.By @socialdiscoverygroup [ 6 Min read ] 
 Explore how an online dating platform scaled AI moderation with ChatGPT, custom prompt engineering, and in-house data labeling to cut review time 60x. Read More.By @permit [ 11 Min read ] 
 Learn how to implement scalable multi-tenant authorization using RBAC and Permit.io, with support for fine-grained and relationship-based permissions. Read More.By @brightdata [ 10 Min read ] 
 Let’s uncover what the Playwright MCP server brings to the table, and how to use it with the OpenAI Agents SDK. Read More.By @markpelf [ 6 Min read ] 
 GitHub Copilot Agent, as of June 2025, looks much more capable than it did 2 months ago Read More.By @ishanpandey [ 5 Min read ] 
 World, formerly Worldcoin, launches Priority Blockspace for Humans on its new blockchain. Discover how it prioritizes human users over bots. Read More.By @terezabizkova [ 9 Min read ] 
 Moonwell founder Luke Youngblood on why crypto UX needs empathy, not ego—from passkeys to Paymasters, Beam, and better onboarding for the next billion. Read More.By @nkmarvel [ 3 Min read ] 
 Learn from a real-world cloud security mishap and discover the four key lessons every developer and DevOps team must know. Read More.By @drone [ 4 Min read ] 
 Why values-led innovation is shaping the next phase of blockchain development Read More.By @therealsjr [ 5 Min read ] 
 In a recent candid conversation, Jeff Mahony, a seasoned investor with a distinctly contrarian philosophy, dismantled the Silicon Valley norms. Read More.]]></content:encoded></item><item><title>Build an AI Agent That Out-Researches Your Competitors</title><link>https://hackernoon.com/build-an-ai-agent-that-out-researches-your-competitors?source=rss</link><author>aifa</author><category>tech</category><pubDate>Wed, 2 Jul 2025 06:06:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[⚠️ A Wake-Up Call for DevelopersMany programmers will lose their jobs to AI in the coming years, but not those who learn to build it. Your mission isn't just to learn how to use ChatGPT or Claude — it's to become the creator of such systems, to build the next Perplexity rather than just use it.1. What You'll Master by Reading This ArticleThis article provides a complete architectural blueprint for building your own deep research AI agent, similar to Perplexity's "Deep Research" feature. You'll learn:: How to design a recursive search system using Next.js 15, OpenAI, and exa.ai that actually works in production: Why deep search is a tree structure, not a linear process — and how this changes everything about AI research: How to integrate external web search with internal vector knowledge bases to create truly unique content that your competitors can't replicate: How to manage server resources and user experience during long-running AI operations without breaking the bank: Concrete TypeScript implementations using modern tech stack that you can deploy todayBy the end of this article, you'll have a clear understanding of how to build a self-hosted SaaS for deep research that can be integrated into any product — giving you a competitive edge that's genuinely hard to replicate.1.1. The Technology Stack That Powers IntelligenceFor implementing our deep research AI agent, we use a modern tech stack optimized for production-ready applications with intensive AI usage. This isn't just a tech demo — it's built for real-world scale and reliability.1.1.1. Frontend and Framework — The latest version with improved performance and new concurrent rendering capabilities that handle complex AI interactions smoothly — Full-featured React framework with App Router, including support for parallel and intercepting routes (perfect for complex AI workflows) — Strict typing for code reliability and superior developer experience when building complex AI systems1.1.2. AI and Integrations — Official SDK for integration with GPT-4 and other OpenAI models, with full streaming support — Universal library for working with various AI providers, giving you flexibility to switch models — Specialized AI-oriented search engine for semantic search that actually understands context — Utility-first CSS framework for rapid development without sacrificing design quality — Headless components for creating accessible interfaces that work across all devices — Modern icon library with consistent design language — Component system built on Radix UI and Tailwind CSS for professional-grade interfaces — High-performance library for form handling that doesn't slow down your AI interfaces — TypeScript-first validation schema with static typing that catches errors before they reach production — Seamless integration between Zod and React Hook Form1.1.5. Content Processing — Markdown content rendering with component support for rich AI-generated reports — Modern library for date handling in AI research timelinesThis technology stack provides the , , and  necessary for creating complex production-level AI applications. Every choice here is intentional — from Next.js 15's parallel routes handling complex AI workflows, to Exa.js providing the semantic search capabilities that make deep research possible. A system that can handle the computational complexity of recursive AI research while maintaining the user experience standards that modern applications demand.Ready to see how these pieces fit together to create something truly powerful? Let's dive into the architecture that makes it all work.The AI revolution isn't coming—it's here. And it's creating a stark divide in the developer community. On one side are those who see AI as just another tool to boost productivity, using ChatGPT to write functions and debug code. On the other side are developers who understand a fundamental truth: the real opportunity isn't in using AI—it's in building it.While most developers are learning to prompt ChatGPT more effectively, a smaller group is mastering the architecture behind systems like Perplexity, Claude, and custom AI agents. This isn't just about staying relevant; it's about positioning yourself on the right side of the most significant technological shift since the internet.: Companies don't need developers who can use AI tools—they need developers who can build AI systems. The difference between these two skills will determine who thrives and who becomes obsolete in the next economic cycle.This article provides a complete architectural blueprint for building your own AI-powered deep research agent, similar to Perplexity's "Deep Research" feature. You'll learn not just the technical implementation, but the mental models and design principles that separate amateur AI integrations from production-ready systems that can become competitive advantages.What you'll master by the end:Recursive search architecture: How to design systems that think in trees, not lines: Integrating external web search with internal knowledge bases: Building AI systems that can evaluate, iterate, and improve their own outputsProduction considerations: Managing server resources, timeouts, and user experience for long-running AI operationsThe goal isn't to give you code to copy-paste. It's to transfer the architectural thinking that will let you design AI systems for any domain, any use case, and any scale. By the end, you'll have the mental framework to build AI agents that don't just answer questions—they conduct research like expert analysts.Ready to move from AI consumer to AI architect? Let's dive into why traditional LLMs need a "guide dog" to navigate the internet effectively.2. Introduction: Life After ChatGPT Changed EverythingWe're living through one of the most transformative periods in tech history. ChatGPT and other Large Language Models (LLMs) have fundamentally revolutionized how we interact with information. But if you've been building serious applications with these tools, you've probably hit the same wall I did: models only know the world up to their training cutoff date, and they hallucinate with alarming confidence.2.1. The Problem: Band-Aid Web Search in Modern LLMsThe teams behind ChatGPT, Claude, and other models tried to solve this with built-in web search. It sounds great in theory, but dig deeper and you'll find some serious architectural flaws that make it unsuitable for production applications:: The system makes one or two search queries, grabs the first few results, and calls it a day. This isn't research — it's glorified Google with a chat interface.: There's no recursive deepening into topics. If the first search doesn't yield comprehensive results, the system doesn't ask follow-up questions or explore alternative angles.: Traditional search engines return HTML pages cluttered with ads, navigation elements, and irrelevant content. The LLM has to "dig out" useful information from this digital junk.: The system can't connect found information with your internal data, company documents, or domain-specific knowledge bases.2.2. The Gold Standard: Perplexity's Deep Research RevolutionPerplexity was the first company to show us how search-LLM integration should actually work. Their approach is fundamentally different from these band-aid solutions:AI-Optimized Search Engines: Instead of generic Google API calls, they use specialized search systems that return clean, structured content designed for AI consumption.Iterative Investigation Process: The system doesn't stop at initial results. It analyzes findings, formulates new questions, and continues searching until it builds a comprehensive picture.: This is an autonomous AI agent that can work for minutes at a time, recursively drilling down into topics and gathering information from dozens of sources.This is exactly the kind of system we're going to build together.2.3. Why This Matters for Every DeveloperIn the AI-first era, every product is racing to become "smarter." But simply plugging in the ChatGPT API is just table stakes now. Real competitive advantage comes when your AI can:Research current information from the internet in real-timeCombine public data with your proprietary knowledge basesGenerate unique insights impossible to get from standard LLMsAdapt to your specific business domain and industry nuances2.4. What You'll Walk Away WithMy goal isn't to give you code to copy-paste (though you'll get plenty of that). I want to transfer the mental models and architectural principles that will enable you to:Understand the philosophy behind deep AI research for your specific use case using modern stack (Next.js 15, OpenAI, exa.ai) into any existing product the system for your needsBy the end of this article, you'll have a complete architectural blueprint and production-ready code examples for building your own "Perplexity" — an AI agent that could become your product's secret weapon.: We'll study not just the technical implementation, but the business logic too. Why is recursive search more effective than linear? How do you properly combine external and internal sources? What UX patterns work for long-running AI operations? These questions are just as critical as the code.2.5. For the Impatient: Skip to the CodeI personally can't stand articles that give you lots of words and little substance. Feel free to clone the repo and get it running in development mode right now.: You'll hit timeout limitations (403 errors) on Vercel's free hosting tier in production, but on localhost you can fully experiment and study the logs to your heart's content.Ready to build the future of AI-powered research? Let's start by understanding why LLMs need a "guide dog" to navigate the internet effectively.3. Why LLMs Need a "Guide Dog": The Critical Role of External Search SystemsHere's a hard truth that many developers learn the expensive way: Large Language Models cannot independently access current information from the internet. This isn't a bug — it's a fundamental architectural limitation that requires a sophisticated solution: integration with specialized search systems designed for AI consumption.3.1. Why Traditional Search Engines Are AI PoisonGoogle, Bing, and other traditional search engines were built for humans browsing the web, not for machines processing data. When you hit their APIs, you get back HTML pages stuffed with:Ad blocks and navigation clutter that confuse content extraction (comments, sidebars, footers, cookie banners) that requires complex parsing and often failsjavascript// The traditional approach - a nightmare for AI
const htmlResponse = await fetch('https://api.bing.com/search?q=query');
const messyHtml = await htmlResponse.text();
// You get HTML soup with ads, scripts, and digital garbage
// Good luck extracting meaningful insights from this mess
I've seen teams spend weeks building HTML parsers, only to have them break every time a major site updates their layout. It's not scalable, and it's definitely not reliable.3.2. Keyword Matching vs. Semantic Understanding: A World of DifferenceTraditional search systems look for exact word matches, completely ignoring context and meaning. A query like "Next.js optimization for e-commerce" might miss an excellent article about "boosting React application performance in online stores," even though they're semantically identical topics.This is like having a research assistant who can only find books by matching the exact words in the title, while ignoring everything about the actual content. For AI agents doing deep research, this approach is fundamentally broken.3.3. AI-Native Search Engines: The Game ChangerSpecialized systems like Exa.ai, Metaphor, and Tavily solve the core problems that make traditional search unusable for AI:Semantic Query Understanding They use vector representations to search by meaning, not just keywords. Your AI can find relevant content even when the exact terms don't match.They return pre-processed content without HTML garbage. No more parsing nightmares or broken extractors.They understand previous queries and the overall research context, enabling truly iterative investigation.javascript// The AI-native approach - clean and powerful
const cleanResults = await exa.search({
  query: "Detailed analysis of Next.js performance optimization for high-traffic e-commerce platforms",
  type: "neural",
  contents: { text: true, summary: true }
});
// You get clean, relevant content ready for AI processing
// No parsing, no cleanup, no headaches
3.4. Why This Matters for Production SystemsThe quality of your input data directly determines the quality of your final research output. AI-native search engines provide:: Structured content without the need for fragile HTML parsing \n : Stable APIs designed for automated, high-volume usage \n : Reduced computational overhead for data processing \n : Better source relevance leads to better final insightsjavascript// Hybrid search: external + internal sources
const [webResults, vectorResults] = await Promise.all([
  exa.search(query),
  vectorStore.similaritySearch(query)
]);

const combinedContext = [...webResults, ...vectorResults];
// Now your AI has both current web data AND your proprietary knowledge
3.5. The Bottom Line: Architecture MattersAI-native search engines aren't just a technical detail — they're the architectural foundation that makes quality AI research agents possible. Without the right "guide dog," even the most sophisticated LLM will struggle to create deep, accurate analysis of current information.Think of it this way: you wouldn't send a brilliant researcher into a library where all the books are written in code and half the pages are advertisements. Yet that's exactly what happens when you connect an LLM to traditional search APIs. Give your AI the right tools for the job. In the next section, we'll dive into the specific architecture patterns that make recursive, deep research possible.Ready to see how the pieces fit together? Let's explore the system design that powers truly intelligent AI research agents.4. Think Like a Tree: The Architecture of Recursive SearchThe human brain naturally structures complex information as hierarchical networks. When a researcher investigates a new topic, they don't move in a straight line — they develop a tree-like knowledge network where each new discovery generates additional questions and research directions. This is exactly the mental model we need to implement in our deep search AI agent architecture.4.1. The Fundamental Difference in ApproachesTraditional search systems and built-in web search in LLMs work linearly: receive query → perform search → return results → generate answer. This approach has critical limitations for deep research.Problems with the Linear Approach:: The system stops at the first facts it finds: Each search query is isolated from previous ones: The system can't see relationships between different aspects of a topic: Results depend entirely on the luck of the initial queryThe tree-based approach solves these problems by modeling the natural process of human investigation. Each discovered source can generate new questions, which become separate research branches.4.2. Anatomy of a Search TreeLet's examine the structure of a deep search tree with a concrete example:textNext.js vs WordPress for AI Projects/
├── Performance/
│   ├── Source 1
│   ├── Source 2
│   └── Impact of AI Libraries on Next.js Performance/
│       └── Source 7
├── Development Costs/
│   ├── Source 3
│   └── Source 4
└── SEO and Indexing/
    ├── Source 5
    └── Source 6
 are the main aspects of the topic that the LLM generates based on analysis of the original query. In our example, these are performance, costs, and SEO. These subtopics aren't formed randomly — the LLM analyzes the semantic space of the query and identifies key research directions. are specific sources (articles, documents, studies) found for each sub-query. Each leaf contains factual information that will be included in the final report. are the most powerful feature of this architecture. When the system analyzes found sources, it can discover new aspects of the topic that require additional investigation. These aspects become new sub-queries, generating their own branches.4.3. Practical Advantages of Tree Architecture: The tree ensures systematic topic coverage. Instead of a random collection of facts, the system builds a logically connected knowledge map where each element has its place in the overall structure.: The system automatically determines which directions require deeper investigation. If one branch yields many relevant sources, the system can go deeper. If a direction proves uninformative, the search terminates early.: Each new search query is formed considering already found information. This allows for more precise and specific questions than isolated searches.: At each tree level, the system can evaluate the relevance and quality of found sources, filtering noise and concentrating on the most valuable information.4.4. Managing Tree Parameters determines how many recursion levels the system can perform. Depth 1 means only main sub-queries without further drilling down. Depth 3-4 allows for truly detailed investigation. controls the number of sub-queries at each level. Too much width can lead to superficial investigation of many directions. Optimal width is usually 3-5 main directions per level. is the average number of child nodes for each tree node. In the context of information search, this corresponds to the number of new sub-queries generated based on each found source.4.5. Optimization and Problem Prevention: The system must track already investigated directions to avoid infinite recursion loops.: More promising branches should be investigated with greater depth, while less informative directions can be terminated earlier.: Different tree branches can be investigated in parallel, significantly speeding up the process when sufficient computational resources are available.: Search results should be cached to avoid repeated requests to external APIs when topics overlap.Execution Time and Server Timeouts: This is another problem that often manifests when implementing deep research, especially if depth exceeds two levels. You could say that increasing the level exponentially increases research complexity. For example, research with four levels of depth can take up to 12 hours.4.6. The Bottom Line: From Chaos to SystemTree architecture transforms the chaotic process of information search into systematic investigation, where each element has its place in the overall knowledge structure. This allows the AI agent to work like an experienced researcher — not just collecting facts, but building a comprehensive understanding of the investigated topic. An AI system that thinks like a human researcher, but operates at machine scale and speed. In the next section, we'll dive into the technical implementation that makes this architectural vision a reality.Ready to see how we translate this conceptual framework into production code? Let's explore the technical stack that powers recursive intelligence.5. The "Search-Evaluate-Deepen" Cycle: Implementing True RecursionRecursive internet parsing isn't just a technical feature — it's a fundamental necessity for creating truly intelligent AI agents. The first page of any search results shows only the tip of the information iceberg. Real insights lie deeper, in related articles, cited sources, and specialized research that most systems never reach.5.1. Data Architecture for Deep InvestigationIn production implementations, the system operates with structured data types that accumulate knowledge at each recursion level:typescripttype Learning = {
  learning: string;
  followUpQuestions: string[];
};

type SearchResult = {
  title: string;
  url: string;
  content: string;
  publishedDate: string;
};

type Research = {
  query: string | undefined;
  queries: string[];
  searchResults: SearchResult[];
  knowledgeBaseResults: string[]; // Vector database responses
  learnings: Learning[];
  completedQueries: string[];
};
This data structure accumulates knowledge at each recursion level, creating unified context for the entire investigation — exactly what separates professional research from random fact-gathering.5.2. Stage 1: "Search" — Intelligent Query GenerationThe system doesn't rely on a single search query. Instead, it generates multiple targeted queries using LLM intelligence:typescriptconst generateSearchQueries = async (query: string, breadth: number) => {
  const {
    object: { queries },
  } = await generateObject({
    model: mainModel,
    prompt: `Generate ${breadth} search queries for the following query: ${query}`,
    schema: z.object({
      queries: z.array(z.string()).min(1).max(10),
    }),
  });
  return queries;
};
: The  parameter controls research width — the number of different topic aspects that will be investigated in parallel. This is where the magic happens: instead of linear search, you get exponential coverage.5.3. Stage 2: "Evaluate" — AI-Driven Result FilteringNot all found sources are equally valuable. The system uses an AI agent with tools for intelligent evaluation of each result:typescriptconst searchAndProcess = async (/* parameters */) => {
  const pendingSearchResults: SearchResult[] = [];
  const finalSearchResults: SearchResult[] = [];

  await generateText({
    model: mainModel,
    prompt: `Search the web for information about ${query}, For each item, where possible, collect detailed examples of use cases (news stories) with a detailed description.`,
    system: "You are a researcher. For each query, search the web and then evaluate if the results are relevant",
    maxSteps: 10,
    tools: {
      searchWeb: tool({
        description: "Search the web for information about a given query",
        parameters: z.object({ query: z.string().min(1) }),
        async execute({ query }) {
          const results = await searchWeb(query, breadth, /* other params */);
          pendingSearchResults.push(...results);
          return results;
        },
      }),
      evaluate: tool({
        description: "Evaluate the search results",
        parameters: z.object({}),
        async execute() {
          const pendingResult = pendingSearchResults.pop();
          if (!pendingResult) return "No search results available for evaluation.";

          const { object: evaluation } = await generateObject({
            model: mainModel,
            prompt: `Evaluate whether the search results are relevant and will help answer the following query: ${query}. If the page already exists in the existing results, mark it as irrelevant.`,
            output: "enum",
            enum: ["relevant", "irrelevant"],
          });

          if (evaluation === "relevant") {
            finalSearchResults.push(pendingResult);
          }
          return evaluation === "irrelevant" 
            ? "Search results are irrelevant. Please search again with a more specific query."
            : "Search results are relevant. End research for this query.";
        },
      }),
    },
  });
  return finalSearchResults;
};
: The system uses an AI agent with tools that can repeatedly search and evaluate results until it finds sufficient relevant information. This is like having a research assistant who doesn't give up after the first Google search.5.4. Vector Knowledge Base IntegrationThe real power emerges from synergy between external and internal search. For each query, the system simultaneously searches the internet and its own vector knowledge base:typescriptasync function getKnowledgeItem(query: string, vectorStoreId: string) {
  const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

  const response = await client.responses.create({
    model: "gpt-4o-mini",
    tools: [
      {
        type: "file_search",
        vector_store_ids: [vectorStoreId],
        max_num_results: 5,
      },
    ],
    input: [
      {
        role: "developer",
        content: `Search the vector store for information. Output format language: ${process.env.NEXT_PUBLIC_APP_HTTP_LANG || "en"}`,
      },
      {
        role: "user",
        content: query,
      },
    ],
  });
  return response.output_text;
}
5.5. Practical ImplementationIn the main research loop, the system queries both sources in parallel:typescriptfor (const query of queries) {
  const searchResults = await searchAndProcess(/* web search */);
  accumulatedResearch.searchResults.push(...searchResults);

  if (vectorStoreId && vectorStoreId !== "") {
    const kbResult = await getKnowledgeItem(query, vectorStoreId);
    accumulatedResearch.knowledgeBaseResults.push(kbResult);
  }
}
5.6. Stage 3: "Deepen" — Generating Follow-Up QuestionsThe most powerful feature: the system's ability to generate new research directions based on already found information:typescriptconst generateLearnings = async (query: string, searchResult: SearchResult) => {
  const { object } = await generateObject({
    model: mainModel,
    prompt: `The user is researching "${query}". The following search result were deemed relevant.
      Generate a learning and a follow-up question from the following search result:

      <search_result>
      ${JSON.stringify(searchResult)}
      </search_result>`,
    schema: z.object({
      learning: z.string(),
      followUpQuestions: z.array(z.string()),
    }),
  });
  return object;
};
Each found source is analyzed to extract new questions that become the foundation for the next search level:typescriptfor (const searchResult of searchResults) {
  const learnings = await generateLearnings(query, searchResult);
  accumulatedResearch.learnings.push(learnings);
  accumulatedResearch.completedQueries.push(query);

  const newQuery = `Overall research goal: ${prompt}
Previous search queries: ${accumulatedResearch.completedQueries.join(", ")}
Follow-up questions: ${learnings.followUpQuestions.join(", ")}`;

  await deepResearch(
    /* search parameters */,
    newQuery,
    depth - 1,
    Math.ceil(breadth / 2), // Reduce width at each level
    vectorOfThought,
    accumulatedResearch,
    vectorStoreId
  );
}
The production implementation shows how to manage exponential complexity growth:typescriptconst deepResearch = async (
  /* multiple filtering parameters */,
  prompt: string,
  depth: number = 2,
  breadth: number = 5,
  vectorOfThought: string[] = [],
  accumulatedResearch: Research = {
    query: undefined,
    queries: [],
    searchResults: [],
    knowledgeBaseResults: [],
    learnings: [],
    completedQueries: [],
  },
  vectorStoreId: string
): Promise<Research> => {
  if (depth === 0) {
    return accumulatedResearch; // Base case for recursion
  }

  // Adaptive query formation based on "thought vector"
  let updatedPrompt = "";
  if (vectorOfThought.length === 0) {
    updatedPrompt = prompt;
  } else {
    const vectorOfThoughItem = vectorOfThought[vectorOfThought.length - depth];
    updatedPrompt = `${prompt}, focus on these important branches of thought: ${vectorOfThoughItem}`;
  }
  // ... rest of implementation
};
: at each level prevents exponential growth \n : allows directing research into specific areas \n : All results are preserved in a unified data structure5.10. The Hybrid Advantage in Practice: Combining public data with internal knowledge enables creating reports that no one else can replicate. Your competitors may access the same public sources, but not your internal cases, statistics, and expertise.: External data provides currency and breadth, internal data provides depth and specificity. The system can find general industry trends online, then supplement them with your own data about how these trends affect your business.: Even if web information is outdated or inaccurate, your internal knowledge base can provide fresher and verified data.6. From Chaos to Order: Generating Expert-Level ReportsAfter completing all levels of recursive search, the system accumulates massive amounts of disparate information: web search results, vector database data, generated learnings, and follow-up questions. The final stage is transforming this chaos into a structured, expert-level report that rivals human analysis.6.1. Context Accumulation: Building the Complete PictureAll collected data is unified into a single  structure that serves as complete context for final synthesis:typescripttype Research = {
  query: string | undefined;           // Original query
  queries: string[];                   // All generated search queries
  searchResults: SearchResult[];       // Web search results
  knowledgeBaseResults: string[];      // Vector database responses
  learnings: Learning[];               // Extracted insights
  completedQueries: string[];          // History of completed queries
};
This isn't just data storage — it's a comprehensive knowledge graph that captures the entire investigation journey. Every insight, every source, every connection is preserved for the final synthesis.6.2. The Master Prompt: Where Intelligence Meets SynthesisThe quality of the final report directly depends on the sophistication of the generation prompt. The system uses OpenAI's most powerful model for synthesis:typescriptconst generateReport = async (
  research: Research,
  vectorOfThought: string[],
  systemPrompt: string
) => {
  const { text } = await generateText({
    model: openai("o3-mini"), // Most powerful model for synthesis
    system: systemPrompt,
    prompt:
      "Use the following structured research data to generate a detailed expert report:\n\n" +
      JSON.stringify(research, null, 2),
  });
  return text;
};
: We're not just asking the AI to summarize — we're providing it with a complete research dataset and asking it to think like a domain expert. The difference in output quality is dramatic.6.3. Structured Output: Beyond Simple SummariesThe system doesn't just create a text summary — it generates structured documents with headers, tables, pro/con lists, and professional formatting, as shown in the result saving:typescriptconsole.log("Research completed!");
console.log("Generating report...");
const report = await generateReport(research, vectorOfThought, systemPrompt);
console.log("Report generated! Saving to report.md");
fs.writeFileSync("report.md", report); // Save as Markdown
 It's the perfect format for AI-generated content — structured enough for professional presentation, flexible enough for various output formats, and readable in any modern development workflow.6.4. Quality Control Through System PromptsThe  allows customizing report style and structure for specific needs: for research papers and scholarly analysis for corporate reports and executive summaries for developer-focused content for financial and strategic reports// Example: Business-focused system prompt const businessSystemPrompt = `You are a senior business analyst creating an executive report. Structure your analysis with:Use data-driven insights and provide specific examples from the research.`;6.5. The Intelligence Multiplier EffectHere's what makes this approach revolutionary: The system doesn't just aggregate information — it synthesizes insights that emerge from the connections between different sources. A human researcher might spend 8-12 hours conducting this level of investigation. Our system does it in 10-60 minutes, often uncovering connections that humans miss.6.6. Production Considerations: With deep research (depth 3-4), the accumulated context can become massive. The system needs to handle large JSON structures efficiently.: The final synthesis prompt can easily exceed token limits. Production implementations need smart truncation strategies that preserve the most valuable insights.: Not all generated reports are equal. Consider implementing scoring mechanisms to evaluate report completeness and coherence.: Hours of human research → Minutes of AI analysis \n : AI can process and connect more sources than humanly possible \n : Every report follows the same rigorous methodology \n : Generate dozens of reports simultaneously7. Conclusion: Building the Future of AI ResearchCreating a deep research AI agent isn't just a technical challenge — it's an architectural solution that can become a competitive advantage for any product. We've covered the complete cycle from concept to implementation, showing how to transform the chaotic process of information search into systematic, expert-level investigation.7.1. Key Architectural PrinciplesThink in Trees, Not Lines: Deep search is about exploring tree-structured information networks, where each discovery generates new questions and research directions.: Specialized search engines like exa.ai aren't optional — they're essential for quality research. They return clean data instead of HTML garbage that traditional search APIs provide.Apply Recursion for Depth: The first page of results is just the tip of the iceberg. Real insights lie in recursive deepening through the "Search-Evaluate-Deepen" cycle.Combine External and Internal Sources: The synergy between public internet data and private organizational knowledge creates unique content that's impossible to obtain any other way.Use LLMs for Both Analysis and Synthesis: AI agents with tools can not only search for information but also evaluate its relevance, generate new questions, and create structured reports.7.2. Production-Ready ResultsRecursive architecture with depth and width managementIntegration of web search with vector knowledge basesAI agents with tools for result evaluationExpert report generation with file saving capabilities7.3. Challenges and Limitations: Research with depth greater than 2 levels can take hours, requiring special solutions for production environments.Exponential Complexity Growth: Each depth level increases the number of queries geometrically, requiring careful resource management.: Even AI search engines can return inaccurate information, requiring additional validation and fact-checking.Now you have a complete architectural blueprint and real code examples. You can:Start with Minimal Implementation: Use the basic version from this article for prototyping \n Explore the Ready Solution: Clone https://github.com/aifa-agi/aifa-deep-researcer-starterand experiment locally \n : Integrate these principles into existing products and workflows8. Homework Challenge: Solving the Long-Wait UX ProblemWe've covered the technical architecture of deep research AI agents, but there's a critically important UX problem remaining: what do you do when the system works for several minutes while the user stares at a blank screen? As the real implementation from aifa-deep-researcer-starter shows, research with depth greater than 2 levels can take hours, while users see only a static loading screen.8.1. The Problem: Server Silence Kills TrustUnlike typical web applications where operations take seconds, deep research AI agents can be silent for minutes. Users get no feedback from the server until the very end of the process. This creates several psychological problems:: Users don't know if the system is working or frozen: No way to understand how much longer to wait: It seems like the application is broken or "ate" the request: Users close the tab without waiting for resultsPerplexity, Claude, and other modern AI products solve this with interactive animations, progress indicators, and dynamic hints. But how do you implement something similar when your server doesn't send intermediate data?8.2. The Developer ChallengeImagine this technical constraint: your Next.js API route performs a long operation (deep research) and can't send intermediate data until completion. The frontend only gets a response at the very end. Classic solutions like Server-Sent Events or WebSockets might be unavailable due to hosting limitations or architecture constraints.How do you create engaging UX under these conditions that will retain users and reduce waiting anxiety?8.3. Discussion QuestionsUX Patterns and Visualization:What UX patterns would you apply to visualize deep search processes when the server is "silent"?How can you simulate "live" progress even without real server status updates?Should you use fake progress bars, or does this violate user trust?What animations and micro-interactions help create a sense of a "living" system?How do you explain to users why waiting might be long? What texts/illustrations to use?Should you show estimated wait times if they can vary greatly (2 to 60 minutes)?How do you visualize process stages ("Generating search queries…", "Analyzing sources…", "Forming expert report…")?What metaphors help users understand the value of waiting?Technical Implementation:What optimistic UI approaches can be applied without server feedback?How do you implement a "conversational" interface that supports users during waiting?Can you use local computations (Web Workers, WASM) to simulate progress?How do you organize graceful degradation if users close the tab during research?8.4. Learning from the BestStudy solutions implemented in Perplexity Deep Research, Bing Copilot, Google Search Generative Experience. What can you take from gaming loading screens that hold attention for minutes? How do Netflix, YouTube, and other platforms solve long content loading problems?: In an era of instant ChatGPT responses, quality waiting can become a competitive advantage. Users are willing to wait if they understand the process value and feel the system is working for them.9. About the Author and AIFA ProjectThe author, Roman Bolshiyanov, in his recent publication series, details the tools and architectural solutions he implements in his ambitious open-source project AIFA (AI agents in evolving and self-replicating architecture).In its current implementation, AIFA already represents an impressive starter template for creating AI-first applications with a unique user interface where artificial intelligence becomes the primary interaction method, while traditional web interface serves as auxiliary visualization.However, this is just the beginning. The project's long-term goal is evolution into a full-fledged AGI system where AI agents will possess capabilities for: and improvement of their algorithms and creation of new specialized agentsCompetition and cooperation in distributed environments in web spaces and blockchain networksThe deep search covered in this article is just one of the basic skills of future AGI agents that will be able to not only research information but also make decisions, create products, and interact with the real world.If you're interested in observing the project's development and experimenting with cutting-edge AI technologies, don't hesitate to fork the AIFA repository and dive into exploring the architecture of the future. Each commit brings us closer to creating truly autonomous artificial intelligence.Ready to build the future? The code is open, the architecture is proven, and the possibilities are limitless. Your next breakthrough in AI-powered research is just a git clone away.]]></content:encoded></item><item><title>Cloudflare Gives Website Owners Option to Charge OpenAI Bots for Scraping</title><link>https://hackernoon.com/cloudflare-gives-website-owners-option-to-charge-openai-bots-for-scraping?source=rss</link><author>Dmitry Baraishuk</author><category>tech</category><pubDate>Wed, 2 Jul 2025 06:04:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Cloudflare has launched a private beta feature called Pay per Crawl, whose sole purpose is to let a website owner charge an AI crawler a fixed fee.]]></content:encoded></item><item><title>Still.js Is Taking Vanilla JavaScript to the Enterprise</title><link>https://hackernoon.com/stilljs-is-taking-vanilla-javascript-to-the-enterprise?source=rss</link><author>Nakassony Bernardo</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:58:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When it comes to complex web application development on the frontend, we often encounter the need to implement features using vanilla JavaScript—whether it’s for direct DOM manipulation or integrating a specific jQuery plugin. Sometimes, a from-scratch implementation using plain HTML, CSS, and vanilla JavaScript is the best (or only) choice.Still.js is a modern Open Source framework that offers the same capabilities as Angular, React, and Vue—without abandoning vanilla JavaScript.Because of its pure JS approach, :Does not require preprocessingDoes not depend on bundlers like Webpack or ViteIs ideal for teams and developers who prefer direct, no-compromise access to native web technologies in addition to those modern features a Web Framework provides.Enterprise-grade web applications need more than just rich UI features. They require: , user permission management, , , , , Micro-frontend architecture (e.g. Frontend embedding and interaction), and more.Still.js supports all of this features natively without the burden of a bundler increasing build time and potential complexity or even tooling overhead.Note: Those will only work within a  project with the correct setup, which is thoroughly explained in the official documentation.Bellow is a simple component implementing a counter, inspite the template is placed inside the class, it can be moved to a .html file which is appropriate in complex apps.import { ViewComponent } from "../../../@still/component/super/ViewComponent.js";

export class CounterComponent extends ViewComponent {

    isPublic = true;
    count = 0;

    template = `
    <div>
        <p>My counter state is @count</p>
        <button (click)="increment()">Increment (@count)</button>
    </div>
    `;

    increment() {
        this.count = this.count.value + 1;
    }
}
\
Basic User authorization managementIn the bellow implementation we're stating that some components won't be accessible by the user, this can be done with proper business logic according to the user role checking.import { StillAppMixin } from "./@still/component/super/AppMixin.js";
import { Components } from "./@still/setup/components.js";
import { AppTemplate } from "./app-template.js";
import { CheckingAccount } from "./app/components/BankAccount/CheckingAccount.js";
import { NewAccountForm } from "./app/components/BankAccount/NewAccountForm.js";
import { SavingAccount } from "./app/components/BankAccount/SavingAccount.js";

export class StillAppSetup extends StillAppMixin(Components) {

    constructor() {
        super();
        //Bellow the entry point component is being set
        this.setHomeComponent(NewAccountForm);
        const blackListComponents = [SavingAccount, CheckingAccount];
        //Make components black-listed by passing it to setBlackList App configuration
        this.setBlackList(blackListComponents);
    }

    async init() {
        return await AppTemplate.newApp();
    }

}
Still.js provides built-in validators, but custom ones can be implemented effortlessly.import { ViewComponent } from "../../../@still/component/super/ViewComponent.js";

export class BasicForm extends ViewComponent {

    isPublic = true;
    firstName = '';
    shoeSize;

    template = `
    <div>
        <form>
            <div class="form-group">
                <label>Shoe Size</label>
                <input 
                    (value)="shoeSize" 
                    (validator)="number" 
                    (validator-warn)="Invalid shoe size, number is required"
                    placeholder="Enter valid shoe size"
                >
            </div>
        </form>
    </div>
    `;

}
A service serves for both Global reactive storage and data flow (e.g. HTTP requests) implementations. Services path can be set in the application level, and we can overriden for specific service with  annotation.import { ViewComponent } from "../../../@still/component/super/ViewComponent.js";
import { CustomersService } from "../../service/api/CustomersService.js";

export class BiddingDisplay extends ViewComponent {

    isPublic = true;

    /** Service declaration, will get injected automatically due to Inject anottation
     *  from the specified Path path due to the annotation
     * @Inject
     * @Path service/api/
     * @type { CustomersService } */
    custService;

    template = `<div></div>`;

    /** Component Hook which takes place when it's completly render and startder */
    stAfterInit() {

        this.custService.customerList.onChange((newValue) => { 
            console.log('Ths customerList store got updated with: ', newValue);
        });

    }

}
\
Hurry up, get yourself started with Still.js is available through NPM on @stilljs/cli. Watch the  and bear for more coming ones.]]></content:encoded></item><item><title>2025 ASIC Miner Buyer’s Guide: Specs, Costs, and What Really Pays Off</title><link>https://hackernoon.com/2025-asic-miner-buyers-guide-specs-costs-and-what-really-pays-off?source=rss</link><author></author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:53:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[So, You’re Still Thinking About Mining?Let’s be real: mining crypto in 2025 is not the get-rich-quick playground it was a decade ago. The latest Bitcoin halving squeezed margins even tighter. Old rigs like the Antminer S9? Basically museum pieces now. Easy Bitcoin is history — but if you run the numbers, plan your power costs, and choose the  ASIC miner, mining can still pay you back — smartly.Look at any legit mining hub — MinerStat, CryptoMinerBros, Oneminers — and it’s obvious: ASICs still dominate Bitcoin’s SHA-256 hashpower (over 90% of it). But here’s the real point: Hashrate alone means nothing if you’re not watching your efficiency, electricity, and ROI window.Why ASICs Still Matter in 2025“Why not just run GPUs?” Or, “Why not rent hashpower on the cloud?” Good questions — but if you’re serious about Bitcoin or any big SHA-256 coin, an ASIC wins hands-down. They’re built for one thing: max hashes with minimum wasted watts.Take Bitmain’s Antminer S21. It cranks out 200 TH/s with far less power per hash than even the best multi-GPU farm. Data from WhatToMine, MinerStat, and CryptoCompare still show the top earners in 2025 are ASICs. GPUs? Great for flexible altcoins — not for your next Bitcoin payout.3 Numbers That Decide If Your Rig Prints Money or Burns CashBefore you click  on some flashy new box, break it down:✅  Raw horsepower. Higher is better, but remember — network difficulty creeps up. \n ✅ How many Joules to squeeze out one Terahash. Top-tier rigs like the S21 run ~17.5 J/TH. Lower is better. \n ✅ Basic math — Upfront Cost ÷ Net Monthly Profit = months to break even. Solid is 12–18 months. If it’s 24+, you’re betting big on Bitcoin staying strong  your rig staying relevant.📌  Bitcointalk pros say: Avoid any ASIC with a payback longer than 20 months — unless your electricity is dirt cheap.The 2025 ASICs People Are Actually BuyingHere’s a , pulled from CryptoMinerBros specs, MinerStat ROI calculators, and real chatter in Discord and Bitcointalk. These are rigs miners trust — not just promo hype.1️⃣ Bitmain Antminer S21 (200 TH/s) Rock-solid workhorse for Bitcoin mining. High hashrate, tight efficiency, and good resale value when you upgrade down the line. Many farms moved from S19s to this. Plug-and-play, proven, easy to flip later. Small-to-mid BTC farms that need a safe bet with fair resale value.2️⃣ Bitmain Antminer S21 Hydro (250 TH/s) Liquid-cooled version of the S21. Higher hashrate, stays cooler, and quieter if you have the setup for it. Big industrial farms love it to max out racks.Power Draw: ~5300W (with cooling) Industrial setups that can handle liquid cooling.3️⃣ Whatsminer M60S (180 TH/s) Whatsminer gives Bitmain real competition. Tough build, easy maintenance, faster shipping. Miners mix these in to dodge Bitmain stock delays. Farms wanting rugged hardware with fewer supply headaches.4️⃣ Canaan Avalon A1566 (185 TH/s) Avalon rigs are all about simple, plug-and-go. Solid durability, strong after-sales support, and easy to run. r/cryptomining folks swear by these for no-fuss farms. Miners who hate tinkering and want dependable output.5️⃣ Bitmain Antminer KS5 Pro (21 TH/s) Purpose-built for Kaspa — an altcoin that’s popped up as a smart hedge against Bitcoin swings. MinerStat shows good profitability for Kaspa this year.Hashrate: 21 TH/s (Kaspa) Miners adding Kaspa or other PoW coins to balance risk.Don’t Forget: New Models Drop FastBitmain, Whatsminer, Goldshell — they push new units every few months. Pre-order? Maybe — but always double-check Discord or Bitcointalk to see if the reseller actually delivers on time. Missed delivery = ROI ruined.You can have the best rig in the world — but if your electricity is pricey, you’re toast. Here’s how serious miners keep it real:✔️ Know your kWh rate — check your bill or ask your provider. \n ✔️ Use live ROI tools — WhatToMine and MinerStat are must-haves. \n ✔️ Pool fees are real — 1–2% adds up. \n ✔️ 95% uptime is realistic — no rig runs perfectly 24/7. \n ✔️ Aim for 12–18 months to break even — 24+ months is a risky bet.✅ Lurk on Discord, Telegram, or Reddit — new deals and hacks show up daily. \n ✅ Smart plugs = catch weird power spikes early. \n ✅ Check resale trends — newer rigs hold value longer. \n ✅ Reseller delays hurt — always verify shipping timelines. \n ✅ Manage the heat — a 3.5 kW rig turns your garage into a sauna fast.Mining in 2025 isn’t plug-and-print money — but smart miners are still turning profits. Get your power costs under control, choose wisely, run the numbers , and keep an eye on new rigs. ASICs remain the backbone — if you plan right, you’ll stay in the green.What’s your mining plan? Are you sticking with Bitcoin or diversifying? Drop your ROI calculations below — let’s swap real numbers and lessons!]]></content:encoded></item><item><title>Cosmic DePIN: How Space‑Based Decentralized Networks Will Redefine Connectivity</title><link>https://hackernoon.com/cosmic-depin-how-spacebased-decentralized-networks-will-redefine-connectivity?source=rss</link><author>Madd Like Mojo</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:51:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In a land where Internet Providers assume 'full' control of our network systems, discover true decentralization from a flying space saucer above.]]></content:encoded></item><item><title>A Software Architect’s No-Bull Take on Vibe Coding</title><link>https://hackernoon.com/a-software-architects-no-bull-take-on-vibe-coding?source=rss</link><author>Andrew Korolov</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:45:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most People Don’t Know What Vibe Coding IsVibe coding is not what people think it is.Y Combinator compares the current state of vibe coding to the first cars, which resembled horse-drawn carriages. As a result, that first car design had too many unnecessary limitations. Once people realized they could do better, real cars emerged.  Similarly, the current state of vibe coding is in its initial suboptimal phase.The analogy is good but incomplete. Vibe coding is in its early stages. It is a vague vision, a sense of what an app should look like. Computer code doesn’t work like that: it needs clear, concrete 0s and 1s. Programmers are forced to translate vague intentions into precise instructions, often doing a significant amount of work to clarify the original intent into concrete rules.There is a Vast Difference Between Traditional Coding and Vibe CodingVibe coding delivers code without requiring specifications. It’s an ongoing experiment. When someone succeeds with vibe coding, they rave about it. When vibe coding fails, few will share their experience. We’re dealing with  here: those who make it tell the story. And there are far fewer stories from those who don’t make it.This is similar to the missing stories of dolphins that forced people to drown. There is a reason you don’t hear about them. People, in general, are less likely to brag about their failures. As a result, we get a distorted picture of how effective vibe coding is: more myth than method, built on selectively shared success stories.When you hear success stories about vibe coding, you think you can repeat it. You can’t.  Those who succeeded most likely had different tasks, used different prompts, and thought differently. AI models hallucinate.  Different prompts yield vastly differing code, whereas traditional coding generally follows more or less the same standards.The Key Principles of Traditional CodingVibe coding follows a process without consistency and repeatability, core qualities of any engineering discipline. Without those, success becomes a lucky exception.Traditional coding was built to avoid that kind of guesswork. It relies on precise algorithms, follows exact patterns, and uses standard structures that make code predictable and maintainable. Experts like Martin Fowler have spent decades formalizing these standards so developers don’t rely on gut instinct. They build on shared logic that scales.I get it; it's an appealing idea for a non-technical person to skip communicating with a boring, nitty-gritty geek and get the code made by yourself. My take on this is, why don’t you apply the same logic to healthcare? Why not ask AI for advice on do-it-yourself heart surgery?Where Vibe Coding Works WellDon’t get me wrong, there are great applications for vibe coding. Making an app just for fun is one such use case. When security and data privacy are not a concern, vibe coding is excellent! Not so when you want to build an app for a health insurance or medical provider. Vibe coding for a flight control system, NASA rocket ship propulsion control system, anyone?  was a pioneering NASA software engineer, here pictured with the code she wrote for the Apollo program that landed humans on the Moon.Would you vibe code that? Maybe you’d risk it, but that’s too much uncertainty for most people. And the problem isn’t that we’re overly cautious or scared of new tech. It’s that we already know these systems sometimes go off the rails, and when they do, it’s not a glitch. It’s real consequences for real people.How AI Works Defines Where Vibe Coding FailsThe way AI works is that it continuously forecasts the next most likely token, regardless of whether the method is applied to a human language or code. To build an AI code generation engine, you take all the existing code in that language that you can get your hands on. Most of the publicly available code in the world is written by Junior-level developers. Senior developers and corporations treat their code as intellectual property and don’t share most of it freely. This is why most good working code is in the private domain and not open source. Let’s take Python. Like a human language, it has a specific standard sentence structure with its own must-have tokens. AI uses forecasting to build code by continuously selecting the next most likely token, thereby arriving at functional code. If you’re interested, Microsoft provides a detailed breakdown of the technical aspects of forecasting.The open-source code is primarily experimental, and only a tiny fraction of that is corporate code that companies release into the public domain, like Lama and Red Hat. Another aspect of open code is that it can be partially open, so it is unlikely to include enterprise-level functions and features, such as advanced access rules, permission systems, authentication, and authorization. Because public code is the primary training source, AI learns from examples that fall short of enterprise-level quality.But simplifying, predicting what “usually” comes next isn’t the same as understanding what’s true. When context shifts or edge cases arise, this kind of pattern matching breaks down. And in code, completing without understanding can quietly introduce bugs that are hard to catch and costly to fix.Who has Access to High-Quality AI-Generated Code (and Why)People using AI want to generate enterprise-quality code, but they can’t. Huge companies like Amazon, Oracle, and Microsoft have massive volumes of trustworthy code that is verified, standardized, and quality-checked by generations of software engineers. AI models trained on code like that would be much more reliable, but the problem is intellectual property rights. Because their code is the corporations' intellectual property and the source of their market valuations, any AI models trained on it can’t and won’t be public. This is why you may have heard about a minor scandal OpenAI got into when using private data from GitHub, a case known as “Zombie Data.” AI models that use private data for training are usually used by those companies internally. Inside those companies, they generate high-quality vibe code. Companies with large volumes or proprietary code can use vibe coding internally with great results. The problem they solve is generating standardized code, not generating just any code fast. With models trained on massive volumes of high-quality code they own, they can ensure the code they produce maintains the style, syntax, and commenting conventions, generating code that’s similar to the code thousands of engineers have written already.  However, companies like that are unlikely to use unverified open-source code from GitHub to train their AI models.  The Way AI Model Training is Designed Causes it to Use Outdated CodeOne of the unspoken issues about AI code generators is the delay in catching up with the latest code updates. Y Combinator and Medium posts feature numerous discussions on version control issues for AI-generated code. AI models train on existing code, so much of what they learn from is already behind current standards. Since most of the available training data is obsolete, the model assigns more weight to older patterns in its predictions. As a result, it tends to generate code using deprecated syntax or practices.Vibe coders don’t realize problems caused by different code versions, but it inevitably becomes a barrier. They’re not engineers, so they cannot tell whether the code they generate is outdated or valid. As a result, they ship things that break silently, and they have no idea where or why they fail.Every new version of a programming language brings changes, and AI models aren’t version-aware. Google’s Vertex AI offers advice on overcoming this problem, but it’s probably not a guide that vibe coders will read. Most version updates fall into three categories:Deprecated functions that stop workingPatches to existing functions that require new syntax, including security updatesNew implementations that the AI model will ignoreVibe coding models often produce old code, and that’s a recipe for security bugs, especially if the latest code version’s critical security patches were missed. Consequently, you get failed supply chain security checks, policy violations, and blocked releases.In theory, AI models could avoid this by learning from user feedback, choosing one code version over another. Stackoverflow and Quora are full of discussions on this topic. But there are two issues here. First, vibe coders don’t have the expertise to evaluate which version is correct, yet the model still relies on them to steer it. Second, new language versions lack real-world code examples, so older patterns dominate and skew the model’s predictions.Until those two problems are solved, vibe coding will stay unreliable, especially when security matters.Great Use Cases for Vibe CodingVibe coding is an excellent tool for creating simple apps and websites faster, cheaper, and with more creativity than before. Many services like Cursor, Bolt, Replit, Lovable, and various AI chatbots like Claude already help anyone develop sites and apps, so long as limited flexibility is not an issue. Quickly and cheaply generating apps and sites with vibe coding is great for creative agencies or entrepreneurs with an innovative vision, so long as no long-term support is needed. Ideas that grow into successful businesses can hire software engineers to create a new codebase that scales, is secure, and remains maintainable. But that handoff isn't always smooth. Replacing AI-generated code often means starting over because the foundation isn't built for extensibility. What starts as a shortcut can become a constraint once real growth begins.Companies can also use vibe coding, but they must have the code and prepare it for model training by providing extensive documentation and code comments that describe how functions, classes, and modules work. Otherwise, the model has no reliable reference point — it starts generating based on patterns. And that's where things fall apart fast: vague outputs, unexpected behaviors, and hours lost trying to reverse-engineer your system.]]></content:encoded></item><item><title>10 Things I Wish I Knew About Product Management Sooner</title><link>https://hackernoon.com/10-things-i-wish-i-knew-about-product-management-sooner?source=rss</link><author>Aditya Vikram Kashyap</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:41:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Think product management is all strategy and shipping features? Think again. This brutally honest, laugh-out-loud guide dives into the real lessons PMs learn the hard way—like saying 'no' without becoming the office villain, surviving roadmap chaos, and fixing the feature no one asked for but everyone hates. Written by someone who’s been through the fire (and the Slack threads), it’s part therapy, part war journal, and all heart. If you've ever found yourself explaining why nothing shipped this sprint while silently questioning your career choices—this one’s for you. Expect awkward user interviews, polite panic, and very real truths about being the calm in the storm.]]></content:encoded></item><item><title>Growth Without Control Can Wreck Your Business</title><link>https://hackernoon.com/growth-without-control-can-wreck-your-business?source=rss</link><author>Engjell Rraklli</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:36:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Imagine you’ve been running your service-based business for some time.You have your customers, a structure up and running and you are already making some money because people like your service.Where do you go from there?This was exactly the question I asked myself and my co-founder. We had been running division5 for 4 years. Everything was stable, we had our recurring customers and we were able to provide a good service.We hit a ceiling though. We were not able to grow beyond that, we simply did not know how.That was the point where we started seeking new partnerships.Partnerships as a growth means.Partnerships were the first way that came to my mind regarding growing my business. We wanted to go in new markets and the best way to do that is through a partner which is local to that market.Here are a few reasons why :Local partners have access to connections and people you would otherwise not have.Local partners speak the native language and are culturally similar to your potential customers.It’s easier to create trust if someone local would vouch for you.Local partners can help you with your pricing strategy as they know the market better than you do.Please leave this field empty.In our case, we struck a deal with Pirate.They not only had a great brand in Germany, they had been also running one of the coolest business conferences in Europe creating a vast network of entrepreneurs and companies.The problem with partnerships though is that you don’t really know how it’s going to go. In the best case scenario, everything would go as intended but there are many reasons why the partnership would fail.Mismatch in values between the companies / shareholders.Misalignment in expectations.Communication problems between partners.So as with every long-term decision, think carefully. In our case, we took our time to fine-tune our agreement, a few months actually. In the end, I am happy with the deal we did and how everything transpired.Partnerships though take control away from you. Your growth depends on a third party. So I would heavily suggest you don’t keep it as your only means of growth.Networking, networking, networking.Yup, that’s right, in the B2B world networking is almost everything.We tend to think that businesses deal with other businesses but in reality, it’s people who deal with other people.In many cases, the decision on who to deal with is based on who you know or / and who you like.Sounds unfair but it’s human nature.So who you know plays a huge difference.  Everyone needs to network and this comes from someone was not a good networker.When I first started, I was anxious to talking to people. I think for some people it comes naturally while for many others, it’s a skill to be learned.So if networking does not come naturally for you either, don’t despair. See it as a skill. To get better at it, you need to practice it more. Show up in events. Talk to new people. Do this for some time and you will soon start seeing the results.I eventually got better and if I could do that, so can you.Networking is great, it really is, it takes a long time though to start seeing results. So while opposed to partnerships, you have more control, it won’t lead to growth. You need marketing / sales processes in place to achieve that.When people hear marketing magic comes to mind.It’s the classic word people use for many different things but few really know the meaning.I like to see marketing as the process that gets people at your door.How you do that can vary. Some people go and knock from door to door spreading the word. Other people do some posts on social media. Many companies run ads on TV. The sole purpose of marketing is getting people interested in your service.Sales on the other hand, is the process that gets people in the building.The sole purpose of sales is getting people to sign the contract and become a customer.If you want to control growth, you need to create effective marketing campaigns that reach many people and have an effective sales process that converts them into customers.Back in the day, this usually meant paying for expensive ads on TV. With the advent of social media, it’s much much easier to reach many people and get them interested in your service.Here are a few marketing techniques that we follow at division5 that any service-based business can follow :Outreach via email or social media.We have been seeing the best results through our outreach via social media but every business is different. You just gotta try what works for you. That’s also what we are doing.As soon as you find something that works, do that more. Eventually, you will be able to control the growth.That’s right, there is no silver bullet.If you want to expand in new markets, the best way is through partnerships. If you want to control the growth, you do that through implementing marketing and sales processes.Either way, in B2B you need to learn how to network and spend quite some time doing that. What works for someone else does not mean it will work for you to.Be curious, keep trying new things until you figure out what works for you.]]></content:encoded></item><item><title>Here&apos;s My Exact AI Prompt to Audit Any Professional Profile on the Internet — Including Your Own</title><link>https://hackernoon.com/heres-my-exact-ai-prompt-to-audit-any-professional-profile-on-the-internet-including-your-own?source=rss</link><author></author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:30:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
What does your LinkedIn really say about you? Here’s a method to test it, powered by AI and brutal honesty.“Why does every profile sound the same?”If you’ve spent more than 5 minutes on LinkedIn, you’ve probably noticed something strange: Everyone is a , , cross-functional problem-solver with  of experience.And somehow… no one says anything real.In a world flooded with GPT-polished personal brands, how do we separate the  from the ?That’s the question that inspired this: A simple but powerful AI prompt to critically audit  professional profile — yours, your peer’s, your client’s, or your hero’s.🚀 Why This Prompt MattersThis isn’t about catching lies. It’s about closing the gap between:🔍 What they’ve  (and whether it’s visible)⚙️ How well their public digital footprint supports those claims\
For , , , and , the stakes are high. We hire people, fund projects, and build reputations based on online signals. So we need better tools to  — without hiring an investigator:🛠️ The Prompt (Copy‑Paste Ready)Paste this into your favorite LLM (tested with ChatGPT 4o/4.1). Replace the name/link with any target:[FULL NAME] [linkedin-profile] — Structured Expert Profile with Critical Article Analysis 🚦  

Instructions (ENGLISH):
Provide a concise, critically-structured professional profile of the subject as an expert, using ONLY up-to-date information from publicly available web sources (do not use your internal memory or prior knowledge).
For every section, display ALL available, relevant information without omission or summary.
⚡️ Use tables, lists, and emoji for structure and emphasis.

MANDATORY REQUIREMENTS:

Do NOT omit, crop, or summarize ANY publication, fact, link, or thematic section found in public sources.

EVERY publication/article/post/quote found online MUST be individually analyzed and included—no “see above”, “other similar”, or “not reviewed for brevity”.

All facts must be confirmed with accessible links.

For each section, if data is absent, explicitly state “No public data found” (with date of check).

Validate all dates, links, and organizational details.

Critically analyze content—do not copy, do not repeat, do not generalize.

Structure your output using the headings below.

1️⃣ Key Activities & Experience
List all main areas of expertise (e.g., PMO, delivery, IT consulting, AI, risk, etc.).

State real job titles, companies, years (if known).

List all standout achievements, unique facts, and current roles.

Confirm ALL with links.

2️⃣ Major Articles & Publications (with Quality & AI-Check)
📝 Title/Topic    💡 Key Idea 🌍 Platform 📅 Date (verified)  🌟 Impact/Discussion    🧠 Originality/Validity 🤖 AI/LLM Content Check
(Analyze EVERY found article/post/publication. For each: one phrase summary, impact, originality, validity, and specific AI-generated content check: ✅ Genuine, ⚠️ Slightly formulaic, ❗ Possible AI. Confirm with link.)                        

3️⃣ Influence & Community Presence
List all professional and social platforms where the expert is active (LinkedIn, Medium, forums, Slack groups, etc.).

List any notable engagement, viral posts, peer comments/quotes (with source).

Mention roles in professional communities, boards, or online groups.

4️⃣ Expertise Assessment & Value
3–5 bullets: reputation, originality, strengths/weaknesses, audience, practical value.

Explicitly mention any “red flags” on originality, credibility, or suspected AI content.

Fact-based, no generalizations.

5️⃣ Collaborations, Events & Certifications
List ALL professional collaborations (projects, joint publications, open source, partnerships).

List ALL conference presentations, panels, podcasts, workshops, juries (date, topic, platform, link).

List ALL professional certificates and courses (with date, organizer, validation link if possible).

Explicitly note any absence of public evidence.

6️⃣ Web & Media Footprint
List EVERY instance where the expert is mentioned outside their own channels:

Third-party articles, reviews, interviews, analytics, “top experts” lists, company/industry sites, media, podcasts, YouTube, SlideShare, ResearchGate, etc.

For each, include link, date, context, and a brief summary.

Check for independent citations and discussions of their work.

Note: If none found, explicitly state this.

7️⃣ Academic & Teaching Activities (optional)
List any teaching, mentoring, course design, scientific or academic publications, lectures, or participation in educational projects (dates, topics, links).

If nothing found, state so.

Technical Reminders:

DO NOT summarize or omit ANY discovered item, however minor.

ALWAYS provide validated links and dates.

If a claimed certificate/publication cannot be independently verified, mark as ⚠️ “Unverified”.

Structure all lists and tables for fast reading; add emojis for clarity.

OUTPUT HEADINGS:
1️⃣ Key Activities & Experience
2️⃣ Major Articles & Publications (with Quality & AI-Check)
3️⃣ Influence & Community Presence
4️⃣ Expertise Assessment & Value
5️⃣ Collaborations, Events & Certifications
6️⃣ Web & Media Footprint
7️⃣ Academic & Teaching Activities

If any section yields no results, explicitly write:

“No public data found as of [date of check].”
🎯 What You Get (If Used Well)A , no-BS profile analysisDetected red flags, gaps, and unverifiable claimsA real sense of what the person’s  doing, not just sayingSignal on originality — whether posts look human or AI‑templatedSurface-level brand vs. deep, verifiable contribution🧑💻 Who This Is Useful For🧩  – evaluating candidates based on real case studies and traceable outcomes📦  – assessing consultants, mentors, and subject-matter experts beyond buzzwords📊  – verifying collaborators and public figures in AI, research, and analytics🧑💼  – filtering inflated profiles and focusing on demonstrable expertise🪞 Content Creators & Professionals – auditing their own digital footprint to improve credibility🤖 What AI Still Can’t Fake (But Tries)When you run this prompt on someone (or yourself), look out for:Articles with  vs. SEO word soupProjects with timelines, roles, and outcomes, not just jargonPosts that show , not just “here’s my new blog post” that can’t be verifiedMedia mentions outside self-posted networks\
If the AI comes back empty or vague — that’s the story, too.Here’s the brave part: Paste your own profile link into the prompt and read the result.If it feels… flat — good. That’s data. Now improve what matters, not just your headline.🌐 Why This Should Be Standard PracticeWe’ve normalized a world where people claim “AI Strategy”, “Leadership Transformation”, or “Researcher” in one paragraph, then share Canva carousels in the next.This prompt is an invitation to make things real again.To bring back credibility — not by gatekeeping, but by showing what’s , , and .If you found this useful:Let’s make profiles worth reading again.]]></content:encoded></item><item><title>Here’s Everything I Learned Building a Bitcoin Wallet from Scratch in 4 Months</title><link>https://hackernoon.com/heres-everything-i-learned-building-a-bitcoin-wallet-from-scratch-in-4-months?source=rss</link><author>Tristan Bietsch</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:26:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I spent 4 months building a Bitcoin wallet solo, live-streaming (on twitch and twitter) the entire build from wireframe to finish to an audience of 3. It’s janky, it works, and I learned more about engineering — and myself — than any tutorial could teach.: This wallet is testnet-only and in ALPHA. Do NOT use it with real Bitcoin. It’s a learning project, not production software.I previously founded BitEscrow (a non-custodial Bitcoin escrow service) and worked on it for a very long time. My co founder and I shipped something amazing. But after 3 long, hard, bootstrapped years I learned a really hard lesson: nobody wants to spend their Bitcoin. After that startup wound down, I did what any reasonable person would do — I binged  for 270 hours… in a .My friend with a PhD in computer science, who has 5000+ hours in the game, mentored me through Factorio at its deepest level, and I was totally hooked. But while playing, something clicked. The game’s systems thinking, optimization patterns, and relentless focus on efficiency rewired how I approach engineering problems. I realized Factorio is just a gamified version of actual systems engineering, and playing it had genuinely sharpened a skill I didn’t fully appreciate I had. When I emerged from my Factorio haze, I knew I wanted to build something substantial again.After taking some time off and well rested, I learned mobile development in 2 weeks and started building a Bitcoin wallet from absolute scratch as an MVP for a bigger project. I’m pivoting to infrastructure again, and this wallet was my technical deep dive before the jump. I originally planned this as an MVP for a product idea. Build wallet → grow waitlist → pitch investors. Classic playbook. But four months of solo development changed my perspective.As the weeks turned to months, I realized:My idea was too niche (BitEscrow 2.0 mistake territory)True market demand wasn’t thereI missed building infrastructure more than consumer productsInfra makes way more money when done correctlyMost importantly, a friend helped me re-frame my thinking of said idea entirely when I was at Bitcoin Las Vegas. By month three, I knew this wallet would be my graduation project — not my destination.I set a hard deadline: 4 months from first commit. Full kanban board. Ship or die. Two months in, I almost quit. The kanban overflowed with features, nothing worked properly, and those early dopamine hits from small wins had evaporated. I seriously considered writing a post-mortem about my failed wallet instead. Then something snapped — in a good way. I realized perfection was optional. Momentum wasn’t.Feature creep nearly killed this project. My kanban had everything: Bitcoin price screens, USD/sats converters, beautiful animations. All nice-to-haves that blocked real progress. Take the Bitcoin price feature I had planned — I kept hitting CoinGecko’s rate limits, so I engineered this elaborate solution: multiple API fallbacks, which led to orchestrated price averaging, centralized state management. Then I had an epiphany — just delete the entire feature.Removing USD conversion eliminated complexity across the entire codebase:No conversion logic in SendAmountScreen.tsx, SendConfirmScreen, InvoiceScreen.tsx, ReceiveScreen.tsxNo storing historical Bitcoin prices for transaction detailsSimpler state management overallI systematically blocked everything non-essential:UI smoke tests (I tested edge cases manually anyway)Branding and fancy animationsComponent refactoring and modularizationSecurity hardening beyond basicsRefactors of certain files in GoOne feature I desperately wanted: Jibberlink for air-gapped transaction signing. Would’ve been cool. Would’ve taken too long. Cut.Here’s what actually matters in a Bitcoin wallet: sending and receiving. That’s it. When I finally built a transaction that passed all UTXO validation, saw that success screen, and watched it confirm on the mempool — that dopamine rush was unreal. I built that. From scratch. Just raw transaction construction.Systems Thinking and AI PartnershipSomewhere around month three, my brain shifted. I stopped thinking in code and started thinking in systems. I’d lie in bed visualizing the wallet’s architecture — not files and functions, but flows and rules. The wallet transformed from “stuff that works” to “a thing with internal logic.”AI became less of a tool and more like a temporary co-founder. I wasn’t just writing prompts — I was thinking in prompts. Prompting is just software architecture in disguise — and thanks to Factorio, that’s a skill I had sharpened. I’d architect the flows, and Claude/Cursor would handle the implementation. It wasn’t perfect, but it gave me a second wind when energy faded. I got  good at prompting.The last week was pure triage. Days blurred together: debugging one minute, rewriting UI the next, then losing hours to obscure Expo polyfills. Context-switching became my primary skill. I kept wanting to refactor. Reorganize files. Create neat folder structures. But that was procrastination dressed as progress. I had to wear all the hats and feel the weight of each one. When I finally committed that last bugfix and watched my transaction go through — signed, broadcasted, confirmed — I let it stay ugly. Because ugly is honest, and shipping is beautiful.I researched extensively, using Blue Wallet as inspiration. Every library choice had a purpose: — The Bitcoin Swiss Army knife — Cross-platform mobile development — Because Vanilla JavaScript sucks — State management that is actually good — HD wallets (hierarchical deterministic) — Mnemonic seed phrases — Additional cryptographic operations (primary) with  fallback with custom rate limiting for fast key-value storage for data fetchingFull send/receive with dedicated hooks (, )Transaction history with complete validation pipelineMulti-network support (testnet, mainnet, regtest)HD wallet generation and importMempool.space integrationNo tutorial, course, textbook, or bootcamp could’ve taught me what this build did. Four months of solo development beats four years of theory. Using “Programming Bitcoin” as my bible, I came to appreciate Bitcoin as the marvel of engineering it truly is. The wallet works. It’s not pretty. But I shipped it, and that’s what matters.What I Learned (Technical)Building a wallet solo is really hard. Don’t do it unless you’re prepared for pain. Do it if you want to level up fast.Key generation, signing, and address derivation will humble you quicklyUTXO handling is where theory meets reality (and usually loses)BitcoinJS is powerful but raw. With a team, I’d use BDK + LDK instead.State management in financial apps needs paranoid-level attention“Securely storing private keys” is harder than yelling “not your keys, not your coins” on TwitterXcode remains the worst development experience in modern computingAI pair programming is the future — it makes good devs better, and bad devs worse.How to run, use, and interface with Bitcoin Core — and why it’s still the gold standard for full nodes.Segwit is actually brilliant once you implement it yourselfWhat I Learned (Personal)After 4 months of solo development, streaming to empty rooms, and wrestling with cryptographic primitives:Perfection is the enemy of momentum. Ship ugly.Execution is the ultimate filter. Ideas are easy, 4-month commitments aren’t.You can do hard things alone. But you probably shouldn’t.Persistence beats everything else.The deepest lesson? I’m an infrastructure engineer at heart. Those months of micro-adjusting UI pixels confirmed what I already knew — I belong in the backend, architecting systems, not pixels for a UI.This wallet works. It’s ugly, under-tested, and desperately needs refactoring. Could I perfect it? %. Will I? I have bigger projects calling my name. This wallet served its purpose: deepening my Bitcoin knowledge, sharpening my engineering skills, and clarifying what I actually want to build.If nothing else, I hope this helps one person understand how Bitcoin wallets actually work under the hood. The architecture is clean, the code is organized, and it’s all there for you to explore, break, and learn from.]]></content:encoded></item><item><title>22 Jobs That AI Will Create (That No One’s Talking About)</title><link>https://hackernoon.com/22-jobs-that-ai-will-create-that-no-ones-talking-about?source=rss</link><author>Paul Dhaliwal</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:20:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Discover 22 new AI jobs of the future—roles in trust, integration, and taste that humans will own as AI transforms the workforce. Future-proof your career.]]></content:encoded></item><item><title>Here&apos;s Why Product and Design Operate Better as One Team</title><link>https://hackernoon.com/heres-why-product-and-design-operate-better-as-one-team?source=rss</link><author>Kunal Abichandani</author><category>tech</category><pubDate>Wed, 2 Jul 2025 05:19:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AI startups are operating at speeds faster than ever before. Teams are always experimenting, learning, and building in places where there’s rarely a clear playbook. If you want to win, you need to stay nimble, move fast, and, above all, keep everyone on the same page.Some of the fastest-growing companies in AI today are proof:Cursor: $0 to $100M ARR in 21 months with just 20 peopleBolt: $0 to $20M ARR in 2 months with 15 peopleLovable: $0 to $10M ARR in 2 months with 15 peopleMidjourney: $0 to $200M ARR in 2 years with 10 peopleMercor: $0 to $50M ARR in 2 years with 30 peopleThe reason this has become possible today is because AI is collapsing the boundaries between roles.From my own experience running product and design at a high-growth AI startup, Rilla, I’ve learned that the best outcomes come when these two roles converge. Not just working closely. But thinking as one: solving problems, crafting experiences, and shaping product direction in a unified way. In these environments, you don’t need to “hand off” a spec or “loop in” design late in the game, because the thinking happens together, from the start.Building Faster by Staying AlignedOne of the biggest unlocks I’ve seen is just getting product and design closer to the customer. Not through decks or market research, but by actually talking to people. Sitting in on support calls, visiting customers IRL, and testing new ideas alongside customers. When you hear what’s confusing or frustrating in someone’s own words, it lands differently.At Rilla, every engineer spends one week doing only support. During this time, they don’t ship code or attend engineering meetings. They talk to customers, field issues, and experience pain points firsthand. Some of our best product ideas have come out of these support weeks. It’s often the week engineers come away saying, “Why didn’t we build this sooner?”We’ve also seen how alignment between product and design directly impacts our speed. There have been multiple instances where we’ve designed and shipped a new feature in a single day, from idea to implementation, because the people involved shared full context from the start. There’s no need for lengthy specs or back and forths. We spot a problem, discuss it together, sketch it out, and ship.The more traditional companies we’ve worked at treated product and design as distinct lanes: product defined what to build, and design figured out how it should look and feel. That separation often created friction. Shipping took longer, context was lost, and the end result often missed the mark. Despite these issues, that setup might be fine when you’re working on well-scoped features. But with AI products, where behavior changes weekly and user expectations are still being defined, that gap becomes a real liability.What has worked better is treating product and design as one role with shared context and ownership. Over time, this builds instinct. You start to see patterns earlier, make faster calls, and avoid overthinking things that don’t matter. It’s not perfect, but it’s closer to how real progress happens when you’re building in a space that’s constantly shifting.Teams like Linear have fully embraced this model, deliberately structuring their organization so that engineers and designers own the core product work. As CEO and co-founder Karri Saarinen puts it, “No product managers, just a head of product. PM duties are distributed across engineering and design.” It’s a clear bet on tight, multidisciplinary teams that build with context, not layers.Taste as a Core Product AdvantageIn today’s AI-powered landscape, baseline functionality is expected. Most products “just work.” What separates the forgettable from the beloved is taste. Design can’t be a final layer or a polish pass; it has to be embedded from the very beginning.“When somebody unwrapped that box and saw somebody gave a sht about me, I think that’s a spiritual thing… it came from a place of love and care.”*That kind of care is what makes a product feel intentional. It shows up in small, thoughtful moments. A helpful message timed just right, a smooth interaction that anticipates confusion, or the decision not to add a feature that clutters the experience. These aren’t surface-level choices. They come from shared ownership and instinct, when product and design think as one, not in handoffs.In my experience, teams that prioritize taste make better calls, faster. They focus less on checking boxes and more on how something will actually feel in the hands of a user. And in a world where AI handles more of the building, that kind of human judgment becomes the real differentiator.The Evolution of the Product Builder RoleAI startups are starting to hire very few PMs, if any at all. Instead, product responsibilities are shared across the team. Designers are sitting in on customer interviews, engineers are helping shape feature priorities, and everyone is involved in defining what success looks like.This shift isn’t just about leaner teams. It’s about moving faster by reducing handoffs and increasing ownership. When the people building the product also understand the “why” behind each decision, they don’t just complete tasks—they care about the outcome. They make smarter calls, notice edge cases sooner, and adjust quickly when things change.AI tools like Cursor and Figma Make make this even more accessible. Even if a designer isn’t fluent in code, they can ask AI to tweak a hover state or fix spacing—and the code updates itself. The feedback loop is much tighter and so the product gets better, faster.This sentiment has been echoed by Brian Chesky, CEO of Airbnb, when he publicly announced that he was restructuring roles within his company. He said, "We got rid of the classic product management function… We elevated design to be alongside product, so it's engineering, design, and product." This approach emphasizes the importance of multidisciplinary collaboration and shared ownership in product development.The Future of Product TeamsPeeking ahead, you’ll see the lines between product, engineering, and design blur even more, thanks largely to AI tools that speed everything up. Designers might pull components straight from a shared library and drop them into production. Engineers could dive into design systems and lay out entire user flows without waiting around for a mockup.This shift will let teams collaborate seamlessly and own the product from end to end. Designers, engineers, and product folks will all be writing specs and documentation together, each contributing directly to how the product takes shape.As AI keeps automating parts of design, writing, research, and development, everyone can focus on what they do best. The “product builder” of tomorrow will be a hybrid: part engineer, part designer, part product thinker. One moment they’re tweaking a model, the next they’re sketching a new feature or digging through user metrics. It’s messy, often overwhelming, and not always clear where the work should start. But when it clicks -- when the right person is close to the problem and empowered to solve it -- the results are fast, focused, and far more impactful.]]></content:encoded></item><item><title>Google&apos;s Data Center Energy Use Doubled In 4 Years</title><link>https://hardware.slashdot.org/story/25/07/01/221237/googles-data-center-energy-use-doubled-in-4-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: No wonder Google is desperate for more power: The company's data centers more than doubled their electricity use in just four years. The eye-popping stat comes from Google's most recent sustainability report, which it released late last week. In 2024, Google data centers used 30.8 million megawatt-hours of electricity. That's up from 14.4 million megawatt-hours in 2020, the earliest year Google broke out data center consumption. Google has pledged to use only carbon-free sources of electricity to power its operations, a task made more challenging by its breakneck pace of data center growth. And the company's electricity woes are almost entirely a data center problem. In 2024, data centers accounted for 95.8% of the entire company's electron budget.
 
The company's ratio of data-center-to-everything-else has been remarkably consistent over the last four years. Though 2020 is the earliest year Google has made data center electricity consumption figures available, it's possible to use that ratio to extrapolate back in time. Some quick math reveals that Google's data centers likely used just over 4 million megawatt-hours of electricity in 2014. That's sevenfold growth in just a decade. The tech company has already picked most of the low-hanging fruit by improving the efficiency of its data centers. Those efforts have paid off, and the company is frequently lauded for being at the leading edge. But as the company's power usage effectiveness (PUE) has approached the theoretical ideal of 1.0, progress has slowed. Last year, Google's company-wide PUE dropped to 1.09, a 0.01 improvement over 2023 but only 0.02 better than a decade ago. Yesterday, Google announced a deal to purchase 200 megawatts of future fusion energy from Commonwealth Fusion Systems, despite the energy source not yet existing. "It's a sign of how hungry big tech companies are for a virtually unlimited source of clean power that is still years away," reports CNN.]]></content:encoded></item><item><title>Nintendo’s Anti-Consumer Anti-Piracy Measures Also Reduce The Value Of The Switch 2</title><link>https://www.techdirt.com/2025/07/01/nintendos-anti-consumer-anti-piracy-measures-also-reduce-the-value-of-the-switch-2/</link><author>Dark Helmet</author><category>tech</category><pubDate>Wed, 2 Jul 2025 03:15:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[When it comes to the anti-piracy efforts taken by some of the more aggressive companies out there, such as Nintendo, the most frustrating part of the whole thing for me is just how completely short-sighted those efforts tend to be. Take Nintendo’s updated EULA for its Switch consoles, for example. The updated agreement makes several changes from its previous iteration, but the most notable is that Nintendo says that if it thinks you’re doing the piracy for any reason, it can suspend all kinds of services on your console, up to and including bricking it completely. And, while the company has yet to go the bricking route so far,  it has already begun suspending all online services on consoles for the use of MIG Switches, cards for Switch devices on which you can load legitimately extracted ROMs from purchased games, or pirated versions of the same.Now, the first layer of how this is short-sighted is easy enough to see. In order to engage in copyright protectionism, Nintendo is risking long-term reputational damage by functionally ruining the consoles of customers for actions that aren’t illegal, or even immoral. Short term protection, longer term risk of everyone thinking you don’t care about your own customers. But there’s another layer to this, as a result of these service suspensions being tied directly to the device rather than the person. And that is what this protectionism means for the secondary market for Nintendo Switches.“I was driving between work sites and stopped at two different Walmarts,” says user Bimmytung. “At the second one I find a Mario Kart edition sitting in the case and couldn’t believe my luck.” They were informed by the Walmart staff that it was an “open box return,” so it was removed from the box to be checked over, and all looked well. The code for the packaged Mario Kart World had been scratched off already, so Walmart knocked another $50 off the price, and it all seemed like a good deal. Until they got home.Finally after work I get a chance to set it up. Quickly realize I need the super special micro SD card and none of the ~half dozen in the house would work. Drive ten minutes to Target and get one there and pick up a few other accessories as well. Get home and go to finish the setup—quickly get Error Code 2124-4508. A quick Google search shows me I’m screwed. FML.”Now, there are several layers of shame here to go around. Shame on Walmart for selling a device without ensuring it would work for the buyer the way it is intended to work. And shame on Nintendo for creating an anti-piracy program such that the punishments meted out are linked to hardware rather than the supposed bad-actor it seeks to punish. But all of that aside, it should also be true that this sort of thing drives the value of a Nintendo Switch console lower than it would be otherwise. Part of the value you gain when you buy a physical thing is the ability to eventually put it on the secondary market at some point. Because of the actions that Nintendo is taking in disabling and/or bricking its own consoles, that injects a great deal of risk into the prospect of buying one on the secondary market. The value of the hardware is, by at least some measure, diminished.But because Nintendo seems to only think about these things in the short term, the company probably doesn’t much care.However, the more immediate issue is for those looking to pick up a Switch 2 from a reseller or previous owner, given their current scarcity at first-party sellers. There’s really no way of knowing at all if a console has been bricked when buying the device online, and this could make the resale market a complete shambles for the whole life cycle of the console. And, grimly, that’s not exactly a priority for Nintendo, given that reselling, either in store or online, gains the company nothing, and some would argue actually costs the company a sale—it’s not like it’ll be in a rush to address the problem.Which is why I won’t be in a rush to buy a Switch 2 anytime soon. And I’m certainly in their target market, having two young children who desperately want one. Instead of the console, however, they will be getting a lesson in making smart buying decisions as a consumer.]]></content:encoded></item><item><title>Laptop Mag Is Shutting Down</title><link>https://hardware.slashdot.org/story/25/07/01/2133224/laptop-mag-is-shutting-down?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Laptop Mag, a tech publication that began in 1991 as a print magazine, is shutting down after nearly 35 years. The Verge reports: Laptop Mag has evolved many times over the years. It started as a print publication in 1991, when Bedford Communications launched the Laptop Buyers Guide and Handbook. Laptop Mag was later acquired by TechMedia Network (which is now called Purch) in 2011 and transitioned to digital-only content in 2013. Future PLC, the publisher that owns brands like PC Gamer, Tom's Guide, and TechRadar, acquired Purch -- and Laptop Mag along with it.
 
"We are incredibly grateful for your dedication, talent, and contributions to Laptop Mag, and we are committed to supporting you throughout this transition," [Faisal Alani, the global brand director at Laptop Mag owner Future PLC] said. Laptop Mag's shutdown follows the closure of long-running tech site AnandTech, which was also owned by Future PLC. It's not clear whether Laptop Mag's archives will be available following the shutdown.]]></content:encoded></item><item><title>Apple Accuses Former Engineer of Taking Vision Pro Secrets To Snap</title><link>https://yro.slashdot.org/story/25/07/01/2128235/apple-accuses-former-engineer-of-taking-vision-pro-secrets-to-snap?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple has filed (PDF) a lawsuit against former Vision Pro engineer Di Liu, accusing him of stealing thousands of confidential files related to his work on Apple's augmented reality headset for the benefit of his new employer Snap. The company alleges Liu misled colleagues about his departure, secretly accepted a job offer from Snap, and attempted to cover his tracks by deleting files -- actions Apple claims violated his confidentiality agreement. The Register reports: Liu secretly received a job offer from Snap on October 18, 2024, a role the complaint describes as "substantially similar" to his Apple position, meaning Liu waited nearly two weeks to resign from Apple, per the lawsuit. "Even then, he did not disclose he was leaving for Snap," the suit said. "Apple would not have allowed Mr. Liu continued access had he told the truth." Liu allegedly copied "more than a dozen folders containing thousands of files" from Apple's filesystem to a personal cloud storage account, dropping the stolen bits in a pair of nested folders with the amazingly nondescript names "Personal" and "Knowledge."
 
Apple said that data Liu copied includes "filenames containing confidential Apple product code names" and files "marked as Apple confidential." Company research, product design, and supply chain management documents were among the content Liu is accused of stealing. The complaint also alleges that Liu deleted files to conceal his activities, a move that may hinder Apple's ability to determine the full scope of the data he exfiltrated. "Mr. Liu additionally took actions to conceal his theft, including deceiving Apple about his job at Snap, and deleting files from his Apple-issued computer that might have let Apple determine what data Mr. Liu stole," the complaint noted.
 
Whatever he has, Apple wants it back. The company demands a jury trial on a single count of breach of contract under a confidentiality and intellectual property agreement Liu was bound to. It also asks the court to compel Liu to return all misappropriated data, award damages to be determined at trial, and reimburse Apple's costs and attorneys' fees.]]></content:encoded></item><item><title>Tinder To Require Facial Recognition Check For New Users In California</title><link>https://yro.slashdot.org/story/25/07/01/2112208/tinder-to-require-facial-recognition-check-for-new-users-in-california?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 2 Jul 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Axios: Tinder is mandating new users in California verify their profiles using facial recognition technology starting Monday, executives exclusively tell Axios. The move aims to reduce impersonation and is part of Tinder parent Match Group's broader effort to improve trust and safety amid ongoing user frustration. The Face Check feature prompts users to take a short video selfie during onboarding. The biometric face scan, powered by FaceTec, then confirms the person is real and present and whether their face matches their profile photos. It also checks if the face is used across multiple accounts. If the criteria are met, the user receives a photo verified badge on their profile. The selfie video is then deleted. Tinder stores a non-reversible, encrypted face map to detect duplicate profiles in the future.
 
Face Check is separate from Tinder's ID Check, which uses a government-issued ID to verify age and identity. "We see this as one part of a set of identity assurance options that are available to users," Match Group's head of trust and safety Yoel Roth says. "Face Check ... is really meant to be about confirming that this person is a real, live person and not a bot or a spoofed account." "Even if in the short term, it has the effect of potentially reducing some top-line user metrics, we think it's the right thing to do for the business," Rascoff said.]]></content:encoded></item><item><title>Figma Files For IPO</title><link>https://slashdot.org/story/25/07/01/2058244/figma-files-for-ipo?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Figma has filed to go public on the NYSE under the ticker "FIG," marking one of the most anticipated IPOs in recent years following its scrapped $20 billion acquisition by Adobe. CNBC reports: Revenue in the first quarter increased 46% to $228.2 million from $156.2 million in the same period a year ago, according to Figma's prospectus. The company recorded a net income of $44.9 million, compared to $13.5 million a year earlier. As of March 31, Figma had 1,031 customers contributing at least $100,000 a year to annual revenue, up 47% from a year earlier. Clients include Amazon Web Services, Google, Microsoft and Netflix. More than half of revenue comes from outside the U.S. Figma didn't say how many shares it plans to sell in the IPO. The company was valued at $12.5 billion in a tender offer last year, and in April it announced that it had confidentially filed for an IPO with the SEC. [...]
 
Figma was founded in 2012 by CEO Dylan Field, 33, and Evan Wallace, and is based in San Francisco. The company had 1,646 employees as of March 31. Before establishing Figma, Field spent over two years at Brown University, where he met Wallace. Field then took a Thiel Fellowship "to pursue entrepreneurial projects," according to the filing. The two-year program that Founders Fund partner Peter Thiel established in 2011 gives young entrepreneurs a $200,000 grant along with support from founders and investors, according to an online description. Field is the biggest individual owner of Figma, with 56.6 million Class B shares and 51.1% of voting power ahead of the IPO. He said in a letter to investors that it was time for Figma to buck the "trend of many amazing companies staying privately indefinitely." "Some of the obvious benefits such as good corporate hygiene, brand awareness, liquidity, stronger currency and access to capital markets apply," wrote Field. "More importantly, I like the idea of our community sharing in the ownership of Figma -- and the best way to accomplish this is through public markets."
 
As a public company, Field said investors should "expect us to take big swings," including through acquisitions.
 
In April, Figma bought the assets and team of an unnamed technology company for $14 million, according to the filing. They also registered over 13 million users per month, one-third of which are designers.]]></content:encoded></item><item><title>Jon McNeill brings the operator’s playbook to TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/jon-mcneill-brings-the-operators-playbook-to-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 23:02:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At TechCrunch All Stage 2025, Jon McNeill, CEO and co-founder of DVx Ventures, will take the Scale Stage to flip the script on conventional startup growth advice.]]></content:encoded></item><item><title>Xerox Buys Lexmark For $1.5 Billion As Print Industry Clings To Relevance</title><link>https://slashdot.org/story/25/07/01/2247221/xerox-buys-lexmark-for-15-billion-as-print-industry-clings-to-relevance?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 23:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli shares a report from NERDS.xyz: In a move that feels straight out of a different era, Xerox has officially acquired Lexmark for $1.5 billion. The deal includes net debt and assumed liabilities, and it pulls Lexmark out of the hands of Chinese ownership and into a freshly restructured Xerox. That's a lot of money for a company best known for making machines that spit out paper.
 
According to Xerox, this is all part of a "Reinvention" strategy. The company now claims it will be one of the top five players in every major print category and the leader in managed print services. [...] Xerox says the new leadership team will include executives from both sides, and the combined business will now support over 200,000 clients in more than 170 countries. They'll also be running 125 manufacturing and distribution centers in 16 countries.]]></content:encoded></item><item><title>AMC Warns Moviegoers To Expect &apos;25-30 Minutes&apos; of Ads and Trailers</title><link>https://entertainment.slashdot.org/story/25/07/01/2052226/amc-warns-moviegoers-to-expect-25-30-minutes-of-ads-and-trailers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AMC Theatres now warns customers that movies start 25-30 minutes after the listed showtime to account for ads and trailers, "making it easier for moviegoers to know the actual start time of their film screening," reports The Verge. From the report: Starting today, AMC will also show more ads than before, meaning its preshow lineup may have to be reconfigured to avoid exceeding the 30-minute mark. The company made an agreement with the National CineMedia ad network that includes as much as five minutes of commercials shown "after a movie's official start time," according to The Hollywood Reporter, and an additional 30-to-60-second "Platinum Spot" that plays before the last one or two trailers.
 
AMC was the only major theater chain to reject the National CineMedia ad spot when it was pitched in 2019, telling Bloomberg at the time that it believed "US moviegoers would react quite negatively." Now struggling financially amid an overall decline in movie theater attendance and box-office grosses, AMC has reversed course, telling The Hollywood Reporter that its competitors "have fully participated for more than five years without any direct impact to their attendance."]]></content:encoded></item><item><title>Congress just greenlit a NASA moon plan opposed by Musk and Isaacman</title><link>https://techcrunch.com/2025/07/01/congress-just-greenlit-a-nasa-moon-plan-opposed-by-musk-and-isaacman/</link><author>Aria Alamalhodaei</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:30:59 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The $10 billion addition to the Artemis architecture, which includes funding for additional Space Launch System rockets and an orbiting station around the moon called Gateway, is a rebuke to critics who wished to see alternative technologies used instead.]]></content:encoded></item><item><title>The Moral Imperative Of Clear Language</title><link>https://www.techdirt.com/2025/07/01/the-moral-imperative-of-clear-language/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:17:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[I need to say something that will make many of you deeply uncomfortable: your refusal to call fascism “fascism” is not sophistication—it’s .When Donald Trump posts explicit orders for “REMIGRATION” and “Mass Deportation Operations” targeting American cities because they are “the core of the Democrat Power Center,” that’s not “controversial immigration policy.” That’s mass deportation directed against political opponents. When federal troops deploy against American civilians exercising constitutional rights, that’s not “enhanced law enforcement.” That’s military occupation. When the systematic dismantling of democratic institutions gets described as “political polarization,” that’s not nuanced analysis—it’s linguistic evasion that enables the very thing it refuses to name.The sophisticates hate this clarity. They prefer the safety of euphemism, the comfort of complexity that never quite arrives at moral judgment. They speak of “concerning developments” and “troubling trends” while democracy burns around them. They perform nuanced understanding while fascism consolidates power through their very refusal to name it.But here’s what they don’t understand: authoritarianism thrives in ambiguity. It requires linguistic fog to operate. It depends on our unwillingness to call things by their proper names. Every euphemism is a small surrender. Every hedge is a tiny collaboration. Every refusal to speak plainly is a gift to those who profit from confusion.Language shapes consciousness. When we refuse to name what we see clearly, we don’t just fail to communicate—we erode our collective capacity to think clearly, to feel appropriately, to respond effectively. We make ourselves complicit in our own moral disorientation.George Orwell understood this when he wrote that “political language is designed to make lies sound truthful and murder respectable, and to give an appearance of solidity to pure wind.” But he was describing propaganda techniques used by totalitarian regimes. What we face now is worse: the voluntary adoption of euphemistic language by people who should know better, who pride themselves on seeing clearly, who claim to defend democratic values.We are doing the propagandists’ work for them.Consider how this linguistic distortion operates in practice. When mass deportation operations targeting millions of people get called “immigration enforcement,” we’re not being diplomatic—we’re making state violence psychologically easier to accept. When systematic attacks on democratic institutions get labeled “political disagreements,” we’re not showing balance—we’re normalizing authoritarianism. When obvious lies get treated as “alternative perspectives,” we’re not being fair—we’re weaponizing false equivalence against truth itself.The euphemism isn’t just descriptive failure—it’s moral failure. It changes how people process information, how they make decisions, how they understand their own moral obligations. When you call fascism “populism,” you’re not just using imprecise language. You’re making it easier for people to support fascism without confronting what they’re supporting.Hannah Arendt spent her life studying how ordinary people enable extraordinary evil, and she identified linguistic evasion as one of the primary mechanisms. In , she showed how bureaucratic language—“evacuation,” “resettlement,” “special treatment”—allowed participants in genocide to avoid confronting the reality of what they were doing. They weren’t murdering children; they were “processing population transfers.” They weren’t operating death camps; they were managing “facilities for the final solution.”The language didn’t just hide the reality from others—it hid it from themselves. It allowed them to participate in evil while maintaining their self-image as decent, law-abiding citizens following proper procedures.Arendt’s insight was that evil becomes possible not primarily through active malice but through the refusal of ordinary people to see and name what’s in front of them. The “banality of evil” is fundamentally about linguistic evasion enabling moral evasion. When we stop calling violence violence, we make violence easier to commit.This is what we’re witnessing now. The systematic training of a population to see clearly but speak obliquely, to understand precisely but describe vaguely, to recognize authoritarianism but call it something else. We have become a society of people who know exactly what’s happening but lack the linguistic courage to say so.The Practice of Plain NamingConsider how this evasion plays out in our current discourse:We don’t say “Trump is implementing fascist policies.” We say “Trump’s approach raises concerns about democratic norms.”We don’t say “Republicans are supporting mass deportation operations.” We say “There are disagreements about immigration enforcement strategies.”We don’t say “Conservative media spreads lies designed to enable authoritarianism.” We say “Different sources present different perspectives on complex issues.”We don’t say “MAGA supporters have chosen to enable fascism.” We say “There are legitimate grievances driving political polarization.”Each euphemism makes the reality a little less clear, a little less urgent, a little less morally demanding. Each hedge creates space for people to avoid confronting what they’re witnessing or participating in. Each refusal to name plainly is a small act of collaboration with the forces that depend on confusion to operate.When Trump orders ICE to conduct “Mass Deportation Operations” in cities he identifies as “the core of the Democrat Power Center,” that’s not immigration policy—it’s the use of state violence against political opponents. When he calls for “REMIGRATION” of millions of people, that’s not border security—it’s forced population transfer. When federal agents separate families and detain children, that’s not law enforcement—it’s state-sanctioned cruelty.The defenders will say “the law is the law”—as if legality were equivalent to morality. But slavery was legal. Segregation was legal. Japanese internment was legal. Every authoritarian regime in history has operated through law, not despite it. “The law is the law” is not a moral position—it’s moral abdication disguised as principled governance.Law without moral foundation is just organized violence. Rules without ethical grounding are just systematized cruelty. When your only defense of a policy is that it’s technically legal, you’ve already admitted it’s morally indefensible.The Sophisticates’ ResistanceThe sophisticates will tell you that such plain language is “inflammatory,” “divisive,” “unhelpful to productive dialogue.” They’ll suggest that calling fascism “fascism” alienates potential allies, shuts down conversation, makes compromise impossible.But here’s what they’re really saying: they prefer the comfort of ambiguity to the responsibility that comes with clarity. They’d rather maintain the illusion of reasoned discourse than confront the reality that one side has abandoned reason entirely. They want to keep playing by rules that the other side has explicitly rejected.This isn’t sophistication—it’s cowardice. It’s the intellectual’s version of appeasing authoritarianism through linguistic accommodation. It’s the belief that if we just find the right words, the right tone, the right approach, we can somehow reason with people who have chosen unreason as their governing principle.But you cannot have productive dialogue with fascists about the merits of fascism. You cannot find common ground with people who reject the premise of shared reality. You cannot compromise with those who view compromise as weakness and good faith as stupidity.What you can do is name what they are doing clearly enough that people understand what’s at stake and what choice they face.The power of plain naming is that it forces moral confrontation. It makes people choose sides. It strips away the comfortable distance that euphemism provides. It demands that people acknowledge what they’re actually supporting rather than hiding behind sanitized language.This is why authoritarians work so hard to control language. They understand that linguistic precision is the enemy of moral confusion. That clear naming makes their projects harder to defend. That euphemism is their friend and clarity is their enemy.They want us to call their fascism “nationalism.” Their lies “alternative facts.” Their cruelty “tough love.” Their mass deportations “border security.” Their authoritarianism “law and order.”Every time we adopt their language, we do their work. Every time we refuse to name their actions plainly, we make those actions easier to defend, easier to rationalize, easier to continue.When we refuse to call fascism “fascism”, we don’t make fascism less dangerous. We make ourselves less capable of recognizing and resisting it. We participate in our own disorientation. We become accomplices to our own confusion.The courage to name things plainly is not the courage to be harsh or inflammatory. It’s the courage to accept the responsibility that comes with seeing clearly. It’s the courage to abandon the comfortable illusion of neutrality and acknowledge that some things cannot be straddled, some positions cannot be hedged, some realities cannot be euphemized away.To say that systematic deployment of federal troops against American cities constitutes military occupation is not inflammatory—it’s accurate. To say that mass deportation operations targeting political opponents constitute fascist policy is not hyperbolic—it’s precise. To say that obvious lies designed to enable authoritarianism are lies is not divisive—it’s necessary.The alternative to plain naming is not diplomatic nuance—it’s moral blindness. It’s the systematic erosion of our capacity to recognize authoritarianism when it appears in familiar forms, speaking familiar languages, wearing familiar clothes.Evil depends on our unwillingness to call it evil. Fascism depends on our refusal to call it fascism. Lies depend on our treatment of them as “alternative perspectives.” State violence depends on our description of it as “tough policy choices.”The moment we name these things plainly, we restore the moral clarity that makes effective resistance possible. We acknowledge what we’re actually facing. We accept the responsibility that comes with seeing clearly. We choose truth over comfort, accuracy over diplomacy, moral clarity over intellectual sophistication.This is not just a linguistic choice—it’s a moral one. Every time we speak plainly about what we’re witnessing, we strike a blow against the forces that depend on confusion to operate. Every time we call fascism “fascism”, we make fascism a little harder to defend. Every time we name state violence as state violence, we make such violence a little less acceptable.Two plus two equals four. There are twenty-four hours in a day. And Trump’s mass deportation operations are fascistic displays of state violence targeting political enemies whether we have the courage to call them that or not.The difference is not in the reality—the difference is in our capacity to respond to reality appropriately.Name it plainly. Not because it’s easy, but because it’s true. Not because it’s comfortable, but because comfort in the face of authoritarianism is itself a form of collaboration. Not because it’s diplomatic, but because diplomacy with fascists is enabling fascism.The revolution is linguistic honesty. The rebellion is calling things by their proper names. The resistance is refusing to participate in the euphemistic erosion of moral clarity.Say what you see. Name what you know. Call fascism .Every minute of every day.Remember what’s real. Because the alternative to naming fascism clearly isn’t moderation or diplomacy—it’s surrender.Mike Brock is a former tech exec who was on the leadership team at Block. Originally published at his Notes From the Circus.]]></content:encoded></item><item><title>ICEBlock, an app for anonymously reporting ICE sightings, goes viral overnight after Bondi criticism</title><link>https://techcrunch.com/2025/07/01/iceblock-an-app-for-anonymously-reporting-ice-sightings-goes-viral-overnight-after-bondi-criticism/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:09:38 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The citizen app for anonymously reporting ICE agents and raids went viral after criticism from the U.S. attorney general.]]></content:encoded></item><item><title>EFFecting Change: EFF Turns 35!</title><link>https://www.eff.org/deeplinks/2025/06/effecting-change-eff-turns-35</link><author>Melissa Srago</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/effectingeffturns35_banner.png" length="" type=""/><pubDate>Tue, 1 Jul 2025 22:09:36 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[We're wishing EFF a happy birthday on July 10! Since 1990, EFF's lawyers, activists, analysts, and technologists have used everything in their toolkit to ensure that technology supports freedom, justice, and innovation for all people of the world. They've seen it all and in this special edition of our EFFecting Change livestream series, leading experts at EFF will explore what's next for technology users.Want to make sure you don’t miss our next livestream? Here’s a link to sign up for updates about this series:eff.org/ECUpdates.]]></content:encoded></item><item><title>Amazon Deploys Its One Millionth Robot, Releases Generative AI Model</title><link>https://hardware.slashdot.org/story/25/07/01/2046242/amazon-deploys-its-one-millionth-robot-releases-generative-ai-model?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 1 Jul 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: After 13 years of deploying robots into its warehouses, Amazon reached a new milestone. The tech behemoth now has 1 million robots in its warehouses, the company announced Monday. This one millionth robot was recently delivered to an Amazon fulfillment facility in Japan. That figure puts Amazon on track to reach another landmark: Its vast network of warehouses may soon have the same number of robots working as people, according to reporting from The Wall Street Journal. The WSJ also reported that 75% of Amazon's global deliveries are now assisted in some way by a robot. Amazon also unveiled a new generative AI model called DeepFleet, built using SageMaker and trained on its own warehouse data, which improves robotic fleet speed by 10% through more efficient route coordination.]]></content:encoded></item><item><title>Figma moves closer to a blockbuster IPO that could raise $1.5B</title><link>https://techcrunch.com/2025/07/01/figma-moves-closer-to-a-blockbuster-ipo-that-could-raise-1-5b/</link><author>Julie Bort</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:55:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The financials are impressive and  founder CEO Dylan Field already cashed out $20 million worth of shares last year.]]></content:encoded></item><item><title>$70M Committed To Boba Network As Foundation Concludes BOBA Token Agreement With FTX Recovery Trust</title><link>https://hackernoon.com/$70m-committed-to-boba-network-as-foundation-concludes-boba-token-agreement-with-ftx-recovery-trust?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:43:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Grand Cayman, Cayman Islands, July 1st, 2025/Chainwire/--Boba Governance Foundation today announced a significant milestone with a $70 million capital commitment secured from Awaken Foundation and LDA Capital to fund the continued development and ecosystem expansion of Boba Network, the leading Layer-2 blockchain for AI-powered decentralized applications (dApps), enabled by its unique HybridCompute technology. The Foundation also announced an agreement with FTX Recovery Trust regarding the BOBA tokens held by the Trust. The $70 million capital infusion will serve as a catalyst for Boba Network's ambitious growth plans. The funding will be strategically allocated to bolster the network's core infrastructure, expand its developer ecosystem, and foster the creation of innovative decentralized applications (dApps) on the platform, with a particular focus on enabling AI-powered dApps."This funding will accelerate the development of the Boba Network ecosystem, attract top-tier talent, and drive the widespread adoption of Boba Network as a premier Layer-2 solution for AI-powered dApps. We are excited to collaborate with Boba Network partners to shape the future of the AI-powered, decentralized web," said Alan Chiu, CEO of Enya Labs, a core contributor to Boba Network.""This substantial capital commitment from Awaken Foundation and LDA Capital is a testament to the transformative potential of Boba Network," said David Acutt, director of Boba Governance Foundation.Awaken Foundation, a key advocate for decentralized infrastructure and digital sovereignty, sees Boba Network as a critical component in the next phase of Web3 evolution.“We are thrilled to support the Boba Governance Foundation in its pursuit of open innovation,”said Nattaphol Vimolchalao, Director at Awaken Foundation. “Boba’s ability to connect smart contracts with off-chain computation—especially AI—unlocks enormous potential across industries.”LDA Capital, known for backing high-growth tech ventures and digital asset ecosystems, echoed that sentiment.“Boba Network is building essential infrastructure for the future of decentralized computation,” said Warren Baker, Managing Partner at LDA Capital. “We believe Boba will play a pivotal role in scaling the next generation of intelligent dApps, and we’re proud to support their mission as they push the boundaries of what’s possible in blockchain technology.”The strategic partnership with Awaken Foundation and LDA Capital goes far beyond financial support. It represents a powerful alignment of vision, expertise, and global reach. Leveraging deep industry knowledge, business development capabilities, and an extensive network of strategic partners, both firms are uniquely positioned to accelerate Boba Network’s growth. This collaboration is set to strengthen Boba’s leadership in blockchain innovation and drive its next phase of global expansion. In addition, LDA Capital offers differentiated value through LDA Velocity, its institutional-grade liquidity and market-making platform that supports healthy, scalable token ecosystems across global exchanges.Infrastructure Enhancement: The funding will be used to strengthen Boba Network's infrastructure, ensuring high throughput, low latency, and robust security for users and developers.Ecosystem Expansion: A portion of the capital will be dedicated to expanding the Boba Network ecosystem by attracting developers, projects, and users through grants and educational initiatives.dApp Development: The funding will support the creation of innovative dApps on Boba Network, with a strong emphasis on AI-powered dApps, ranging from decentralized finance (DeFi) protocols to real-world assets (RWA) applications.Community Engagement: Boba Governance Foundation will continue to foster a vibrant and engaged community by providing resources, support, and opportunities for collaboration.Resolution with FTX Recovery TrustIn addition, Boba Governance Foundation has executed an agreement with FTX Recovery Trust whereby all the BOBA tokens held by the Trust have been transferred to the Foundation. FTX Recovery Trust, in addition to other consideration and mutual release of claims, received the right to purchase up to approximately 29.4M BOBA tokens from Boba Governance Foundation at $0.09 per token within the next 18 months.“This agreement represents a momentous milestone for Boba Network, as it removes a major source of uncertainty over the BOBA token and strengthens the Foundation’s ability to support the continued development of Boba Network and its ecosystem,” said Acutt.About Boba Governance Foundation is a non-profit organization dedicated to the advancement and growth of Boba Network. It supports the development of the network's technology, fosters community engagement, and promotes the adoption of Boba Network across various industries. Boba Network is the leading Layer-2 blockchain for AI-powered decentralized applications (dApps), enabled by its unique HybridCompute technology. is a private investment firm founded by seasoned crypto, venture capital, and public market investors. The firm seeks to invest in established blockchain protocols to help further develop its technology. Awaken provides strategic capital, accelerated business development, and engineered exits for protocols that Awaken believes have a promising future in the modern economy. is a global alternative investment group with expertise in cross-border transactions worldwide. The team has collectively executed over 350 transactions in both the public and private middle markets across 43 countries with aggregate transaction values of over USD $11 billion. LDA’s investment activities across Web3 include 27+ transactions totaling $400m+ in capital commitments.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Decentralized Public-Key Infrastructure: The Future of Supply Chain Security</title><link>https://hackernoon.com/decentralized-public-key-infrastructure-the-future-of-supply-chain-security?source=rss</link><author>Nneoma Uche</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:31:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[All products, digital and physical, pass through a supply chain—a network of actors that supports their life cycle. But as the global market becomes increasingly interconnected, supply chain attacks are on the rise.In March 2023, cybercriminals infiltrated 3CX’s build environment, injecting malicious code into a library file for its macOS and Windows desktop apps. The compromised file was distributed through official updates, exposing users to malware. This breach emphasized the shortcomings of centralized systems in securing supply chains, as a single compromised vendor can jeopardize the privacy of the entire customer base.Web3 supply chains, leveraging blockchain and Decentralized Public Key Infrastructure (DPKI), offer a robust alternative. By prioritizing transparency, traceability, and tamper-proof security, they present a stronger defense against supply chain threats.This article explores how DPKI in blockchain-driven networks outperforms traditional PKI, and why a supply chain powered by the latter presents a tougher nut for bad actors to crack.Understanding Public-Key Infrastructure (PKI)IBM defines ‘Public-Key Infrastructure (PKI)’ as a comprehensive framework used to assign and verify user identity through digital certificates, for secure digital communications. The entire PKI framework relies on asymmetric cryptography- the use of a public and private key pair to encrypt and decrypt data, respectively.PKI allows us to associate identities with particular key pairs. Although the public key can be visible to anyone on the network, only the entity with the corresponding private key can access specific features or information.In a supply chain, PKI combines digital certificates and asymmetric cryptography to establish trust and ensure integrity.  The public keys are embedded in a digital certificate, which authenticates the user or device communicating across the network.Certificate Authority (CA): The Certificate Authority is a trusted entity that issues digital certificates to participants in the supply chain, e.g., developers, analytics providers, payment gateways, cloud providers, etc.Certificate: Digital certificates are cryptographic credentials, issued and signed by the CA, used to verify the identity of and secure communication among supply chain actors. They typically include public keys and identity details, accessible upon request.Registration Authority (RA): The RA ensures that only authorized participants can obtain a digital certificate, thus enhancing security within the supply chain. The CA can double as the registration authority, although trusted third-party services are just as efficient.Certificate database: This PKI component is a secure repository or location that stores issued digital certificates, alongside their metadata, i.e., public keys, revocation status, and validity details.Certificate policy: This is a formal document outlining the procedures and requirements governing the issuance, usage and management of digital certificates within the supply chain.Central directory: The central directory is a public repository where cryptographic keys, digital certificates and Certificate Revocation Lists(CRLs) are indexed and stored. It enables anyone in the ecosystem to authenticate a digital signature and encrypt data to a specific key owner.Traditional Supply Chain Systems (Web2) vs. Web3-Driven Supply ChainsWeb2-based supply chains are centralized; participants rely on Certificate Authorities to verify other actors and establish trust. Moreover, digital certificates and cryptographic keys are stored in central directories, leaving room for supply chain attacks in the absence of robust security measures.Another feature of Web2-based supply chains is the opacity around certificate issuance and revocation. There’s no universal metric to determine eligibility for certificate issuance. Instead, entities must operate on a trust assumption that the CA has properly vetted the requester.Oftentimes, delayed updates and limited visibility associated with Certificate Revocation Lists (CRLs) may result in revoked certificates appearing valid to related devices or applications. This can impact supply chain integrity, due to unauthorized access, tampered goods, compliance issues and a loss of trust. In 2024, Google delisted Entrust (a formerly reputable Certificate Authority), from its Chrome Root Program due to malfunctions in its certificate issuance and revocation operations. A few months prior, Entrust admitted to misissuing over 26,000 digital certificates and failing to revoke them within the revocation timeline outlined by the Certificate Authority/Browser Forum.Web3-driven supply chains, on the other hand, leverage decentralized systems, smart contracts, and pseudonymous transactions to create a trustless, transparent, and secure ecosystem. Unlike traditional supply chains that rely on centralized authorities, Web3 enables each participant to interact directly on a shared blockchain, reducing intermediaries and single points of failure.In addition, PKI data (i.e. public keys and certificates) is stored immutably on a blockchain, making them nearly tamper-proof, while being easily accessible for verification. Together, these features make Web3 supply chains more resilient and trustworthy than ttheir raditional counterparts.From PKI to DPKI: Strengthening Supply Chain Integrity in Web3In Web3-driven systems, PKI implementation shifts from traditional Certificate Authorities (CAs) to decentralized or distributed models that align with Web3 principles, hence the term—Decentralized Public-Key Infrastructure (DPKI).The idea behind it is simple: enable tamper-proof verification of supply chain data and participants, without relying on a centralized database.Here's how DPKI enhances supply chain integrity in a Web3 ecosystem:Decentralized trust models manage authentication and verificationRather than a centralized entity, DPKI relies on a web of trust—a network of on-chain participants who collectively verify and authenticate information. Each supplier on-chain uses a decentralized identifier (DID), which functions as a unique digital signature, to prove authenticity, access data and sign transactions.Vendors interacting in a decentralized supply network use their DIDs to access proprietary data, verify purchase orders, and access secure channels. Similarly, a product within the chain can be assigned a DID, enabling participants to verify its origin and authenticity at every step.Smart contracts ensure transparency and data integritySmart contracts are self-executing programs stored on a blockchain that automatically trigger specific actions once preset conditions are met. They automate various processes within a supply chain, such as processing payments, issuing tickets, or approving shipments. All interactions with the contract are recorded on the blockchain, creating a permanent and tamper-proof audit trail. This record allows stakeholders to trace:The journey of goods through the supply chain.Compliance with standards at each stage.Discrepancies back to their source.Automation through digital contracts reduces the risk of fraud, human error, and compliance issues.Blockchain-powered scalabilityBy eliminating reliance on a Certificate Authority and other intermediaries, decentralized public key infrastructure offers more scalability to Web3-based supply chains. In traditional PKI, managing certificates across a complex supply chain can leave the supply network vulnerable to single points of failure. Moreover, scaling may require the involvement of multiple Certificate Authorities, likely resulting in delays or bottlenecks around certificate issuance and revocation. This approach is resource-intensive and may be unrealistic for global supply chains involving numerous entities.In contrast, Web3-driven supply chains leverage blockchain as a trust anchor, enabling a distributed system where records and identities are verifiable by all participants on the chain. The result is a more efficient, scalable infrastructure, tailored to the complexity of modern supply chains.Advantages of DPKI to (Web3) Supply ChainsEliminates Central Authority Risks: No single entity can compromise the supply chain.Self-Sovereign Identity: Supply chain participants control their cryptographic identities, reducing the overhead associated with traditional certificate issuance and management.Enhanced Transparency: All actions (such as key creation and revocation), and transactions are publicly recorded, promoting trust and accountability across the chain.Efficiency: Automation through smart contracts streamlines processes such as inspections and approvals, saving time and resources.Enhanced Security: Cryptographic signatures prevent data tampering and fraud, thus protecting data authenticity.Improved Scalability: Participants can manage their keys and verify others without bottlenecks from centralized authorities.Integrating blockchain: a path to modernizing supply chainsThe shift from centralized to decentralized systems is no longer a futuristic concept, but a growing reality for Web2 companies looking to modernize their supply chains. Companies like IBM, with its blockchain-powered Food Trust, and  De Beers’ Tracr, used for tracking diamonds from source to store, demonstrate how blockchain and DPKI can integrate seamlessly into existing supply chain models to enhance transparency and trust.Harnessing the benefits of decentralized supply systems doesn't require a sudden overhaul of the existing supply chain. Web2 companies can adopt an incremental approach—beginning by identifying use cases—and testing DPKI in targeted areas, before gradually scaling the integration across the entire supply chain.]]></content:encoded></item><item><title>Landmark EU Tech Rules Holding Back Innovation, Google Says</title><link>https://tech.slashdot.org/story/25/07/01/1811254/landmark-eu-tech-rules-holding-back-innovation-google-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google will tell European Union antitrust regulators Tuesday that the bloc's Digital Markets Act is stifling innovation and harming European users and businesses. The tech giant faces charges under the DMA for allegedly favoring its own services like Google Shopping, Google Hotels, and Google Flights over competitors. Potential fines could reach 10% of Google's global annual revenue. 

Google lawyer Clare Kelly will address a European Commission workshop, arguing that compliance changes have forced Europeans to pay more for travel tickets while airlines, hotels, and restaurants report losing up to 30% of direct booking traffic.]]></content:encoded></item><item><title>Complete Gemini CLI Setup Guide for Your Terminal</title><link>https://hackernoon.com/complete-gemini-cli-setup-guide-for-your-terminal?source=rss</link><author>Vladislav Guzey</author><category>tech</category><pubDate>Tue, 1 Jul 2025 21:19:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Have you ever wished for an AI assistant right inside your terminal window? Well, your dream may have just come true because Google just released Gemini CLI. In this tutorial, I’m going to show you everything you need to know about this new open-source AI agent. We’ll cover how to use it, the pricing, and some useful tips and tricks. So, if you’re ready, let’s get started! ;)Gemini CLI is a free and open-source AI agent that works directly in your terminal. This powerful tool brings Google’s Gemini models to your command line, allowing for natural language interaction to get work done. You can ask it to:You don’t have to constantly switch between a web app and your terminal. And the best part? It’s , with no complicated setup.Getting Started with Gemini CLIStep 1: Install Gemini CLI on LinuxYou can install  on , , and . All the setup we will do inside the terminal. I am using Linux, but for Mac and Windows, the commands are almost the same, so you can follow my steps.To get started, make sure you have Node.js version 18 or higher. You can check this by running:If you don’t have it, use the following command to install it:sudo apt update && sudo apt install nodejs npm
Then, I installed Gemini CLI globally with:npm install -g @google/gemini-cli
If you don’t want to install it globally, you can also use:npx https://github.com/google-gemini/gemini-cli
After installing, just type:After that, you need to log in with your personal Google account.This gives you access to a free Gemini Code Assist license, which includes:Access to Gemini 2.5 Pro.A massive 1 million token context window.60 model requests per minute.1,000 model requests per day at no charge.Now you are ready to start asking questions and running tasks. You can ask the Agent to create a project, fix the bugs, explain the code in specific files, etc. Ensure that you run the agent within your project folder.> What does the file index.js do?
It read the file, analyzed it, and gave a clear explanation.> Add error handling to index.js
You can also run shell commands directly by using , like this:Creating a Simple To-Do App with Google CLINow that we’re all set up, let’s ask the AI to create a simple to-do application using HTML, CSS, and JavaScript. I will type “create a simple to-do app using simple js and html” into the Gemini CLI.” Watch the video to see the step-by-step process and the result.Gemini CLI has some handy built-in tools. You can use commands like: (). Lists files and folders in a directory—just like the shell  command. (). Reads the full content of a single file, useful for summaries or analysis. (). Reads multiple files at once, typically matching a glob pattern (e.g., all  files) (). Searches for files by pattern (e.g., find all  across your project). (). Searches within files for text, like finding all  comments. (). Applies code changes via diffs. Gemini previews edits and asks for approval before applying them. (). Creates new files (for example, ) with user-provided content. (). Runs commands you prefix with  (e.g., ) directly in the terminal . (). Fetches content from the web (HTML or JSON), enabling Gemini to analyze external data. (). Performs a Google search to ground responses with real-world information (e.g., explanation for an error). (). Stores facts or preferences during a session (like “I prefer async/await”) to improve consistencyTo see all available tools, you can use the  command.You can add specific instructions for the AI for a particular project by creating a  file in your project’s root directory. Inside this file, you can define project rules, code styles, and the tools the agent should use. This ensures that the generated code is consistent with your project’s standards.Google CLI MCP IntegrationFor most day-to-day uses, the built-in tools will suffice. But what if you want Gemini CLI to do something very domain-specific, like interact with specific APIs or use a specialized model (say an image generator or a security analysis tool)? This is where MCP (Model Context Protocol) comes in.MCP is essentially an open standard that allows developers to add new tools/abilities to the AI by running a server that the CLI can communicate with. In Gemini CLI, you can configure “MCP servers” in a JSON settings file, and the CLI will treat those as additional tools it can use.How to Set Up the MCP Server in Google CLIAs an example, I am going to show you how to set up the GitHub MCP server in Gemini CLI.Inside your project folder, create a folder by using the command:mkdir -p .gemini && touch .gemini/settings.json
Inside the file, add the following code:{  
  "mcpServers": {  
    "github": {  
      "command": "npx",  
      "args": ["-y", "@modelcontextprotocol/server-github"],  
      "env": { "GITHUB_PERSONAL_ACCESS_TOKEN": "[YOUR-TOKEN]" }  
    }  
  }  
}
After that  , from the Gemini CLI, and then reopen it.Write  command, and you will see a list of .Now you, Agent, can interact with GitHub. That simple! :)You can try it free for personal usage, but there is also a paid version that is billed based on token usage.: Free with Google account: Up to  and Great for individual developers and small-scale useUse your own  for higher usageBilled based on tokens consumed (model and usage dependent)Available through Gemini Code Assist Standard or Enterprise plansIncludes advanced features like governance, audit logs, and shared quotasAs you can see, Gemini CLI is a really powerful tool with a lot of potential. I’m excited to see how I’ll be using it in my daily workflow.If you write code, debug things, or manage files often, this tool is worth checking out.If you have any feedback, please share it in the comments below. ;)]]></content:encoded></item><item><title>AMD Preps Some Compute Driver Fixes For Polaris &amp; Hawaii Era GPUs With Linux 6.17</title><link>https://www.phoronix.com/news/Linux-6.17-AMDGPU</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:47:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD today submitted their initial batch of "new stuff" for queuing into DRM-Next of their kernel graphics/compute driver changes they have prepared for the upcoming Linux 6.17 cycle opening in a few weeks...]]></content:encoded></item><item><title>How Trusted Execution Environments Power Scalable, Private Smart Contracts</title><link>https://hackernoon.com/how-trusted-execution-environments-power-scalable-private-smart-contracts?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:45:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The layer-two solution is a straightforward approach that combines the TEE and blockchain to provide smart contracts with confidentiality while keeping scalability. In such systems, the operations of smart contracts are decoupled from their underlying blockchain systems. The smart contracts are executed in an independent layer outside blockchain systems.\
 In a general layer-two solution, the blockchain is used as a dispute resolution layer. The smart contract is executed outside the blockchain, making TEEs act as an agent between users and blockchain systems. Suppose that a user aims to use a private contract. She first needs to compile the original contract code, push binary codes to a TEE, and then upload execution results to the public ledger. As illustrated in Fig.4, we extract a generic data flow as follows. A user sends the encrypted input data to a TEE-powered node. Then, the TEE decrypts the input data and executes the contract. After that, the encrypted execution results are sent to the blockchain platform for verification and storage. Finally, the user fetches and decrypts the blockchain-confirmed results.\
Privacy-preserving property. The  is an essential property. In layer-two systems, such as [46], [68], [69], the contract computations run inside Intel SGX enclaves, while TZ4Fabric [44] moves contract executions into ARM Trusted Zone. Since the contract state-transition process happens inside TEEs, any intermediate states remain invisible to the outside. Meanwhile, to achieve the full lifecycle security for a smart contract, the input sent to a TEE and the output returned from this TEE are also required to be encrypted. For example, in ShadowEth [68], PDOs [46], Phala [69] and Hybridchain [41], the contract invocation arguments are encrypted with the TEE public key. They can only be decrypted within the enclave. Also, before transferring execution results to the blockchain (or users), the intermediate (or final) states in an enclave are encrypted. Some variants also enhance the privacy-preserving properties from other aspects. In Phala [69], only authorized queries to the contract will be answered. The smart contract source codes in ShadowEth [68] are hidden during the procedures of deployment and synchronization. This further reduces the possibility of data leakage in subsequent contract executions. Considering a fixed address may expose the user who has invoked the contract, PDOs [46] also allows the user to use pseudonym addresses for submitting a transaction (including TEE outputs) to the blockchain.\
Blockchain intrinsic feature. ShadowEth [68] and Taxa [70] introduce an external distributed service to manage the contracts, achieving the properties of code immutability, high availability and . Meanwhile, layer-two systems satisfy state consistency for reasons that the encrypted states of contracts in different blockchain nodes will eventually get consistent when reaching a successful agreement. Intuitively, the contracts deployed in layer-two systems should retain the features given by original blockchains. However, some fundamental properties are lost when using layer-two solutions. For example, most layer-two systems lose contract interoperability since each contract is executed in different machines. Among all the evaluated systems, only Phala [69] identifies this issue and proposes a command query responsibility segregation architecture to ensure certain interoperability. Also, public verifiability is a crucial property for the blockchain since it allows each contract invocation, and contract execution to be publicly verifiable. Unfortunately, contracts are executed in TEEs so that the outputs are encrypted. To check whether the TEE has executed contracts following loaded contract specifications is a non-trivial task.\
 An attacker may control the network between users and TEE hosts. Meanwhile, TEEs are assumed to always produce correct results, and the smart contracts inside TEEs cannot deviate from their specifications. The main difference compared with the assumption of layer-one systems is that an adversary can observe the network activities between the TEE interfaces and active blockchain nodes.\
 Several layer-two solutions adopt incentive or punishment mechanisms to encourage TEE hosts to provide a stable and secure environment for executing confidential contracts. For example, Fastkitten [43] and Erdstall [75], [76] propose  transactions, in which a host will be punished if its malicious behavior has been identified. In particular, if the TEE execution is aborted, the host will be charged according to previous deposits. In Taxa [70], every node can identify any faulty nodes with reliable proofs for executing further economic punishment. On another route, TEE hosts in Phala [69] will get paid by providing their computing resources to users. Similarly, the remuneration in ShadowEth [68] will be transferred to TEE hosts who execute private contracts. These mechanisms can effectively prevent malicious TEE hosts from an economic aspect. However, they are powerless against external threats. An adversary may directly terminate a TEE host at any time. Even worse, the TEE provides users with an open interface that is vulnerable to DoS [77] or single-point attack. To overcome such issues and achieve fault tolerance, different methods are proposed. Fastkitten provides low-level fault tolerance by periodically saving an encrypted snapshot of current states in enclaves. If the enclave fails, the TEE host can instantiate a new enclave and restart the computation starting from the encrypted snapshot. Similarly, Taxa [70] stores a session file for maintaining and recovering user’s requests. However, a malicious attacker may directly terminate the TEE host, and Fastkitten does not tolerate such host failures. Another technical route is to maintain a secure network. ShadowEth maintains a group of TEE nodes to ensure consistency via a Paxos-like [78] algorithm. Taxa adopts TEE-enabled computing nodes powered by a PBFT-derived PoS [79] algorithm. Any node in the network has the same responsibility to privately execute smart contracts and transfer execution results to the blockchain. However, this brings additional authentication issues. A TEE host must be carefully authenticated to ensure her TEE capability when joining an external network.\
Meanwhile, the systems PDOs [46], Phala [69], Ekiden [42] and COMMITEE [73] introduce an expendable and interchangeable solution. TEEs are stateless: any particular TEE can be easily replaced once it has clashed or finished its task. Unfortunately, these solutions are along with new challenges. Firstly, even if TEEs are changeable, detecting a compromised TEE is still difficult. For instance, PDOs can re-execute a method multiple times for the verification. Given the same input parameters to different TEEs, TEEs are believed to work securely only if their outcomes match. Then, the outputs of enclaves are allowed to commit to the blockchain. COMMITEE adopts  TEE host mechanism. If the master TEE host is proved to be malicious, a backup TEE host will continue to work without communications to the master TEE host. Nevertheless, this model increases the attack interface and makes the whole system vulnerable. Secondly, TEE hosts are stateless. That means, to ensure an exceptional execution is recoverable, any persistent state must be stored in the blockchain or a trusted third party (TTP). However, for a non-deterministic blockchain system such as Ethereum (PoS version) [2], verifying whether an item has been stored on the blockchain is a non-trivial task. Meanwhile, storing data in TTPs may lead to the single-point failure, which goes against the blockchain’s real intention.\
 A contract runs inside TEE, and heavily depends on remote attestation service. The SGX-supported blockchain systems including PDOs [46], Fastkitten [43], ShadowEth [68], Phala [69] and Ekiden [42] assume that Intel Attestation Service (IAS) is trusted. IAS can correctly and completely report whether a certain output with cryptographic material ( [80]) is produced by SGX-enabled hardware. However, IAS might be compromised, posing a risk to these architectures. A compromised or hijacked remote attestation service may maliciously report an attestation with the wrong cryptographic material that does not belong to its corresponding TEE hardware, breaking the promised security. Meanwhile, a centralized service might be crashed, causing the leakage of private states. Unfortunately, none of layer-two schemes consider these risks in designs or implementations.\
As discussed, current TEE implementations have memory limitations for confidential executions. If the memory usage exceeds the threshold, it may confront significant performance and security issues [81]. Hybridchain [41] optimizes the storage by maintaining transaction records outside Intel SGX. Meanwhile, TZ4Fabric [44] minimizes TCB by avoiding all the executions inside TEEs. However, these approaches increase the implementation complexity. A well-known fact is that a TEE is vulnerable to physical vulnerabilities [57]. Unfortunately, very few layer-two solutions provide remedial measures to reduce the risk of being attacked.\
 A poorly-written contract might deviate from designated functionalities and further leak the secret information. This part discusses the potential pitfalls and remedies when deploying contracts.\
In original smart contract systems, gas mechanism is a powerful tool to prevent  attacks [2]. Since the layertwo systems execute smart contract outside the blockchain, a similar mechanism must be considered. Fastkitten [43] and Hybridchain [41] protect against such attacks by using the  mechanism. Limitations are firstly defined on the maximum amount of execution steps that allow to perform inside a TEE per round. Then, TEE monitors smart contract operations. If the number of execution steps exceeds a predefined threshold, the enclave will terminate executions. ShadowEth [68] combines a timeout mechanism with a  mechanism. Similar to the gas mechanism in Ethereum [2], TEE hosts can still gain remuneration even if a contract exits after timeout since they provide sufficient computing power. These mechanisms effectively protect against endless loops and denial-of-service (DoS) launched by external attackers.\
The TEE itself lacks self-awareness of input data, since it cannot distinguish which state is fresh. A lack of input data authentication makes the system vulnerable to the rollback attack [82], [59]. A malicious user may attempt to invoke the confidential contract many times to seek the leaked secret information. Authentication of the user’s identity is helpful to prevent this attack. However, none layer-two solution provides these remedies for these potential pitfalls. On the other hand, the TEE input may come from a non-deterministic blockchain system [83], [84], in which deciding whether an input has been confirmed is tricky. Fastkitten [43] and COMMITEE [73] mitigate this issue by using a  mechanism. As for TEE output conflicts, Ekiden [42] uses a probabilistic proofof-publication protocol to avoid the ambiguous input.\
After the invocation of a private contract, the outputs returned from TEEs are uploaded on-chain for the final confirmation. But a malicious TEE host may send an exceptional result to the blockchain. Even worse, two hosts may publish different updates towards the same contract simultaneously. To prevent such malicious publications and to evade conflicts, PDOs [46] depends on Coordination and Commit Log (CCL) to manage the synchronization in the execution of interacting contracts and enables a contract owner to decide on selecting the enclave for contract executions, which effectively avoid conflicts. Phala [69] adopts an event sourcing command query responsibility segregation architecture to scale up and avoid conflicts, in which the write operations are recorded as events and read operations can be served by the current view of states. Again, these solutions contradict the property of decentralization. Ekiden [42] and ShadowEth [68] rely on the blockchain to resolve conflicts resulting from concurrency. In particular, ShadowEth [68] requires a worker to specify the version number with a timestamp when pushing data to the blockchain. Even miners accept different responses at first, they will eventually reach an agreement by comparing version number and the timestamp, with the help of the consensus procedure. Yet, such an approach is inefficient, especially in non-deterministic blockchain systems.\
 PDOs [46] uses a key provisioning service to distribute private keys. The drawback is obvious: A compromised provisioning service could make the entire system fail. To increase the robustness of a private key, Ekiden [42] designs a distributed key generation (DKG) [85] protocol using the secret sharing scheme [86]. Even if one key manager is compromised, an adversary cannot obtain the entire key. However, this solution does not completely solve the key leakage issue. The final keys are assembled and replicated among all end-TEEs. If an adversary compromises an end-TEE, exposing all the contract state becomes a trivial task. The key rotation technology, adopted by Ekiden [42], Fastkitten [43], Phala [69] partially solves the above issue by providing a short-term key in every epoch. An adversary cannot corrupt a future or previous committed state, which minimizes the possibility of key exposure to attackers and further helps the layer-two system to achieve forward secrecy. Also, layer-two projects such as COMMITEE [73] mitigate these key issues by providing each TEE per secret key. Even if a certain TEE’s private key were stolen, this only would affect the smart contract running on that compromised TEE. Furthermore, Phala Network [69], equips each contract with an asymmetric key called the , which also enhances the key security to a certain degree.\
The layer-two solution decreases computational burden and avoids latency by decoupling the smart contract executions from consensus mechanisms. The solution merely puts the execution results on-chain rather than all processing states. Meanwhile, the layer-two solution does not require a dedicated public ledger, meaning that such a solution can smoothly\
integrate with existing public blockchain platforms. Unfortunately, this method also brings security and functionality challenges when delegating the task of contract management to an external TEE layer.\
Firstly, the layer-two solution complexifies contract data management. The contracts that are deployed outside the blockchain require an external execution/storage party. A malicious storage maintainer may reject to provide the service, while a malicious host may abort TEE executions, terminate enclaves or delay/drop messages. Even an honest host might accidentally lose states in a power cycle. To solve the centralization issue and tolerate host failures, many countermeasures such as the TEE network, stateless TEEs and punishment mechanisms, are proposed. However, these solutions are not effortless, inevitably making the system complicated and hard to implement in practice.\
Secondly, the layer-two solution increases the attack surface and thus becomes vulnerable to rollback attacks. There is a high probability that an adversary node can revert transactions where temporary forks, representing inconsistent blockchain views, are allowed in blockchain systems with probabilistic consensus (e.g., PoW). Since TEEs provide no guarantee on verification of input data; they cannot distinguish whether an input state is fresh or not. An attacker may offer stale states to resume a TEE’s execution. This enables rollback attacks against randomized TEEs programs. Even worse, plugging up these loopholes needs much effort.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Layer-One Confidential Smart Contracts: Architecture, Threats, and Tradeoffs</title><link>https://hackernoon.com/layer-one-confidential-smart-contracts-architecture-threats-and-tradeoffs?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The layer-one approach enables blockchain nodes to run contracts in their isolated areas, as well as conducting the consensus (see Fig. 3). This approach combines the consensus procedure and state execution, either in terms of logically or physically. The reason why we call this method layer-one is that all executions are completed in the same layer of the blockchain network. The key to such an approach is to equip every blockchain node with a TEE. Indeed, this requires more integration efforts, but also comes with several advantages. The smart contract can implement stateful functionalities that receive arguments and update states instantly. In particular, a smart contract can directly access the ledger data stored in a local disk, greatly saving time often wasted in the interactive network communications.\
 In a layer-one execution model, the operation of ledger update (consensus) and state transition (contract execution) are coupled. Like Ethereum [2], smart contracts run inside blockchain nodes. Assume that a user plans to use the private contract; she only needs to upload data to the blockchain service and wait for results. The remaining procedures are completed by TEE-assisted distributed nodes. A TEE in these nodes acts as a black box for data processing and output targeted results without the data leakage. This approach greatly improves convenience for users due to its easy access and management. As illustrated in Fig.3, a generic data flow goes as follows: A contract creator deploys the code into blockchain. Then, a user sends the transaction with an encrypted argument to an arbitrary blockchain node. Her request is confidentially executed inside TEEs in this node and output encrypted state. Then, the consensus algorithm in this node broadcasts the encrypted results to peers. After the encrypted results are confirmed by other blockchain nodes, users fetch on-chain results and decrypt them for the plaintext.\
Privacy-preserving property. This property indicates that contract states and the procedure of contract executions are hidden from the public. To achieve privacy, layer-one systems execute these confidential contracts inside TEEs in every distributed node. CCF [45], Fabric [60] and CONFIDE [37] follow this straightforward design where confidential contracts are loaded to the TEE of each consensus node, which encrypts both the inputs and outputs of contract states, together with their operating logic and predefined rules. Enigma[1] [61] introduces the secret network and allows users to submit their transactions together with encrypted data to miners. We also notice that current layer-one solutions only focus on internal procedures rather than the linkability and anonymity of addresses and transactions. This indicates that confidential smart contracts only protect the contents that have been loaded into TEEs, while the data that relates to external users is out of the scope of this work.\
Blockchain intrinsic feature. The layer-one systems inherit most of the features empowered by blockchain. More precisely, the properties of code immutability, high availability, explicit invocation, decentralized execution, automatic execution and  remain the same because basic contract executions still rely on their underlying blockchain systems. Also, the property of (confidential)  in Enigma [61], CCF [45] and Fabric [60] remains unchanged. The states and executions from these systems follow the procedures of online consensus processes. Then, the returned results from inside TEEs still require to be confirmed on-chain. This makes their actions effectively perform the same functions as a normal smart contract, except for that the contents of states are transmitted from plaintext to ciphertext. In contrast, the property of contract interoperability is lost since the contracts are executed in isolated TEEs. This isolation requires additional communications such as dispatching keys through the remote attestation service, bringing much complexity.\
The layer-one solution encapsulates TEE computations into blockchain nodes. Every node in the network has to take responsibility for conducting confidential executions and performing the consensus. The design to coordinate TEEs and consensus within the same physical space brings many distinguished features. We start the analysis from their threat model and then dive into each component of these systems.\
 Users in the layer-one approach are assumed to be unreliable. They may have mistakes unconsciously, like dropping messages or mis-sending transactions. Even worse, a malicious user can arbitrarily behave like faking messages, identities, or compromising other nodes. As for TEE hosts, an external attacker can monitor, eavesdrop or even compromise part of involved TEE hosts among these distributed nodes, but cannot block all traffic transmitted in communication channels. Subsequently, a TEE is supposed to work in a good condition: The attestation service is trusted, and the cryptographic primitives used inside TEEs are secure. Meanwhile, as for the blockchain network, the basic systems (ledgers) are assumed to be robust [62], [63], [64]. When running the consensus, the majority (might be two-third, depends on specific consensus algorithms) of nodes are assumed to be honest [65]. Also, forging smart contract codes or states will happen in honest blockchain nodes with a negligible possibility. Based on that, we analyse securities from four aspects.\
 Firstly, we focus on the security of TEE hosts, or equally, individual nodes that run TEEs. Unlike classical blockchain systems, there are no explicit incentive or punishment mechanisms in this solution. This is easy to understand: A node with malicious behaviors will be instantly moved out of the committee and replaced by a new honest participant. Meanwhile, due to the fact that CCF [45] and Enigma [61] rely on Tendermint (a BFT variant) consensus algorithm, they can tolerate at most one-third of TEE Byzantine nodes. But the sacrifice is the increased difficulty in synchronization, especially when every node has to establish a secure channel for communications of distributed TEEs. In layer-one systems, host authentication is necessary. The node who wants to join the committee has to obtain permission from communities by proving her TEE capability. For instance, CONFIDE [37] builds a mutual authenticated protocol (MAP) (supported by SGX remote attestation techniques [66]) among blockchain nodes. Any nodes joining in the network have to pass the authentication via MAP.\
 Then, we analyse TEE-level securities. Attestation service is an essential part of TEE techniques. Systems in the layer-one solution still require such services for network connection and verification. Enigma [61], Fabric [60] and CCF [45] follow the original attestation mechanism with an implicit rule: The Intel Attestation Service (IAS) should be reliable. However, this cannot be guaranteed in the case of IAS being comprised. In contrast, CONFIDE [37] utilizes a customized Decentralized Attestation Service to provide the robust authentication. As for memory limitations, layer-one systems load contract executions and consensus algorithms into one TEE-embedded node, causing an increase in disk and memory usage of individual nodes. Once the usage of TEE memory runs over the predefined settings, a decrease in the performance is inevitable [34]. This may further cause an unpredictably severe result like system crash-down. Fortunately, Fabric [60] mitigates such issues by separating the operations into two types (execution and ordering) and delays the transaction- procedures after state-. Among them, only the state- parts are processed inside TEEs. This decreases computation complexity and limits the memory usage to a suitable range. Physical attacks like the Spectre and Meltdown vulnerabilities [57] are intrinsic design pitfalls that may occur inside the TEE kernel. To our knowledge, no layerone solutions mention them or provide the remedies.\
TEE program security. Next, we focus on the program-level security. Issues like overburdening may frequently happen, especially when a malicious developer deploys a contract with infinite loop logic. Unlike using the gas mechanism in Ethereum [2], systems in the layer-one model constrain their running programs by the  mechanism. It sets a threshold, namely, a suitable range of time that allows processing contract operations. When exceeding the timebound, the system will abort under-processing states and restart a new round. As for the flaw detection, no formal techniques or verification tools, based on our observation, have been applied to layer-one systems. This gap needs further exploration. Similar to the previous discussion, the properties of data verification (covering both user data authenticity and blockchain data confirmation) and output conflicts are guaranteed by their underlying consensus algorithms. Each time performing the consensus, these properties are automatically checked. For instance, Enigma [61] relies on trusted validators, who equip with TEEs to conduct the verification procedure. Such validators maintain both the privacy of executions inside TEEs and the consistency of states that connects to peers. Once conflicts occur, validators will quickly make decisions on a block and remove another conflicting block. Fabric [60] performs such a process inside TEEs among committee nodes and then submits the passed results to its abstract ordering service. This service prevents forks caused by conflicting states, as well as proving a fact that: All executed messages are valid and integral once reaching the consensus agreement. It should be noted that, successful consensus procedures can merely guarantee the integrity of transactions and states, rather than linkability and authenticity that relates to physical entities.\
 Lastly, we move to the aspect of TEE key management. In layer-one systems, the key management service takes over the task of creating and managing keys for activities like attestation, verification, encryption, etc. To achieve the key management service among distributed nodes, several types of designs have been proposed. CCF [45] relies on the public key infrastructure (PKI) for certificate issuance, management, and revocation. It creates key pairs and dispatches them to every participated TEE, where each TEE holder is authenticated by the certificate. Similarly, Fabric [60] adopts an admin peer to provision the specific decryption key to  during bootstrapping. Enigma [61] setups an independent key management component to reply to the requests for encryption. Such designs help to simplify complex management procedures, as well as providing distinguishable keys for each TEE. However, these independent key management services lead to centralization even they are maintained by a group of nodes in the committee. CONFIDE [37] mitigates this issue by proposing a decentralized key management protocol. Two types of keys are involved in this protocol: the  used to decrypt confidential transactions from clients and the  used for state encryption/decryption between the confidential engine and storage service.\
The layer-one solution provides a highly integrated approach towards confidential smart contracts.\
The layer-one solution provides a consistent interface for users without changing the customer’s habits transformed from non-TEE blockchain systems. A user can use the layer-one system by directly interacting with the blockchain interface, without considering cumbersome and complicated operations between the TEE and blockchain. However, the layer-one solution still confronts several common disadvantages.\
Minimizing the size of Trusted Computing Base (TCB) contributes to the TEE security [67]. In particular, a small TCB has fewer errors and can reduce attack surfaces. However, complicated interactive operations for contract execution and consensus agreement in the L1 solution greatly increase the size of TCB. Meanwhile, TEE products have limited secure memory. For example, in the current implementation of Intel SGX [35], the enclave page caches are constrained to 128 MB, and only 93 MB of those is available for applications, which limits the concurrent execution.\
Furthermore, the layer-one solution lacks compatibility, which means being incompatible with existing blockchain systems. The solution integrates the consensus procedure and the contract execution into the same blockchain node, requiring every node having to equip a TEE hardware. Nevertheless, this requirement is difficult to be fulfilled in a public blockchain while already in use (e.g., Ethereum [2]).(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.[1] Enigma’s secret network consists of a list of secret nodes equipped with TEE, which is categorised as a layer-one solution in the context of our definition (Sec.III-A). We also note that such a secret network can be regarded as a layer-two solution in the traditional classifications in terms of Ethereum, namely, either on-Ethereum chain (L1) or off-Ethereum chain (L2).]]></content:encoded></item><item><title>What Most Blockchain Devs Get Wrong About TEE Security and Smart Contract Privacy</title><link>https://hackernoon.com/what-most-blockchain-devs-get-wrong-about-tee-security-and-smart-contract-privacy?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[III. SYSTEMATIZATION METHODOLOGYTo find common aspects (e.g., offered functionality, design model, adversary model), we extract recurring design patterns from publicly available publications and projects, focusing on systematization and evaluation of desirable properties (the main target of TCSC) and potential pitfalls of underlying systems. Our systematization methodology follows the idea in [52]: classification and evaluation. We firstly make a classification for the current systems and then define a framework to evaluate them. Details are presented as below.\
\
We classify the existing systems into two main categories: layer-one solution (L1) and layer-two solution (L2). The layer-one solution executes the contract inside a TEE in the blockchain, requiring every blockchain node to equip a TEE. Instead, the layer-two solution decouples contract computations from the blockchain. It performs most of the smart contract computations off-chain. For a clear understanding, we make a comparison of the original blockchain (e.g., Ethereum), L1 solution, L2 solution. As in Tab.II, Ethereum runs smart contracts (in EVM) and consensus procedures in the same machine of distributed nodes. All the contract and transaction operations are publicly verifiable due to their total transparency. The layer-one solution performs such operations (contract execution and consensus) in the same machine, but contract operations are separate from consensus procedures. In contrast, the layer-two solution makes both of them operate independently. Contracts are executed outside the blockchain network, while the consensus happens inside each node.\
Ideally, moving smart contract executions into TEEs brings additional privacy as well as maintaining the original benefits of blockchain systems. Therefore, we have identified the desirable properties in two main categories: privacy-preserving property and .\
Privacy-preserving property. The property of confidentiality is the most distinguished feature in TCSC.\
A1. Specification hidden. The source code of a smart contract is hidden during the deployment and the subsequent synchronization and execution.\
 The inputs fed into a confidential smart contract are hidden from the public.\
 The outputs returned from a confidential smart contract should be kept private.\
 The execution procedure is hidden from unauthorized parties. An adversary cannot learn the operation knowledge inside a TEE.\
A5. Address unlinkability. The address pseudonymity does not entail strong privacy guarantees [53], [54]. This property prevents an adversary to learn the address linkability by observing users’ activities.\
 The contract caller’s identity (a user who invokes a smart contract) is hidden from an anonymity set [24] (see Appendix B).\
Blockchain intrinsic feature. TEE-assisted smart contracts inherit features given by original blockchain systems. We summarize these features as follows.\
 Once a contract is successfully deployed, its source code cannot be altered.\
A8. (Confidential) state consistency. Executions happening at a certain blockchain height will output the same result across different nodes.\
A9. Contract interoperability. A smart contract can call another contract and be called by others.\
 A smart contract is continuously accessible without the single point of failure.\
A11. Decentralized execution. A smart contract runs over the decentralized network.\
A12. Automatic execution. A smart contract can be automatically executed once conditions are satisfied.\
 Operations running on the smart contract will be charged with gas fees [2].\
A14. Explicit invocation. Each invocation will be formatted as a transaction and stored on blockchain.\
A15. Public verifiability. The procedure of contract execution and result are publicly verifiable.\
A16. Consensus verifiability. The consensus procedure on the (confidential) state is publicly verifiable.\
Essentially, all TCSC systems share the same principle: a TEE will handle the data from users. After that, encrypted data flows from the  The TEE plays a crucial role. Thus, this part defines a framework for evaluating underlying blockchain systems from four aspects: , and  This framework aims to identify potential design flaws and pitfalls based on the threat model and data workflow.\
 Our threat model mainly captures three types of attackers, which are stated as follows.\
T1. User adversary (active/passive). An attacker may control network between users and TEE host nodes.\
T2. TEE adversary (active/passive). An adversary may control TEE hosts or control the network between TEE and blockchain platforms.\
T3. Blockchain adversary (active/passive). An adversary may drop, modify and delay the blockchain messages. But the majority (or two-thirds) of the blockchain nodes are assumed to be honest.\
Note that adversaries are not necessarily exclusive. In some cases, adversaries in different types may collude.\
 This section defines four metrics regarding system security according to the data workflow: approaches to enhance the security of a TEE host, countermeasures to mitigate intrinsic TEE issues, methods to prevent program flaws or bugs inside TEEs, and solutions to clear up the TEE key security dilemma.\
 A TEE and its interaction with the external environment (e.g., with users or the blockchain) are operated and controlled by a host (such as a L1 blockchain node). A malicious host has abilities to abort the executions of a TEE, delay and modify inputs, or even drop any ingoing or outgoing messages. The following metrics discuss the approaches to improve the TEE host’s security.\
P1. Host punishment mechanism. Penalty mechanisms to reduce the risk of doing evil by a TEE host.\
P2. Host incentive mechanism. Incentive mechanisms to promote a TEE host to behave honestly.\
P3. Host fault tolerance. Solutions to make systems continually operate despite malfunctions or failures.\
 Methods to check the identity and the capability of a TEE host.\
 A TEE has inevitable weaknesses. For example, a TEE is vulnerable to side-channel attacks [55], [56]. These innate weaknesses directly pose severe challenges to the design and implementation of TEE-assisted contract systems. This part defines the defence approaches against these threats.\
P5. TEE attestation security. Methods to prevent TEE attestation service from being abnormally broken.\
P6. TEE memory limitation. Methods to optimize the memory size to prevent confidential data overflow.\
P7. TEE physical attacks. Approaches to prevent physical attacks, such as the Spectre vulnerability or the Meltdown vulnerability [57].\
 Approaches to provide a trusted timer when running a TEE.\
 Even a TEE is secure as assumed, a program bug may destroy the contract’s confidentiality in the real world. This part focuses on the measurements to prevent TEE programs from flaws or bugs.\
P9. Workload measurement. The workload measurement approach to prevent an infinite loop attack.\
 Formal techniques used for the modelling and verification of the source code of smart contracts to reduce the vulnerabilities.\
P11. User query restriction. The restriction on users’ queries, aiming to avoid data leakage resulting from differentialprivacy analysis [58].\
P12. Blockchain data confirmation. Methods for a TEE to check whether input data from blockchain has been confirmed to prevent the rollback attack [59].\
P13. TEE output conflicts. Methods to avoid multiple TEEs to produce a conflict result.\
 Various keys (cf. Appendix A) are involved in the contract execution, including TEE internal keys such as the attestation key and TEE service keys for state encryption/decryption. Since service keys directly affect the protection of contract states, the key security evaluation in this SoK mainly focuses on the generation, exchange, and storage of the TEE service key.\
P14. Distributed key protocol. The keys of confidential contracts are managed by a distributed protocol.\
P15. Key rotation protocol. The TEE replaces an old key with a fresh key for future contract encryption.\
P16. Independent contract key. Each contract is associated with a unique key, independent from the TEE.\
P17. Independent TEE key. Each TEE has a unique key, and different contracts share the same key.\
 The  shows a general view of the TCSC systems. Desirable property focuses on evaluating contract service provided by a TEE-assisted blockchain system.  describes the potential threats and system assumptions.  show the evaluating indicator for current TEE-assisted systems. In the following section IV-B and V-B, we attempt to answer the following questions: (i) What are the potential pitfalls in each security aspect; (ii) Do these pitfalls have significant security impacts; (iii) Do the designers/developers consider these pitfalls and accordingly come up with feasible remedies in their systems; (iv) What are the remedies and do they address above problems. Note that hundreds of TCSC systems have been proposed in both industry and academia. An exhaustive analysis is undesirable and infeasible. We only selected the projects that provide publicly accessible technical reports or academic papers.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>Four Key Steps to Confidential Smart Contract Execution</title><link>https://hackernoon.com/four-key-steps-to-confidential-smart-contract-execution?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This section gives a high-level description and offers a running example to illustrate how a typical confidential smart contract operates. From existing literature [42], [43], [41], [44], [45], [46], establishing a confidential smart contract mainly requires four steps, namely , ,  and  (see Fig. 1).\
From a bird’s eye view, a TCSC can be used as an ideal contract-based black box [47] with secrecy and correctness. This idea has been adopted by several advanced security protocols [48], [49]. We provide a secret e-voting example borrowed from Oasislabs [50].\
A TCSC can be well qualified for the role of decentralized vote manager in an e-voting system [17], [51]. Once a contract-based manager is deployed successfully, the voting logic is loaded into a TEE and corresponding secret keys are privately generated and stored inside TEEs. The encrypted state is then confirmed by the blockchain nodes. This offers the e-voting protocol with confidentiality, neutrality, auditability and accountability. Firstly, the voter’s input cu is encrypted, and intermediate parameters (e.g., mb) are privately processed through TEEs. External attackers cannot obtain the knowledge of sensitive information, and thus the confidentiality is achieved. Secondly, the predefined voting logic only occurs in the decentralized network when certain conditions are satisfied, bringing neutrality for the access control management. Thirdly, if a voter wants to vote for a candidate, she needs to in advance build a channel to the TEE and then send a transaction Tx to call the contract. Due to the protection of encrypted channels, transaction arguments are kept secret. Meanwhile, such invoking records in the form of transactions remain visible and will become immutable, ensuring the voting process accountable. Unfortunately, verifiability, as one of fundamental properties, performs not smooth in the context of encryption. Contracts that are executed inside TEEs make the execution procedures lack public verifiability. Only the nodes who install TEEs with correct corresponding keys can verify the correctness of contract executions. However, the metadata of the transaction Tx retains unencrypted, making it possible to verify the absence of double spending.(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.]]></content:encoded></item><item><title>To Fix Smart Contracts, Start With Their Secrets</title><link>https://hackernoon.com/to-fix-smart-contracts-start-with-their-secrets?source=rss</link><author>Blockchainize Any Technology</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:44:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;(3) Qi Wang, Southern University of Science and Technology, China;(4) David Galindo, University of Birmingham, United Kingdom;(5) Mark Ryan, University of Birmingham, United Kingdom.\
—The blockchain-based smart contract lacks privacy since the contract state and instruction code are exposed to the public. Combining smart-contract execution with Trusted Execution Environments (TEEs) provides an efficient solution, called TEE-assisted smart contracts, for protecting the confidentiality of contract states. However, the combination approaches are varied, and a systematic study is absent. Newly released systems may fail to draw upon the experience learned from existing protocols, such as repeating known design mistakes or applying TEE technology in insecure ways. In this paper, we first investigate and categorize the existing systems into two types: the layer-one solution and layer-two solution. Then, we establish an analysis framework to capture their common lights, covering the desired properties (for contract services), threat models, and security considerations (for underlying systems). Based on our taxonomy, we identify their ideal functionalities, and uncover the fundamental flaws and reason for the challenges in each specification’s design. We believe that this work would provide a guide for the development of TEE-assisted smart contracts, as well as a framework to evaluate future TEE-assisted confidential contract systems.Smart contract was originally introduced by Szabo [1] and further developed by Ethereum [2] in the blockchain systems. The blockchain-based smart contracts [3], [4], [5] adopt Turing-complete scripting languages to achieve complicated functionalities [6] and execute the predefined logic through state transition replication over consensus algorithms to realize final consistency. Smart contracts enable unfamiliar and distributed participants to fairly exchange without trusted third parties, and are further used to establish a uniform approach for developing decentralized applications (DApps [7]). However, blockchain-based smart contract lacks confidentiality. The state information and the instruction code are completely transparent [8], [9], [10]. Any states with their changes are publicly accessible and all users’ transaction data and contract variables are visible to external observers. Without privacy, building advanced DApps that rely on the user’s sensitive data becomes a challenge [11], [12], [13], [14]. For instance, smart contracts in Ethereum [2] cannot be directly applied to Vickrey auction [15], [16] or e-voting systems [17], [18], where the bid and vote require to be hidden from the public. Moreover, DApps without privacy protection might be prohibited by European Union because they go against the General Data Protection Regulation [19], [20]. Thus, the complete transparency of smart contracts constrains their wide adoption. Recently, researchers have explored many cryptographic solutions to solve these issues, including utilizing techniques of zero-knowledge proof (ZKP) [21], [22], [12], [23], [24], [25], homomorphic encryption (HE) [26] and secure multiparty computation (MPC) [27]. However, these approaches are merely applicable to applications requiring simple computations.\
Although various TCSC protocols have been proposed, newly released projects may fail to draw upon the experience learned from existing protocols, such as repeating known design mistakes or applying cryptography in insecure ways. For example, an absence of economic incentives will pose security risks and decrease the protocol’s stability. However, the recentproposed TCSC scheme Hybridchain [41] repeats similar pitfalls by simply combining the TEE with a permissioned blockchain network, omitting considerations on the miner’s incentive mechanism. The repeating of pitfalls comes from twofold. Firstly, in-the-wild projects differ from one to another, and a relatively unique model is absent, which narrows the developers’ vision. Meanwhile, a unified evaluation framework is missing, causing many security threats to be uncovered and resulting in considerable loss from applications underpinning the execution of confidential smart contracts. This paper aims to abstract a high-level framework to simply and clearly systematize knowledge on current TCSC schemes. We attempt to capture some commonalities among these projects regarding their features, properties, and potential security vulnerability. We believe that establishing evaluation criteria to measure features and identify problems and flaws of existing TCSC protocols will offer a good guide for industry communities and promote the DApps prosperity. Main contributions (a visualized guideline in Fig.2) are:\
• We provide a systematization of existing TCSC systems driven from academic work and  Based on their operating mechanisms and ways of combination, we investigate and categorize a set of typical protocols into two main classifications: the  solution and the  solution.\
• We establish a unified evaluation framework for confidential smart contract systems. We consider two parts: the smart contracts used as , and underlying supported blockchain systems. Accordingly, the framework covers three aspects:  for contract services,  and  for underlying systems. Specifically, we discuss two different types of desirable properties:  that inherit from traditional smart contracts and featured privacy-related properties. Then, we emphasize practical issues, pitfalls, and remedies in designing TEE-assisted blockchains from four aspects ( securities and  services).\
• We conduct a comparative analysis of existing protocols based on our evaluation framework. We discuss systems both from their  (system classification, threat model) and  (designs, properties). The common designs show us the consistent idea when re-designing the system, while the distinguished features highlight the ingenuity of each system design that deviates from others (see Tab.III/Tab.IV).\
• We further give a comprehensive discussion of current designs and implementations, including a running example, comparisons between layer-one and layer-two systems from the perspectives of ,  and , and common issues on . Unfortunately, a mature design is still not ready for large-scale applications. We thereby point out research  in this field, wishing to give insights for communities on defining their models and discovering possible solutions of designing TCSC systems.\
The rest of the paper is organized as follows. Sec.II gives a high-level introduction on how to operate a confidential smart contract inside TEEs. Sec.III provides the systematization methodology ( and ). Layer-one and layer-two systems are analysed in Sec.IV and Sec.V, respectively. Discussions are provided in Sec.VI. Research challenges are summarised in Sec.VII. Finally, Sec.VIII gives concluding remarks. Supplementary details are stated in Appendix A-D.]]></content:encoded></item><item><title>Road to Battlefield: Central Eurasia’s gateway to TechCrunch Startup Battlefield</title><link>https://techcrunch.com/2025/07/01/road-to-battlefield-central-eurasias-gateway-to-techcrunch-startup-battlefield/</link><author>Cindy Zackney</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:41:37 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Historic regional competition launches to showcase Central Eurasia’s rising startup ecosystem on Silicon Valley’s biggest stage. For the first time in its history, Central Eurasia will have a direct pathway to TechCrunch Startup Battlefield through the launch of “Road to Battlefield,” a groundbreaking regional competition that promises to put the underrepresented region firmly on the […]]]></content:encoded></item><item><title>Tech Hobbyist Destroys 51 MicroSD Cards To Build Ultimate Performance Database</title><link>https://it.slashdot.org/story/25/07/01/155208/tech-hobbyist-destroys-51-microsd-cards-to-build-ultimate-performance-database?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Tech enthusiast Matt Cole has created a comprehensive MicroSD card testing database, writing over 18 petabytes of data across nearly 200 cards since July 2023. Cole's "Great MicroSD Card Survey" uses eight machines running 70 card readers around the clock, writing 101 terabytes daily to test authenticity, performance, and endurance. 

The 15,000-word report covering over 200 different cards reveals significant quality disparities. Name-brand cards purchased from Amazon performed markedly better than identical models from AliExpress, while cards with "fake flash" -- inflated capacity ratings -- performed significantly worse than authentic storage. Sandisk and Kingston cards averaged 4,634 and 3,555 read/write cycles before first error, respectively, while Lenovo cards averaged just 291 cycles. Some off-brand cards failed after only 27 cycles. Cole tested 51 cards to complete destruction during the endurance testing phase.]]></content:encoded></item><item><title>You will own NOTHING and be HAPPY! (SKG)</title><link>https://www.youtube.com/watch?v=rAsgjKBkKMA</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/rAsgjKBkKMA?version=3" length="" type=""/><pubDate>Tue, 1 Jul 2025 20:35:53 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[Please sign if you're in the EU or UK!
EU: https://eci.ec.europa.eu/045/public/#/screen/home
UK: https://petition.parliament.uk/petitions/702074/

Things I mentioned in this video:

  - Stop Killing Games: https://www.stopkillinggames.com
  - My video takedown (and getting a 2nd appeal): https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful-updated
  - Bosch Dishwasher video: https://www.youtube.com/watch?v=5M_hmwBBPnc
  - XKCD on WiFi appliances: https://xkcd.com/3109/
  - Dead game list: https://stopkillinggames.wiki.gg/wiki/Dead_game_list

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff

Contents:

00:00 - Controlling your own destiny. And dishwasher.
01:27 - Games (please stop killing them)
03:35 - What you can do (in the EU)]]></content:encoded></item><item><title>X is piloting a program that lets AI chatbots generate Community Notes</title><link>https://techcrunch.com/2025/07/01/x-is-piloting-a-program-that-lets-ai-chatbots-generate-community-notes/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:26:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The social platform X will pilot a feature that allows AI chatbots to generate Community Notes, a Twitter-era feature that Elon Musk has expanded under his ownership of the service now called X.]]></content:encoded></item><item><title>Trump Launches America’s Newest Concentration Camp, Complete With Tacky Merch</title><link>https://www.techdirt.com/2025/07/01/trump-launches-americas-first-concentration-camp-complete-with-tacky-merch/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:06:47 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Not content with just shipping people to a foreign concentration camp, Donald Trump now has his own, homegrown concentration camp in Florida. Trump, DHS Secretary Kristi Noem, and Florida Governor Ron DeSantis gleefully toured the hastily constructed concentration camp in the Florida Everglades, obnoxiously referred to as Alligator Alcatraz, in reference to (1) the infamous island prison in San Francisco that Trump is obsessed with and (2) the number of alligators (and crocodiles — the one place in the world that has both) that live in and around the Everglades.There’s no way to look at what the US government is doing here and not think of it more as Auschwitz than Alcatraz. The parallels are unmistakable: hastily constructed camps in remote locations, euphemistic naming designed to obscure their true purpose, and—most tellingly—officials proudly touring the facilities while discussing plans to build “a system” of such camps nationwide.But here’s where today’s American concentration camps differ from their 20th-century predecessors: the Trump regime isn’t trying to hide what they’re doing. They’re merchandising it. They’re selling t-shirts celebrating human suffering as if it were a sports team or a vacation destination.The United States government is literally selling branded merchandise to celebrate putting human beings in cages surrounded by dangerous predators. This isn’t just about policy—it’s about turning cruelty into a consumer product. It’s about making the suffering of others into something you can wear to own the libs.This commodification of human rights violations represents something uniquely American and uniquely horrifying: the gamification of genocide. Previous authoritarian regimes at least had the decency to be ashamed of their concentration camps. Trump is selling tickets to the show.These are the sorts of things that history books (should they exist in the future) will talk about as one of the many moments of pure evil that some people gleefully embraced without recognizing that people setting up concentration camps are, inherently, “the baddies.”For what it’s worth, Trump did little to dispel the notion that this is part of his new fascist campaign to imprison anyone who disagrees with him. During the tour, Trump and Noem talked about prosecuting CNN for their reporting and for releasing an app that alerts people to where ICE agents are located (both of which would violate the First Amendment, if it were still a thing anyone believed in).Trump admitted that he had brought up this idea as a joke, but his idiot advisors ran with it:“Is this a dream come true for you, sir” a reporter asks.“It was meant more as a joke, but the more I thought of it, the more I liked it… they were actually crocodiles,” Trump said.“We’d like to see them in many states. At some point, they might morph into a system,” Trump said on Tuesday.A “system.” The word choice isn’t accidental. This is the language of industrial-scale human rights violations, spoken with the same casual tone you’d use to discuss a chain restaurant expansion.In case you’re wondering how much it costs to go full Nazi, this one concentration camp will cost the American taxpayer nearly half a billion dollars a year. That money will come from FEMA, the organization that Trump (with an assist from former friend Elon Musk and DOGE) stripped budget from, meaning there will be even less to pay for actual emergencies, because all of that money will be used to jail people Trump doesn’t like in a swamp.The Everglades facility will cost Florida some $450 million to run for one year, according to DHS, though much of that will be reimbursed by the Federal Emergency Management Agency (FEMA). While the airstrip is owned by Miami-Dade County, where officials have viewed the plan with skepticism, DeSantis is using his emergency authority to proceed on a tight schedule.We are watching the latest march forward of American fascism in real time, complete with branded merchandise and gleeful photo ops. The US government is building concentration camps and selling t-shirts about it. This isn’t hyperbole. This isn’t partisan hysteria. This is what’s actually happening.Every day you don’t call this what it is—fascism—you become complicit in normalizing it. Every time you treat this as just another political story, you help them make it routine. They’re counting on your exhaustion, your normalization, your willingness to look away.The survivors of the Holocaust warned us this could happen again. They’re mostly gone now, but their warnings echo: it starts with camps, it starts with dehumanization, and it starts with good people doing nothing while evil wraps itself in flags and sells t-shirts.]]></content:encoded></item><item><title>AT&amp;amp;T Now Lets Customers Lock Down Account To Prevent SIM Swapping Attacks</title><link>https://tech.slashdot.org/story/25/07/01/181213/att-now-lets-customers-lock-down-account-to-prevent-sim-swapping-attacks?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AT&T has launched a new Account Lock feature designed to protect customers from SIM swapping attacks. The security tool, available through the myAT&T app, prevents unauthorized changes to customer accounts including phone number transfers, SIM card changes, billing information updates, device upgrades, and modifications to authorized users. 

SIM swapping attacks occur when criminals obtain a victim's phone number through social engineering techniques, then intercept messages and calls to access two-factor authentication codes for sensitive accounts. The attacks have become increasingly common in recent years. AT&T began gradually rolling out Account Lock earlier this year, joining T-Mobile, Verizon, and Google Fi, which already offer similar fraud prevention features.]]></content:encoded></item><item><title>Catalio Capital closes over $400M Fund IV</title><link>https://techcrunch.com/2025/07/01/catalio-capital-closes-over-400m-fund-iv/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Tue, 1 Jul 2025 20:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Catalio Capital Management announced the closing of its more than $400 million Fund IV. The fund will continue the firm’s thesis of backing healthcare and biotechnology companies. ]]></content:encoded></item><item><title>Google’s data center energy use doubled in 4 years</title><link>https://techcrunch.com/2025/07/01/googles-data-center-energy-use-doubled-in-four-years/</link><author>Tim De Chant</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:52:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google has pledged to use only carbon-free sources of electricity to power its operations, a task made more challenging by its breakneck pace of data center growth.]]></content:encoded></item><item><title>7 Iconic TV Characters Whose Names Remain a Mystery</title><link>https://hackernoon.com/7-iconic-tv-characters-whose-names-remain-a-mystery?source=rss</link><author>Fayam Ayekame</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:37:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Names are one of the most prominent features of TV, holding an almost spiritual significance. Likewise, the absence of a name can also be a powerful tool, adding mystery to your favourite characters. Whether accidental or intentional, several beloved TV characters have remained nameless and managed to be loved by fans regardless. Here are some of the most prominent small-screen characters who remain nameless throughout their shows’ run:That ‘70s Show produced several comedic gems and running tropes over its eight-season run. Through it all, one of the key features on the show was Fez, the foreign exchange student who wasn't quite exchanged back. The lovable foreigner quickly became a fan favourite with his witty comebacks and his ever-elusive search for love.Fez’s real name is mentioned during the show, but school bells drown it out in keeping with the running Gag. His home nation, on the other hand, is never mentioned, with further confusion when his best friend from home comes to visit, and is unmistakably British. Penny - The Big Bang TheoryPenny was the ultimate girl next door, bringing socialisation skills and down-to-earth relationships to her scientist neighbours. While her first name is a common feature in the world of TV, her last name remains a mystery to date. Later in the series, she takes on Hofstadter after her marriage to Leonard, but her original family name is never mentioned. Despite her entire family making an appearance on the show, the family name is miraculously left out, creating the ultimate running gag. It's no surprise to see that the longest-running show on TV has the longest-running gag, “What is the doctor’s name?” This trope is also a play on the show’s title, with the question hidden in plain sight, “Dr Who”. While he is known throughout the cosmos as “The Doctor”, the man from Gallifrey has a name, which is never actually disclosed. The title of “Doctor” was chosen as a promise to the universe and has since become his identity. Throughout the series, many have sought to find out his actual name, with only  achieving this objective. We are introduced to our mystery man, Lucas Hood, in the very first episode of Banshee. After witnessing Lucas Hood’s name, our main character assumes his identity and position as the new sheriff of Banshee. The new Lucas Hood brandishes a unique brand of justice throughout the series, as he attempts to honour the badge he now wears. While there are characters who know his real name, it is never revealed, and his true identity remains a mystery. A brief glance into his past reveals a name on file as , which is just another aliasJohn Reese - Person Of InterestPerson of Interest lasted five seasons and introduced us to the computing power of AI with  and  at the centre of it all. John Reese is an ex-CIA operative whose real name has long been buried, operating under an alias, and running errands for “”. Despite assuming multiple identities throughout the series, John Reese remains the central identity of our main character. Rip Hunter - DC’s Legends of TomorrowDC’s Legends of Tomorrow revolves around a group of time travellers roaming the timeline and fixing problems. Our time travellers are put together and captained by , a 22nd-century time agent seeking retribution for the death of his family. As an agent of the Time Bureau, all records of his previous life and ancestry have been wiped clean, with the alias Rip Hunter becoming his identity. While his time on the show is short-lived, his identity remains a mystery to ensure the protection of the timeline.  is one of the most beloved side characters on Scrubs, thanks to his long-running feud with . Despite his constant presence on the show, we never actually learn his real name, and it eventually makes for an interesting trope. At one point, he jokes that his real name is  a play on Janitor, only to mess with JD and make a point.During his wedding, he is referred to as the Janitor, with his wife seemingly becoming known as Mrs. Janitor, and seemingly accepting it. While the Janitor ]]></content:encoded></item><item><title>IT Worker Sentenced To Seven Months After Trashing Company Network</title><link>https://it.slashdot.org/story/25/07/01/1552216/it-worker-sentenced-to-seven-months-after-trashing-company-network?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: A judge has sentenced a disgruntled IT worker to more than seven months in prison after he wreaked havoc on his employer's network following his suspension, according to West Yorkshire Police. 

According to the police, Mohammed Umar Taj, 31, from the Yorkshire town of Batley, was suspended from his job in nearby Huddersfield in July 2022. But the company didn't immediately rescind his network credentials, and within hours, he began altering login names and passwords to disrupt operations, the statement says. 

The following day, he allegedly changed access credentials and the biz's multi-factor authentication settings that locked out the firm and its clients in Germany and Bahrain, eventually causing an estimated $274,200 in lost business and reputational harm.]]></content:encoded></item><item><title>Automattic puts Tumblr migration to WordPress on hold</title><link>https://techcrunch.com/2025/07/01/automattic-puts-tumblr-migration-to-wordpress-on-hold/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:13:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Automattic CEO Matt Mullenweg confirmed that the company is no longer working on migrating its Tumblr blogging platform to WordPress, as previously announced. ]]></content:encoded></item><item><title>Why They’ll Never Get You—and Why That’s Okay</title><link>https://hackernoon.com/why-theyll-never-get-youand-why-thats-okay?source=rss</link><author>BenoitMalige</author><category>tech</category><pubDate>Tue, 1 Jul 2025 19:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What I realized when I stopped needing to be understood.This newsletter is sponsored by.. no one. That being said, have you read the first chapter of  Available here for download:This morning I sat at a café in silence.\
Not on purpose. I just didn’t have much left to say.\
I’ve been talking a lot lately—about my book, about overthinking, about clarity and freedom and becoming. But when you’re in the middle of sharing your story with the world, something strange happens: you start losing touch with the parts of it that were never meant to be shared.\
You start forgetting which thoughts were sacred, and which were strategic.\
So I sat down, let the noise settle, and wrote one sentence.\
“Your depth was never meant to be understood by those who live on the surface.”\
And then, without thinking:\
“That works for your ego, too.”\
It wasn’t planned. It wasn’t poetic. But it felt true enough to stop me. Because somewhere in those two lines was the real reason I’ve felt so disconnected lately—not just from others, but from myself.\
For most of my life, I’ve believed that being misunderstood was one of the worst feelings in the world.\
And in many ways, it is.\
When you’re trying to be honest, when you’re doing the work, when you’ve found the courage to show up without the mask.. and someone still doesn’t get it? Still doesn’t get ? It cuts deeper than silence ever could.\
But that’s not what I’ve been struggling with lately.\
Because I haven’t been misunderstood. And .\
And the person who did that… was me.\
I’ve simplified my words so they wouldn’t intimidate. Softened my thoughts so they’d feel more relatable. Downplayed my insights so they wouldn’t sound like I was trying too hard.\
Rounded off the edges. Wrapped it all in a layer of warmth and good intentions.\
Not because I’m fake. But because I’ve been trying to stay visible—to . To anyone. And that need? That quiet, gnawing hunger to be understood?\
It sounds noble to want to be understood.But sometimes it’s just a socially acceptable way of saying: We dilute ourselves to become palatable. And then we blame the world for not tasting the truth.\
This morning, I sat with the discomfort of that.\
Because maybe the goal was never to be fully understood in the first place.Maybe that’s just what the ego tells us when it’s tired of feeling alone.\
And maybe wholeness is quieter than we expected. Not loud, not proven, not validated: Just intact.\
Here’s what I’m holding close right now:If someone only sees your surface, it doesn’t mean your depth doesn’t exist.Your ego will always want to be seen. Your soul just wants to be .You weren’t misunderstood. You were translated.You don’t owe anyone a version of you that fits better.And you don’t have to shrink to stay safe anymore.This morning’s message to myself was simple:]]></content:encoded></item><item><title>GM’s Home-Grown LMR Battery Opens New Front in EV Competition</title><link>https://spectrum.ieee.org/general-motors-lmr-battery</link><author>Lawrence Ulrich</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEzNTM0OS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3ODUxOTYxOH0.wzZ6yTRrw7VtBOVPomWDmAx98DZ4ZJw9GVYgQNL8kCQ/image.jpg?width=600" length="" type=""/><pubDate>Tue, 1 Jul 2025 18:58:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Manganese-rich batteries could leapfrog China’s lithium-ion phosphate cells]]></content:encoded></item><item><title>Winning capital for your AI startup? Kleida Martiro is leading the conversation at TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/winning-capital-for-your-ai-startup-kleida-martiro-is-leading-the-conversation-at-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:52:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AI-native startups are rewriting the rules of what early traction looks like — and too often, investors are still playing by the old ones. At TechCrunch All Stage, happening in Boston on July 15, Kleida Martiro, partner at Glasswing Ventures, will lead a breakout that cuts straight to the core of this disconnect.]]></content:encoded></item><item><title>AI is Now Screening Job Candidates Before Humans Ever See Them</title><link>https://slashdot.org/story/25/07/01/186240/ai-is-now-screening-job-candidates-before-humans-ever-see-them?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AI agents are now conducting first-round job interviews to screen candidates before human recruiters review them, according to WashingtonPost, which cites job seekers who report being contacted by virtual recruiters from different staffing companies. The conversational agents, built on large language models, help recruiting firms respond to every applicant and conduct interviews around the clock as companies face increasingly large talent pools. 

LinkedIn reported that job applications have jumped 30% in the last two years, partially due to AI, with some positions receiving hundreds of applications within hours. The Society for Human Resource Management said a growing number of organizations now use AI for recruiting to automate candidate searches and communicate with applicants during interviews. The AI interviews, conducted by phone or video, can last anywhere from a few minutes to 20 minutes depending on the candidate's experience and the hiring firm's questions.]]></content:encoded></item><item><title>Nothing releases its first over-the-ear headphones, the $299 Headphone (1)</title><link>https://techcrunch.com/2025/07/01/nothing-releases-its-first-over-the-ear-headphones-the-299-headphone-1/</link><author>Maggie Stamets</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:38:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing stepped away from sensors in favor of a simple button to trigger your AI assistant or ChatGPT, if you have the Nothing X app, and a volume roller that can also be pressed to play, pause, and turn on and off noise canceling.]]></content:encoded></item><item><title>Limitless Raise $4m Strategic Funding, Launch Points Ahead Of TGE</title><link>https://hackernoon.com/limitless-raise-$4m-strategic-funding-launch-points-ahead-of-tge?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:12:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[New York, NY, USA, July 1st, 2025/Chainwire/--The largest prediction market on Base, , today announces the closure of $4M in fresh funding in a strategic round and welcomes Arthur Hayes as an advisor alongside an investment from his family office, Maelstrom. The funding follows a prior $3M pre-seed round led by 1confirmation, and comes after the team found breakout demand for short term prices markets on assets like BTC, which are similar in nature to 0DTE options but a much easier way to trade and feature even shorter dated expiries such as hourly, racking up over $250M in volume soon after launch. This brings Limitless' total funding raised to $7M, backed by Coinbase Ventures, 1confirmation, Maelstrom, Collider, Node Capital, Paper Ventures, Public Works, Punk DAO, and WAGMI Ventures, as well as individual investors via the Base Ecosystem Fund group on Echo. In preparation for an upcoming TGE, the team today launched a points program targeted at prediction market enthusiasts who can get skin in the game by using the product, providing liquidity, and referring their friends to join the platform. Limitless seems likely to become the first major prediction market platform to launch a token and distribute an airdrop to its early customers, marking a notable opportunity for retail traders. The team also just introduced a new mobile-first trading experience that enables people around the world to seamlessly wager on their favorite assets' performance in the next hour or day. “The future of trading is easy, fast, and powered by an army of token holders. We’re excited to bring this vision to reality,” said CJ Hetherington, CEO at Limitless Labs. is the largest prediction market on Base with over $250M bet on unique contracts that allow users to wager on the performance of their favorite assets in the next minutes, hour or day - a net new, easy way to trade for casual users.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Block3 Unveils Prompt-To-Game AI Engine As Presale Launches</title><link>https://hackernoon.com/block3-unveils-prompt-to-game-ai-engine-as-presale-launches?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:05:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[SF, CA, July 1st, 2025/Chainwire/--AI is steadily making deeper inroads into major tech industries, and its latest leap into the gaming sector marks a notable shift.Thousands have already backed : to build the world’s most advanced prompt-to-play engine, which will let anyone generate playable worlds in minutes from a simple text prompt. The scale of the impact is already spawning an equally vocal counter-movement, with experts saying this could be the .Already being called the “”, Block3 has a solution to one of AI’s ongoing challenges, which is how to get enough data for its LLM to learn and grow.By introducing a create2earn element, where users get paid for contributing, Block3 has found a practical route to accelerate product development. To enable this ambitious vision, Block3 is opening a limited-time token presale for just 90 days.The BL3 token goes on sale at 9AM UTC today, July 1st, with tokens initially available at just $0.01. Prices increase by 5% every 72 hours, offering early investors gains of 312% by the time it hits major exchanges. The token is only available for purchase through their .What is Block3 and Prompt-to-play GamingBlock3 is made up of a team of developers who are going head-to-head with the gaming industry, by using AI to put creative power in the hands of gamers rather than gaming studios.Think about it like an AI chatbot, where the user puts in prompts and gets an answer back from AI. With Block3, the mechanism is the same, except the output is a fully-formed video game, or in essence an immersive world customised to whatever the person typing can imagine.This concept is prompting a growing number of users to explore decentralized exchange (DEX) platforms. The potential applications for this technology are wide-ranging.For the more technical, the  reveals the complexity behind the concept and reveals the team’s roadmap. This is where the scale of the project is revealed, over a series of ambitious milestones that may see Block3 burgeon from fanatical fanbase to Unicorn over months of intensive development.The Threat to Gaming: Saving a $665 Billion IndustryAnd now, its blockbuster moment may have arrived. tackles this issue head-on and has thrown a spanner in the works. Even a 0.16% share of projected market revenue would generate over $1 billion.In a statement from the team, they said: “Traditional game dev is dead. For the first time, anyone can build games, not just studios with bloated teams and red tape. We’re here to unlock the imagination of gamers, and if we break a few corporate conglomerates along the way, then so be it.”The BL3 Presale is Now LiveThe gaming industry shows no sign of stopping, and AI gaming specifically is experiencing an impressive .Block3 is well-positioned to benefit from these mega-trends, and with $665 billion of potential revenue up for grabs, there is ample incentive for the developers to scale this as big and fast as they possibly can.The concept has already attracted an active community, generating notable attention online. With the presale set at a launch price of $0.01, early participants are positioning themselves ahead of the official rollout.Both the crypto and AI sectors are known for rapid innovation, and this project represents one of the more expansive efforts to emerge recently. is pioneering a new era in gaming by building the world’s first AI-native prompt-to-play platform. Designed to let anyone generate fully playable game worlds from a simple prompt, Block3 is eliminating the traditional barriers to game creation. By merging generative AI with real-time game logic and deployable environments, it’s changing how games are built and who gets to build them.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Cloudflare Flips AI Scraping Model With Pay-Per-Crawl System For Publishers</title><link>https://tech.slashdot.org/story/25/07/01/1745245/cloudflare-flips-ai-scraping-model-with-pay-per-crawl-system-for-publishers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 18:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Cloudflare today announced a "Pay Per Crawl" program that allows website owners to charge AI companies for accessing their content, a potential revenue stream for publishers whose work is increasingly being scraped to train AI models. The system uses HTTP response code 402 to enable content creators to set per-request prices across their sites. Publishers can choose to allow free access, require payment at a configured rate, or block crawlers entirely. 

When an AI crawler requests paid content, it either presents payment intent via request headers for successful access or receives a "402 Payment Required" response with pricing information. Cloudflare acts as the merchant of record and handles the underlying technical infrastructure. The company aggregates billing events, charges crawlers, and distributes earnings to publishers. 

Alongside Pay Per Crawl, Cloudflare has switched to blocking AI crawlers by default for its customers, becoming the first major internet infrastructure provider to require explicit permission for AI access. The company handles traffic for 20% of the web and more than one million customers have already activated its AI-blocking tools since their September 2024 launch, it wrote in a blog post.]]></content:encoded></item><item><title>National Guard Troops Sent To California By Trump Are Just Out There Doing Drug Busts</title><link>https://www.techdirt.com/2025/07/01/national-guard-troops-sent-to-california-by-trump-are-just-out-there-doing-drug-busts/</link><author>Tim Cushing</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:59:52 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Martial law? Police state? These are just things the alleged Leader of Free World , rather than things a nation founded on rejecting these options should be in the process of instituting. And yet, here we are, barely six months into Trump’s return to office, staring down the barrel of both of these related horrors.If it looks like fascism, it’s probably not intentional. Trump simply isn’t smart enough to implement the real thing. But he does like authoritarianism, which looks a lot like fascism, because he’s always felt a president should be treated like a king — someone who answers to no one, not even his 340 million employers. Trump tested the waters on martial law during his last term, threatening to send troops out to handle George Floyd protests. This time around, he’s amped everything up, openly hoping to turn every “Democrat” city into Kent State.Legally, he’s not allowed to do this. But his administration is relying on some vagueness in the law to get around the long-standing prohibition of sending in the army (so to speak) to police the populace. So, we get the sort of thing we’ve seen recently, where a Los Angeles swap meet was treated like an open-air market in some Middle Eastern country we’re currently at (undeclared) war with.The National Guard troops sent to Los Angeles are presumably still working without pay and/or beds, but that isn’t stopping them from blending in with federal law enforcement to aid and abet actual law enforcement work. First reported by CBS, a combined force of more than 500 federal officers and National Guard troops walked away from the ICE raids and the protection of federal property to perform a bog standard drug bust. Nicholas Slayton has more details for Task and Purpose, a military-oriented publication: California National Guard soldiers operating under federal orders helped the Drug Enforcement Administration and other federal personnel carry out a raid on a large marijuana growth operation in the eastern Coachella Valley last week, 130 miles from downtown Los Angeles. It’s unclear how many National Guard troops participated in the operation, but the force totalled roughly 500 people. According to the DEA, other agencies included Customs and Border Patrol, Bureau of Alcohol, Tobacco, Firearms and Explosives, Immigration and Custom Enforcement and the Federal Bureau of Investigation.While this commandeering of California National Guard troops may have originally been for the unstated purpose of pushing back against anti-ICE protests, now that they’re here, the administration has decided to just use them for whatever. This raid of multiple marijuana farms occurred more than  from the boundaries of Los Angeles County and even further away from the location these troops were originally sent: downtown Los Angeles. According to Trump’s military, everything about this is good and fine and nothing to be concerned about. After all, the law says the military can help federal cops, even if it (supposedly) prevents them from doing actual cop work. That’s the Title 10 vagueness the military is relying on when it serves up statements like this” “The catalyst of this order was related to events occurring in Los Angeles; however, the president’s order and NORTHCOM’s mission is not constrained by the geography of Southern California. Recently, Title 10 forces supported a Drug Enforcement Agency operation a few hours outside of Los Angeles. Title 10 forces protect federal personnel who are performing federal law enforcement functions…”Hence the military-provided shots of alleged National Guard troops allegedly manning the perimeter of the places being raided. And, also hence, the narrative no one can definitively dispute because — despite the National Guard embedding with federal law enforcement agencies — no journalists are being allowed to embed with military-esque operations occurring  the borders of the United States.Of course, we’ve already seen Marines detain people for the purpose of handing them over to law enforcement. And we’ve seen National Guard troops swarm a swap meet like they’re looking for terrorists in a foreign country, rather than just anyone looking kind of Hispanic who might not have the proper paperwork on them. The more things like this occur, the more easily many people will just come to accept this is the way the United States operates now. Many of them will cheer on these efforts, failing to recognize the abuse of these powers may, at some point, target them. But for the rest of us, this shouldn’t be allowed to pass without notice. Trump may be a blowhard and an idiot, but he’s surrounded by people who truly desire an opportunity to perform a hard reset on democracy and its principles, replacing it with jackboot heels, racism, fascism, and — eventually — a return of the British Empire, this time wrapped in an American flag.]]></content:encoded></item><item><title>Daily Deal: Academy of Educational Engineering</title><link>https://www.techdirt.com/2025/07/01/daily-deal-academy-of-educational-engineering-3/</link><author>Daily Deal</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:53:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The Academy of Educational Engineering is a premier platform tailored for aspiring and professional geeks. This all-in-one educational ecosystem is designed to empower you with expert-level knowledge and hands-on experience across embedded systems, electronics, IoT, and software development. As a premium member, you’ll access comprehensive tools, engaging projects, personalized feedback, and direct mentorship, helping you elevate your career in the tech industry. Whether you’re a beginner or a professional, this is your ultimate gateway to mastering the future of technology. It’s on sale for $50.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Nothing releases its first over-the-ear headphones</title><link>https://techcrunch.com/video/nothing-releases-its-first-over-the-ear-headphones/</link><author>TC Video</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:52:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing has revealed its first over-the-ear headphones. The aluminum and transparent Headphone (1) was designed in collaboration with KEF. They offer adaptive noise canceling, bass enhancement for deeper low frequencies, and spatial audio, for $299 at launch. But spoiler alert: The buttons on these headphones might be the best part.]]></content:encoded></item><item><title>Midas And 0G Partner To Bring Real-World Assets To AI-Native Blockchain Infrastructure</title><link>https://hackernoon.com/midas-and-0g-partner-to-bring-real-world-assets-to-ai-native-blockchain-infrastructure?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:49:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, Republic of Singapore, July 1st, 2025/Chainwire/--Tokenization protocol Midas and AI blockchain 0G have announced a strategic partnership to unlock the next wave of onchain finance through modular design. By combining Midas’ tokenisation infrastructure with 0G’s decentralized AI-native compute, the partners will develop new solutions that intelligently leverage real-world assets (RWAs).As part of the partnership, Midas will deploy on the 0G mainnet, scheduled for late Q3 2025, bringing its full stack of tokenization infrastructure. In parallel, 0G will integrate Midas’ tokenized instruments and vault logic into its optimized AI layer. This will position both platforms to serve institutions, developers, and liquidity providers at scale.Midas offers a compliant protocol suite for issuing tokenized certificates tracking institutional-grade strategies. Its tokens, including mF-ONE, mMEV, mEDGE, mRE7YIELD, mBASIS and mTBILL, provide exposure via tokenized certificates to reference real-world assets across private credit, US short-term treasuries, and market-neutral strategies.0G Labs CEO Michael Heinrich said: “Midas have made huge strides in expanding compliant access to tokenized RWAs and we’re delighted that they’ve chosen to build on 0G. We’re excited to be collaborating with them to develop new financial products that will combine AI with tokenized assets, giving users greater onchain opportunities than ever before.” By launching on 0G, Midas will introduce compliant, composable tokens into a modular environment optimized for AI-powered workflows and smart contract automation. Use cases range from onchain lending vaults and automated credit exposures to AI-enhanced risk analytics and composable strategy deployment. 0G’s modular Layer 1 blockchain is purpose-built for AI-native applications.It combines high-performance compute, decentralized storage, data availability, and low-latency smart contract execution, ideal for deploying data-intensive financial applications and real-time DeFi logic. 0G’s architecture supports seamless integration with EVM and non-EVM ecosystems, while its recent Galileo testnet demonstrated sustained throughput and low gas costs. It also saw significant developer adoption with over 170 million transactions and 13 million accounts in under two months. The collaboration between Midas and 0G reflects a shared vision: to make programmable, compliant financial infrastructure natively interoperable with the AI applications of the future.Midas is a tokenisation platform building institutional-grade financial products for the open web. Its ERC-20 tokens are structured to track dedicated strategies with verifiable on-chain performance, combining TradFi-grade standards with DeFi composability. Midas is backed by leading investors like Framework Ventures, BlockTower Capital, and GSR, and partners with regulated custodians to ensure strong compliance and risk controls. Learn more: https://midas.app/About 0G 0G is the first decentralized AI protocol (AIP), purpose-built to power a truly democratized future of intelligence. As a modular and infinitely scalable Layer 1, 0G enables the execution of decentralized AI applications at scale. It unifies high-performance decentralized storage, compute, and data availability (DA) to support the next generation of AI-native use cases. With verifiable AI processing and a permissionless agent ecosystem, 0G is laying the foundation for an open and unstoppable AI economy. Learn more: https://0g.ai/Disclaimer This announcement is for informational purposes only and does not constitute investment advice or an offer to sell or buy any financial instrument. Midas-issued tokens are not available to US & UK persons and entities, or those from sanctioned jurisdictions. This is not investment advice.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Nothing launches its most expensive flagship yet, Phone (3)</title><link>https://techcrunch.com/2025/07/01/nothing-launches-its-most-expensive-flagship-phone-3/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nothing launched its newest flagship phone after a two-year gap. At an event in London, the company unveiled the Phone (3), which starts at $799 and aims to take on bigwigs like Samsung and Apple.]]></content:encoded></item><item><title>AI Arms Race Drives Engineer Pay To More Than $10 Million</title><link>https://tech.slashdot.org/story/25/07/01/1536223/ai-arms-race-drives-engineer-pay-to-more-than-10-million?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Tech companies are paying AI engineers unprecedented salaries as competition for talent intensifies, with some top engineers earning more than $10 million annually and typical packages ranging from $3 million to $7 million. OpenAI told staff this week it is seeking "creative ways to recognize and reward top talent" after losing key employees to rivals, despite offering salaries near the top of the market. 

The move followed OpenAI CEO Sam Altman's claim that Meta had promised $100 million sign-on bonuses to the company's most high-profile AI engineers. Mark Chen, OpenAI's chief research officer, sent an internal memo saying he felt "as if someone has broken into our home and stolen something" after recent departures. 

AI engineer salaries have risen approximately 50% since 2022, with mid-to-senior level research scientists now earning $500,000 to $2 million at major tech companies, compared to $180,000 to $220,000 for senior software engineers without AI experience.]]></content:encoded></item><item><title>Best iPad apps to boost productivity and make your life easier</title><link>https://techcrunch.com/2025/07/01/best-ipad-apps-to-boost-productivity-and-make-your-life-easier/</link><author>Aisha Malik</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:18:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[There are many iPad apps to help you organize recipes; sync tasks across devices; be more productive; and manage your notes.]]></content:encoded></item><item><title>Raising a Series C+? Cathy Gao’s bringing the real playbook to TechCrunch All Stage</title><link>https://techcrunch.com/2025/07/01/raising-a-series-c-cathy-gaos-bringing-the-real-playbook-to-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:13:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Cathy Gao of Sapphire Ventures shares her playbook to scale a Series C+ at TechCrunch All Stage on July 15 in Boston. Register now to save more than 60% on your tickets.]]></content:encoded></item><item><title>Amazon deploys its 1 millionth robot, releases generative AI model</title><link>https://techcrunch.com/2025/07/01/amazon-deploys-its-1-millionth-robot-releases-generative-ai-model/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Tue, 1 Jul 2025 17:01:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As Amazon's fleet of robots has reaches a milestone, the company is also releasing a new AI model to make them more efficient. ]]></content:encoded></item><item><title>Global Warming Is Speeding Up and the World Is Feeling the Effects</title><link>https://news.slashdot.org/story/25/07/01/164239/global-warming-is-speeding-up-and-the-world-is-feeling-the-effects?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Summer started barely a week ago, and already the United States has been smothered in a record-breaking "heat dome." Alaska saw its first-ever heat advisory this month. And all of this comes on the heels of 2024, the hottest calendar year in recorded history. The world is getting hotter, faster. A report published last week found that human-caused global warming is now increasing by 0.27 degrees Celsius per decade. That rate was recorded at 0.2 degrees in the 1970s, and has been growing since. 

"Each additional fractional degree of warming brings about a relatively larger increase in atmospheric extremes, like extreme downpours and severe droughts and wildfires," said Daniel Swain, a climate scientist at the University of California. While this aligns with scientific predictions of how climate change can intensify such events, the increase in severity may feel sudden to people who experience them. 

"Back when we had lesser levels of warming, that relationship was a little bit less dramatic," Dr. Swain said. "There is growing evidence that the most extreme extremes probably will increase faster and to a greater extent than we used to think was the case," he added. Take rainfall, for example. Generally, extreme rainfall is intensifying at a rate of 7 percent with each degree Celsius of atmospheric warming. But recent studies indicate that so-called record-shattering events are increasing at double that rate, Dr. Swain said.]]></content:encoded></item><item><title>The End of the Guessing Game? Why Describing Data Beats Estimating It</title><link>https://hackernoon.com/the-end-of-the-guessing-game-why-describing-data-beats-estimating-it?source=rss</link><author>Impute</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:38:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Achiam, J., Andrychowicz, M., Beattie, A., Clark, J., Drozdov, N., Ecoffet, A., Edwards, D., Giddings, J., Goldberg, I., Gomez, M., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)Batista, G.E., Monard, M.C.: A study of k-nearest neighbour as an imputation method. In: Frontiers in Artificial Intelligence and Applications. vol. 87, pp. 251–260. HIS (2002)Biessmann, F., Salinas, D., Schelter, S., Schmidt, P., Lange, D.: "deep" learning for missing value imputation in tables with non-numerical data. In: Proceedings of the 27th ACM International Conference on Information and Knowledge Management. p. 2017–2025. CIKM ’18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3269206.3272005, https://doi.org/10.1145/3269206.3272005Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are Few-Shot Learners. In: Advances in Neural Information Processing Systems. vol. 33, pp. 1877–1901. Curran Associates, Inc. (2020)Buuren, S.v., Groothuis-Oudshoorn, K.: Mice: Multivariate imputation by chained equations in r. Journal of Statistical Software 45, 1–67 (2011)Camino, R.D., Hammerschmidt, C.A., State, R.: Improving missing data imputation with deep generative models. arXiv preprint arXiv:1902.10666 pp. 1–8 (2019)Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., Fiedel, N.: PaLM: Scaling Language Modeling with Pathways (Oct 2022), http://arxiv.org/abs/2204.02311, arXiv:2204.02311 [cs]Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological) 39(1), 1–22 (1977)Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetuning of quantized llms (2023)Emmanuel, T., Maupong, T., Mpoeleng, D., Semong, T., Mphago, B., Tabona, O.: A survey on missing data in machine learning. J Big Data 8(1), 140 (2021). https://doi.org/10.1186/s40537-021-00516-9, epub 2021 Oct 27. PMID: 34722113; PMCID: PMC8549433Gimpy, M.: Missing value imputation in multi attribute data set. Int. J. Comput. Sci. Inf. Technol. 5(4), 1–7 (2014)Gondara, L., Wang, K.: Mida: Multiple imputation using denoising autoencoders. In: PacificAsia conference on knowledge discovery and data mining. pp. 260–272. Springer (2018)Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al.: Generative adversarial nets. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger, K.Q. (eds.) Advances in Neural Information Processing Systems. vol. 27, pp. 2672–2680. Curran Associates, Inc., Montréal, Canada (2014)Gupta, A., Lam, M.S.: Estimating missing values using neural networks. Journal of the Operational Research Society 47(2), 229–238 (1996)Hallaji, E., Razavi-Far, R., Saif, M.: Dlin: Deep ladder imputation network. IEEE Transactions on Cybernetics 52(9), 8629–8641 (2021)Jäger, S., Allhorn, A., Biessmann, F.: A benchmark for data imputation methods. Front Big Data 4, 693674 (2021). https://doi.org/10.3389/fdata.2021.693674, pMID: 34308343; PMCID: PMC8297389 Enhancing Imputation Accuracy with Contextual Large Language Models 15Little, R.J., Rubin, D.B.: Statistical Analysis with Missing Data, vol. 793. John Wiley & Sons, 3 edn. (2019)Little, R.J.A., Rubin, D.B.: Statistical Analysis with Missing Data. John Wiley & Sons, Hoboken, 2 edn. (2002)Lu, H.m., Perrone, G., Unpingco, J.: Multiple imputation with denoising autoencoder using metamorphic truth and imputation feedback. arXiv preprint arXiv:2002.08338 (2020)McCoy, J.T., Kroon, S., Auret, L.: Variational autoencoders for missing data imputation with application to a simulated milling circuit. IFAC-PapersOnLine 51(21), 141–146 (2018), 5th IFAC Workshop on Mining, Mineral and Metal Processing MMM 2018Nazabal, A., Olmos, P.M., Ghahramani, Z., Valera, I.: Handling incomplete heterogeneous data using vaes. arXiv preprint arXiv:1807.03653 (2018)Qiu, Y.L., Zheng, H., Gevaert, O.: Genomic data imputation with variational auto-encoders. GigaScience 9(8), giaa082 (2020)Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21(1), 140:5485–140:5551 (Jan 2020)Roberts, A., Raffel, C., Shazeer, N.: How Much Knowledge Can You Pack Into the Parameters of a Language Model? In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5418–5426. Association for Computational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.437, https://aclanthology.org/2020.emnlp-main.437Rubin, D.B.: Multiple imputations in sample surveys-a phenomenological bayesian approach to nonresponse. In: Proceedings of the survey research methods section of the American Statistical Association. vol. 1, pp. 20–34. American Statistical Association, Alexandria, VA, USA (1978)Rubin, D.B.: Multiple Imputation for Nonresponse in Surveys. John Wiley & Sons, New York, NY (2004)Schafer, J.L.: Analysis of Incomplete Multivariate Data. Chapman & Hall/CRC, London, UK (1997)Schelter, S., Rukat, T., Biessmann, F.: JENGA - A framework to study the impact of data errors on the predictions of machine learning models. In: Velegrakis, Y., Zeinalipour-Yazti, D., Chrysanthis, P.K., Guerra, F. (eds.) Proceedings of the 24th International Conference on Extending Database Technology, EDBT 2021, Nicosia, Cyprus, March 23 - 26, 2021. pp. 529–534. OpenProceedings.org (2021). https://doi.org/10.5441/002/EDBT.2021.63, https://doi.org/10.5441/002/edbt.2021.63Sharpe, P.K., Solly, R.: Dealing with missing values in neural network-based diagnostic systems. Neural Computing & Applications 3(2), 73–77 (1995)Stekhoven, D.J., Bühlmann, P.: Missforest—non-parametric missing value imputation for mixed-type data. Bioinformatics 28(1), 112–118 (2012)Stoyanovich, J., Howe, B., Jagadish, H.V.: Responsible data management. Proceedings of the VLDB Endowment 13, 3474–3488 (2020). https://doi.org/10.14778/ 3415478.3415570Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (Feb 2023), http://arxiv.org/ abs/2302.13971, arXiv:2302.13971 [cs]Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.: Llama 2: Open foundation and fine-tuned chat models (2023)Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th international conference on machine learning. pp. 1096–1103 (2008)Yang, K., Huang, B., Stoyanovich, J., Schelter, S.: Fairness-aware instrumentation of preprocessing pipelines for machine learning. In: Proceedings of the Workshop on HumanIn-the-Loop Data Analytics (HILDA’20). ACM (2020). https://doi.org/10.1145/3398730.3399194Yoon, J., Jordon, J., van der Schaar, M.: Gain: Missing data imputation using generative adversarial nets. In: International conference on machine learning. pp. 5689–5698. PMLR (2018)Yoon, J., Jordon, J., van der Schaar, M.: Gain: Missing data imputation using generative adversarial nets (2018)]]></content:encoded></item><item><title>Why Mamdani’s Refusal To Condemn Speech He Never Made Is Good Free Speech Advocacy</title><link>https://www.techdirt.com/2025/07/01/why-mamdanis-refusal-to-condemn-speech-he-never-made-is-good-free-speech-advocacy/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:28:52 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[At a time when politicians on both sides reflexively call for censorship and speech policing, it’s refreshing to see someone actually defend free speech principles—especially when it would be politically easier to cave.That’s exactly what New York City Democratic mayoral nominee Zohran Mamdani did when NBC’s  tried to pressure him into condemning language he’s never used. Rather than take the bait, Mamdani delivered a strong defense of free speech principles. It’s a better defense of free speech than we’ve seen from most politicians lately.What makes this particularly frustrating is that many of the Democrats attacking Mamdani should be laser-focused on the existential threat Trump poses to democracy. Instead, they’re wasting time and energy going after someone who actually accomplished what establishment Democrats claim they desperately want: activating young people who often fail to vote. Mamdani didn’t just talk about engaging young voters—he did it, handily winning the Democratic primary by mobilizing exactly the demographic Democrats say they need. His reward? A coordinated attack campaign.The controversy stems from demands that Mamdani condemn the phrase “globalize the intifada”—language he doesn’t use but which critics insist he must denounce to prove he’s not antisemitic. It’s the kind of ridiculous purity test that marginalized politicians routinely face (but somehow, white, Christian, male politicians never do), demanding they repeatedly distance themselves from the words of others simply because they share some demographic or political similarity.But rather than playing that game, Mamdani chose to defend the principle that government officials shouldn’t be in the business of policing speech—even speech they personally disagree with. At the same time, he used the opportunity to move from the “gotcha” kind of question to a focus on how to tackle the actual problems of racism and bigotry, beyond just focusing on specific language questions.There’s been a lot of pressure on Mamdani to specifically criticize pro-Palestinian language used by others. And, over the weekend, he went on Meet the Press and gave, what I think, is a really strong answer to a silly gotcha question that I think others could learn from:I want to ask you about an issue that has divided some New Yorkers in recent weeks. You were recently asked about the term “globalize the intifada,” if it makes you uncomfortable. In that moment you did not condemn the phrase. Now, just so folks understand, it’s a phrase that many people hear as a call to violence against Jews. There’s been a lot of attention on this issue, so I want to give you an opportunity to respond here and now. Do you condemn that phrase “globalize the intifada?”That’s not language that I use. The language that I use and the language that I will continue to use to lead this city is that which speaks clearly to my intent, which is an intent grounded in a belief in universal human rights. And ultimately, that’s what is the foundation of so much of my politics, the belief that freedom and justice and safety are things that, to have meaning, have to be applied to all people, and that includes Israelis and Palestinians as well.But do you actually condemn it? I think that’s the question and the outstanding issue that a number of people, both of the Jewish faith and beyond, have. Do you condemn that phrase, “globalize the intifada,” which a lot of people hear as a call to violence against Jews?I’ve heard from many Jewish New Yorkers who have shared their concerns with me, especially in light of the horrific attacks that we saw in Washington, D.C. and in Boulder, Colorado about this moment of antisemitism in our country and in our city. And I’ve heard those fears and I’ve had those conversations. And ultimately, they are part and parcel of why, in my campaign, I’ve put forward a commitment to increase funding for anti-hate crime programming by 800%. I don’t believe that the role of the mayor is to police speech in the manner, especially of that of Donald Trump, who has put one New Yorker in jail, who’s just returned to his family, Mahmoud Khalil, for that very supposed crime of speech. Ultimately, what I think I need to show is the ability to not only talk about something but to tackle it and to make clear that there’s no room for antisemitism in this city. And we have to root out that bigotry, and ultimately we do that through the actions. And that is the mayor I will be, one that protects Jewish New Yorkers and lives up to that commitment through the work that I do.But very quickly for the people who care about the language and who feel really concerned by that phrase, why not just condemn it?My concern is to start to walk down the line of language and making clear what language I believe is permissible or impermissible takes me into a place similar to that of the president, who is looking to do those very kinds of things, putting people in jail for writing an oped. Putting them in jail for protesting. Ultimately, it’s not language that I use. It’s language I understand there are concerns about. And what I will do is showcase my vision for this city through my words and my actions.Note what he does here. It would be easy enough to give into the framing and make statement condemning the language. And while some will (in bad faith) argue his failure to outright condemn the language is an endorsement of it, that’s bullshit. His answer is actually very thoughtful and a good way to approach such bad faith questions.He starts out with a direct and clear denial of using that language:That’s not language that I use.This immediately deflates the premise that he’s somehow responsible for words he’s never spoken.He then immediately shifts to a more positive framing of how he views what he’s focused on in his hopes of becoming mayor: human rights for all.The language that I use and the language that I will continue to use to lead this city is that which speaks clearly to my intent, which is an intent grounded in a belief in universal human rights. And ultimately, that’s what is the foundation of so much of my politics, the belief that freedom and justice and safety are things that, to have meaning, have to be applied to all people, and that includes Israelis and Palestinians as well.When NBC’s Welker trots out the purity test point, demanding he condemn it, he points out that he shouldn’t be in the business of policing language, but rather is focused on actual concerns of the people he’s hoping to represent. In doing so, he makes it clear that he’s concerned about actual antisemitism and actual threats and risks, and he’s looking at what might actually help rather than policing specific language:I’ve heard from many Jewish New Yorkers who have shared their concerns with me, especially in light of the horrific attacks that we saw in Washington, D.C. and in Boulder, Colorado about this moment of antisemitism in our country and in our city. And I’ve heard those fears and I’ve had those conversations. And ultimately, they are part and parcel of why, in my campaign, I’ve put forward a commitment to increase funding for anti-hate crime programming by 800%.And then he pivots to a reasonable defense of free speech, not in the misleading sense the way others view it, but rather in noting that government shouldn’t be in the business of policing speech (as Trump is doing) but focusing on where the  problems of hate and bigotry show up.I don’t believe that the role of the mayor is to police speech in the manner, especially of that of Donald Trump, who has put one New Yorker in jail, who’s just returned to his family, Mahmoud Khalil, for that very supposed crime of speech. Ultimately, what I think I need to show is the ability to not only talk about something but to tackle it and to make clear that there’s no room for antisemitism in this city. And we have to root out that bigotry, and ultimately we do that through the actions.After Welker desperately goes back to the “but won’t you condemn the language” nonsense, he makes it clear that speaking out on specific language choices is not productive when his focus is on dealing with the actual underlying problems:My concern is to start to walk down the line of language and making clear what language I believe is permissible or impermissible takes me into a place similar to that of the president, who is looking to do those very kinds of things, putting people in jail for writing an oped. Putting them in jail for protesting. Ultimately, it’s not language that I use. It’s language I understand there are concerns about. And what I will do is showcase my vision for this city through my words and my actions.This final answer is particularly smart because it connects his refusal to condemn specific language to Trump’s actual authoritarian attacks on free speech. Rather than getting trapped in semantic debates about particular phrases, he’s defending the broader principle that government officials shouldn’t be arbiters of acceptable speech.The contrast is stark: while the Trump regime is literally jailing people for their speech, critics want Mamdani to engage in the kind of speech policing that leads down that same authoritarian path. His refusal isn’t endorsement of problematic language—it’s recognition that the role of government isn’t to play word police.This is exactly the kind of principled free speech defense we need more of, especially from Democrats who have too often been willing to compromise these principles for short-term political gain. While it would have been easy for Mamdani to simply condemn the phrase and move on, his more thoughtful approach actually serves the cause of free speech better.The irony is that many of the same people attacking Mamdani are Democrats who claim to be defending democracy against Trump’s authoritarianism. Yet they’re demanding exactly the kind of speech policing that authoritarian governments excel at—forcing officials to take public positions on specific language as loyalty tests.And yes, some could argue that simply condemning certain language is not the same as censoring it. It’s not. It’s stating an opinion. But there’s value in Mamdani making it clear he’d rather focus on the real underlying issues around bigotry and hatred than trying to say magic words to appease a media that would never ask similar questions of a white, Christian politician.In an era where politicians routinely cave to demands for performative condemnations and symbolic gestures, Mamdani’s approach stands out. He’s more interested in actual solutions—like his 800% increase in anti-hate crime funding—than in playing the gotcha game that dominates political discourse.This is what defending free speech actually looks like: not demanding the right to be an asshole without consequences, but refusing to let government officials become the arbiters of acceptable speech—and politely reframing the issue when the media insists on playing such a gotcha game. If more politicians followed Mamdani’s lead, we’d have a much healthier democratic discourse.]]></content:encoded></item><item><title>Built for Scale: eXchange1 Brings Institutional-Grade Crypto Trading to Indian Users</title><link>https://hackernoon.com/built-for-scale-exchange1-brings-institutional-grade-crypto-trading-to-indian-users?source=rss</link><author>ZEX MEDIA</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:16:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As India continues its ascent as the world’s most active crypto market, eXchange1 arrives with the infrastructure, innovation, and integrity to match the moment. With its official launch, the European-regulated platform opens access to a full suite of digital asset tools designed for modern traders, institutional clients, and ecosystem partners alike.While many platforms promise speed and scale, few back it up with the regulatory foundation and high-performance architecture that eXchange1 offers by default.eXchange1 positions itself as more than a trading venue. It is a complete digital finance ecosystem that merges cutting-edge technology with seamless user experience. The platform provides:Spot, Margin, and Futures TradingCopy Trading (for both Spot and Futures)Tokenized Investment ProductsWhether you’re a new investor exploring basic trades or an institution managing complex portfolios, eXchange1 offers functionality at every level. The platform is accessible across web, mobile, and API, enabling real-time participation with global markets from any device.Its Copy Trading feature, for example, empowers less experienced traders to mirror strategies of seasoned investors—while the automated tools allow advanced users to deploy algorithmic trades with precision.Built for India. Backed by the World.India’s fast-growing crypto base—119 million users in 2024 alone—demands more than just flashy apps. It demands scalability, reliability, and regulatory clarity. eXchange1 delivers on all fronts.The platform is built on a high-liquidity architecture designed to support institutional-scale trade volumes without bottlenecks. Back-end infrastructure is optimized for rapid execution, real-time price discovery, and minimal slippage—ensuring traders get exactly what they see.To support this mission in India, eXchange1 has partnered with a leading global fintech firm to ensure localized performance, reduced latency, and seamless access—even during market surges.“We’re not here with a one-size-fits-all model,” said CEO Ms. Sandoval Mera. “India’s scale and diversity deserve a platform that’s tailored, responsive, and resilient. We’ve invested in both infrastructure and partnerships to deliver that.”Enterprise-Grade Security by DefaultInfrastructure isn’t just about speed—it’s also about safety. eXchange1 operates with enterprise-grade security systems that ensure operational integrity, user protection, and compliance with global standards.Key security features include:Real-time risk monitoring systemsInstitutional-grade custodial protocolsMulti-layer authentication and transaction securityAudit-ready transparency for regulators and partnersWith the rising sophistication of crypto-related threats, such features are no longer optional—they are critical. eXchange1 ensures its infrastructure protects both user assets and market confidence.The platform is licensed under the Markets in Crypto-Assets Regulation (MiCA) by the Financial Crime Investigation Service (FCIS) of Lithuania, and is registered with India’s Financial Intelligence Unit (FIU).This dual compliance enables eXchange1 to integrate seamlessly with India’s evolving regulatory environment while retaining the global credibility needed to serve cross-border investors.Being MiCA-compliant also brings with it rigorous operating standards, including rules around asset segregation, risk exposure, transparency, and consumer protection—all of which are reflected in eXchange1’s infrastructure design.“The future of digital assets depends on platforms that are both innovative and accountable,” said Dr. James Newsome, Chairman of eXchange1 and former head of the U.S. Commodity Futures Trading Commission (CFTC). “We’ve built eXchange1 to be just that—a high-performance platform that regulators can work with, and users can depend on.”One of eXchange1’s key advantages is its ability to scale with its users. Retail traders benefit from an intuitive UI/UX and educational support, while institutions get access to:Advanced APIs for high-frequency tradingReal-time market data feedsDedicated account supportTokenized investment vehicles for diversificationThis dual approach—serving both ends of the crypto maturity spectrum—makes eXchange1 uniquely capable of addressing India’s diverse and rapidly evolving investor base.And with multilingual customer service available 24/7, the platform ensures every user, from first-time investor to fund manager, gets the assistance they need—when they need it.Future-Proofed for a Changing IndustryCrypto is changing fast. New asset classes, regulatory shifts, institutional adoption, and user behavior all demand platforms that can adapt in real time. eXchange1’s architecture is modular and forward-compatible, allowing it to integrate future features like:Cross-chain liquidity poolsCompliance automation for new jurisdictionsThis future-readiness is part of the platform’s DNA. It’s also why eXchange1 insists on building for long-term resilience rather than short-term speculation.Final Thoughts: A New Infrastructure StandardAs more Indian users seek robust, regulated platforms to trade and invest in crypto, infrastructure will become the dividing line between the serious players and the short-lived ones.eXchange1 has entered the Indian market not to test the waters, but to build a foundation—one grounded in regulation, engineered for performance, and designed for trust.From institutional-grade systems to retail-friendly tools, and from global oversight to local integration, eXchange1 represents what a next-generation exchange should look like: fast, safe, scalable, and accountable.]]></content:encoded></item><item><title>Nikita Bier joins X as head of product: ‘I’ve officially posted my way to the top’</title><link>https://techcrunch.com/2025/07/01/nikita-bier-joins-x-as-head-of-product-ive-officially-posted-my-way-to-the-top/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 1 Jul 2025 16:13:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At X, Bier could potentially build features that drive adoption beyond the user base that's typically drawn to text-first social networks.]]></content:encoded></item></channel></rss>