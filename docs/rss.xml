<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Liveboat Demo</title><link>https://konrad.website/liveboat-github-runner/</link><description>Liveboat RSS Feed</description><item><title>go-playground/validator</title><link>https://github.com/go-playground/validator</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[💯Go Struct and Field validation, including Cross Field, Cross Struct, Map, Slice and Array divingPackage validator implements value validations for structs and individual fields based on tags.It has the following  features:Cross Field and Cross Struct validations by using validation tags or custom validators.Slice, Array and Map diving, which allows any or all levels of a multidimensional field to be validated.Ability to dive into both map keys and values for validationHandles type interface by determining it's underlying type prior to validation.Handles custom field types such as sql driver Valuer see ValuerAlias validation tags, which allows for mapping of several validations to a single tag for easier defining of validations on structsExtraction of custom defined Field Name e.g. can specify to extract the JSON name while validating and have it available in the resulting FieldErrorCustomizable i18n aware error messages.Default validator for the gin web framework; upgrading from v8 to v9 in gin see herePlease read the discussiong started here if you are interested in contributing/helping maintain this package.go get github.com/go-playground/validator/v10
Then import the validator package into your own code.import "github.com/go-playground/validator/v10"
Validation functions return type errorThey return type error to avoid the issue discussed in the following, where err is always != nil:Validator returns only InvalidValidationError for bad validation input, nil or ValidationErrors as type error; so, in your code all you need to do is check if the error returned is not nil, and if it's not check if error is InvalidValidationError ( if necessary, most of the time it isn't ) type cast it to type ValidationErrors like so:err := validate.Struct(mystruct)
validationErrors := err.(validator.ValidationErrors)
If new to using validator it is highly recommended to initialize it using the WithRequiredStructEnabled option which is opt-in to new behaviour that will become the default behaviour in v11+. See documentation for more details.validate := validator.New(validator.WithRequiredStructEnabled())
Field Equals Another Field (relative)Field Equals Another FieldCheck the indicated characters are present in the FieldCheck the indicated characters are not present in the fieldField Greater Than Another Relative FieldField Greater Than or Equal To Another Relative FieldField Greater Than or Equal To Another FieldField Greater Than Another FieldLess Than Another Relative FieldLess Than or Equal To Another Relative FieldLess Than or Equal To Another FieldField Does Not Equal Another Field (relative)Field Does Not Equal Another FieldClassless Inter-Domain Routing CIDRClassless Inter-Domain Routing CIDRv4Classless Inter-Domain Routing CIDRv6Full Qualified Domain Name (FQDN)Internet Protocol Address IPInternet Protocol Address IPv4Internet Protocol Address IPv6Internet Protocol Address IPInternet Protocol Address IPv4Internet Protocol Address IPv6Media Access Control Address MACTransmission Control Protocol Address TCPv4Transmission Control Protocol Address TCPv6Transmission Control Protocol Address TCPUser Datagram Protocol Address UDPv4User Datagram Protocol Address UDPv6User Datagram Protocol Address UDPUnix domain socket end point AddressBusiness Identifier Code (ISO 9362)Bitcoin Bech32 Address (segwit)mongodb_connection_stringMongoDB Connection StringSpiceDb ObjectID/Permission/Typee164 formatted phone numberInternational Standard Book NumberInternational Standard Book Number 10International Standard Book Number 13International Standard Serial NumberTwo-letter country code (ISO 3166-1 alpha-2)Three-letter country code (ISO 3166-1 alpha-3)Numeric country code (ISO 3166-1 numeric)Country subdivision code (ISO 3166-2)Luhn Algorithm Checksum (for strings and (u)int)postcode_iso3166_alpha2_fieldSocial Security Number SSNUniversally Unique Identifier UUIDUniversally Unique Identifier UUID v3Universally Unique Identifier UUID v3 RFC4122Universally Unique Identifier UUID v4Universally Unique Identifier UUID v4 RFC4122Universally Unique Identifier UUID v5Universally Unique Identifier UUID v5 RFC4122Universally Unique Identifier UUID RFC4122Semantic Versioning 2.0.0Universally Unique Lexicographically Sortable Identifier ULIDCommon Vulnerabilities and Exposures Identifier (CVE id)hexcolor|rgb|rgba|hsl|hslaiso3166_1_alpha2|iso3166_1_alpha3|iso3166_1_alpha_numericRun on MacBook Pro Max M3go version go1.23.3 darwin/arm64
goos: darwin
goarch: arm64
cpu: Apple M3 Max
pkg: github.com/go-playground/validator/v10
BenchmarkFieldSuccess-16                                                42461943                27.88 ns/op            0 B/op          0 allocs/op
BenchmarkFieldSuccessParallel-16                                        486632887                2.289 ns/op           0 B/op          0 allocs/op
BenchmarkFieldFailure-16                                                 9566167               121.3 ns/op           200 B/op          4 allocs/op
BenchmarkFieldFailureParallel-16                                        17551471                83.68 ns/op          200 B/op          4 allocs/op
BenchmarkFieldArrayDiveSuccess-16                                        7602306               155.6 ns/op            97 B/op          5 allocs/op
BenchmarkFieldArrayDiveSuccessParallel-16                               20664610                59.80 ns/op           97 B/op          5 allocs/op
BenchmarkFieldArrayDiveFailure-16                                        4659756               252.9 ns/op           301 B/op         10 allocs/op
BenchmarkFieldArrayDiveFailureParallel-16                                8010116               152.9 ns/op           301 B/op         10 allocs/op
BenchmarkFieldMapDiveSuccess-16                                          2834575               421.2 ns/op           288 B/op         14 allocs/op
BenchmarkFieldMapDiveSuccessParallel-16                                  7179700               171.8 ns/op           288 B/op         14 allocs/op
BenchmarkFieldMapDiveFailure-16                                          3081728               384.4 ns/op           376 B/op         13 allocs/op
BenchmarkFieldMapDiveFailureParallel-16                                  6058137               204.0 ns/op           377 B/op         13 allocs/op
BenchmarkFieldMapDiveWithKeysSuccess-16                                  2544975               464.8 ns/op           288 B/op         14 allocs/op
BenchmarkFieldMapDiveWithKeysSuccessParallel-16                          6661954               181.4 ns/op           288 B/op         14 allocs/op
BenchmarkFieldMapDiveWithKeysFailure-16                                  2435484               490.7 ns/op           553 B/op         16 allocs/op
BenchmarkFieldMapDiveWithKeysFailureParallel-16                          4249617               282.0 ns/op           554 B/op         16 allocs/op
BenchmarkFieldCustomTypeSuccess-16                                      14943525                77.35 ns/op           32 B/op          2 allocs/op
BenchmarkFieldCustomTypeSuccessParallel-16                              64051954                20.61 ns/op           32 B/op          2 allocs/op
BenchmarkFieldCustomTypeFailure-16                                      10721384               107.1 ns/op           184 B/op          3 allocs/op
BenchmarkFieldCustomTypeFailureParallel-16                              18714495                69.77 ns/op          184 B/op          3 allocs/op
BenchmarkFieldOrTagSuccess-16                                            4063124               294.3 ns/op            16 B/op          1 allocs/op
BenchmarkFieldOrTagSuccessParallel-16                                   31903756                41.22 ns/op           18 B/op          1 allocs/op
BenchmarkFieldOrTagFailure-16                                            7748558               146.8 ns/op           216 B/op          5 allocs/op
BenchmarkFieldOrTagFailureParallel-16                                   13139854                92.05 ns/op          216 B/op          5 allocs/op
BenchmarkStructLevelValidationSuccess-16                                16808389                70.25 ns/op           16 B/op          1 allocs/op
BenchmarkStructLevelValidationSuccessParallel-16                        90686955                14.47 ns/op           16 B/op          1 allocs/op
BenchmarkStructLevelValidationFailure-16                                 5818791               200.2 ns/op           264 B/op          7 allocs/op
BenchmarkStructLevelValidationFailureParallel-16                        11115874               107.5 ns/op           264 B/op          7 allocs/op
BenchmarkStructSimpleCustomTypeSuccess-16                                7764956               151.9 ns/op            32 B/op          2 allocs/op
BenchmarkStructSimpleCustomTypeSuccessParallel-16                       52316265                30.37 ns/op           32 B/op          2 allocs/op
BenchmarkStructSimpleCustomTypeFailure-16                                4195429               277.2 ns/op           416 B/op          9 allocs/op
BenchmarkStructSimpleCustomTypeFailureParallel-16                        7305661               164.6 ns/op           432 B/op         10 allocs/op
BenchmarkStructFilteredSuccess-16                                        6312625               186.1 ns/op           216 B/op          5 allocs/op
BenchmarkStructFilteredSuccessParallel-16                               13684459                93.42 ns/op          216 B/op          5 allocs/op
BenchmarkStructFilteredFailure-16                                        6751482               171.2 ns/op           216 B/op          5 allocs/op
BenchmarkStructFilteredFailureParallel-16                               14146070                86.93 ns/op          216 B/op          5 allocs/op
BenchmarkStructPartialSuccess-16                                         6544448               177.3 ns/op           224 B/op          4 allocs/op
BenchmarkStructPartialSuccessParallel-16                                13951946                88.73 ns/op          224 B/op          4 allocs/op
BenchmarkStructPartialFailure-16                                         4075833               287.5 ns/op           440 B/op          9 allocs/op
BenchmarkStructPartialFailureParallel-16                                 7490805               161.3 ns/op           440 B/op          9 allocs/op
BenchmarkStructExceptSuccess-16                                          4107187               281.4 ns/op           424 B/op          8 allocs/op
BenchmarkStructExceptSuccessParallel-16                                 15979173                80.86 ns/op          208 B/op          3 allocs/op
BenchmarkStructExceptFailure-16                                          4434372               264.3 ns/op           424 B/op          8 allocs/op
BenchmarkStructExceptFailureParallel-16                                  8081367               154.1 ns/op           424 B/op          8 allocs/op
BenchmarkStructSimpleCrossFieldSuccess-16                                6459542               183.4 ns/op            56 B/op          3 allocs/op
BenchmarkStructSimpleCrossFieldSuccessParallel-16                       41013781                37.95 ns/op           56 B/op          3 allocs/op
BenchmarkStructSimpleCrossFieldFailure-16                                4034998               292.1 ns/op           272 B/op          8 allocs/op
BenchmarkStructSimpleCrossFieldFailureParallel-16                       11348446               115.3 ns/op           272 B/op          8 allocs/op
BenchmarkStructSimpleCrossStructCrossFieldSuccess-16                     4448528               267.7 ns/op            64 B/op          4 allocs/op
BenchmarkStructSimpleCrossStructCrossFieldSuccessParallel-16            26813619                48.33 ns/op           64 B/op          4 allocs/op
BenchmarkStructSimpleCrossStructCrossFieldFailure-16                     3090646               384.5 ns/op           288 B/op          9 allocs/op
BenchmarkStructSimpleCrossStructCrossFieldFailureParallel-16             9870906               129.5 ns/op           288 B/op          9 allocs/op
BenchmarkStructSimpleSuccess-16                                         10675562               109.5 ns/op             0 B/op          0 allocs/op
BenchmarkStructSimpleSuccessParallel-16                                 131159784                8.932 ns/op           0 B/op          0 allocs/op
BenchmarkStructSimpleFailure-16                                          4094979               286.6 ns/op           416 B/op          9 allocs/op
BenchmarkStructSimpleFailureParallel-16                                  7606663               157.9 ns/op           416 B/op          9 allocs/op
BenchmarkStructComplexSuccess-16                                         2073470               576.0 ns/op           224 B/op          5 allocs/op
BenchmarkStructComplexSuccessParallel-16                                 7821831               161.3 ns/op           224 B/op          5 allocs/op
BenchmarkStructComplexFailure-16                                          576358              2001 ns/op            3042 B/op         48 allocs/op
BenchmarkStructComplexFailureParallel-16                                 1000000              1171 ns/op            3041 B/op         48 allocs/op
BenchmarkOneof-16                                                       22503973                52.82 ns/op            0 B/op          0 allocs/op
BenchmarkOneofParallel-16                                                8538474               140.4 ns/op             0 B/op          0 allocs/op
Here is a list of software that complements using this library either pre or post validation.form - Decodes url.Values into Go value(s) and Encodes Go value(s) into url.Values. Dual Array and Full map support.mold - A general library to help modify or set data within data structures and other objectsMaintenance and support for SDK major versionsSee prior discussion here for more details.This package is aligned with the Go release policy in that support is guaranteed for the two most recent major versions.This does not mean the package will not work with older versions of Go, only that we reserve the right to increase the MSGV(Minimum Supported Go Version) when the need arises to address Security issues/patches, OS issues & support or newly introduced functionality that would greatly benefit the maintenance and/or usage of this package.If and when the MSGV is increased it will be done so in a minimum of a  release bump.Distributed under MIT License, please see license file within the code for more details.This project has grown large enough that more than one person is required to properly support the community. If you are interested in becoming a maintainer please reach out to me https://github.com/deankarn]]></content:encoded></item><item><title>Oliveriver/5d-diplomacy-with-multiverse-time-travel</title><link>https://github.com/Oliveriver/5d-diplomacy-with-multiverse-time-travel</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[5D Diplomacy With Multiverse Time TravelA new standard in measuring how galaxy-brained you are, 5D Diplomacy With Multiverse Time Travel combines the classic game of pure negotiation with the modern classic game of pure disorientation. Can you convince your opponent to support an attack in the present while simultaneously backstabbing them five years ago and seven timelines over?Inspired by and indebted to the board game Diplomacy and the video game 5D Chess With Multiverse Time Travel. Both are excellent in their own right, so we recommend picking up a copy of each to understand the rules for 5D Diplomacy. is a trademark of Avalon Hill. 5D Chess With Multiverse Time Travel is a trademark of Thunkspace, LLC. 5D Diplomacy With Multiverse Time Travel and its creators are not affiliated with either  or 5D Chess With Multiverse Time Travel.If you find a bug, please raise an issue.Note that official development of new features has come to an end. Issues requesting new or modified gameplay features will probably be rejected. Only bug fixes, performance improvements, and quality of life adjustments are likely to be accepted as suggestions.Feel free to fork this repo and modify the code there if you wish to experiment with more radical changes to the rules or UI. Visit the 5D Diplomacy Discord server to discuss new rules and theory with others.There are currently two options for installing 5D Diplomacy. Use quick installation if you just want to play the game. Use manual installation if you want to make code changes.The correct version of Docker for your operating system.Open Docker and leave it running.Open the CLI for your operating system and navigate inside the folder where you've downloaded this repository. If you don't know how to do this, use this tutorial.Via the CLI, run the command docker compose build frontend backend and wait for it to complete.Via the CLI, run the command  and wait for it to complete.Wait an extra few seconds for the server to start up. If you experience errors creating a game in the next step, try waiting longer.If you ever update the code (manually or via a pull from this repository), you will need to run docker compose down --rmi local, then run through the steps above again. Note that this may result in the database being wiped.To read server logs, run docker compose logs -f backend.The game consists of two components, found in the  and  directories. You must set up and run both to play 5D Diplomacy, unless you're connecting to someone else's server or have implemented a custom client.The  directory contains the original proof of concept from 2021. None of its contents are required for running the latest version of 5D Diplomacy.Navigate to the  directory.(Optional) If you want to connect to a custom database, copy  to create a new file in the same directory called appsettings.Development.json. Add your database's connection string as the value for the appropriate provider under , then set the value for  to match the name of the connection string.Run one of the following commands, depending on your configuration: 
  If you aren't using a custom database, i.e. if you didn't follow the optional step above, run dotnet ef database update --context SqliteGameContext.If you're using a custom SQLite database, run dotnet ef database update --context SqliteGameContext.If you're using a custom SQL Server database, run dotnet ef database update --context SqlServerGameContext.Run  to start the server.The server will print its address to the console, likely http://localhost:5000 but it may be different. Note this down for later.Note that if you ever update the code with changes that affect the database schema (e.g. if you pull a change from this repository that includes a new migration), you will have to run the appropriate dotnet ef database update command again.Navigate to the  directory.Copy  to create a new file in the same directory called .Inside , replace  with the address of the server noted earlier.Run  to start the client in the default browser.First see installation instructions above. 5D Diplomacy can be set to run normal games (where seven players join and enter orders individually) or sandbox games (where a single user enters all orders).If you wish to play a normal game or let other people see one of your sandbox games, you'll need to expose the domains of your client and/or server (if everyone has set up the client themselves, only a server needs to be exposed). There are various ways to do this, although this guide does not cover them.If you modify the code and host a game that others interact with, you must provide a link to your modified source code to comply with the terms of the AGPL license. We suggest updating the link to the source code in client/src/components/pages/LandingPage.tsx.To create a normal game, one player must choose the new game option from the main menu. They must choose the adjacency setting (see game rules below). After a game has been created, the initiating player enters the game and sees the game ID in the top left corner, which they must copy and send to other players.Other players can then use the join game option from the main menu to join with the supplied game ID.Note that 5D Diplomacy has no in-built messaging system. Unless you want to play without press, you require a separate program to send and receive press, e.g. a messaging app or voice calls.A possible exploit exists when playing multiplayer games. Since 5D Diplomacy has no user logins or verification, a player can join as someone else and enter their orders before them. The alternative - allowing each nation to join only once - would mean players can't rejoin after a break or connection issues. While Diplomacy is a game about breaking trust, you'll simply have to trust players not to be quite this devious.To create a sandbox game, select new game from the main menu and choose the sandbox option. Also set the adjacency setting (see game rules below).In sandbox mode, turns advance after submission whether all nations have orders or not.The rules of 5D Diplomacy generally extend the rules of regular Diplomacy. This guide covers only deviations from the rules of the base game.The game world consists of a grid of Diplomacy boards. Each row is a timeline, and each timeline progresses with boards following the standard Diplomacy turns (Spring 1901, then Fall 1901, then Winter 1901, then Spring 1902, etc.).At a given time, only units on the active boards (those furthest to the right in each timeline) can have new orders assigned. Other units are locked into their pre-existing orders, which can't be changed, though their resolution can.Units in spring or fall turns can be given hold, move, support or convoy orders. These are validated against standard Diplomacy adjacency rules, with extra possibilities for multiverse travel. The adjacency strictness setting (chosen when a new game is created) determines how units can move through the multiverse.With strict adjacencies, a unit can move/support/convoy to:Any adjacent region on its own board.The same region on a different board exactly one timeline up or down, e.g. moving from Paris in Timeline 2 to Paris in Timeline 1.The same region on a different board exactly one board in the past, e.g. moving from Berlin in Fall 1901 to Berlin in Spring 1901. Note that moving to winter boards is forbidden and these are skipped when determining board adjacencies, so Spring 1902 is adjacent to Fall 1901.Any region it is successfully convoyed to (see below).With loose adjacencies, a unit can move/support/convoy to:Any adjacent region on its own board.The same region on a different board exactly one timeline up or down, or any region adjacent to that region within its board, e.g. moving from Paris in Timeline 2 to Gascony in Timeline 1.The same region on a different board exactly one board in the past, or any region adjacent to that region within its board, e.g. moving from Berlin in Fall 1901 to Kiel in Spring 1901. Winter boards are still ignored.Any region it is successfully convoyed to (see below).In either case, note in particular that movement one board diagonally is not permitted (without a convoy).Convoys extend the quirk of standard Diplomacy that allows armies to move an arbitrary distance in a single turn if a chain of convoying fleets exists. Providing each fleet is adjacent to the next and all are ordered to perform the same convoy, an army could go almost anywhere.Units are however forbidden from moving into boards that don't exist yet, even with convoys. Convoys and supports though can anticipate a future unit moving back in time, so the player can use the ghost board to enter supports/convoys via an arbitrary location in the multiverse.Any units in (spring or fall) boards not assigned orders are given a hold order by default.The rule of thumb for adjudication: each time all orders for a turn are submitted, all orders in the entire world are adjudicated together, as if in a single enormous Diplomacy board.In particular, new orders could affect a prior resolution of existing orders, e.g. a unit that bounced now has support and so moves successfully. This extends to supports/convoys across time, e.g. convoys that were previously invalid may become valid if the future army appears and performs the expected move.If the new resolution matches an existing child board that spawned from this one, then no new timeline splits. So if two units bounced and both receive one new support from their relative future next turn, they still bounce and no new board is created (assuming no other changes elsewhere on this board).If the new resolution does not match an existing child board that spawned from this one, a new timeline appears. New timelines always appear below all existing timelines, and are always created in a canonical order (earliest board first; if boards are of equal age, lowest timeline number first).Note that this is different to 5D Chess where boards can spawn above or below existing timelines, potentially changing the coordinates of existing boards. There's no concept of a turn belonging to a player in Diplomacy (instead, they belong to everyone simultaneously) and 5D Diplomacy extends this thinking, so timelines spawn in only one direction. Board coordinates also never change.Main turns (spring and fall) and winter boards adjudicate simultaneously if all are at the end of their respective timelines. So a player may be creating builds on one board and creating moves on another in the same turn. Though of course these must be kept separate, so building is not permitted on movement boards and vice versa.Build/disband counts are per board. If a player controls fewer centres than they have units in one timeline but more in another, the difference does not cancel out: they must disband in the former and may build only in the latter. If they fail to enter enough disbands on a given board, units are removed from that board at random.If any board requires retreats, adjudication pauses for all boards without retreats. Retreats may only move to an adjacent region on the same board.A player achieves victory under one of the following conditions:They are the only player to control at least 18 unique supply centres across all active boards. Unique here means unique by region name, so controlling Serbia in Timeline 1 and Serbia in Timeline 2 counts as only one supply centre.If more than one player controls more than 18 unique supply centres, they are the only one with a clear majority. It's possible for two players to reach 18 centres in the same turn, e.g. if they have targeted different timelines.As with regular Diplomacy, it's possible for 5D Diplomacy to feature variant maps with completely different region arrangements. Other variants, such as variants with new rules, are not supported.Modifying the server to adjudicate custom variants in 5D is simple. First, edit the list of nations in . Then edit the JSON files in the folder  to match the intended board. Any subsequent run of the server will use those to create and adjudicate worlds.Modify  to change supply centres and starting/home centres.Modify  to change connections between regions.Modify  to change regions.Modify  to change starting units.Modifying the client is tricker as it is much more tied to this particular Diplomacy board. While client/src/data/regions.ts contains the list of regions and associated data, you will also need to replace the SVG files in  and then reference them in client/src/hooks/useRegionSvg.tsx.]]></content:encoded></item><item><title>juspay/hyperswitch</title><link>https://github.com/juspay/hyperswitch</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[An open source payments switch written in Rust to make payments fast, reliable and affordable
  Single API to access the payments ecosystem and its features 
 Juspay, founded in 2012, is a global leader in payment orchestration and checkout solutions, trusted by 400+ leading enterprises and brands worldwide. Hyperswitch is Juspay's new generation of composable, commercial open-source payments platform for merchant and brands. It is an enterprise-grade, transparent and modular payments platform designed to provide digital businesses access to the best payments infrastructure. 
Here are the key components of Hyperswitch that deliver the whole solution:Hyperswitch Backend: Hyperswitch backend enables seamless payment processing with comprehensive support for various payment flows - authorization, authentication, void and capture workflows along with robust management of post-payment processes like refunds and chargeback handling. Additionally, Hyperswitch supports non-payment use cases by enabling connections with external FRM or authentication providers as part of the payment flow. The backend optimizes payment routing with customizable workflows, including success rate-based routing, rule-based routing, volume distribution, fallback handling, and intelligent retry mechanisms for failed payments based on specific error codes.SDK (Frontend): The SDK, available for web, Android, and iOS, unifies the payment experience across various methods such as cards, wallets, BNPL, bank transfers, and more, while supporting the diverse payment flows of underlying PSPs. When paired with the locker, it surfaces the user's saved payment methods.Control Center: The Control Center enables users to manage the entire payments stack without any coding. It allows the creation of workflows for routing, payment retries, and defining conditions to invoke 3DS, fraud risk management (FRM), and surcharge modules. The Control Center provides access to transaction, refund, and chargeback operations across all integrated PSPs, transaction-level logs for initial debugging, and detailed analytics and insights into payment performance.You can run Hyperswitch on your system using Docker compose after cloning this repository.git clone --depth 1 --branch latest https://github.com/juspay/hyperswitch
cd hyperswitch
docker compose up -d
Check out the local setup guide for a more details on setting up the entire stack or component wise. This takes 15-mins and gives the following output[+] Running 2/2
✔ hyperswitch-control-center Pulled 2.9s
✔ hyperswitch-server Pulled 3.0s
[+] Running 6/0

✔ Container hyperswitch-pg-1 Created 0.0s
✔ Container hyperswitch-redis-standalone-1 Created 0.0s
✔ Container hyperswitch-migration_runner-1 Created 0.0s
✔ Container hyperswitch-hyperswitch-server-1 Created 0.0s
✔ Container hyperswitch-hyperswitch-web-1 Created 0.0s
✔ Container hyperswitch-hyperswitch-control-center-1 Created 0.0s

Attaching to hyperswitch-control-center-1, hyperswitch-server-1, hyperswitch-web-1, migration_runner-1, pg-1, redis-standalone-1
The fastest and easiest way to try Hyperswitch on AWS is via our CDK scriptsClick on the following button for a quick standalone deployment on AWS, suitable for prototyping. No code or setup is required in your system and the deployment is covered within the AWS free-tier setup.Sign-in to your AWS console.Follow the instructions provided on the console to successfully deploy Hyperswitch. This takes 30-45mins and gives the following outputhttp://hyperswitch-<host-id.region>.elb.amazonaws.comhttp://<cloudfront.host-id>/0.103.1/v0/HyperLoader.jsControl center server running onhttp://hyperswitch-control-center-<host-id.region>.elb.amazonaws.com, Login with Email: Hyperswitch Demo Store running onhttp://hyperswitch-sdk-demo-<host-id.region>.elb.amazonaws.comhttp://hyperswitch-logs-<host-id.region>.elb.amazonaws.com, Login with username: , password: We support deployment on GCP and Azure via Helm charts which takes 30-45mins. You can read more at Hyperswitch docs.You can experience the product by signing up for our hosted sandbox. The signup process accepts any email ID and provides access to the entire Control Center. You can set up connectors, define workflows for routing and retries, and even try payments from the dashboard.Support, Feature requests & BugsFor any support, join the conversation in SlackFor new product features, enhancements, roadmap discussions, or to share queries and ideas, visit our GitHub DiscussionsPayments are evolving rapidly worldwide, with hundreds of processors, fraud detection systems, authentication modules, and new payment methods and flows emerging. Businesses building or managing their own payment stacks often face similar challenges, struggle with comparable issues, and find it hard to innovate at the desired pace.Hyperswitch serves as a well-architected designed reference platform, built on best-in-class design principles, empowering businesses to own and customize their payment stack. It provides a reusable core payments stack that can be tailored to specific requirements while relying on the Hyperswitch team for enhancements, support, and continuous innovation.Embrace Payments Diversity: It will drive innovation in the ecosystem in multiple ways.Make it Open Source: Increases trust; Improves the quality and reusability of software.Be community driven: It enables participatory design and development.Build it like Systems Software: This sets a high bar for Reliability, Security and Performance SLAs.Maximise Value Creation: For developers, customers & partners.This project is being created and maintained by JuspayThe core team of 150+ engineers building Hyperswitch. Keep up the great work! 🥂]]></content:encoded></item><item><title>GitHubDaily/GitHubDaily</title><link>https://github.com/GitHubDaily/GitHubDaily</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[坚持分享 GitHub 上高质量、有趣实用的开源技术教程、开发者工具、编程网站、技术资讯。A list cool, interesting projects of GitHub.多年以前，我曾看到 GitHub 开源项目作者、全栈工程师 TJ Holowaychunk 说过这么一句话："I don't read books, never went to school, I just read other people's code and always wonder how things work"。从那时起，我便认为，通过阅读源码，站在前辈的角度上，去思考代码架构与程序逻辑，乃是提升编程技巧最好的方式。因此，GitHub 也自然而然的，成为我最喜爱的开发者平台。秉着挖掘开源价值的初衷，GitHubDaily 自 2015 年 10 月 10 日正式成立。我们希望能通过这一举措，帮助开发者们发现当下最火的开源项目，掌控最新技术动态, 扩大技术视野, 并从开源项目的学习中获得编程能力的提升。目前，GitHubDaily 已累积分享超过 8000 个开源项目，内容包括但不限于 GitHub 上的开源技术资料、开发者工具、编程网站以及成熟应用。除了 GitHub 之外，我们也开始在下面多个社交媒体平台，帮助开发者传播与分享优质开源项目，挖掘其未来的技术应用前景。如果你想接收最新的 GitHub 开源项目资讯，可以关注一下👇有不错的开源项目，也欢迎到本仓库的 issues 推荐或自荐项目，我们期待你的分享。下面是对 GitHubDaily 在 2024 年所推荐的项目进行分类整理，方便大家查找以往分享过的内容。]]></content:encoded></item><item><title>cypress-io/cypress</title><link>https://github.com/cypress-io/cypress</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[Fast, easy and reliable testing for anything that runs in a browser. The web has evolved. Finally, testing has too.  Fast, easy and reliable testing for anything that runs in a browser. Install Cypress for Mac, Linux, or Windows, then get started.npm install cypress --save-dev
pnpm add cypress --save-dev
This project is licensed under the terms of the MIT license.Configure a badge for your project's README to show your test status or test count in the Cypress Cloud.Or let the world know your project is using Cypress with the badge below.[![Cypress.io](https://img.shields.io/badge/tested%20with-Cypress-04C38E.svg)](https://www.cypress.io/)
]]></content:encoded></item><item><title>microsoft/markitdown</title><link>https://github.com/microsoft/markitdown</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[Python tool for converting files and office documents to Markdown.[!IMPORTANT] MarkItDown 0.0.2 alpha 1 (0.0.2a1) introduces a plugin-based architecture. As much as was possible, command-line and Python interfaces have remained the same as 0.0.1a3 to support backward compatibility. Please report any issues you encounter. Some interface changes may yet occur as we continue to refine MarkItDown to a first non-alpha release.MarkItDown is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc). It supports:Images (EXIF metadata and OCR)Audio (EXIF metadata and speech transcription)Text-based formats (CSV, JSON, XML)ZIP files (iterates over contents)To install MarkItDown, use pip: . Alternatively, you can install it from the source:git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e packages/markitdown
markitdown path-to-file.pdf > document.md
Or use  to specify the output file:markitdown path-to-file.pdf -o document.md
You can also pipe content:cat path-to-file.pdf | markitdown
MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:markitdown --list-plugins
markitdown --use-plugins path-to-file.pdf
To find available plugins, search GitHub for the hashtag . To develop a plugin, see packages/markitdown-sample-plugin.Azure Document IntelligenceTo use Microsoft Document Intelligence for conversion:markitdown path-to-file.pdf -o document.md -d -e "<document_intelligence_endpoint>"
More information about how to set up an Azure Document Intelligence Resource can be found herefrom markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
Document Intelligence conversion in Python:from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="<document_intelligence_endpoint>")
result = md.convert("test.pdf")
print(result.text_content)
To use Large Language Models for image descriptions, provide  and :from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o")
result = md.convert("example.jpg")
print(result.text_content)
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest < ~/your-file.pdf > output.md
This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.Navigate to the MarkItDown package:Install  in your environment and run tests:pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
(Alternative) Use the Devcontainer which has all the dependencies installed:# Reopen the project in Devcontainer and run:
hatch test
Run pre-commit checks before submitting a PR: pre-commit run --all-filesContributing 3rd-party PluginsYou can also contribute by creating and sharing 3rd party plugins. See packages/markitdown-sample-plugin for more details.This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.]]></content:encoded></item><item><title>codecrafters-io/build-your-own-x</title><link>https://github.com/codecrafters-io/build-your-own-x</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[What I cannot create, I do not understand — Richard Feynman.]]></content:encoded></item><item><title>zaidmukaddam/scira</title><link>https://github.com/zaidmukaddam/scira</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[Scira (Formerly MiniPerplx) is a minimalistic AI-powered search engine that helps you find information on the internet. Powered by Vercel AI SDK! Search with models like Grok 2.0.A minimalistic AI-powered search engine that helps you find information on the internet.Tavily AI - For search grounding and web search capabilities: Get answers to your questions using Anthropic's Models.: Search the web using Tavily's API.: Get information from a specific URL.: Get the current weather for any location using OpenWeather's API.: Run code snippets in multiple languages using E2B's API.: Get the location of any place using Google Maps API, Mapbox API, and TripAdvisor API.: Track flights using AviationStack's API.Trending Movies and TV Shows: Get information about trending movies and TV shows.: Get information about any movie or TV show.Set Scira as your default search engineOpen the Chrome browser settings:Click on the three vertical dots in the upper right corner of the browser.Select "Settings" from the dropdown menu.Go to the search engine settings:In the left sidebar, click on "Search engine."Then select "Manage search engines and site search."Click on "Add" next to "Site search."Set the search engine name:Enter  in the "Search engine" field.Set the search engine URL:Enter  in the "URL with %s in place of query" field.Set the search engine shortcut:Enter  in the "Shortcut" field.Click on the three dots next to the search engine you just added.Select "Make default" from the dropdown menu.After completing these steps, you should be able to use Scira as your default search engine in Chrome.To run the example locally you need to:Sign up for accounts with the AI providers you want to use. OpenAI and Anthropic are required, Tavily is required for the web search feature.Obtain API keys for each provider.Set the required environment variables as shown in the  file, but in a new file called . to install the required dependencies. to launch the development server.This project is licensed under the MIT License - see the LICENSE file for details.]]></content:encoded></item><item><title>kuchin/awesome-cto</title><link>https://github.com/kuchin/awesome-cto</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[— Hello, my name is Dima and I'm a CTO]]></content:encoded></item><item><title>golang/go</title><link>https://github.com/golang/go</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[The Go programming languageGo is an open source programming language that makes it easy to build simple, reliable, and efficient software.Unless otherwise noted, the Go source files are distributed under the BSD-style license found in the LICENSE file.If a binary distribution is not available for your combination of operating system and architecture, visit https://go.dev/doc/install/source for source installation instructions.Go is the work of thousands of contributors. We appreciate your help!Note that the Go project uses the issue tracker for bug reports and proposals only. See https://go.dev/wiki/Questions for a list of places to ask questions about the Go language.]]></content:encoded></item><item><title>Zipstack/unstract</title><link>https://github.com/Zipstack/unstract</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 15:53:27 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Github Trending</source><content:encoded><![CDATA[No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documentsPrompt Studio's primary reason for existence is so you can develop the necessary prompts for document data extraction super efficiently. It is a purpose-built environment that makes this not just easy for you—but, lot of fun! The document sample, its variants, the prompts you're developing, outputs from different LLMs, the schema you're developing, costing details of the extraction and various tools that let you measure the effectiveness of your prompts are just a click away and easily accessible. Prompt Studio is designed for effective and high speed development and iteration of prompts for document data extraction. Welcome to IDP 2.0!🧘‍♀️ Three step nirvana with Workflow StudioAutomate critical business processes that involve complex documents with a human in the loop. Go beyond RPA with the power of Large Language Models.🌟 : Add documents to no-code Prompt Studio and do prompt engineering to extract required fields  🌟 : Configure Prompt Studio project as API deployment or configure input source and output destination for ETL Pipeline 🌟 : Deploy Workflows as unstructured data APIs or unstructured data ETL Pipelines!Linux or MacOS (Intel or M-series)Docker Compose (if you need to install it separately)Next, either download a release or clone this repo and do the following:That's all there is to it!Follow these steps to change the default username and password.See user guide for more details on managing the platform. Another really quick way to experience Unstract is by signing up for our hosted version. It comes with a 14 day free trial!Unstract comes well documented. You can get introduced to the basics of Unstract, and learn how to connect various systems like LLMs, Vector Databases, Embedding Models and Text Extractors to it. The easiest way to wet your feet is to go through our Quick Start Guide where you actually get to do some prompt engineering in Prompt Studio and launch an API to structure varied credit card statements!Contributions are welcome! Please see CONTRIBUTING.md for further details to get started easily.👋 Join the LLM-powered automation communityDo copy the value of  config in either  or  file to a secure location.Adapter credentials are encrypted by the platform using this key. Its loss or change will make all existing adapters inaccessible!In full disclosure, Unstract integrates Posthog to track usage analytics. As you can inspect the relevant code here, we collect the minimum possible metrics. Posthog can be disabled if desired by setting  to  in the frontend's .env file.]]></content:encoded></item><item><title>Machine Learning with TypeScript and TensorFlow: Training your first model</title><link>https://wanago.io/2025/02/03/machine-learning-typescript-tensorflow-training-first-model/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Code Like a Pro: Implementing SOLID Design in React Ecosystem</title><link>https://jsdev.space/solid-design-react/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Magento Development With Vue.js and React.js</title><link>https://techlogitic.net/magento-development-with-vue-js-and-react-js/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Worker Threads in Node.js: A Comprehensive Guide to Multi-Threading</title><link>https://www.infinitecircuits.dev/blogs/blog/worker-threads-in-nodejs-a-comprehensive-guide-to-multi-threading-KFIXCLEtBbxCStErRwGy</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to Completely Remove Node.js from Your Computer</title><link>https://jsdev.space/howto/remove-nodejs/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to Hire Kotlin Developers: A Comprehensive Guide</title><link>https://7span.com/blog/how-to-hire-kotlin-developers</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Raydium Volume Bot V3: Volume Bot in Raydium and Meteora</title><link>https://github.com/aiagentdev-sol/raydium-volume-bot</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Brisa v.0.2.5! Increasingly robust 🦾</title><link>https://brisa.build/blog/brisa-0.2.5</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>JavaScript Decorators &amp; Annotations: A Practical Guide to Metaprogramming</title><link>https://www.infinitecircuits.dev/blogs/blog/javascript-decorators-annotations-a-practical-guide-to-metaprogramming-vjl6dckieL0K6jpwqQrL</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mastering React Refs: Advanced Techniques with useCombinedRef Hook</title><link>https://jsdev.space/usecombinedref-react/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Boost React Performance: Avoid Re-Renders, Optimize State &amp; Server Components</title><link>https://www.infinitecircuits.dev/blogs/blog/boost-react-performance-avoid-re-renders-optimize-state-server-components-nLGvJLbVKt48HQLcvftw</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>HowTo Generate a GUID/UUID in JavaScript: Best Methods &amp; Examples</title><link>https://jsdev.space/howto/generate-uuid-js/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Future of Frontend Engineering: Trends for 2025 and Beyond</title><link>https://www.infinitecircuits.dev/blogs/blog/the-future-of-frontend-engineering-trends-for-2025-and-beyond-NzB9LPTcdQZBWiRK5FH9</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Autho: Your Authy Desktop Alternative and Beyond (Open Source)</title><link>https://blog.niradler.com/autho-your-authy-desktop-alternative-and-beyond-open-source</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>React Server Components Explained: The Future of High-Performance React Apps?</title><link>https://www.infinitecircuits.dev/blogs/blog/react-server-components-explained-the-future-of-high-performance-react-apps-0jzYaIwUrSvbKpwP2yuz</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>RxDB - The Firestore Alternative That Can Sync with Your Own Backend</title><link>https://rxdb.info/articles/firestore-alternative.html</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ESLint plugin for transforming negated boolean expressions via De Morgan’s laws</title><link>https://github.com/azat-io/eslint-plugin-de-morgan</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>API with NestJS #186. What’s new in Express 5?</title><link>https://wanago.io/2025/02/10/api-nestjs-express-5/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Build a Vector Image Service Using ThreeJS and Vite | Tutorial</title><link>https://jsdev.space/image-vectorization/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>🚀 How JavaScript Works: Understanding V8 JIT and Its Impact on UI Performance</title><link>https://www.infinitecircuits.dev/blogs/blog/how-javascript-works-understanding-v8-jit-and-its-impact-on-ui-performance-CVx99743MWKhLx9oTjbY</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chronos v15.0</title><link>http://www.echojs.com/news/42822</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SVAR Svelte Editor: Easy Way to Edit Structured Data Records</title><link>https://svar.dev/blog/svelte-editor-released/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to Copy Text to Clipboard in JavaScript | Modern Methods</title><link>https://jsdev.space/howto/copy-text-js/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>dinogameapp</title><link>https://dinogame.app/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Flutter vs. React Native – Which is Better for Your Project?</title><link>https://7span.com/blog/flutter-vs-react-native</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CORS Policy Explained: How to Fix Cross-Origin Errors in Web Development</title><link>https://www.infinitecircuits.dev/blogs/blog/cors-policy-explained-how-to-fix-cross-origin-errors-in-web-development-2CYsMVB4ceZmRjOoovZY</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mastering React&apos;s useReducer Hook: State Management Guide</title><link>https://jsdev.space/react-hook-usereducer/</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Magento Development With Vue.js and React.js</title><link>https://parthdeveloper.livepositively.com/magento-development-with-vue-js-and-react-js/new=1</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Display the version of a deployed Angular application</title><link>https://medium.com/ekino-france/display-the-version-of-a-deployed-angular-application-608cd6fff3d6</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Network Security Explained: HTTPS, TLS, SSL, and Encryption 🔐</title><link>https://www.infinitecircuits.dev/blogs/blog/network-security-explained-https-tls-ssl-and-encryption-lbVrUEv541JEEAecVcpy</link><author></author><category>echojs</category><pubDate>Sat, 15 Feb 2025 15:53:12 +0000</pubDate><source url="http://www.echojs.com/">EchoJS</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Unlocking the Potential of Quantum Computing</title><link>https://developers.googleblog.com/en/unlocking-the-potential-of-quantum-computing-a-developers-guide-to-error-correction/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[A free Coursera course on quantum error correction, developed by Google Quantum AI, explains the importance of error correction in quantum computing and provides an overview of quantum errors.]]></content:encoded></item><item><title>The next chapter of the Gemini era for developers</title><link>https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Gemini 2.0 Flash has enhanced capabilities like multimodal outputs and native tool use, and introduces new coding agents to improve developer productivity, now available for testing in Google AI Studio.]]></content:encoded></item><item><title>Registration is Open for Google Cloud Next 2025</title><link>https://developers.googleblog.com/en/registration-is-open-for-google-cloud-next-2025/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Google Cloud Next 2025, happening April 9-11 in Las Vegas, will feature expanded developer content, interactive experiences, and opportunities to connect with peers and Google experts.]]></content:encoded></item><item><title>Introducing the Android XR SDK Developer Preview</title><link>https://developers.googleblog.com/en/introducing-android-xr-sdk-developer-preview/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[The Android XR SDK, a new platform for building extended reality (XR) experiences on Android, is now available for devs to try, and to give feedback.]]></content:encoded></item><item><title>Celebrating Flutter’s “Production Era”</title><link>https://developers.googleblog.com/en/celebrating-flutters-production-era/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Flutter is transitioning into its “production era,” marked by widespread developer adoption and success stories, including deeper iOS fidelity, seamless platform integration, and enhanced developer tools, making it a top choice for building apps at scale.]]></content:encoded></item><item><title>Apigee API hub is now generally available</title><link>https://developers.googleblog.com/en/apigee-api-hub-is-now-generally-available/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Apigee API hub is a centralized repository for your entire API ecosystem, providing a single source of truth.]]></content:encoded></item><item><title>Build Go applications using Project IDX and the Gemini API</title><link>https://developers.googleblog.com/en/build-go-applications-project-idx-gemini-api/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Learn how to build Go applications using Project IDX, an AI-assisted workspace for full-stack app development.]]></content:encoded></item><item><title>Dive Deep into Gemini: Explore Starter Apps in AI Studio</title><link>https://developers.googleblog.com/en/deep-dive-gemini-developer-ready-starter-apps/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Explore three Gemini starter apps that provide developers with production ready tools to build AI-powered projects with open source functionalities like spatial analysis, and video interactions.]]></content:encoded></item><item><title>Celebrating innovation: Gemini API Developer Competition</title><link>https://developers.googleblog.com/en/celebrating-innovation-gemini-api-developer-competition/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Explore innovative projects from the Gemini API Competition, like Omni, EcoTrack, ITMZ, Jana AI, Eagle AI, and AlphaFit, demonstrating the potential of the Gemini API to enhance productivity, promote sustainability, and improve well-being.]]></content:encoded></item><item><title>Beyond English: How Gemma open models are bridging the language gap</title><link>https://developers.googleblog.com/en/building-more-inclusive-llms-using-gemma-open-models/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[AI Singapore and INSAIT teams have leveraged Gemma, a family of open-source language models, to create LLMs tailored to the unique needs of their communities, in a show of innovation and inclusivity in AI.]]></content:encoded></item><item><title>See the Similarity: Personalizing Visual Search with Multimodal Embeddings</title><link>https://developers.googleblog.com/en/see-the-similarity-personalizing-visual-search-with-multimodal-embeddings/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Learn how to build a visual search tool using Google's Multimodal Embeddings API and explore how to apply this technology for searching images, slides, and more.]]></content:encoded></item><item><title>Gemini 2.0: Level Up Your Apps with Real-Time Multimodal Interactions</title><link>https://developers.googleblog.com/en/gemini-2-0-level-up-your-apps-with-real-time-multimodal-interactions/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[The Multimodal Live API for Gemini 2.0 enables real-time multimodal interactions between humans and computers, and can be used to build real-time virtual assistants and adaptive educational tools.]]></content:encoded></item><item><title>Build the future of home with Google Home APIs</title><link>https://developers.googleblog.com/en/build-the-future-of-home-with-google-home-apis/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[The Google Home APIs are now in public developer beta for Android, allowing developers to build innovative smart home experiences for over 600M devices using Google's hubs, Matter infrastructure, and automation engine.]]></content:encoded></item><item><title>Building a better smart home</title><link>https://developers.googleblog.com/en/building-a-better-smart-home-expanding-access-for-developers-and-users/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[The public beta launch of Home APIs for Android allows developers to create better smart home experiences. This launch emphasizes investments in Matter to improve device connectivity and interoperability, and increase smart home accessibility.]]></content:encoded></item><item><title>Google AI Developers Community Spotlight Contest</title><link>https://developers.googleblog.com/en/google-ai-developers-community-spotlight-contest/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[The Google AI Developers Community Spotlight Contest is seeking innovative, diverse projects using Google's AI tools like Gemini and Gemma to create code repositories, videos, documentation, and more.]]></content:encoded></item><item><title>Vertex AI RAG Engine: A developers tool</title><link>https://developers.googleblog.com/en/vertex-ai-rag-engine-a-developers-tool/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Vertex AI RAG Engine, a managed orchestration service, streamlines the process of retrieving and feeding relevant information to Large Language Models. This enables developers to build robust, grounded generative AI apps that ensure responses are factually grounded.]]></content:encoded></item><item><title>Gemini 2.0: Flash, Flash-Lite and Pro</title><link>https://developers.googleblog.com/en/gemini-2-family-expands/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[The Gemini 2.0 model family is seeing significant updates, including the release of Gemini 2.0 Flash, which is now production-ready and boasts higher rate limits, enhanced performance, and simplified pricing. Developers can also start testing an updated experimental version of Gemini 2.0 Pro today. Additionally, a new variant called Gemini 2.0 Flash-Lite, specifically designed for large-scale workloads, will be made available next week.]]></content:encoded></item><item><title>Imagen 3 arrives in the Gemini API</title><link>https://developers.googleblog.com/en/imagen-3-arrives-in-the-gemini-api/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Imagen 3 – now available in Google AI Studio and the Gemini API – offers developers state-of-the-art image generation with brighter, better-composed images in diverse styles, and simplified image generation through text prompts.]]></content:encoded></item><item><title>Get ready for Google I/O May 20-21</title><link>https://developers.googleblog.com/en/get-ready-for-google-io-2025/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[Google I/O returns May 20-21. Watch the livestreams for updates on Android, AI, web, and cloud. Registration is open on the Google I/O website.]]></content:encoded></item><item><title>Beyond the Chatbot: Agentic AI with Gemma</title><link>https://developers.googleblog.com/en/beyond-the-chatbot-agentic-ai-with-gemma/</link><author></author><category>googledev</category><pubDate>Sat, 15 Feb 2025 15:53:03 +0000</pubDate><source url="https://developers.googleblog.com/rss/">Google Developers</source><content:encoded><![CDATA[A practical guide to constructing a Gemma 2-based Agentic AI system – a type of AI that can make its own decisions and use external tools to achieve goals – that can generate dynamic content for a fictional game world.]]></content:encoded></item><item><title>Jeep Claims &apos;Software Glitch&apos; Disabled Opting-Out of In-Vehicle Pop-Up Ads in &apos;a Few&apos; Cases</title><link>https://tech.slashdot.org/story/25/02/15/0149202/jeep-claims-software-glitch-disabled-opting-out-of-in-vehicle-pop-up-ads-in-a-few-cases?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 15 Feb 2025 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Remember Jeep's new in-dash pop-up ads which reportedly appeared every time you stopped? 
"Since I'm a journalist, or at least close enough, I decided that I should at least get Stellantis/Jeep's side of things," writes car-culture site The Autopian:


Would Stellantis do something so woefully misguided and annoying? I reached out to our Stellantis/Jeep contact to ask and was initially told that they were "investigating" on their end, which to me felt like a stalling tactic while the proper ass-covering plans were conceived. I eventually got this response from a Stellantis spokesperson: 

 "This was an in-vehicle message designed to inform Jeep customers about Mopar extended vehicle care options. A temporary software glitch affected the ability to instantly opt out in a few isolated cases, though instant opt-out is the standard for all our in-vehicle messages. Our team had already identified and corrected the error, and we are following up directly with the customer to ensure the matter is fully resolved..." 

I suppose a glitch is possible, though I've not seen any examples of this ad popping up with the instant opt-out option available, but I guess it must exist, since not all Jeep owners seem to have had to deal with these ads. I suspect if this was happening to more people than these "few isolated cases" we'd still be cleaning up from the aftermath of the riots and uprisings. 

Because, as they write, "Really, I can't think of a quicker way to incur the wrath of nearly every human..."]]></content:encoded></item><item><title>What is an encryption backdoor?</title><link>https://techcrunch.com/2025/02/15/what-is-an-encryption-backdoor/</link><author>Natasha Lomas</author><category>tech</category><pubDate>Sat, 15 Feb 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Talk of backdoors in encrypted services is once again doing the rounds after reports emerged that the U.K. government is seeking to force Apple to open up iCloud’s end-to-end encrypted (E2EE) device backup offering. Officials were said to be leaning on Apple to create a “backdoor” in the service that would allow state actors to […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Infocon: green</title><link>https://isc.sans.edu/diary.html?rss</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Sat, 15 Feb 2025 14:35:03 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[The Danger of IP Volatility]]></content:encoded></item><item><title>Antarctica&apos;s Only Insect</title><link>https://www.404media.co/antarcticas-only-insect/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-09.17.41@2x.png" length="" type=""/><pubDate>Sat, 15 Feb 2025 14:00:13 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[Welcome back to the Abstract, 404 Media's weekly roundup of scientific studies to distract us from our present dystopia!This week, we are traveling back in time to 16th century Transylvania, so please make sure you are up to date on your bubonic plague shots. A study reconstructed wild weather events through the eyes of record-keepers during this fraught period, opening a tantalizing window into climate extremes unleashed by a vengeful God (according to contemporary reports).Then: making love the medaka way (get those anal fins ready). Next, the chillest insect in Antarctica (also: the only one). Finally, these turtles will dance for food, and yes, it’s very cute.The Haunting Weather Reports of 16th Century TransylvaniaRejoice, for this week has delivered one of the best varieties of study: Science via historical documents. Sure, ice cores and geological strata are great for reconstructing past climates, but nobody can bitch about the weather better than a good old-fashioned red-blooded member of team . To that end, researchers searched for mentions of weird weather across a trove of diaries, monastery records, travel notes, and other documents from 16th century Transylvania, during a “pivotal moment in climate history” when a centuries-long cooling event called the Little Ice Age intensified, according to researchers led by Ovidiu Răzvan Gaceu of the University of Oradea. These types of studies are packed with colorful human testimonies that can corroborate natural records. More importantly, though, they are just fun to read, especially during such an evocative time and place, freshly haunted by the vampiric spirit of Vlad the Impaler. Some highlights:In August 1526, heavy rainfall caused freak floods in Braşov that “washed the walls of the fortress, demolished the main gate, and the fish also got caught in the big church,” according to the Annals of Brașov. Fish in the church! The ultimate baptism.  In autumn 1553, people in the city of Cluj reported unusual weather events including “October strawberries.” For real, October is for pumpkins, get out of here with the strawbs. Turned out it was a bad omen—there was a plague the following winter. Keep that in mind if you see any late autumn strawberries: Kill on sight.Naturally, a lot of these accounts are heartbreaking. Locusts “sometimes covered the whole sky and destroyed grain crops” and caused terrible famines. A storm-related fire “killed 14 people and made 60 poor.” On September 29, 1582, “there was such a big storm, as it was said that it had never been seen before in the city of Cluj, which uprooted the trees and raised the roofs of the houses, people believed that it is sent by divinity to punish the crimes committed by them.” I mean, I’m not saying these people weren’t doing crimes. It’s 16th century Transylvania. Do what you gotta do. But that's not why there is extreme weather. You’re just in the Little Ice Age. The study ultimately identified “multiple pieces of evidence associated with extreme weather events, including 40 unusually warm summers and several years of excess precipitation or drought.” Taken together with natural archives, the documents paint a picture of troubled times, exacerbated by an unstable climate and possible emergent vampires. Relatable! Valentine’s Day is over, but the romantic mood is still in the air—or in the water, if you’re a medaka (flawless segue). Scientists have discovered that wild medaka, also known as Japanese rice fish, are fans of late-night booty calls, which is a behavior that has not been observed in captivity.“Although medaka and other model organisms are invaluable in laboratories, their ecology in the wild remains largely unknown,” said researchers led by Yuki Kondo of Osaka Metropolitan University. “This study showed that medaka in the wild initiate spawning during late nocturnal hours and exhibit vigorous courtship behavior at midnight.”Kondo and her colleagues recorded this vigorous courtship by placing GoPros into streams over the course of several summer nights in Gifu, Japan. The tapes revealed that medaka like to spawn in the dark, possibly to avoid predators during copulation. The results “provide the first empirical evidence that medaka mating begins significantly earlier than previously reported in the laboratory.”  For anyone who feels clueless about courtship, may I offer a page from the Medaka Sutra: “The spawning behavior of medaka follows a sequence of events: the male chases the female (following), the male swims rapidly around the female (quick circle), the male wraps his dorsal and anal fins around the female (wrapping), the female releases eggs, the male releases sperm (egg and sperm release), and the male leaves the female (leaving),” according to Kondo’s team.The only true love language is, indeed, spoken with anal fins.Major bonus points also go to Osaka Metropolitan University’s press team for throwing together this version of Edward Hopper’s famous “Nighthawks” painting with medaka getting drinks at a bar that is also named Medaka. It is genuinely one of the most inspired public relations efforts I have ever seen, and I’m going to get a print of it to hang on my wall.The Insect at the Edge of Earth, or the Antarctic midge, is the only insect that lives year-round on its namesake continent. Do you know how weird you have to be to be the  insect somewhere? But this midge doesn’t care. It just lives out its bug life, which lasts two years, in an otherwise bugless wasteland. Humans definitely care about the midge, though—how could we not? What is it doing there? How is it not dead? What can it teach us about cryopreservation? These questions are addressed in a new study that resolved mysteries about the animal’s interesting life cycle.“Freeze tolerance and cryoprotective dehydration are cold tolerance strategies used by various invertebrate species in polar regions and indeed,  utilises both for overwintering,” said researchers led by Mizuki Yoshida of the Ohio State University, who completed the project while at Osaka Metropolitan University (OMU killing it this week). “Larvae that are frozen in ice and cryoprotectively dehydrated readily survived 32 days of simulated overwintering,” the team said. “Unlike many insects restricted to highly specific microhabitats,  larvae inhabit a remarkably diverse range of substrates that differ in vegetation, substrate type, slope, drainage, and thermal and hydric conditions.”I love the phrasing of “readily survived” as if the midges were eager to show off their cryoprotective superpowers. After this 32-day period they emerged with “That all you got?” energy. By studying the bugs in these simulated conditions, the researchers confirmed that they rely on multiple overwintering strategies, including a state of arrested development called “obligate diapause.” “Diapause has long been assumed to be uncommon in Antarctic species, but the present study reveals that  utilises diapause for seasonal adaptation, as in many temperate species,” Yoshida and her colleagues said. In addition to being the only endemic Antarctic insect, this midge has the smallest genome of any known insect while also being the largest fully terrestrial animal on the continent, even though it’s only a few millimeters long. In other words, it is the biggest animal in Antarctica that doesn’t fly or swim. Okay, Antarctic midge. You just keep doing you.Last, turtles do a little victory dance when they find food. Yes, it is cute. Yes, there is a video.The footage (along with ) is part of a study that tested if turtles could distinguish the magnetic signatures of two geographical areas. When the turtles were exposed to signatures associated with an area they associated with food, they danced in anticipation of a meal, demonstrating that they could tell the signals apart—and party accordingly.  “Hallmarks of the behaviour include some or all of the following: tilting the body vertically, holding the head near or above water, opening the mouth, rapid alternating movement of the front flippers, and, occasionally, even spinning in place, hence the name ‘turtle dance,’” said researchers led by Kayla Goforth of Texas A&M University. “Turtles exhibited significantly higher levels of turtle dance behaviour when experiencing the field in which they had been fed.”With that, let’s all tilt vertically, spin in place, and shell-abrate the long weekend. Thanks for reading! See you next week.  ]]></content:encoded></item><item><title>Paragraf Is Building a &quot;Blank Canvas&quot; Graphene Foundry</title><link>https://spectrum.ieee.org/paragraf-graphene-foundry</link><author>Liam Critchley</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjQ2NTMyMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4MzQyMzgzM30.wJGF4-_y2VSVjXLsYFhhL9DuC-xHiTHWB-Ciq4DHQTU/image.jpg?width=600" length="" type=""/><pubDate>Sat, 15 Feb 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The company wants to make graphene sensors more accessible to industries]]></content:encoded></item><item><title>These Google Photos alternatives offer tons of storage options at a reasonable price</title><link>https://techcrunch.com/2025/02/15/these-google-photos-alternatives-offer-tons-of-storage-options-at-a-reasonable-price/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Sat, 15 Feb 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google Photos is a great service for storing images across devices. But Google Drive and Gmail only offer 15GB of storage for free. Google Photos used to offer free unlimited storage of images, but that is not the case anymore. If you are looking for a better photo storage plan, different features, or just want […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Career transition in to Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/</link><author>/u/Similar-Secretary-86</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 13:41:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA["I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated ]]></content:encoded></item><item><title>Will AI Lead to the Disintermediation of Knowledge?</title><link>https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/</link><author>Bill Schmarzo</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 13:28:49 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Key Blog Points: For decades, organizations have operated under the central assumption that knowledge flows downward. Senior leaders, industry veterans, and domain experts have traditionally been the primary gatekeepers of critical information. Their insights, honed over years of experience, have been the cornerstone of strategic decision-making. Enter artificial intelligence (AI). Many folks are concerned that… Read More »]]></content:encoded></item><item><title>Beyond Chat: Bringing Models to The Canvas • Lu Wilson • YOW! 2024</title><link>https://www.youtube.com/watch?v=pLvMsGG7zE8</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/pLvMsGG7zE8?version=3" length="" type=""/><pubDate>Sat, 15 Feb 2025 13:01:10 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at YOW! Australia 2024. #GOTOcon #YOW
https://yowcon.com

Lu Wilson - Software Engineer at tldraw @TodePond 

RESOURCES
https://bsky.app/profile/todepond.com
https://mastodon.social/@TodePond
https://twitter.com/TodePond
https://www.todepond.com

Links
https://tldraw.dev
https://makereal.tldraw.com
https://drawfast.tldraw.com
https://teach.tldraw.com

ABSTRACT
Whenever a new technology appears, our first instinct as developers is to offer "text" as the primary method of interaction. This has happened throughout computing history, with the computer terminal, with early smartphones, and now it's happening again with AI.

At tldraw, we’ve been working on moving AI interaction away from the chat-based interface, towards a richer canvas environment. It hasn't been easy! I'll show you all the challenges we've faced, and how we're currently overcoming them. Some of the solutions have been surprising. [...]

TIMECODES
00:00 Intro
00:58 The canvas
06:16 Beyond chat?
07:09 Demo
09:25 Demo: Make Real
14:52 Demo: Draw Fast
19:01 Demo: Teach
27:28 Demo: Computer
35:42 Demo: Fight Simulator
28:13 Conclusion
41:10 Outro

Read the full abstract here:
https://yowcon.com/brisbane-2024/sessions/3533

RECOMMENDED BOOKS
Alex Castrounis • AI for People and Business • https://amzn.to/3NYKKTo
Phil Winder • Reinforcement Learning • https://amzn.to/3t1S1VZ
Holden Karau, Trevor Grant, Boris Lublinsky, Richard Liu & Ilan Filonenko • Kubeflow for Machine Learning • https://amzn.to/3JVngcx
Kelleher & Tierney • Data Science (The MIT Press Essential Knowledge series) • https://amzn.to/3AQmIRg
Lakshmanan, Robinson & Munn • Machine Learning Design Patterns • https://amzn.to/2ZD7t0x
Lakshmanan, Görner & Gillard • Practical Machine Learning for Computer Vision • https://amzn.to/3m9HNjP

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#AI #GenAI #GenerativeAI #ArtificialIntelligence #ChatGPT #ML #MakeRealtldraw #MakeReal #tldraw #Teachtldraw #AIDriven #LuWilson #YOWcon

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>The IRS Is Buying an AI Supercomputer From Nvidia</title><link>https://tech.slashdot.org/story/25/02/15/0540249/the-irs-is-buying-an-ai-supercomputer-from-nvidia?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 15 Feb 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[According to The Intercept, the IRS is set to purchase an Nvidia SuperPod AI supercomputer to enhance its machine learning capabilities for tasks like fraud detection and taxpayer behavior analysis. From the report: With Elon Musk's so-called Department of Government Efficiency installing itself at the IRS amid a broader push to replace federal bureaucracy with machine-learning software, the tax agency's computing center in Martinsburg, West Virginia, will soon be home to a state-of-the-art Nvidia SuperPod AI computing cluster. According to the previously unreported February 5 acquisition document, the setup will combine 31 separate Nvidia servers, each containing eight of the company's flagship Blackwell processors designed to train and operate artificial intelligence models that power tools like ChatGPT. The hardware has not yet been purchased and installed, nor is a price listed, but SuperPod systems reportedly start at $7 million. The setup described in the contract materials notes that it will include a substantial memory upgrade from Nvidia.
 
Though small compared to the massive AI-training data centers deployed by companies like OpenAI and Meta, the SuperPod is still a powerful and expensive setup using the most advanced technology offered by Nvidia, whose chips have facilitated the global machine-learning spree. While the hardware can be used in many ways, it's marketed as a turnkey means of creating and querying an AI model. Last year, the MITRE Corporation, a federally funded military R&D lab, acquired a $20 million SuperPod setup to train bespoke AI models for use by government agencies, touting the purchase as a "massive increase in computing power" for the United States.
 
How exactly the IRS will use its SuperPod is unclear. An agency spokesperson said the IRS had no information to share on the supercomputer purchase, including which presidential administration ordered it. A 2024 report by the Treasury Inspector General for Tax Administration identified 68 different AI-related projects underway at the IRS; the Nvidia cluster is not named among them, though many were redacted. But some clues can be gleaned from the purchase materials. "The IRS requires a robust and scalable infrastructure that can handle complex machine learning (ML) workloads," the document explains. "The Nvidia Super Pod is a critical component of this infrastructure, providing the necessary compute power, storage, and networking capabilities to support the development and deployment of large-scale ML models."
 
The document notes that the SuperPod will be run by the IRS Research, Applied Analytics, and Statistics division, or RAAS, which leads a variety of data-centric initiatives at the agency. While no specific uses are cited, it states that this division's Compliance Data Warehouse project, which is behind this SuperPod purchase, has previously used machine learning for automated fraud detection, identity theft prevention, and generally gaining a "deeper understanding of the mechanisms that drive taxpayer behavior."]]></content:encoded></item><item><title>Why OOP &amp; FP are the Two Main Paradigms</title><link>https://www.youtube.com/watch?v=l_3AGwVwP_k</link><author>/u/OkMemeTranslator</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:56:13 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chinese Vice Minister says China and the US must work together to control rogue AI: &quot;If not... I am afraid that the probability of the machine winning will be high.&quot;</title><link>https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:27:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in  (AI).But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.“Realistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,” Fu said.“As long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.”The panel discussion is part of a two-day global  that started in Paris on Monday.Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden’s administration and the United Nations.]]></content:encoded></item><item><title>Karol Herbst steps down as Nouveau maintainer due to “thin blue line comment”</title><link>https://www.reddit.com/r/linux/comments/1iq09g6/karol_herbst_steps_down_as_nouveau_maintainer_due/</link><author>/u/mdedetrich</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:24:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA["I was pondering with myself for a while if I should just make it official that I'm not really involved in the kernel community anymore, neither as a reviewer, nor as a maintainer.Most of the time I simply excused myself with "if something urgent comes up, I can chime in and help out". Lyude and Danilo are doing a wonderful job and I've put all my trust into them.However, there is one thing I can't stand and it's hurting me the most. I'm convinced, no, my core believe is, that inclusivity and respect, working with others as equals, no power plays involved, is how we should work together within the Free and Open Source community.I can understand maintainers needing to learn, being concerned on technical points. Everybody deserves the time to understand and learn. It is my true belief that most people are capable of change eventually. I truly believe this community can change from within, however this doesn't mean it's going to be a smooth process.The moment I made up my mind about this was reading the following words written by a maintainer within the kernel community:"we are the thin blue line"This isn't okay. This isn't creating an inclusive environment. This isn't okay with the current political situation especially in the US. A maintainer speaking those words can't be kept. No matter how important or critical or relevant they are. They need to be removed until they learn. Learn what those words mean for a lot of marginalized people. Learn about what horrors it evokes in their minds.I can't in good faith remain to be part of a project and its community where those words are tolerated. Those words are not technical, they are a political statement. Even if unintentionally, such words carry power, they carry meanings one needs to be aware of. They do cause an immense amount of harm.I wish the best of luck for everybody to continue to try to work from within. You got my full support and I won't hold it against anybody trying to improve the community, it's a thankless job, it's a lot of work. People will continue to burn out.I got burned out enough by myself caring about the bits I maintained, but eventually I had to realize my limits. The obligation I felt was eating me from inside. It stopped being fun at some point and I reached a point where I simply couldn't continue the work I was so motivated doing as I've did in the early days.Please respect my wishes and put this statement as is into the tree. Leaving anything out destroys its entire meaning.]]></content:encoded></item><item><title>Karol Herbst Steps Down As Nouveau Maintainer Due To Linux Kernel&apos;s Toxic Environment</title><link>https://www.phoronix.com/news/Karol-Herbst-Nouveau-No</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 15 Feb 2025 11:40:27 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Karol Herbst has been a Nouveau driver developer for over a decade working on this open-source, reverse-engineered NVIDIA Linux graphics driver. He went on to become employed by Red Hat. While he's known more these days for his work on Mesa and the Rusticl OpenCL driver for it, he's still remained a maintainer of the Nouveau kernel driver. But today he announced he's resigning as a Nouveau driver maintainer due to differences with the upstream Linux kernel developer community...]]></content:encoded></item><item><title>KDE Developers Addressing Early Bugs From Plasma 6.3</title><link>https://www.phoronix.com/news/KDE-Plasma-6.3-Early-Bugs</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 15 Feb 2025 11:27:40 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE Plasma 6.3 released this week as the newest step forward for the KDE desktop. While it was smooth on the whole, there were some early bugs that KDE developers were dealing with this week. KDE developer Nate Graham is out with his usual weekly development summary for the Plasma desktop...]]></content:encoded></item><item><title>[P] Daily ArXiv filtering powered by LLM judge</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/</link><author>/u/MadEyeXZ</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:14:16 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My new blog post comparing networking in EKS vs. GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/</link><author>/u/jumiker</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 11:06:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/jumiker ]]></content:encoded></item><item><title>Richard Stallman in Polytechnic University of Turin, Italy</title><link>https://www.reddit.com/r/linux/comments/1ipz4wy/richard_stallman_in_polytechnic_university_of/</link><author>/u/ShockleyTransistor</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:05:38 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go Nullable with Generics v2.0.0 - now supports omitzero</title><link>https://github.com/LukaGiorgadze/gonull</link><author>/u/Money-Relative-1184</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 11:00:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Deep Dive into VPA Recommender</title><link>https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/</link><author>/u/erik_zilinsky</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 10:26:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.Based on my findings, I wrote a blog post about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.]]></content:encoded></item><item><title>what do you use golang for?</title><link>https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/</link><author>/u/Notalabel_4566</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 10:24:28 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is there any other major use than web development?]]></content:encoded></item><item><title>Show HN: Kreuzberg – Modern async Python library for document text extraction</title><link>https://github.com/Goldziher/kreuzberg</link><author>nhirschfeld</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 10:07:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I'm excited to showcase Kreuzberg!Kreuzberg is a modern Python library built from the ground up with async/await, type hints, and optimized I/O handling.It provides a unified interface for extracting text from documents (PDFs, images, office files) without external API dependencies.Key technical features:
- Built with modern Python best practices (async/await, type hints, functional-first)
- Optimized async I/O with anyio for multi-loop compatibility
- Smart worker process pool for CPU-bound tasks (OCR, doc conversion)
- Efficient batch processing with concurrent extractions
- Clean error handling with context-rich exceptionsI built this after struggling with existing solutions that were either synchronous-only, required complex deployments, or had poor async support. The goal was to create something that works well in modern async Python applications, can be easily dockerized or used in serverless contexts, and relies only on permissive OSS.Key advantages over alternatives:
- True async support with optimized I/O
- Minimal dependencies (much smaller than alternatives)
- Perfect for serverless and async web apps
- Local processing without API calls
- Built for modern Python codebases with rigorous typing and testingThe library is MIT licensed and open to contributions.]]></content:encoded></item><item><title>Linux in any distribution is unobtainable for most people because the first two installation steps are basically impossible.</title><link>https://www.reddit.com/r/linux/comments/1ipyc1o/linux_in_any_distribution_is_unobtainable_for/</link><author>/u/trollfinnes</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 10:05:47 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Recently, just before Christmas, I decided to check out Linux again (tried it ~20 years ago) because Windows 11 was about to cause an aneurysm.I was expecting to spend the "weekend" getting everything to work; find hardware drivers, installing various open source software and generally just 'hack together something that works'.To my surprise everything worked flawlessly first time booting up. I had WiFi, sound, usb, webcam, memory card reader, correct screen resolution. I even got battery status and management! It even came with a nice litte 'app center' making installation of a bunch of software as simple as a click!And I remember thinking any Windows user could  install Linux and would get comfortable using it in an afternoon.I'm pretty 'comfortable' in anything PC and have changed boot orders and created bootable things since the early 90's and considered that part of the installation the easiest part.However, most people have never heard about any of them, and that makes the two steps seem 'impossible'.I recently convinced a friend of mine, who also couldn't stand Window11, to install Linux instead as it would easily cover all his PC needs. And while he is definitely in the upper half of people in terms of 'tech savvyness', both those "two easy first steps" made it virtually impossible for him to install it. He easily managed downloading the .iso, but turning that iso into a bootable USB-stick turned out to be too difficult. But after guiding him over the phone he was able to create it.But he wasn't able to get into bios despite all my attempts explaining what button to push and whenNext day he came over with his laptop. And just out of reflex I just started smashing the F2 key (or whatever it was) repeatingly and got right into bios where I enabled USB boot and put it at the top at the sequence.After that he managed to install Linux just fine without my supervision.But it made me realise that the two first steps in installing Linux, that are second nature to me and probably everyone involved with Linux from people just using it to people working on huge distributions, makes them virtually impossible for most people to install it.I don't know enough about programming to know of this is possible:Instead of an .iso file for download some sort of .exe file can be downloaded that is able to create a bootable USB-stick and change the boot order?That would 'open up' Linux to  more people, probably orders of magnitude..]]></content:encoded></item><item><title>Eating From Plastic Takeout Containers Can Increase Heart Failure Risk, Study Finds</title><link>https://science.slashdot.org/story/25/02/15/0555235/eating-from-plastic-takeout-containers-can-increase-heart-failure-risk-study-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 15 Feb 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A new study suggests that frequent consumption of food from plastic takeout containers significantly increases the risk of congestive heart failure due to gut biome changes that trigger inflammation and circulatory damage. The Guardian reports: The authors used a two-part approach, first looking into the frequency with which over 3,000 people in China ate from plastic takeout containers, and whether they had heart disease. They then exposed rats to plastic chemicals in water that was boiled and poured in carryout containers to extract chemicals. "The data revealed that high-frequency exposure to plastics is significantly associated with an increased risk of congestive heart failure," the authors wrote. [...] They put boiling water in the containers for one, five or 15 minutes because plastic chemicals leach at much higher rates when hot contents are placed in containers -- the study cited previous research that found as many as 4.2m microplastic particles per sq cm can leach from plastic containers that are microwaved.
 
The authors then gave rats the water contaminated with leachate to drink for several months, then analyzed the gut biome and metabolites in the feces. It found notable changes. "It indicated that ingestion of these leachates altered the intestinal microenvironment, affected gut microbiota composition, and modified gut microbiota metabolites, particularly those linked to inflammation and oxidative stress," the authors wrote. They then checked the rats' heart muscle tissue and found it had been damaged. The study did not find a statistical difference in the changes and damage among rats that were exposed to water that had been in contact with plastic for one minute versus five or fifteen. The study has been published in the journal Ecotoxicology and Environmental Safety.]]></content:encoded></item><item><title>Jane Street&apos;s Figgie card game</title><link>https://www.figgie.com/</link><author>eamag</author><category>hn</category><pubDate>Sat, 15 Feb 2025 09:59:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Jane Street's fast-paced
              Figgie game simulates exciting elements of markets and trading. At
              Jane Street, Figgie is a game we teach and also one we really
              enjoy playing.
            Read our FAQs
              for more. If you have a question that isn’t answered there, we’d
              like to hear
              what’s missing and what
              would be helpful to know, and we’ll do our best to update FAQs
              along the way.
            ]]></content:encoded></item><item><title>Lessons from David Lynch: A Software Developer&apos;s Perspective</title><link>https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/</link><author>/u/aijan1</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 09:40:30 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He’s perhaps best known for the groundbreaking TV series Twin Peaks, which inspired countless shows, including The X-Files, The Sopranos, and Lost.Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down – even those who truly deserved it.Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that Mulholland Drive remained compulsively watchable while refusing to yield to interpretation.While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I’d like to share my perspective on his life lessons from a software developer’s viewpoint.Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you’ve got to go deeper.We’ve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one –because they’re so rare– write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it’s a film, a painting, or software.The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.Software development is part art, part engineering. We don’t build the same software over and over again – virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it’s very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.It’s a good habit to listen to what users have to say, but they often can only describe their problems – they rarely come up with good ideas to solve them. And that’s OK. It’s our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  – that magical state of mind where we lose track of time and produce code effortlessly. That’s why many developers hate meetings – they are toxic to our productivity.I believe you need technical knowledge. And also, it’s really, really great to learn by doing. So, you should make a film.Software development is one of those rare fields where a college degree isn’t required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It’s crucial to never stop learning, experimenting, and iterating on our craft.Happy accidents are real gifts, and they can open the door to a future that didn’t even exist.Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.Be kind to your teammates, don’t embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety –that is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by Google’s research on the subject.It’s OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.Most of Hollywood is about making money - and I love money, but I don’t make the films thinking about money.Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.What makes these projects remarkable is that they didn’t emerge from corporate boardrooms – they were built by communities of passionate developers, collaborating across the world.Money is just a means to an end. Unfortunately, many get this confused.David, thank you for making the world a better place!]]></content:encoded></item><item><title>Container Networking - Kubernetes with Calico</title><link>https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/</link><author>/u/tkr_2020</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 07:25:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[: VLAN 10: VLAN 20When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:The inner IP header reflects:The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?]]></content:encoded></item><item><title>The Danger of IP Volatility, (Sat, Feb 15th)</title><link>https://isc.sans.edu/diary/rss/31688</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Sat, 15 Feb 2025 07:22:45 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[What do I mean by “IP volatility”? Today, many organizations use cloud services and micro-services. In such environments, IP addresses assigned to virtual machines or services can often be volatile, meaning they can change or be reassigned to other organizations or users. This presents a risk for services relying on static IPs for security configurations and may introduce impersonation or data leakage issues.This morning, I was setting up a new environment. I got a new IP address assigned by my hosting company and deployed a classic configuration: a reverse-proxy redirecting to many web services and generating Let’s Encrypt certificates.Once the reverse proxy was in place, I started to deploy more services but detected some activity in the log (always keep an eye on your logs!) and saw this:{"level":"debug","time":"2025-02-15T06:22:33Z","caller":"github.com/traefik/traefik/v3/pkg/tls/tlsmanager.go:228","message":"Serving default certificate for request: \"postmaster.xxxxxxxx.hu\""}
{"level":"debug","time":"2025-02-15T06:46:36Z","caller":"github.com/traefik/traefik/v3/pkg/tls/tlsmanager.go:228","message":"Serving default certificate for request: \"pop3.xxxxxxxx.hu\""}
{"level":"debug","time":"2025-02-15T07:04:16Z","caller":"github.com/traefik/traefik/v3/pkg/tls/tlsmanager.go:228","message":"Serving default certificate for request: \”xxxxxxxx.hu\""}
A quick DNS request confirmed that these hosts are resolving to my newly assigned IP!Worse, this organization seems to still be using POP3, and a user (or a script) is still trying to fetch emails using this protocol!When you move to another hosting solution, update your DNS recordsCleanup your DNS zones and remove unwanted entriesUse mechanisms to preserve your IP addresses (like “Elastic IPs” provided by AWS)Xavier Mertens (@xme)
Xameco
Senior ISC Handler - Freelance Cyber Security ConsultantPGP Key

 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>The TechBeat: Futures of Ethereum II - Censorship Resistance (2/15/2025)</title><link>https://hackernoon.com/2-15-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sat, 15 Feb 2025 07:11:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @rootstock_io [ 3 Min read ] 
 Rootstock merges Bitcoin’s security with Ethereum’s flexibility, enabling AI-driven blockchain apps for trustless governance, security, and fraud detection. Read More.By @boxhero [ 13 Min read ] 
 Discover how AI-powered sentiment analysis tools deliver accurate insights from customer reviews and feedback to help improve your business strategy. Read More.By @lumoz [ 3 Min read ] 
 On February 13, Lumoz announced the official launch of Lumoz Chain and released the migration guide and reward plan for Verifier nodes.  Read More.By @stellar [ 5 Min read ] 
 Regulatory shifts in 2025 will shape crypto wallets. Learn how compliance, DeFi, and Stellar’s Soroban ecosystem will impact the future of Web3 wallets. Read More.By @adambakay [ 22 Min read ] 
 By understanding market microstructure, you might be able to add more precision into your trading. Read More.By @mexcmedia [ 6 Min read ] 
 Liquidity is key to crypto trading, ensuring price stability, seamless transactions, and reduced slippage. Learn how MEXC excels in liquidity management. Read More.By @2077research [ 11 Min read ] 
 Explore the evolution of crypto options and perpetual futures, diving into innovations like panoptions, liquidity challenges, and decentralized trading. Read More.By @2077research [ 17 Min read ] 
 The article explores Ethereum's efforts to ensure censorship resistance, focusing on solutions like PBS and encrypted mempools amid regulatory pressures. Read More.By @hayday [ 4 Min read ] 
 Is the rise of vibe coding also the end of software engineering? How will vibeware change the nature of the software entrepreneur, and the meaning of work? Read More.By @mexcmedia [ 5 Min read ] 
 Discover key crypto trends of 2025, from Bitcoin’s surge to MEXC’s role in shaping the future of digital asset trading with liquidity, security, & innovation. Read More.By @noda [ 4 Min read ] 
 2025 is the tipping point for pay-by-bank. Lower fees, instant payments, and new regulations make it the future of digital transactions.  Read More.By @matteopisani91 [ 34 Min read ] 
 A cybersecurity engineer built a sci-fi bass from scratch, packed with a synth, a wireless transmitter, a hacked built-in tuner and voltmeters. Read More.By @ishanpandey [ 4 Min read ] 
 Discover how CrossFi is revolutionizing crypto payments in this exclusive interview with CEO Alex Mamasidikov. Read More.By @menaskop [ 6 Min read ] 
 Ethereum has many so-called "killers," though most of them look more like self-destructive projects. Read More.By @wezam [ 4 Min read ] 
 There’s plenty of disagreement on how AI will change the product management landscape.  Read More.By @awsmarketplace [ 8 Min read ] 
 Discover the best endpoint protection solutions, top platforms to consider, and key evaluation criteria to enhance your organization's cybersecurity defenses. Read More.By @vitae [ 5 Min read ] 
 We present a deterministic game incorporating two key mechanisms: Controlled Chaos Shifts (CCS) and Accepting Loss of Control (ALC). Read More.By @edwinliavaa [ 3 Min read ] 
 While Bitcoin's design brilliantly enables decentralization, human nature consistently pulls us toward centralization. Read More.]]></content:encoded></item><item><title>Bookshop.org launches Kindle alternative, sends e-book sales to local bookstores</title><link>https://www.usatoday.com/story/entertainment/books/2025/01/28/bookshop-org-ereader-ebook-app/77928209007/</link><author>dotcoma</author><category>hn</category><pubDate>Sat, 15 Feb 2025 07:05:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Bibliophiles don’t need to commit to one style of reading – with myriad titles available digitally or as audiobooks, diversifying reading habits has never been easier.However, as platforms vary, the seller often doesn’t. Amazon accounts for more than 80% of all online book sales, according to market research firm IBISWorld. Not to mention it owns Kindle, a wildly popular format for e-reading.Bookshop.org is ready to change that. Today, the online platform that connects readers to local bookstores launched an e-reading platform of its own. For the first time, local independent bookstores can sell e-books to customers, says CEO and founder Andy Hunter. Bookshop.org launches e-book platform to support local bookstoresHunter is in the business of solving logistical problems for readers. In 2020, he launched Bookshop.org to support local businesses losing sales to Amazon Books. Years later, as an e-reader himself, he realized how hard it was to continue his “socially-conscious consumerism” mindset of buying locally when it came to digital titles.“Right now, no matter how much you love your local bookstore, if you want to read an e-book, you have to go to Amazon or another alternate platform and you can’t support your local bookstore,” Hunter tells USA TODAY. “E-books are an important revenue stream for publishers and authors and they could be for independent bookstores.”The result is an app (available for iOS and Android, and accessible from a web browser) with nearly a million titles from major publishing houses. Readers can annotate, add notes, look up words and sync across all their devices. There's a search function if consumers know the book they want to read, and if they don't, there's an explore page with suggestions by genre.Most importantly for Hunter, booksellers will take home the profits, which he says has prevented local businesses from thriving in e-book sales until now. “There’s no real way to make it profitable, but we don’t care about it being profitable,” Hunter says, citing the socially conscious company’s B corporation status. “Our mission is to help independent bookstores survive and thrive in the digital age.”With discounts, quick shipping and Kindle synching, Amazon Books has long been an appealing option to many readers. But now, Hunter says there’s “no reason on Earth to give a billionaire your $9.99 for your e-book,” as publishers set a fixed price for e-books and there’s no shipping to compete with. Bookshop.org’s new initiative follows a growing movement across social media – not just exclusively in bookish communities – to forgo Amazon and other big-box sellers in favor of shopping locally. “In an age of billionaires and giant e-commerce companies and our loss of control of our lives to Silicon Valley oligarchs, that they know we’re going to fight back against that and we’re going to stay local,” Hunter says. “It doesn’t have to be this way. We can invest in, we can support our local community even when we’re reading digital books.”E-reader features to include social sharingBookshop.org's platform also includes a “quote-sharing” feature for social media, Hunter says. He wants books to catch up to the way other media is shared – it’s easy to share your Spotify or Apple Music listening to your Instagram story, and you can even post about your workouts with some apps. But if you want to share a quote from a book, readers have to take a photo of the physical book or a screenshot. “They should be able to share that to social media and somebody else should be able to click in, buy the book using Apple Pay and start reading it right away,” Hunter says. “That’ll fuel all kinds of discussions about books on TikTok and places like that, and all of that will benefit the independent bookstores because it will be them selling the books that people are talking about.”Looking for your next great read? USA TODAY has you covered.Clare Mulroy is USA TODAY’s Books Reporter, where she covers buzzy releases, chats with authors and dives into the culture of reading. Find her or tell her what you’re reading at cmulroy@usatoday.com.]]></content:encoded></item><item><title>AI Used To Design a Multi-Step Enzyme That Can Digest Some Plastics</title><link>https://science.slashdot.org/story/25/02/15/0549201/ai-used-to-design-a-multi-step-enzyme-that-can-digest-some-plastics?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 15 Feb 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Leveraging AI tools like RFDiffusion and PLACER, researchers were able to design a novel enzyme capable of breaking down plastic by targeting ester bonds, a key component in polyester. Ars Technica reports: The researchers started out by using the standard tools they developed to handle protein design, including an AI tool named RFDiffusion, which uses a random seed to generate a variety of protein backgrounds. In this case, the researchers asked RFDiffusion to match the average positions of the amino acids in a family of ester-breaking enzymes. The results were fed to another neural network, which chose the amino acids such that they'd form a pocket that would hold an ester that breaks down into a fluorescent molecule so they could follow the enzyme's activity using its glow.
 
Of the 129 proteins designed by this software, only two of them resulted in any fluorescence. So the team decided they needed yet another AI. Called PLACER, the software was trained by taking all the known structures of proteins latched on to small molecules and randomizing some of their structure, forcing the AI to learn how to shift things back into a functional state (making it a generative AI). The hope was that PLACER would be trained to capture some of the structural details that allow enzymes to adopt more than one specific configuration over the course of the reaction they were catalyzing. And it worked. Repeating the same process with an added PLACER screening step boosted the number of enzymes with catalytic activity by over three-fold.
 
Unfortunately, all of these enzymes stalled after a single reaction. It turns out they were much better at cleaving the ester, but they left one part of it chemically bonded to the enzyme. In other words, the enzymes acted like part of the reaction, not a catalyst. So the researchers started using PLACER to screen for structures that could adopt a key intermediate state of the reaction. This produced a much higher rate of reactive enzymes (18 percent of them cleaved the ester bond), and two -- named "super" and "win" -- could actually cycle through multiple rounds of reactions. The team had finally made an enzyme.
 
By adding additional rounds alternating between structure suggestions using RFDiffusion and screening using PLACER, the team saw the frequency of functional enzymes increase and eventually designed one that had an activity similar to some produced by actual living things. They also showed they could use the same process to design an esterase capable of digesting the bonds in PET, a common plastic. The research has been published in the journal Science.]]></content:encoded></item><item><title>Reading Documentation Shouldn&apos;t Be a Chore</title><link>https://hackernoon.com/reading-documentation-shouldnt-be-a-chore?source=rss</link><author>Rami James</author><category>tech</category><pubDate>Sat, 15 Feb 2025 06:48:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As a developer, documentation is both your greatest ally and your worst nemesis. It's the key to unlocking the power of libraries, frameworks, APIs, and even entire programming languages. Yet, many developers struggle to effectively navigate and extract the information they need from documentation. It's a skill that can be learned, and it's one that can make a huge difference in your productivity and the quality of your work. New developers often lack this critical skill, and find themselves stopped in their progress towards becoming a better dev because of it.\
I'm currently writing the docs for Vewrite, a project management tool for writing teams. It occurs to me that I've been writing documentation for a long time, and I've gotten pretty good at it. I've also gotten pretty good at reading documentation, and I think that this is a skill that is often overlooked.\
Let's talk about how to equip you with the skills to read documentation better, turning this often daunting task into a smooth and efficient process.Why is reading documentation challenging?\
Good documentation is essential. It provides the definitive guide to how software works, saving you countless hours of pointless, frustrating trial and error. Bad documentation is a nightmare. It's like trying to navigate a maze blindfolded, with no map and no sense of direction, guided by a liar.\
If you have access to the code you are working against, that's great and you should use it. Poke around and see what you're supposed to do by reading the code. But, in many cases (and almost all of the time with proprietary software), you're going to have to rely on the documentation to get you through. However, documentation can vary wildly in quality. Some is meticulously crafted, while others can be uninformative, outdated, or even misleading. This inconsistency is one of the reasons why reading documentation can be challenging.Some docs are just a dumping ground for information, with no clear structure or organization. This can make it difficult to find what you're looking for, and you end up spending more time searching than actually learning. A well-structured set of documentation will have a clear table of contents, with sections and subsections that guide you through the material in a logical order. This makes it easier to navigate and find the information you need.In general, I'll also recommend some Getting Started section which points different types of users to the information which is most critical to them.Information overload is another common hurdle. Docs can cover a vast range of features and functionalities, making it difficult to find the specific information you need. For example, a large framework might have hundreds of classes and methods, and trying to find the one you need can feel like searching for a needle in a haystack.Unclear technical jargon can also be a barrier, as documentation is inherently technical, and sometimes the terminology can be confusing, especially for newcomers. This often stems from the assumption that the reader has a certain level of prior knowledge as it is largely written by the team who has developed the software and has an intimate knowledge of its innner workings.Outdated information can be a major problem. Software evolves rapidly, and documentation can sometimes lag behind, leading to frustration and wasted time. A library might have deprecated a certain function, but the documentation might still describe it as the primary way to perform a task. The teams who produce software should be including accurate documentation creation and upkeep in their backlog as part of their process. Unfortunately, in the haste to get features shipped to market, corners are often cut. I've spent the better part of the last decade advocating for better processes that lead to better docs at a number of companies, and the biggest challenge here has been explaining to stakeholders where the value is.How to read documentation effectivelyOver the years, I've developed a structured approach to how I tackle reading new documentation. I've found that this approach helps me quickly get up to speed with new tools and technologies, and I hope it can help you too.\
By and large this method has three parts: understanding the structure of the documentation itself, understanding the way that the system I'm learning works, and lastly engaging with the material in the docs. First I get the lay of the land, and then I try to focus in on the details.Understand the structure of the documentationDocs are written by people, and every sysyem is going to be different. That means that every set of docs is going to be different. Some docs are organized by feature, others by use case, and still others by API endpoint. Understanding the structure of the documentation is key to finding the information you need quickly. Start by scanning the table of contents to get an overview of the topics covered. This will help you get a sense of the scope of the documentation and where to find specific information. You'll want to quickly click through the docs and get a sense of how things work, how they are organized, and where you can find reference materials if there are any.\
I find that this initial run through also trickles some information about the system and its components into my brain. It's not comprehensive at all, but it's a good start.Every piece of software is a system, and understanding how that system works is key to understanding how to use it. This is where the documentation can be a huge help. Good documentation will provide you with a high-level overview of the system, explaining how the different components fit together and how they interact. This can help you understand the context in which a particular feature or function is used, and how it relates to the rest of the system. This is where you start to get a sense of how the system is supposed to work, and how you can use it to solve your problems.\
A lot of the problems that you'll encounter when building software will be because you fundamentally don't understand how what you're building with actually works. This is where the documentation can be a huge help. It can provide you with the context you need to understand how the system is supposed to work, and how you can use it to solve your problems. Use that to your advantage and you'll be able to build better software, faster.This is often the most tedious and boring part. You have to actually take a few hours and read through the entire docs. For smaller libraries or APIs, this isn't going to be a big deal and you'll be able to fit most of it into your head. For larger systems, this is going to be a multi-day process. You'll want to take notes, ask questions, and engage with the material in a way that helps you understand it. This is where you start to get a sense of how the system is supposed to work, and how you can use it to solve your problems. If it doesn't all fit into your head now, that's ok. You'll remember more than you think you will, and when you're actually doing development it will be an order of magnitude faster to find an answer if you've already seen it once.I've actually found that modern AIs are absolutely awesome for this step. You can use them as you would a more experienced developer and ask them questions about the system, the functionality, and the docs themselves. You can't trust everything that they say, but you can't really trust what another developer always tells you. I've found that being able to have a discussion with an AI about a library or API really helps me understand how stuff works at a deeper level.Strategies for effectively reading docsDespite these challenges, mastering the art of reading documentation is doable. Here are some strategies to keep in mind while you are digging around in docs, looking for an answer.If you don't know where you're going, you may never get there. Before you dive into a set of docs, you may want to set a clear objective for yourself. Are you trying to understand a specific function, integrate a new library, or troubleshoot an error? Having a clear objective will help you focus your search.\
For example, if you're trying to fix a bug where a specific function isn't returning the expected value, and your code matches what the docs say that you're supposed to do, your goal is to understand the function's place in the wider system. The problem is going to be a few levels up and the docs are likely going to be the key to understanding how the software works at a higher level.\
Don't immediately jump into the nitty-gritty details. Begin with the introductory sections, tutorials, or quick start guides to get a high-level understanding of the system. Your assumptions will often be wrong, and it is good to base your work on a foundation of truth. Think of it like reading the abstract of a research paper before diving into the full text. These introductory materials often provide a roadmap to the rest of the documentation.Use the structure of the docs to your advantageUse the table of contents and search function (these are your best friends) to quickly locate the sections you need. Don't be afraid to experiment with different search terms. Scan through the entire table of contents. Poke around and get a really good feel for how the documentation is structured. Sometimes you will find that your answers are hidden adjacent to where you thought that they would be.Real-world examples are invaluable, and the best docs are going to include them. They demonstrate how to use the code in practice and can often clarify confusing concepts. Pay close attention to the context in which the examples are used. A code snippet demonstrating how to use a particular function is much more helpful if it's accompanied by an explanation of what the function does and why it's being used in that specific scenario.Pay attention to the detailsWhen looking at function or method definitions, carefully examine the data types of the parameters and return values. This will help you understand how to use them correctly. Sometimes, crucial information is hidden in footnotes, warnings, or less prominent sections. Make sure you scan the entire page, not just the headlines. A seemingly minor note might contain a critical piece of information that can save you a lot of trouble. Documentation often contains links to related topics or other parts of the documentation. Don't hesitate to follow these links to gain a deeper understanding. These links can often lead you to more detailed explanations, examples, or even the source code itself.This is one of the biggest reasons why I believe that open-source can be such a boon for companies who want to build an ecosystem of developers around their software. Sometimes, the best way to understand how something works is to look at the source code itself. Don't be afraid to dive in, especially if the documentation is unclear. Reading the source code can give you a much clearer picture of what's happening under the hood. Try out the code examples and modify them to see how they behave. This is a great way to solidify your understanding and discover edge cases. Experimenting with the code is like conducting your own little science experiment. You can change the inputs, observe the outputs, and gain a deeper understanding of how the code works. If you're still stuck, don't hesitate to reach out to the community for help. Forums, mailing lists, and online communities are valuable resources for getting your questions answered. The developer community is generally very helpful, and there are often people who have encountered the same problems you're facing.Reading documentation shouldn't be a passive activity. Engage with the material by taking notes, jotting down key points, examples, and any questions you have. Annotating the documentation (if possible), highlighting important sections and adding your own comments can also be helpful. Creating your own examples, writing small programs that use the features you're learning about, is a great way to reinforce your understanding. It's like practicing a musical instrument – the more you do it, the better you get.Reading documentation is a skill that improves with practice. By adopting these strategies and actively engaging with the material, you can transform documentation from a source of frustration into a powerful tool for learning and development. So, embrace the documentation, and unlock the full potential of the software you're working with!]]></content:encoded></item><item><title>How to Expose (And Fix) Hidden Bottlenecks in Adobe Experience Manager</title><link>https://hackernoon.com/how-to-expose-and-fix-hidden-bottlenecks-in-adobe-experience-manager?source=rss</link><author>Giuseppe Baglio</author><category>tech</category><pubDate>Sat, 15 Feb 2025 06:45:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Learn how to read thread dumps and take control of your application’s runtime behaviour.\
When your Adobe Experience Manager (or in general any JAVA application) instance shows signs of sluggishness, it’s time to roll up your sleeves and dive into the world of thread dumps. IBM Thread Analyzer (TDA) is here to help you untangle the web of threads and pinpoint performance bottlenecks. In this guide, we’ll walk you through how to use IBM TDA to diagnose performance issues in AEM like a pro.Before you can start analyzing thread dumps, you’ll need to download and install IBM Thread Analyzer. Head over to the official IBM website or your organization’s repository to grab the latest version. Once downloaded, follow the installation instructions for your operating system. It’s quick, easy, and sets the stage for some serious troubleshooting.Thread dumps are snapshots of all the threads running in your AEM instance at a specific moment. To capture them:Use tools like , , or AEM’s built-in functionality to generate thread dumps. There is a well-documented page on Adobe Docs.Save the thread dump files to your local machine.\
Pro Tip: Capture multiple thread dumps at intervals (e.g., every 10 seconds) to get a clearer picture of long-running issues.Launch IBM TDA and open the thread dump files you’ve captured. Simply drag and drop the files into the application or use the “Open” option to load them. Once loaded, you’ll see a list of thread dumps on the left-hand panel.To analyze a specific thread dump:Select the file from the listing.Click the Thread Detail button at the top\
This will display a detailed view of all the threads in that dump. Now, let’s sort the threads by Stack Depth, ensuring the longest stacks appear at the top. Why? Threads with deeper stacks often indicate more complex operations, which are usually where performance issues hide.Focus on threads with a stack depth of 10 lines or longer. These threads are typically the ones consuming the most resources. Take notes on any threads that stand out — whether due to their names, states, or stack traces.Next, sort the threads by their State. Scroll down to the Runnable threads. These are the threads that were actively using CPU time when the dump was taken. Keep an eye out for application-specific threads, such as:Background job threads: Handling tasks like indexing or replication.Request threads: Named like 127.0.0.1 [timestamp] GET /path HTTP/1.1.For each request thread, extract the timestamp from its name (e.g., ). This Unix epoch timestamp tells you when the user’s browser made the request. Convert it to a human-readable date/time using a tool like https://www.epochconverter.com/. Compare this with the thread dump’s timestamp to calculate how long the request has been active.If the difference is unusually large (e.g., several seconds or minutes), it could indicate a bottleneck in your application.\
Pro Tip: Keep an eye out for patterns. Are certain types of requests consistently taking longer? For example, requests involving complex queries or resource-heavy operations might be worth optimizing. Additionally, if you notice that specific URLs or endpoints are frequently associated with long-running threads, consider profiling those areas of your codebase.Thread analysis requires a nuanced approach that goes beyond simple waiting states. While the IBM Thread Analyzer (TDA) interface provides valuable insights into thread relationships, understanding the full context of thread behavior helps create a more complete picture of your application’s performance characteristics.Understanding Thread StatesWhen examining threads in TDA, you’ll encounter several important states:: These threads are either currently executing or ready to execute when CPU time becomes available. A Runnable state doesn’t necessarily indicate a problem — it’s the natural state for actively working threads.: These threads have temporarily paused execution while waiting for a condition to be met. The waiting state can occur for many legitimate reasons, including:Resource availability (database connections, file handles)Task completion in other threads\
: These threads are specifically waiting to acquire a monitor or lock. While similar to waiting, blocked states specifically indicate synchronization-related pauses.Analyzing Thread RelationshipsWhen you identify a thread of interest, examine its relationships with other threads using this systematic approach:Direct Lock Relationships:Examine the Waiting Threads panel for immediate dependenciesReview the stack traces of waiting threads to understand why they’re blockedNote the duration of the wait states if available\
2. Resource Usage Patterns:Look for patterns in resource acquisition and releaseIdentify potential resource bottlenecksConsider alternative resource management strategies\
3. Architectural Implications:Evaluate if the observed behaviour aligns with the system’s designConsider if the current threading model is appropriateAssess the impact on scalabilityUnderstanding Lock Types and VisibilityThread dumps may not show all types of contention. Modern Java applications use various synchronization mechanisms:Intrinsic Locks (synchronized keyword):Show clear owner-waiter relationshipsStack traces indicate synchronization points\
2. Explicit Locks (java.util.concurrent):May require additional tooling to visualize\
3. Non-blocking Mechanisms (Don’t appear as traditional locks but can impact performance):When you identify genuine contention issues, consider these approaches:Implement finer-grained lockingConsider non-blocking alternativesImplement backoff strategiesConsider caching solutions\
3. Architectural ChangesEvaluate asynchronous processingConsider parallel execution pathsImplement queue-based approaches\
Remember that thread analysis is an iterative process. Patterns that emerge in one thread dump might not represent consistent behaviour. Always validate your findings across multiple dumps and different time periods before making significant changes to your application.Comparing thread dumps across time reveals important performance patterns in your AEM instance. Start by establishing a baseline during normal operation, including peak usage periods and maintenance windows. This baseline provides context for identifying abnormal thread behaviour.To determine if a thread is persistent across time:Select multiple thread dumps from different points in time.Click the Compare Threads button in IBM TDA.Look for threads that remain in the Runnable state across all dumps, especially those with consistently long stack traces.\
Use IBM TDA’s Compare Threads feature to analyze dumps from different time points. Focus on threads that persist across multiple dumps, examining their states, stack depths, and resource usage. Remember that thread persistence alone doesn’t automatically indicate a problem — background services naturally run continuously, while request threads should complete within expected timeframes.\
When analyzing persistent Runnable threads, correlate their behaviour with system metrics like CPU usage, memory consumption, and response times. Consider the thread’s purpose: background services, request processing, or maintenance tasks each have different expected patterns. For request threads, compare their duration against defined service level agreements and business requirements.\
Got a suspicious thread pattern? Don’t jump to conclusions just yet! Try to recreate the issue in your test environment first — it’s like having a dress rehearsal before the main show. Take a good look at your code, double-check those config settings, and consider what else might be stirring up trouble in your environment. Keep track of what you find with real performance numbers and test results — you’ll thank yourself later.\
Once you’re sure you’ve caught a real performance culprit (backed by solid evidence, of course), it’s time to fix it.If analyzing threads doesn’t yield actionable insights, switch to the Monitor Detail view:Go back to the thread listing.Select a thread dump and click the Monitor Detail button.IBM TDA will display a tree view of monitor-owning threads and their waiting threads.\
This view helps you identify threads that are holding monitors and causing contention. Understanding thread monitors is like viewing the nervous system of your application. These synchronization mechanisms control how threads access shared resources, preventing potential conflicts and ensuring smooth operation.\
Monitor interactions can reveal critical performance insights. Some threads will be actively processing requests, while others wait for resource acquisition or participate in coordinated activities. Not all waiting or idle threads indicate a problem — they’re often part of the application’s natural resource management strategy.\
However, not all threads are equally important:Ignore idle thread pool threads: These threads typically have ≤10 stack lines and are part of thread pools like the servlet engine. They’re usually harmless unless they dominate the thread pool.Focus on application-specific monitors: Look for monitors tied to your application’s business logic, such as database connections, caching mechanisms, or custom synchronization blocks.\
Remember that thread and monitor analysis is both an art and a science. Each application has unique characteristics, so approach performance optimization with curiosity and a holistic perspective. The goal is not to eliminate all waiting threads but to understand and optimize their interactions.\
Advanced Tip: If you notice certain monitors are frequently contended, consider refactoring your code to reduce lock granularity. For example:Replace coarse-grained locks with fine-grained ones.Use non-blocking algorithms or concurrent data structures where possible.Optimize database queries to reduce the time threads spend waiting for locks.In some thread dumps, you might notice the  appearing frequently. This service handles tasks like Garbage Collection, memory management, and resource cleanup. While the Collector Service might seem like a mysterious background process, understanding its behaviour is key to maintaining optimal system performance — think of it like a diligent janitor in a large office building.\
When you notice frequent Collector Service activity, don’t immediately assume disaster. It’s normal for the Collector Service to show up occasionally, but excessive activity could indicate underlying issues:Memory leaks: Objects that are not being garbage collected can cause frequent GC cycles.High object churn: Rapid creation and destruction of objects can overwhelm the garbage collector.Improper JVM settings: Misconfigured heap sizes or GC algorithms can lead to inefficiencies.\
Here are some considerations to optimize resource usage:Tuning your JVM settings (e.g., increasing heap size, switching to G1GC).Reviewing your application’s memory allocation patterns to reduce unnecessary object creation.\
Garbage Collection is not a problem to be solved, but a dynamic system to be understood and optimized. Each application has unique characteristics, and there’s no universal solution.Thread dump analysis is a developer’s superpower — transforming you from a code writer to a performance detective. IBM Thread Analyzer (TDA) is your key to understanding complex system behaviours, revealing hidden bottlenecks that impact your Java/AEM instance’s performance.\
Like learning an instrument, your skill improves with practice. Each thread dump becomes clearer, revealing intricate patterns of system interactions. The more you analyze, the more intuitive performance optimization becomes.\
Remember, practice makes perfect — the more you analyze thread dumps, the sharper your diagnostic skills will become. 📊💪\
🛠 ️Happy troubleshooting! And don’t forget to share your findings with your team to keep your Java/AEM instance running smoothly.]]></content:encoded></item><item><title>Launch a Product in a New Market Is Hard, So I Made a 4-Step Framework for Success</title><link>https://hackernoon.com/launch-a-product-in-a-new-market-is-hard-so-i-made-a-4-step-framework-for-success?source=rss</link><author>Roman Shimanskiy</author><category>tech</category><pubDate>Sat, 15 Feb 2025 06:43:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[ because there’s just no real demand for what they’re offering. A staggering number and a logical reason behind it – and it’s the main one, according to CB Insights. It’s not about a lack of funding, bad hires, or tough competition. The real question is whether people actually need what you’re about to offer them.\
Plus, having a great product isn’t enough. To win, you need to understand the competitive landscape, your audience, and how to get your brand in front of the right people. Drawing from my experience launching Yango Play in MENA — where we developed an AI-powered Entertainment SuperApp that quickly became one of the region’s leading streaming platforms — I want to share the key steps we took to make it the success it is now.Selecting the right market is a balance between competition and profitability. Some regions offer low competition but limited revenue potential, while others may be more lucrative but very, very saturated. Sub-Saharan Africa, for example, has a large population and low competition, making it an appealing early-stage entry option. But, lower consumer spending restricts growth. MENA, on the other hand, has a smaller population but significantly higher consumer purchasing power, making it an attractive place for expansion.\
 is just as critical as market selection. Entering too early may mean slow adoption while entering too late risks facing intense competition.If competition is low, it provides an advantage and accelerates market growth. The key is to enter before the market experiences a surge in adoption.\
In the CIS region, the subscription economy remained relatively flat at 1–2 million subscribers for several years before suddenly accelerating to 100% annual growth. The same pattern emerged in MENA, indicating that the region was nearing a similar tipping point, making it the optimal time to enter.\
Market data is here to reinforce this statement. MENA’s total population is around 500 million, with 13.5 million already subscribed to video streaming services. Just for reference, penetration of subscription services in CIS region stands at 15-20%, with slight differences between countries. Even without complex economic modeling, these figures indicated a major opportunity for subscription-based services in MENA. So, an ideal market for expansion. In this case, the potential formed the perfect ground for selecting the market and strategy – the choice was easy.\
Still, the region was already highly competitive, with more than 50 video streaming platforms and many global and local music services. We had strong competitors in each category, so for us entering the market required more than just identifying demand.Product and Content DifferentiationCreating a strong differentiator was key, and competing head-on with zero brand awareness would have been very much an uphill battle.\
Take YANGO’s case study on product differentiation. We simultaneously implemented two strategies — with **differentiation through the product and content. \n An Entertainment SuperApp — a single subscription combining music, movies, and games. At the time, there wasn’t anything similar, so we gladly took the opportunity for this strategic entry point. Users typically juggle multiple subscriptions: one for music, another for video, and yet another for gaming. We built everything into one app under a single subscription. This way, we got on the growing global trend toward bundled entertainment services and also filled a gap in our target market. Win-win.Netflix has long become the go-to platform for global content. We took a different route and built an ecosystem tailored to local tastes. Our SuperApp has regional music, Arabic-language films, and localized gaming options, making it much more relevant to MENA audiences. This became a critical differentiator, making our product not just another streaming service but a culturally attuned entertainment hub.\
Conclusion: To successfully differentiate a product, the best strategy is to identify weaknesses or unmet needs in competing products — and fill those gaps.\
In crowded markets, pricing is as crucial as the product. It’s not the features that make or break the product – it’s about making your product the most logical choice.\
So, we introduced a long trial period and a bundled pricing model, allowing users to get music and movies for the price of one. Instead of discounting, we maximized perceived value, making the subscription an easy, rational decision.\
Expanding into a new market meant building a local team and adapting to language and cultural nuances. With deep market research, perfect timing, and a product tailored to real consumer needs, the SuperApp secured a strong foothold in a high-growth market. How did we tackle these challenges? That’s a story for another article.Audience Differentiation: Defining the Target UserUsers have mental “boxes” where they sort every new product they see. Food delivery? That goes into one box. A streaming service? Another box. But if your product doesn’t fit neatly into a box, it triggers confusion — people don’t know what to do with it. And when something feels unfamiliar, the default reaction is to ignore it.\
That’s why crystal-clear positioning isn’t optional — it’s survival. If users can’t instantly grasp who your product is for and why they need it, you’re fighting an uphill battle. But just fitting into a category isn’t enough. You have to own it. When people think about entertainment subscriptions, they should immediately think of your brand.\
How do you make that happen? Precision. Messaging needs to be sharp, clear, and unmistakable. Instead of vague positioning, spell it out: “This is a service for modern Arabic-speaking users.” That clarity eliminates confusion and makes it easier for potential subscribers to see how the product fits into their everyday world.\
Brand anchoring is everything— the simpler and more memorable the positioning, the stronger the audience connection. If users struggle to define what the product is or who it’s for, they’ll tune it out. That’s a death sentence for any brand. The fix? Build an instant mental link. The moment someone thinks “entertainment subscription,” your SuperApp should be the first thing they recall.For startups that can’t outspend competitors, traditional marketing funnels — awareness → consideration → purchase — might not be the best play. Instead, skip the awareness stage and go straight for user acquisition.\
Here’s the logic: if someone downloads your app, they already know it exists. There’s no need to waste resources on brand awareness before conversion. Yes, this approach raises the cost per acquisition (CPA) since users are coming in cold, but the trade-off is greater control over spending and a more direct path to growth.\
The real power lies in performance-driven marketing, which focuses on conversions rather than broad exposure. Even brands with large media budgets rely on performance marketing as the most efficient way to drive actual user engagement. For lean startups, this strategy isn’t just viable — it’s essential.A product launch is a one-time shot at grabbing audience attention —  over a long, drawn-out campaign. The smartest play? Go loud.\
The launch moment is your biggest marketing asset — a rare window where PR and buzz can be built just around your existence. Instead of slow, passive brand-building, focus on making a splash that gets people talking.\
Once the initial hype kicks in, then comes the next phase — brick by brick, reinforcing brand awareness and earning user trust. We also saw this proven in the case of Kinopoisk (video streaming platform). We focused our marketing around major premieres such as Zack Snyder's Justice League, , and big exclusive original premieres.Shifting from Traditional Advertising to Alternative ChannelsFrom experience, buying direct media ads is often far less effective than leveraging influencers, word of mouth, and strong PR.\
Spending time on finding the right influencer, crafting a viral PR angle, or developing creative content that resonates delivers higher ROI than just pouring money into paid ads. In a budget-constrained environment, the smarter play is a two-step approach:Performance marketing first — drive installs and conversions while simultaneously learning how users interact with the product.Organic & influencer-driven marketing second — building credibility and momentum through earned media, viral content, and strategic partnerships.With influencers, you’re hitting two targets at once. First, their audience gets exposed to your service, boosting brand awareness. But the real power of influencer marketing is in its high conversion rates — it doesn’t just build visibility; it drives action. It works like performance marketing in many ways. Even better? The creative content influencers produce often performs exceptionally well in paid performance campaigns, making it a valuable asset beyond the initial collaboration. So, really, you’re getting even more value out of every campaign.\
This keeps acquisition costs under control and ensures real engagement and long-term user retention.\
One of the most successful cases of leveraging PR instead of direct advertising is Netflix. When expanding into new regions, the company rarely invests in traditional media ads. Instead, it focuses on localized content, influencer collaborations, and large-scale PR campaigns. This approach has allowed Netflix to dominate new markets without massive ad spending, making the brand recognizable through organic user discussions and viral interest.\
There are plenty of examples of viral brands and services — take Dubai Chocolate or Clubhouse, for instance. These apps and platforms managed to gain massive audiences with little to no marketing spend, relying purely on organic growth and word of mouth.The Founder as the Brand’s Leading AmbassadorIn a competitive market, PR isn’t just about press releases and media coverage — it’s about people. A founder holds a unique advantage: their voice becomes the product’s most powerful marketing tool.\
People don’t engage with companies — they engage with other people. Instead of relying on corporate statements or press releases, modern PR is about creating shareable content and building influence through authentic, human-driven narratives.\n Whether you are a startup or an established company, entering a new market requires more than a strong product — it asks for adaptability to evolving industry rules and dynamics. The strategies that worked in the past may no longer be effective in an environment where competition is fierce, consumer behavior is shifting, and digital ecosystems are rapidly changing. Success hinges on thorough market research, precise audience targeting, strategic product differentiation, and a well-structured promotional approach.]]></content:encoded></item><item><title>Every New Apple Device Expected in 2025</title><link>https://hackernoon.com/every-new-apple-device-expected-in-2025?source=rss</link><author>David Perru</author><category>tech</category><pubDate>Sat, 15 Feb 2025 06:21:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How Hospitals Use AI to Boost Efficiency in Medical Imaging Technology</title><link>https://hackernoon.com/how-hospitals-use-ai-to-boost-efficiency-in-medical-imaging-technology?source=rss</link><author>Beth Rush</author><category>tech</category><pubDate>Sat, 15 Feb 2025 06:21:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Advancing health care technologies are crucial for doctors and patients. They could make medical services easier to provide while giving people better results, but only if everyone understands the bigger picture. Artificial intelligence (AI) is a critical tool that may become part of your health care services in the near future. Check out how physicians are using AI in medical imaging and diagnostics to learn why it could revolutionize the industry.What Is Medical Imaging Technology?Medical imaging technology is any tool that  to monitor normal and abnormal cases. The term includes various services that assist health care providers in making diagnoses. While they’ve existed long before automation, artificial intelligence in medical imaging is becoming more widespread through software updates.Different Types of Imaging TechniquesYou’ll better understand why AI could be an essential scientific tool if you know which imaging techniques people undergo. Learn more about each form of technology to understand AI’s advantages and challenges more easily.X-ray machines are penetrative imaging tools that look at targeted areas inside the body before sending the results to imaging technology. The machine sends ionizing radiation into a patient to gather pictures for diagnosis. X-rays are especially useful for bone inquiries because the radiation passes through them and makes them bright white.Magnetic Resonance Imaging (MRI)If you need an MRI, a doctor might inject a contrast agent into your bloodstream  before you sit in the machine. Radiowaves, a magnet and a computer processor look inside the selected part of your body to reveal everything from tissues to joints.Ultrasound imaging  to see what’s in your body. The machinery bounces soundwaves off of your organs and tissues instead. This isn’t a penetrative imaging technique, even though tools like transvaginal wands do work inside the vaginal canal.Doctors needing more complex imaging may order CT scans. They combine an X-ray with a computer to  before it bounces back on a plate behind the patient. The machinery scans in a circle, making it more useful for 3D images of internal damage or tumors.How Each Technique Benefits From AIIf health care experts have used imaging techniques since before AI existed, why would they benefit from it? There are a few key ways AI in medical imaging and diagnostics could improve each technique for providers and patients.Algorithms Can Process Images FasterWhen doctors order penetrative imaging, they wait for specialized hospital staff to conduct the process and return the pictures. The physician needs time to sit and analyze the image, which may not happen immediately if they have a busy schedule. Given that the U.S. will have a health care worker shortage , the high work demand on doctors is a long-term challenge.\
You don’t need to worry about not getting medical imaging results back quickly if your local hospital uses machine-learning algorithms to process patient results. AI can review results , providing recommendations for doctors when they have time to check the scans. A health care provider will still review your results, but they could get back to you faster with computerized assistance.AI May Catch Small Details BetterArtificial intelligence in medical imaging can do much more than summarize scans. The algorithms can also delineate structures in the human body. Advanced programs , tumors, and blood vessels and catch abnormalities in imaging results.\
Doctors might accidentally overlook similar developments if they’re too small to see with the human eye. Instead of going back for additional imaging in a few weeks or months, you could get earlier and more precise results with AI-supported medical imaging techniques.\
Remember — your doctor will still have the final say over your diagnosis. AI provides additional insights, but you’ll always need to talk with your health care provider before moving forward with treatments.Predictive Analytics May Provide Personalized Treatment SuggestionsPersonalized treatment plans start with diagnostic tools like medical imaging before combining your results with your health history. AI may assist with that process in more hospitals over time. Advanced algorithms can  while checking your scan results. Afterward, your doctor will review its recommended treatment options and use that data to start conversations with you.\
Together, you’ll decide the next best steps. AI won’t determine how you get medical care. Machine learning programs are an evergrowing tool that could make those conversations more straightforward by supplying potentially more accurate data.Some surgeries require medical imagery techniques for real-time data. Machine learning  in specialties like cardiothoracic, ophthalmology and general surgery. When health care professionals use it in real time, the algorithms can review images guiding surgeons through a procedure and assist in augmented visualization. Decision support may also be helpful for instant recommendations based on the patient’s collective medical history, like the likelihood of complications.Does AI Medical Imaging Guarantee Increased Efficiency?There’s no guarantee that AI medical imaging will increase the efficiency of real-time surgical decisions, diagnoses or treatment plans. Algorithms are only as good as their programming. Your doctor will always need to consider the data provided by any machine learning program before using their comprehensive medical knowledge and understanding of your needs to give an informed opinion.Potential Challenges Hospitals May FaceAs more hospitals start using AI programs to read results like medical scans, it’s important to understand the barriers that may prevent implementation. Keep those factors in mind if you’re hoping to see AI-assisted medical services where you live.Data Storage and ProtectionEverything processed and produced by artificial intelligence in medical imaging consists of sensitive data. Hospitals need robust storage capabilities to utilize AI-assisted medical imaging and keep patient information safe. Experts estimate data breaches  in 2024 and may get worse in years to come.\
Hospital administrative teams may need to partner with information technology experts internally or externally to set up data storage methods with extra protection. The process may delay AI implementation, depending on where each facility’s current cybersecurity measures stand.Accurate InterpretabilityMedical imaging technology with AI programs is relatively new. Hospital staff will likely need some form of training before they can use it with patients. Human error could cause less accurate interpretability as medical providers get used to reading and considering AI results while making recommendations.Integrating AI With Current Workflow SystemsAdding a software system to a new service is much less complicated than integrating it into something people are already doing. Hospitals are serving patients around the clock, so adding AI to their medical imaging processes without compromising patient care or service speed is challenging.\
Researchers also point out that health care systems  due to things like mismatching software programs or outdated technology. Analyzing, collecting and storing imaging data may require other updates before it can happen smoothly. Communicating AI information across departments or between hospitals may create additional struggles for health care providers.Anticipate More AI in Medical Imaging and DiagnosticsHealth care professionals are interested in artificial intelligence in medical imaging because it could help them work faster, provide personalized results and give more accurate treatment plans. It can also require complex integration planning before patients benefit from it. Staying up to date on those factors will keep you in the know with the ever-changing health care industry.]]></content:encoded></item><item><title>[D] What&apos;s the most promising successor to the Transformer?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/</link><author>/u/jsonathan</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 06:17:01 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also xLSTM and Aaren.What do y'all think is the most promising alternative architecture to the transformer?]]></content:encoded></item><item><title>Tech Leaders Reveal New Approaches to Corporate Sustainability</title><link>https://devops.com/executive-strategies-driving-corporate-sustainability/</link><author>Bonnie Schneider</author><category>devops</category><pubDate>Sat, 15 Feb 2025 05:54:47 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[Over the past two years, I’ve interviewed more than 100 executives on tech innovation. Key insights emerged. But one stood out: sustainability is no longer a “nice to have.” It’s now a core business strategy. That’s the focus of my inaugural, exclusive report: Decisions That Define: Executive Strategies Driving Corporate Sustainability. Why 2025 is a […]]]></content:encoded></item><item><title>Kafka Delay Queue: When Messages Need a Nap Before They Work</title><link>https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need</link><author>/u/Sushant098123</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 05:08:28 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Webassembly and go 2025</title><link>https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/</link><author>/u/KosekiBoto</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 05:00:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[so I found this video and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you   submitted by    /u/KosekiBoto ]]></content:encoded></item><item><title>Ikey Doherty&apos;s Serpent OS Rebranding As AerynOS</title><link>https://www.phoronix.com/news/Serpent-OS-To-AerynOS</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 15 Feb 2025 05:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The nearly three year old Serpent OS Linux distribution started by Ikey Doherty of Solus fame is going to re-brand as AerynOS...]]></content:encoded></item><item><title>Vq: A Vector Quantization Library for Rust 🦀</title><link>https://www.reddit.com/r/rust/comments/1ipu2jg/vq_a_vector_quantization_library_for_rust/</link><author>/u/West-Bottle9609</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 04:56:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've created a Rust library called Vq that implements several vector quantization algorithms. At the moment, these algorithms include binary, scalar, product, optimized product, tree-structured, and residual quantization. I think the library can be useful for tasks like data compression, similarity search, creating RAG pipelines, and speeding up machine learning computations.This is my second Rust project, as I'm currently learning Rust. I'd like to get some feedback from the community and hear about any use cases you might have for the library, so I'm making this announcement.The library is available on crates.io: vq, and the source code is on GitHub: vq.]]></content:encoded></item><item><title>Modern Java Deep Dive</title><link>https://www.youtube.com/watch?v=z4qsidg261E</link><author>/u/BlueGoliath</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 03:59:24 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Phenomenology of Dark Matter Explained</title><link>https://hackernoon.com/the-phenomenology-of-dark-matter-explained?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Sat, 15 Feb 2025 03:44:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.4 Phenomenology of dark matter\
• The relic density coming from Planck satellite data [49]\
The total relic abundance of DM in our model is given by the sum of the scalar (𝜒) and fermion (𝑁3) relic abundances:\
Only for solutions falling exactly within the band given in Eq. (3.31) the totality of the DM can be explained by 𝜒 and 𝑁3.\
• Direct detection cross-section of DM scattering of nucleon set by various experiments such as XENON1T [66], LUX [65] and PandaX-II [174]\
We implemented the model in the SARAH package [175] to calculate all the vertices, mass matrices, tadpole equations etc. The thermal cross sections and DM relic abundance are determined using micrOMEGAS-5.0.8 [176]. Even though the model introduces new free parameters, not all of them are important to DM analysis. For example, self-quartic coupling 𝜆𝜒 does not play any role in DM phenomenology. Hence we choose to fix 𝜆𝜒 = 0.1 in our analysis. The remaining free parameters relevant for DM analysis can be chosen as:\
In the next sections, we will study how the DM phenomenology of this model depends on the free parameters and to do that we choose the following benchmark points which are allowed from all the above-mentioned constraints:\
which we can be utilized to compute the relic density of both the components,The direct detection study of our DM candidates 𝜒𝑅 and 𝑁3 are done here. The current experimental constraints on the DM direct detection assume the existence of only one DM candidate. As in our model, two-component DM candidates are predicted, and the contribution of each candidate to the direct detection cross-section should be rescaled by the fraction contributing to the total relic density. Hence it is convenient to define the fraction of the mass density of 𝑖th DM in the case of multi-component DM [156,157,178,179]\
The upper limit on the direct detection now can be recast as\
The above formula in Eq. (3.40) is an extension of the expression corresponding to the singlet scalar DM case [180]. The relative negative sign between the ℎ1 and ℎ2 contributions arises in our considered model as the couplings get modified according to Eq. (3.42). Due to the presence of the two different channels, depending on the parameter space, we can have destructive interference between these two channels, and direct detection can be very small.(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>No Personal Liability For DOGE Yet, But With Two More Lawsuits We Get Closer</title><link>https://www.techdirt.com/2025/02/14/no-personal-liability-for-doge-yet-but-with-two-more-lawsuits-we-get-closer/</link><author>Cathy Gellis</author><category>tech</category><pubDate>Sat, 15 Feb 2025 03:39:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[I’m going to keep pounding the drum for personal liability against Musk and DOGE, partly to scare them into backing off from their unlawful seizure of our government, and eventually to compensate us for the immense harm they’ve caused. So far it doesn’t seem like anyone has tried to personally sue them for damages, but several lawsuits are taking what might be a predicate step to establish the lawlessness of their claimed power, upon which liability claims would later be based. In addition to the AFGE litigation against OPM we already wrote about, which also names OPM itself for it wrongfully giving DOGE access to its systems, and the states’ lawsuit against the Treasury department for giving DOGE access to theirs, now we have (at this writing at least) two more lawsuits. But while those lawsuits were directed at specific agencies and the wrongfulness of Musk and DOGE’s misuse of power at these agencies, these new lawsuits come gunning for Musk and DOGE and their illegal seizure of power generally.They both base this argument on the Appointments Clause of the Constitution, but we’ll use the states’ complaint to illustrate how. As it sets forth:The Founders of this country fought for independence from the British monarchy due in no small part to the King’s despotic power to create an unlimited number of governmental offices and to fill those offices with the King’s supporters. In fact, this practice so severely undermined the Founders’ freedoms that it is a listed grievance in the Declaration of Independence. Informed by that history, the Framers of the Constitution crafted the Appointments Clause to protect against such tyranny in our system of government. The Appointments Clause was designed to buttress the separation of powers in two ways: first by requiring that Congress create an office before the President can fill it, and second by requiring that the Senate confirm a nominee to an office created by law. These limitations on the President’s power make executive appointments accountable to Congress and make the Senate’s confirmation decisions accountable to the people. See United States v. Arthrex, 594 U.S. 1, 12 (2021). In this way, the Appointments Clause serves a vital role in curbing Executive abuses of power.Yet here we have Musk wielding a shocking amount of power, the complaint continues:Mr. Musk’s seemingly limitless and unchecked power to strip the government of its workforce and eliminate entire departments with the stroke of a pen or click of a mouse would have been shocking to those who won this country’s independence. There is no office of the United States, other than the President, with the full power of the Executive Branch, and the sweeping authority now vested in a single unelected and unconfirmed individual is antithetical to the nation’s entire constitutional structure.We have an Appointments Clause for this very reason, the complaint reminds, “because it prevents one branch from “aggrandizing its power” or “dispensing it too freely . . . to inappropriate members of the Executive Branch.” It explains that there are three types of personnel that can work for the Executive Branch, “Principle Officers,” “Inferior Officers,” and employees. The last category doesn’t require Senate confirmation, but it also isn’t endowed with the sort of executive power that Musk has been claiming. The other categories are, which is why they require nominations by the President and Senate approval, unless Congress has already passed a law foregoing that process. But Congress can only do that for inferior officers, it has not done so here, and in any case Musk is acting more like a Principle Officer anyway.Furthermore, even for Principle Officers the President simply can’t make up a job with such power and appoint someone to it. And even Justice Thomas agrees! The complaint cites what he wrote less than a year ago in :Importantly, the Appointments Clause only grants the President the power to nominate officers to offices that Congress has already “established by Law.” U.S. Const. art. II, § 2, cl. 2. “If Congress has not reached a consensus that a particular office should exist, the Executive lacks the power to unilaterally create and then fill that office.” Trump v. United States, 603 U.S. 593, 650 (2024) (Thomas, J., concurring). “By keeping the ability to create offices out of the President’s hands, the Founders ensured that no President could unilaterally create an army of officer positions to then fill with his supporters. Instead, our Constitution leaves it in the hands of the people’s elected representatives to determine whether new executive offices should exist.” Id. at 646 (Thomas, J., concurring).Yet here we are, with Trump having done exactly what Justice Thomas said he could not, for the very reasons Justice Thomas himself said he could not.The complaint then takes 30 pages to document Musk and DOGE’s unlawful rampage through executive branch agencies, in what is surely only scratching the surface of the full depth of how he has abused his unlawful power, and still continues to abuse it.And so the lawsuit asks for this power to be enjoined so that Musk and DOGE are forced to stop their destruction. In fact, it’s also now asked for a temporary restraining order to get Musk and DOGE to stop what they are doing immediately:[T]he States ask the court to issue a temporary restraining order that immediately and temporarily, until such time as the Court may hear a motion for preliminary injunction, orders Mr. Musk to identify all ways in which any data obtained through unlawful agency access was used, including whether it was used to train any algorithmic models or create or obtain derivative data, orders Mr. Musk to destroy any copies or any derivative data from such unauthorized access in his or DOGE’s possession, custody, or control, and bars Mr. Musk and personnel associated with DOGE from:(a) ordering any change in the disbursement of public funds by agencies;(b) extending offers on behalf of the United States that would bind the government to anappropriation that has not been authorized by law;(c) cancelling government contracts;(d) disposing of government property;(e) ordering the rescission or amendment of regulations;(f) making personnel decisions for agency employees;(g) taking steps to dismantle agencies created by law or otherwise asserting control oversuch agencies, including, e.g., placing employees on administrative leave;(h) accessing sensitive and confidential agency data, using agency data for other than itsauthorized purpose;(i) altering agency data systems without authorization by law and without taking allappropriate protections against cybersecurity risks;(j) engaging in any other conduct that violates the Appointments Clause or exceedsstatutory authority.But beyond that the complaint also asks for declaratory relief such that a court finally speaks to the unlawful nature of Musk’s power (as well as that of its DOGE agency, which, as the complaint explains, is also malformed if it is to claim the sort of supervisory power that it has, which is a power that can only be endowed by Congress):DOGE has purported to exercise authority of its own, and not merely to have acted as an adviser to the President. “Administrative agencies are creatures of statute. They accordingly possess only the authority that Congress has provided.” Nat’l Fed’n of Indep. Bus. v. Dep’t of Lab., Occupational Safety & Health Admin., 595 U.S. 109, 117 (2022) (per curiam). Congress has not provided any authority to DOGE. The Constitution does not provide any authority to DOGE. The temporary organization statute, 5 U.S.C. § 3161, [that Trump claimed in his Executive Order was empowering DOGE] does not provide DOGE with the authority it has purported to exercise. That statute provides that a “temporary organization” is defined as an organization “established by law or Executive order for a specific period not in excess of three years for the purpose of performing a specific study or other project.” 5 U.S.C. § 3161(a)(1) (emphasis added). There is no plausible definition of “project” that would include DOGE’s attempt to remake the entire Executive Branch, as described above, or to destroy agencies, fire personnel, halt funding, or dispose of government property.In asking for declaratory relief a few things are accomplished. For one thing, it gives those whom Musk and DOGE are bossing around the ability to say no. In fact, if gives them the obligation to say no, because what they are being asked to do would be an unlawful order and thus unlawful for them to do. (Of course, the injunction/TRO would also restrict Musk and DOGE from even making such demands.)But it also inches us forward to the real prize here: holding everyone involved with DOGE personally liable for the harm they have caused. By establishing that what they have done has been unlawful it provides the predicate basis for potentially all sorts of forms of liability, including the CFAA, which the USAID workers suit also provides more evidence of liability for, including in its allegations that . See page 6-7 of the complaint:*J. Doe 2 understands that the DOGE personnel had administrative privileges into all the USAID systems and tools and that DOGE personnel took information out of the agency and sent it elsewhere. DOGE’s actions have caused J. Doe 2 emotional injury, as J. Doe 2 is aware of the extent of confidential information that has been breached and the privacy laws broken.And the declaratory judgment would help overcome another legal issue: whether anyone associated with DOGE would be entitled to any sort of governmental immunity for the harm they’ve caused. This will be an issue to analyze further, because we have, and would normally want to have, some immunity shielding government officials from liability for doing their jobs, if we are going to leave them sufficiently free to do their jobs. But here no one in DOGE actually had a job that would have entitled them to do what they have done. Which is what these 14 states are asking a court to finally and declaratively say.]]></content:encoded></item><item><title>The 20 year old PSP can now connect to WPA2 WiFi Networks</title><link>https://wololo.net/2025/02/14/the-20-year-old-psp-can-now-connect-to-wpa2-wifi-networks/</link><author>zdw</author><category>hn</category><pubDate>Sat, 15 Feb 2025 03:31:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Screenshot source: Zekiu_ on youtubeAcid_Snake and the ARK Development team have released a significant update to the ARK custom Firmware for the Sony PSP. Custom Firmware now allows the Playstation Portable to connect to WPA2 encrypted Wifi networks. This is thanks to the recently released  plugin, created by developer  and published on the PSP Homebrew discord.Playstation Portable gets WPA2 Wifi accessThe PSP has been out of official support from Sony for years, but lots of enthusiasts keep maintaining this great handheld through homebrew and custom Firmware updates. As technology evolves around us, older devices such as the PlayStation Portable can lose some of their features.For example, as WPA2 has become the defacto encryption standard for home wifi networks (WPA3’s adoption rate remains low), older devices such as the PSP, that do not support these new* encryption standards become technically unable to access the internet.Wifi access was a very strong feature of the PSP when it was released, and, although it’s probably less important nowadays, losing that feature because newer networks aren’t compatible is a bummer.WPA2 support has been a request by many enthusiasts for years on PSP discussion channels, and it seems that the wpa2psp plugin by developer Moment now brings this to life. According to Acid_Snake, the developer was kind enough to provide the source code of the plugin, which allowed the ARK team to embed it into the ARK Custom Firmware for PSP.This reddit thread by Nebula_NL covers a lot of details on how to install and use the plugin. But the bottom line is: install the latest release of the ARK CFW on your PSP, and take it from there. (Note that you can also manually install the plugin if you’re using another CFW than ARK)This is of course the first iteration of this plugin, and it comes with limitations, specifically:2.4 GHz Only
WPA2 support works with 2.4 GHz WiFi.If your router uses a single SSID for both 2.4 GHz and 5 GHz, you may need to separate them and connect your PSP to the 2.4 GHz network.WPA2 AES Only
Requires WPA2 with AES (AES-CCMP) encryption.TKIP is not supported and will not work.WEP/WPA Compatibility
While WPA2 is active, WEP and WPA encryption will not work.To use WEP or WPA again, disable WPA2, and they will function normally.WPA2/WPA3 Mixed Mode
If your router is set to WPA2/WPA3 mixed mode, your PSP may struggle to obtain an IP address.Try manually setting the IP address instead of using DHCP in [AUTO] mode.Download and install ARK-4 + enable WPA2 Support for the PSP* WPA2 was certified in 2004… It’s “new” from the PSP’s perspective which launched the same year and didn’t “need” to support it at the time. WPA3 launched in 2018 but its adoption is taking time]]></content:encoded></item><item><title>Brake Pad Dust Can Be More Toxic Than Exhaust Emissions, Study Says</title><link>https://tech.slashdot.org/story/25/02/15/0016236/brake-pad-dust-can-be-more-toxic-than-exhaust-emissions-study-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 15 Feb 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Bruce66423 shares a report from The Guardian: Microscopic particles emitted from brake pads can be more toxic than those emitted in diesel vehicle exhaust, a study has found. This research shows that even with a move to electric vehicles, pollution from cars may not be able to be eradicated. The researchers found that a higher concentration of copper in some commonly used brake pads was associated with increased harmful effects on sensitive cells from people's lungs, as a result of particles being breathed in.
 
Exposure to pollution generated by cars, vans and lorries has been previously been linked to an increased risk of lung and heart disease. While past attention has mainly concentrated on exhaust emissions, particles are also released into the air from tyre, road and brake pad wear. These emissions are largely unregulated by legislation and the study found that these âoenon-exhaustâ pollution sources are now responsible for the majority of vehicle particulate matter emissions in the UK and parts of Europe, with brake dust the main contributor among them.
 
[...] The scientists examined the effects on lung health of particulate matter from four different types of brake pad with differing chemical compositions; low metallic, semi-metallic, non-asbestos organic and hybrid-ceramic. Results showed that of the four types of brake pads, non-asbestos organic pads were the most potent at inducing inflammation and other markers of toxicity, and were found to be more toxic to human lung cells than diesel exhaust particles. Ceramic pads were the second most toxic. Dr. Ian Mudway, senior lecturer at the school of public health at Imperial College London, cautioned that while the research on brake pad emissions appears sound, it is premature to conclude they are worse than diesel exhaust due to "uncontrolled variables" like brake disc types and particle composition.
 
Slashdot reader Bruce66423 also notes it "doesn't discuss the significance of regenerative breaking, which is a feature of at least some electric cars [that reduces brake pad wear by using the electric motor to slow down the vehicle and recover energy]."
 
The research has been published in the journal Particle and Fibre Technology.]]></content:encoded></item><item><title>Theoretical and Experimental Constraints: Discussing Different Constraints on the Model Parameters</title><link>https://hackernoon.com/theoretical-and-experimental-constraints-discussing-different-constraints-on-the-model-parameters?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Sat, 15 Feb 2025 03:06:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.3 Theoretical and experimental constraintsWe discuss different constraints on the model parameters such as𝑈(1)𝑋 gauge coupling and scalar mixing angle. To estimate the constraints we consider vacuum stability, perturbative unitarity, and collider searches of BSM Higgs and 𝑍′ boson respectively.The above scalar potential must be bounded from below. To determine the conditions for 𝑉(𝐻, Φ, 𝜒) to be bounded from below, we need to check the following symmetric matrix which comes from the quadratic part of the potential,\
Requiring such a matrix to be positive-definite, we obtain the following conditions,3.3.2 Higgs Invisible decay\
Hence the total invisible decay width of SM Higgs boson ℎ1 is given a\
Accordingly, the invisible branching ratio for ℎ1 is given b3.3.4 Bounds on the mixing parameter between physical mass eigenstates(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>A Phenomenological Study of WIMP Models: Scalar Sector, Gauge Sector, and More</title><link>https://hackernoon.com/a-phenomenological-study-of-wimp-models-scalar-sector-gauge-sector-and-more?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Sat, 15 Feb 2025 03:00:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
In the next subsections, we discuss various parts of the lagrangian of the model,\
where 𝛼 is the mixing angle. The rotation matrix satisfies\
The real and imaginary components of 𝜒 have the following massesTo determine the gauge boson spectrum, we have to expand the scalar kinetic terms and replaceThe Yukawa sector of the model can be written in a gauge-invariant way as(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>Studying a Two-Component Dark Matter Model: An Introduction</title><link>https://hackernoon.com/studying-a-two-component-dark-matter-model-an-introduction?source=rss</link><author>Phenomenology Technology</author><category>tech</category><pubDate>Sat, 15 Feb 2025 02:55:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In this chapter, we study a two-component DM model interacting with SM via Higgs and Z portals. The results are based on the work: Arindam Das, Shivam Gola, Sanjoy Mondal, Nita Sinha, "Two-Components Scalar and Fermionic Dark Matter candidates in a generic U(1)𝑋 model, Phys.Lett.B 2022.137117”.Underpinning the origin of neutrino mass and elucidating the nature of DM would constitute a major step forward in particle physics. Several simple extensions of the SM that can account for the DM have already been studied [136–143]. In these models, the SM particle content is extended by additional fields, and a discrete symmetry is usually introduced to guarantee the stability of the DM particle in cosmological scale. In recent years, a class of models has been proposed to incorporate the neutrino mass generation and the existence of DM in a unified framework. Motivated by this, people have studied well-motivated BSM framework based on the gauged 𝑈(1)𝑋 model [144–146]. The most intriguing aspect of this model is that including three generations of right-handed neutrinos, as in the type-I seesaw process for creating light neutrino masses, is no longer an option, but emerges as the simplest solution to eliminate the gauge and mixed gauge-gravity anomalies [147]. The scalar DM can be inherently stable in such models due to its 𝑈(1)𝑋 charge, but the fermionic DM cannot be realized in the simplest 𝑈(1)𝑋 model. Additional discrete symmetries can be introduced, which can stabilize one of the right-handed neutrinos to play the role of DM, while the other two neutrinos participate in the type I seesaw process to generate the required light neutrino masses and flavor mixing. Also, there are many models proposed in the literature, where neutrino mass generation is intimately connected with DM [148–153]. In these types of models, DM is a mediator of neutrino mass generation.\
A single-particle DM model may not be sufficient to account for the relic density of DM observed in the universe. Many such models face strong constraints from direct detection experiments and other observations. Therefore, it is reasonable to consider multi-particle DM scenarios, where two or more particles contribute to the DM abundance. [24,62,126]. Multi-component DM refers to a situation in which two or more particles contribute to the measured DM density. This has been already studied in many BSM scenarios [31, 56, 57, 154–166]. The multi-component DM model has also some benefits over the single dark matter scenario. For instance, it can avoid some stringent constraints arising from various experiments that probe the properties and interactions of dark matter and other particles. A multi-component DM model can also accommodate different observational features of dark matter, such as its distribution and abundance in the universe.(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.]]></content:encoded></item><item><title>Bringing Nest.js to Rust: Meet Toni.rs, the Framework You’ve Been Waiting For! 🚀</title><link>https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/</link><author>/u/Mysterious-Rust</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 02:42:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a Rust developer coming from TypeScript, I’ve been missing a Nest.js-like framework — its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesn’t have a direct counterpart (yet!), I decided to build one myself! 🛠️Introducing… Toni.rs — a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And it’s live in beta! 🎉Here’s what makes this project interesting:Scalable maintainability 🧩:A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code — each module lives in its own context, clean and focused.Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?Automatic Dependency Injection 🤖:Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.Leave your thoughts below — suggestions, questions, or even just enthusiasm! 🚀 ]]></content:encoded></item><item><title>How I Became A Machine Learning Engineer (No CS Degree, No Bootcamp)</title><link>https://towardsdatascience.com/how-i-became-a-machine-learning-engineer-no-cs-degree-no-bootcamp/</link><author>Egor Howell</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 02:33:01 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Machine learning and AI are among the most popular topics nowadays, especially within the tech space. I am fortunate enough to work and develop with these technologies every day as a machine learning engineer!In this article, I will walk you through my journey to becoming a machine learning engineer, shedding some light and advice on how you can become one yourself!In one of my previous articles, I extensively wrote about my journey from school to securing my first Data Science job. I recommend you check out that article, but I will summarise the key timeline here.Pretty much everyone in my family studied some sort of STEM subject. My great-grandad was an engineer, both my grandparents studied physics, and my mum is a maths teacher.So, my path was always paved for me.I chose to study physics at university after watching The Big Bang Theory at age 12; it’s fair to say everyone was very proud!At school, I wasn’t dumb by any means. I was actually relatively bright, but I didn’t fully apply myself. I got decent grades, but definitely not what I was fully capable of.I was very arrogant and thought I would do well with zero work.I applied to top universities like Oxford and Imperial College, but given my work ethic, I was delusional thinking I had a chance. On results day, I ended up in clearing as I missed my offers. This was probably one of the saddest days of my life.Clearing in the UK is where universities offer places to students on certain courses where they have space. It’s mainly for students who don’t have a university offer.I was lucky enough to be offered a chance to study physics at the University of Surrey, and I went on to earn a first-class master’s degree in physics!There is genuinely no substitute for hard work. It is a cringy cliche, but it is true!My original plan was to do a PhD and be a full-time researcher or professor, but during my degree, I did a research year, and I just felt a career in research was not for me. Everything moved so slowly, and it didn’t seem there was much opportunity in the space.During this time, DeepMind released theirdocumentary on YouTube, which popped up on my home feed.From the video, I started to understand how AI worked and learn about neural networks, reinforcement learning, and deep learning. To be honest, to this day I am still not an expert in these areas.Naturally, I dug deeper and found that a data scientist uses AI and machine learning algorithms to solve problems. I immediately wanted in and started applying for data science graduate roles.I spent countless hours coding, taking courses, and working on projects. I applied to and eventually landed my first data science graduate scheme in September 2021.You can hear more about my journey from a podcast.I started my career in an insurance company, where I built various supervised learning models, mainly using gradient boosted tree packages like CatBoost, XGBoost, and generalised linear models (GLMs).I built models to predict: — Did someone fraudulently make a claim to profit.— What’s the premium we should give someone.— How many claims will someone have. — What’s the average claim value someone will have.I made around six models spanning the regression and classification space. I learned so much here, especially in statistics, as I worked very closely with Actuaries, so my maths knowledge was excellent.However, due to the company’s structure and setup, it was difficult for my models to advance past the PoC stage, so I felt I lacked the “tech” side of my toolkit and understanding of how companies use machine learning in production.After a year, my previous employer reached out to me asking if I wanted to apply to a junior data scientist role that specialises in time series forecasting and optimisation problems. I really liked the company, and after a few interviews, I was offered the job!I worked at this company for about 2.5 years, where I became an expert in forecasting and combinatorial optimisation problems.I developed many algorithms and deployed my models to production through AWS using software engineering best practices, such as unit testing, lower environment, shadow system, CI/CD pipelines, and much more.Fair to say I learned a lot. I worked very closely with software engineers, so I picked up a lot of engineering knowledge and continued self-studying machine learning and statistics on the side.Over time, I realised the actual value of data science is using it to make live decisions. There is a good quote by Pau Labarta BajoML models inside Jupyter notebooks have a business value of $0There is no point in building a really complex and sophisticated model if it will not produce results. Seeking out that extra 0.1% accuracy by staking multiple models is often not worth it.You are better off building something simple that you can deploy, and that will bring real financial benefit to the company.With this in mind, I started thinking about the future of data science. In my head, there are two avenues: -> You work primarily to gain insight into what the business should be doing and what it should be looking into to boost its performance. -> You ship solutions (models, decision algorithms, etc.) that bring business value.I feel the data scientist who analyses and builds PoC models will become extinct in the next few years because, as we said above, they don’t provide tangible value to a business.That’s not to say they are entirely useless; you have to think of it from the business perspective of their return on investment. Ideally, the value you bring in should be more than your salary.You want to say that you did “X that produced Y”, which the above two avenues allow you to do.The engineering side was the most interesting and enjoyable for me. I genuinely enjoy coding and building stuff that benefits people, and that they can use, so naturally, that’s where I gravitated towards.To move to the ML engineering side, I asked my line manager if I could deploy the algorithms and ML models I was building myself. I would get help from software engineers, but I would write all the production code, do my own system design, and set up the deployment process independently.And that’s exactly what I did.Coincidentally, my current employer contacted me around this time and asked if I wanted to apply for a machine learning engineer role that specialises in general ML and optimisation at their company!Call it luck, but clearly, the universe was telling me something. After several interview rounds, I was offered the role, and I am now a fully fledged machine learning engineer!Fortunately, a role kind of “fell to me,” but I created my own luck through up-skilling and documenting my learning. That is why I always tell people to show their work — you don’t know what may come from it.I want to share the main bits of advice that helped me transition from a machine learning engineer to a data scientist. — A machine learning engineer is  an entry-level position in my opinion. You need to be well-versed in data science, machine learning, software engineering, etc. You don’t need to be an expert in all of them, but have good fundamentals across the board. That’s why I recommend having a couple of years of experience as either a software engineer or data scientist and self-study other areas. — If you are from data science, you must learn to write good, well-tested production code. You must know things like typing, linting, unit tests, formatting, mocking and CI/CD. It’s not too difficult, but it just requires some practice. I recommend asking your current company to work with software engineers to gain this knowledge, it worked for me! — Most companies nowadays deploy many of their architecture and systems on the cloud, and machine learning models are no exception. So, it’s best to get practice with these tools and understand how they enable models to go live. I learned most of this on the job, to be honest, but there are courses you can take. — I am sure most of you know this already, but every tech professional should be proficient in the command line. You will use it extensively when deploying and writing production code. I have a basic guide you can checkout here.Data Structures & Algorithms — Understanding the fundamental algorithms in computer science are very useful for MLE roles. Mainly because you will likely be asked about it in interviews. It’s not too hard to learn compared to machine learning; it just takes time. Any course will do the trick. — Again, most tech professionals should know Git, but as an MLE, it is essential. How to squash commits, do code reviews, and write outstanding pull requests are musts. — Many MLE roles I saw required you to have some specialisation in a particular area. I specialise in time series forecasting, optimisation, and general ML based on my previous experience. This helps you stand out in the market, and most companies are looking for specialists nowadays.The main theme here is that I basically up-skilled my software engineering abilities. This makes sense as I already had all the math, stats, and machine learning knowledge from being a data scientist.If I were a software engineer, the transition would likely be the reverse. This is why securing a machine learning engineer role can be quite challenging, as it requires proficiency across a wide range of skills.Summary & Further ThoughtsI have a free newsletter, , where I share weekly tips and advice as a practising data scientist. Plus, when you subscribe, you will get my and short PDF version of my AI roadmap!]]></content:encoded></item><item><title>AI and the Future of Work: Transforming Industries</title><link>https://hackernoon.com/ai-and-the-future-of-work-transforming-industries?source=rss</link><author>Ombir Sharma</author><category>tech</category><pubDate>Sat, 15 Feb 2025 02:13:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AI or artificial intelligence is the capability and ability of a machine to imitate human intelligence. Intelligent systems are capable of learning, reasoning, and self-correcting. AI includes a number of technologies such as machine learning, natural language processing, robotics, and computer vision. It is able to perform a wide array of functions from simple repetitive work to making complex decisions which changes the very fabric of industries.The Rise of Robotics and AIThe combination of AI and robotics is changing the world at an unprecedented rate; human errors are greatly reduced and proficiency is greatly improved across multiple industries. Robots powered by AI can be programmed to accomplish tasks that previously only humans could perform. From AI-trained chatbots at customer service to assembly lines in factories, this newly developed technology changes the workplace alongside societal norms.Effect of Artificial Intelligence on EmploymentAI is a double-edged sword with its impact on employment. The technology has created several new opportunities and increased overall productivity in numerous ways, however, it has also posed great threats to certain jobs and industries which can lead to displacement of employees.An important shift is on the horizon as the job market is about to undergo a role reversal change. As AI technology evolves, it is taking over the more labor-intensive tasks and as a result, human workers are becoming more strategic and analytic. AI alleviates repetitive tasks and allows employees to focus on creative problem-solving.The new approach AI takes centers around, just like the Industrial Revolution. While focusing on increasing productivity, ChatGPT and AutoGPT have the potential to perform tasks such as data entry, scheduling, and even more complex problem-solving, increasing overall demand for customer service representatives and support agents. In every age, these shifts in productivity have led to increased job opportunities, just like the multiple industries that formed alongside the technology integrations.Facilitation of EmploymentThe introduction of ChatGPT can aid everyday business operations, serving as virtual assistants that communicate primarily with customers. By streamlining business functions for flexible resource allocation, productivity is sure to increase. Efficient use of ChatGPT makes it incredibly simple for businesses to automate their customer inquiries, increasing profit margins.ChatGPT encourages an environmentally friendly approach by seamlessly optimizing supply chains with effective waste reduction strategies. AI, without a doubt, would optimize ChatGPT-provided decision-making to improve overall energy consumption and ensure economic goals are catered towards too.In the absence of people, ChatGPT opens doors to new interactions that drastically improve service and business operations through automation. Communication via virtual assistants and chatbots allows for effortless collaboration and automated customer interactions on a new level.\
In every way possible, AI improves business decision-making by offering adequate utility and data for correct allocation. Business AI helps set prices, predict market movement, and analyze how much risk varies. Keeping track of tools like ChatGPT can aid project-based decisions by providing up-to-date information.Elimination of Junior and Mid-level Staff PositionsAs a result of automation, junior and mid-level roles, particularly in data entry and administrative duties and tasks, are being skinned out. More companies seem to be using AI, which, in turn, is leading to an increased need for highly specialized and senior positions that require a supervisor to think strategically.Across different industries, AI is changing the future of work. From performing simple services to making complex decisions, AI has a broad range of applications, and as time goes by, the list only seems to grow.The Use Of AI In BusinessArtificial intelligence is being adopted in all sectors to boost productivity, accuracy, and satisfaction of customers. It is used in recruitment HR, in through chatbots, and in marketing through brand campaigns. Work processes are now being automated with AI, thereby, minimizing the costs of running the business.Impact Of AI On Business OperationsSoftware developments with regard to Artificial intelligence utilizing machine learning and automation present the future of the workplace, resulting in increased productivity. Businesses will continuously adopt AI to better their decisions, improve work processes, and promote creativity. Because of this, more skills in AI will be needed which will translate to new positions being created.In the field of finance, AI handles fraud detection, automates trading, and enhances risk assessments. Robo-advisors make investment suggestions based on a client’s unique profile, and AI chatbots provide further engagement with customers.AI assists in diagnosing diseases and personalizes treatments for patients in healthcare. With the help of AI-led robots, humans can make fewer mistakes while performing surgeries, which helps enhance patient care.In the automotive sector, AI enables self-driving cars, predictive maintenance, and smart traffic control. AI technologies also help improve vehicle safety and fuel efficiency.Cybersecurity is enhanced by AI technologies that assess new threats, detect anomalies, and provide assistance during the occurrence of cyber-attacks. Data security is also improved and business risks are suppressed by AI technologies.In the e-commerce sector, AI improves customer experiences, automates supply chains, and increases the level of service. Recommendation systems based on users' behavior increase the total sales and satisfaction level of customers.We see the growth of AI in hospitality in chatbots, smart booking systems, and customer experience applications. AI analytics also help with effective guest price settings and greater guest satisfaction.AI greatly assists in providing relevant lists of jobs that best suit a candidate's skills. AI technology helps to read a CV, understand the candidate's level of skills, and determine his success.Are My Working Opportunities On The Line Due To AI?Filling positions for repetitive work is indeed scalable with AI which is why it poses a threat for employment. In reality, AI also creates new possibilities by taking up low-productivity tasks. It is expected that new jobs that require creative thinking, emotions, and strategy-based decision-making will still exist. To ensure that they remain employed, workers must learn new skills and adjust to the new reality.AI is enabling the future of work by performing tasks that require time and effort, growing productivity, and changing the existing categorization of industries. Some roles may be lost, but there are always new positions that have to be filled. Being able to utilize AI together with other novel technologies is crucial to be successful for business professionals in the new world.]]></content:encoded></item><item><title>How do I configure Minikube to use my local IP address instead of the cluster IP?</title><link>https://www.reddit.com/r/kubernetes/comments/1ipr3i1/how_do_i_configure_minikube_to_use_my_local_ip/</link><author>/u/Own_Appointment5630</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Sat, 15 Feb 2025 02:02:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[Hi there!! How can I configure Minikube on Windows (using Docker) to allow my Spring Boot pods to connect to a remote database on the same network as my local machine? When I create the deployment, the pods use the same IP as the Minikube cluster which gets rejected by the database. Is there any way that Minikube uses my local IP in order to connect correctly?.]]></content:encoded></item><item><title>&apos;Please Stop Inviting AI Notetakers To Meetings&apos;</title><link>https://slashdot.org/story/25/02/15/006253/please-stop-inviting-ai-notetakers-to-meetings?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 15 Feb 2025 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Most virtual meeting platforms these days include AI-powered notetaking tools or bots that join meetings as guests, transcribe discussions, and/or summarize key points. "The tech companies behind them might frame it as a step forward in efficiency, but the technology raises troubling questions around etiquette and privacy and risks undercutting the very communication it's meant to improve (paywalled; alternative source)," writes Chris Stokel-Walker in a Weekend Essay for Bloomberg. From the article: [...] The push to document every workplace interaction and utterance is not new. Having a paper trail has long been seen as a useful thing, and a record of decisions and action points is arguably what makes a meeting meaningful. The difference now is the inclusion of new technology that lacks the nuance and depth of understanding inherent to human interaction in a meeting room. In some ways, the prior generation of communication tools, such as instant messaging service Slack, created its own set of problems. Messaging that previously passed in private via email became much more transparent, creating a minefield where one wrong word or badly chosen emoji can explode into a dispute between colleagues. There is a similar risk with notetaking tools. Each utterance documented and analyzed by AI includes the potential for missteps and misunderstandings.
 
Anyone thinking of bringing an AI notetaker to a meeting must consider how other attendees will respond, says Andrew Brodsky, assistant professor of management at the McCombs School of Business, part of the University of Texas at Austin. Colleagues might think you want to better focus on what is said without missing out on a definitive record of the discussion. Or they might think, "You can't be bothered to take notes yourself or remember what was being talked about," he says. For the companies that sell these AI interlopers, the upside is clear. They recognize we're easily nudged into different behaviors and can quickly become reliant on tools that we survived without for years. [...] There's another benefit for tech companies getting us hooked on AI notetakers: Training data for AI systems is increasingly hard to come by. Research group Epoch AI forecasts there will be a drought of usable text possibly by next year. And with publishers unleashing lawsuits against AI companies for hoovering up their content, the tech firms are on the hunt for other sources of data. Notes from millions of meetings around the world could be an ideal option.
 
For those of us who are the source of such data, however, the situation is more nuanced. The key question is whether AI notetakers make office meetings more useless than so many already are. There's an argument that meetings are an important excuse for workers to come together and talk as human beings. All that small talk is where good ideas often germinate -- that's ostensibly why so many companies are demanding staff return to the office. But if workers trade in-person engagement for AI readbacks, and colleagues curb their words and ideas for fear of being exposed by bots, what's left? If the humans step back, all that remains is a series of data points and more AI slop polluting our lives.]]></content:encoded></item><item><title>Uber sues DoorDash, alleging anti-competitive tactics</title><link>https://techcrunch.com/2025/02/14/uber-sues-doordash-alleging-anti-competitive-tactics/</link><author>Kirsten Korosec, Maxwell Zeff</author><category>tech</category><pubDate>Sat, 15 Feb 2025 01:35:37 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Ride-share giant Uber filed a lawsuit Friday against DoorDash, accusing the delivery outfit of stifling competition by intimidating restaurant owners into exclusive deals. Uber alleges in the lawsuit, filed in Superior Court of California, that its chief rival bullied restaurants into only working with DoorDash. Uber claims that DoorDash, which holds the largest share of […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>what was the Linux expirance like in the 90&apos;s and 00&apos;s?</title><link>https://www.reddit.com/r/linux/comments/1ipql9k/what_was_the_linux_expirance_like_in_the_90s_and/</link><author>/u/mrcrabs6464</author><category>linux</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 01:35:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I started using Linux about 2 years ago really right at the beginning of the proton revolution. And I know that Gaming in specif was the biggest walls for mass adaption of Linux throughout the 2010's and late 2000's but Ive heard things about how most software ran through WINE until Direct x and other API's became more common. but gaming aside what was the expirance and community like at the time?   submitted by    /u/mrcrabs6464 ]]></content:encoded></item><item><title>Q2DOS – Quake 2 backported to MS-DOS</title><link>https://dk.toastednet.org/Q2DOS/</link><author>jsheard</author><category>hn</category><pubDate>Sat, 15 Feb 2025 01:32:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>NYC Is Giving Free E-Bikes To Delivery Workers Using Unsafe Models</title><link>https://hardware.slashdot.org/story/25/02/14/2336232/nyc-is-giving-free-e-bikes-to-delivery-workers-using-unsafe-models?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 15 Feb 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New York City's Department of Transportation is offering delivery workers the opportunity to swap out uncertified e-bikes for safer UL-compliant models. "Millions of people rely on such workers for timely deliveries, yet the low wages and brutal conditions of the job have forced many riders to seek out low-cost electric bicycles to perform the work -- exactly the kind of e-bikes that are least likely to have received safety certifications," reports Electrek. From the report: The NYC DOT has already begun accepting applications for the new E-Bike Trade-In Program, which is open to delivery workers with non-compliant electric bicycles as well as the often-seen electric scooters/mopeds that don't really qualify as e-bikes, despite their ubiquitous use in the industry. Interestingly, the program even accepts gasoline-powered mopeds that are not able to be legally registered with the DMV, including those that lack VINs. In exchange for trading in a non-certified vehicle, the delivery worker will receive a new UL-certified electric bike with a spare UL-certified battery.
 
There are a few requirements for eligibility. The worker has to have earned at least US $1,500 by working in the food delivery industry last year in 2024, live in one of the five New York City boroughs, be at least 18 years old, and own/use one of the eligible devices for trade-in. The program is free to participate in with no additional cost for the delivery workers. However, the supply of free electric bicycles is described as "limited." Those interested need to submit an application before the window closes on March 10, 2025.]]></content:encoded></item><item><title>Go 1.24 Brings Performance Improvements, Better WebAssembly Support</title><link>https://www.phoronix.com/news/Go-1.24-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 15 Feb 2025 01:24:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Go 1.24 was released this week by Google engineers as the newest step forward for this popular programming language...]]></content:encoded></item><item><title>Introducing Impressions at Netflix</title><link>https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb?source=rss----2615bd06b42e---4</link><author>Netflix Technology Blog</author><category>dev</category><category>official</category><pubDate>Sat, 15 Feb 2025 01:13:20 +0000</pubDate><source url="https://netflixtechblog.com/?source=rss----2615bd06b42e---4">Netflix Tech Blog</source><content:encoded><![CDATA[Part 1: Creating the Source of Truth for ImpressionsImagine scrolling through Netflix, where each movie poster or promotional banner competes for your attention. Every image you hover over isn’t just a visual placeholder; it’s a critical data point that fuels our sophisticated personalization engine. At Netflix, we call these images ‘impressions,’ and they play a pivotal role in transforming your interaction from simple browsing into an immersive binge-watching experience, all tailored to your unique tastes.Capturing these moments and turning them into a personalized journey is no simple feat. It requires a state-of-the-art system that can track and process these impressions while maintaining a detailed history of each profile’s exposure. This nuanced integration of data and technology empowers us to offer bespoke content recommendations.In this multi-part blog series, we take you behind the scenes of our system that processes billions of impressions daily. We will explore the challenges we encounter and unveil how we are building a resilient solution that transforms these client-side impressions into a personalized content discovery experience for every Netflix viewer.Why do we need impression history?To tailor recommendations more effectively, it’s crucial to track what content a user has already encountered. Having impression history helps us achieve this by allowing us to identify content that has been displayed on the homepage but not engaged with, helping us deliver fresh, engaging recommendations.By maintaining a history of impressions, we can implement frequency capping to prevent over-exposure to the same content. This ensures users aren’t repeatedly shown identical options, keeping the viewing experience vibrant and reducing the risk of frustration or disengagement.Highlighting New ReleasesFor new content, impression history helps us monitor initial user interactions and adjust our merchandising efforts accordingly. We can experiment with different content placements or promotional strategies to boost visibility and engagement.Additionally, impression history offers insightful information for addressing a number of platform-related analytics queries. Analyzing impression history, for example, might help determine how well a specific row on the home page is functioning or assess the effectiveness of a merchandising strategy.The first pivotal step in managing impressions begins with the creation of a Source-of-Truth (SOT) dataset. This foundational dataset is essential, as it supports various downstream workflows and enables a multitude of use cases.Collecting Raw Impression EventsAs Netflix members explore our platform, their interactions with the user interface spark a vast array of raw events. These events are promptly relayed from the client side to our servers, entering a centralized event processing queue. This queue ensures we are consistently capturing raw events from our global user base.After raw events are collected into a centralized queue, a custom event extractor processes this data to identify and extract all impression events. These extracted events are then routed to an Apache Kafka topic for immediate processing needs and simultaneously stored in an Apache Iceberg table for long-term retention and historical analysis. This dual-path approach leverages Kafka’s capability for low-latency streaming and Iceberg’s efficient management of large-scale, immutable datasets, ensuring both real-time responsiveness and comprehensive historical data availability.Filtering & Enriching Raw ImpressionsOnce the raw impression events are queued, a stateless Apache Flink job takes charge, meticulously processing this data. It filters out any invalid entries and enriches the valid ones with additional metadata, such as show or movie title details, and the specific page and row location where each impression was presented to users. This refined output is then structured using an Avro schema, establishing a definitive source of truth for Netflix’s impression data. The enriched data is seamlessly accessible for both real-time applications via Kafka and historical analysis through storage in an Apache Iceberg table. This dual availability ensures immediate processing capabilities alongside comprehensive long-term data retention.Ensuring High Quality ImpressionsMaintaining the highest quality of impressions is a top priority. We accomplish this by gathering detailed column-level metrics that offer insights into the state and quality of each impression. These metrics include everything from validating identifiers to checking that essential columns are properly filled. The data collected feeds into a comprehensive quality dashboard and supports a tiered threshold-based alerting system. These alerts promptly notify us of any potential issues, enabling us to swiftly address regressions. Additionally, while enriching the data, we ensure that all columns are in agreement with each other, offering in-place corrections wherever possible to deliver accurate data.We handle a staggering volume of 1 to 1.5 million impression events globally every second, with each event approximately 1.2KB in size. To efficiently process this massive influx in real-time, we employ Apache Flink for its low-latency stream processing capabilities, which seamlessly integrates both batch and stream processing to facilitate efficient backfilling of historical data and ensure consistency across real-time and historical analyses. Our Flink configuration includes 8 task managers per region, each equipped with 8 CPU cores and 32GB of memory, operating at a parallelism of 48, allowing us to handle the necessary scale and speed for seamless performance delivery. The Flink job’s sink is equipped with a data mesh connector, as detailed in our Data Mesh platform which has two outputs: Kafka and Iceberg. This setup allows for efficient streaming of real-time data through Kafka and the preservation of historical data in Iceberg, providing a comprehensive and flexible data processing and storage solution.We utilize the ‘island model’ for deploying our Flink jobs, where all dependencies for a given application reside within a single region. This approach ensures high availability by isolating regions, so if one becomes degraded, others remain unaffected, allowing traffic to be shifted between regions to maintain service continuity. Thus, all data in one region is processed by the Flink job deployed within that region.Addressing the Challenge of Unschematized EventsAllowing raw events to land on our centralized processing queue unschematized offers significant flexibility, but it also introduces challenges. Without a defined schema, it can be difficult to determine whether missing data was intentional or due to a logging error. We are investigating solutions to introduce schema management that maintains flexibility while providing clarity.Automating Performance Tuning with AutoscalersTuning the performance of our Apache Flink jobs is currently a manual process. The next step is to integrate with autoscalers, which can dynamically adjust resources based on workload demands. This integration will not only optimize performance but also ensure more efficient resource utilization.Improving Data Quality AlertsRight now, there’s a lot of business rules dictating when a data quality alert needs to be fired. This leads to a lot of false positives that require manual judgement. A lot of times it is difficult to track changes leading to regression due to inadequate data lineage information. We are investing in building a comprehensive data quality platform that more intelligently identifies anomalies in our impression stream, keeps track of data lineage and data governance, and also, generates alerts notifying producers of any regressions. This approach will enhance efficiency, reduce manual oversight, and ensure a higher standard of data integrity.Creating a reliable source of truth for impressions is a complex but essential task that enhances personalization and discovery experience. Stay tuned for the next part of this series, where we’ll delve into how we use this SOT dataset to create a microservice that provides impression histories. We invite you to share your thoughts in the comments and continue with us on this journey of discovering impressions.We are genuinely grateful to our amazing colleagues whose contributions were essential to the success of Impressions: Julian Jaffe, Bryan Keller, Yun Wang, Brandon Bremen, Kyle Alford, Ron Brown and Shriya Arora.]]></content:encoded></item><item><title>Microsoft Study Finds Relying on AI Kills Your Critical Thinking Skills</title><link>https://slashdot.org/story/25/02/14/2320203/microsoft-study-finds-relying-on-ai-kills-your-critical-thinking-skills?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 15 Feb 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A new study (PDF) from researchers at Microsoft and Carnegie Mellon University found that increased reliance on AI tools leads to a decline in critical thinking skills. Gizmodo reports: The researchers tapped 319 knowledge workers -- a person whose job involves handling data or information -- and asked them to self-report details of how they use generative AI tools in the workplace. The participants were asked to report tasks that they were asked to do, how they used AI tools to complete them, how confident they were in the AI's ability to do the task, their ability to evaluate that output, and how confident they were in their own ability to complete the same task without any AI assistance.
 
Over the course of the study, a pattern revealed itself: the more confident the worker was in the AI's capability to complete the task, the more often they could feel themselves letting their hands off the wheel. The participants reported a "perceived enaction of critical thinking" when they felt like they could rely on the AI tool, presenting the potential for over-reliance on the technology without examination. This was especially true for lower-stakes tasks, the study found, as people tended to be less critical. While it's very human to have your eyes glaze over for a simple task, the researchers warned that this could portend to concerns about "long-term reliance and diminished independent problem-solving."
 
By contrast, when the workers had less confidence in the ability of AI to complete the assigned task, the more they found themselves engaging in their critical thinking skills. In turn, they typically reported more confidence in their ability to evaluate what the AI produced and improve upon it on their own. Another noteworthy finding of the study: users who had access to generative AI tools tended to produce "a less diverse set of outcomes for the same task" compared to those without.]]></content:encoded></item><item><title>Did Semgrep Just Get a Lot More Interesting?</title><link>https://fly.io/blog/semgrep-but-for-real-now/</link><author>ghuntley</author><category>hn</category><pubDate>Sat, 15 Feb 2025 00:40:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[This bit by Geoffrey Huntley is super interesting to me and, despite calling out that LLM-driven development agents like Cursor have something like a 40% success rate at actually building anything that passes acceptance criteria, makes me think that more of the future of our field belongs to people who figure out how to use this weird bags of model weights than any of us are comfortable with. I’ve been dinking around with Cursor for a week now (if you haven’t, I think it’s something close to malpractice not to at least take it — or something like it — for a spin) and am just now from this post learning that Cursor has this rules feature. The important thing for me is not how Cursor rules work, but rather how Huntley uses them. He turns them back on themselves, writing rules to tell Cursor how to organize the rules, and then teach Cursor how to write (under human supervision) its own rules.Cursor kept trying to get Huntley to use Bazel as a build system. So he had cursor write a rule for itself: “no bazel”. And there was no more Bazel. If I’d known I could do this, I probably wouldn’t have bounced from the Elixir project I had Cursor doing, where trying to get it to write simple unit tests got it all tangled up trying to make Mox work. But I’m burying the lead. Security people have been for several years now somewhat in love with a tool called Semgrep. Semgrep is a semantics-aware code search tool; using symbolic variable placeholders and otherwise ordinary code, you can write rules to match pretty much arbitary expressions and control flow. If you’re an appsec person, where you obviously go with this is: you build a library of Semgrep searches for well-known vulnerability patterns (or, if you’re like us at Fly.io, you work out how to get Semgrep to catch the Rust concurrency footgun of RWLocks inside if-lets).The reality for most teams though is “ain’t nobody got time for that”. But I just checked and, unsurprisingly, 4o seems to do reasonably well at generating Semgrep rules? Like: I have no idea if this rule is actually any good. But it looks like a Semgrep rule?What interests me is this: it seems obvious that we’re going to do more and more “closed-loop” LLM agent code generation stuff. By “closed loop”, I mean that the thingy that generates code is going to get to run the code and watch what happens when it’s interacted with. You’re just a small bit of glue code and a lot of system prompting away from building something like that right now: Chris McCord is building a thingy that generates whole Elixir/Phoenix apps and runs them as Fly Machines. When you deploy these kinds of things, the LLM gets to see the errors when the code is run, and it can just go fix them. It also gets to see errors and exceptions in the logs when you hit a page on the app, and it can just go fix them.With a bit more system prompting, you can get an LLM to try to generalize out from exceptions it fixes and generate unit test coverage for them. With a little bit more system prompting, you can probably get an LLM to (1) generate a Semgrep rule for the generalized bug it caught, (2) test the Semgrep rule with a positive/negative control, (3) save the rule, (4) test the whole codebase with Semgrep for that rule, and (5) fix anything it finds that way. That is a lot more interesting to me than tediously (and probably badly) trying to predict everything that will go wrong in my codebase a priori and Semgrepping for them. Which is to say: Semgrep — which I have always liked — is maybe a lot more interesting now? And tools like it?]]></content:encoded></item><item><title>AI’s Hallucinations Are Over</title><link>https://hackernoon.com/ais-hallucinations-are-over?source=rss</link><author>Oleksandr Zabashnyi</author><category>tech</category><pubDate>Sat, 15 Feb 2025 00:39:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[First of all, let me describe the problem. I am a software developer, and I don't use AI to write code just because of hallucinations. For creating pictures or writing texts they are not so critical, but for the task of writing code, they are overkill. I have identified two subproblems. Firstly, it is difficult to identify the errors made by AI. It is quite easy for a designer as he counts the fingers in the pictures and that's it; the picture is accepted as the result. \
However, I have to deal with the code and seek the mistakes in it. I hate to do it. Secondly, it is the irreproducibility of the result. For example, I tried to write unit tests: the first generated unit test is successful, but the second one is not, even for the same method. This makes working with it impossible.\
Now, about the solution. Our hypothesis was as follows: what if we could assign the corresponding result data to a certain set of input information? And do that at least sometimes during the task-solving process. Would that reduce the number of hallucinations? Oh yes! After thorough research, we have come to the conclusion that the presence of a critical mass of such ‘rigidly fixed’ nodes almost completely removed hallucinations from the output. All that remained was to find the criteria for identifying these correspondences between input and output information and to modify the math apparatus to enable the use of this approach. We called this method the "preset landscape."\
Certainly, our approach has several limitations. First of all, the subject area should imply the existence of only one correct answer for a given set of output data. Fortunately, software development is exactly such an area. The second limitation is that at the stage of landscape formation, the participation of an expert in the subject field is required. In other words, it is impossible to apply the math apparatus to the areas where a human cannot articulate the rules. These limitations greatly narrow the scope of application of our approach. However, its usage in such areas as software development, law, healthcare, and engineering tasks is more than enough.\
To demonstrate the capabilities of the math apparatus, we have developed a plugin for IntelliJ Idea (JetBrains). You can install it and make sure that there are no hallucinations. Here, you can find the instructions.\
While we were working on the plugin, AI services came into vogue, which provides API and you can use them in your own solutions. Therefore, we are planning to make such a platform for software development tasks. We will most likely start with Java. If you have an insight into how this approach can be implemented for lawyers or healthcare professionals - feel free to share.]]></content:encoded></item><item><title>Tabiew 0.8.4 Released</title><link>https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/</link><author>/u/shshemi</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 00:21:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...📊 Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and Sqlite🗂️ Multi-table functionalityUI is updated to be more modern and responsiveHorizontally scrollable tablesVisible data frame can be referenced with name "_"Compatibility with older versions of glibcTwo new themes (Tokyo Night and Catppuccin)]]></content:encoded></item><item><title>PIN AI Launches Mobile App Letting You Make Your Own Personalized, Private AI Model</title><link>https://mobile.slashdot.org/story/25/02/14/2227222/pin-ai-launches-mobile-app-letting-you-make-your-own-personalized-private-ai-model?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 15 Feb 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from VentureBeat: A new startup PIN AI (not to be confused with the poorly reviewed hardware device the AI Pin by Humane) has emerged from stealth to launch its first mobile app, which lets a user select an underlying open-source AI model that runs directly on their smartphone (iOS/Apple iPhone and Google Android supported) and remains private and totally customized to their preferences. Built with a decentralized infrastructure that prioritizes privacy, PIN AI aims to challenge big tech's dominance over user data by ensuring that personal AI serves individuals -- not corporate interests. Founded by AI and blockchain experts from Columbia, MIT and Stanford, PIN AI is led by Davide Crapis, Ben Wu and Bill Sun, who bring deep experience in AI research, large-scale data infrastructure and blockchain security. [...]
 
PIN AI introduces an alternative to centralized AI models that collect and monetize user data. Unlike cloud-based AI controlled by large tech firms, PIN AI's personal AI runs locally on user devices, allowing for secure, customized AI experiences without third-party surveillance. At the heart of PIN AI is a user-controlled data bank, which enables individuals to store and manage their personal information while allowing developers access to anonymized, multi-category insights -- ranging from shopping habits to investment strategies. This approach ensures that AI-powered services can benefit from high-quality contextual data without compromising user privacy.
[...]
The new mobile app launched in the U.S. and multiple regions also includes key features such as: - The "God model" (guardian of data): Helps users track how well their AI understands them, ensuring it aligns with their preferences. - Ask PIN AI: A personalized AI assistant capable of handling tasks like financial planning, travel coordination and product recommendations. - Open-source integrations: Users can connect apps like Gmail, social media platforms and financial services to their personal AI, training it to better serve them without exposing data to third parties. - "With our app, you have a personal AI that is your model," Crapis added. "You own the weights, and it's completely private, with privacy-preserving fine-tuning." Davide Crapis, co-founder of PIN AI, told VentureBeat that the app currently supports several open-source AI models, including small versions of DeepSeek and Meta's Llama. "With our app, you have a personal AI that is your model," Crapis added. "You own the weights, and it's completely private, with privacy-preserving fine-tuning."
 
You can sign up for early access to the PIN AI app here.]]></content:encoded></item><item><title>Ctrl-Alt-Speech: Backdoors And Backsteps</title><link>https://www.techdirt.com/2025/02/14/ctrl-alt-speech-backdoors-and-backsteps/</link><author>Mike Masnick</author><category>tech</category><pubDate>Sat, 15 Feb 2025 00:01:03 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[In this week’s round-up of the latest news in online speech, content moderation and internet regulation, Mike and Ben are joined by a group of students from the Media Law and Policy class at the American University School of Communication. Together they cover:This episode is brought to you with financial support from the Future of Online Trust & Safety Fund.]]></content:encoded></item><item><title>Conditional types in TypeScript</title><link>https://2ality.com/2025/02/conditional-types-typescript.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In TypeScript, conditional types let us make decisions (think if-then-else expressions) – which is especially useful in generic types. They are also an essential tool for working with union types because they let use “loop” over them. Read on if you want to know how all of that works.]]></content:encoded></item><item><title>Court filings show Meta paused efforts to license books for AI training</title><link>https://techcrunch.com/2025/02/14/court-filings-show-meta-paused-efforts-to-license-books-for-ai-training/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 23:35:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[New court filings in an AI copyright case against Meta add credence to earlier reports that the company “paused” discussions with book publishers on licensing deals to supply some of its generative AI models with training data. The filings are related to the case Kadrey v. Meta Platforms — one of many such cases winding through […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Show HN: VimLM – A Local, Offline Coding Assistant for Vim</title><link>https://github.com/JosefAlbers/VimLM</link><author>JosefAlbers</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 23:34:41 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[VimLM is a local, offline coding assistant for Vim. It’s like Copilot but runs entirely on your machine—no APIs, no tracking, no cloud.- Deep Context: Understands your codebase (current file, selections, references).  
- Conversational: Iterate with follow-ups like "Add error handling".  
- Vim-Native: Keybindings like `Ctrl-l` for prompts, `Ctrl-p` to replace code.  
- Inline Commands: `!include` files, `!deploy` code, `!continue` long responses.Perfect for privacy-conscious devs or air-gapped environments.Try it:  
```
pip install vimlm
vimlm
```]]></content:encoded></item><item><title>AI Alexa and AI Siri face bugs and delays</title><link>https://techcrunch.com/2025/02/14/ai-alexa-and-ai-siri-face-bugs-and-delays/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Fri, 14 Feb 2025 23:24:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Amazon and Apple are struggling to put generative AI technology in their digital assistants — Alexa and Siri, respectively — according to a pair of reports that came out on Friday. Amazon hoped to release its new Alexa during an event in New York on February 26. Now Amazon plans to delay the release of […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Netflix Accidentally Made Its Content Show Up In the Apple TV App</title><link>https://apple.slashdot.org/story/25/02/14/2213202/netflix-accidentally-made-its-content-show-up-in-the-apple-tv-app?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 14 Feb 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Netflix content briefly appeared in the Apple TV app due to an unintentional glitch, sparking excitement among users before the company swiftly rolled back the integration. Engadget reports: A Netflix spokesperson told The Verge on Friday that the Apple TV app integration was an error that has been rolled back. Indeed, Redditors who had been tracking the forbidden fruit with unbridled glee confirmed that all signs of Netflix content had since vanished from Apple's streaming hub. Netflix giveth, and Netflix taketh away.
 
While the boo-boo was still active, PC World reported it let you add Netflix originals like Stranger Things, Cobra Kai and The Crown but lacked licensed shows and movies. Even the available content was a buggy mess. For example, only season five of The Crown was available, leaving you to wonder what hijinks Liz and the gang had gotten into before or after the grunge era. The "Add to Watchlist" and "Continue Watching" features were also said to be spotty.]]></content:encoded></item><item><title>GOGC &amp; GOMEMLIMIT ?</title><link>https://www.reddit.com/r/golang/comments/1ipnxxk/gogc_gomemlimit/</link><author>/u/mistyrouge</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 23:19:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[If the GC cost is fixed with regards to the amount of memory being freed up. Why would I not want to put  and  to say 70% of the memory I have available? Specially in an application that is known to be cpu bound.   submitted by    /u/mistyrouge ]]></content:encoded></item><item><title>A decade later, a decade lost (2024)</title><link>https://meyerweb.com/eric/thoughts/2024/06/07/a-decade-later-a-decade-lost/</link><author>ZeWaka</author><category>hn</category><pubDate>Fri, 14 Feb 2025 23:10:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[I woke up this morning about an hour ahead of my alarm, the sky already light, birds calling.  After a few minutes, a brief patter of rain swept across the roof and moved on.I just lay there, not really thinking.  Feeling.  Remembering.Almost sixteen years to the minute before I awoke, my second daughter was born.  Almost ten years to the same minute before, she’d turned six years old, already semi-unconscious, and died not quite twelve hours later.So she won’t be taking her first solo car drive today.  She won’t be celebrating with dinner at her favorite restaurant in the whole world.  She won’t kiss her niece good night or affectionately rag on her siblings.Or maybe she wouldn’t have done any of those things anyway, after a decade of growth and changes and paths taken.  What would she really be like, at sixteen?We will never know.  We can’t even guess.  All of that, everything she might have been, is lost.This afternoon, we’ll visit Rebecca’s grave, and then go to hear her name read in remembrance at one of her very happiest places, Anshe Chesed Fairmount Temple, for the last time.  At the end of the month, the temple will close as part of a merger.  Another loss.A decade ago, I said that I felt the weight of all the years she would never have, and that they might crush me.  Over time, I have come to realize all the things she never saw or did adds to that weight.  Even though it seems like it should be the same weight.  Somehow, it isn’t.I was talking about all of this with a therapist a few days ago, about the time and the losses and their accumulated weight.  I said, “I don’t know how to be okay when I failed my child in the most fundamental way possible.”“You didn’t fail her,” they said gently.“I know that,” I replied. “But I don’t feel it.”A decade, it turns out, does not change that.  I’m not sure now that any stretch of time ever could.]]></content:encoded></item><item><title>Pull Request Testing on Kubernetes: How to Test Locally and on GitHub Workflows</title><link>https://hackernoon.com/pull-request-testing-on-kubernetes-how-to-test-locally-and-on-github-workflows?source=rss</link><author>Nicolas Fränkel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 23:05:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Imagine an organization with the following practices:Runs its CI/CD pipelines with GitHub ActionsRuns its production workload on Kubernetes\
A new engineer manager arrives and asks for the following:On every PR, run integration tests in a Kubernetes cluster similar to the production one.\
In this series of posts, I'll show how you can do it. My plan is the following:This blog post focuses on the app, the basic GitHub workflow setup, and testing both locally and during the workflow run.The second blog post will detail the setup of a Google Kubernetes Engine instance and how to adapt the workflow to use it.Unit Testing vs. Integration TestingIntegration Testing is a strategy to test the collaboration of at least two components.\
I translated it in Object-Oriented Programming as:Integration Testing is a strategy to test the collaboration of at least two classes.Let’s consider the making of a car. Single-class testing is akin to testing each nut and bolt separately. Imagine testing of such components brought no issue to light. Still, it would be very risky to mass manufacture the car without having built a prototype and sent it to a test drive.\
However, technology has evolved since that time.I use the word "technology" very generally, but I have Testcontainers in mind:Unit tests with real dependencies\
  Testcontainers is an open source library for providing throwaway, lightweight instances of databases, message brokers, web browsers, or just about anything that can run in a Docker container.\
In effect, Testcontainers replaces  with "real" dependencies-containerized. It's a real game-changer: instead of painfully writing mocking code to stub dependencies just set them up regularly.\
For example, without Testcontainers, you'd need to provide mocks for your  in tests; with it, you only need to start a database container, and off you go.\
At the time, the cost of having a local Docker daemon in your testing environment offset many benefits. It's not the case anymore, as Docker daemons are available (nearly) everywhere.\
My definition of Integration Testing has changed a bit:Integration Testing is testing that requires significant setup.\
The definition is vague on purpose, as significance has a different meaning depending on the organization, the team, and the individual. Note that Google defines two categories of tests: fast and slow. Their definition is equally vague, meant to adapt to different contexts.In any case, the golden rule still applies: the closer you are to the final environment, the more risks you cover, and the more valuable your tests are. \
If our target production environment is Kubernetes, we will reap the most benefits from running the app on Kubernetes and testing it as a black box. It doesn't mean that white box testing in a more distant environment is not beneficial; it means that the more significant the gap between the testing environment and the target environment, the fewer issues we will uncover.\
For the purposes of this blog post, we will use GitHub as the base testing environment for unit testing and a full-fledged Kubernetes cluster for integration testing. There is no absolute truth regarding what is the best practice™, as contexts vary widely across organizations and even across teams within the same organization. It's up to every engineer to decide within their specific context the ROI of setting up such an environment because the closer you are to production, the more complex and, thus, expensive it will be.Use-case: Application With DatabaseLet's jump into how to test an app that uses a database to store its data. I don't want anything fancy, just solid, standard engineering practices. I'll be using a CRUD(Create Read Update Delete) JVM-based app, but most of the following can easily apply to other stacks as well. The following blog posts will involve less language-specific content.Kotlin, because I love the languageSpring Boot: it's the most widespread framework for JVM-based applicationsMaven-there's nothing elseProject Reactor and coroutines, because it makes things more interestingPostgreSQL-at the moment, it's a very popular database, and it's well-supported by Spring\
If you don't know Flyway, it allows you to track database schemas and data in a code repository and manage changes, known as migrations, between versions. Each migration has a unique version, , v1.0, v1.1, v2.1.2, etc. Flyway tries to apply migration in order. If it has already applied a migration, it skips it. Flyway stores its data in a dedicated table to track the applied migrations.\
This approach is a must-have; Liquibase is an alternative that follows the same principles.\
Spring Boot fully integrates Flyway and Liquibase. When the app starts, the framework will kickstart them. If a pod is killed and restarted, Flyway will first check the migrations table to apply only the one that didn't run previously.\
I don't want to bore you with the app details; you can find the code on GitHub.Per my definition above, unit testing should be easy to set up. With Testcontainers, it is.\
The testing code counts the number of items in a table, inserts a new item, and counts the number of items again. It then checks that:There's one additional item compared to the initial countThat the new item is the one we inserted@SpringBootTest                                                              //1
class VClusterPipelineTest @Autowired constructor(private val repository: ProductRepository) { //2

    @Test
    fun `When inserting a new Product, there should be one more Product in the database and the last inserted Product should be the one inserted`() { //3
        runBlocking {                                                        //4
            val initialCount = repository.count()                            //5
            // The rest of the test
        }
    }
}
Initialize the Spring contextPraise Kotlin for allowing for descriptive function namesRun non-blocking code in a blocking function\
We now need a PostgreSQL database; Testcontainers can provide one for us. However, to avoid conflicts, it will choose a random port until it finds an unused one. We need it to connect to the database, run the Flyway migration, and run the testing code.\
For this reason, we must write a bit of additional code:@Profile("local")                                                              //1
class TestContainerConfig {

    companion object {
        val name = "test"
        val userName = "test"
        val pass = "test"
        val postgres = PostgreSQLContainer<Nothing>("postgres:17.2").apply {   //1
            withDatabaseName(name)
            withUsername(userName)
            withPassword(pass)
            start()
        }
    }
}

class TestContainerInitializer : ApplicationContextInitializer<ConfigurableApplicationContext> {
    override fun initialize(applicationContext: ConfigurableApplicationContext) {
        if (applicationContext.environment.activeProfiles.contains("local")) {
            TestPropertyValues.of(                                             //2
                "spring.r2dbc.url=r2dbc:postgresql://${TestContainerConfig.postgres.host}:${TestContainerConfig.postgres.firstMappedPort}/$name",
                "spring.r2dbc.username=$name",
                "spring.r2dbc.password=$pass",
                "spring.flyway.url=jdbc:postgresql://${TestContainerConfig.postgres.host}:${TestContainerConfig.postgres.firstMappedPort}/$name",
                "spring.flyway.user=$name",
                "spring.flyway.password=$pass"
            ).applyTo(applicationContext.environment)
        }
    }
}
Start the container, but only if the Spring Boot profile  is active.Override the configuration values.\
We need to specify neither the  nor the  if we hacked the  to reuse the R2BC parameters of the same name:spring:
  application:
    name: vcluster-pipeline
  r2dbc:
    username: test
    password: test
    url: r2dbc:postgresql://localhost:8082/flyway-test-db
  flyway:
    user: ${SPRING_R2DBC_USERNAME}                                             #1
    password: ${SPRING_R2DBC_PASSWORD}                                         #1
    url: jdbc:postgresql://localhost:8082/flyway-test-db
Smart hack to DRY configuration further down.\
We also annotate the previous test class to use the initializer:@SpringBootTest
@ContextConfiguration(initializers = [TestContainerInitializer::class])
class VClusterPipelineTest @Autowired constructor(private val repository: ProductRepository) {

    // No change
}
\
Spring Boot offers a couple of options to activate profiles. For local development, we can use a simple JVM property, , mvn test -Dspring.profiles.active=local; in the CI pipeline, we will use environment variables instead.I'll also use Flyway to create the database structure for integration testing. In the scope of this example, the System Under Test will be the entire app; hence, I'll test from the HTTP endpoints. It's end-to-end testing for APIs. The code will test the same behavior, albeit treating the System Under Test as a black box.class VClusterPipelineIT {

    val logger = LoggerFactory.getLogger(this::class.java)

    @Test
    fun `When inserting a new Product, there should be one more Product in the database and the last inserted Product should be the one inserted`() {

        val baseUrl = System.getenv("APP_BASE_URL") ?: "http://localhost:8080" //1

        logger.info("Using base URL: $baseUrl")

        val client = WebTestClient.bindToServer()                              //2
            .baseUrl(baseUrl)
            .build()

        val initialResponse: EntityExchangeResult<List<Product?>?> = client.get() //3
            .uri("/products")
            .exchange()
            .expectStatus().isOk
            .expectBodyList(Product::class.java)
            .returnResult()

        val initialCount = initialResponse.responseBody?.size?.toLong()        //4

        val now = LocalDateTime.now()
        val product = Product(
            id = UUID.randomUUID(),
            name = "My awesome product",
            description = "Really awesome product",
            price = 100.0,
            createdAt = now
        )

        client.post()                                                          //5
            .uri("/products")
            .bodyValue(product)
            .exchange()
            .expectStatus().isOk
            .expectBody(Product::class.java)

        client.get()                                                           //6
            .uri("/products")
            .exchange()
            .expectStatus().isOk
            .expectBodyList(Product::class.java)
            .hasSize((initialCount!! + 1).toInt())
    }
}
Get the deployed app URL.Create a web client that uses the former.Get the initial item list.Get the size; we definitely should offer a count function if there are too many items.Insert a new item and assert everything works out fine.Get the list of items and assert the item count is higher by one.\
Before going further, let's run the tests in a GitHub workflow.I'll assume you're familiar with GitHub workflows. If you aren't, a GitHub workflow is a declarative description of an automated job. A job consists of several steps. GitHub offers several triggers: Manual, scheduled, or depending on an event.\
We want the workflow to run on each Pull Request to verify that tests run as expected.name: Test on PR                                                               #1

on:
  pull_request:
    branches: [ "master" ]                                                     #2
Trigger on a PR to the master branch.\
The first steps are pretty standard:jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install JRE
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: 21
          cache: maven                                                         #1
The  action includes a caching option for build tools. Here, it will cache dependencies across runs, speeding up consecutive runs. Unless you have good reasons not to, I recommend using this option.\
For the same reason, we should cache our built artifacts. While researching for this post, I learned that GitHub discards them across runs and steps in the same run. Hence, we can speed up the runs by caching them explicitly:      - name: Cache build artifacts
        uses: actions/cache@v4                                                 <1>
        with:
          path: target
          key: ${{ runner.os }}-build-${{ github.sha }}                        <2>
          restore-keys:
            ${{ runner.os }}-build                                             <3>
Use the same action that  uses under the hood.Compute the cache key. In our case, the  should be immutable, but this should be how you run matrices across different operating systems.Reuse the cache if it's the same OS.      - name: Run "unit" tests
        run: ./mvnw -B test
        env:
          SPRING_PROFILES_ACTIVE: local                                        <1>
Activate the local profile. The workflow's environment provides a Docker daemon. Hence, Testcontainer successfully downloads and runs the database container.\
At this point, we should run the integration test. Yet, we need the app deployed to run this test. For this, we need available infrastructure.Alternative "Unit testing" on GitHubThe above works perfectly on GitHub, but we can move closer to the deployment setup by leveraging GitHub service containers. Let's migrate PostgreSQL from Testcontainers to a GitHub service container.\
Removing Testcontainers is pretty straightforward: we do not activate the  profile.\
Using GitHub's service container requires an additional section in our workflow:jobs:
  build:
    runs-on: ubuntu-latest
    env:
      GH_PG_USER: testuser                                                     #1
      GH_PG_PASSWORD: testpassword                                             #1
      GH_PG_DB: testdb                                                         #1
    services:
      postgres:
        image: postgres:15
        options: >-                                                            #2
          --health-cmd "pg_isready -U $POSTGRES_USER"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
            - 5432/tcp                                                         #3
        env:
          POSTGRES_USER: ${{ env.GH_PG_USER }}                                 #4
          POSTGRES_PASSWORD: ${{ env.GH_PG_PASSWORD }}                         #4
          POSTGRES_DB: ${{ env.GH_PG_DB }}                                     #4
Define environment variables at the job level to use them across steps. You can use secrets, but in this case, the database instance is not exposed outside the workflow and will be switched off when the latter finishes. Environment variables are good enough to avoid adding unnecessary secrets.Make sure that PostgreSQL works before going further.Assign a random port and map it to the underlying  port.Use the environment variables.\
To run the tests using the above configuration is straightforward.      - name: Run "unit" tests
        run: ./mvnw -B test
        env:
          SPRING_FLYWAY_URL: jdbc:postgresql://localhost:${{ job.services.postgres.ports['5432'] }}/${{ env.GH_PG_DB }} #1
          SPRING_R2DBC_URL: r2dbc:postgresql://localhost:${{ job.services.postgres.ports['5432'] }}/${{ env.GH_PG_DB }} #1
          SPRING_R2DBC_USERNAME: ${{ env.GH_PG_USER }}
          SPRING_R2DBC_PASSWORD: ${{ env.GH_PG_PASSWORD }}
GitHub runs PostgreSQL on a local Docker, so the host is . We can get the random port with the ${{ job.services.postgres.ports['5432'] }} syntax.In this post, we laid the ground for a simple app's unit- and integration-testing, leveraging Testcontainers in the local environment. We then proceeded to automate unit testing via a GitHub workflow with the help of GitHub service containers. In the next post, we will prepare the Kubernetes environment on a Cloud provider infrastructure, build the image, and deploy it to the latter.\
The complete source code for this post can be found on GitHub.Originally published on A Java Geek on February 9th, 2025]]></content:encoded></item><item><title>Figure AI is in talks to raise $1.5B at 15x its last valuation</title><link>https://techcrunch.com/2025/02/14/figure-ai-is-in-talks-to-raise-1-5b-at-15x-its-last-valuation/</link><author>Charles Rollet</author><category>tech</category><pubDate>Fri, 14 Feb 2025 22:51:49 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Robotics startup Figure AI is raising $1.5 billion at a $39.5 billion valuation, a whopping 15 times higher than before.© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>If you ever stacked cups in gym class, blame my dad</title><link>https://defector.com/if-you-ever-stacked-cups-in-gym-class-blame-my-dad</link><author>nonoobs</author><category>hn</category><pubDate>Fri, 14 Feb 2025 22:45:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[The boxes came from Tokyo: first by tanker, then overland via container truck from a Pacific port, across the Continental Divide, and finally backed into a driveway at the end of a cul-de-sac in a south Denver suburban enclave. This was a neighborhood with Razor scooters dumped in trimmed front lawns. Where family walks with leashed dogs happened down the middle of intentionally curved streets named after long demolished natural landmarks like "Timbercrest" and "Forest Trails." Where the HOA (because of course there was an HOA) banned the installation of driveway basketball hoops.Receiving industrial freight deliveries, freshly cleared through international customs, probably wasn't explicitly prohibited in the homeowner's handbook. But then, why would it need to be? Nobody would think to bring that kind of commercial chaos into the burgeoning middle-class peace of Castle Pines North in 1998.If neighbors peeking behind curtains at the idling 18-wheeler thought to call in a complaint, the husband and wife receiving the delivery didn't notice. They were too busy unloading boxes—more than 800 of them. Balancing four at a time on a handcart, it took 200 trips through the open garage door and down the unfinished basement steps. The boxes, holding smaller rectangular packages inscribed with Japanese lettering, were piled to the ceiling. There was enough room left for a skinny aisle leading back up the stairs, and two plastic tables ladened with tape and flat-rate USPS packaging. 5,800 miles away, a man in middle management at the global toy conglomerate Hasbro must have been very pleased. The delivery represented a small yet unexpected boon. Those boxes were dead inventory, wasting space in a nondescript warehouse. They should have been headed for a landfill and a tax write-off. Instead, a Mr. Toshio Takiguchi brokered the export at a ¥1,300 per-unit cost. Not a tidy profit, but no longer a loss on the annual P&L. The remains of a failed business decision disappeared across the ocean. It was a certain Mr. Fox's problem now.That problem cost $43,000, a sum that represented the entire life savings of Mr. Fox and his wife, who at that point had been surviving on public-school salaries. Friends and family never really said it out loud, but they were certainly thinking it: This was insane. What about the three kids and that mortgage? The couple, though, never had a doubt. "He always used to tell me, 'If I had 10,000 of these, I could sell them in a year,'" Mrs. Fox recalled, 27 years later.That's the unabashed confidence you'd expect from any good entrepreneur, especially with the benefit of hindsight. But ask yourself this: Would you, watching this couple unload box after box into their bank-financed home, have bet on this man and his family's future if you knew exactly what he needed to sell?Tucked inside those boxes, in nested columns of 12, were 120,000 plastic cups. They were turned upside down, each with a hole drilled through the middle of the base, rendering them useless in terms of a cup's normal, and really only, task.The couple's future hinged on accomplishing what the world's second largest toy company could not: convincing thousands of kids that stacking these plastic cups in pre-determined patterns was … . More critically, the couple needed to convince the parents of those kids to actually buy these cups, despite not even being able to drink from them. Mr. Takiguchi could breathe a sigh of relief. Mr. and Mrs. Fox had to get to work.The first thing you need to know about Bob Fox is that he used to be a clown ... The second thing you need to know about my dad is that he was a really, really good clown. Every few months, in different corners of the internet, someone asks a version of the same question: Why did we all stack plastic cups in elementary school PE class?You might have even asked it yourself, perhaps after a few beers when a friend turned some Solo Cups upside down, activating one of your mid-aughts memories of sitting cross-legged on a linoleum gym floor, surrounded by the clatter of your classmates piling cups into pyramids.Sport Stacking, as it's officially known (cup stacking and speed stacking as it is colloquially known), exists in the same cloud of millennial nostalgia where you'll find vague recollections of SlamBall and JoJo. It has appeared on pretty much every morning and late show, been a trend piece in both the  and , and was once labeled by Glenn Beck as "what’s really wrong with America."It has been a minor plot line in , Matt LeBlanc's , and ; it's been the major plot line in a 2022 Thai-language film released on Netflix titled  (100 percent on Rotten Tomatoes). The infamous "Oh my gosh!" scream in Skrillex's "Scary Monsters and Nice Sprites" was sampled from a viral sport stacking video from 2008.At its peak, between 2002 and 2011, roughly 5,000 American schools included it as part of their annual curriculum, according to Mr. and Mrs. Fox. That means somewhere between five and eight percent of U.S. adults between the ages of 22 and 35 share the same core memory—and in the ensuing years have asked themselves, their friends, or social media the same question: Why did credentialed educational professionals make us do this ludicrous activity in gym class?I am, perhaps, the person best suited on the planet to answer this question. Because the answer … is my dad.The first thing you need to know about Bob Fox is that he used to be a clown. Not in the figurative, funnyman personality sense. In the literal red-nose, black-eyeliner, juggling-tennis-balls-at-children’s-birthday-parties sense.The second thing you need to know about my dad is that he was a really, really good clown. He'd scoff at the mental image you probably have of him right now: oversized shoes, cartoonish honks, and bumbling choreography. My dad's act was entirely silent, required immense skill, and was predicated on audience participation. He was equally good at riding a unicycle and convincing the most curmudgeonly dad in the room to leave their back corner and join him onstage to try—and always fail—at blowing up a balloon animal. Kids went bananas. Adults always laughed. The San Francisco 49ers hired him to be the walk-around entertainment for their post–Super Bowl ring ceremony in 1995. We used to shut the shades so neighbors didn't think he robbed a bank as he counted the cash he earned during Denver's largest busking festival.This was a man who took the unserious very, very seriously. As a high school theater teacher, he'd spend 60–80 hours a week building out increasingly elaborate sets for the annual musical. He built a life-sized plant puppet that could actually eat cast members for , and cut a full-sized muscle car in half so it would fit on stage during ; another car was hoisted onto the school's roof to promote the show. Before her bevy of Oscar nominations, Amy Adams was one of his choreographers and students.He quit because his wife, Jill, needed him home more to help raise their three children. So he became an elementary school classroom and PE teacher instead, where he would end up teaching hundreds of kids how to ride unicycles and produce an annual show on the blacktop basketball court called KidZircus.The first thing you need to know about Jill Fox is that before working as a communications administrator at a school district, she used to be a journalist at a small newspaper, where she was assigned to write a profile about a local clown on "sabbatical" from his full-time job. The second thing you need to know about Jill Fox is that despite the word "sabbatical" doing a generous amount of euphemistic work in her final published piece, she married that unemployed clown.This was a woman who was very, very good at seeing—and supporting—the potential in the crazy.They had to figure out how to tap into the supply chain of global toy conglomerates. Fortunately, they had a guy: Uncle Johnny.It took them 10 months to sell the 10,000 sets of 12 cups. Kids in south Denver, it turns out, went absolutely nuts over this new phenomenon called "cup stacking." That's mostly because my dad figured out the one thing Hasbro did not: The level of interest in the activity hinged almost exclusively on the enthusiasm and skill level of a real-life teacher. Without that, it was just inanimate plastic ephemera sitting on a shelf.The basic rules take about 15 minutes to learn. Like a track meet, the sport is built on completing different "distances" as fast as you can. The shortest, and simplest, is called the 3-3-3. Envision each one of those 3's as a nested column of cups. You start on one side and "upstack" the first 3 into a pyramid, then move to the middle, and then the last 3. Then you return to where you started, and "downstack" them back into columns in the same order. The 3-6-3 has the same principle, but with a pyramid of six cups in the middle. The "Cycle," is the sport's primetime event, starting with the 3-6-3, morphing into a 6-6, and ending with a 1-10-1.If you skimmed over that clunky paragraph, you've proved my dad's foundational epiphany: You really need to see it to buy in. In fact, scrap reading this for a few seconds (that's all it will take) and watch this video instead.Place a set of these cups in the corner of a room or on a toy shelf with paper instructions, and you might earn five minutes of an 8-year-old's frenzied attention. Put that same 8-year-old on the gym floor in front of a professionally trained clown with a passion for juggling, and they will be utterly hooked.A brief digression on nomenclature: The sport was known as "cup stacking" until 2005, when my parents had the savvy to officially change the name to "sport stacking." Why? 1) Because it sounds way cooler, and if enough people call it a sport then it must damn well be one. 2) The phrase "cup stacking" out of context evokes toddlers playing with blocks, a vibe they wanted to avoid. Confusingly, you might have also seen it referenced as "speed stacking," after the company they eventually founded, Speed Stacks. They strategically eschewed this label to avoid the fate of Roller Blades, Frisbee, and Kleenex, all formerly good standing trademarks, until you lot ruined everything. Except for this early-stage reference, I'll be calling it sport stacking hereafter so as to avoid a grumpy phone call from my dad (he'll probably still call about my use of "damn," though). If you have a problem with that, consider the last time you went berserk over a nine-darter or screamed at the Swedish curling team during the Olympics. As my dad likes to say, there used to be a man trying to convince the world that tossing balls made from dried cow skin into retrofitted peach baskets was fun—and you all bought in.It is important to note that my dad did not actually invent sport stacking. Credit goes to a group of bored kids at a Boys and Girls Club in Oceanside, Calif., in the early 1980s. They'd been given a stack of paper cups, and told to figure out a game that didn't involve messy liquid. Under the guidance of a program director named Wayne Godinet, both the rules and equipment evolved. He is responsible for the sport's first major innovation: He drilled a hole through the base of hard plastic cups to reduce air friction and prevent sticking. The California group's high-water mark came on Nov. 2, 1990, when they appeared on The Tonight Show with Johnny Carson. Among the millions watching that night, from his bed in a south Denver suburb: a clown and future PE teacher who thought it looked a bit like upside-down juggling.Hasbro must have been watching, too. Perhaps chasing the Hoola Hoop and Pogs high of the '80s, the company scooped up a license to sell "Kup Stax," popping sets in a rectangular plastic sleeve on toy store shelves around the world. Instead of becoming a fad, the product moved to the clearance rack. Someone in middle management was probably scolded. Manufacturing ceased. In 1995, my dad attended a physical education development workshop to give new teachers a dozen ideas on how to keep kids occupied, exercised, and docile. The classics were all there: four-wheeled square scooters designed to give 80 percent of users elbow burn, dodgeballs primed to pop kids like me in the face, that large circus tent you toss above your head until it temporarily inflates so you can sit under it for a few seconds. So were the cups. He brought them home and taught his kids. I was 4, my brother 6, our sister 9. We not only fell in love with them, we got good—. We weren't outliers. Among an elementary school of 500 students, 250 signed up for an afterschool program to keep stacking. My mom drove to every Toys "R" Us and Walmart within a 50-mile radius. She bought every set of Kup Stax she could find. They were always discounted, tucked in a back corner.The local supply ran dry as my parents hosted workshops. Demand started to skyrocket from other PE teachers. My parents needed to go upstream. They had to figure out how to tap into the supply chain of global toy conglomerates. Fortunately, they had a guy: .Now 64 with a Colonel Sanders goatee and passion for intricate leather carving, my dad's brother has been at certain points in his life: a professor at the Ringling Bros. and Barnum & Bailey Clown College; a street mime in San Diego; a director of a Las Vegas magic show, performed entirely on ice; and a consultant for Japan's oldest and largest circus. If you need to find a man in middle management at Hasbro's Tokyo corporate office, you call Uncle Johnny.The deal was struck. The shipment arrived. And the cups, rather magically, disappeared from our basement.By 1998, Bob Fox was a dealer with no product and an increasingly addicted customer base. There was only one logical next step: Transition from distribution to fabrication.He did make a final good-faith effort to partner with the sport's originator, Wayne Godinet, offering to purchase $20,000 worth of product. It was almost a third of the profit earned from reselling the Japanese shipment. Godinet sent back two sample sets with a bill for the cost of goods and shipping.In a similar vein, Nike was founded in 1964 only after a running shoe company called Onitsuka lost interest in partnering with a recent Oregon track grad named Phil Knight. While his shoe empire was born between the grooves of a waffle iron, Bob and Jill's cup empire was sketched on the back of a Fresh Fish Co. paper placemat.Speed Stacks LLC was incorporated in December 1998. My parents quit their full-time public school jobs in June 2000. This was to be an official test of a core tenet of the American Dream: With enough passion and even more prodigious work ethic, you can turn a ludicrous idea into a successful business. To get started, they just needed four things.First: a mold to mass produce the cups. It required $20,000 and overcoming the absolute befuddlement of executives at a Denver plastics manufacturer who did not understand the  to include three small holes at the base. As each cup came off the line, our family would inspect the shape; minor defects in its roundness caused sticking. Nearly every cup in the first run had to be tossed out for deformities.Second: a nylon and mesh cup-carrying bag. Those were sewn in the back basement of a Vietnamese immigrant–owned textile company. We bagged each set of 12 by hand, my parents staying up until 2:00 in the morning to fulfill orders after working full school days. Bob wasn't satisfied unless the logo of every cup perfectly faced forward.Third: to hit the road. Speed Stack's first tagline was "See it! Believe it! Teach it!" My parents meant it: They knew that success was linearly related to the number of PE teachers who witnessed the sport in person. Our family of five put almost 100,000 miles on our 1997 maroon Suburban in a little over four years.Fourth: kids who could stack the cups. . For some reason, all three of the Fox progeny developed both the interest and intrinsic skill to do just that.At a booth in hotel conference halls and city convention centers, we'd stack for hours, drawing crowds spread three and four rows deep, often to the annoyance of the dodgeball or shuttlecock purveyors next door. Legendary ring announcer Michael Buffer’s voice was always our music cue. "Ladies and gentlemen, welcome to the main event!"It would blast over portable speakers on either side of a gym floor. Emmy, my older sister, would lead us out from a hidden corner, followed by my brother, Brennan, then me: a 7-year-old with Coke-bottle glasses, immensely satisfied by the fact that hundreds of eyes were watching my every move.We stopped when we reached separate tables spread evenly across our makeshift stage, usually under the basketball hoop. We stood still to let the tempo build. "Let’s get ready to rumble!"The synths would drop, and we would commence stacking plastic cups. Faster and with more skill than the students and teachers in Amarillo or Ft. Lauderdale or Butzbach, Germany, or Copenhagen, Denmark, could ever think possible. Our family performed hundreds of school assemblies in almost every corner of the country (and some overseas). The 30-minute spectacle was choreographed specifically to blow the minds of kids between the ages of 7 and 14 as quickly as possible. This wasn't a talk from your fire chief about safety, or puppets teaching you the value of sharing. We humiliated principals and popular teachers by making them race us. There was a fabricated jacket with holes cut in the back that we fitted onto a volunteer, then stuck our arms through the sleeves to make it appear like they were magically adept (performed to the  soundtrack). The finale was a choreographed stacking routine, set to "Dueling Banjos" from .But a school-by-school roadshow wasn't a scalable business model. So we went to the nexus of physical education sweatsuitdom: state and national teacher conventions. At a booth in hotel conference halls and city convention centers, we'd stack for hours, drawing crowds spread three and four rows deep, often to the annoyance of the dodgeball or shuttlecock purveyors next door. It's most likely that you learned how to sport stack because your teacher, with a lanyard badge bouncing over her multi-colored windbreaker on her way to get a free muffin from the guys who sell tug-o-war ropes, stopped in her tracks in the back corner of Exhibit Hall B at an exurban Renaissance hotel. She would watch as our hands manipulated dishware so fast it blurred.If my dad was adept at putting on a good show, my mom was equally adept at corralling the press. Her journalism and PR–honed chops put sport stacking in front of millions, from the D-block of local newscasts to 30 Rock Plaza during the . That's how I found myself chatting with a shirtless Simon Cowell in the makeup room before we both appeared on Ellen DeGeneres's show, taught Tiki Barber the basics of the 3-3-3, and tried to convince a baffled Michael Caine in the green room of LIVE with Regis and Kelly that yes, indeed, kids really do stack plastic cups for fun.All press, to my parents, was good. My dad was absolutely thrilled when Glenn Beck spent four minutes of his daily Fox News monologue staring directly into the camera, railing against the dangers of stacking cups."We can't fix our country by keeping our children weak," Beck sputtered, roughly two minutes in. "They need to have spines. They need to get hit in the face a few times with a ball, you know what I mean? They have to learn how to live with and thrive on past failures."Brands soon took advantage of sport stacking's essential power: No matter the context, it attracts near-instant attention. In 2005, I appeared in a nationwide Comcast commercial for high speed internet. FritoLay shut down part of Times Square to feature our stacking talents as it launched Stax, a competitor to Pringles. A beverage company in England flew a dozen of us to London to help launch an abomination of a drink called Freekee Soda. Main ingredient? Carbonated milk. (Did I pretend to enjoy drinking that carbonated milk so as to not piss off any of the execs helping to pay for my college education? Obviously. But it is a cursed flavor.)Perhaps the only media my parents turned down was an appearance on the infamous reality show . The producers, naturally, presumed the scions of a ludicrous gym activity would have some weird baggage to mine for the masses. Unfortunately, despite the cups, we were pretty normal.To stack as fast as you possibly can in a World Championship is to strike a balance between fluidity and chaos.It was the Germans who made everything more serious.They arrived at the 2004 World Sport Stacking Championships in matching flag-inspired sweatsuits. Multiple coaches would analyze everything from hand-tag efficiency during the four-person relay event to the mental routines of stackers before their individual competition.Sport stacking, I believe, grew into a minor phenomenon for three reasons: 1) It's visually arresting, 2) it's easy to learn, and 3) competing against the clock is incredibly addicting.Media buzz and schoolteachers kindled the first two. But my dad's magnum opus—the reason the sport didn't wither in the way of skip ball and snake boards—was that he (with the help of the 20 employees at the burgeoning HQ) built the infrastructure and equipment to spur global competition. The sport's growth coincided with two of my dad's innovations: the StackMat, a patented self-timing device that allowed kids to practice on their own for hours, aiming to trim hundredths of a second off their best times; and the World Sport Stacking Association, a governing body to codify the rules and serve as the keeper of world, national, and local records.In 1997, 250 kids were hand-timed with stop watches at the first Colorado State Championship. By 2007, ESPN aired an hourlong David Lloyd–hosted special of the World Championships, featuring more than 1,000 competitors from seven countries (re-aired in 2020 during their pandemic "The Ocho" promotion). Like everything in his life, my dad choreographed a spectacle. He ordered a three-story banner to hang behind garbage can–sized cups that served as center stage. The competition floor was arranged with dozens of tables, all marked with laser-lined tape to demarcate relay boundaries.During the final "Stack of Champions," the top competitors (and relay teams) from each event competed under the scrutiny of spotlights, three officials, an instant replay camera, and hundreds of spectators. In individual events, you're allotted two warmups and three tries. Relays are conducted head-to-head in teams of four.For a few years, I was among those finalists. To stack as fast as you possibly can in a World Championship is to strike a balance between fluidity and chaos. Push too hard and the cups tumble. Pull back and you'll lose by thousandths of a second.A relay race in the Stack of Champions, in which I (the third stacker on the left team) stumble at the start, allowing my brother (the last stacker on the right team) to beat us by two-hundredths of a second.For those moments, you exist at the outer physical boundary of human biology. When performing at your peak, the cups feel less like they're being stacked and more like they're being sprayed. In 2001, the world record for sport stacking's marquee event, the Cycle, was 7.43 seconds. It was held by my sister. At the time, most people assumed a sub-seven-second time was physically impossible. The pattern requires more than 40 separate moves. At those speeds, hands make micro-adjustments faster than the brain can register. Her time stood for nearly four years.In 2025, the world record for the cycle is 4.739 seconds. Please understand: I know this whole endeavor is silly, that it's worthy of your slight mockery and general patina of confusion. But also understand that my dad created a culture in which German coaches passed strategic tips to Australian competitors. Where Japanese teenagers became pen pals with suburban Texans. Where nerves collapsed faster than the cups. Where kids and adults shrieked with joy over millisecond improvements. Where tension and drama and friendship mingled with the clattering cacophony of sliding and tapping plastic (there really is no sound like the one at these competitions). Sport stacking had broad exposure in both mainstream and weird places. But it had a deep impact in a much smaller community. It changed lives.You've probably heard Rachael’s voice. Her effusive reaction on a stacking YouTube video meant for her small group of friends is behind the iconic "YES, OH MY GOSH!" scream, sampled in one of EDM's most mainstream hits, "Scary Monsters and Nice Sprites," by Skrillex.Milo Ferguson discovered sport stacking in first-grade PE class. "My teacher was the final boss of butch lesbians. That's the only way I know how to describe her," Ferguson, now a 23-year-old co-founder of an independent animation studio, said. Their teacher loved competition and was beloved by her students, exactly the archetype to stop at a conference booth and believe in a quirky new cup-centric educational trend.Rachael Nedrow, now a 29-year-old product manager at Amazon, first saw the sport while scrolling YouTube when she was 11. She pulled paper cups from the kitchen drawer to try and mimic the movement.Both became obsessed, quickly. Though for different reasons."I was never the fastest kid. I was always kind of chubby. I was much more the creative person than an athletic one," Ferguson said. "Sport stacking was about hand-eye coordination more than strength. It was intricate. Delicate." For the first time, they were excited to attend PE class.Nedrow, an accomplished tennis player, was intrigued by the self-improvement and rigid strictures of the clock. She bought an official set of Speed Stacks and started logging her progress on her own YouTube channel."My first competition was at a local church," Nedrow said. "I did terribly, but I still won."Despite differing motivations, Ferguson and Nedrow's paths to competitive sport stacking mirror each other. Friends weren't interested, so they went looking for a community online and at competitions. For the thousand kids that congregated on the arena floor at each World Championships, like Ferguson and Nedrow, the sentiment was common. They had found  people. They fit in, sometimes for the first time in their life. There were future college athletes and kids with autism. There were rugby fans from New Zealand (riffing off their national team, they called themselves the "All Stacks") and South Koreans who had never been to the United States. Roll your eyes, but my dad loved to say that the sport only helped build "positive pyramids." Spend a few hours watching kids pat each other on the back after a fumbled cup, or earn a hug for achieving a new personal best, and you'd buy into the platitude too.Compared to other top-level stackers, Nedrow and Ferguson's experiences only differ in one way. You've probably heard Rachael's voice. Her effusive reaction on a stacking YouTube video meant for her small group of friends is behind the iconic "YES, OH MY GOSH!" scream, sampled in one of EDM's most mainstream hits, "Scary Monsters and Nice Sprites," by Skrillex. Between Spotify and YouTube, it has more than 500 million streams.And you probably know Ferguson's dad, Craig, the comedian and host of The Late Late Show with Craig Ferguson from 2005 to 2014.Nedrow's collision with virality gave her backstage passes to several Skrillex concerts and the greatest fodder for "Two Truths and a Lie" of all time. But sport stacking also gave her something else she wasn’t expecting: a wide group of online friends."I don't think I would have taken stacking so far if it wasn't for the online community," Nedrow said. "When you are doing anything kind of competitive, it seems like it's much better when you are doing it with people. I really loved the aspect of kids commenting on my videos, saying they started stacking because of me. What greater pleasure can you get than inspiring other people to do what you love the most?"Sport stacking gave Ferguson inclusion and validation. But perhaps more important than what it gave them is what it gave their parents. While Craig and his ex-wife Sascha had been separated for a few years by the time their kid started competing, both fully committed to supporting this weird, wholesome obsession. Sascha helped Milo learn how to breathe and center themselves before starting to stack. And she supplied an ever-growing collection of stacking equipment, stored in a dedicated room in their Southern California home.To see what sport stacking gave Craig, you need to watch his show from April 20, 2009. He spends four minutes—more than half of the entire monologue—bragging about Milo. Beneath the scripted jokes and of-the-era references (Octomom is mentioned twice), you see a guy just genuinely proud of his kid. He's like any neighbor, going off a bit too effusively about the weekend's peewee football game or math test. Except this was done for millions. And it was about ... cups. "I wanted to give kids, no matter their athletic ability, the opportunity for success. It gave them something new and different. And it created a community. It brought kids around the world together."Sport stacking gave different things to a lot of people.For a few, it genuinely improved their life. College funds were paid through YouTube viewership revenue and small brand endorsements. Children with autism and special needs learned, practiced, competed, and socialized with others for the first time. For me and my siblings, it gave us a stable home, trips to a dozen countries, and some excellent college essay material. My sister, Emmy, went on to play basketball for four years at the University of Minnesota, was drafted into the WNBA by the Minnesota Lynx, and then played professionally overseas with Sheryl Swoopes. My brother Brennan played D-III college football and turned into a mechanical engineer at a global cosmetics company. I turned into a fairly unathletic writer.For my parents, it gave them an early retirement. A sometimes-operational 1957 VW Bug and an obsession with growing ever-larger pumpkins occupied my dad, while training to run another marathon at age 70 became a focus for my mom. They left their active roles at Speed Stacks in 2015, leaving control to some of its earliest employees. Programs are still active in thousands of schools around the world."I was motivated to spread sport stacking for the same reason I taught my students how to juggle or unicycle," my dad recalled, nearly 10 years into retirement. "I wanted to give kids, no matter their athletic ability, the opportunity for success. It gave them something new and different. And it created a community. It brought kids around the world together. And believe it or not, that all happened because of a couple million plastic cups."For some of you reading this, sport stacking gave you an identical memory. A cross-legged seat on a linoleum gym floor. The hush from the principal. The music crescendoing into ' most iconic synth riff. The sound of cups, stacking at lightning speed. You knew, in that moment, that you weren't in for another monotonous few hours at school. Your day was about to rock. For that, you can thank my dad.]]></content:encoded></item><item><title>Final Fantasy iOS Game Shuts Down Over Unfixable Bug</title><link>https://it.slashdot.org/story/25/02/14/223226/final-fantasy-ios-game-shuts-down-over-unfixable-bug?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 14 Feb 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Verge's Jay Peters reports: Square Enix has shut down the iOS version of Final Fantasy Crystal Chronicles and removed it from the App Store following an unfixable bug that blocked people from accessing content they had paid for. [...] The company says that if you made in-app purchases in January 2024 or later, you're eligible to request a refund by contacting Apple Support. Square Enix says that Final Fantasy Crystal Chronicles will continue to be supported on other platforms. The game is also available on Android, PlayStation, and Nintendo Switch. "The issue is due to changes made to the in-app purchases model," Square Enix says in a post. "Further investigation revealed that we are unable to completely fix the bug and implement the new changes, making it unlikely to resume service for the game." Square Enix says it started receiving reports on January 24th about the issue, which "extends to the full paid version of the game."]]></content:encoded></item><item><title>We were wrong about GPUs</title><link>https://fly.io/blog/wrong-about-gpu/</link><author>mxstbr</author><category>hn</category><pubDate>Fri, 14 Feb 2025 22:36:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[We’re building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren’t going anywhere, but: GPUs aren’t going anywhere.A Fly Machine is a Docker/OCI container running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It’s a Fly Machine that can do fast CUDA.Like everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn’t fit the moment. It’s a bet that doesn’t feel like it’s paying off.If you’re using Fly GPU Machines, don’t freak out; we’re not getting rid of them. But if you’re waiting for us to do something bigger with them, a v2 of the product, you’ll probably be waiting awhile.GPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines Intel’s Cloud Hypervisor, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors.GPUs terrified our security team. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers(not even bidirectional: in common configurations, GPUs talk to each other)with arbitrary, end-user controlled computation, all operating outside our normal security boundary.We did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren’t mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there’s a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers.We funded two very large security assessments, from Atredis and Tetrel, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time.Security wasn’t directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason.We could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we’d have been on Nvidia’s driver happy-path.Alternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path.Instead, we burned months trying (and ultimately failing) to get Nvidia’s host drivers working to map virtualized GPUs into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU.I’m not sure any of this really mattered in the end. There’s a segment of the market we weren’t ever really able to explore because Nvidia’s driver support kept us from thin-slicing GPUs. We’d have been able to put together a really cheap offering for developers if we hadn’t run up against that, and developers love “cheap”, but I can’t prove that those customers are real.On the other hand, we’re committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer’s OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our  orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying!And, of course, we bought GPUs. A lot of GPUs. Expensive GPUs.The biggest problem: developers don’t want GPUs. They don’t even want AI/ML models. They want LLMs.  may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But  don’t care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can’t just give them a GPU.For those developers, who probably make up most of the market, it doesn’t seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of “tokens per second” aren’t counting milliseconds.(you should all feel sympathy for us)This makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they’ll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn’t seem to matter yet, so the market doesn’t care.Past that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough.People doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s.Near as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device.We think there’s probably a market for users doing lightweight ML work getting tiny GPUs. This is what Nvidia MIG does, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it’s not baked; we can’t use it. And I’m not sure how many of those customers there are, or whether we’d get the density of customers per server that we need.That leaves the L40S customers. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they’re the one part we have in our inventory people seem to get a lot of use out of. We’re happy with them. But they’re just another kind of compute that some apps need; they’re not a driver of our core business. They’re not the GPU bet paying off.Really, all of this is just a long way of saying that for most software developers, “AI-enabling” their app is best done with API calls to things like Claude and GPT, Replicate and RunPod.A very useful way to look at a startup is that it’s a race to learn stuff. So, what’s our report card?First off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of  models, the world Elixir Bumblebee looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems.But Cursor happened, and, as they say, how are you going to keep ‘em down on the farm once they’ve seen Karl Hungus? It seems much clearer where things are heading.GPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing.Another way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn’t a winning strategy. I’d rather we’d flopped the nut straight, but I think going in on this hand was the right call.A really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our costs here aren’t recoverable. But the hardware parts that aren’t generating revenue will ultimately get liquidated; like with our portfolio of IPv4 addresses, I’m even more comfortable making bets backed by tradable assets with durable value.In the end, I don’t think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I’m very happy about is that we didn’t compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we’re scaling back our GPU ambitions without having sacrificed any of our isolation story, and, ironically, GPUs  are making that story a lot more important. The same thing goes for our Fly Machine developer experience.We started this company building a Javascript runtime for edge computing. We learned that our customers didn’t want a new Javascript runtime; they just wanted their native code to work. We shipped containers, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That’s usually how we figure out the right answers:  by being wrong about a lot of stuff.]]></content:encoded></item><item><title>Dell Nears $5 Billion AI Server Deal for Elon Musk’s xAI</title><link>https://www.bnnbloomberg.ca/business/technology/2025/02/14/dell-nears-us5-billion-ai-server-deal-for-elon-musks-xai/</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 22:25:37 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[(Bloomberg) -- Dell Technologies Inc. is in advanced stages of securing a deal worth more than $5 billion to provide Elon Musk’s xAI with servers optimized for artificial intelligence work.The company will sell servers containing Nvidia Corp. GB200 semiconductors to Musk’s AI startup for delivery this year, according to people familiar with the matter, who asked to not to be named because the work is private. Some details are being finalized and still may change, some of the people added. Demand for computing to run AI workloads has led to a boom for makers of high-powered servers like Dell, Super Micro Computer Inc. and Hewlett Packard Enterprise Co. Musk’s companies, including carmaker Tesla Inc. and xAI, have emerged as major customers for the hardware. Dell and Nvidia declined to comment. xAI didn’t respond to a request for comment.Dell shares jumped as much as 6% to $116.88 Friday on the news before paring some gains. The stock had slipped 4.3% this year through Thursday’s close.A supercomputer project being built by xAI in Memphis has used a mix of Dell and Super Micro servers. In December, Dell said it had deployed tens of thousands of graphics processing units, or GPUs, there and was working to win an “unfair share” of the remaining build-out. GPUs are the key chips to power AI workloads and Nvidia is the top maker of those processing units.Analysts expect Dell will have shipped more than $10 billion of AI servers in the fiscal year ending last month and project that value will jump to $14 billion in the fiscal year ending in January 2026. Dell is scheduled to report fiscal fourth-quarter earnings on Feb. 27, with the AI server business a major focus for investors.The deal with xAI “would firmly establish the company as a leading AI-server provider and boost sales, though the impact on profitability is less clear,” wrote Woo Jin Ho, an analyst at Bloomberg Intelligence.AI startup xAI’s main product, a chatbot called Grok, has primarily been available to paying users of X, the social network formerly known as Twitter. Firms that Musk runs are known to share employees, technology and computing power.--With assistance from Ian King and Kurt Wagner.(Updates with comments from analyst in the eighth paragraph.)]]></content:encoded></item><item><title>Django Weblog: DjangoCongress JP 2025 Announcement and Live Streaming!</title><link>https://www.djangoproject.com/weblog/2025/feb/14/djangocongress-jp-2025-announcement-and-livestream/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 22:12:10 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[It will be streamed on the following YouTube Live channels:This year there will be talks not only about Django, but also about FastAPI and other asynchronous web topics. There will also be talks on Django core development, Django Software Foundation (DSF) governance, and other topics from around the world. Simultaneous translation will be provided in both English and Japanese.The Async Django ORM: Where Is it?Speed at Scale for Django Web ApplicationsImplementing Agentic AI Solutions in Django from scratchDiving into DSF governance: past, present and futureGetting Knowledge from Django Hits: Using Grafana and PrometheusCulture Eats Strategy for Breakfast: Why Psychological Safety Matters in Open SourceµDjango. The next step in the evolution of asynchronous microservices technology.A public viewing of the event will also be held in Tokyo. A reception will also be held, so please check the following connpass page if you plan to attend.]]></content:encoded></item><item><title>Complex dynamics require complex solutions</title><link>https://mathstodon.xyz/@tao/113873092369347147</link><author>ckemere</author><category>hn</category><pubDate>Fri, 14 Feb 2025 22:05:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SailPoint’s dull debut did little to loosen the stuck IPO window, expert says</title><link>https://techcrunch.com/2025/02/14/sailpoints-dull-debut-did-little-to-loosen-the-stuck-ipo-window-expert-says/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 14 Feb 2025 22:00:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[SailPoint’s IPO on Thursday was a disappointment for anyone hoping it would indicate that tech IPOs are hot again. The first day’s trading ended below the $23 initial price. The stock fared a tad better Friday, closing at over $24. But that’s nothing close to the big bang companies and VCs hope for. For instance, […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>OpenAI Eases Content Restrictions For ChatGPT With New &apos;Grown-Up Mode&apos;</title><link>https://slashdot.org/story/25/02/14/2156202/openai-eases-content-restrictions-for-chatgpt-with-new-grown-up-mode?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 14 Feb 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: On Wednesday, OpenAI published the latest version of its "Model Spec," a set of guidelines detailing how ChatGPT should behave and respond to user requests. The document reveals a notable shift in OpenAI's content policies, particularly around "sensitive" content like erotica and gore -- allowing this type of content to be generated without warnings in "appropriate contexts." The change in policy has been in the works since May 2024, when the original Model Spec document first mentioned that OpenAI was exploring "whether we can responsibly provide the ability to generate NSFW content in age-appropriate contexts through the API and ChatGPT."
 
ChatGPT's guidelines now state that that "erotica or gore" may now be generated, but only under specific circumstances. "The assistant should not generate erotica, depictions of illegal or non-consensual sexual activities, or extreme gore, except in scientific, historical, news, creative or other contexts where sensitive content is appropriate," OpenAI writes. "This includes depictions in text, audio (e.g., erotic or violent visceral noises), or visual content." So far, experimentation from Reddit users has shown that ChatGPT's content filters have indeed been relaxed, with some managing to generate explicit sexual or violent scenarios without accompanying content warnings. OpenAI notes that its Usage Policies still apply, which prohibit building AI tools for minors that include sexual content.]]></content:encoded></item><item><title>Eli Bendersky: Decorator JITs - Python as a DSL</title><link>https://eli.thegreenplace.net/2025/decorator-jits-python-as-a-dsl/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 21:49:31 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Spend enough time looking at Python programs and packages for machine learning,
and you'll notice that the "JIT decorator" pattern is pretty popular. For
example, this JAX snippet:In both cases, the function decorated with  doesn't get executed by the
Python interpreter in the normal sense. Instead, the code inside is more like
a DSL (Domain Specific Language) processed by a special purpose compiler built
into the library (JAX or Triton). Another way to think about it is that Python
is used as a  to describe computations.In this post I will describe some implementation strategies used by libraries to
make this possible.Preface - where we're goingThe goal is to explain how different kinds of  decorators work by using
a simplified, educational example that implements several approaches from
scratch. All the approaches featured in this post will be using this flow: Expr IR --> LLVM IR --> Execution" /> Expr IR --> LLVM IR --> Execution" class="align-center" src="https://eli.thegreenplace.net/images/2025/decjit-python.png" />
These are the steps that happen when a Python function wrapped with
our educational  decorator is called:The function is translated to an "expression IR" - .This expression IR is converted to LLVM IR.Finally, the LLVM IR is JIT-executed.First, let's look at the  IR. Here we'll make a big simplification -
only supporting functions that define a single expression, e.g.:Naturally, this can be easily generalized - after all, LLVM IR can be used to
express fully general computations.Here are the  data structures:To convert an  into LLVM IR and JIT-execute it, we'll use this function:It uses the  class to actually generate LLVM IR from .
This process is straightforward and covered extensively in the resources I
linked to earlier; take a look at the full code here.My goal with this architecture is to make things simple, but .
On one hand - there are several simplifications: only single expressions are
supported, very limited set of operators, etc. It's very easy to extend this!
On the other hand, we could have just trivially evaluated the 
without resorting to LLVM IR; I do want to show a more complete compilation
pipeline, though, to demonstrate that an arbitrary amount of complexity can
be hidden behind these simple interfaces.With these building blocks in hand, we can review the strategies used by
 decorators to convert Python functions into s.Python comes with powerful code reflection and introspection capabilities out
of the box. Here's the  decorator:This is a standard Python decorator. It takes a function and returns another
function that will be used in its place ( ensures that
function attributes like the name and docstring of the wrapper match the
wrapped function).After  is applied to , what  holds is the
wrapper. When  is called, the wrapper is invoked with
.The wrapper obtains the AST of the wrapped function, and then uses
 to convert this AST into an :When  finishes visiting the AST it's given, its
 field will contain the  representing the function's
return value. The wrapper then invokes  with this .Note how our decorator interjects into the regular Python execution process.
When  is called, instead of the standard Python compilation and
execution process (code is compiled into bytecode, which is then executed
by the VM), we translate its code to our own representation and emit LLVM from
it, and then JIT execute the LLVM IR. While it seems kinda pointless in this
artificial example, in reality this means we can execute the function's code
in any way we like.AST JIT case study: TritonThis approach is almost exactly how the Triton language works. The body of a
function decorated with  gets parsed to a Python AST, which then
- through a series of internal IRs - ends up in LLVM IR; this in turn is lowered
to PTX by the
NVPTX LLVM backend.
Then, the code runs on a GPU using a standard CUDA pipeline.Naturally, the subset of Python that can be compiled down to a GPU is limited;
but it's sufficient to run performant kernels, in a language that's much
friendlier than CUDA and - more importantly - lives in the same file with the
"host" part written in regular Python. For example, if you want testing and
debugging, you can run Triton in "interpreter mode" which will just run the
same kernels locally on a CPU.Note that Triton lets us import names from the  package
and use them inside kernels; these serve as the  for the language
- special calls the compiler handles directly.Python is a fairly complicated language with  of features. Therefore,
if our JIT has to support some large portion of Python semantics, it may make
sense to leverage more of Python's own compiler. Concretely, we can have it
compile the wrapped function all the way to bytecode,
and start our translation from there.Here's the  decorator that does just this :The Python VM is a stack machine; so we emulate a stack to convert the
function's bytecode to  IR (a bit like an RPN evaluator).
As before, we then use our  utility function to lower
 to LLVM IR and JIT execute it.Using this JIT is as simple as the previous one - just swap 
for :Bytecode JIT case study: NumbaNumba is a compiler for Python itself. The idea
is that you can speed up specific functions in your code by slapping a
 decorator on them. What happens next is similar in spirit to
our simple , but of course much more complicated because it
supports a very large portion of Python semantics.Numba uses the Python compiler to emit bytecode, just as we did; it then
converts it into its own IR, and then to LLVM using .By starting with the bytecode, Numba makes its life easier (no need to rewrite
the entire Python compiler). On the other hand, it also makes some analyses
, because by the time we're in bytecode, a lot of semantic information
existing in higher-level representations is lost. For example, Numba has to
sweat a bit to recover control flow information from the bytecode (by
running it through a special interpreter first).The two approaches we've seen so far are similar in many ways - both rely on
Python's introspection capabilities to compile the source code of the JIT-ed
function to some extent (one to AST, the other all the way to bytecode), and
then work on this lowered representation.The tracing strategy is very different. It doesn't analyze the source code of
the wrapped function at all - instead, it  its execution by means of
specially-boxed arguments, leveraging overloaded operators and functions, and
then works on the generated trace.The code implementing this for our smile demo is surprisingly compact:Each runtime argument of the wrapped function is assigned a , and
that is placed in a , a placeholder class which lets us
do operator overloading:The remaining key function is :To understand how this works, consider this trivial example:After the decorated function is defined,  holds the wrapper function
defined inside . When  is called, the wrapper runs:For each argument of  itself (that is  and ), it creates
a new  holding a . This denotes a named variable in
the  IR.It then calls the wrapped function, passing it the boxes as runtime
parameters.When (the wrapped)  runs, it invokes . This is caught by the overloaded
 operator of , and it creates a new  with
the s representing  and  as children. This
 is then returned .The wrapper unboxes the returned  and passes it to
 to emit LLVM IR from it and JIT execute it with the
actual runtime arguments of the call: .This might be a little mind-bending at first, because there are two different
executions that happen:The first is calling the wrapped  function itself, letting the Python
interpreter run it as usual, but with special arguments that build up the IR
instead of doing any computations. This is the .The second is lowering this IR our tracing step built into LLVM IR and then
JIT executing it with the actual runtime argument values ; this is
the .This tracing approach has some interesting characteristics. Since we don't
have to analyze the source of the wrapped functions but only trace through
the execution, we can "magically" support a much richer set of programs, e.g.:This  with our basic . Since Python variables are
placeholders (references) for values, our tracing step is oblivious to them - it
follows the flow of values. Another example:This also just works! The created  will be a long chain of 
additions of 's runtime values through the loop, added to the 
for .This last example also leads us to a limitation of the tracing approach; the
loop cannot be  - it cannot depend on the function's arguments,
because the tracing step has no concept of runtime values and wouldn't know
how many iterations to run through; or at least, it doesn't know this unless
we want to perform the tracing run for every runtime execution .Tracing JIT case study: JAXThe JAX ML framework uses a tracing
approach very similar to the one described here. The first code sample in this
post shows the JAX notation. JAX cleverly wraps Numpy with its own version which
is traced (similar to our , but JAX calls these boxes "tracers"),
letting you write regular-feeling Numpy code that can be JIT optimized and
executed on accelerators like GPUs and TPUs via XLA. JAX's tracer builds up an underlying IR (called
jaxpr) which can then be
emitted to XLA ops and passed to XLA for further lowering and execution.For a fairly deep overview of how JAX works, I recommend reading the
autodidax doc.As mentioned earlier, JAX has some limitations
with things like data-dependent control flow in native Python. This won't work,
because there's control flow
that depends on a runtime value ():When  is executed, JAX will throw an exception, saying something
like:
This concrete value was not available in Python because it depends on the
value of the argument count.As a remedy, JAX has its
own built-in intrinsics from the jax.lax package.
Here's the example rewritten in a way that actually works: (and many other built-ins in the  package) is something JAX
can trace through, generating a corresponding XLA operation (XLA has support for
While loops, to which this
 can be lowered).The tracing approach has clear benefits for JAX as well; because it only cares
about the flow of values, it can handle arbitrarily complicated Python code,
as long as the flow of values can be traced. Just like the local variables and
data-independent loops shown earlier, but also things like closures. This makes
meta-programming and templating easy .The full code for this post is available on GitHub.]]></content:encoded></item><item><title>Trump’s DOJ Corruption Laid Bare… By His Own Conservative Prosecutors</title><link>https://www.techdirt.com/2025/02/14/trumps-doj-corruption-laid-bare-by-his-own-conservative-prosecutors/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:49:29 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[ make sure you read the update at the end of this story.Here’s a fun thing about corruption investigations: Usually when prosecutors uncover one quid pro quo, they don’t resolve it by offering an even bigger quid pro quo. And yet, that appears to be exactly what’s happening with NYC Mayor Eric Adams, who was indicted last fall for allegedly trading favors with Turkish officials, and is now watching those charges evaporate in exchange for helping the Trump administration with its immigration agenda.The twist — and there’s always a twist — is that the people most effectively pointing out this corruption aren’t the usual suspects. Instead, it’s coming from a bunch of dyed-in-the-wool conservative prosecutors at SDNY who are resigning en masse rather than participate in what they see as a perversion of justice. When the Federalist Society crowd starts quitting over corruption, you know something interesting is happening.The apparent corruption here isn’t just brazen — it’s documented in black and white. The Justice Department’s order to drop the case doesn’t even pretend to assess the merits of the charges. Instead, Acting Deputy Attorney General Emil Bove explicitly tied the dismissal to Adams’ willingness to assist with federal deportation efforts — a textbook example of weaponizing prosecutorial discretion for political ends.Even more disturbing is the mechanism: the dismissal is “without prejudice,” meaning charges could be refiled at any time. This isn’t just prosecutorial discretion — it’s prosecutorial extortion. The Trump administration has effectively created a sword of Damocles to hang over Adams’ head, ensuring his continued compliance with their immigration agenda. The message is clear: step out of line, and those charges might suddenly become relevant again. It’s the kind of institutional corruption that would make a banana republic blush.It means that Adams’ personal freedom now outweighs the best interests of the people of New York City.The system’s response to this corruption has been revealing. For several days after the initial order, an unusual silence descended over the Southern District office — a silence that spoke volumes about the internal struggle taking place. Then came something remarkable: a scathing letter from Acting US Attorney Danielle Sassoon to Attorney General Pam Bondi. Sassoon — a Federalist Society stalwart and former Scalia clerk who’s about as far from a “progressive prosecutor” as you can get — laid bare the rot at the core of this decision in a document that reads like a conservative legal scholar’s manifesto against institutional corruption.Because the law does not support a dismissal, and because I am confident that Adams has committed the crimes with which he is charged, I cannot agree to seek a dismissal driven by improper considerations. As Justice Robert Jackson explained, “the prosecutor at his best is one of the most beneficent forces in our society, when he acts from malice or other base motives, he is one of the worst.” The Federal Prosecutor, 24 J. Am. Jud. Soc’y 18 (“This authority has been granted by people who really wanted the right thing done—wanted crime eliminated— but also wanted the best in our American traditions preserved. “). I understand my duty as a prosecutor to mean enforcing the law impartially, and that includes prosecuting a validly returned indictment regardless whether its dismissal would be politically advantageous, to the defendant or to those who appointed me. A federal prosecutor “is the representative not of an ordinary party to a controversy, but of a sovereignty whose obligation to govern impartially is as compelling as its obligation to govern at all.” Berger v. United States, 295 U.S. 78, 88 (1935).For the reasons explained above, I do not believe there are reasonable arguments in support of a Rule 48(a) motion to dismiss a case that is well supported by the evidence and the law. I understand that Mr. Bove disagrees, and I am mindful of your recent order reiterating prosecutors’ duty to make good-faith arguments in support of the Executive Branch’s positions. See Feb. 5, 2025 Mem. “General Policy Regarding Zealous Advocacy on Behalf of the United States.” But because I do not see any good-faith basis for the proposed position, I cannot make such arguments consistent with my duty of candor. N.Y.R.P.C.3.3; id. cmt. 2 (“A lawyer acting as an advocate in an adjudicative proceeding has an obligation to present the client’s case with persuasive force. Performance of that duty while maintaining confidences of the client, however, is qualified by the advocate’s duty of candor to the tribunal. ” ).In particular, the rationale given by Mr. Bove—an exchange between a criminal defendant and the Department of Justice akin to the Bout exchange with Russia— is, as explained above, a bargain that a prosecutor should not make. Moreover, dismissing without prejudice and with the express option of again indicting Adams in the future creates obvious ethical problems, by implicitly threatening future prosecution if Adams’s cooperation with enforcing the immigration laws proves unsatisfactory to the Department. See In re Christoff, 690 N.E.2d 1135 (Ind. 1997) (disciplining prosecutor for threatening to renew a dormant criminal investigation against a potential candidate for public office in order to dissuade the candidate from running); Bruce A. Green & Rebecca Roiphe, Who Should Police Politicization of the DOJ?, 35 Notre Dame J.L. Ethics & Pub. Pol’y 671, 681 (2021) (noting that the Arizona Supreme Court disbarred the elected chief prosecutor of Maricopa County, Arizona, and his deputy, in part, for misusing their power to advance the chief prosecutor’s partisan political interests) . Finally, given the highly generalized accusations of weaponization, weighed against the strength of the evidence against Adams, a court will likely question whether that basis is pretextual. See, e.g. , United States v. Greater Blouse, Skirt & Neckwear Contractors, 228 F. Supp. 483, 487 (S.D.N.Y. 1964)(courts “ should be satisfied that the reasons advanced for the proposed dismissal are substantial and the real grounds upon which the application is based”)I remain baffled by the rushed and superficial process by which this decision was reached, in seeming collaboration with Adams’s counsel and without my direct input on the ultimate stated rationales for dismissal. Mr. Bove admonished me to be mindful of my obligation to zealously defend the interests of the United States and to advance good-faith arguments on behalf of the Administration. I hope you share my view that soliciting and considering the concerns of the U.S. Attorney overseeing the case serves rather than hinders that goal, and that we can find time to meet.But wait, it gets better! There’s a footnote in Sassoon’s letter that tells you everything you need to know about how modern corruption works. The old-school way was to have your shady meetings in smoke-filled back rooms. The new way, apparently, is to have them in official conference rooms while actively preventing anyone from taking notes:I attended a meeting on January 31, 2025, with Mr. Bove, Adams’s counsel, and members of my office. Adams’s attorneys repeatedly urged what amounted to a quid pro quo, indicating that Adams would be in a position to assist with the Department’s enforcement priorities only if the indictment were dismissed. Mr. Bove admonished a member of my team who took notes during that meeting and directed the collection of those notes at the meeting’s conclusionNothing quite says you know you’re engaging in some shady ass shit like demanding you collect the notes of anyone in attendance.What makes this story particularly significant is who’s blowing the whistle. Sassoon isn’t some “woke prosecutor” that the MAGA world can easily dismiss. She’s a card-carrying member of the conservative legal establishment who, until this week, was seen as a rising star in those circles. Her willingness to sacrifice her standing in that world to uphold basic constitutional principles reveals just how far the corruption has spread — and perhaps offers a glimmer of hope that some institutional guardrails still hold.Sassoon’s stand has triggered a cascade of resignations within SDNY, with seven prosecutors (and counting) choosing to walk away rather than participate in this corruption of justice. The latest resignation letter, a scorching indictment from lead prosecutor Hagan Scotten, is particularly noteworthy. Scotten — who clerked for both Justices Roberts and Kavanaugh and explicitly states his support for the Trump administration — makes it clear that this isn’t about politics; it’s about fundamental principles of justice being trampled for political gain.There is a tradition in public service of resigning in a last-ditch effort to head off a serious mistake. Some will view the mistake you are committing here in the light of their generally negative views of the new Administration. I do not share those views. I can even understand how a Chief Executive whose background is in business and politics might see the contemplated dismissal-with-leverage as a good, if distasteful, deal. But any assistant U.S. attorney would know that our laws and traditions do not allow using the prosecutorial power to influence other citizens, much less elected officials, in this way.If no lawyer within earshot of the President is willing to give him that advice, then I expect you will eventually find someone who is enough of a fool, or enough of a coward, to file your motion. But it was never going to be me.Scotten’s prediction proved grimly prophetic. As reported just hours ago, Bove and Bondi found their willing executioner — though the circumstances reveal yet another layer of institutional corruption:The prosecutor acquiesced to file the motion in an attempt to spare other career staff from potentially being fired by Emil Bove, the acting US deputy attorney general and former personal lawyer to Trump, sources briefed on the matter told Reuters. The news agency named the lawyer as Ed Sullivan, a veteran career prosecutor, who agreed to alleviate pressure on his colleagues in the department’s public integrity section of 30 attorneys, two sources said, after his team was given an hour by Bove to decide between them who would file the motion.“This is not a capitulation – this is a coercion,” one of the people briefed on the meeting later told Reuters. “That person, in my mind, is a hero.” The whole section had reportedly discussed resigning en masse.The cruel irony of forcing the Public Integrity Section to compromise its own integrity isn’t lost on anyone. This is how institutions die — not with a bang, but with an ultimatum.There’s a special kind of institutional poetry here: The Public Integrity Section was given an hour to decide who would compromise their integrity. And someone did, not out of cowardice or foolishness, but to protect their colleagues. “A hero,” his colleague called him, and maybe that’s right. But it’s the kind of heroism that only exists in broken systems.The NY Times has revealed even more disturbing details about the behind-the-scenes machinations. In what reads like a playbook for corrupting justice, Bove apparently coached Adams’ legal team (including Alex Spiro, better known as Elon Musk’s go-to counsel) in a wink-wink-nudge-nudge fashion on exactly what political commitments would make the charges disappear.During the meeting, Mr. Bove signaled that the decision about whether to dismiss the case had nothing to do with its legal merits.Instead, Mr. Bove said he was interested in whether the case was hindering Mr. Adams’s leadership, particularly with regard to the city’s ability to cooperate with the federal government on Mr. Trump’s crackdown on illegal immigration.Mr. Bove also said he was interested in whether the case, brought by the former U.S. attorney, Damian Williams, was a politically motivated prosecution meant to hurt Mr. Adams’s re-election prospects.In her letter to Ms. Bondi, Ms. Sassoon said that she was “baffled by the rushed and superficial process by which this decision was reached, in seeming collaboration with Adams’s counsel and without my direct input on the ultimate stated rationales for dismissal.”There’s something almost elegant about it, in a horrifying sort of way. The Justice Department has managed to transform a corruption prosecution into what amounts to a compliance manual for corruption. It’s like they’ve created a template: “Here’s how to trade criminal charges for political favors while maintaining plausible deniability.” And the really wild part? This is all happening after years of the MAGA world screaming about supposed “lawfare” against conservatives. Turns out they weren’t complaining about weaponized justice — they were planning how to do it themselves.History rhymes: While mass resignations of principled lawyers helped topple Nixon’s presidency, in Trump’s second term they’ve become just another item in the daily digest of institutional erosion. The difference this time? It’s not the usual suspects sounding the alarm. Instead, it’s career conservatives — products of the Federalist Society pipeline — who are putting their careers on the line to preserve what’s left of prosecutorial independence.As we’ve previously discussed, any path through this constitutional crisis requires principled conservatives to find their voice. The fact that it’s taking career prosecutors to do what elected Republicans won’t speaks volumes about where the real courage in conservative circles resides.The question now isn’t just whether our institutions can survive this assault, but whether these acts of principled resistance can inspire others before the machinery of justice is fully converted into a tool of political control. The American experiment has survived previous challenges through the courage of individuals willing to place principle above party. We’re about to find out if enough of those individuals still exist. Incredibly, that report that a prosecutor had agreed to file the dismissal turned out to not be accurate. Many hours later, after no such filing was actually made a few very bizarre things happened. First, Emil Bove filed a notice of appearance in the case. That is… not normal.Finally, the “nolle prosequi” (a notice saying “we no longer want to prosecute”) was filed. But even the way it was filed is weird and somewhat unprecedented. Two lawyers, including Ed Sullivan (who was mentioned above as effectively agreeing to be the fool to protect his coworkers) signed , but they did not sign the final statement. Instead, there was a further “order” from the DOJ, signed by Bove alone, telling the Court to effectively dismiss the case: Even the language here is bizarre. The prosecutors don’t get to “direct” the Court to do anything. That’s likely why Bacon and Sullivan signed the part about “respectfully requests” that the Court issue an order. But Bove leaps in, acting like he gets to order around the judge, and separately signs that part.What will be interesting now, is to see what Judge Dale Ho does.]]></content:encoded></item><item><title>The hardest working font in Manhattan</title><link>https://aresluna.org/the-hardest-working-font-in-manhattan/</link><author>robinhouston</author><category>hn</category><pubDate>Fri, 14 Feb 2025 21:45:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[
		In 2007, on my first trip to New York City, I grabbed a brand-new DSLR camera and photographed all the fonts I was supposed to love. I admired American Typewriter in all of the I <3 NYC logos, watched Akzidenz Grotesk and Helvetica fighting over the subway signs, and even caught an occasional appearance of the flawlessly-named Gotham, still a year before it skyrocketed in popularity via Barack Obama’s first campaign. 
	
		But there was one font I didn’t even notice, even though it was everywhere around me.
			
		Last year in New York, I walked over 100 miles and took thousands of photos of one and one font only.
			
		The font’s name is Gorton.
			
		It’s hard to believe today that there was a time before I knew of Gorton and all its quirks and mysteries. The first time I realized the font even existed was some time in 2017, when I was researching for my book about the history of typing. 
			
		Many keyboards, especially older ones, sported a particular distinctive font on their keycaps. It was unusually square in proportions, and a weird mélange of “mechanical” and “childish.”
			 
		The more I looked at it, the more I realized how bizarre and amateurish it was. The G always felt like it was about to roll away on its side. There was a goofy wavy hook sticking out of Q. P and R were often too wide. & and @ symbols would be laughed away in a type crit, and the endings of C felt like grabbing something next to it – a beginning of a ligature that never came.
			
		The strangeness extended to the digits. There was a top-flatted 3 resembling a Cyrillic letter, 7 sloping down in a unique way, a very geometric 4, an unusual – perhaps even naïve – symmetry between 6 and 9, and a conflation of O with 0 that would be a fireable offense elsewhere.
				
		Looking at just a few keyboards, it was also obvious that it wasn’t just one rigid font. There were always variations, sometimes even on one keyboard. 0 came square, dotted, or slashed. The usually very narrow letter I sometimes sported serifs. The R and the 6 moved their middles higher or lower. There also seemed to be a narrower version of the font, deployed when a keycap needed a word and not just a letter. (Lowercase letters existed too, but not very often.) 
			
		My first thought was: What a mess. Is this how “grotesque” fonts got their name?
			
		Then, the second thought: I kind of like it.
	The most distinctive letterforms of Gorton		
		But what font was it? What The Font website posited TT Rounds, Identifont suggested it could be Divulge, my early guess was DIN Rounded or something related to road signage. Whatever it was, a flat R clearly separated it from Helvetica, and the shapes were not as round as even the un-rounded Gotham’s.
			
		A few places for keyboard nerds referred to the font as “Gorton,” but that phrase yielded zero results anywhere I typically looked for fonts I could download and install.
					
		I originally thought this had to do with how keys were made. Only in newer keyboards are the letters printed on top of the keys, or charred from their surface by a laser. In older ones – those from the early 1960s laboratory computers, or the 1980s microcomputers – the way every key was constructed was by first molding the letter from plastic of one color, and then grabbing a different plastic and molding the key around the letter. A Gorton letter was as physical as the key itself. It made the keys virtually indestructible – the legend could not wear off any more than its key – and I imagined required some specialized keyboard-making machinery that came with the “keyboard font” already there.
	
		An example of a “double-shot” key from above and from below
				
		But then, I started seeing Gorton in other places.
						
		Hours of looking at close-ups of keys made me sensitive to the peculiar shapes of some of its letters. No other font had a Q, a 9, or a C that looked like this.
						
		One day, I saw what felt like Gorton it on a ferry traversing the waters Bay Area. A few weeks later, I spotted it on a sign in a national park. Then on an intercom. On a street lighting access cover. In an elevator. At my dentist’s office. In an alley. 
							
		These had one thing in common. All of the letters were carved into the respective base material – metal, plastic, wood. The removed shapes were often filled in with a different color, but sometimes left alone.
						
		At one point someone explained to me Gorton must have been a routing font, meant to be carved out by a milling machine rather than painted on top or impressed with an inked press.
								
		Some searches quickly led me to George Gorton Machine Co., a Wisconsin-based company which produced various engraving machines. The original model 1 led to model 1A and then 3U and then, half a decade later, P1-2. They were all pantograph engravers: They allowed you to install one or more letter templates and then trace their shape by hand. A matching rotating cutter would mimic your movements, and the specially configured arms would enlarge or reduce the output to the size you wanted.
									
		This immediately explained both the metaphorical and literal rough edges of Gorton.
						
		A lot of typography has roots in calligraphy – someone holding a brush in their hand and making natural but delicate movements that result in nuanced curves filled with thoughtful interchanges between thin and thick. Most of the fonts you ever saw follow those rules; even the most “mechanical” fonts have surprising humanistic touches if you inspect them close enough.
				
		But not Gorton. Every stroke of Gorton is exactly the same thickness (typographers would call such fonts “monoline”). Every one of its endings is exactly the same rounded point. The italic is merely an oblique, slanted without any extra consideration, and while the condensed version has some changes compared to the regular width, those changes feel almost perfunctory.
		
		Monoline fonts are not respected highly, because every type designer will tell you: This is not how you design a font. 
			
		It seemed at this point that perhaps P1-2 and its predecessors were a somewhat popular machining product during the 20th century’s middle decades. But casual research through materials preserved by some of George Gorton Machine Company’s fans – including the grandson of the founder – revealed something even more interesting. Gorton the font was a lot older than I expected. 
			
		I found a 1935 catalog showing the very same font. Then one from 1925. And then, there was one all the way from 1902, showing the shapes I was starting to be mildly obsessed with.
				
		To put it in perspective: the font I first assumed was a peer to 1950s Helvetica was already of retirement age the day Helvetica was born. Gorton was older than Gill Sans, Futura, or Johnston’s London Underground font. It was contemporaneous to what today we recognize as the first modern sans serif font, Akzidenz-Grotesk, released but three years before the end of the century.
	
		Imagine how stripped down and exotic Gorton must have felt right next to George Gorton Machine’s then-current logo!
						
		I started researching Gorton more. Unfortunately, as I already suspected, no one ever wrote “I used Gorton to typeset this,” because Gorton was a tenuous name at best. It was the first font, and perhaps originally the  font that came with the engraver, so it suffered a nameless fate, familiar later to many bespoke bitmap fonts adoring the screens of early computers.
						
		The difference from these fonts, however, was that Gorton was meant to travel. And so, since searching for it by name was impossible, for months and years I just kept looking around for the now-familiar shapes.
	
		Gorton wasn’t just on computer keyboards, intercom placards, and sidewalk messages visited by many shoes. Gorton was there on typewriter keyboards, too. And on office signs and airline name tags. On boats, desk placards, rulers, and various home devices from fridges to tape dispensers.
						
		It was also asked to help in situations other fonts rarely did. I spotted Gorton on overengineered buttons that were put to heavy industrial and military use. I saw it carved into surfaces of traffic control devices, elevators and escalators, locomotives and subway trains, submarines and jet fighters. Gorton made its way to peace- and wartime nuclear facilities, it was there on the elevator at the Kennedy Space Center with labels marked EARTH and SPACE… and it went  and then the Moon, as key legends on Apollo’s onboard computer.
								
		But why? Why would anyone choose this kind of an ugly font where so many nicer fonts have already been around for ages?
						
		Some of it might be the power of the default. Popular engraving equipment comes with a built-in font that’s so standard it reuses the router’s name? Of course you will see it, the same way you saw a lot of Arial in the 1990s, or Calibri today.
	
		Gorton was also convenient. If your previous engraving work required you do to the routing equivalent of handwriting or lettering – every letter done by hand – then a modern font you could simply  and one designed with “a minimum of sharp corners for rapid tracing with a smooth stroke,” must have felt like a breath of fresh air.
								
		But why engraving to begin with? Because the affordable and casual printing options we enjoy today – the office laser printers and home inkjets, the FedEx Kinko’s, the various cheap labelers – weren’t there. Even things that today feel obsolete, like dot matrix printers, Letraset, and popular letter stencils, were yet to be invented. Often, your only realistic option was the complicated and time-consuming lettering by hand.
				
		On top of that, Gorton’s longevity must have felt attractive. Ink smudges. Paint fades away. Paper can catch fire (quickly) or germs (slowly). Carve something into plastic, on the other hand, and it can survive decades. Substitute plastic for metal, and you just turned decades into centuries. The text is not added atop heavy-duty material. The text  the material.
	Various items from the 20th century typeset in Gorton					
		I felt good about all my findings: What a strange story of a strange routing font! 
	
		But it turns out I was just getting started. Because soon, I noticed Gorton as ink on paper, and as paint on metal.
	We’re used to the flexibility of fonts today. Fonts as bits inside a computer can become a website, paint on paper, CNC routing, a wall projection, and many other things. But those freedoms weren’t as easy back when fonts were made out of metal. Life’s not as much fun outside of the glamor of a TTF file, and a routing font couldn’t immediately become a regular font – so seeing Gorton being additive and not subtractive was an unexpected discovery.
								
		It turns out that there developed a small cottage industry of things that extended Gorton past its engraving origins.
								
		A company called Keuffel & Esser Co. grabbed Gorton’s machines, and used them to create lettering sets called Leroy. This was Gorton abstracted away – still a pantograph, but cheap, small, completely manual, and a vastly simplified one: no possibility to make things bigger and smaller, and no carving – instead, you’d mount a special pen and draw letters by tracing them.
								
		Another company, Wood-Regan Instrument Co., made a similar set called (semi-eponymously) Wrico. But then, they simplified the process even more. Instead of a pantograph, they offered for sale a set of simple lettering guides used to guide your pen directly on paper.
								
		Some of the traditional draftspeople pooh-poohed these inventions – one handbook wrote “[Those are] of value chiefly to those who are not skilled in lettering. A professional show-card writer could work better and faster without it. A Leroy or Wrico lettering set permits work that is neat, rapid, and nearly foolproof, if not inspired.”
				
		But the products ended up being popular and influential. Their output appeared in many technical documents, but spread even a bit further than that. Eventually, there were stencils made by Unitech, Lutz, Tacro, Teledyne Post, Tamaya, Tech Graphic, Ridgway’s, Faber Castell, Zephyr, Charvoz, Rotring, Pickett, and probably many more.
				
		Then, both EC Comics and All-Star Comics used Leroy in the 1940s and 1950s, most notably in the first comic book that introduced Wonder Woman. This was Gorton spreading further than just technical documents, and inspiring more people.
				
		Elsewhere silkscreening – a pretty cool technique of applying paint on surfaces through a wet mesh of fabric – took Gorton and Leroy in a different direction, by allowing paint on metal.
				
		There was more. The popular plastic letters attached to felt boards, popularized by restaurants decades ago, and more recently revisited by Instagram mom influencers, also clearly derive from Gorton and Leroy.
	
		I also counted at least three different systems of “Gorton movable type” – some where you could assemble physical letters, and some where you could impress them into soft materials using steel types – and I imagine there were probably more.
				
		Letraset, a cheap technique of applying a font by rubbing a letter from a provided sheet onto paper, popular throughout the 1960s, introduced first- or second-hand Leroy too – and so did a few competitors.
						
		In the regulatory space, the U.S. military canonized Gorton in 1968 as a standard called MIL-SPEC-33558 for aircraft and other equipment dials, cancelled it in 1998… then brought it back again in 2007. NATO and STANAG followed. ANSI, American standardization body, made a more rounded Leroy an official font for technical lettering via ANSI Y14.2M, and so did institutions like the US National Park Service.
					
		Gorton went on and on and on. The early Hershey vector fonts, developed on very early computers and still popular in CAD applications today, were also derived from Gorton/Leroy shapes, simplified so that the already-simple curves weren’t even necessary – any letter could now be drawn by a series of straight lines.
						
		And even in the first universe Gorton inhabited things weren’t standing still. 
	
		As the engraving industry learned what’s popular and what is not, the offerings started getting more and more sophisticated. A promotional booklet called “The Whereabouts of 230 Engraving Machines” listed Gorton customers ranging from biscuit makers to fire engine constructors. Othercatalogsproudly listed applications like book covers, billiard balls, organ keys, and toothbrushes, as well as “tools making more tools” – using Gorton engravers to create legends for other machines.
			
		After you bought your pantograph engraver, you could buy attachments for sometimes surprising use cases:
	
		The original machine-shop pantographs were supplanted by smaller portable units (called Pantoetchers) on one side, and by increasingly complex  devices on the other. First generation of those were still huge room-size endeavors with Nixie tubes and complex interfaces labeled… in Gorton itself. 
	
		But the technology matured quickly and soon more and more early manual “tracer-guided” pantographs that forced the operator to put letters side by side and then trace them by hand, were superseded by computerized ones, with both the composition and the routing completely automated. They came from George Gorton Machine Co., and from competitors like New Hermes or H.P. Preis.
					
		You no longer had to buy the chromium-plated brass alphabets weighing up to 13 pounds, choosing the right size from 3/8´´ to 3´´ ahead of time (pantographs allowed for reductions and enlargements, but only gave you a few steps within a specific range.) 
	
		Now, fonts came as digits or formulas built into computer memory, or – for a moment in time – as separate cartridges you’d insert in eager slots. (And yes, before you ask: there were other routing monoline fonts, too. But I really don’t care about any of them.)
						
		It was the same story as in word processing right next door, where old-fashioned Gutenberg-era typesetting was being replaced by increasingly smaller and cheaper computers equipped with first-laughable-then-capable software.
						
		And automation came for the Leroy branch of the tree as well. A few companies grabbed Leroy lettering templates and abstracted them away once more. They created curious small typewriter/plotter hybrids where typing letters on a keyboard would make the pen draw them on paper for you. (I own one of them, a Max Cadliner. It might be one of the strangest typewriters I’ve seen – a weird combination of a machine pretending to be another machine pretending to be a human hand.)
					
		If this was a Gorton typewriter, there were also Gorton , even more sophisticated 1980s machines whose text could be programmed in advance rather than typed one line at a time, and mixed with graphics.
				
		I don’t think the – by now 80 years and counting – fractal explosion of Gorton made its original creators rich.
				
		Copy protection in the world of typography is complicated. The font’s name can be trademarked and other companies legally prevented from using it, and you can’t just grab matrices or font files and copy them without appropriate licenses. But take any text output using a font and then redraw it – and you are within your right to do so, and even to sell the final result. At least in America, or in some other countries until somewhat recently, the shapes of the letters themselves are not legally protected.
				
		This is why Keuffel & Esser, Wood-Regan Instrument, and Letraset could potentially grab Gorton and claim it their own, as long as they didn’t name it Gorton. 
								
		But of course, Gorton was barely named “Gorton” to begin with. In the early days of George Gorton pantographs, as the default pantograph font, it came without a name. (The font sets for purchase were called “standard copies.”) Then, as other fonts were added, it was retroactively named Gorton Normal – the name of the company and the most generic word possible.
						
		Leroy lettering sets started with one font, so similarly to Gorton the font started to be known as “Leroy,” then “Series C,” then “Gothic.” New Hermes called it simply “Block,” Letraset went with “Engineering Standard,” and Rotring – another producer of little computerized plotters – with “Universal.” I’ve also seen “A style,” “Plain Gothic,” and, mysteriously, “Standpoint.” 
								
		I don’t think this was meant to be disrespectful. “Standard,“ “Universal,” “A style” might not have had the connotations of “generic” we associate with them today, but rather meaning “the only one you need,” “approved of by millions,” or “the ultimate.”
								
		But there  one name that felt somewhat inconsiderate. It appeared in one product in the 1980s, a few decades after the birth of another font whose name became recognizable and distinguished. In that product, Gorton was referred to as “Linetica.”
	A few rare examples of Gorton Extended in use						
		Each of these reappearances made small changes to the shapes of some letters. Leroy’s ampersand was a departure from Gorton’s. Others softened the middle of the digit 3, and Wrico got rid of its distinctive shape altogether. Sometimes the tail of the Q got straightened, the other times K cleaned up. Punctuation – commas, quotes, question marks – was almost always redone. But even without hunting down the proof confirming the purchase of a Gorton’s pantograph or a Leroy template set as a starting point, the lineage of its lines was obvious. (The remixes riffed off of Gorton Condensed or the normal, squareish edition… and at times both. The extended version – not that popular to begin with – was often skipped.)
	Classic Gorton vs. Gorton Modified						
		The only “official” update to Gorton I know of, and one actually graced with a name, was Gorton Modified. It was made some time in the 1970s by one of the main keyboard keycap manufacturers, Comptec (later Signature Plastics). It was almost a fusion of Gorton and Futura, with more rounded letterforms. Gone was the quirkiness of 3, 7, Q, C, and the strange, tired ampersand. This is the version people might recognize from some of the 1980s computers, or mechanical keyboards today. 
	
		It is also that last Gorton that mattered.
	A collection of movies and TV shows featuring Gorton							
		My every walk in Chicago or San Francisco was counting down “time to Gorton” – sometimes mere minutes before I saw a placard or an intercom with the familiar font.
								
		This might be embarrassing to admit, but I have never been so happy seeing a font in the wild, particularly as there was almost always some new surprise – a numero, a line going through the Z, a new use, or a new imperfection. And, for a font that didn’t exist, I saw it surprisingly often.
										
		I even spotted Gorton a few times in Spain, or the U.K., and didn’t make too much of it, not thinking about the likelihood of machines from George Gorton’s company in a small town of Racine, Wisconsin making it all the way to different continents. In hindsight I should have.
	Gorton on old British cars, with a particularly delightful Rolls Royce logo made by a simple duplication of the classic Gorton letter R						
		It was only on a trip to Australia where something started connecting. Here, once more, I saw Gorton on the streets, put to work in all sorts of unglamorous situations:
								
		Some letterforms in the above photos felt slightly odd, and so did Gorton on the heavy machinery in an abandoned shipyard on an island near Sydney:
								
		And a visit to a naval museum cemented it all:
	
		It was Gorton, although with some consistent quirks: 2, 5, 6, and 9 were shorter, the centers of M and W didn’t stretch all the way across, and the distinctive shape of S was slightly different here.
								
		Fortunately, this time around, a type designer familiar with my now-public obsession with Gorton clued me in. Gorton didn’t actually originate from Racine, Wisconsin in the late 19th century. It started a bit earlier, and quite a bit further away, at a photographic lens maker in the U.K. called Taylor, Taylor & Hobson. 
								
		In 1894, TT&H needed some way to put markings on their lenses. This being late 19th century, their options were limited to manual engraving, which must have felt tricky given the small font sizes necessary. So the company did what makers sometimes do – instead of searching for a solution that might not have even existed, they made new types of machines to carve out letters, and then designed a font to be used with them.
	
		I don’t know how this first proto-Gorton was designed – unfortunately, Taylor, Taylor & Hobson’s history seems sparse and despite personal travels to U.K. archives, I haven’t found anything interesting – but I know simple technical writing standards existed already, and likely influenced the appearance of the newfangled routing font.
	From a 1895 “Free-hand lettering” book by Frank T. Daniels					
		This was perhaps the first modern pantograph engraver, and perhaps even the arrival of a concept of an engraving font – the first time technical writing was able to be replicated consistently via the aid of the machine.
				
		No wonder that other companies came knocking. Only a few years later, still deep within the 19th century, Taylor, Taylor & Hobson licensed their stuff to a fledgling American company named after its founder. Gorton Model 1 was the first U.S. version of the engraver, and the TT&H font must have been slightly adjusted on arrival. 
	A Taylor-Hobson pantograph in use in 1942			
		This adds to the accomplishments of Gorton – the font was actually  than even Akzidenz-Grotesk, and has been used on World War II equipment and later on on British rifles and motorcycles (and 3,775 finger posts in one of the UK’s national parks), but it complicates the story of the name even more. Turns out, the font without a name has even less of a name than I suspected.
				
		If the Taylor, Taylor & Hobson (or, Taylor-Hobson, as their engravers were known) “branch” of Gorton were more used, should it usurp the at least somewhat popular Gorton name? Or should it just because it was first and the letterform changes were small? Does it matter? Where does one font end and another begin? (Unsurprisingly, TT&H didn’t properly name the font either, eventually calling it “A style” for regular and “C style” for condensed variants. Google searches for “taylor hobson font” are a lot more sparse than those for Gorton.)  
	GortonGorton CondensedThe Gorton quasisuperfamily
		In the end, I’m sticking with Gorton for the whole branch since that feels the most well-known name, but I feel ill-equipped to make that call for everyone. You might choose to call it Gorton, Leroy, TT&H, Taylor-Hobson, or one of the many other names. (Just, ideally, not Linetica.)
	A comparison of all major editions of Gorton					
		And so, throughout the 20th century, Gorton has lived two parallel lives – one originating in the U.K. and later expanding to its colonies and the rest of Europe, and another one in America. 
								
		I am still tracing various appearences of Gorton and perhaps you, dear reader, will help me with that. (Chances are, you will see Gorton later today!) I’m curious about whether Gorton made it to Eastern Europe, Africa, or Asia. I’m interested in seeing if it appeared in Germany where the objectively better-designed DIN fonts became much more popular in Gorton’s niche.
	
		The history of this strange font spans over a century and I’ve seen it in so many countries by now, used in so many situations. But it’s impossible for me to say Gorton is the most hard-working font in the world.
								
		To this title, there are many contenders. Garamond has a head start of 300+ years and has been released in more versions than letters in any alphabet. Helvetica is so famous and used so much that even its ugly copy, Arial, became a household name. Whatever font MS Office or a popular operating system appoint to be “the default” – from Times New Roman through Calibri to Roboto – immediately enjoys the world premiere that any Hollywood movie would be envious of. There is even a 5×7 pixel font originally started by Hitachi that you can see everywhere on cheap electronic displays in cash registers and intercoms.
								
		But there is one place in the world where Gorton pulls triple duty, and I feel confident in saying at least this: Gorton is the hardest working font in Manhattan.
							
		In 2007, on my first trip to New York City, I grabbed my brand-new DSLR camera and photographed all the fonts I was supposed to love: American Typewriter, Helvetica, Gotham. But, in hindsight, I missed the most obvious one.
								
		Gorton is everywhere in Manhattan. It’s there in the elevators, in the subway, on ambulances, in various plaques outside and inside buildings. And god knows it’s there on so, so many intercoms.
						
		I wouldn’t be surprised if there weren’t a single block without any Gorton in a whole of Manhattan.
	A complete inventory of Gorton outside, near my hotel, between 5th and 7th avenues and 25th and 35th streets. I didn’t have access to the interiors of most buildings.	
		The omnipresence of Gorton makes it easy to collect all the type crimes layered on top of the font’s already dubious typographical origins. Walking through Manhattan, you can spot the abominable lowercase that should better be forgotten:
								
		You can see all sorts of kerning mistakes:
										
		You will notice the many, many routing imperfections – an unfinished stroke, a shaky hand, or services of a pantograph that never felt the loving touch of regular maintenance:
	
		There are all the strange decisions to haphazardly mix various styles of Gorton, or even to mix Gorton with other fonts:
												
		You can even spot reappearing strange characters like a weirdly deep 3, or a flattened 4:
	
    I wish I understood how they came to be, but I have a hunch. The nature of pantographic reproduction is that Gorton carved into metal is not that far away from the original Gorton font template you started with! So in addition to the George Gorton and Taylor Hobson originals, and the other named and above-the-table copies, they might have been bigger or smaller Gorton . I have one myself, carved into acrylic, of unknown provenance and even more nameless than I thought possible for an already name-free font.
  
		But New York Gorton holds pleasant surprises, too. Despite the simplicity of Gorton itself, the combinations of font sizes, cutter sizes, materials, reproductions, and applications can still yield some striking effects:
	
	
			All my Gorton walks in Manhattan in 2024
		

		This was what made me walk 100 miles. Over and over again, Gorton found ways to make itself interesting. Without hyperbole, I consider the above photos simply beautiful.
	
		In a city that never sleeps, Gorton wasn’t allowed to sleep, either. Even in the richest and most glamorous neighborhoods of Manhattan, the font would be there, doing the devil’s work without complaining. Gorton made Gotham feel bougie; American Typewriter touristy.
	
		And once in a while, I’d find Gorton that would wink at me with a story – followed by that aching in the heart as I realized I’d never know what the story was.
				
		You’re not supposed to fall in love with an ugly font. No one collects specimens of Arial. No one gets into eBay fights for artifacts set in Papyrus. No one walks a hundred miles in a hot New York summer, sweating beyond imagination, getting shouted at by security guys, to capture photos of Comic Sans.
								
		So why do I love Gorton so much? 
								
		The Occam’s Razor seems sharp on this one. Perhaps I like it because I’m a boy and Gorton is often attached to heavy machinery. 
					
		But there must be more to it. Perhaps it’s all about the strange contrasts Gorton represents. The font is so ubiquitous, but also profoundly unrecognizable, sporting no designer and no name. Gorton is a decidedly toy-like, amateurish font deployed to for some of the most challenging type jobs: nuclear reactors, power plants, spacecraft. More than most other fonts, Gorton feels it’s been made by machines for machines – but in its use, it’s also the font that allows you to see so many human mistakes and imperfections.
					
		Gorton also feels mistake-friendly. The strange limitations of Gorton mean that some of the transgressions of other fonts don’t apply here. The monoline nature of the font means that messing with the size of Gorton is okay: Shrinking the font for small caps or superscript, for example, gives you still-valid letterforms, almost by accident. 
	
		Stretching or slanting Gorton is not as much a typographical crime as it would be with other fonts because you don’t stretch the tip of the router itself.
								
		There are genuinely moments where I felt Gorton gave people freedoms to maul it decades before variable fonts allowed us similar flexibiity.
		And on top of that, the simplicity of the letterforms themselves feels compatible with the typical naïveté of Gorton’s typesetting. 
	Various accessories and attachments allowing you to shift Gorton around in a way other fonts would not allow
    Sure, there are really bad renditions that are inexcusable. 

		But most of the time, the imperfections and bad decisions are what makes Gorton come alive. They don’t feel like a profound misunderstandings of typography, typesetting, or Gorton itself. They don’t feel like abuses or aberrations. No, they feel exactly how Gorton was supposed to be used – haphazardly, without much care, to solve a problem and walk away. (Later routing fonts copied Helvetica, but seeing Helvetica in this context with all the same mistakes grates so much more.)
				
		The transgressions are not really transgressions. They all feel honest. The font and its siblings just show up to work without pretense, without ego, without even sporting a nametag. Gorton isn’t meant to be admired, celebrated, treasured. It’s meant to do some hard work and never take credit for it. Gorton feels like it was always a font, and never a typeface. (Depending on how rigid you are with your definitions, some versions of Gorton – especially those without instructions on how letters are positioned against each other – might not even classify as a font!)
						
		And I think I love Gorton because over the years I grew a little tired of the ultra flat displays rendering miniature pixels with immaculate precision. 	
		With Gorton, carving into metal or plastic means good-looking fixes are impossible:
	
		And unsurprising given its roots, Gorton has dimensionality that most fonts cannot ever enjoy: A routing tip picked in the 1980s and a sun coming in from just the right angle forty years later can create a moment that thousands of letterpress cards can only dream of.
	
		Perhaps above everything else, Gorton is all about . 
  
    Every kind of engraving has it, of course. But these are not precise submillimeter letters at the bottom of your MacBook Pro or Apple Watch. This is the utilitarian, often harried, sometimes downright  Gorton, carved into steel of a  
		mid-century intercom and filled in with special paste or wax, or put on an office placard made out of a special two-layer material made especially so engraving it reveals the second color underneath, without the need for infill. 
				
		(This is also true when it comes to the original reason I learned of Gorton. Letters on keycaps show the same artifacts – you just have to look very, very closely.)
	
		That’s the last, and perhaps the best thing to fall in love with. 
	
		You won’t be able to fully appreciate it here, of course, but maybe this will give an approximation of how beautiful Gorton’s non-beauty can be:
						
		This has been a strange thing to write. Gorton has been around for over 135 years and used in so many countries for so many reasons, and yet I found no single article about it. 
						
		I feel the burden of being an amateur historian, wanting to know and share so much more, but only being able to provide little. I don’t know the full extent of Gorton’s use. I don’t know who designed it. My chronology is rickety and pieced together from a few breadcrumbs. I dream of seeing the original drawings or drafts once laid on the tables of Taylor, Taylor & Hobson offices, or some notes, or some correspondence. I fear they might no longer exist.
						
		Also, if part of the allure of Gorton is shying away from the limelight and not being admired, am I doing it a disservice by writing about it?
						
		But mostly, I can’t shake the feeling that we all missed a window. That this essay can’t be just a celebration, but also needs to be the beginnings of a eulogy.
						
		Walking around New York, you get a sense that even Gorton carved into metal can disappear. Some of the signs are rusted or destroyed beyond repair. Others get replaced by more modern, charmless equivalents.
								
		Gorton itself is obsolete. All the keyboards that use Gorton Modified you can still buy new today are tipping a hat to nostalgia. The omnipresence of Gorton in New York City is already time shifted from its decades of glory, a simple confirmation of what Robert Moses knew so well: that once built, cities don’t change all that much. But few of the new placards use Gorton, and none of the new intercoms do. 
	
		Taylor, Taylor & Hobson went through multiple splits and mergers and survives as a subsidiary of Ametek, chiefly working on measuring devices. George Gorton Machine Co. from Racine has been bought by Kearny & Trecker, which became Cross & Trecker, was acquired by Giddings & Lewis, and then acquired  by ThyssenKrupp, but not before the Gorton branch was spun off as Lars, and in a sequence of events now resembling a telenovella, eventually bought by Famco in 1987. I do not believe any corporate grandchildren of TT&H and George Gorton’s company are today selling Gorton in any capacity.
								
		It will take decades, perhaps even centuries, but one day the last of this font will be gone. The modern recreations (I eventually found quite a few) won’t help. They are perhaps all missing a point, anyway.
	
		But there’s a somewhat silver lining. Yes, when Gorton is carved into fresh metal, there might be nothing more pretty than seeing its depths glistening in the sun.
	
		But fresh, shining metal is at this point rare. Fortunately, the Gorton I love most is the weathered Gorton.
								
		Manhattan’s tired Gorton is the best variant of Gorton: infill cracked by hot summers followed by frigid winters, the surface scratched by keys or worn out by many finger presses, the routing snafus meeting decades of wear and tear. Gorton’s no stranger to water, snow, rust, or dirt.
			
		This is, perhaps, how you become gortonpilled. You learn to recognize the 7 with a crooked hook, the Q with a swung dash, the strange top-heavy 3, the simple R. You start noticing the endings of each character being consistently circular, rather than occasionally flat. A routing mistake, suspicious kerning, or the absence of lowercase are not a wrongdoing – they’re a .
								
		You find yourself enchanted with how this simple font went so very far. And then you touch the letters, just to be sure. If you can  them, chances are this is Gorton.		
	]]></content:encoded></item><item><title>OpenAI says its board of directors ‘unanimously’ rejects Elon Musk’s bid</title><link>https://techcrunch.com/2025/02/14/openai-says-its-board-of-directors-unanimously-rejects-musks-bid/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:45:07 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI’s board of directors has “unanimously” rejected billionaire Elon Musk’s offer to buy the nonprofit that effectively governs OpenAI, the company said on Friday. In a statement shared via OpenAI’s press account on X, Bret Taylor, board chair, called Musk’s bid “an attempt to disrupt his competition.” “OpenAI is not for sale, and the board […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Bluesky gets growth and analytics tools with BlueSkyHunter launch</title><link>https://techcrunch.com/2025/02/14/bluesky-gets-growth-and-analytics-tools-with-blueskyhunter-launch/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:42:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A new startup is addressing the need for an all-in-one toolset built for people who want to grow, manage, and track their Bluesky presence and following. The subscription service BlueSkyHunter, which launched Friday, introduces an online dashboard that combines access to analytics and other tools to schedule posts and automate DMs (direct messages), alongside other […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Global electricity demand expected to grow 4% annually through 2027</title><link>https://techcrunch.com/2025/02/14/global-electricity-demand-expected-to-grow-4-annually-through-2027/</link><author>Tim De Chant</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:40:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meeting that demand will require adding more generating capacity than all of Japan — every year. © 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>The Iconic 3DBenchy Enters the Public Domain</title><link>https://www.nti-group.com/home/information/news/3dbenchy/</link><author>kotaKat</author><category>hn</category><pubDate>Fri, 14 Feb 2025 21:39:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How this weekend’s ‘Tesla Takeover’ protests against Elon Musk came together on Bluesky</title><link>https://techcrunch.com/2025/02/14/how-this-weekends-tesla-takeover-protests-against-elon-musk-came-together-on-bluesky/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:34:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As Elon Musk and his acolytes rip through the federal government looking for agencies to throw into the “wood chipper,” a grassroots effort to hit the world’s richest man where it hurts is picking up steam. The courts are busy contesting the actions of Musk’s Department of Government Efficiency, but the judicial system is slow […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.</title><link>https://v.redd.it/sglstazd96je1</link><author>/u/eternviking</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 21:24:03 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Meta To Build World&apos;s Longest Undersea Cable</title><link>https://tech.slashdot.org/story/25/02/14/1852233/meta-to-build-worlds-longest-undersea-cable?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 21:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Meta unveiled on Friday Project Waterworth, a 50,000-kilometer subsea cable network that will be the world's longest such system. The multi-billion dollar project will connect the U.S., Brazil, India, South Africa, and other key regions. The system utilizes 24 fiber pairs and introduces what Meta describes as "first-of-its-kind routing" that maximizes cable placement in deep water at depths up to 7,000 meters. 

The company developed new burial techniques for high-risk areas near coasts to protect against ship anchors and other hazards. A joint statement from President Trump and Prime Minister Modi confirmed India's role in maintaining and financing portions of the undersea cables in the Indian Ocean using "trusted vendors." According to telecom analysts Telegeography, Meta currently has ownership stakes in 16 subsea networks, including the 2Africa cable system that encircles the African continent. This new project would be Meta's first wholly owned global cable system.]]></content:encoded></item><item><title>What is device code phishing, and why are Russian spies so successful at it?</title><link>https://arstechnica.com/information-technology/2025/02/russian-spies-use-device-code-phishing-to-hijack-microsoft-accounts/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/03/phishing.jpeg" length="" type=""/><pubDate>Fri, 14 Feb 2025 21:16:11 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Researchers have uncovered a sustained and ongoing campaign by Russian spies that uses a clever phishing technique to hijack Microsoft 365 accounts belonging to a wide range of targets, researchers warned.The technique is known as device code phishing. It exploits “device code flow,” a form of authentication formalized in the industry-wide OAuth standard. Authentication through device code flow is designed for logging printers, smart TVs, and similar devices into accounts. These devices typically don’t support browsers, making it difficult to sign in using more standard forms of authentication, such as entering user names, passwords, and two-factor mechanisms.Rather than authenticating the user directly, the input-constrained device displays an alphabetic or alphanumeric device code along with a link associated with the user account. The user opens the link on a computer or other device that’s easier to sign in with and enters the code. The remote server then sends a token to the input-constrained device that logs it into the account.]]></content:encoded></item><item><title>GitHub - Clivern/Peanut: 🐺 Deploy Databases and Services Easily for Development and Testing Pipelines.</title><link>https://github.com/Clivern/Peanut</link><author>/u/Clivern</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 21:07:48 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DeepSeek founder Liang Wenfeng is reportedly set to meet with China’s Xi Jinping</title><link>https://techcrunch.com/2025/02/14/deepseek-founder-liang-wenfeng-is-reportedly-set-to-meet-with-chinas-xi-jinping/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:45:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Chinese AI startup DeepSeek founder Liang Wenfeng is reportedly set to meet with China’s top politicians, including Chinese leader Xi Jinping, during a summit that Alibaba founder Jack Ma is also expected to attend. The summit, which could happen as soon as next week, may be intended as a signal by China’s Communist Party that […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>The Whole World Is Going To Use a Lot More Electricity, IEA Says</title><link>https://hardware.slashdot.org/story/25/02/14/1742226/the-whole-world-is-going-to-use-a-lot-more-electricity-iea-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Electricity demand is set to increase sharply in the coming years as people around the world use more power to run air conditioners, industry and a growing fleet of data centers. From a report: Over the next three years, global electricity consumption is set to rise by an "unprecedented" 3,500 terawatt hours, according to a report by the International Energy Agency. That's an addition each year of more than Japan's annual electricity consumption. 

The roughly 4% annual growth in that period is the fastest such rate in years, underscoring the growing importance of electricity to the world's overall energy needs. "The acceleration of global electricity demand highlights the significant changes taking place in energy systems around the world and the approach of a new Age of Electricity," Keisuke Sadamori, IEA's director of energy markets and security, said in a statement. "But it also presents evolving challenges for governments in ensuring secure, affordable and sustainable electricity supply."]]></content:encoded></item><item><title>Why PAUL Needs a Massive Dataset to Improve Its Movements</title><link>https://hackernoon.com/why-paul-needs-a-massive-dataset-to-improve-its-movements?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:30:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.3 PAUL: Design and Manufacturing4 Data Acquisition and Open-Loop Control4.3 Dataset Generation: Table-Based ModelsDue to the complexity of the robot, model-based methodologies, such as PCC or the ones based on Cosserat Rod Theory were discarded. Although the usage of FEM is an avenue that will not be closed in future work, the large number of parameters to be set experimentally for each segment (Young’s modulus, moment of inertia. . . ), given that the manufacturing process is so variable meant that, in this first phase, we opted to use some type of PAUL modelling based on data collection.\
The output of the system is taken as the position and orientation reached by the final end –thus, at this stage, all the positions of the intermediate segments are ignored– and as input, the inflation times of each of the bladders. As there were not enough pressure sensors available at the time of the construction of the robot, it was decided to take inflation time as an input variable. As the working pressure is limited by the pressure limiting valve and the flow rate into each bladder can be assumed to be constant, the time is equivalent to the volume of air introduced into each cavity.\
All the control options considered have in common the need for a large amount of empirical data, which leads to the need to develop an experimental design to systematise the collection of this data. Since the capture of this information is done in different phases and the datasets have to represent the behaviour of the robot in an objective way, the re-applicability of the experiment takes on special importance.\
The data stored in the datasets was the position of the robot tip and the set of inflation times that achieve this configuration. The aforementioned limitation that only two of the three bladders in the segment are inflated reduces redundancies. As previously stated, more than two segments lead to redundancies, which implies that the inverse kinematic model of the robot can have multiple solutions.\
The data collection process involves several sequential steps. Initially, a set number of samples is determined. For each sample, Matlab commands dispatch a random combination of nine inflation times, corresponding to each valve of PAUL, to the actuation bench. Times are generated below a maximum time limit Tmax, and ensuring that only two cavities per segment are inflated. Following this, the robot’s bladders are inflated based on the sent times. Subsequently, the vision system’s two cameras capture images to determine the position and orientation of the robot’s end. This entire procedure is repeated for the specified number of iterations, and upon completion, the collected data is stored in the dataset\
The information on swelling times is stored as a percentage, with a value of 0 corresponding to a zero swelling of that segment and 100 corresponding to Tmax, the swelling for the maximum number of milliseconds defined for this data collection session. This value Tmax is stored, together with the values, in the dataset, in order to be able to compare different datasets. The reason for this coding comes from the lack of information, a priori, on what is the maximum pressure that a PAUL bladder admits. Although it is true that it was experimentally determined that inflation times of more than 1500 ms in a row led to punctures, the application of lower times during a repeated number of cycles also generated leaks. On this basis, it was decided never to inflate any valve, either in one or several steps, more than 1000 ms.\
Along with the inflation times of each bladder, the position and orientation reached by the end tip is stored, based on the camera readings. In particular, the position of the green marker and the orientation of the trihedron are stored. The latter is expressed in Euler angles, as it is a much efficient form of storage than a rotation matrix. In addition, the dataset also contains metadata from the collection process that are believed to influence the results, such as the pneumatic line pressure or the ambient temperature.\
Some aspects in the pneumatic system merit attention. Initially, bladder inflation and deflation are not symmetrical processes. Geometric constraints in the pneumatic components result in a lower deflation rate compared to inflation. Consequently, when the PAUL receives a deflation time, it multiplies it by an empirically derived factor, approximately 1.45 for a 1.2 bar working pressure. This multiplier compensates for the discrepancy between inflation and deflation times of a singular group of bladders, ensuring that the deflation time aligns with the time required to reach the same inflation point.\
Similarly, although it is physically possible to inflate several valves at the same time, it has been shown that this parallel flow distribution means that the effective fillings of each valve are not the same as if they were inflated individually. To prevent this phenomenon, it was decided to inflate each bladder individually both during the data acquisition process and utterly, when PAUL was asked to reach certain positions.\
Finally, there are hysteresis phenomena in the silicone that cause the position reached by inflating for a time t to be different from the position reached by inflating first for a time t1 and then for a time t2 = t − t1. The strategy employed to tackle this problem was to capture the dataset bringing PAUL back to its zero position between each sample. Nevertheless, when controlling the robot in open-loop this is not possible, or, at least, not desirable, as one may wish to follow trajectories or travel through a sequence of points. Therefore, transitioning from position x1 to x2 requires an additional factor of 1.2, also derived experimentally, to account for hysteresis effects.Once the dataset is generated, it can be used to model the behaviour of PAUL for open-loop control. It is foreseen, as a future line, to train a neural network for the direct kinematics and another one for the inverse kinematics. However, given the large amount of data that may be required (in [62] 24389 samples are used for a three-segment robot like this one), a table look-up method has been used for this work.\
The method for direct kinematics –which allows obtaining the position and orientation of the final end of the robot from the inflation times of the nine bladders– consists of searching, in the generated dataset in the previous step, the three inflation time values located at a shorter distance from the inflation time given as a reference. Obviously, if the set of inflation times sought were in the table, the value associated with these times would be returned as a result of the direct kinematic model. Otherwise, the average of the position and orientation values associated with the three closest inflation times, weighted by the distance (Euclidean norm) existing between each of them and the values of reference inflation times, is returned as the position and orientation value of the robot.\
\
with them, it is possible to calculate the position returned by the direct kinematic model using the expression:]]></content:encoded></item><item><title>NATO backs its first cohort of European dual-use startups</title><link>https://techcrunch.com/2025/02/14/nato-backs-its-first-cohort-of-european-dual-use-startups/</link><author>Mike Butcher</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:16:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[With both Vice President J.D. Vance and U.S. Defense Secretary Pete Hegseth making loud noises Friday about Europe stepping up to the plate in spending more on its own defense, it might come as a surprise that Europe is already on the path toward far greater investment in defense, especially in tech. Not only has […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>How PAUL the Robot Tracks Its Own Movements Using Cameras and LEDs</title><link>https://hackernoon.com/how-paul-the-robot-tracks-its-own-movements-using-cameras-and-leds?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:15:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.3 PAUL: Design and Manufacturing4 Data Acquisition and Open-Loop Control4 Data Acquisition and Open-Loop ControlIn order to provide the manipulator with a solid and stable fastening system, which would also allow reliable and predictable data capture of the positions and orientations of its end, the metal structure shown in Figure 10 was built. It is a cube made of steel profiles with methacrylate sheets on the walls. The pneumatic bench, the power supply and the microcontroller were placed on top of the structure.\
The aim of the data acquisition system is to be able to measure, whenever required, the position and orientation of the end of the robot in order to be able to relate it to the inflation times of each bladder and thus be able to create an open-loop model of PAUL. For this purpose, three elements are available: the cameras, the calibration grid and the trihedron.\
Two Spedal AF926H USB cameras with 1920 x 1080 px, 80◦ field of view and a frequency of 60 fps are used to capture the images. These have been placed on two tripods external to the robot’s structure. They are calibrated with a checkerboard of 11 x 8 squares of 20 mm each, which can be seen in Figure 11a.\
The vision beacon, on the other hand, has the task of being recognised in space to determine the position and orientation of the mobile system with respect to the fixed system. The trihedron, displayed in Figure 11b, consists of three spheres, manufactured by 3D printing in PLA, inside which three LED diodes have been embedded. Thanks to these, it is possible to vary the luminosity of the spheres by means of software, keeping the system functioning correctly when the workplace or the environmental or lighting conditions vary.\
The existence of the central rod, which moves the luminous spheres away from the base of the robot end, makes possible the spheres to be visible to the cameras in all the poses that the robot can adopt. If the spheres were otherwise directly attached to the\
end of the robot, there would be numerous poses in which it would not be possible to determine the position, as the spheres would be hidden by the robot itself.4.2 Vision Capture System\
\
Because coordinates of the real world are independent of the camera, if Equation (1) is applied for both cameras and rk vector cleared in the two equations, it can be said that:\
\
System of equations (2) can be solved using the Least Squares Method:\
\
and then use the Rodrigues’ rotation formula to obtain it, respect to the real world base in the form of a rotation matrix:\
\
and I denotes the identity matrix of size 3.]]></content:encoded></item><item><title>Drugs Have Won The War On Drugs: Drugged-Up Rat Infestation Edition</title><link>https://www.techdirt.com/2025/02/14/drugs-have-won-the-war-on-drugs-drugged-up-rat-infestation-edition/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:07:39 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[50+ years of hardline prohibition have only resulted in better prices, better purity, and a slew of states legalizing or decriminalizing personal use amounts of any number of drugs, with marijuana leading the way in terms of overall legalization. Treating drugs users as just as terrible as drug dealers has led to a ton of incarceration and a  ton of collected evidence that is apparently being consumed by things cops and crooks are used to dealing with: rats.“Drug-addicted rats” are eating narcotics seized and stored by Houston police, prompting a change in how long the police department is required to store the evidence, officials said.Houston Mayor John Whitmire, Harris County District Attorney Sean Teare and Houston Police Chief J. Noe Diaz announced new steps Friday to dispose of drugs and other evidence kept at police headquarters downtown, some of which has been sitting there for decades, attracting rodents, even though cases they are linked to have long been adjudicated.This is where it becomes clear that drugs have won the Drug War. This single evidence locker contains enough weed to start a drug empire. “We got 400,000 pounds of marijuana in storage,” Whitmire said. “The rats are the only ones enjoying it.”The Houston PD (which has more than its share of corrupt drug cops) is sitting on 200  of marijuana it has amassed over the years. And yet, I would wager that no Houston resident has any trouble obtaining this product on short notice.The same goes for the rest of the stuff in the warehouse, which apparently (until recently) contained cocaine seized in 1996. As police chief J. Noe Diaz pointed out, the evidence has outlasted the case. The suspect pled guilty, served his time, and was free to go long before the evidence used to convict him was.It’s apparently a nationwide problem, according to some of the forensic experts interviewed by NBC News, which carried a story early last year about a similar drug-eating rat problem in New Orleans. New rules are being put in place to destroy this evidence more frequently. On one hand, it makes sense to destroy evidence after defendants serve their time in jail. On the other hand, I wouldn’t get  carried away giving PDs permission to destroy evidence, since that’s the sort of thing that lends itself to cover-ups and the disappearance of evidence prisoners might use to challenge their convictions — a process that can take years, thanks to the justice system’s reluctance to reconsider its own calls and the byzantine processes convicted people are expected to navigate just to have (a very looooong) shot at having their cases heard. But above all that, there’s the sheer amount of drugs being held in evidence warehouses. If there’s nearly a half-million pounds of weed just laying around at any given time, it would seem law enforcement isn’t scoring many meaningful wins in the Drug War. At best, it’s a game of Whack-a-mole that won’t generate enough tickets to make it worth visiting the merchandise booth at the arcade. At worst, it’s just cops looking busy, a meaningless waste of time that unfortunately results in people losing years of their lives to a system that not only can’t fix what’s broken, but clearly prefers doing the things that don’t work as often as possible in perpetuity. ]]></content:encoded></item><item><title>Western Digital Aims For 100TB Hard Drives by 2030</title><link>https://hardware.slashdot.org/story/25/02/14/1817233/western-digital-aims-for-100tb-hard-drives-by-2030?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 14 Feb 2025 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Western Digital plans to introduce its first heat-assisted magnetic recording (HAMR) drives in late 2026, with 36TB conventional magnetic recording (CMR) and 44TB shingled UltraSMR variants. Volume production won't begin until the first half of 2027, following qualification by cloud data center providers in late 2026. 

The company projects that HAMR technology, combined with OptiNAND, increased platter count, and mechanical improvements, will enable drives reaching 80TB CMR and 100TB UltraSMR capacities around 2030 -- a departure from Western Digital's previous commitment to microwave-assisted magnetic recording (MAMR) in 2017, which evolved into the energy-assisted perpendicular magnetic recording (ePMR) technology used in current drives.]]></content:encoded></item><item><title>Elon Musk’s AI company, xAI, said to be in talks to raise $10B</title><link>https://techcrunch.com/2025/02/14/elon-musks-ai-company-xai-said-to-be-in-talks-to-raise-10b/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 19:54:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Elon Musk’s AI company, xAI, is said to be in talks to raise $10 billion in a round that would value xAI at $75 billion. Bloomberg reported Friday that xAI is canvassing existing investors, including Sequoia Capital, Andreessen Horowitz, and Valor Equity Partners for the round, which would bring xAI’s total raised to $22.4 billion, […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Shutdown Go server</title><link>https://www.reddit.com/r/golang/comments/1ipj5zn/shutdown_go_server/</link><author>/u/Kennedy-Vanilla</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 19:46:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, recently I saw that many people shutdown their servers like this or similarserverCtx, serverStopCtx serverCtx, serverStopCtx := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { <-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { <-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal("graceful shutdown timed out.. forcing exit.") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf("error shutting down server: %v", err) } serverStopCtx() }() log.Printf("Server starting on port %s...\n", port) err = server.ListenAndServe() if err != nil && err != http.ErrServerClosed { log.Printf("error starting server: %v", err) os.Exit(1) } <-serverCtx.Done() log.Println("Server stopped") } := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { <-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { <-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal("graceful shutdown timed out.. forcing exit.") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf("error shutting down server: %v", err) } serverStopCtx() }() log.Printf("Server starting on port %s...\n", port) err = server.ListenAndServe() if err != nil && err != http.ErrServerClosed { log.Printf("error starting server: %v", err) os.Exit(1) } <-serverCtx.Done() log.Println("Server stopped") Is it necessary? Like it's so many code for the simple operation   submitted by    /u/Kennedy-Vanilla ]]></content:encoded></item><item><title>Discovering discovery coding (Friends)</title><link>https://changelog.com/friends/80</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/friends/80/changelog--friends-80.mp3" length="" type=""/><pubDate>Fri, 14 Feb 2025 19:45:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Fire up a REPL, grab your favorite Stephen King novel, and hold on to the seat of your pants! Jimmy Miller returns to reveal why, at least for some of us, discovery coding is where it’s at.Changelog++ members save 9 minutes on this episode because they made the ads disappear. Join today!Temporal – Build invincible applications. Manage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail. Register for Replay in London, March 3-5 to break free from the status quo.
Augment Code – Developer AI that uses deep understanding of your large codebase and how you build software to deliver personalized code suggestions and insights. Augment provides relevant, contextualized code right in your IDE or Slack. It transforms scattered knowledge into code or answers, eliminating time spent searching docs or interrupting teammates.
Notion – Notion is a place where any team can write, plan, organize, and rediscover the joy of play. It’s a workspace designed not just for making progress, but getting inspired. Notion is for everyone — whether you’re a Fortune 500 company or freelance designer, starting a new startup or a student juggling classes and clubs.
]]></content:encoded></item><item><title>the ref keyword</title><link>https://www.reddit.com/r/rust/comments/1ipixny/the_ref_keyword/</link><author>/u/Tickstart</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 19:36:57 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've made a quick mock situation which is analogous to my situation the other day:fn main() { let mut v: Option<Vec<usize>> = None; let mut h = 20; while h.ne(&0) { if (h % 3).ge(&1) { match v { Some(ref mut v) => (*v).push(h), None => v = Some(vec![h]) } } h -= 1 } println!("{v:?}") } I was a bit confused on how it "should" be solved. My issue is the "ref mut". It made sense to me that I didn't want to consume the vector v, just add to it if it existed and I tried adding ref (then mut), which worked. When I goodled, it seemed ref was a legacy thing and not needed anymore. My question is, how is the idiomatic way to write this? Perhaps it's possible to do in a much simpler way and I just found a way to complicate it for no reason.Also, don't worry I know this is a terrible pattern, it was mostly for tesing something.]]></content:encoded></item><item><title>Researcher Captures Contents of ‘DEI.gov’ Before It Was Hidden Behind a Password</title><link>https://www.404media.co/dei-waste-gov-doge-list-behind-password/</link><author>Samantha Cole</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/Screenshot-2025-02-14-at-2.25.54-PM.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 19:32:43 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[A German researcher captured the contents of the White House’s “DEI.gov” during a brief period when it was not password protected.The capture shows that the site contains a list of vague, alleged government-funded tasks and their costs, without sources or context, like “$1.3 million to Arab and Jewish photographers," “$1.5 million for ‘art for inclusion of people with disabilities,’” and "$3.4 million for Malaysian drug-fueled gay sex app.” DEI.gov redirects to waste.gov and is currently inaccessible without a password; Elon Musk told reporters on Tuesday that his Department of Government Efficiency (DOGE) is “trying to be as transparent as possible.”⁨The researcher is Henrik Schönemann⁩, a historian who started the Safeguarding Research & Culture archivalist project, posted screenshots on Mastodon showing the contents. Schönemann⁩ also shared the specific site scrapes that he was able to capture, which showed the contents of the site. He told 404 Media he set up a change detection app using PikaPods, and is monitoring changes across hundreds of government websites. When the dei.gov and waste.gov sites were registered 10 days ago, he started tracking them, too. Before the site administrators added a Wordpress template to the pages, the list was online at those URLs. This list was only online for a maximum of 30 minutes, starting around 4:50 p.m. EST; by 5:23 p.m. on February 11, it was gone from public view, according to the snapshots Schönemann’s app⁩ captured. According to the screenshots provided by Schönemann⁩, the list includes (all of the following are direct quotes): $78,000 to Palestinian activist group whose chairman was photographed attending an anniversary event celebrating the founding of the Popular Front for the Liberation of the Palestine terrorist group$1 Million for foreign DEI programs, including ‘indigenous language technology’ in Guatemala, per non-public funding docs reviewed by WFB$5 million for effort to treat eating disorders by “affirming” LGBTQIA+ patients’ sexual orientation and gender claimsUp to $3 million to defund the police advocacy group to pursue “climate justice” for convictsFunded performances of play “Angels in America: A Gay Fantasia on National Themes,” in which God is bisexual and communists are good, in North MacedoniaDisbursed $15,000 to “queer” Muslim writers in IndiaShelled out tens of thousands to create army of 2,500 LGBTQI+ alliesUp to $10 million worth of USAID-funded meals went to al Qaeda-linked terrorist group the Nusra Front$500,000 to group that “empowers women” in attempt to solve sectarian violence in Israel just ten days before Hamas’ Oct. 7 attacks$4.67 million to EcoHealth Alliance – one of the key NGOs funding bat virus research at Wuhan Institute of Virology — in late 2021. Later refused to answer key questions about the funding.$7.9 million to a project that would teach Sri Lankan journalists to avoid “binary-gendered language”$1.3 million to Arab and Jewish photographers$1.5 million for “art for inclusion of people with disabilities”$2 million to promote “LGBT equality through entrepreneurship…in developing Latin American countries.”Education Week: “Biden Administration Cites 1619 Project as Inspiration in History Grant Proposal”VA took at least a dozen actions aimed at bolstering DEI during the Biden-Harris administration while the number of homeless veterans increased and the amount of claims in the VA’s backlog grew from ~211,000 to ~378,000NASA has allocated roughly $10 million to grants advancing DEI and “environmental justice” since 2020Following President Trump’s executive order on DEI at federal agencies, the ATF “quietly changing the job title of its former diversity officer… to ‘senior executive’ with the ATF.The Department of Labor requested additional funding in 2023 for “The Chief Evaluation Office for a new rigorous interagency evaluation of actions aimed at improving Diversity, Equity, Inclusivity, and Accessibility across the federal workforce,” more than $6.5 million “to restore employee benefits programs that will advance equity by specifically addressing how opportunities can be expanded for underserved communities and vulnerable populations,” and $5 million “to evaluate actions aimed at improving diversity, equity, inclusion, and accessibility (DEIA) within the federal workforce.”Fox Business: “FOX Business’ ‘Trouble in the Skies,’ a six month investigation of the FAA’s new hiring practices, uncovered changes that may put the nation’s flying public at risk as well as allegations that the newest air traffic control recruits had access to answers on a key test that helped them gain jobs with the FAA…Also uncovered was an FAA effort to promote diversity that discarded 3000 qualified college graduates with degrees in air traffic control despite their following FAA procedure and obtaining FAA accredited degrees.”Schönemann⁩ told 404 Media he wanted to share a sentiment alongside his find: “People all around the world care, you are not alone. And: #TransRights.” Earlier this week, we reported that the Trump administration had set up a website called waste.gov, which was live on the internet with a sample page from a default WordPress template. Both DEI.gov and waste.gov were created at the same time, according to Reuters, and DEI.gov was recently set up to redirect to waste.gov. After our reporting, both websites were put behind a password wall.]]></content:encoded></item><item><title>Meta’s next big bet may be humanoid robotics</title><link>https://techcrunch.com/2025/02/14/metas-next-big-bet-may-be-humanoid-robotics/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 19:16:11 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta is forming a new team within its Reality Labs hardware division to build robots that can assist with physical tasks, Bloomberg reported. The team will be responsible for developing humanoid robotics hardware, potentially including hardware that can perform household chores. Meta’s new robotics group, which will be led by Marc Whitten, driverless car startup […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>How PAUL, a Soft Robot, is Designed and Built</title><link>https://hackernoon.com/how-paul-a-soft-robot-is-designed-and-built?source=rss</link><author>EScholar: Electronic Academic Papers for Scholars</author><category>tech</category><pubDate>Fri, 14 Feb 2025 19:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.3 PAUL: Design and Manufacturing4 Data Acquisition and Open-Loop ControlThe first step in the manufacturing process is to obtain the wax cores which, when inserted into the mould, are used to create the holes for what, in the finished segment, will be the bladders. These are made by pouring paraffin wax into previously made female moulds (Figure 5a).\
After half an hour, the wax has solidified and the cores can be removed and inserted into the mould (Figure 5b). The mould consists of four 3D printed parts (two sides, a bottom cap and a top grip on which the cores rest) which are screwed together and then sealed with a hot silicone bead to prevent leakage during subsequent curing (Figure 5c).\
The silicone can then be poured into the mould, which must be filled to the top to counteract the aforementioned shrinkage. In particular, TinSil8015 requires a mass ratio of 10:1 liquid to catalyst. For the dimensions of the segment, about 175 g of total mixture are required.\
The curing process lasts 24 hours at ambient temperature (Figure 5d), after which it can be removed from the mould. It may be necessary to use a scalpel to remove the silicone burrs (Figure 5e).\
Once the segment has been built, the cores that have been used to create the bladders are removed. While the wood can be removed by pulling, it is necessary to apply heat to the segment to remove the wax. Thus, it is first placed in an oven at 110 ◦C (Figure 5f) and then immersed in a boiling water bath for 15 minutes, which ensures the elimination of the residual traces of wax (Figure 5g).\
Since the males are through, it is required to close the lower part of the segment. To do this, a layer of silicone is poured onto the plate of Figure 5h, glued onto the segment and left to cure for 24 hours. Finally, the pneumatic tubes are joined to the segment, adhering them with cyanoacrylate and strengthening the tightness with the usage of plastic flanges (Figure 5i).\
The final result, a functional segment is depicted in Figure 6. Experimentally, it is found that its weight is 161 g and that, as designed, it has a height of 100 mm and an external diameter of 45 mm.Within the robot, the function of the pneumatic bench is to control the flow of compressed air from the compressor according to the control signals. Specifically, the PAUL bench consists of 6 pairs of 2/2 valves (SMC VDW20BZ1D model) and 3/2 valves (SMC Y100 model) placed in series, which will therefore allow up to 12 degrees of freedom. Both are shown in Figure 7. The physical characteristics of the 2/2 valves limited the total pressure of the assembly to 4 bar, but to reduce the risk of segment leakage, it was reduced with a flow regulator to 2 bar. Figure 8 presents a schematic of the pneumatic circuit.\
\
The valves are operated via 24 V voltage signals. A MOSFET (model IRF540) is the switch in charge of managing them. Initially, the use of relays was considered, but the high current they would consume made their use unfeasible. An Arduino Due was chosen as the bench controller. A PC power supply, capable of supplying up to 8.5 A, is responsible for powering the unit, whose final layout is illustrated in Figure 9.]]></content:encoded></item><item><title>No Startup Has Ever Failed Because it Didn’t Have a Blog</title><link>https://hackernoon.com/no-startup-has-ever-failed-because-it-didnt-have-a-blog?source=rss</link><author>susie liu</author><category>tech</category><pubDate>Fri, 14 Feb 2025 19:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Somewhere along the way, startup marketing got hijacked by creator economy logic.“If I build an audience, I’ll have an easier time selling.” “If I post valuable content, customers will come to me.” “Content is how you earn trust.”No startup has ever failed because it didn’t have a blog. Name one high-growth startup that failed because it didn’t have a content marketing strategy. \
  . Because it’s never happened. \
Startups fail because: 1) They didn’t find product-market fit. 2) They spent money like fools. 3) A combo of shaky financials plus bad operations. Not because they skipped the “10 SaaS Growth Hacks” Medium post.The Case Against Content Marketing\
Content isn’t just eating up your time—it’s going to eat your startup for lunch. You think you’re building something, but really, you’re just shuffling pixels around and calling it momentum. ==You’ll die because you’re wasting your best thinking on being interesting instead of being necessary.==\
Don’t try to be a media company when you haven’t even figured out how to be a business.You Ignore Your Most Powerful Asset: Your Product\
The best startups don’t need a perfect LinkedIn strategy. They need a product so good that people can’t shut up about it.Tesla didn’t become Tesla because Musk wrote aspirational posts. (Tesla happened, then people started dissecting every Elon tweet.) Superhuman didn’t win because they ranked for “best email productivity tool”.Notion didn’t explode because they had a “high-value” newsletter.\
They each built something so good that content wasn’t necessary.\
==But content marketing makes you spend more time talking about your product than making it actually worth talking about.== If you’re creating Canva graphics explaining why your product is amazing, ask yourself: Why isn’t it obvious already?You’re Playing To Your Weakness\
If you’re a tech founder, chances are your biggest strength isn’t crafting “valuable content.” But content marketing throws you into the most crowded, attention-starved battlefield on earth that’s populated by all the creative genius humanity has to offer: the Internet. \
Content marketing pulls you away from your strengths. It forces you into a slow, bloated, competitive game where you’re fighting against seasoned creators and media companies—people whose  is content. And you’re out here thinking you can compete?\
While you’re wasting time learning how to be a copywriter, you’re neglecting the strengths that gave you the guts to build a company in the first place. \
Content marketing is a wait-and-see game.Do this long enough, and you'll start convincing yourself that the reason you're not making progress is because you haven’t published enough. So, you rinse and repeat, cranking out more content—and waiting for something to happen. I promise you, nothing will. ==Unless you’re a media company, the real problem won’t be solved through content volume==. Look at the Airbnb guys. They tackled crappy check-in rates by bringing their cameras into homes and taking better photos, not by blogging about the future of holiday stays. \
Founders love content marketing because it’s a safe way to feel productive without doing the uncomfortable shit. Well, if you want to stay away from the uncomfortable, you should consider going back to your desk job. You’re Sprinting For The Wrong Finish Line\
==Content marketing subtly shifts the goalpost of what success looks like.== Instead of $$$ in the bank, founders start measuring social impressions. Instead of customers using the product, they’re tracking newsletter signups. Instead of winning market share, they’re focused on LinkedIn comments.\
 Awareness and engagement doesn’t pay the bills. Real growth is customers in, cash in. If you don’t know the difference between attention and action, content marketing is eating your brain. Are you really growing, or have you just gotten more people to consume free content?You’re Killing Your Brand\
Weird, right? You’d think viral content will help you build your brand. Nope, because going viral means you’ve got to jump on every buzzword, every new format, and every other trend because it’s what everyone’s talking about. Well, trends don’t care two cents about what your brand stands for. \
Every time you chase a trend, you’re stretching your brand thin, trying to force it into a mold that doesn’t fit. You’re watering down your purpose for a quick hit, all while ignoring the fact that ==building a memorable brand means sticking to your guns, even if it’s what the algorithm despises.==Inbound’s Still In. You Just Don’t Need Content To Do It.The content marketing trend is a byproduct of people misunderstanding HubSpot’s whole inbound marketing thing. \
  Inbound was and still is genius, but inbound marketing was never about “just create content.” It was about ==creating undeniable value before asking for the sale.== (That, and not relying on paid ads for awareness.) \
HubSpot made content work because information was scarce in 2006—they gave away what no one else would. Today, everyone’s got an “ultimate guide” to something. And if they don’t, GPT can make something up based on a prompt. The problem now isn’t lack of content. \
Here’s how to do inbound marketing in the post-content age.1. Figure Out If Content Even Makes Sense for You\
Before you waste another hour on a blog post, ask yourself:Is my audience actively looking for information, but struggling to find it? Is there a knowledge gap that makes them hesitate to buy? Are my competitors hoarding insights instead of sharing them?\
If the answer is yes, fine. Write. Publish. Own the space. If the answer is no, STOP. No one needs your “Top 10 Trends” list. They have Google and chatbots for that.\
Know what’s better than teaching people how to do something? ***Making sure they never have to learn it in the first place. ***\
You can write another 1,500-word thesis. Or, you can build something—a tool, a feature, an experience—that solves the same problem but without needing people to read a bunch of text. Look at the content that’s dominating your space—the posts that get all the likes, shares, and comments. Then ask yourself: How can I make this educational BS obsolete?Here’s a thought: Tell your audience exactly how much time and money you’re burning on content—and then tell them you’re ditching it to build something free that actually helps them. "We were going to spend $20K this quarter on content, but instead, we’re using it to build a free AI-powered email draft generator for sales teams. Tell us what you need, and we’ll make it happen." \
People will talk about it. They’ll share it, they’ll use it, and they’ll remember the one who actually did something instead of just saying something.3. Content Isn’t The Only Thing That Gets Organic Attention\
The other principle of inbound marketing?  Yes, content generates organic awareness and conversation. So do a lot of other things. . This is a last resort because stunts could backfire (but for a startup, any PR is good PR), but at least you’re exercising your creative brain cells. If you have to force people to see your content, why not just skip the middle step and force them to see your product instead? \
Stop badgering GPT for content ideas, and use that time to hijack attention—starting fights on X, sitting on the doorsteps of potential clients for weeks on end with a cardboard sign, anything that gets people talking about your company now, not in six months when Google decides to bless you with traffic.\
(Tip: When pulling a stunt, ==just make sure it barely costs you anything to execute.== That way you won’t hate yourself if you don’t get returns.) Final Thoughts: Why Are You Marketing Like a Coward?\
You built something from nothing. You ignored the safe path, told the doubters to shove it, and bet on yourself. You made a move.\
So where’d that nerve go?The second it came time to market, you swapped instinct for “best practices.” \
  ==Innovation doesn’t come from following rules, and attention won’t be earned through compliance.== \
  It’s taken. It’s ripped out of the market’s hands by people willing to do what no one else is doing.Treat marketing like you treat your product—or get used to being ignored.]]></content:encoded></item><item><title>Police Union Still Insists NY Misconduct Records Are Secret Despite Court Decisions Saying Otherwise</title><link>https://www.techdirt.com/2025/02/14/police-union-still-insists-ny-misconduct-records-are-secret-despite-court-decisions-saying-otherwise/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:55:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[When you’re playing with house money, playing one losing hand after another isn’t a sign of tenacity. It’s just a way of signaling you can’t be trusted with the house’s money.That’s why appeal after appeal from government entities don’t tend to indicate that they’re in the right. It just means they don’t care how much money they spend because it’s not coming from their own pockets. The same principle applies to police unions. The money they use to litigate comes from the officers they’re supposed to be serving. At some point, you’d think they would experience some fleeting shame against lighting their contributors’ money on fire repeatedly, but the sad fact is that most cops represented by New York’s Police Benevolent Association (PBA) are more than happy to keep burning their own money if it means there’s even the most remote chance their past misdeeds won’t come back to haunt them.There have been plenty of legal challenges against the repeal of 50-a, the shorthand that refers to the law that — up until June 2020 — gave law enforcement agencies the option to withhold misconduct records requested by members of the public.The Police Benevolent Association of the City of New York asked a seven-judge panel of the New York Court of appeals to apply retroactivity analysis on the question of whether or not state lawmakers intended a confidentiality provision enacted in 1976 to be conferred to police officers as a vested right after it was repealed in 2020.“We’re just stuck, at best, with a very ambiguous record,” Police Benevolent Association lawyer Matthew Daly told the appeals court on Thursday afternoon. “Policy arguments can be made on both sides — there are policy arguments for disclosure, but there’s also policy to protect rights.”Fighting off document requests from the New York Post, the union says the mandate to retroactively make those old records public would infringe vested rights of police officers and other covered employees “who for more than four decades relied on the statutory confidentiality in deciding how to respond to disciplinary matters.”PBA attorney Matthew Daly is, at best, being disingenuous here. The PBA is asking this court to basically codify something that isn’t present in the law: a limitation that prevents disclosure of records generated before the law’s repeal in 2020. As the court pointed out in late 2023, there’s nothing in the law that forbids access to records created prior to this repeal. By their nature, FOIL requests seek records that were generated prior to the request date. In amending the Public Officers Law to provide for the disclosure of records relating to law enforcement disciplinary proceedings, the Legislature did not limit disclosure under FOIL to records generated after June 12, 2020, and we will not impose such a limitation ourselves. Yet, the PBA persists, guided by little more than its access to other people’s money and a burning desire to progress the rot in its barrel of apples by pretending there’s no way the legislature intended to make this law retroactive. It’s a stupid point to make, as NY Post attorney, Jeremy Chase, told the court:“The Legislature, if they wanted to carve out this period from 1976 to June 2020, they easily could have done that, they didn’t do that,” he said.This leaves the PBA as the last bulwark against… um… serving the public trust. If the PBA (or the NYPD officers it represents) actually cared about rebuilding trust and setting it back on the path towards earning the nickname “New York’s Finest,” it wouldn’t be blowing cash in court trying to keep its dirty laundry buried under empty body cam boxes in the back of the metaphorical closet. Instead, it has chosen to spend nearly a half-decade fighting this small move towards greater transparency despite having lost at every judicial level to this point. ]]></content:encoded></item><item><title>Daily Deal: Linux/UNIX Training Bundle</title><link>https://www.techdirt.com/2025/02/14/daily-deal-linux-unix-training-bundle/</link><author>Daily Deal</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:45:29 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Linux and UNIX operating systems have become increasingly popular in commercial computing environments. Due to their rapid growth in today’s businesses, Linux/UNIX administrators have also become very much in demand. This Linux/UNIX Training Bundle will help you learn the knowledge and skills to install, configure, & support a Linux/UNIX server, and more. It’s on sale for $50.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Lead Asahi Linux Developer Quits Days After Leaving Kernel Maintainer Role</title><link>https://linux.slashdot.org/story/25/02/14/1842251/lead-asahi-linux-developer-quits-days-after-leaving-kernel-maintainer-role?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>slashdot</category><category>linux</category><pubDate>Fri, 14 Feb 2025 18:42:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Hector Martin has resigned as the project lead of Asahi Linux, weeks after stepping down from his role as a Linux kernel maintainer for Apple ARM support. His departure from Asahi follows a contentious exchange with Linus Torvalds over development processes and social media advocacy. After quitting kernel maintenance earlier this month, the conflict escalated when Martin suggested that "shaming on social media" might be necessary to effect change. 

Torvalds sharply rejected this approach, stating that "social media brigading just makes me not want to have anything at all to do with your approach" and suggested that Martin himself might be the problem. In his final resignation announcement from Asahi, Martin wrote: "I no longer have any faith left in the kernel development process or community management approach." 

The dispute reflects deeper tensions in the Linux kernel community, particularly around the integration of Rust code. It follows the August departure of another key Rust for Linux maintainer, Wedson Almeida Filho from Microsoft. According to Sonatype's research, more than 300,000 open source projects have slowed or halted updates since 2020.]]></content:encoded></item><item><title>Dynamic Triple Buffering Merged For GNOME 48</title><link>https://www.phoronix.com/news/GNOME-48-Triple-Buffering</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:35:10 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As quite a Valentine's Day treat, the long-in-development dynamic triple buffering support for GNOME's Mutter compositor was just merged ahead of next month's GNOME 48 desktop release!..]]></content:encoded></item><item><title>Jay Xiao on the Power of Technology and Surety Through SuretyNow</title><link>https://hackernoon.com/jay-xiao-on-the-power-of-technology-and-surety-through-suretynow?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:31:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
 has always believed in the power of technology to improve people's lives. As a millennial, Xiao witnessed life before the internet as we know it and how much easier things became after it became more accessible to everyday people. He is now the co-founder and president of SuretyNow, a company that works to help protect the public from dishonest businesses.A Heart and Talent for Tech\
After graduating high school, Xiao attended Queen's University in Canada for commerce and computer science. Though his original goal was to go into consulting and finance, he made a decision to switch fully into tech after an unfulfilling internship at an asset management firm his first summer. During this time, he was balancing the internship while working on a startup with friends, eventually leading him to leave the asset management internship behind to dedicate his time to that startup.\
However, Xiao did not know how to code, so he began taking computer science classes to expand his horizons. He interned at a Toronto startup called Nuology and then parlayed that internship into full-time roles at Google, where he worked on the Google Ads and Google Stadia teams.Making an Impact with the Biggest Names in TechBreaking into the tech field was difficult for Xiao, partly because he was new to the industry and didn't go to a school with a strong history of tech company recruitment. As a result, he worked to better himself in technology through education and went from zero tech experience to interning with Google within three years. At Google Stadia, he helped build Stadia's publisher analytics platform from zero to launch. He helped facilitate an analytics platform used by around 30 publishers at its peak, including Ubisoft and Electronic Arts.As the driving force behind SuretyNow, Xiao's role extends far beyond the title of co-founder and president. He single-handedly built the company's website, developed the internal tooling and infrastructure, and now manages multiple teams. His strategic vision and hands-on approach have been instrumental in the company's success, making it a prime example of his entrepreneurial prowess.\
At SuretyNow, Xiao says that their agents can produce two times the industry average due to the automation they've built to make them more efficient. SuretyNow serves businesses who wish to buy surety to work like other modern tools they love, all with fast and affordable customer service. With exceptional customer service and a commitment to innovation through technology, they aim to educate and deliver peace of mind with their expertise in surety.\
Xiao's future goals for SuretyNow are simple: to lead and inspire others by hiring and mentoring interns. Ultimately, Xiao wants to make buying surety bonds and insurance as easy as purchasing a keychain on Amazon—a fact that can be difficult for some and challenging for others.]]></content:encoded></item><item><title>Choosing Wallets for Decentralized Applications</title><link>https://hackernoon.com/choosing-wallets-for-decentralized-applications?source=rss</link><author>EVEDEX</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
For modern blockchain applications, it’s crucial to balance ease of use for experienced users with accessibility for newcomers. One of the first challenges for beginners is selecting and creating a wallet. Wallet interaction must be simple enough to maintain high conversion rates while preserving blockchain’s core advantage - decentralization - without compromising user experience (UX).Two Audiences: Crypto Enthusiasts and Regular UsersWhen developing blockchain solutions, it’s important to account for varying user experience levels. In the context of decentralized exchanges like EVEDEX, two main audiences can be identified:Crypto Enthusiasts ("Cryptans") – users who are already familiar with blockchain technology, have experience with cold wallets, and value autonomy and decentralization.Regular Users (“Normies”) – those who want to trade and interact with decentralized financial instruments without having to dive into the technical aspects of blockchain.\
For the second group, social logins and embedded wallets are especially important, as they make the onboarding process and platform interaction as seamless as possible.Why is Social Login Important?Social login plays a key role in lowering the entry barrier for new users. Traditional crypto wallets require users to store seed phrases, install extensions, and connect hardware devices. This is complicated, especially for those just starting their journey in Web3.\
EVEDEX offers a built-in wallet with social login, allowing users to sign in via Google. This reduces the likelihood of losing access and makes the platform more convenient for a broader audience.\
Advantages of Social Login:Registration takes only a few seconds.No need for a seed phrase.Access to the account via multiple devices.More users can try to use DeFi without technical difficulties.\
The social login concept allows users to instantly enter the platform without memorizing complex seed phrases or going through lengthy registration processes. This is particularly important for newcomers encountering DeFi for the first time and who might be discouraged by the complexity of traditional wallets. By simplifying the login process, we lower the entry barrier for new users and grow the potential audience, allowing more people to explore decentralized finance effortlessly.Why is Wallet Choice Important?One of the main problems with modern blockchain solutions is the complexity of the first experience for new users. To start using decentralized applications, one must learn many new concepts, such as private keys, transactions, fees, and security. This discourages less technically inclined users and significantly reduces engagement. Therefore, choosing the most suitable wallet technology is not just a matter of convenience but also a way to grow the audience and improve the exchange’s conversion rates.\
EVEDEX is a decentralized exchange focused on providing an interface similar to traditional centralized platforms. To achieve this goal, wallets must combine decentralization (user-controlled funds) with Web2-level convenience.Sponsored transactions or fee payments in non-native currenciesTwo-factor authentication (2FA) supportCross-device account accessThe fastest way to implement these features is through Wallets as a Service (WaaS) solutions like Magic, Fireblocks, or Thirdweb.: Providers offer ready-made solutions with social login and 2FA support. Users do not need to understand blockchain technicalities, the wallet is ready to use immediately after registration.: Key storage is distributed, and the provider does not have direct access to the user's funds. Additionally, two-factor authentication significantly reduces user risks.: Users can quickly start using the wallet without complicated setup or seed phrase storage, relying only on social networks or email.Limited Blockchain Support: Some providers only work with specific networks.: Account recovery is implemented strictly according to the provider's mechanics.: For large user volumes, WaaS services can become prohibitively expensive.Geographical Restrictions: Providers may block access for users from certain regions.Account Abstraction is a technology that allows the use of smart contracts as wallets. Unlike classic wallets, which rely on private keys, smart contract wallets provide virtually unlimited flexibility in implementing interaction logic.\
Currently, the technology is actively developing on EVM-compatible blockchains, thanks to the ERC-4337 standard, which enables AA without changes to Ethereum's base protocol. However, this increases the complexity of the solution's architecture.\
For implementing a wallet with account recovery and two-factor authentication, the optimal approach is to choose a standard open-source smart account wallet implementation that is well-tested and audited.Advantages of AA for UsersFlexibility and Customization: The ability to tailor wallet logic to user needs and preferred verification methods.Decentralization and Full Asset Control: Eliminates risks of account blocking or access restrictions due to the absence of databases and centralized service controls.Access Recovery Through Pre-Defined Mechanisms: Useful in case of key loss or other unforeseen circumstances.Disadvantages of AA (Primarily for Projects, Not Users):Complexity of Implementation: Integrating AA with 2FA and recovery mechanisms requires significant amounts of resources.: It is possible to encounter bugs as the technology is still evolving rapidly.Need for AA Infrastructure Deployment in New Networks.EVEDEX is actively exploring hybrid solutions for maximum user convenience. One key aspect is the integration of Account Abstraction for enhanced security and user interaction with the platform. The platform offers embedded wallets with social login while maintaining decentralization. This allows traders to use a DEX with the ease of a CEX, without having to trust a centralized entity.\
Additionally, EVEDEX is testing the possibility of sponsored transactions, enabling users to pay fees in project tokens, as well as flexible access recovery mechanisms without handing over private keys.EVEDEX allows users to choose between different interaction options: (Recommended for Beginners):Quickly create an embedded wallet.Connecting an Existing Wallet:Use MetaMask, Trust Wallet, Ledger, and other wallets.Full control over assets.Requires seed phrase storage.An innovative approach is combining embedded wallets with Account Abstraction. In this model, an embedded wallet is used to create and manage a smart contract (AA) wallet. This solution allows for seamless integration of social login and 2FA functionality while retaining the ability to implement custom logic at the AA wallet level.Choosing the right wallet for decentralized applications requires balancing user convenience with decentralization principles. Embedded wallets provide quick setup and ready-made solutions but may limit flexibility and increase costs. Account Abstraction offers extensive possibilities but demands significant effort during implementation. A hybrid approach enables easy account recovery, social logins, and additional features without sacrificing wallet functionality.\
EVEDEX is betting on hybrid solutions, integrating advanced technologies into a user-friendly interface to offer the best of both worlds: CEX-like convenience and DEX-level security.]]></content:encoded></item><item><title>Macro-Less, Highly Integrated OpenAPI Document Generation in Rust with Ohkami</title><link>https://medium.com/@kanarus786/macro-less-highly-integrated-openapi-document-generation-in-rust-with-ohkami-912de388adc1</link><author>/u/kanarus</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 18:26:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[In Rust web dev, utoipa is the most popular crate for generating OpenAPI document from server code. While it’s a great tool, it can be frustrating due to excessive macro use.A new web framework Ohkami offers a ,  way to generate OpenAPI document with its “openapi” feature.Let’s take following code as an example. It’s the same sample from the “openapi” section of the README, but with openapi-related parts removed:While this compiles and works as a pseudo user management server, activating “openapi” feature causes a compile error, telling that User and CreateUser don’t implement ohkami::openapi::Schema.As indicated by this, Ohkami with “openapi” feature effectively handles type information and intelligently collects its endpoints’ metadata. It allows code like:to assemble metadata into an OpenAPI document and output it to a file .Then, how we implement Schema? Actually we can easily impl Schema by hand, or just #[derive(Schema)] is available! In this case, derive is enough:That’s it! Just adding these derives allows Ohkami::generate to output following file:Additionally, it’s easy to define the User schema as a component instead of duplicating inline schemas. In derive, just add #[openapi(component)] helper attribute:And  #[operation] attribute is available to set summary, description, and override operationId and each response’s description:Let’s take a look at how this document generation works!First, the #[derive(Schema)]s are expanded as following:The DSL enables to easily impl manually.Schema trait links the struct to an item of type called “SchemaRef”.2. openapi_* hooks of FromParam, FromRequest, IntoResponseFromParam, FromRequest and IntoResponse are Ohkami’s core traits appeared in the handler bound:When “openapi” feature is activated, they additionally have following methods:Ohkami leverages these methods in IntoHandler to generate consistent openapi::Operation, reflecting the actual handler signature like this.Moreover, Ohkami properly propagates schema information in common cases like this, allowing users to focus only on the types and schemas of their app.3. routes metadata of RouterIn Ohkami, what’s called router::base::Router has “routes” property that stores all the routes belonging to an Ohkami instance. This is returned alongside router::final::Router from “finalize” step, and is used to assemble metadata of all endpoints.What Ohkami::generate itself does is just to serialize an item of type openapi::document::Document and write it to a file.The openapi::document::Document item is created by “gen_openapi_doc” of router::final::Router, summarized as follows:That’s how Ohkami generates OpenAPI document!There is, however, a problem in , Cloudflare Workers: Ohkami is loaded to Miniflare or Cloudflare Workers as WASM, so it can only generate OpenAPI document andcannot write it to the user’s local file system.To work around this, Ohkami provides a CLI tool scripts/workers_openapi.js. This is, for example, used in package.json of Cloudflare Workers + OpenAPI template:generates OpenAPI document!]]></content:encoded></item><item><title>Trump Admin Adds Note Rejecting ‘Gender Ideology’ on Sites Court Ordered Them to Restore</title><link>https://www.404media.co/trump-admin-adds-note-rejecting-gender-ideology-on-sites-court-ordered-them-to-restore/</link><author>Emanuel Maiberg</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-10.31.48@2x.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 18:25:45 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[After being forced by a court order to  about gender and diversity to government websites, the Trump administration has added a note to the top of those pages saying “Any information on this page promoting gender ideology is extremely inaccurate, and disconnected from the immutable biological reality that there are two sexes, male and female.”Per a court order, HHS is required to restore this website as of February 14, 2025 at 11:59 p.m. Any information on this page promoting gender ideology is extremely inaccurate, and disconnected from the immutable biological reality that there are two sexes, male and female. The Trump Administration rejects gender ideology and condemns the harms it causes to children, by promoting their chemical and surgical mutilation, and to women, by depriving them of their dignity, safety, well-being, and opportunities. This page does not reflect biological reality and therefore the Administration and this Department rejects it.The note essentially seems like a way for the current administration to legally comply with a court order while still signaling that it entirely rejects any government funded or endorsed research or policy sympathetic to LGBTQ+ community and diversity, equity, and inclusion, which Trump and Elon Musk’s Department of Government Efficiency have been purging from government websites. Earlier this week,  that a federal judge ordered the Department of Health and Human Services, Centers for Disease Control, and Food and Drug Administration to restore several webpages they removed as a result of Trump’s executive order attacking diversity, equity, and inclusion. The agencies were given until 11:59 p.m. on February 11 to restore the webpages. The court ordered the administration to restore the webpages “to their versions as of January 30, 2025, meaning they were supposed to revert the webpages to what they looked like on January 30 with no changes. The versions that have been restored now have this additional disclaimer.A joint status update filed Thursday by lawyers for the Department of Justice and the Public Citizen Litigation Group says that the government has provided the court with a list of websites that it has restored, though the list of websites is not available. It also specifically says that the government is refusing to restore the website reproductiverights.gov: “Defendants have objected to restoring the website ‘reproductiverights.gov.’ Plaintiff’s counsel is conferring with their client,” it says.“Plaintiff’s lists include websites from Department of Health and Human Services (HHS) components other than the Centers for Disease Control and Prevention and the Food and Drug Administration. The parties disagree about whether such websites properly fall within the scope of the Order. However, given Plaintiff’s forthcoming amended complaint and to avoid further emergency motions practice, Defendants will restore those websites consistent with the Order,” it adds.]]></content:encoded></item><item><title>OpenAI: The Age of AI Is Here!</title><link>https://www.youtube.com/watch?v=97kQRYwL3P0</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/97kQRYwL3P0?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 18:18:07 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

📝 The paper "Competitive Programming with Large Reasoning Models" is available here:
https://arxiv.org/abs/2502.06807

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu]]></content:encoded></item><item><title>HyveDA Secures $50 Million in Pre-Deposits After Symbiotic Mainnet Launch</title><link>https://hackernoon.com/hyveda-secures-$50-million-in-pre-deposits-after-symbiotic-mainnet-launch?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:13:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[HyveDA reached a supply cap of $50 million in pre-deposits from early contributors. The deposit phase began with the launch of the Symbiotic mainnet on January 28 and completed within two weeks. The deposits occurred during a market decline triggered by President Trump’s tariff policy.\
Contributors participated through two vault types. One vault is managed by HyveDA and additional vaults are provided by partners including EtherFi, Renzo, MEV Capital, Re7, Steakhouse, Gautlet, and P2P. Investors deposited assets into the vaults to become eligible for points that will be assigned after the deposit phase and carried forward to later phases.\
The deposited funds are not subject to penalties or risks from operations. The deposit process is structured to maintain a controlled onboarding process while allowing the protocol to expand. HyveDA operates a network that uses a delegated staking system through Symbiotic’s restaking process. The network depends on the participation of nodes to maintain data availability.\
The deposit phase may affect the protocol’s progress. A group of participants engaged in the process during a period of market instability. The method for assigning points after the deposit phase may influence future participation in the ecosystem. Observers note that the outcome may serve as a reference for protocols that seek to raise funds during periods of market change.\
The experience of this deposit phase stands as a point of reference for protocols seeking similar funding processes. Future updates will determine how this approach influences the transition from the pre-testnet phase to subsequent network stages and the overall development of the protocol.The pre-deposit event resulted in a funding limit of 50 million dollars collected from early contributors. The process relied on a deposit mechanism with a defined cap and used multiple vault options. Contributors deposited assets into a native vault and several partner vaults. The method permitted a spread of contributions across many participants during a period when market conditions were affected by external policy changes.\
The deposit process provided an opportunity to test a funding method under conditions where market sentiment was shifting. Early participants engaged in the process despite the influence of external events. The use of a points system to reward participation introduces a variable for future involvement. This points system will be carried into subsequent phases and will affect how contributors interact with the network as it develops.\
The event also offered an example of a funding process that limits resource collection through a preset cap. The method used may serve as a reference for other protocols that plan to gather funds in similar market environments. The choice to spread contributions across several vaults instead of a single source appears to reduce the concentration of deposits. This approach may have an effect on how risk is distributed among network participants.\
Don’t forget to like and share the story! :::tip
Vested Interest Disclosure: This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYOR]]></content:encoded></item><item><title>Dynamic triple/double buffering merge request for GNOME was just merged!</title><link>https://gitlab.gnome.org/GNOME/mutter/-/merge_requests/1441</link><author>/u/joojmachine</author><category>linux</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 18:09:19 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI and security startups blossom on cloudy days</title><link>https://techcrunch.com/2025/02/14/ai-and-security-startups-blossom-on-cloudy-days/</link><author>Anna Heim</author><category>tech</category><pubDate>Fri, 14 Feb 2025 18:05:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Welcome to Startups Weekly — your weekly recap of everything you can’t miss from the world of startups. Want it in your inbox every Friday? Sign up here. This week confirmed that even when current events cloud the outlook, some startups still manage to raise significant amounts of funding, with those tied to security and […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Meta confirms ‘Project Waterworth,’ a global subsea cable project spanning 50,000 kilometers</title><link>https://techcrunch.com/2025/02/14/meta-confirms-project-waterworth-a-global-subsea-cable-project-spanning-50000km/</link><author>Ingrid Lunden</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:59:07 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Back in November, we broke the news that Meta — owner of Facebook, Instagram, and WhatsApp, with billions of users accounting for 10% of all fixed and 22% of all mobile traffic — was close to announcing work on a major new, $10 billion+ subsea cable project to connect up the globe. The aim was […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>[P] GNNs for time series anomaly detection</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/</link><author>/u/Important-Gear-325</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 17:56:59 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[For the past few months, my partner and I have been working on a project exploring the use of Graph Neural Networks (GNNs) for Time Series Anomaly Detection (TSAD). As we are near the completion of our work, I’d love to get feedback from this amazing community!Any comments, suggestions, or discussions are more than welcome! If you find the repo interesting, dropping a ⭐ would mean a lot. : )We're also planning to publish a detailed report with our findings and insights in the coming months, so stay tuned!The repo is still under development so don't be too harsh :)Looking forward to hearing your thoughts!]]></content:encoded></item><item><title>For those managing or working with multiple clusters, do you use a combined kubeconfig file or separate by cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1ipg99n/for_those_managing_or_working_with_multiple/</link><author>/u/trouphaz</author><category>reddit</category><category>k8s</category><category>devops</category><pubDate>Fri, 14 Feb 2025 17:44:13 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Kubernetes</source><content:encoded><![CDATA[I wonder if I'm in the minority. I have been keeping my kubeconfigs separate per cluster for years while I know others that combine everything to a single file. I started doing this because I didn't fully grasp yaml when I started and when I had an issue with the kubeconfig, I didn't have any idea on how to repair it. So I'd have to fully recreate it.So, each cluster has its own kubeconfig file named for the cluster's name and I have a function that'll set my KUBECONFIG variable to the file using the cluster name.sc() { CLUSTER_NAME="${1}" export KUBECONFIG="~/.kube/${CLUSTER_NAME}" } ]]></content:encoded></item><item><title>Mastodon is working to add the controversial ‘quote posts’ feature</title><link>https://techcrunch.com/2025/02/14/mastodon-is-working-to-add-the-controversial-quote-posts-feature/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:42:24 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Mastodon, the decentralized alternative to X, is going to adopt a controversial feature from the platform formerly known as Twitter: quote posts. The company on Friday shared the progress it’s making on the implementation of the feature, which has divided users over its potential to be used for online abuse and bullying. Critics have long […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Incremental Archival from Postgres to Parquet for Analytics</title><link>https://www.crunchydata.com/blog/incremental-archival-from-postgres-to-parquet-for-analytics</link><author>/u/gtobbe</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 17:42:23 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[PostgreSQL is commonly used to store event data coming from various kinds of devices. The data often arrives as individual events or small batches, which requires an operational database to capture. Features like time partitioning help optimize the storage layout for time range filtering and efficient deletion of old data.The PostgreSQL feature set gives you a lot of flexibility for handling a variety of IoT scenarios, but there are certain scenarios for it is less suitable, namely:Long-term archival of historical dataFast, interactive analytics on the source dataIdeally, data would get automatically archived in cheap storage, in a format optimized for large analytical queries.We developed two open source Postgres extensions that help you do that:pg_parquet can export (and import) query results to the Parquet file format in object storage using regular COPY commandspg_incremental can run a command for a never-ending series of time intervals or files, built on top of pg_cronWith some simple commands, you can set up a reliable, fully automated pipeline to export time ranges to the columnar Parquet format in S3.Then, you can use a variety of analytics tools to query or import the data. My favorite is of course Crunchy Data Warehouse.On any PostgreSQL server that has the pg_parquet and pg_incremental extensions, you can set up a pipeline that periodically exports data to in S3 in two steps.The pg_incremental extension has a create_time_interval_pipeline function that will run a given command once the time interval has passed, with 2 timestamp parameters set to the start and end of the hour. We cannot directly use query parameters in a COPY command, but we can define a simple PL/pgSQL function that generates and executes a custom COPY command using the parameters.-- existing raw data table
create table events (
  event_id bigint not null generated by default as identity,
  event_time timestamptz not null default now(),
  device_id bigint not null,
  sensor_1 double precision
);

insert into events (device_id, sensor_1)
values (297, 20.4);


insert into events (device_id, sensor_1)
values (297, 20.4);

-- define an export function that wraps a COPY command
create or replace function export_events(start_time timestamptz, end_time timestamptz)
returns void language plpgsql as $function$
begin
  execute format(
    $$
      copy (select * from events where event_time >= %L and event_time < %L)
      to 's3://mybucket/events/%s.parquet' with (format 'parquet');
    $$,
    start_time, end_time, to_char(start_time, 'YYYY-MM-DD-HH')
  );
end;
$function$;

-- export events hourly from the start of the year, and keep exporting in the future
select incremental.create_time_interval_pipeline('event-export',
  time_interval := '1 hour',                      /* export data by the hour                */
  batched := false,                               /* process 1 hour at a time               */
  start_time := '2025-01-01',                     /* backfill from the start of the year    */
  source_table_name := 'events',                  /* wait for writes on events to finish    */
  command := $$ select export_events($1, $2) $$   /* run export_events with start/end times */
);

By running these commands, Postgres will export all the data from the start of the year into hourly Parquet files in S3, and will keep doing so after every hour and automatically retry on failure.To use pg_parquet Crunchy Bridge, you can add your S3 credentials for pg_parquet to your Postgres server via the dashboard under Settings -> Data lake.Once data is in Parquet, you can use a variety of tools and approaches to query the data. If you want to keep using Postgres, you can use Crunchy Data Warehouse which has two different ways of working with Parquet data.The simplest way to start querying Parquet files in S3 in Crunchy Data Warehouse is to use a lake analytics table. You can easily create a table for all Parquet files that match a wildcard pattern:create foreign table events_parquet ()
server crunchy_lake_analytics
options (path 's3://mybucket/events/*.parquet');
You can then immediately query the data and the files get cached in the background, so queries will quickly get faster.A downside of querying Parquet directly is that eventually we will have a lot of hourly files that match the pattern, and there will be some overhead from listing them for each query (listing is not cached). We also cannot easily change the schema later.A more flexible approach is to import the Parquet files into an Iceberg table. Iceberg tables are also backed by Parquet in S3, but the files are compacted and optimized, and the table supports transactions and schema changes.You can create an Iceberg table that has the same schema as a set of Parquet files using the definition_from option. You could also load the data using load_from, but we’ll do that separately.create table events_iceberg () using iceberg
with (definition_from = 's3://mybucket/events/*.parquet');
Now we need a way to import all existing Parquet files and also import new files that show up in S3 into Iceberg. This is another job for pg_incremental. Following a similar approach as before, we create a function to generate a COPY command using a parameter.-- define an import function that wraps a COPY command to import from a URL
create function import_events(path text)
returns void language plpgsql as $function$
begin
  execute format($$copy events_iceberg from %L$$, path);
end;
$function$;

-- create a pipeline to import new files into a table, one by one.
-- $1 will be set to the path of a new file
select incremental.create_file_list_pipeline('event-import',
   file_pattern := 's3://mybucket/events/*.parquet',
   list_function := 'crunchy_lake.list_files',
   command := $$ select import_events($1) $$,
);

-- optional: do compaction immediately
vacuum events_iceberg;

After running these commands, your data will be continuously archived from your source Postgres server into Iceberg in S3. You can then run fast analytical queries directly from Crunchy Data Warehouse, which uses a combination of parallel, vectorized query processing and file caching to speed up queries. You can additionally set up (materialized) views and assign read permissions to the relevant users.No complex ETL pipelines required.To give you a sense of the performance benefit of using Parquet, we loaded 100M rows into the source table, which got automatically mirrored in Parquet and Iceberg via our pipelines. We then ran a simple analytical query on each table:select device_id, avg(sensor_1) from events group by 1;
The runtimes in milliseconds are shown in the following chart:In this case the source server is a standard-16 instance (4 vcpus) on Crunchy Bridge, and the warehouse is a warehouse-standard-16 instance (4 vcpus). So, using Crunchy Data Warehouse we can analyze 100M rows in well under a second on a small machine, and get >10x speedup with Iceberg.The use of compression also means the size went from 8.9GB in PostgreSQL to 1.2GB in Iceberg using object storage.With pg_parquet and pg_incremental, you can incrementally export data from PostgreSQL into Parquet in S3, and with Crunchy Data Warehouse you can process and analyze that data very quickly while still using PostgreSQL.One of the nice characteristics of the approach described in this blog is that the pipelines are fully transactional. It means that every import or export step either fully succeeds or fails and then it will be retried until it does succeed. That’s how we can create production-ready pipelines with a few simple SQL commands.Under the covers, pg_incremental keeps track of which time ranges or files have been processed. The bookkeeping happens in the same transaction as the COPY commands. So if a command fails because of an ephemeral S3 issue, the data will not end up being ingested twice or go missing. Having transactions takes away a huge amount of complexity for building reliable pipelines. There can of course be other reasons for pipeline failures that cannot be resolved through retries (e.g. changing data format), so it is still important to monitor your pipelines.]]></content:encoded></item><item><title>DeepSeek: Everything you need to know about the AI chatbot app</title><link>https://techcrunch.com/2025/02/14/deepseek-everything-you-need-to-know-about-the-ai-chatbot-app/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:37:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[DeepSeek has gone viral. Chinese AI lab DeepSeek broke into the mainstream consciousness this week after its chatbot app rose to the top of the Apple App Store charts (and Google Play, as well). DeepSeek’s AI models, which were trained using compute-efficient techniques, have led Wall Street analysts — and technologists — to question whether the U.S. can maintain its […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>DOGE’s ‘Genius’ Coders Launch Website So Full Of Holes, Anyone Can Write To It</title><link>https://www.techdirt.com/2025/02/14/doges-genius-coders-launch-website-so-full-of-holes-anyone-can-write-to-it/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:25:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[If you want to write something on the U.S. government’s official DOGE website, apparently you can just… do that. Not in the usual way of submitting comments through a form, mind you, but by directly injecting content into their database. This seems suboptimal.The story here is that DOGE — Elon Musk’s collection of supposed coding “geniuses” brought in to “disrupt” government inefficiency — finally launched their official website. And what they delivered is a masterclass in how not to build government infrastructure. One possibility is that they’re brilliant disruptors breaking all the rules to make things better. Another possibility is that they have no idea what they’re doing.The latter seems a lot more likely.Last week, it was reported that the proud racist 25-year-old Marko Elez had been given admin access and was pushing untested code to the US government’s $6 trillion/year payment system. While the Treasury Department initially claimed (including in court filings!) that Elez had “read-only” access, others reported he had write access. After those reports came out, the Treasury Dept. “corrected” itself and said Elez had been “accidentally” given write privileges for the payments database, but only for the data, not the code. Still, they admitted that while they had put in place some security protections, it’s possible that Elez did copy some private data which “may have occasionally included screenshots of payment systems data or records.”Now, you might think that having a racist twenty-something with admin access to trillion-dollar payment systems would concern people. But Musk’s defenders had a compelling counterargument: he must be a genius! Because… well, because Musk hired him, and Musk only hires geniuses. Or so we’re told.The DOGE team’s actual coding prowess is turning out to be quite something. First, they decided that government transparency meant hiding everything from FOIA requests. When questioned about this interesting interpretation of “transparency,” Musk explained that actually DOGE was being super transparent by putting everything on their website and ExTwitter account.There was just one small problem with this explanation. At the time he said it, the DOGE website looked like this:That was it. That was the whole website.On Thursday, they finally launched a real website. Sort of. If by “real website” you mean “a collection of already-public information presented in misleading ways by people who don’t seem to understand what they’re looking at.” But that’s not even the interesting part.These supposed technical geniuses managed to build what might be the least secure government website in history. Let’s start with something basic: where does the website actually live? According to Wired, the source code actually tells search engines that ExTwitter, not DOGE.gov, is the real home of this government information:A WIRED review of the page’s source code shows that the promotion of Musk’s own platform went deeper than replicating the posts on the homepage. The source code shows that the site’sA canonical tag is a snippet of code that tells search engines what the authoritative version of a website is. It is typically used by sites with multiple pages as a search engine optimization tactic, to avoid their search ranking being diluted.In DOGE’s case, however, the code is informing search engines that when people search for content found on, they should not show those pages in search results, but should instead display the posts on X.“It is promoting the X account as the main source, with the website secondary,” Declan Chidlow,, tells WIRED. “This isn’t usually how things are handled, and it indicates that the X account is taking priority over the actual website itself.”If you’re not a web developer, here’s what that means: When you build a website, you can tell search engines “hey, if you find copies of this content elsewhere, this version here is the real one.” It’s like telling Google “if someone copied my site, mine is the original.”But DOGE did the opposite. They told search engines “actually, ExTwitter has the real version of this government information. Our government website is just a copy.” Which is… an interesting choice for a federal agency? It’s a bit like the Treasury Department saying “don’t look at our official reports, just check Elon’s tweets.”You might think that a government agency directing people away from its official website and toward the private company of its leader would raise some conflict-of-interest concerns. And you’d be right!But wait, it gets better. Or worse. Actually, yeah, it’s worse.Who built this government website? Through some sloppy coding, security researcher Sam Curry figured out it was DOGE employee Kyle Shutt. The same Kyle Shutt who, according to Drop Site News, has admin access to the FEMA payments system. The same Kyle Shutt who used the exact same Cloudflare ID to build Musk’s America PAC Trump campaign website. Because why maintain separate secure credentials for government systems and political campaigns when you can just… not do that?But the real cherry on top came Thursday when people discovered something amazing about the DOGE site database:  Not “anyone with proper credentials.” Not “anyone who passes security checks.” Just… anyone. As 404 Media reported, if you know basic database operations, you too can be a government website administrator:The doge.gov website that was spun up to track Elon Musk’s cuts to the federal government is insecure and pulls from a database that can be edited by anyone, according to two separate people who found the vulnerability and shared it with 404 Media. One coder added at least two database entries that are visible on the live site and say “this is a joke of a .gov site” and “THESE ‘EXPERTS’ LEFT THEIR DATABASE OPEN -roro.” While I imagine those will be taken down shortly, for now, the insertions are absolutely visible:Look, there’s a reason we called this whole thing a cyberattack. When someone takes over your computer systems and leaves them wide open to anyone who wants to mess with them, we usually don’t call that “disruption” or “innovation.” We call it a cybersecurity breach.“Feels like it was completely slapped together,” they added. “Tons of errors and details leaked in the page source code.”Both sources said that the way the site is set up suggests that it is not running on government servers.has its codebase, probably through GitHub or something,” the other developer who noticed the insecurity said. “They’re deploying the website on Cloudflare Pages from their codebase, andis a custom domain that theirURL is set to. So rather than having a physical server or even something like Amazon Web Services, they’re deploying using Cloudflare Pages which supports custom domains.”Here’s the thing about government computer systems: They’re under constant attack from foreign adversaries. Yes, they can be inefficient. Yes, they can be bloated. But you know what else they usually are? Not completely exposed to the entire internet. It turns out that some of that inefficient “bureaucracy” involves basic things like “security” and “not letting random people write whatever they want in federal databases.”This isn’t some startup where “move fast and break things” is a viable strategy. This is the United States government. And it’s been handed over to people whose main qualification appears to be “posts spicy memes on 4chan.” The implications go far beyond embarrassing database injections — this level of technical negligence in federal systems creates genuine national security concerns. When your “disruption” involves ignoring decades of hard-learned lessons about government systems security, you’re not innovating — you’re inviting disaster.]]></content:encoded></item><item><title>Friday Squid Blogging: Squid the Care Dog</title><link>https://www.schneier.com/blog/archives/2025/02/friday-squid-blogging-squid-the-care-dog.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 14 Feb 2025 17:05:38 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[The Vanderbilt University Medical Center has a pediatric care dog named “Squid.”]]></content:encoded></item><item><title>Upcoming Speaking Engagements</title><link>https://www.schneier.com/blog/archives/2025/02/upcoming-speaking-engagements-43.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 14 Feb 2025 17:01:21 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[This is a current list of where and when I am scheduled to speak:I’m speaking at Boskone 62 in Boston, Massachusetts, USA, which runs from February 14-16, 2025. My talk is at 4:00 PM ET on the 15th.]]></content:encoded></item><item><title>Support the open source projects you love this Valentine’s Day</title><link>https://github.blog/open-source/support-the-open-source-projects-you-love-this-valentines-day/</link><author>Martin Woodward</author><category>official</category><pubDate>Fri, 14 Feb 2025 17:00:04 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[While everyone likes flowers and chocolates, why not show your love for your favorite open source projects this Valentine’s and give appreciation to the maintainers who keep them running?Many of the open source projects we rely on every day are maintained by dedicated volunteers. Sponsoring projects isn’t just about altruism, it’s about investing in the future. Many maintainers work on open source projects in their spare time. Sponsorships can help them dedicate more time to the projects you depend on. With financial support, maintainers can help cover costs such as development and hosting.When you sponsor a maintainer, you’re helping them continue their work and letting them know you value the creativity and love they’ve poured into their project. For this Valentine’s Day, let’s show love to the maintainers who keep our favorite projects alive and thriving.The benefits of financial sponsorshipSponsorship is a tangible way to show support for the open source community. It can be a huge morale boost, as well as bring visibility and validation. Sponsored projects often gain more attention, leading to a virtuous cycle of more contributors and users, which results in better software for everyone.Getting started: How to best invest in open sourceIdentify critical dependencies:Review your project’s dependencies to identify which open source libraries and tools are crucial to your operations. If you use tools like npm, pip, or maven, you can review their dependencies and prioritize which are critical to your project’s success.  Evaluate the project activity: Check the recent and trending activity of repositories and look for signs of contributions, shipped features, bug fixes, active maintenance, and community engagement. Helpful metrics like recent commits, issue resolution, and community engagement are all good markers for where funding could help. Projects with a high impact but low funding could be great candidates to prioritize for sponsorship.   Take the opportunity to look at the issues and contribution guidelines. Try to understand their funding needs and how your support makes a difference. But also consider investing in ways beyond just financial support. You could provide bug fixes if you feel confident in your code. You could also take the time to tell someone about the project and why you think it’s awesome. Maintainers can show love for their sponsors as well. Sponsors appreciate knowing their contribution is making a difference. Whether it’s a simple mention on social media to acknowledge their contribution, featuring them on your GitHub Sponsors profile, exclusive updates and behind-the-scenes insights about your project and community, or even a brief personalized thank-you message, tokens of appreciation can help bring more funding your way.This Valentine’s Day, let’s show our favorite projects a bit more appreciation. Whether it’s a heartfelt contribution to fix bugs or docs, spreading the word about why it’s a great project and helping people use it, or a sweet one-time donation, every bit helps keep our beloved digital world turning.Ready to start sponsoring? Visit GitHub Sponsors and find the projects that matter most to you.Happy Valentine’s Day to all the open source contributors and supporters out there! And remember, you don’t need to wait for a special occasion like Valentine’s Day to show your appreciation for maintainers—every day is a perfect day to support open source! 💖]]></content:encoded></item><item><title>Video Friday: PARTNR</title><link>https://spectrum.ieee.org/video-friday-partnr</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjQ5NjM2MS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc0MDk5OTA2Nn0.hMaiDCMfAle2DiX2_S3CZKODrsXEYRxu4BQGU0eByc4/image.png?width=600" length="" type=""/><pubDate>Fri, 14 Feb 2025 17:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your weekly selection of awesome robot videos]]></content:encoded></item><item><title>A job ad for Y Combinator startup Firecrawl seeks to hire an AI agent for $15K a year</title><link>https://techcrunch.com/2025/02/14/a-job-ad-for-y-combinator-startup-firecrawl-seeks-to-hire-an-ai-agent-for-15k-a-year/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 14 Feb 2025 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Last week, an ad from the Y Combinator job board for a tiny startup called Firecrawl went viral on X.  That’s because the ad wasn’t for a human. “Please apply only if you are an AI agent, or if you created an AI agent that can fill this job,” the job posting read.  The seven-person […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Roadmap to Becoming a Data Scientist, Part 4: Advanced Machine Learning</title><link>https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-4-advanced-machine-learning/</link><author>Vyacheslav Efimov</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 17:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Data science is undoubtedly one of the most fascinating fields today. Following significant breakthroughs in machine learning about a decade ago, data science has surged in popularity within the tech community. Each year, we witness increasingly powerful tools that once seemed unimaginable. Innovations such as the , , the Retrieval-Augmented Generation (RAG) framework, and state-of-the-art  — including  — have had a profound impact on our world.However, with the abundance of tools and the ongoing hype surrounding AI, it can be overwhelming — especially for beginners — to determine which skills to prioritize when aiming for a career in data science. Moreover, this field is highly demanding, requiring substantial dedication and perseverance.The first three parts of this series outlined the necessary skills to become a data scientist in three key areas: math, software engineering, and machine learning. While knowledge of classical Machine Learning and neural network algorithms is an excellent starting point for aspiring data specialists, there are still many important topics in machine learning that must be mastered to work on more advanced projects.This article will focus solely on the math skills necessary to start a career in Data Science. Whether pursuing this path is a worthwhile choice based on your background and other factors will be discussed in a separate article.The importance of learning evolution of methods in machine learningThe section below provides information about the evolution of methods in natural language processing (NLP).In contrast to previous articles in this series, I have decided to change the format in which I present the necessary skills for aspiring data scientists. Instead of directly listing specific competencies to develop and the motivation behind mastering them, I will briefly outline the most important approaches, presenting them in chronological order as they have been developed and used over the past decades in machine learning.The reason is that I believe it is crucial to study these algorithms from the very beginning. In machine learning, many new methods are built upon older approaches, which is especially true for NLP and computer vision.For example, jumping directly into the implementation details of modern large language models (LLMs) without any preliminary knowledge may make it very difficult for beginners to grasp the motivation and underlying ideas of specific mechanisms.Given this, in the next two sections, I will highlight in  the key concepts that should be studied.Natural language processing (NLP) is a broad field that focuses on processing textual information. Machine learning algorithms cannot work directly with raw text, which is why text is usually preprocessed and converted into numerical vectors that are then fed into neural networks.Before being converted into vectors, words undergo , which includes simple techniques such as , stemming, lemmatization, normalization, or removing . After preprocessing, the resulting text is encoded into . Tokens represent the smallest textual elements in a collection of documents. Generally, a token can be a part of a word, a sequence of symbols, or an individual symbol. Ultimately, tokens are converted into numerical vectors.The  method is the most basic way to encode tokens, focusing on counting the frequency of tokens in each document. However, in practice, this is usually not sufficient, as it is also necessary to account for token importance — a concept introduced in the  and  methods. While TF-IDF improves upon the naive counting approach of bag of words, researchers have developed a completely new approach called embeddings. are numerical vectors whose components preserve the semantic meanings of words. Because of this, embeddings play a crucial role in NLP, enabling input data to be trained or used for model inference. Additionally, embeddings can be used to compare text similarity, allowing for the retrieval of the most relevant documents from a collection.Embeddings can also be used to encode other unstructured data, including images, audio, and videos.As a field, NLP has been evolving rapidly over the last 10–20 years to efficiently solve various text-related problems. Complex tasks like text translation and text generation were initially addressed using recurrent neural networks (RNNs), which introduced the concept of memory, allowing neural networks to capture and retain key contextual information in long documents.Although RNN performance gradually improved, it remained suboptimal for certain tasks. Moreover, RNNs are relatively slow, and their sequential prediction process does not allow for parallelization during training and inference, making them less efficient.Additionally, the original Transformer architecture can be decomposed into two separate modules:  and . Both of these form the foundation of the most state-of-the-art models used today to solve various NLP problems. Understanding their principles is valuable knowledge that will help learners advance further when studying or working with other large language models (LLMs).When it comes to LLMs, I strongly recommend studying the evolution of at least the first three GPT models, as they have had a significant impact on the AI world we know today. In particular, I would like to highlight the concepts of  and , introduced in GPT-2, which enable LLMs to solve text generation tasks without explicitly receiving any training examples for them.Another important technique developed in recent years is retrieval-augmented generation (RAG). The main limitation of LLMs is that they are only aware of the context used during their training. As a result, they lack knowledge of any information beyond their training data.The retriever converts the input prompt into an embedding, which is then used to query a vector database. The database returns the most relevant context based on the similarity to the embedding. This retrieved context is then combined with the original prompt and passed to a generative model. The model processes both the initial prompt and the additional context to generate a more informed and contextually accurate response.A good example of this limitation is the first version of the ChatGPT model, which was trained on data up to the year 2022 and had no knowledge of events that occurred from 2023 onward.To address this limitation, OpenAI researchers developed a RAG pipeline, which includes a constantly updated database containing new information from external sources. When ChatGPT is given a task that requires external knowledge, it queries the database to retrieve the most relevant context and integrates it into the final prompt sent to the machine learning model.The goal of distillation is to create a smaller model that can imitate a larger one. In practice, this means that if a large model makes a prediction, the smaller model is expected to produce a similar result.In the modern era, LLM development has led to models with millions or even billions of parameters. As a consequence, the overall size of these models may exceed the hardware limitations of standard computers or small portable devices, which come with many constraints.Quantization is the process of reducing the memory required to store numerical values representing a model’s weights.This is where optimization techniques become particularly useful, allowing LLMs to be compressed without significantly compromising their performance. The most commonly used techniques today include ,, and .Pruning refers to discarding the least important weights of a model.Regardless of the area in which you wish to specialize, knowledge of  is a must-have skill! Fine-tuning is a powerful concept that allows you to efficiently adapt a pre-trained model to a new task.Fine-tuning is especially useful when working with very large models. For example, imagine you want to use BERT to perform semantic analysis on a specific dataset. While BERT is trained on general data, it might not fully understand the context of your dataset. At the same time, training BERT from scratch for your specific task would require a massive amount of resources.Here is where fine-tuning comes in: it involves taking a pre-trained BERT (or another model) and freezing some of its layers (usually those at the beginning). As a result, BERT is retrained, but this time only on the new dataset provided. Since BERT updates only a subset of its weights and the new dataset is likely much smaller than the original one BERT was trained on, fine-tuning becomes a very efficient technique for adapting BERT’s rich knowledge to a specific domain.Fine-tuning is widely used not only in NLP but also across many other domains.As the name suggests,  involves analyzing images and videos using machine learning. The most common tasks include image classification, object detection, image segmentation, and generation.Most CV algorithms are based on neural networks, so it is essential to understand how they work in detail. In particular, CV uses a special type of network called convolutional neural networks (CNNs). These are similar to fully connected networks, except that they typically begin with a set of specialized mathematical operations called .In simple terms, convolutions act as filters, enabling the model to extract the most important features from an image, which are then passed to fully connected layers for further analysis.The next step is to study the most popular CNN architectures for classification tasks, such as AlexNet, VGG, Inception, ImageNet, and .Speaking of the object detection task, the  algorithm is a clear winner. It is not necessary to study all of the dozens of versions of YOLO. In reality, going through the original paper of the first YOLO should be sufficient to understand how a relatively difficult problem like object detection is elegantly transformed into both classification and regression problems. This approach in YOLO also provides a nice intuition on how more complex CV tasks can be reformulated in simpler terms.While there are many architectures for performing image segmentation, I would strongly recommend learning about , which introduces an encoder-decoder architecture.Finally, image generation is probably one of the most challenging tasks in CV. Personally, I consider it an optional topic for learners, as it involves many advanced concepts. Nevertheless, gaining a high-level intuition of how generative adversial networks (GAN) function to generate images is a good way to broaden one’s horizons.In some problems, the training data might not be enough to build a performant model. In such cases, the data augmentation technique is commonly used. It involves the artificial generation of training data from already existing data (images). By feeding the model more diverse data, it becomes capable of learning and recognizing more patterns.It would be very hard to present in detail the Roadmaps for all existing machine learning domains in a single article. That is why, in this section, I would like to briefly list and explain some of the other most popular areas in data science worth exploring.First of all, recommender systems (RecSys) have gained a lot of popularity in recent years. They are increasingly implemented in online shops, social networks, and streaming services. The key idea of most algorithms is to take a large initial matrix of all users and items and decompose it into a product of several matrices in a way that associates every user and every item with a high-dimensional embedding. This approach is very flexible, as it then allows different types of comparison operations on embeddings to find the most relevant items for a given user. Moreover, it is much more rapid to perform analysis on small matrices rather than the original, which usually tends to have huge dimensions. often goes hand in hand with RecSys. When a RecSys has identified a set of the most relevant items for the user, ranking algorithms are used to sort them to determine the order in which they will be shown or proposed to the user. A good example of their usage is search engines, which filter query results from top to bottom on a web page.Closely related to ranking, there is also a  problem that aims to optimally map objects from two sets, A and B, in a way that, on average, every object pair is mapped “well” according to a matching criterion. A use case example might include distributing a group of students to different university disciplines, where the number of spots in each class is limited. is an unsupervised machine learning task whose objective is to split a dataset into several regions (clusters), with each dataset object belonging to one of these clusters. The splitting criteria can vary depending on the task. Clustering is useful because it allows for grouping similar objects together. Moreover, further analysis can be applied to treat objects in each cluster separately.The goal of clustering is to group dataset objects (on the left) into several categories (on the right) based on their similarity. is another unsupervised problem, where the goal is to compress an input dataset. When the dimensionality of the dataset is large, it takes more time and resources for machine learning algorithms to analyze it. By identifying and removing noisy dataset features or those that do not provide much valuable information, the data analysis process becomes considerably easier. is an area that focuses on designing algorithms and data structures (indexes) to optimize searches in a large database of embeddings (vector database). More precisely, given an input embedding and a vector database, the goal is to  find the most similar embedding in the database relative to the input embedding.The goal of similarity search is to approximately find the most similar embedding in a vector database relative to a query embedding.The word “approximately” means that the search is not guaranteed to be 100% precise. Nevertheless, this is the main idea behind similarity search algorithms — sacrificing a bit of accuracy in exchange for significant gains in prediction speed or data compression. involves studying the behavior of a target variable over time. This problem can be solved using classical tabular algorithms. However, the presence of time introduces new factors that cannot be captured by standard algorithms. For instance:the target variable can have an overall , where in the long term its values increase or decrease (e.g., the average yearly temperature rising due to global warming).the target variable can have a  which makes its values change based on the currently given period (e.g. temperature is lower in winter and higher in summer).Most of the time series models take both of these factors into account. In general, time series models are mainly used a lot in financial, stock or demographic analysis.Another advanced area I would recommend exploring is , which fundamentally changes the algorithm design compared to classical machine learning. In simple terms, its goal is to train an agent in an environment to make optimal decisions based on a reward system (also known as the “trial and error approach”). By taking an action, the agent receives a reward, which helps it understand whether the chosen action had a positive or negative effect. After that, the agent slightly adjusts its strategy, and the entire cycle repeats.Reinforcement learning is particularly popular in complex environments where classical algorithms are not capable of solving a problem. Given the complexity of reinforcement learning algorithms and the computational resources they require, this area is not yet fully mature, but it has high potential to gain even more popularity in the future.Currently the most popular applications are:. Existing approaches can design optimal game strategies and outperform humans. The most well-known examples are chess and Go.. Advanced algorithms can be incorporated into robots to help them move, carry objects or complete routine tasks at home.. Reinforcement learning methods can be developed to automatically drive cars, control helicopters or drones.This article was a logical continuation of the previous part and expanded the skill set needed to become a data scientist. While most of the mentioned topics require time to master, they can add significant value to your portfolio. This is especially true for the NLP and CV domains, which are in high demand today.After reaching a high level of expertise in data science, it is still crucial to stay motivated and consistently push yourself to learn new topics and explore emerging algorithms.Data science is a constantly evolving field, and in the coming years, we might witness the development of new state-of-the-art approaches that we could not have imagined in the past.All images are by the author unless noted otherwise.]]></content:encoded></item><item><title>Siren Call of SQLite on the Server</title><link>https://pid1.dev/posts/siren-call-of-sqlite-on-the-server/</link><author>/u/sausagefeet</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 16:46:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Behind the Blog: Backdoors and the Miracle of Wikipedia</title><link>https://www.404media.co/behind-the-blog-backdoors-and-the-miracle-of-wikipedia/</link><author>Samantha Cole</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/nl2.14--1-.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 16:40:01 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[This is Behind the Blog, where we share our behind-the-scenes thoughts about how a few of our top stories of the week came together. This week, we discuss Apple's iCloud, Wikipedia is a miracle of humankind, and good soup.After our relatively unhinged BTBs last week, many of you left extremely nice comments, reached out individually, or otherwise gave us encouragement. You all are the best, and it made us feel very good. Thank you!]]></content:encoded></item><item><title>Unlocking global AI potential with next-generation subsea infrastructure</title><link>https://engineering.fb.com/2025/02/14/connectivity/project-waterworth-ai-subsea-infrastructure/</link><author></author><category>dev</category><category>official</category><pubDate>Fri, 14 Feb 2025 16:28:06 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I&apos;m very impressed by how Rust supports both beginners and pro&apos;s</title><link>https://www.reddit.com/r/rust/comments/1ipe6m7/im_very_impressed_by_how_rust_supports_both/</link><author>/u/ConstructionShot2026</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 16:16:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I would go as far saying it supports a syntax abstraction that is simpler than python to read.I just find it amazing, with a performance level so close to C++.Its your choice how many complex features you want to add for control and optimization, and the compiler is so cool, that it can add them automatically if I don't see it necessary.I believe if more knew how simple it could be, more would use it outside systems programming :D]]></content:encoded></item><item><title>Publish Interactive Data Visualizations for Free with Python and Marimo</title><link>https://towardsdatascience.com/publish-interactive-data-visualizations-for-free-with-python-and-marimo/</link><author>Sam Minot</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 16:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[Working in Data Science, it can be hard to share insights from complex datasets using only static figures. All the facets that describe the shape and meaning of interesting data are not always captured in a handful of pre-generated figures. While we have powerful technologies available for presenting interactive figures — where a viewer can rotate, filter, zoom, and generally explore complex data  —  they always come with tradeoffs.Here I present my experience using a recently released Python library — marimo — which opens up exciting new opportunities for publishing interactive visualizations across the entire field of data science.Interactive Data VisualizationThe tradeoffs to consider when selecting an approach for presenting data visualizations can be broken into three categories: — what visualizations and interactivity am I able to present to the user? — what are the resources needed for displaying this visualization to users (e.g. running servers, hosting websites)? – how much of a new skillset / codebase do I need to learn upfront? is the foundation of portable interactivity. Every user has a web browser installed on their computer and there are many different frameworks available for displaying any degree of interactivity or visualization you might imagine (for example, this gallery of amazing things people have made with three.js). Since the application is running on the user’s computer, no costly servers are needed. However, a significant drawback for the data science community is ease of use, as JS does not have many of the high-level (i.e. easy-to-use) libraries that data scientists use for data manipulation, plotting, and interactivity. provides a useful point of comparison. Because of its continually growing popularity, some have called this the “Era of Python”. For data scientists in particular, Python stands alongside R as one of the foundational languages for quickly and effectively wielding complex data. While Python may be easier to use than Javascript, there are fewer options for presenting interactive visualizations. Some popular projects providing interactivity and visualization have been Flask, Dash, and Streamlit (also worth mentioning — bokeh, HoloViews, altair, and plotly). The biggest tradeoff for using Python has been the cost for publishing – delivering the tool to users. In the same way that shinyapps require a running computer to serve up the visualization, these Python-based frameworks have exclusively been server-based. This is by no means prohibitive for authors with a budget to spend, but it does limit the number of users who can take advantage of a particular project. is an intriguing middle ground — Python code running directly in the web browser using WebAssembly (WASM). There are resource limitations (only 1 thread and 2GB memory) that make this impractical for doing the heavy lifting of data science. , this can be more than sufficient for building visualizations and updating based on user input. Because it runs in the browser, no servers are required for hosting. Tools that use Pyodide as a foundation are interesting to explore because they give data scientists an opportunity to write Python code which runs directly on users’ computers without their having to install or run anything outside of the web browser.As an aside, I’ve been interested previously in one project that has tried this approach: stlite, an in-browser implementation of Streamlit that lets you deploy these flexible and powerful apps to a broad range of users. However, a core limitation is that Streamlit itself is distinct from stlite (the port of Streamlit to WASM), which means that not all features are supported and that advancement of the project is dependent on two separate groups working along compatible lines.The interface resembles a Jupyter , which will be familiar to users.Execution of cells is , so that updating one cell will rerun all cells which depend on its output. can be captured with a flexible set of UI components.Notebooks can be quickly converted into , hiding the code and showing only the input/output elements.Apps can be run locally or converted into using WASM/Pyodide.marimo balances the tradeoffs of technology in a way that is well suited to the skill set of the typical data scientists: — user input and visual display features are rather extensive, supporting user input via Altair and Plotly plots. — deploying as static webpages is basically free — no servers required — for users familiar with Python notebooks, marimo will feel very familiar and be easy to pick up.Publishing Marimo Apps on the WebAs a simple example of the type of display that can be useful in data science, consisting of explanatory text interspersed with interactive displays, I have created a barebones GitHub repository. Try it out yourself here.Using just a little bit of code, users can:Generate visualizations with flexible interactivityWrite narrative text describing their findingsPublish to the web for free (i.e. using GitHub Pages)Public App / Private DataThis new technology offers an exciting new opportunity for collaboration — publish the app publicly to the world, but users can only see specific datasets that they have permission to access.Rather than building a dedicated data backend for every app, user data can be stored in a generic backend which can be securely authenticated and accessed using a Python client library — all contained within the user’s web browser. For example, the user is given an OAuth login link that will authenticate them with the backend and allow the app to temporarily access input data.As a proof of concept, I built a simple visualization app which connects to the Cirro data platform, which is used at my institution to manage scientific data. Full disclosure: I was part of the team that built this platform before it spun out as an independent company. In this manner users can:Load the public visualization app — hosted on GitHub PagesConnect securely to their private data storeLoad the appropriate dataset for displayShare a link which will direct authorized collaborators to the same dataAs a data scientist, this approach of publishing free and open-source visualization apps which can be used to interact with private datasets is extremely exciting. Building and publishing a new app can take hours and days instead of weeks and years, letting researchers quickly share their insights with collaborators and then publish them to the wider world.]]></content:encoded></item><item><title>Linux 6.15 To Ensure PlayStation 5 Controllers Use The Correct Driver</title><link>https://www.phoronix.com/news/Linux-6.15-Ensures-PS5-Driver</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 15:49:09 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A change queued up by an Amazon engineer ahead of the upcoming Linux 6.15 kernel cycle will ensure that PlayStation 5 controllers on Linux load with the correctly desired driver...]]></content:encoded></item><item><title>Why does Linux open large file bases much faster than windows?</title><link>https://www.reddit.com/r/linux/comments/1ipd8a7/why_does_linux_open_large_file_bases_much_faster/</link><author>/u/AlternativeCarpet494</author><category>linux</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 15:34:56 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[So I have a 4TB hard drive with around a 100 GB dataset on it. I was going to some useless uni classes today and thought oh I’ll just work on some of my code to process the data set on my windows laptop. Anyways, the file explorer crashed. Why is the windows file system so much worse?]]></content:encoded></item><item><title>The largest sofa you can move around a corner</title><link>https://www.quantamagazine.org/the-largest-sofa-you-can-move-around-a-corner-20250214/</link><author>abe94</author><category>hn</category><pubDate>Fri, 14 Feb 2025 15:24:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[In 1992, Joseph Gerver of Rutgers University proposed a particularly clever curved shape with an area of approximately 2.2195. Mathematicians suspected that it answered Moser’s question. But they couldn’t prove it.Now a young postdoctoral researcher has. In a 119-page paper, Jineon Baek of Yonsei University in Seoul showed that Gerver’s sofa is the largest shape that can successfully pass through the hallway.The paper isn’t just noteworthy for resolving a 60-year-old problem. It has also garnered attention because mathematicians had expected any eventual proof of the conjecture to require a computer. Baek’s proof did not. Mathematicians now hope that the techniques he used might help them make progress on other kinds of optimization problems.Perhaps even more intriguing, Gerver’s sofa, unlike more familiar shapes, is defined in such a way that its area can’t be expressed in terms of known quantities (such as π or square roots). But for the moving sofa problem — a very simple question — it’s the optimal solution. The result illustrates that even the most straightforward optimization problems can have counterintuitively complicated answers.The first major progress on the moving sofa problem came in 1968, just two years after Moser posed it. John Hammersley connected two quarter circles with a rectangle, then cut a semicircle out of it to form a shape that resembled an old telephone. Its area was π/2 + 2/π, or approximately 2.2074.Hammersley also showed that any solution to the problem could have an area of at most $latex  2\sqrt{2}$, or about 2.8284.A couple of years later, Gerver, then a graduate student at the University of California, Berkeley, learned about the question. “Another grad student told me this problem and challenged me to find the answer,” he said. “He never said anything about it being unsolved. So I thought about it for a few days. Finally, I came back to him and said ‘OK, I give up. What’s the solution?’ And he refused to tell me! He said, ‘Just keep thinking about it. You’ll get it.’”Gerver thought about it sporadically over the next 20 years. But it wasn’t until 1990, when he mentioned it to the renowned mathematician John Conway, that he found out it had never been solved. The realization motivated him to spend more time with the problem, and he soon came up with a potential solution.Gerver’s sofa looked a lot like Hammersley’s telephone, but it was far more complicated to describe, consisting of 18 different pieces. (As it turned out, Ben Logan, an engineer at Bell Labs, independently uncovered the same shape but never published his work.) Some of the pieces were simple line segments and arcs. Others were more exotic, and tougher to describe.Still, Gerver suspected that this complicated shape was optimal: It possessed many features that mathematicians expected the optimal sofa to have, and he was able to prove that making small perturbations to its contours wouldn’t yield a suitable shape with a bigger area.]]></content:encoded></item><item><title>DataRobot Acquires Agnostic to Gain Distributed Covalent Platform for AI Apps</title><link>https://devops.com/datarobot-acquires-agnostic-to-gain-distributed-covalent-platform-for-ai-apps/</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 14 Feb 2025 15:22:34 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Hugo van Kemenade: Improving licence metadata</title><link>https://hugovk.dev/blog/2025/improving-licence-metadata/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 15:11:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[PEP 639 defines a spec on how to document licences
used in Python projects.Change  as follows.I usually use Hatchling as a build backend, and support was added in 1.27:Replace the freeform  field with a valid SPDX license expression, and add
 which points to the licence files in the repo. There’s often only one,
but if you have more than one, list them all:Optionally delete the deprecated licence classifier:Then make sure to use a PyPI uploader that supports this.pip can also show you the metadata:A lot of work went into this. Thank you to PEP authors
Philippe Ombredanne for creating the first draft in
2019, to C.A.M. Gerlach for the second draft in 2021,
and especially to Karolina Surma for getting the third
draft finish line and helping with the implementation.And many projects were updated to support this, thanks to the maintainers and
contributors of at least:]]></content:encoded></item><item><title>5 Tips for Building a Data Science Portfolio</title><link>https://www.kdnuggets.com/5-tips-building-data-science-portfolio</link><author>Nate Rosidi</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/Rosidi_5_Tips_for_Building_a_DS_Portfolio_4.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 15:00:25 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Not every data science portfolio is worth showcasing. Follow these five tips to build a portfolio that impresses employers and gets you a job.]]></content:encoded></item><item><title>Evolving Workflow Orchestration // Alex Milowski // #291</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/Evolving-Workflow-Orchestration--Alex-Milowski--291-e2us8at</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/98492189/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-14%2F394870366-44100-2-f81b1b5d49c1e.mp3" length="" type=""/><pubDate>Fri, 14 Feb 2025 14:39:03 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[ is a researcher, developer, , mathematician, and .Evolving Workflow Orchestration // MLOps Podcast #291 with Alex Milowski, Entrepreneur and Computer Scientist.// AbstractThere seems to be a shift from workflow languages to code - mostly annotation pythons - happening and getting us. It is a symptom of how complex workflow orchestration has gotten. Is it a dominant trend or will we cycle back to “DAG specifications”? At Stitchfix, we had our own DSL that “compiled” into airflow DAGs and at MicroByre, we used a external workflow langauge. Both had a batch task executor on K8s but at MicroByre, we had human and robot in the loop workflows.// BioDr. Milowski is a serial entrepreneur and computer scientist with experience in a variety of data and machine learning technologies. He holds a PhD in Informatics (Computer Science) from the University of Edinburgh, where he researched large-scale computation over scientific data. Over the years, he's spent many years working on various aspects of workflow orchestration in industry, standardization, and in research.// MLOps Swag/Merchhttps://shop.mlops.community/// Related LinksWebsite: https://www.milowski.com/ --------------- ✌️Connect With Us ✌️ -------------Join our slack community: https://go.mlops.community/slackFollow us on Twitter: @mlopscommunitySign up for the next meetup: https://go.mlops.community/registerCatch all episodes, blogs, newsletters, and more: https://mlops.community/Connect with Demetrios on LinkedIn: https://www.linkedin.com/in/dpbrinkm/Connect with Alex on LinkedIn: https://www.linkedin.com/in/alexmilowski/]]></content:encoded></item><item><title>Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11</title><link>https://www.youtube.com/watch?v=eHa6GhK7L0I</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/eHa6GhK7L0I?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 13:59:30 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Fwupd 2.0.6 Adds Support For HPE Gen10/Gen10+ Servers</title><link>https://www.phoronix.com/news/Fwupd-2.0.6-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 13:52:56 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Fwupd 2.0.6 is out today as the newest update to this widely-used open-source solution for system and peripheral device firmware updating under Linux...]]></content:encoded></item><item><title>Price Hikes, Enshittification Trigger 700K Customer Losses At Disney+, ESPN+</title><link>https://www.techdirt.com/2025/02/14/price-hikes-enshittification-trigger-700k-customer-losses-at-disney-espn/</link><author>Karl Bode</author><category>tech</category><pubDate>Fri, 14 Feb 2025 13:29:03 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[That has involved chasing pointless “growth of growth’s sake” megamergers and imposing bottomless price hikes and new annoying restrictions (like equating password sharing with “piracy”) — all while simultaneously cutting corners on product quality in a bid to give Wall Street that sweet, impossible, unlimited quarterly growth it demands.Disney+, Hulu, and ESPN+ (all now owned by the same company thanks to consolidation) all recently raised prices to access streaming catalogs of deteriorating quality. Some of Disney’s price hikes were as much as 25 percent, hitting ad-based and ad-free versions alike. Customers were quick to complain.So not surprisingly, Disney+ has now seen the first quarterly subscriber loss in the streaming platform’s history, with 700,000 customers cancelling service. ESPN+, ESPN’s streaming service, also saw a 700,000 subscriber loss:“Total paid Disney+ subscriptions currently rest at 124.6 million compared with 125.3 million at the end of the fiscal fourth quarter. ESPN+ also saw a loss of 700,000 subscribers, currently at 24.9 million, compared with 25.6 million at the end of last quarter.”Publicly-traded companies can’t just provide a quality, affordable service people like. That’s simply not allowed. They have to provide Wall Street ever-escalating quarterly returns in the pursuit of scale, even if that pursuit proves disastrous. If it’s not possible to achieve those returns through innovation and subscriber growth (which is no longer possible now that the streaming market is saturated), that’s when big companies get in trouble and start creatively nickel-and-diming their user base.Traditional cable TV, of course, went through this exact life cycle. And despite the fact many of those executives have shifted over to streaming, they’ve learned nothing from history or experience because they’re not financially incentivized to learn from experience. They’re incentivized to make stock values climb at any cost, then flee when things get rough; fat executive or investor compensation in hand.Which is to say don’t expect things to change, even if the economy tightens and customers increasingly balk at higher streaming video prices.Two, I suspect companies will work tirelessly to make cancelling streaming services (a major advantage over traditional bloated cable TV) more difficult, whether that means complicated wireless/broadband bundling that makes dumping services a confusing hassle (is your Hulu subscription discount tied to your Amazon or wireless bill?), or some creative new restrictions we haven’t seen previously.]]></content:encoded></item><item><title>Handwriting an SVG Heart, With Our Hearts</title><link>https://css-tricks.com/handwriting-an-svg-heart-with-our-hearts/</link><author>Ryan Trimble</author><category>frontend</category><pubDate>Fri, 14 Feb 2025 13:25:12 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[According to local grocery stores, it’s the Valentine’s Day season again, and what better way to express our love than with the symbol of love: . A while back on CSS-Tricks, we shared several ways to draw hearts, and the response was dreamy. Check out all these amazing, heart-filled submissions in this collection on CodePen:Temani Afif’s CSS Shapes site offers a super modern heart using only CSS:Now, to show  love, I wanted to do something personal, something crafty, something with a mild amount of effort.Handwriting a love note is a classic romantic gesture, but have you considered handwriting an SVG? We won’t need some fancy vector drawing tool to express our love. Instead, we can open a blank HTML document and add an  tag:We’ll need a way to see what we are doing inside the “SVG realm” (as I like to call it), which is what the  attribute provides. The 2D plane upon which vector graphics render is as infinite as our love, quite literally, complete with an x- and y-axis and all (like from math class).We’ll set the start coordinates as  and end coordinates as  to make a handsome, square . Oh, and by the way, we don’t concern ourselves over pixels,  values, or any other unit types; this is , and we play by our own rules.We add in these coordinates to the  as a string of values:<svg viewBox="0 0 10 10">

</svg>Now we can begin drawing our heart,  our heart. Let’s make a line. To do that, we’ll need to know a lot more about coordinates, and where to stick ’em. We’re able to draw a line with many points using the  element, which defines paths using the  attribute. SVG path commands are difficult to memorize, but the effort means you care. The path commands are:, , , , , , We’re only interested in drawing line segments for now, so together we’ll explore the first two:  and . MDN romantically describes MoveTo as picking up a drawing instrument, such as a pen or pencil: we aren’t yet drawing anything, just moving our pen to the point where we want to begin our confession of love.We’ll  the coordinates of  represented in the  attribute as :<svg viewBox="0 0 10 10">
  <path d="M2,2" />
</svg>Not surprising then to find that  is akin to putting pen to paper and  from one point to another. Let’s draw the first segment of our heart by drawing a  with coordinates , represented as  next in the  attribute:<svg viewBox="0 0 10 10">
  <path d="M2,2 L4,4" />
</svg>We’ll add a final line segment as another LineTo  with coordinates , again appended to the  attribute as :<svg viewBox="0 0 10 10">
  <path d="M2,2 L4,4 L6,2" />
</svg>If you stop to preview what we’ve accomplished so far, you may be confused as it renders an upside-down triangle; that’s not quite a heart yet, Let’s fix that.SVG shapes apply a  by default, which we can remove with :<svg viewBox="0 0 10 10">
  <path d="M2,2 L4,4 L6,2" fill="none" />
</svg>Rather than filling in the shape, instead, let’s display our line path by adding a , adding color to our heart.<svg viewBox="0 0 10 10">
  <path 
    d="M2,2 L4,4 L6,2" 
    fill="none" 
    stroke="rebeccapurple" />
</svg>Next, add some weight to the stroke by increasing the :<svg viewBox="0 0 10 10">
  <path 
    d="M2,2 L4,4 L6,2" 
    fill="none" 
    stroke="rebeccapurple" 
    stroke-width="4" />
</svg>Finally, apply a  of  (sorry, no time for  jokes) to round off the start and end points of our line path, giving us that classic symbol of love:<svg viewBox="0 0 10 10">
  <path 
    d="M2,2 L4,4 L6,2" 
    fill="none"
    stroke="rebeccapurple"
    stroke-width="4"
    stroke-linecap="round" />
</svg>. Now all that’s left to do is send it to that special someone. ]]></content:encoded></item><item><title>Show HN: Transform your codebase into a single Markdown doc for feeding into AI</title><link>https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html</link><author>tesserato</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 13:23:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.
The output for the current repository can be found here.Comprehensive Codebase Documentation: Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format. Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types). Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.Simple Command-Line Interface:  Offers an intuitive command-line interface with straightforward options for customization.If you have Go installed, run go install github.com/tesserato/CodeWeaver@latestto install the latest version of CodeWeaver or go install github.com/tesserato/CodeWeaver@vX.Y.Z to install a specific version.Alternatively, download the appropriate pre built executable from the releases page.If necessary, make the  executable by using the  command:The root directory to scan and document.The name of the output Markdown file.-ignore "<regex patterns>"Comma-separated list of regular expression patterns for paths to exclude.-included-paths-file <filename>File to save the list of paths that were included in the documentation.-excluded-paths-file <filename>File to save the list of paths that were excluded from the documentation.Display this help message and exit.Generate documentation for the current directory:This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).Specify a different input directory and output file:./codeweaver -dir=my_project -output=project_docs.md
This command will process the  directory and save the documentation to .Ignore specific file types and directories:./codeweaver -ignore="\.log,temp,build" -output=detailed_docs.md
This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.Save lists of included and excluded paths:./codeweaver -ignore="node_modules" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md
This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.CodeWeaver is released under the MIT License. See the  file for complete license details.]]></content:encoded></item><item><title>AI and Civil Service Purges</title><link>https://www.schneier.com/blog/archives/2025/02/ai-and-civil-service-purges.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 14 Feb 2025 13:03:22 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Donald Trump and Elon Musk’s chaotic approach to reform is upending government operations. Critical functions have been halted, tens of thousands of federal staffers are being encouraged to resign, and congressional mandates are being disregarded. The next phase: The Department of Government Efficiency reportedly wants to use AI to cut costs. According to , Musk’s group has started to run sensitive data from government systems through AI programs to analyze spending and determine what could be pruned. This may lead to the elimination of human jobs in favor of automation. As one government official who has been tracking Musk’s DOGE team told the, the ultimate aim is to use AI to replace “the human workforce with machines.” (Spokespeople for the White House and DOGE did not respond to requests for comment.)Using AI to make government more efficient is a worthy pursuit, and this is not a new idea. The Biden administration disclosed more than 2,000 AI applications in development across the federal government. For example, FEMA has started using AI to help perform damage assessment in disaster areas. The Centers for Medicare and Medicaid Services has started using AI to look for fraudulent billing. The idea of replacing dedicated and principled civil servants with AI agents, however, new—and complicated.The civil service—the massive cadre of employees who operate government agencies—plays a vital role in translating laws and policy into the operation of society. New presidents can issue sweeping executive orders, but they often have no real effect until they actually change the behavior of public servants. Whether you think of these people as essential and inspiring do-gooders, boring bureaucratic functionaries, or as agents of a “deep state,” their sheer number and continuity act as ballast that resists institutional change.This is why Trump and Musk’s actions are so significant. The more AI decision making is integrated into government, the easier change will be. If human workers are widely replaced with AI, executives will have unilateral authority to instantaneously alter the behavior of the government, profoundly raising the stakes for transitions of power in democracy. Trump’s unprecedented purge of the civil service might be the last time a president needs to replace the human beings in government in order to dictate its new functions. Future leaders may do so at the press of a button.To be clear, the use of AI by the executive branch doesn’t have to be disastrous. In theory, it could allow new leadership to swiftly implement the wishes of its electorate. But this could go very badly in the hands of an authoritarian leader. AI systems concentrate power at the top, so they could allow an executive to effectuate change over sprawling bureaucracies instantaneously. Firing and replacing tens of thousands of human bureaucrats is a huge undertaking. Swapping one AI out for another, or modifying the rules that those AIs operate by, would be much simpler.Social-welfare programs, if automated with AI, could be redirected to systematically benefit one group and disadvantage another with a single prompt change. Immigration-enforcement agencies could prioritize people for investigation and detainment with one instruction. Regulatory-enforcement agencies that monitor corporate behavior for malfeasance could turn their attention to, or away from, any given company on a whim.Even if Congress were motivated to fight back against Trump and Musk, or against a future president seeking to bulldoze the will of the legislature, the absolute power to command AI agents would make it easier to subvert legislative intent. AI has the power to diminish representative politics. Written law is never fully determinative of the actions of government—there is always wiggle room for presidents, appointed leaders, and civil servants to exercise their own judgment. Whether intentional or not, whether charitably or not, each of these actors uses discretion. In human systems, that discretion is widely distributed across many individuals—people who, in the case of career civil servants, usually outlast presidencies.Today, the AI ecosystem is dominated by a small number of corporations that decide how the most widely used AI models are designed, which data they are trained on, and which instructions they follow. Because their work is largely secretive and unaccountable to public interest, these tech companies are capable of making changes to the bias of AI systems—either generally or with aim at specific governmental use cases—that are invisible to the rest of us. And these private actors are both vulnerable to coercion by political leaders and self-interested in appealing to their favor. Musk himself created and funded xAI, now one of the world’s largest AI labs, with an explicitly ideological mandate to generate anti-“woke” AI and steer the wider AI industry in a similar direction.But there’s a second way that AI’s transformation of government could go. AI development could happen inside of transparent and accountable public institutions, alongside its continued development by Big Tech. Applications of AI in democratic governments could be focused on benefitting public servants and the communities they serve by, for example, making it easier for non-English speakers to access government services, making ministerial tasks such as processing routine applications more efficient and reducing backlogs, or helping constituents weigh in on the policies deliberated by their representatives. Such AI integrations should be done gradually and carefully, with public oversight for their design and implementation and monitoring and guardrails to avoid unacceptable bias and harm.Governments around the world are demonstrating how this could be done, though it’s early days. Taiwan has pioneered the use of AI models to facilitate deliberative democracy at an unprecedented scale. Singapore has been a leader in the development of public AI models, built transparently and with public-service use cases in mind. Canada has illustrated the role of disclosure and public input on the consideration of AI use cases in government. Even if you do not trust the current White House to follow any of these examples, U.S. states—which have much greater contact and influence over the daily lives of Americans than the federal government—could lead the way on this kind of responsible development and deployment of AI.As the political theorist David Runciman has written, AI is just another in a long line of artificial “machines” used to govern how people live and act, not unlike corporations and states before it. AI doesn’t replace those older institutions, but it changes how they function. As the Trump administration forges stronger ties to Big Tech and AI developers, we need to recognize the potential of that partnership to steer the future of democratic governance—and act to make sure that it does not enable future authoritarians.This essay was written with Nathan E. Sanders, and originally appeared in The Atlantic.]]></content:encoded></item><item><title>How Neurodiversity Can Help Your Life, Team &amp; Business • Katrine Hald Kjeldsen • GOTO 2024</title><link>https://www.youtube.com/watch?v=CIBlbmk1U_c</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/CIBlbmk1U_c?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 13:00:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at GOTO Copenhagen 2024. #GOTOcon #GOTOcph
https://gotocph.com

Katrine Hald Kjeldsen - Neuro-inclusion Specialist @katrinehaldkjeldsen6145 

ORIGINAL TALK TITLE
Change Your Life, Help Your Team & Boost Your Business with Neurodiversity

RESOURCES
https://www.linkedin.com/in/katrinekjeldsen

ABSTRACT
Are you a little spicy? If not, your colleague most probably is. Either way, this talk is for you.

In this session, Katrine will shine a light on neurodiversity and the transformative impact it can have on your life, your team, and your business.

Neurodiversity is not only abundantly prevalent in tech organizations; it is a powerful, often underutilized asset that manifests in every corner of our work environments. It is paramount that we adjust our understanding and strategies around this diversity to thrive. Katrine will explore how neurodiversity shows up in the most unexpected ways and why it’s both surprisingly easy and surprisingly challenging to harness its potential in team collaboration and leadership.

This talk will emphasize practical daily strategies to strengthen collaboration and leverage the advantages of neurodiversity in everyday work contexts, whether it’s adopting clear communication methods, allowing flexible work arrangements, or designing team workflows that play to diverse cognitive strengths. [...]

TIMECODES
00:00 Intro
04:58 Neurodivergent
16:37 Dopamine & ADHD
21:25 Autism
24:47 Diverse perspective
34:40 Conclusions
42:52 Outro

Download slides and read the full abstract here:
https://gotocph.com/2024/sessions/3472

RECOMMENDED BOOKS
Ph.D. Thomas Armstrong • The Power of Neurodiversity • https://amzn.to/3OrbPOg
Maureen Dunne • The Neurodiversity Edge • https://amzn.to/494dGSE
Emily Kircher-Morris & Amanda Morin • Neurodiversity-Affirming Schools • https://amzn.to/3ZjJRdI
Nancy Doyle & Almuth McDowall • Neurodiversity Coaching • https://amzn.to/4fGDdnn

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#Neurodiversity #Neurodivergent #Autism #ADHD #ParallelPlay #DeepCuriosity #Diversity #Inclusivity #Autism #DiverseTeams #Teams #KatrineHaldKjeldsen

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Fake BSOD Delivered by Malicious Python Script, (Fri, Feb 14th)</title><link>https://isc.sans.edu/diary/rss/31686</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Fri, 14 Feb 2025 12:18:05 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[I found a Python script that implements a funny anti-analysis trick. The script has a low score on VT (4/59) (SHA256:d716c2edbcdb76c6a6d31b21f154fee7e0f8613617078b69da69c8f4867c9534)[1]. This sample attracted my attention because it uses the tkinter[2] library. This library is used to create graphical user interfaces (GUIs). It provides tools to create windows, dialogs, buttons, labels, text fields, and other interactive elements, allowing developers to build desktop applications with visual interfaces in Python. Most Python scripts are intended to be executed from a command line. That's why I consider this library as a good sign of suspicious behavior (It does not mean that all Python scripts using this library are malicious!)While reviewing the script, a variable contains an interesting piece of text:info = "\nA problem has been detected and windows has been shut down to prevent damage\nto your computer ... (removed) ..."The interesting piece of code is here:root = tk.Tk()
root.configure(background="dark blue")
ex = Example(root)
w, h = root.winfo_screenwidth(), root.winfo_screenheight()
root.overrideredirect(1)
root.geometry("%dx%d+0+0" % (w, h))
root.wm_attributes("-topmost", 1)
root.mainloop()
The attribute "-topmost" set to "1" or "TRUE" will make the window remain open on top of all windows. The window will also be created to fill the screen without any control elements to close or resize it. When the script is executed, you will get this screen:To be honest, that's not the best BSOD ("Blue Screen of Death") that I saw... but it's a nice trick to annoy the victim or slow down (a bit) the analysis of the file.Xavier Mertens (@xme)
Xameco
Senior ISC Handler - Freelance Cyber Security ConsultantPGP Key

 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>Show HN: A New Way to Learn Languages</title><link>https://www.langturbo.com/</link><author>sebnun</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 12:08:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ubuntu Making Progress On Replacing initramfs-tools With Dracut</title><link>https://www.phoronix.com/news/Ubuntu-Dracut-Still-WIP</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 12:00:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As a follow-up to the news from last October of Ubuntu considering Dracut to replace initramfs-tools for initrd generation, that work remains ongoing with some improvements since having been prepared for the upcoming Ubuntu 25.04 release but it remains overall an active affair...]]></content:encoded></item><item><title>Real Python: The Real Python Podcast – Episode #239: Behavior-Driven vs Test-Driven Development &amp;amp; Using Regex in Python</title><link>https://realpython.com/podcasts/rpp/239/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 12:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[What is behavior-driven development, and how does it work alongside test-driven development? How do you communicate requirements between teams in an organization? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.]]></content:encoded></item><item><title>GNOME Software May Eventually Drop RPM Support In Favor Of Flatpaks</title><link>https://www.phoronix.com/news/GNOME-Software-RPM-Flatpak</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:37:56 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Stemming from the ongoing discussion around the issues raised with Fedora's Flatpak package of OBS Studio and how Flatpaks should be prioritized within the GNOME Software app center/store, the future of RPM support within GNOME Software raised...]]></content:encoded></item><item><title>Show Your Love For Linux Hardware Coverage This Valentine&apos;s Day</title><link>https://www.phoronix.com/news/Phoronix-Premium-Valentine-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:30:02 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[If you wish to show your appreciation for all of the Linux hardware reviews, Linux benchmarking, and open-source news provided on Phoronix each and every day, you can join Phoronix Premium this Valentine's Day weekend at a discounted rate...]]></content:encoded></item><item><title>Vulkan 1.4.308 Brings NVIDIA&apos;s Provisional Present Metering Extension</title><link>https://www.phoronix.com/news/Vulkan-1.4.308-Present-Metering</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:14:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Vulkan 1.4.308 was quietly released last week and besides a few fixes what makes it interesting is the provisional VK_NV_present_metering extension...]]></content:encoded></item><item><title>Valkey 8.1-rc1 Delivers Fresh Performance Improvements</title><link>https://www.phoronix.com/news/Valkey-8.1-rc1</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 11:05:55 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Valkey as the open-source in-memory store forked from Redis is preparing for its next feature release...]]></content:encoded></item><item><title>Alaa Abd El Fattah&apos;s Mother, Laila Soueif, Calls on UK Government to Help as She Continues Hunger Strike</title><link>https://www.eff.org/deeplinks/2025/02/alaa-abd-el-fattahs-mother-laila-soueif-calls-uk-government-help-hunger-strike</link><author>Karen Gullo</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/offline-alaa.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 10:48:50 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Learning Microsoft Azure • Jonah Andersson &amp; Håkan Silfvernagel</title><link>https://www.youtube.com/watch?v=gEQZBmoN8LM</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/gEQZBmoN8LM?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 10:41:00 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This interview was recorded for the GOTO Book Club.
http://gotopia.tech/bookclub

Read the full transcription of the interview here
 (https://gotopia.tech/episodes/339) 
Jonah Andersson - Senior Azure Consultant at Solidify & Author of "Learning Microsoft Azure"
Håkan Silfvernagel - Senior AI Architect at Sopra Steria & Microsoft AI MVP

RESOURCES
Jonah
https://x.com/cjkodare
https://medium.com/@jonah.andersson
https://www.linkedin.com/in/jonahandersson
https://github.com/jonahandersson
https://jonahandersson.tech

Håkan
https://x.com/agrevlis
https://www.linkedin.com/in/hakansilfvernagel
https://sessionize.com/hakan-silfvernagel

Links
https://www.meetup.com/azureusergroupsundsvallsverige
https://podcasters.spotify.com/pod/show/extendwomenintechpodcast

DESCRIPTION
Jonah Andersson, interviewed by Håkan Silfvernagel, shared her experiences writing "Learning Microsoft Azure". With a focus on making the content accessible for beginners, she carefully structured the book to cover essential Azure topics such as cloud computing fundamentals, security, and AI, while drawing on insights from experts in her network, including Microsoft Certified Trainers and MVPs.

Over nearly two years, Jonah Andersson navigated the challenges of a rapidly evolving tech landscape, incorporating emerging trends like generative AI. Passionate about fostering inclusivity in tech, she emphasized the importance of mentorship and community support for underrepresented groups, particularly women. Looking ahead, Jonah Andersson expressed a desire to write a biography that reflects her life lessons and professional journey in technology.

RECOMMENDED BOOKS
Jonah Andersson • Learning Microsoft Azure
 (https://amzn.to/48cOw3T) Adrienne Braganza Tacke • "Looks Good to Me": Constructive Code Reviews
 (https://amzn.to/3E75XrD) Adrienne Braganza Tacke • Coding for Kids
 (https://amzn.to/3Ukd5Gw) Martin Fowler • Refactoring
 (https://amzn.to/3EVcHXQ) Matthew Skelton & Manuel Pais • Team Topologies (http://amzn.to/3sVLyLQ) 


Bluesky (https://bsky.app/profile/gotocon.com) 
Twitter (https://twitter.com/GOTOcon) 
Instagram (https://www.instagram.com/goto_con) 
LinkedIn (https://www.linkedin.com/company/goto-) 
Facebook (https://www.facebook.com/GOTOConferences) 

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket: gotopia.tech (https://gotopia.tech) 

SUBSCRIBE TO OUR YOUTUBE CHANNEL (https://www.youtube.com/user/GotoConferences/?sub_confirmation=1)  - new videos posted daily!]]></content:encoded></item><item><title>Daniel Roy Greenfeld: Building a playing card deck</title><link>https://daniel.feldroy.com/posts/2025-02-deck-of-cards</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Fri, 14 Feb 2025 09:50:04 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Today is Valentine's Day. That makes it the perfect day to write a blog post about showing how to not just build a deck of cards, but show off cards from the heart suite.]]></content:encoded></item><item><title>Anyone Can Push Updates to the DOGE.gov Website</title><link>https://www.404media.co/anyone-can-push-updates-to-the-doge-gov-website-2/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/Screen-Shot-2025-02-13-at-10.38.20-PM.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 06:42:14 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[The doge.gov website that was spun up to track Elon Musk’s cuts to the federal government is insecure and pulls from a database that can be edited by anyone, according to two separate people who found the vulnerability and shared it with 404 Media. One coder added at least two database entries that are visible on the live site and say “this is a joke of a .gov site” and “THESE ‘EXPERTS’ LEFT THEIR DATABASE OPEN -roro.” Doge.gov was hastily deployed after Elon Musk  that his Department of Government Efficiency is “trying to be as transparent as possible. In fact, our actions—we post our actions to the DOGE handle on X, and to the DOGE website.” At the time, DOGE was an essentially blank webpage. It was built out further Wednesday and Thursday, and now shows a mirror of the @DOGE X account posts, as well as various stats about the U.S. government’s federal workforce. Two different web development experts who asked to remain anonymous because they were probing a federal website told 404 Media that doge.gov is seemingly built on a Cloudflare Pages site that is not currently hosted on government servers. The database it is pulling from can be and has been written to by third parties, and will show up on the live website. Both sources told 404 Media that they noticed Doge.gov is pulling from a Cloudflare Pages website, where the code that runs it is actually deployed.]]></content:encoded></item><item><title>ChatLoopBackOff Episode 46 (Dragonfly)</title><link>https://www.youtube.com/watch?v=gd6HRgr8KcA</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/gd6HRgr8KcA?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 05:56:56 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. 

This CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.]]></content:encoded></item><item><title>TrueNAS 25.04 &quot;Fangtooth&quot; Beta Unifies Linux SCALE &amp; FreeBSD CORE Efforts</title><link>https://www.phoronix.com/news/TrueNAS-25.04-Beta</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 05:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[TrueNAS 25.04 beta was released on Thursday as another step toward unifying the TrueNAS CORE OS derived from FreeBSD and the Linux-based TrueNAS SCALE...]]></content:encoded></item><item><title>Mississippi’s Top Court Says Rights Violations Are OK If Cops Don’t Know How To Do Their Jobs</title><link>https://www.techdirt.com/2025/02/13/mississippis-top-court-says-rights-violations-are-ok-if-cops-dont-know-how-to-do-their-jobs/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 14 Feb 2025 04:10:01 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[There are things you can and can’t do when setting up checkpoints. If it’s DUI enforcement, you can talk to drivers and see if they seem intoxicated. If it’s near a border, you can stop every vehicle to search for undocumented immigrants or contraband.What you can’t do, however, is just set up a checkpoint to stop all cars just to eyeball the interiors and question occupants in hopes of discovering contraband or evidence of illegal activity. That’s pretty much the same thing as entering every house on the block to search for criminal evidence. Just because cars aren’t houses and are traveling on public roads doesn’t give the government the right to engage in some “general rummaging.” In a 6-3 opinion delivered by Justice Sandra Day O’Connor, the Court held that because the checkpoint program’s primary purpose was indistinguishable from the general interest in crime control, the checkpoints violated the Fourth Amendment. “We cannot sanction stops justified only by the generalized and ever-present possibility that interrogation and inspection may reveal that any given motorist has committed some crime,” wrote Justice O’Connor.The city argued it had a justifiable reason for these checkpoints — during which occupants were questioned and drug dogs performed open-air sniffs: drug interdiction. Not good enough, said the Supreme Court. And yet, law enforcement agencies seem convinced they can still do this sort of thing. They can’t. Not legally. But they  get away with it, especially if they hand the job to the officer with the least amount of experience. Once again, ignorance of the law is the best excuse… at least if you’re a cop. That’s the upshot of this Mississippi Supreme Court decision [PDF], brought to our attention by FourthAmendment.com. In this case, two Puckett, MS police officers set up a “safety checkpoint” at a four-way intersection connecting two local highways. Lavern Kendrick was stopped and questioned by Officer Adam McGuffee. McGuffee claimed Kendrick was nervous, that he smelled “burnt plastic” which he took to be indicative of meth use, and happened to see a meth pipe inside Kendrick’s car. One thing led to another, which led to Kendrick’s arrest, and this suppression motion. The trial court saw no problem with any of this, even though Officer McGuffee testified that this checkpoint was mostly exploratory, allowing the officer to check licenses, registration, insurance, and “then whatever is seen in the vehicles.” That alone should have put it out of constitutional bounds, especially 23 years after the Supreme Court’s ruling in .But the lower court decided the cop was just too ignorant to understand what he was doing or saying, and credited the officer’s inexperience, rather than decide it was a clear and obvious violation of the Fourth Amendment.Following the hearing, the trial court acknowledged that Officer McGuffee’s testimony “was a little bit all over the board about the purpose for the stop.” The trial court stated that it was not surprised by this because Officer McGuffee“…was the most inexperienced law enforcement officer for the Town of Puckett. That’s not his fault. That’s just the nature of what it is. He started in January of 2023. He’s a part-time worker. He does other work primarily. This is a part-time job for him.“In other words, the less a cop knows, the more they can get away with. Kendrick argued more logically that McGuffee’s testimony during the trial made it clear the real purpose of the checkpoint was “general crime control,” rather than just ensuring drivers were properly licensed and insured. Unfortunately, the state’s top court agrees with the lower court: inexperience is a halo. We find that the record supports the trial court’s conclusion. With permission from the chief of police, Officer McGuffee and Officer Lathum set up a safety checkpoint, the primary purpose of which was to check that each driver passing through it was properly licensed and insured. Evidence was presented that this was a routine safety checkpoint and that every vehicle passing through was briefly stopped.The primary purpose of checking driver’s licenses and insurance cards substantially outweighed the minimal intrusion of Kendrick’s individual liberty. And the safety checkpoint did not violate the search-and-seizure provision of either the Fourth Amendment or the state constitution.And there it is: if a cop doesn’t have the experience to do this job properly, he can just blunder around and allow the court to undo the constitutional damage he has done… at least in Mississippi. My guess is these “safety checkpoints” are really the kind outlawed by the Supreme Court’s  decision but McGuffee just hadn’t been on the job long enough to know to how to better hide this fact from trial courts while testifying. He did his job just poorly enough to salvage an illicit possession/DUI bust. And the state’s law enforcement get their free pass on shady checkpoints extended.]]></content:encoded></item><item><title>Building a Data Engineering Center of Excellence</title><link>https://towardsdatascience.com/building-a-data-engineering-center-of-excellence/</link><author>Richie Bachala</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 02:35:48 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[As data continues to grow in importance and become more complex, the need for skilled data engineers has never been greater. But what is data engineering, and why is it so important? In this blog post, we will discuss the essential components of a functioning data engineering practice and why data engineering is becoming increasingly critical for businesses today, and how you can build your very own Data Engineering Center of Excellence!I’ve had the privilege to build, manage, lead, and foster a sizeable high-performing team of data warehouse & ELT engineers for many years. With the help of my team, I have spent a considerable amount of time every year consciously planning and preparing to manage the growth of our data month-over-month and address the changing reporting and analytics needs for our 20000+ global data consumers. We built many data warehouses to store and centralize massive amounts of data generated from many OLTP sources. We’ve implemented Kimball methodology by creating star schemas both within our on-premise data warehouses and in the ones in the cloud.The objective is to enable our user-base to perform fast analytics and reporting on the data; so our analysts’ community and business users can make accurate data-driven decisions.It took me about three years to transform  () of data warehouse and ETL programmers into one cohesive Data Engineering team.I have compiled some of my learnings building a global data engineering team in this post in hopes that Data professionals and leaders of all levels of technical proficiency can benefit.Evolution of the Data EngineerIt has never been a better time to be a data engineer. Over the last decade, we have seen a massive awakening of enterprises now recognizing their data as the company’s heartbeat, making data engineering the job function that ensures accurate, current, and quality data flow to the solutions that depend on it.Historically, the role of Data Engineers has evolved from that of data warehouse developers and the  (extract, transform and load).The data warehouse developers are responsible for designing, building, developing, administering, and maintaining data warehouses to meet an enterprise’s reporting needs. This is done primarily via extracting data from operational and transactional systems and piping it using extract transform load methodology (ETL/ ELT) to a storage layer like a data warehouse or a data lake. The data warehouse or the data lake is where data analysts, data scientists, and business users consume data. The developers also perform transformations to conform the ingested data to a data model with aggregated data for easy analysis.A data engineer’s prime responsibility is to produce and make data securely available for multiple consumers.Data engineers oversee the ingestion, transformation, modeling, delivery, and movement of data through every part of an organization. Data extraction happens from many different data sources & applications. Data Engineers load the data into data warehouses and data lakes, which are transformed not just for the Data Science & predictive analytics initiatives (as everyone likes to talk about) but primarily for data analysts. Data analysts & data scientists perform operational reporting, exploratory analytics, service-level agreement (SLA) based business intelligence reports and dashboards on the catered data. In this book, we will address all of these job functions.The role of a data engineer is to acquire, store, and aggregate data from both cloud and on-premise, new, and existing systems, with data modeling and feasible data architecture. Without the data engineers, analysts and data scientists won’t have valuable data to work with, and hence, data engineers are the first to be hired at the inception of every new data team. Based on the data and analytics tools available within an enterprise, data engineering teams’ role profiles, constructs, and approaches have several options for what should be included in their responsibilities which we will discuss in this chapter.Software is increasingly automating the historically manual and tedious tasks of data engineers. Data processing tools and technologies have evolved massively over several years and will continue to grow. For example, cloud-based data warehouses (Snowflake, for instance) have made data storage and processing affordable and fast. Data pipeline services (like Informatica IICS, Apache Airflow, Matillion, Fivetran) have turned data extraction into work that can be completed quickly and efficiently. The data engineering team should be leveraging such technologies as force multipliers, taking a consistent and cohesive approach to integration and management of enterprise data, not just relying on legacy siloed approaches to building custom data pipelines with fragile, non-performant, hard to maintain code. Continuing with the latter approach will stifle the pace of innovation within the said enterprise and force the future focus to be around managing data infrastructure issues rather than how to help generate value for your business.The primary role of an enterprise Data Engineering team should be to  into a shape that’s ready for analysis — laying the foundation for real-world analytics and data science application.The Data Engineering team should serve as the  for enterprise-level data with the responsibility to curate the organization’s data and act as a resource for those who want to make use of it, such as Reporting & Analytics teams, Data Science teams, and other groups that are doing more self-service or business group driven analytics leveraging the enterprise data platform. This team should serve as the  of organizational knowledge, managing and refining the catalog so that analysis can be done more effectively. Let’s look at the essential responsibilities of a well-functioning Data Engineering team.Responsibilities of a Data Engineering TeamThe Data Engineering team should provide a  within the enterprise that cuts across to support both the Reporting/Analytics and Data Science capabilities to provide access to clean, transformed, formatted, scalable, and secure data ready for analysis. The Data Engineering teams’ core responsibilities should include:· Build, manage, and optimize the core data platform infrastructure· Build and maintain custom and off-the-shelf data integrations and ingestion pipelines from a variety of structured and unstructured sources· Manage overall data pipeline orchestration· Manage transformation of data either before or after load of raw data through both technical processes and business logic· Support analytics teams with design and performance optimizations of data warehousesData is an Enterprise Asset.Data as an Asset should be shared and protected.Data should be valued as an Enterprise asset, leveraged across all Business Units to enhance the company’s value to its respective customer base by accelerating decision making, and improving competitive advantage with the help of data. Good data stewardship, legal and regulatory requirements dictate that we protect the data owned from unauthorized access and disclosure.In other words, managing Security is a crucial responsibility.Why Create a Centralized Data Engineering Team?Treating Data Engineering as a standard and core capability that underpins both the Analytics and Data Science capabilities will help an enterprise evolve how to approach Data and Analytics. The enterprise needs to stop vertically treating data based on the technology stack involved as we tend to see often and move to more of a horizontal approach of managing a  or  that cuts across the organization and can connect to various technologies as needed drive analytic initiatives. This is a new way of thinking and working, but it can drive efficiency as the various data organizations look to scale. Additionally — there is value in creating a dedicated structure and career path for Data Engineering resources. Data engineering skill sets are in high demand in the market; therefore, hiring outside the company can be costly. Companies must enable programmers, database administrators, and software developers with a career path to gain the needed experience with the above-defined skillsets by working across technologies. Usually, forming a data engineering center of excellence or a capability center would be the first step for making such progression possible.Challenges for creating a centralized Data Engineering TeamThe centralization of the Data Engineering team as a service approach is different from how Reporting & Analytics and Data Science teams operate. It does, in principle, mean giving up some level of control of resources and establishing new processes for how these teams will collaborate and work together to deliver initiatives.The Data Engineering team will need to demonstrate that it can effectively support the needs of both Reporting & Analytics and Data Science teams, no matter how large these teams are. Data Engineering teams must effectively prioritize workloads while ensuring they can bring the right skillsets and experience to assigned projects.Data engineering is essential because it serves as the backbone of data-driven companies. It enables analysts to work with clean and well-organized data, necessary for deriving insights and making sound decisions. To build a functioning data engineering practice, you need the following critical components:The Data Engineering team should be a core capability within the enterprise, but it should effectively serve as a support function involved in almost everything data-related. It should interact with the Reporting and Analytics and Data Science teams in a collaborative support role to make the entire team successful.The Data Engineering team doesn’t create direct business value — but the value should come in making the Reporting and Analytics, and Data Science teams more productive and efficient to ensure delivery of maximum value to business stakeholders through Data & Analytics initiatives. To make that possible, the six key responsibilities within the data engineering capability center would be as follow –Let’s review the 6 pillars of responsibilities:1. Determine Central Data Location for Collation and WranglingUnderstanding and having a strategy for a (a centralized data repository or data warehouse for the mass consumption of data for analysis). Defining requisite data tables and where they will be joined in the context of data engineering and subsequently converting raw data into digestible and valuable formats.2. Data Ingestion and TransformationMoving data from one or more sources to a new destination (your data lake or cloud data warehouse) where it can be stored and further analyzed and then converting data from the format of the source system to that of the destinationExtracting, transforming, and loading data from one or more sources into a destination system to represent the data in a new context or style.Data modeling is an essential function of a data engineering team, granted not all data engineers excel with this capability. Formalizing relationships between data objects and business rules into a conceptual representation through understanding information system workflows, modeling required queries, designing tables, determining primary keys, and effectively utilizing data to create informed output.I’ve seen engineers in interviews mess up more with this than coding in technical discussions. It’s essential to understand the differences between Dimensions, Facts, Aggregate tables.Ensuring that sensitive data is protected and implementing proper authentication and authorization to reduce the risk of a data breach6. Architecture and AdministrationDefining the models, policies, and standards that administer what data is collected, where and how it is stored, and how it such data is integrated into various analytical systems.The six pillars of responsibilities for data engineering capabilities center on the ability to determine a central data location for collation and wrangling, ingest and transform data, execute ETL/ELT operations, model data, secure access and administer an architecture. While all companies have their own specific needs with regards to these functions, it is important to ensure that your team has the necessary skillset in order to build a foundation for big data success.Besides the Data Engineering following are the other capability centers that need to be considered within an enterprise:Analytics Capability CenterThe analytics capability center enables consistent, effective, and efficient BI, analytics, and advanced analytics capabilities across the company. Assist business functions in triaging, prioritizing, and achieving their objectives and goals through reporting, analytics, and dashboard solutions, while providing operational reports and visualizations, self-service analytics, and required tools to automate the generation of such insights.Data Science Capability CenterThe data science capability center is for exploring cutting-edge technologies and concepts to unlock new insights and opportunities, better inform employees and create a culture of prescriptive information usage using Automated AI and Automated ML solutions such as H2O.ai, Dataiku, Aible, DataRobot, C3.aiThe data governance office empowers users with trusted, understood, and timely data to drive effectiveness while keeping the integrity and sanctity of data in the right hands for mass consumption.As your company grows, you will want to make sure that the data engineering capabilities are in place to support the six pillars of responsibilities. By doing this, you will be able to ensure that all aspects of data management and analysis are covered and that your data is safe and accessible by those who need it. Have you started thinking about how your company will grow? What steps have you taken to put a centralized data engineering team in place?]]></content:encoded></item><item><title>ISC Stormcast For Friday, February 14th, 2025 https://isc.sans.edu/podcastdetail/9324, (Fri, Feb 14th)</title><link>https://isc.sans.edu/diary/rss/31684</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Fri, 14 Feb 2025 02:00:02 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[
 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>OBS Studio Raises Issues With Fedora&apos;s Flatpak Package</title><link>https://www.phoronix.com/news/OBS-Studio-Poor-Fedora-Flatpak</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 14 Feb 2025 01:39:52 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The OBS Studio open-source screencasting and streaming app has called out Fedora's poor Flatpak packaging of the application and is threatening as going as far as legal action if it isn't addressed...]]></content:encoded></item><item><title>Risky Biz Soap Box: Run your own open source IDP with Authentik</title><link>https://risky.biz/soapbox93/</link><author></author><category>Risky Business blog</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/media3.risky.biz/soapbox93.mp3" length="" type=""/><pubDate>Fri, 14 Feb 2025 00:24:24 +0000</pubDate><source url="https://risky.biz/">Risky Business</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Kubernetes 1.31
completed the largest migration in Kubernetes history, removing the in-tree
cloud provider. While the component migration is now done, this leaves some additional
complexity for users and installer projects (for example, kOps or Cluster API) . We will go
over those additional steps and failure points and make recommendations for cluster owners.
This migration was complex and some logic had to be extracted from the core components,
building four new subsystems.One of the most critical functionalities of the cloud controller manager is the node controller,
which is responsible for the initialization of the nodes.As you can see in the following diagram, when the  starts, it registers the 
object with the apiserver, Tainting the node so it can be processed first by the
cloud-controller-manager. The initial  is missing the cloud-provider specific information,
like the Node Addresses and the Labels with the cloud provider specific information like the
Node, Region and Instance type information.sequenceDiagram
autonumber
rect rgb(191, 223, 255)
Kubelet->>+Kube-apiserver: Create Node
Note over Kubelet: Taint: node.cloudprovider.kubernetes.io
Kube-apiserver->>-Kubelet: Node Created
end
Note over Kube-apiserver: Node is Not Ready Tainted, Missing Node Addresses*, ...
Note over Kube-apiserver: Send Updates
rect rgb(200, 150, 255)
Kube-apiserver->>+Cloud-controller-manager: Watch: New Node Created
Note over Cloud-controller-manager: Initialize Node:Cloud Provider Labels, Node Addresses, ...
Cloud-controller-manager->>-Kube-apiserver: Update Node
end
Note over Kube-apiserver: Node is Ready
This new initialization process adds some latency to the node readiness. Previously, the kubelet
was able to initialize the node at the same time it created the node. Since the logic has moved
to the cloud-controller-manager, this can cause a chicken and egg problem
during the cluster bootstrapping for those Kubernetes architectures that do not deploy the
controller manager as the other components of the control plane, commonly as static pods,
standalone binaries or daemonsets/deployments with tolerations to the taints and using
 (more on this below)Examples of the dependency problemAs noted above, it is possible during bootstrapping for the cloud-controller-manager to be
unschedulable and as such the cluster will not initialize properly. The following are a few
concrete examples of how this problem can be expressed and the root causes for why they might
occur.These examples assume you are running your cloud-controller-manager using a Kubernetes resource
(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods
rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it
will schedule properly.Example: Cloud controller manager not scheduling due to uninitialized taintAs noted in the Kubernetes documentation, when the kubelet is started with the command line
flag --cloud-provider=external, its corresponding  object will have a no schedule taint
named node.cloudprovider.kubernetes.io/uninitialized added. Because the cloud-controller-manager
is responsible for removing the no schedule taint, this can create a situation where a
cloud-controller-manager that is being managed by a Kubernetes resource, such as a 
or , may not be able to schedule.If the cloud-controller-manager is not able to be scheduled during the initialization of the
control plane, then the resulting  objects will all have the
node.cloudprovider.kubernetes.io/uninitialized no schedule taint. It also means that this taint
will not be removed as the cloud-controller-manager is responsible for its removal. If the no
schedule taint is not removed, then critical workloads, such as the container network interface
controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.Example: Cloud controller manager not scheduling due to not-ready taintThe next example would be possible in situations where the container network interface (CNI) is
waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not
tolerated the taint which would be removed by the CNI."The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly."One of the conditions that can lead to a  resource having this taint is when the container
network has not yet been initialized on that node. As the cloud-controller-manager is responsible
for adding the IP addresses to a  resource, and the IP addresses are needed by the container
network controllers to properly configure the container network, it is possible in some
circumstances for a node to become stuck as not ready and uninitialized permanently.This situation occurs for a similar reason as the first example, although in this case, the
node.kubernetes.io/not-ready taint is used with the no execute effect and thus will cause the
cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is
not able to execute, then it will not initialize the node. It will cascade into the container
network controllers not being able to run properly, and the node will end up carrying both the
node.cloudprovider.kubernetes.io/uninitialized and node.kubernetes.io/not-ready taints,
leaving the cluster in an unhealthy state.There is no one “correct way” to run a cloud-controller-manager. The details will depend on the
specific needs of the cluster administrators and users. When planning your clusters and the
lifecycle of the cloud-controller-managers please consider the following guidance:For cloud-controller-managers running in the same cluster, they are managing.Use host network mode, rather than the pod network: in most cases, a cloud controller manager
will need to communicate with an API service endpoint associated with the infrastructure.
Setting “hostNetwork” to true will ensure that the cloud controller is using the host
networking instead of the container network and, as such, will have the same network access as
the host operating system. It will also remove the dependency on the networking plugin. This
will ensure that the cloud controller has access to the infrastructure endpoint (always check
your networking configuration against your infrastructure provider’s instructions).Use a scalable resource type.  and  are useful for controlling the
lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy
as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using
these primitives to control the lifecycle of your cloud controllers and running multiple
replicas, you must remember to enable leader election, or else your controllers will collide
with each other which could lead to nodes not being initialized in the cluster.Target the controller manager containers to the control plane. There might exist other
controllers which need to run outside the control plane (for example, Azure’s node manager
controller). Still, the controller managers themselves should be deployed to the control plane.
Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the
control plane to ensure that they are running in a protected space. Cloud controllers are vital
to adding and removing nodes to a cluster as they form a link between Kubernetes and the
physical infrastructure. Running them on the control plane will help to ensure that they run
with a similar priority as other core cluster controllers and that they have some separation
from non-privileged user workloads.
It is worth noting that an anti-affinity stanza to prevent cloud controllers from running
on the same host is also very useful to ensure that a single node failure will not degrade
the cloud controller performance.Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud
controller container to ensure that it will schedule to the correct nodes and that it can run
in situations where a node is initializing. This means that cloud controllers should tolerate
the node.cloudprovider.kubernetes.io/uninitialized taint, and it should also tolerate any
taints associated with the control plane (for example, node-role.kubernetes.io/control-plane
or node-role.kubernetes.io/master). It can also be useful to tolerate the
node.kubernetes.io/not-ready taint to ensure that the cloud controller can run even when the
node is not yet available for health monitoring.For cloud-controller-managers that will not be running on the cluster they manage (for example,
in a hosted control plane on a separate cluster), then the rules are much more constrained by the
dependencies of the environment of the cluster running the cloud-controller-manager. The advice
for running on a self-managed cluster may not be appropriate as the types of conflicts and network
constraints will be different. Please consult the architecture and requirements of your topology
for these scenarios.This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is
important to note that this is for demonstration purposes only, for production uses please
consult your cloud provider’s documentation.apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
name: cloud-controller-manager
namespace: kube-system
spec:
replicas: 2
selector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
strategy:
type: Recreate
template:
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
annotations:
kubernetes.io/description: Cloud controller manager for my infrastructure
spec:
containers: # the container details will depend on your specific cloud controller manager
- name: cloud-controller-manager
command:
- /bin/my-infrastructure-cloud-controller-manager
- --leader-elect=true
- -v=1
image: registry/my-infrastructure-cloud-controller-manager@latest
resources:
requests:
cpu: 200m
memory: 50Mi
hostNetwork: true # these Pods are part of the control plane
nodeSelector:
node-role.kubernetes.io/control-plane: ""
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- topologyKey: "kubernetes.io/hostname"
labelSelector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
tolerations:
- effect: NoSchedule
key: node-role.kubernetes.io/master
operator: Exists
- effect: NoExecute
key: node.kubernetes.io/unreachable
operator: Exists
tolerationSeconds: 120
- effect: NoExecute
key: node.kubernetes.io/not-ready
operator: Exists
tolerationSeconds: 120
- effect: NoSchedule
key: node.cloudprovider.kubernetes.io/uninitialized
operator: Exists
- effect: NoSchedule
key: node.kubernetes.io/not-ready
operator: Exists
When deciding how to deploy your cloud controller manager it is worth noting that
cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple
replicas of a cloud controller manager is good practice for ensuring high-availability and
redundancy, but does not contribute to better performance. In general, only a single instance
of a cloud controller manager will be reconciling a cluster at any given time.]]></content:encoded></item><item><title>It&apos;s time to go ESM-only</title><link>https://javascriptweekly.com/issues/723</link><author></author><category>dev</category><category>frontend</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://javascriptweekly.com/">Javascript Weekly</source><content:encoded><![CDATA[🤯 Pfft, vehicle data is a joke without CarsXE. API goes brrrrr. VIN decoding, plate lookup, market value reports. Get Serious!]]></content:encoded></item><item><title>Mapped types in TypeScript</title><link>https://2ality.com/2025/02/mapped-types-typescript.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[A mapped type is a loop over keys that produces an object or tuple type and looks as follows:{[]: }
In this blog post, we examine how mapped types work and see examples of using them. Their most importing use cases are transforming objects and mapping tuples.]]></content:encoded></item><item><title>Democrats And The Price Of Protection</title><link>https://www.techdirt.com/2025/02/13/democrats-and-the-price-of-protection/</link><author>Mike Masnick</author><category>tech</category><pubDate>Thu, 13 Feb 2025 23:50:51 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[While democracy burns, corporate America is busy checking the wind direction. Google renames the Gulf of Mexico to flatter a wannabe autocrat’s ego. Business leaders draft contingency plans for the end of constitutional government. And the Democratic Party, funded by these same genuflecting corporations, responds with all the urgency of someone scheduling a dental cleaning.This isn’t just a failure of nerve—it’s a revelation of structural rot. We are witnessing a disturbing confluence: the same corporations prostrating themselves before Trump’s authoritarianism are simultaneously bankrolling the Democratic Party meant to resist it. It’s as if we’ve discovered that the fire department is taking donations from the arsonists while counseling residents not to use the emergency exits.While democracy burns, the Democratic leadership is worried about too many phone calls from concerned citizens. In a closed-door meeting this week, House Minority Leader Hakeem Jeffries expressed frustration not with corporate America’s growing acquiescence to authoritarianism, but with grassroots organizations urging a more confrontational defense of democracy.“What leverage do we have?” Jeffries laments, apparently forgetting that the power to resist tyranny doesn’t come from parliamentary procedure but from the moral clarity and collective will of the people.When Jeffries asks this question, he’s revealing more than he intends about the Democratic Party’s compromised position. Their primary source of funding comes from corporations that are already signaling their willingness to accommodate authoritarian rule. Each act of corporate genuflection to Trump—whether it’s Google’s cartographic flattery or business leaders’ quiet contingency planning—further weakens the party’s ability to mount effective resistance.This creates a perverse incentive structure where the supposed defenders of democracy are financially dependent on institutions actively enabling its decline. Is it any wonder that Democratic leadership seems more irritated by grassroots pressure than by corporate America’s growing comfort with authoritarianism? They’re caught in a protection racket where the supposed guardians are funded by those hedging their bets on democracy’s collapse.The result is a kind of political paralysis dressed up as procedural wisdom. While activists and citizens recognize the urgent need for confrontational defense of democratic institutions, party leadership counsels restraint and bipartisan common ground. They’re treating an existential threat to democracy like a normal policy disagreement—as if the proper response to an emerging autocracy is to schedule more committee hearings.Now I am a capitalist, but there must be a healthy separation between the forces of wealth and power. The Founders understood this. We re-learned this lesson in the Gilded Age and we must re-learn it now.The danger isn’t markets themselves—it’s the fusion of corporate and political power into a single, self-reinforcing system. When corporations can simultaneously fund both political parties while hedging their bets on authoritarianism, we’ve lost the crucial separation that makes both democracy and healthy capitalism possible. We’re watching the creation of a system where political power becomes just another tradable commodity, where democracy itself becomes subject to market forces rather than civic virtue.This is exactly what the Founders feared when they warned about the corrupting influence of concentrated power. It’s what Progressive Era reformers fought against when they broke up the trusts and established regulatory frameworks to check corporate overreach. They understood that democracy requires maintaining boundaries between economic and political power—that when these boundaries collapse, both systems become corrupted.What we’re seeing now—with corporations funding Democrats while accommodating Trump’s authoritarianism—is the end stage of this collapse. It’s a form of political arbitrage where democracy itself becomes just another risk to be hedged against.Unfortunately, this institutional paralysis isn’t happening in normal times—it’s occurring while a literal coup is in motion. Elon Musk, his mind deranged by the hall of funhouse mirrors he calls X (formerly Twitter), leads a charge against democratic institutions with his merry band of neoreactionaries flanking him as they storm the administrative state.This isn’t just metaphorical—we’re watching in real-time as they attempt to dismantle the basic infrastructure of governance. While Democratic leadership wrings its hands about “leverage” and corporate America calculates its positions, Musk and his allies are actively working to replace democratic processes with private control. The distorted reality he sees reflected in his social media echo chamber has become a blueprint for dismantling democratic institutions.The neoreactionaries who once theorized about replacing democracy with corporate governance are now finding their way into positions of actual power. They’re not just writing blog posts anymore—they’re drafting executive orders, restructuring agencies, and building the infrastructure for authoritarian control. And they’re doing it while the supposed defenders of democracy debate parliamentary procedure and worry about donor relations.And so we find ourselves in the absurd position where Democratic leadership, in the form of Jeffries and Schumer, treats an ongoing coup like an inconvenient scheduling conflict. Their response to democracy’s crisis amounts to little more than checking  to see if authoritarianism requires a two-thirds majority.While Rome burns, they’re busy drafting strongly worded letters to the arsonists, pausing only to scold the citizens who dare suggest using the fire extinguishers. Their position would be merely comedic if it weren’t so catastrophically dangerous—like watching someone respond to a home invasion by suggesting the burglar fill out a visitor’s form.“What leverage do we have?” Jeffries asks—apparently unaware that he’s providing his own epitaph. The leverage of moral clarity. The leverage of democratic legitimacy. The leverage of millions of citizens demanding their representatives actually represent them. But perhaps that’s too much to expect from leaders who’ve grown so comfortable in their donor-funded cages that they mistake their chains for jewelry.“We know now that Government by organized money is just as dangerous as Government by organized mob.” — Franklin D. Roosevelt.Mike Brock is a former tech exec who was on the leadership team at Block. Originally published at his Notes From the Circus.]]></content:encoded></item><item><title>Show HN: SQL Noir – Learn SQL by solving crimes</title><link>https://www.sqlnoir.com/</link><author>chrisBHappy</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 21:49:16 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Bojan Mihelac: Prefixed Parameters for Django querystring tag</title><link>http://code.informatikamihelac.com/en/query-string-with-prefixed-parameters/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 21:37:18 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[An overview of Django 5.1's new querystring tag and how to add support for prefixed parameters.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 5: The Training</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-5-the-training/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 21:04:32 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[In this fifth part of my series, I will outline the steps for creating a Docker container for training your image classification model, evaluating performance, and preparing for deployment.AI/ML engineers would prefer to focus on model training and data engineering, but the reality is that we also need to understand the infrastructure and mechanics behind the scenes.I hope to share some tips, not only to get your training run running, but how to streamline the process in a cost efficient manner on cloud resources such as Kubernetes.I will reference elements from my previous articles for getting the best model performance, so be sure to check out Part 1 and Part 2 on the data sets, as well as Part 3 and Part 4 on model evaluation.Here are the learnings that I will share with you, once we lay the groundwork on the infrastructure:Building your Docker containerExecuting your training runFirst, let me provide a brief description of the setup that I created, specifically around Kubernetes. Your setup may be entirely different, and that is just fine. I simply want to set the stage on the infrastructure so that the rest of the discussion makes sense.This is a server you deploy that provides a user interface to for your subject matter experts to label and evaluate images for the image classification application. The server can run as a pod on your Kubernetes cluster, but you may find that running a dedicated server with faster disk may be better.Image files are stored in a directory structure like the following, which is self-documenting and easily modified.Image_Library/
  - cats/
    - image1001.png
  - dogs/
    - image2001.pngIdeally, these files would reside on local server storage (instead of cloud or cluster storage) for better performance. The reason for this will become clear as we see what happens as the image library grows.Cloud Storage allows for a virtually limitless and convenient way to share files between systems. In this case, the image library on your management system could access the same files as your Kubernetes cluster or Docker engine.However, the downside of cloud storage is the latency to open a file. Your image library will have  of images, and the latency to read each file will have a significant impact on your training run time. Longer training runs means more cost for using the expensive GPU processors!The way that I found to speed things up is to create a  file of your image library on your management system and copy them to cloud storage. Even better would be to create multiple tar files , each containing 10,000 to 20,000 images.This way you only have network latency on a handful of files (which contain thousands, once extracted) and you start your training run much sooner.Kubernetes or Docker engineA Kubernetes cluster, with proper configuration, will allow you to dynamically scale up/down nodes, so you can perform your model training on GPU hardware as needed. Kubernetes is a rather heavy setup, and there are other container engines that will work.The technology options change constantly!The main idea is that you want to spin up the resources you need — for only as long as you need them — then scale down to reduce your time (and therefore cost) of running expensive GPU resources.Once your GPU node is started and your Docker container is running, you can extract the  files above to  storage, such as an , on your node. The node typically has high-speed SSD disk, ideal for this type of workload. There is one caveat — the storage capacity on your node must be able to handle your image library.Assuming we are good, let’s talk about building your Docker container so that you can train your model on your image library.Building your Docker containerBeing able to execute a training run in a consistent manner lends itself perfectly to building a Docker container. You can “pin” the version of libraries so you know exactly how your scripts will run every time. You can version control your containers as well, and revert to a known good image in a pinch. What is really nice about Docker is you can run the container pretty much anywhere.The tradeoff when running in a container, especially with an Image Classification model, is the speed of file storage. You can attach any number of volumes to your container, but they are usually  attached, so there is latency on each file read. This may not be a problem if you have a small number of files. But when dealing with hundreds of thousands of files like image data, that latency adds up!This is why using the  file method outlined above can be beneficial.Also, keep in mind that Docker containers could be terminated unexpectedly, so you should make sure to store important information outside the container, on cloud storage or a database. I’ll show you how below.Knowing that you will need to run on GPU hardware (here I will assume Nvidia), be sure to select the right base image for your Dockerfile, such as  with the “devel flavor that will contain the right drivers.Next, you will add the script files to your container, along with a “batch” script to coordinate the execution. Here is an example Dockerfile, and then I’ll describe what each of the scripts will be doing.#####   Dockerfile   #####
FROM nvidia/cuda:12.8.0-devel-ubuntu24.04

# Install system software
RUN apt-get -y update && apg-get -y upgrade
RUN apt-get install -y python3-pip python3-dev

# Setup python
WORKDIR /app
COPY requirements.txt
RUN python3 -m pip install --upgrade pip
RUN python3 -m pip install -r requirements.txt

# Pythong and batch scripts
COPY ExtractImageLibrary.py .
COPY Training.py .
COPY Evaluation.py .
COPY ScorePerformance.py .
COPY ExportModel.py .
COPY BulkIdentification.py .
COPY BatchControl.sh .

# Allow for interactive shell
CMD tail -f /dev/nullDockerfiles are declarative, almost like a cookbook for building a small server — you know what you’ll get every time. Python libraries benefit, too, from this declarative approach. Here is a sample  file that loads the TensorFlow libraries with CUDA support for GPU acceleration.#####   requirements.txt   #####
numpy==1.26.3
pandas==2.1.4
scipy==1.11.4
keras==2.15.0
tensorflow[and-cuda]Extract Image Library scriptIn Kubernetes, the Docker container can access local, high speed storage on the physical node. This can be achieved via the  volume type. As mentioned before, this will only work if the local storage on your node can handle the size of your library.#####   sample 25GB emptyDir volume in Kubernetes   #####
containers:
  - name: training-container
    volumeMounts:
      - name: image-library
        mountPath: /mnt/image-library
volumes:
  - name: image-library
    emptyDir:
      sizeLimit: 25GiYou would want to have another  to your cloud storage where you have the  files. What this looks like will depend on your provider, or if you are using a persistent volume claim, so I won’t go into detail here.Now you can extract the  files — ideally in parallel for an added performance boost — to the local mount point.As AI/ML engineers, the model training is where we want to spend most of our time.This is where the magic happens!With your image library now extracted, we can create our train-validation-test sets, load a pre-trained model or build a new one, fit the model, and save the results.One key technique that has served me well is to load the most recently trained model as my base. I discuss this in more detail in Part 4 under “Fine tuning”, this results in faster training time and significantly improved model performance.Be sure to take advantage of the local storage to checkpoint your model during training since the models are quite large and you are paying for the GPU even while it sits idle writing to disk.This of course raises a concern about what happens if the Docker container dies part-way though the training. The risk is (hopefully) low from a cloud provider, and you may not want an incomplete training anyway. But if that does happen, you will at least want to understand , and this is where saving the main log file to cloud storage (described below) or to a package like MLflow comes in handy.After your training run has completed and you have taken proper precaution on saving your work, it is time to see how well it performed.Normally this evaluation script will pick up on the model that just finished. But you may decide to point it at a previous model version through an interactive session. This is why have the script as stand-alone.With it being a separate script, that means it will need to read the completed model from disk — ideally local disk for speed. I like having two separate scripts (training and evaluation), but you might find it better to combine these to avoid reloading the model.Now that the model is loaded, the evaluation script should generate predictions on  image in the training, validation, test, and benchmark sets. I save the results as a  matrix with the softmax confidence score for each class label. So, if there are 1,000 classes and 100,000 images, that’s a table with 100 million scores!I save these results in  files that are then used in the score generation next.Taking the matrix of scores produced by the evaluation script above, we can now create various metrics of model performance. Again, this process could be combined with the evaluation script above, but my preference is for independent scripts. For example, I might want to regenerate scores on previous training runs. See what works for you.Here are some of the  functions that produce useful insights like F1, log loss, AUC-ROC, Matthews correlation coefficient.from sklearn.metrics import average_precision_score, classification_report
from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_scoreAside from these basic statistical analyses for each dataset (train, validation, test, and benchmark), it is also useful to identify:Which  labels get the most number of errors?Which  labels get the most number of incorrect guesses?How many ground-truth-to-predicted label pairs are there? In other words, which classes are easily confused?What is the  when applying a minimum softmax confidence score threshold?What is the  above that softmax threshold?For the “difficult” benchmark sets, do you get a sufficiently  score?For the “out-of-scope” benchmark sets, do you get a sufficiently  score?As you can see, there are multiple calculations and it’s not easy to come up with a single evaluation to decide if the trained model is good enough to be moved to production.In fact, for an image classification model, it is helpful to manually review the images that the model got wrong, as well as the ones that got a low softmax confidence score. Use the scores from this script to create a list of images to manually review, and then get a  for how well the model performs.Check out Part 3 for more in-depth discussion on evaluation and scoring.All of the heavy lifting is done by this point. Since your Docker container will be shutdown soon, now is the time to copy the model artifacts to cloud storage and prepare them for being put to use.The example Python code snippet below is more geared to Keras and TensorFlow. This will take the trained model and export it as a . Later, I will show how this is used by TensorFlow Serving in the  section below.# Increment current version of model and create new directory
next_version_dir, version_number = create_new_version_folder()

# Copy model artifacts to the new directory
copy_model_artifacts(next_version_dir)

# Create the directory to save the model export
saved_model_dir = os.path.join(next_version_dir, str(version_number))

# Save the model export for use with TensorFlow Serving
tf.keras.backend.set_learning_phase(0)
model = tf.keras.models.load_model(keras_model_file)
tf.saved_model.save(model, export_dir=saved_model_dir)This script also copies the other training run artifacts such as the model evaluation results, score summaries, and log files generated from model training. Don’t forget about your label map so you can give human readable names to your classes!Bulk identification scriptYour training run is complete, your model has been scored, and a new version is exported and ready to be served. Now is the time to use this latest model to assist you on trying to identify unlabeled images.As I described in Part 4, you may have a collection of “unknowns” — really good pictures, but no idea what they are. Let your new model provide a best guess on these and record the results to a file or a database. Now you can create filters based on closest match and by high/low scores. This allows your subject matter experts to leverage these filters to find new image classes, add to existing classes, or to remove images that have very low scores and are no good.By the way, I put this step inside the GPU container since you may have thousands of “unknown” images to process and the accelerated hardware will make light work of it. However, if you are not in a hurry, you could perform this step on a separate CPU node, and shutdown your GPU node sooner to save cost. This would especially make sense if your “unknowns” folder is on slower cloud storage.All of the scripts described above perform a specific task — from extracting your image library, executing model training, performing evaluation and scoring, exporting the model artifacts for deployment, and perhaps even bulk identification.One script to rule them allTo coordinate the entire show, this batch script gives you the entry point for your container and an easy way to trigger everything. Be sure to produce a log file in case you need to analyze any failures along the way. Also, be sure to write the log to your cloud storage in case the container dies unexpectedly.#!/bin/bash
# Main batch control script

# Redirect standard output and standard error to a log file
exec > /cloud_storage/batch-logfile.txt 2>&1

/app/ExtractImageLibrary.py
/app/Training.py
/app/Evaluation.py
/app/ScorePerformance.py
/app/ExportModel.py
/app/BulkIdentification.pyExecuting your training runSo, now it’s time to put everything in motion…Let’s go through the steps to prepare your image library, fire up your Docker container to train your model, and then examine the results.Image library ‘tar’ filesYour image management system should now create a  file backup of your data. Since  is a single-threaded function, you will get significant speed improvement by creating multiple tar files in parallel, each with a portion of you data.Now these files can be copied to your shared cloud storage for the next step.All the hard work you put into creating your container (described above) will be put to the test. If you are running Kubernetes, you can create a Job that will execute the  script.Inside the Kubernetes Job definition, you can pass environment variables to adjust the execution of your script. For example, the batch size and number of epochs are set here and then pulled into your Python scripts, so you can alter the behavior without changing your code.#####   sample Job in Kubernetes   #####
containers:
  - name: training-job
    env:
      - name: BATCH_SIZE
        value: 50
      - name: NUM_EPOCHS
        value: 30
    command: ["/app/BatchControl.sh"]Once the Job is completed, be sure to verify that the GPU node properly scales back down to zero according to your scaling configuration in Kubernetes — you don’t want to be saddled with a huge bill over a simple configuration error.With the training run complete, you should now have model artifacts saved and can examine the performance. Look through the metrics, such as F1 and log loss, and benchmark accuracy for high softmax confidence scores.As mentioned earlier, the reports only tell part of the story. It is worth the time and effort to manually review the images that the model got wrong or where it produced a low confidence score.Don’t forget about the bulk identification. Be sure to leverage these to locate new images to fill out your data set, or to find new classes.Once you have reviewed your model performance and are satisfied with the results, it is time to modify your TensorFlow Serving container to put the new model into production.TensorFlow Serving is available as a Docker container and provides a very quick and convenient way to serve your model. This container can listen and respond to API calls for your model.Let’s say your new model is version 7, and your  script (see above) has saved the model in your cloud share as /image_application/models/007. You can start the TensorFlow Serving container with that volume mount. In this example, the  points to folder for version 007.#####   sample TensorFlow pod in Kubernetes   #####
containers:
  - name: tensorflow-serving
    image: bitnami/tensorflow-serving:2.18.0
    ports:
      - containerPort: 8501
    env:
      - name: TENSORFLOW_SERVING_MODEL_NAME
        value: "image_application"
    volumeMounts:
      - name: models-subfolder
        mountPath: "/bitnami/model-data"

volumes:
  - name: models-subfolder
    azureFile:
      shareName: "image_application/models/007"A subtle note here — the export script should create a sub-folder, named 007 (same as the base folder), with the saved model export. This may seem a little confusing, but TensorFlow Serving will mount this share folder as  and detect the numbered sub-folder inside it for the version to serve. This will allow you to query the API for the model version as well as the identification.As I mentioned at the start of this article, this setup has worked for my situation. This is certainly not the only way to approach this challenge, and I invite you to customize your own solution.I wanted to share my hard-fought learnings as I embraced cloud services in Kubernetes, with the desire to keep costs under control. Of course, doing all this while maintaining a high level of model performance is an added challenge, but one that you can achieve.I hope I have provided enough information here to help you with your own endeavors. Happy learnings!]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 3: The Evaluation</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 21:00:06 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[In this third part of my series, I will explore the evaluation process which is a critical piece that will lead to a cleaner data set and elevate your model performance. We will see the difference between evaluation of a  model (one not yet in production), and evaluation of a  model (one making real-world predictions).In Part 1, I discussed the process of labelling your image data that you use in your Image Classification project. I showed how to define “good” images and create sub-classes. In Part 2, I went over various data sets, beyond the usual train-validation-test sets, such as benchmark sets, plus how to handle synthetic data and duplicate images.Evaluation of the trained modelAs machine learning engineers we look at accuracy, F1, log loss, and other metrics to decide if a model is ready to move to production. These are all important measures, but from my experience, these scores can be deceiving especially as the number of classes grows.Although it can be time consuming, I find it very important to manually review the images that the model gets , as well as the images that the model gives a  softmax “confidence” score to. This means adding a step immediately after your training run completes to calculate scores for  images — training, validation, test, and the benchmark sets. You only need to bring up for manual review the ones that the model had problems with. This should only be a small percentage of the total number of images. See the Double-check process belowWhat you do during the manual evaluation is to put yourself in a “” to ensure that the labelling standards are being followed that you setup in Part 1. Ask yourself:“Is this a good image?” Is the subject front and center, and can you clearly see all the features?“Is this the correct label?” Don’t be surprised if you find wrong labels.You can either remove the bad images or fix the labels if they are wrong. Otherwise you can keep them in the data set and force the model to do better next time. Other questions I ask are:“Why did the model get this wrong?”“Why did this image get a low score?”“What is it about the image that caused confusion?”Sometimes the answer has nothing to do with  specific image. Frequently, it has to do with the  images, either in the ground truth class or in the predicted class. It is worth the effort to Double-check all images in both sets if you see a consistently bad guess. Again, don’t be surprised if you find poor images or wrong labels.When doing the evaluation of the trained model (above), we apply a lot of subjective analysis — “Why did the model get this wrong?” and “Is this a good image?” From these, you may only get a .Frequently, I will decide to hold off moving a model forward to production based on that gut feel. But how can you justify to your manager that you want to hit the brakes? This is where putting a more  analysis comes in by creating a weighted average of the softmax “confidence” scores.In order to apply a weighted evaluation, we need to identify sets of classes that deserve adjustments to the score. Here is where I create a list of “commonly confused” classes.Commonly confused classesCertain animals at our zoo can easily be mistaken. For example, African elephants and Asian elephants have different ear shapes. If your model gets these two mixed up, that is not as bad as guessing a giraffe! So perhaps you give partial credit here. You and your subject matter experts (SMEs) can come up with a list of these pairs and a weighted adjustment for each.This weight can be factored into a modified cross-entropy loss function in the equation below. The back half of this equation will reduce the impact of being wrong for specific pairs of ground truth and prediction by using the “weight” function as a lookup. By default, the weighted adjustment would be 1 for all pairings, and the commonly confused classes would get something like 0.5.In other words, it’s better to be unsure (have a  confidence score) when you are wrong, compared to being super confident and wrong.Once this weighted log loss is calculated, I can compare to previous training runs to see if the new model is ready for production.Confidence threshold reportAnother valuable measure that incorporates the confidence threshold (in my example, 95) is to report on accuracy and false positive rates. Recall that when we apply the confidence threshold before presenting results, we help reduce false positives from being shown to the end user.In this table, we look at the breakdown of “true positive above 95” for each data set. We get a sense that when a “good” picture comes through (like the ones from our train-validation-test set) it is very likely to surpass the threshold, thus the user is “happy” with the outcome. Conversely, the “false positive above 95” is extremely low for good pictures, thus only a small number of our users will be “sad” about the results.We expect the train-validation-test set results to be exceptional since our data is curated. So, as long as people take “good” pictures, the model should do very well. But to get a sense of how it does on extreme situations, let’s take a look at our benchmarks.The “difficult” benchmark has more modest true positive and false positive rates, which reflects the fact that the images are more challenging. These values are much easier to compare across training runs, so that lets me set a min/max target. So for example, if I target a minimum of 80% for true positive, and maximum of 5% for false positive on this benchmark, then I can feel confident moving this to production.The “out-of-scope” benchmark has no true positive rate because  of the images belong to any class the model can identify. Remember, we picked things like a bag of popcorn, etc., that are not zoo animals, so there cannot be any true positives. But we do get a false positive rate, which means the model gave a confident score to that bag of popcorn as some animal. And if we set a target maximum of 10% for this benchmark, then we may not want to move it to production.Right now, you may be thinking, “Well, what animal did it pick for the bag of popcorn?” Excellent question! Now you understand the importance of doing a manual review of the images that get bad results.Evaluation of the deployed modelThe evaluation that I described above applies to a model immediately after . Now, you want to evaluate how your model is doing in the . The process is similar, but requires you to shift to a “” and asking yourself, “Did the model get this correct?” and “Should it have gotten this correct?” and “Did we tell the user the right thing?”So, imagine that you are logging in for the morning — after sipping on your cold brew coffee, of course — and are presented with 500 images that your zoo guests took yesterday of different animals. Your job is to determine how satisfied the guests were using your model to identify the zoo animals.Using the softmax “confidence” score for each image, we have a threshold before presenting results. Above the threshold, we tell the guest what the model predicted. I’ll call this the “happy path”. And below the threshold is the “sad path” where we ask them to try again.Your review interface will first show you all the “happy path” images one at a time. This is where you ask yourself, “Did we get this right?” Hopefully, yes!But if not, this is where things get tricky. So now you have to ask, “Why not?” Here are some things that it could be:“Bad” picture — Poor lighting, bad angle, zoomed out, etc — refer to your labelling standards.Out-of-scope — It’s a zoo animal, but unfortunately one that isn’t found in  zoo. Maybe it belongs to another zoo (your guest likes to travel and try out your app). Consider adding these to your data set.Out-of-scope — It’s not a zoo animal. It could be an animal in your zoo, but not one typically  there, like a neighborhood sparrow or mallard duck. This might be a candidate to add.Out-of-scope — It’s something found in the zoo. A zoo usually has interesting trees and shrubs, so people might try to identify those. Another candidate to add.Prankster — Completely out-of-scope. Because people like to play with technology, there’s the possibility you have a prankster that took a picture of a bag of popcorn, or a soft drink cup, or even a selfie. These are hard to prevent, but hopefully get a low enough score (below the threshold) so the model did not identify it as a zoo animal. If you see enough pattern in these, consider creating a class with special handling on the front-end.After reviewing the “happy path” images, you move on to the “sad path” images — the ones that got a low confidence score and the app gave a “sorry, try again” message. This time you ask yourself, “ the model have given this image a higher score?” which would have put it in the “happy path”. If so, then you want to ensure these images are added to the training set so next time it will do better. But most of time, the low score reflects many of the “bad” or out-of-scope situations mentioned above.Perhaps your model performance is suffering and it has nothing to do with your model. Maybe it is the ways you users interacting with the app. Keep an eye out of non-technical problems and share your observations with the rest of your team. For example:Are your users using the application in the ways you expected?Are they not following the instructions?Do the instructions need to be stated more clearly?Is there anything you can do to improve the experience?Collect statistics and new imagesBoth of the manual evaluations above open a gold mine of data. So, be sure to collect these statistics and feed them into a dashboard — your manager and your future self will thank you!Keep track of these stats and generate reports that you and your can reference:How often the model is being called?What times of the day, what days of the week is it used?Are your system resources able to handle the peak load?What classes are the most common?After evaluation, what is the accuracy for each class?What is the breakdown for confidence scores?How many scores are above and below the confidence threshold?The single best thing you get from a deployed model is the additional real-world images! You can add these now images to improve coverage of your existing zoo animals. But more importantly, they provide you insight on  classes to add. For example, let’s say people enjoy taking a picture of the large walrus statue at the gate. Some of these may make sense to incorporate into your data set to provide a better user experience.Creating a new class, like the walrus statue, is not a huge effort, and it avoids the false positive responses. It would be more embarrassing to identify a walrus statue as an elephant! As for the prankster and the bag of popcorn, you can configure your front-end to quietly handle these. You might even get creative and have fun with it like, “Thank you for visiting the food court.”It is a good idea to double-check your image set when you suspect there may be problems with your data. I’m not suggesting a top-to-bottom check, because that would a monumental effort! Rather specific classes that you suspect could contain bad data that is degrading your model performance.Immediately after my training run completes, I have a script that will use this new model to generate predictions for my  data set. When this is complete, it will take the list of incorrect identifications, as well as the low scoring predictions, and automatically feed that list into the Double-check interface.This interface will show, one at a time, the image in question, alongside an example image of the ground truth and an example image of what the model predicted. I can visually compare the three, side-by-side. The first thing I do is ensure the original image is a “good” picture, following my labelling standards. Then I check if the ground-truth label is indeed correct, or if there is something that made the model think it was the predicted label.Remove the original image if the image quality is poor.Relabel the image if it belongs in a different class.During this manual evaluation, you might notice dozens of the same wrong prediction. Ask yourself why the model made this mistake when the images seem perfectly fine. The answer may be some incorrect labels on images in the ground truth, or even in the predicted class!Don’t hesitate to add those classes and sub-classes back into the Double-check interface and step through them all. You may have 100–200 pictures to review, but there is a good chance that one or two of the images will stand out as being the culprit.With a different mindset for a trained model versus a deployed model, we can now evaluate performances to decide which models are ready for production, and how well a production model is going to serve the public. This relies on a solid Double-check process and a critical eye on your data. And beyond the “gut feel” of your model, we can rely on the benchmark scores to support us.In Part 4, we kick off the training run, but there are some subtle techniques to get the most out of the process and even ways to leverage throw-away models to expand your library image data.]]></content:encoded></item><item><title>Zed Editor Introduces Open-Source &quot;Zeta&quot; Edit Prediction Model</title><link>https://www.phoronix.com/news/Zed-Zeta-AI-Model-Edit-Predict</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 20:58:48 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Zed code editor for macOS and Linux systems has proven to be quite popular for this Rust-based editor started by the creators of the Atom editor. Their latest feature being introduced is Zeta as an open-source edit prediction model to further enhance this code editor with AI capabilities...]]></content:encoded></item><item><title>Build a dynamic, role-based AI agent using Amazon Bedrock inline agents</title><link>https://aws.amazon.com/blogs/machine-learning/build-a-dynamic-role-based-ai-agent-using-amazon-bedrock-inline-agents/</link><author>Ishan Singh</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:56:28 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[AI agents continue to gain momentum, as businesses use the power of generative AI to reinvent customer experiences and automate complex workflows. We are seeing Amazon Bedrock Agents applied in investment research, insurance claims processing, root cause analysis, advertising campaigns, and much more. Agents use the reasoning capability of foundation models (FMs) to break down user-requested tasks into multiple steps. They use developer-provided instructions to create an orchestration plan and carry out that plan by securely invoking company APIs and accessing knowledge bases using Retrieval Augmented Generation (RAG) to accurately handle the user’s request.Although organizations see the benefit of agents that are defined, configured, and tested as managed resources, we have increasingly seen the need for an additional, more dynamic way to invoke agents. Organizations need solutions that adjust on the fly—whether to test new approaches, respond to changing business rules, or customize solutions for different clients. This is where the new inline agents capability in Amazon Bedrock Agents becomes transformative. It allows you to dynamically adjust your agent’s behavior at runtime by changing its instructions, tools, guardrails, knowledge bases, prompts, and even the FMs it uses—all without redeploying your application.In this post, we explore how to build an application using Amazon Bedrock inline agents, demonstrating how a single AI assistant can adapt its capabilities dynamically based on user roles.Inline agents in Amazon Bedrock AgentsThis runtime flexibility enabled by inline agents opens powerful new possibilities, such as: – Inline agents minimize the time-consuming create/update/prepare cycles traditionally required for agent configuration changes. Developers can instantly test different combinations of models, tools, and knowledge bases, dramatically accelerating the development process.A/B testing and experimentation – Data science teams can systematically evaluate different model-tool combinations, measure performance metrics, and analyze response patterns in controlled environments. This empirical approach enables quantitative comparison of configurations before production deployment.Subscription-based personalization – Software companies can adapt features based on each customer’s subscription level, providing more advanced tools for premium users.Persona-based data source integration – Institutions can adjust content complexity and tone based on the user’s profile, providing persona-appropriate explanations and resources by changing the knowledge bases associated to the agent on the fly. – Developers can create applications with hundreds of APIs, and quickly and accurately carry out tasks by dynamically choosing a small subset of APIs for the agent to consider for a given request. This is particularly helpful for large software as a service (SaaS) platforms needing multi-tenant scaling.Inline agents expand your options for building and deploying agentic solutions with Amazon Bedrock Agents. For workloads needing managed and versioned agent resources with a pre-determined and tested configuration (specific model, instructions, tools, and so on), developers can continue to use InvokeAgent on resources created with CreateAgent. For workloads that need dynamic runtime behavior changes for each agent invocation, you can use the new InvokeInlineAgent API. With either approach, your agents will be secure and scalable, with configurable guardrails, a flexible set of model inference options, native access to knowledge bases, code interpretation, session memory, and more.Our HR assistant example shows how to build a single AI assistant that adapts to different user roles using the new inline agent capabilities in Amazon Bedrock Agents. When users interact with the assistant, the assistant dynamically configures agent capabilities (such as model, instructions, knowledge bases, action groups, and guardrails) based on the user’s role and their specific selections. This approach creates a flexible system that adjusts its functionality in real time, making it more efficient than creating separate agents for each user role or tool combination. The complete code for this HR assistant example is available on our GitHub repo.This dynamic tool selection enables a personalized experience. When an employee logs in without direct reports, they see a set of tools that they have access to based on their role. They can select from options like requesting vacation time, checking company policies using the knowledge base, using a code interpreter for data analysis, or submitting expense reports. The inline agent assistant is then configured with only these selected tools, allowing it to assist the employee with their chosen tasks. In a real-world example, the user would not need to make the selection, because the application would make that decision and automatically configure the agent invocation at runtime. We make it explicit in this application so that you can demonstrate the impact.Similarly, when a manager logs in to the same system, they see an extended set of tools reflecting their additional permissions. In addition to the employee-level tools, managers have access to capabilities like running performance reviews. They can select which tools they want to use for their current session, instantly configuring the inline agent with their choices.The inclusion of knowledge bases is also adjusted based on the user’s role. Employees and managers see different levels of company policy information, with managers getting additional access to confidential data like performance review and compensation details. For this demo, we’ve implemented metadata filtering to retrieve only the appropriate level of documents based on the user’s access level, further enhancing efficiency and security.Let’s look at how the interface adapts to different user roles.The employee view provides access to essential HR functions like vacation requests, expense submissions, and company policy lookups. Users can select which of these tools they want to use for their current session.The manager view extends these options to include supervisory functions like compensation management, demonstrating how the inline agent can be configured with a broader set of tools based on user permissions.The manager view extends these capabilities to include supervisory functions like compensation management, demonstrating how the inline agent dynamically adjusts its available tools based on user permissions. Without inline agents, we would need to build and maintain two separate agents.As shown in the preceding screenshots, the same HR assistant offers different tool selections based on the user’s role. An employee sees options like Knowledge Base, Apply Vacation Tool, and Submit Expense, whereas a manager has additional options like Performance Evaluation. Users can select which tools they want to add to the agent for their current interaction.This flexibility allows for quick adaptation to user needs and preferences. For instance, if the company introduces a new policy for creating business travel requests, the tool catalog can be quickly updated to include a Create Business Travel Reservation tool. Employees can then choose to add this new tool to their agent configuration when they need to plan a business trip, or the application could automatically do so based on their role.With Amazon Bedrock inline agents, you can create a catalog of actions that is dynamically selected by the application or by users of the application. This increases the level of flexibility and adaptability of your solutions, making them a perfect fit for navigating the complex, ever-changing landscape of modern business operations. Users have more control over their AI assistant’s capabilities, and the system remains efficient by only loading the necessary tools for each interaction.Technical foundation: Dynamic configuration and action selectionInline agents allow dynamic configuration at runtime, enabling a single agent to effectively perform the work of many. By specifying action groups and modifying instructions on the fly, even within the same session, you can create versatile AI applications that adapt to various scenarios without multiple agent deployments.The following are key points about inline agents: – Change the agent’s configuration, including its FM, at runtime. This enables rapid experimentation and adaptation without redeploying the application, reducing development cycles. – Apply governance and access control at the tool level. With agents changing dynamically at runtime, tool-level governance helps maintain security and compliance regardless of the agent’s configuration. – Provide only necessary tools and instructions at runtime to reduce token usage and improve the agent accuracy. With fewer tools to choose from, it’s less complicated for the agent to select the right one, reducing hallucinations in the tool selection process. This approach can also lead to lower costs and improved latency compared to static agents because removing unnecessary tools, knowledge bases, and instructions reduces the number of input and output tokens being processed by the agent’s large language model (LLM). – Create reusable actions for dynamic selection based on specific needs. This modular approach simplifies maintenance, updates, and scalability of your AI applications.The following are examples of reusable actions:Enterprise system integration – Connect with systems like Salesforce, GitHub, or databases – Perform common tasks such as sending emails or managing calendars – Interact with specialized internal tools and services – Analyze text, structured data, or other information – Fetch weather updates, stock prices, or perform web searches – Use specific machine learning (ML) models for targeted tasksWhen using inline agents, you configure parameters for the following:Contextual tool selection based on user intent or conversation flowAdaptation to different user roles and permissionsSwitching between communication styles or personasModel selection based on task complexityThe inline agent uses the configuration you provide at runtime, allowing for highly flexible AI assistants that efficiently handle various tasks across different business contexts.Building an HR assistant using inline agentsLet’s look at how we built our HR Assistant using Amazon Bedrock inline agents: – We developed a demo catalog of HR-related tools, including: 
   – Using Amazon Bedrock Knowledge Bases for accessing company policies and guidelines based on the role of the application user. In order to filter the knowledge base content based on the user’s role, you also need to provide a metadata file specifying the type of employee’s roles that can access each file– For requesting and tracking time off.– For submitting and managing expense reports. – For performing calculations and data analysis.– for conducting and reviewing employee compensation assessments (manager only access). – We defined multiple conversation tones to suit different interaction styles: 
   – For formal, business-like interactions. – For friendly, everyday support. – For upbeat, encouraging assistance. – We implemented role-based access control. The application backend checks the user’s role (employee or manager) and provides access to appropriate tools and information and passes this information to the inline agent. The role information is also used to configure metadata filtering in the knowledge bases to generate relevant responses. The system allows for dynamic tool use at runtime. Users can switch personas or add and remove tools during their session, allowing the agent to adapt to different conversation needs in real time.Integrate the agent with other services and tools – We connected the inline agent to: 
  Amazon Bedrock Knowledge Bases for company policies, with metadata filtering for role-based access.AWS Lambda functions for executing specific actions (such as submitting vacation requests or expense reports).A code interpreter tool for performing calculations and data analysis. – We created a Flask-based UI that performs the following actions: 
  Displays available tools based on the user’s role.Allows users to select different personas.Provides a chat window for interacting with the HR assistant.To understand how this dynamic role-based functionality works under the hood, let’s examine the following system architecture diagram.As shown in preceding architecture diagram, the system works as follows:The end-user logs in and is identified as either a manager or an employee.The user selects the tools that they have access to and makes a request to the HR assistant.The agent breaks down the problems and uses the available tools to solve for the query in steps, which may include: 
  Amazon Bedrock Knowledge Bases (with metadata filtering for role-based access).Lambda functions for specific actions.Code interpreter tool for calculations.Compensation tool (accessible only to managers to submit base pay raise requests).The application uses the Amazon Bedrock inline agent to dynamically pass in the appropriate tools based on the user’s role and request.The agent uses the selected tools to process the request and provide a response to the user.This approach provides a flexible, scalable solution that can quickly adapt to different user roles and changing business needs.In this post, we introduced the Amazon Bedrock inline agent functionality and highlighted its application to an HR use case. We dynamically selected tools based on the user’s roles and permissions, adapted instructions to set a conversation tone, and selected different models at runtime. With inline agents, you can transform how you build and deploy AI assistants. By dynamically adapting tools, instructions, and models at runtime, you can:Create personalized experiences for different user rolesOptimize costs by matching model capabilities to task complexityStreamline development and maintenanceScale efficiently without managing multiple agent configurationsFor organizations demanding highly dynamic behavior—whether you’re an AI startup, SaaS provider, or enterprise solution team—inline agents offer a scalable approach to building intelligent assistants that grow with your needs. To get started, explore our GitHub repo and HR assistant demo application, which demonstrate key implementation patterns and best practices.To learn more about how to be most successful in your agent journey, read our two-part blog series:To get started with Amazon Bedrock Agents, check out the following GitHub repository with example code. is a Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building Generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau. is a Senior Generative AI Data Scientist at AWS. With a background in machine learning, she has over 10 years of experience architecting and building AI applications with customers across industries. As a technical lead, she helps customers accelerate their achievement of business value through generative AI solutions on Amazon Bedrock. In her free time, Maira enjoys traveling, playing with her cat, and spending time with her family someplace warm. is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Mark’s work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS certifications, including the ML Specialty Certification. is a Sr. Enterprise Solutions Architect at AWS, experienced in Software Engineering, Enterprise Architecture, and AI/ML. He is deeply passionate about exploring the possibilities of generative AI. He collaborates with customers to help them build well-architected applications on the AWS platform, and is dedicated to solving technology challenges and assisting with their cloud journey. is a Software Development Engineer at Amazon Web Services (AWS). He specializes in backend system design, distributed architectures, and scalable solutions, contributing to the development and launch of high-impact systems at Amazon. Outside of work, he spends his time playing ping pong and hiking through Cascade trails, enjoying the outdoors as much as he enjoys building systems. is a Software Development Engineer at Amazon Web Services (AWS), working in Agents for Amazon Bedrock. He focuses on developing scalable systems on the cloud that enable AI applications frameworks and orchestrations. Shubham also has a background in building distributed, scalable, high-volume-high-throughput systems in IoT architectures. is a Principal Engineer for Amazon Bedrock. He focuses on building deep learning-based AI and computer vision solutions for AWS customers. Oustide of work, Vivek enjoys trekking and following cricket.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 1: The Data</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:55:53 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[It is said that in order for a machine learning model to be successful, you need to have good data. While this is true (and pretty much obvious), it is extremely difficult to define, build, and sustain good data. Let me share with you the unique processes that I have learned over several years building an ever-growing image classification system and how you can apply these techniques to your own application.With persistence and diligence, you can avoid the classic “garbage in, garbage out”, maximize your model accuracy, and demonstrate real business value.In this series of articles, I will dive into the care and feeding of a multi-class, single-label image classification app and what it takes to reach the highest level of performance. I won’t get into any coding or specific user interfaces, just the main concepts that you can incorporate to suit your needs with the tools at your disposal.Here is a brief description of the articles. You will notice that the model is last on the list since we need to focus on curating the data first and foremost:Over the past six years, I have been primarily focused on building and maintaining an image classification application for a manufacturing company. Back when I started, most of the software did not exist or was too expensive, so I created these from scratch. In this time, I have deployed two identifier applications, the largest handles 1,500 classes and achieves 97–98% accuracy.It was about eight years ago that I started online studies for Data Science and machine learning. So, when the exciting opportunity to create an AI application presented itself, I was prepared to build the tools I needed to leverage the latest advancements. I jumped in with both feet!I quickly found that building and deploying a model is probably the easiest part of the job. Feeding high quality data into the model is the best way to improve performance, and that requires focus and patience. Attention to detail is what I do best, so this was a perfect fit.It all starts with the dataI feel that so much attention is given to the model selection (deciding which neural network is best) and that the data is just an afterthought. I have found the hard way that even one or two pieces of bad data can significantly impact model performance, so that is where we need to focus.For example, let’s say you train the classic cat versus dog image classifier. You have 50 pictures of cats and 50 pictures of dogs, however one of the “cats” is clearly (objectively) a picture of a dog. The computer doesn’t have the luxury of ignoring the mislabelled image, and instead adjusts the model weights to make it fit. Square peg meets round hole.Another example would be a picture of a cat that climbed up into a tree. But when you take a wholistic view of it, you would describe it as a picture of a tree (first) with a cat (second). Again, the computer doesn’t know to ignore the big tree and focus on the cat — it will start to identify trees as cats, even if there is a dog. You can think of these pictures as outliers and should be removed.It doesn’t matter if you have the best neural network in the world, you can count on the model making poor predictions when it is trained on “bad” data. I’ve learned that any time I see the model make mistakes, it’s time to review the data.Example Application — Zoo animalsFor the rest of this write-up, I will use an example of identifying zoo animals. Let’s assume your goal is to create a mobile app where guests at the zoo can take pictures of the animals they see and have the app identify them. Specifically, this is a multi-class, single-label application. — There are a lot of different animals at the zoo and many of them look very similar. — Guests using the app don’t always take good pictures (zoomed out, blurry, too dark), so we don’t want to provide an answer if the image is poor. — The zoo keeps expanding and adding new species all the time. — Occasionally you might find that people take pictures of the sparrows near the food court grabbing some dropped popcorn. — Just for fun, guests may take a picture of the bag of popcorn just to see what it comes back with.These are all real challenges — being able to tell the subtle differences between animals, handling out-of-scope cases, and just plain poor images.Before we get there, let’s start from the beginning.There are a lot of tools these days to help you with this part of the process, but the challenge remains the same — collecting, labelling, and curating the data.Having data to collect is challenge #1. Without images, you have nothing to train. You may need to get creative on sourcing the data, or even creating synthetic data. More on that later.A quick note about image pre-processing. I convert all my images to the input size of my neural network and save them as PNG. Inside this square PNG, I preserve the aspect ratio of the original picture and fill the background black. I don’t stretch the image nor crop any features out. This also helps center the subject.Challenge #2 is to establish standards for data quality…and ensure that these standards are followed! These standards will guide you toward that “good” data. And this assumes, of course, correct labels. Having both is much easier said than done!I hope to show how “good” and “correct” actually go hand-in-hand, and how important it is to apply these standards to every image.First, I want to point out that the image data discussed here is for the training set. What qualifies as a good image for  is a bit different than what qualifies as a good image for . More on that in Part 3.So, what is “good” data when talking about images? “A picture is worth a thousand words”, and if the  you use to describe the picture do not include the subject you are trying to label, then it is not good and you need remove it from your training set.For example, let’s say you are shown a picture of a zebra and (removing bias toward your application) you describe it as an “open field with a zebra in the distance”. In other words, if “open field” is the first thing you notice, then you likely do  want to use that image. The opposite is also true — if the picture is way too close, you would described it as “zebra pattern”.What you want is a description like, “a zebra, front and center”. This would have your subject taking up about 80–90% of the total frame. Sometimes I will take the time to crop the original image so the subject is framed properly.Keep in mind the use of image augmentation at the time of training. Having that buffer around the edges will allow “zoom in” augmentation. And “zoom out” augmentation will simulate smaller subjects, so don’t start out less than 50% of the total frame for your subject since you lose detail.Another aspect of a “good” image relates to the label. If you can only see the back side of your zoo animal, can you really tell, for example, that it is a cheetah versus a leopard? The key identifying features need to be visible. If a human struggles to identify it, you can’t expect the computer to learn anything.What does a “bad” image look like? Here is what I frequently watch out for:Wide angle lens stretchingHigh contrast or dark shadows“Doctored” images, drawn lines and arrows“Unusual” angles or situationsPicture of a mobile device that has a picture of your subjectIf you have a team of subject matter experts (SMEs) on hand to label the images, you are in a good starting position. Animal trainers at the zoo know the various species, and can spot the differences between, for example, a chimpanzee and a bonobo.To a Machine Learning Engineer, it is easy for you to assume all labels from your SMEs are correct and move right on to training the model. However, even experts make mistakes, so if you can get a second opinion on the labels, your error rate should go down.In reality, it can be prohibitively expensive to get one, let alone two, subject matter experts to review image labels. The SME usually has years of experience that make them more valuable to the business in other areas of work. My experience is that the machine learning engineer (that’s you and me) becomes the second opinion, and often the first opinion as well.Over time, you can become pretty adept at labelling, but certainly not an SME. If you do have the luxury of access to an expert, explain to them the labelling standards and how these are required for the application to be successful. Emphasize “quality over quantity”.It goes without saying that having a  label is so important. However, all it takes is one or two mislabelled images to degrade performance. These can easily slip into your data set with careless or hasty labelling. So, take the time to get it right.Ultimately, we as the ML engineer are responsible for model performance. So, if we take the approach of only working on model training and deployment, we will find ourselves wondering why performance is falling short.A lot of times, you will come across a really good picture of a very interesting subject, but have no idea what it is! It would be a shame to simply dispose of it. What you can do is assign it a generic label, like “Unknown Bird” or “Random Plant” that are  included in your training set. Later in Part 4, you’ll see how to come back to these images at a later date when you have a better idea what they are, and you’ll be glad you saved them.If you have done any image labelling, then you know how time consuming and difficult it can be. But this is where having a model, even a less-than-perfect model, can help you.Typically, you have a large collection of unlabelled image and you need to go through them one at a time to assign labels. Simply having the model offer a best guess and display the top 3 results lets you step through each image in a matter of seconds!Even if the top 3 results are wrong, this can help you narrow down your search. Over time, newer models will get better, and the labelling process can even be somewhat fun!In Part 4, I will show how you can bulk identify images and take this to the next level for faster labelling.I mentioned the example above of two species that look very similar, the chimpanzee and the bonobo. When you start out building your data set, you may have very sparse coverage of one or both of these species. In machine learning terms, we these “classes”. One option is to roll with what you have and hope that the model picks up on the differences with only a handful of example images.The option that I have used is to merge two or more classes into one, at least temporarily. So, in this case I would create a class called “chimp-bonobo”, which is composed of the limited example pictures of chimpanzee and bonobo species classes. Combined, these may give me enough to train the model on “chimp-bonobo”, with the trade-off that it’s a more generic identification.Sub-classes can even be normal variations. For example,  pink flamingos are grey instead of pink. Or, male and female orangutans have distinct facial features. You wan to have a fairly balanced number of images for these normal variations, and keeping sub-classes will allow you to accomplish this.Don’t be concerned that you are merging completely different looking classes — the neural network does a nice job of applying the “OR” operator. This works both ways — it can help you identify male or female variations as one species, but it can hurt you when “bad” outlier images sneak in like the example “open field with a zebra in the distance.”Over time, you will (hopefully) be able to collect more images of the sub-classes and then be able to successfully split them apart (if necessary) and train the model to identify them separately. This process has worked very well for me. Just be sure to double-check all the images when you split them to ensure the labels didn’t get accidentally mixed up — it will be time well spent.All of this certainly depends on your user requirements, and you can handle this in different ways either by creating a unique class label like “chimp-bonobo”, or at the front-end presentation layer where you notify the user that you have intentionally merged these classes and provide guidance on further refining the results. Even after you decide to split the two classes, you may want to caution the user that the model could be wrong since the two classes are so similar.I realize this was a long write-up for something that on the surface seems intuitive, but these are all areas that I have tripped me up in the past because I didn’t give them enough attention. Once you have a solid understanding of these principles, you can go on to build a successful application.In Part 2, we will take the curated data we collected here to create the classic data sets, with a custom benchmark set that will further enhance your data. Then we will see how best to evaluate our trained model using a specific “training mindset”, and switch to a “production mindset” when evaluating a deployed model.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 4: The Model</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:53:42 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[In this latest part of my series, I will share what I have learned on selecting a model for Image Classification and how to fine tune that model. I will also show how you can leverage the model to accelerate your labelling process, and finally how to justify your efforts by generating usage and performance statistics.In Part 1, I discussed the process of labelling your image data that you use in your image classification project. I showed how define “good” images and create sub-classes. In Part 2, I went over various data sets, beyond the usual train-validation-test sets, with benchmark sets, plus how to handle synthetic data and duplicate images. In Part 3, I explained how to apply different evaluation criteria to a trained model versus a deployed model, and using benchmarks to determine when to deploy a model.So far I have focused a lot of time on labelling and curating the set of images, and also evaluating model performance, which is like putting the cart before the horse. I’m not trying to minimize what it takes to design a massive neural network — this is a very important part of the application you are building. In my case, I spent a few weeks experimenting with different available models before settling on one that fit the bill.Once you pick a model structure, you usually don’t make any major changes to it. For me, six years into deployment, I’m still using the same one. Specifically, I chose Inception V4 because it has a large input image size and an adequate number of layers to pick up on subtle image features. It also performs inference fast enough on CPU, so I don’t need to run expensive hardware to serve the model.Your mileage may vary. But again, the main takeaway is that focusing on your data will pay dividends versus searching for the best model.I will share a process that I found to work extremely well. Once I decided on the model to use, I randomly initialized the weights and let the model train for about 120 epoch before improvements plateau at a fairly modest accuracy, like 93%. At this point, I performed the evaluation of the trained model (see Part 3) to clean up the data set. I also incorporated new images as part of the data pipeline (see Part 1) and prepared the data sets for the next training run.Before starting the next training run, I simply take the last trained model, pop the output layer, and add it back in with random weights. Since the number of output classes are constantly increasing in my case, I have to pop that layer anyway to account for the new number of classes. Importantly, I leave the rest of the trained weights as they were and allow them to continue updating for the new classes.This allows the model to train much faster before improvements stall. After repeating this process dozens of times, the training reaches plateau after about 20 epochs, and the test accuracy can reach 99%! The model is building upon the low-level features that it established from the previous runs while re-learning the output weights to prevent overfitting.It took me a while to trust this process, and for a few years I would train from scratch every time. But after I attempted this and saw the training time (not to mention the cost of cloud GPU) go down while the accuracy continued to go up, I started to embrace the process. More importantly, I continue to see the evaluation metrics of the deployed model return solid performances.During training, you can apply transformations on your images (called “augmentation”) to give you more diversity from you data set. With our zoo animals, it is fairly safe to apply left-right flop, slight rotations clockwise and counterclockwise, and slight resize that will zoom in and out.With these transformations in mind, make sure your images are still able to act as good training images. In other words, an image where the subject is already small will be even smaller with a zoom out, so you probably want to discard the original. Also, some of your original pictures may need to be re-oriented by 90 degrees to be upright since a further rotation would make them look unusual.As I mentioned in Part 1, you can use the trained model to assist you in labelling images one at a time. But the way to take this even further is to have your newly trained model identify hundreds at a time while building a list of the results that you can then filter.Typically, we have large collections of  images that have come in either through regular usage of the application or some other means. Recall from Part 1 assigning “unknown” labels to interesting pictures but you have no clue what it is. By using the bulk identification method, we can sift through the collections quickly to target the labelling once we know what they are.By combining your current image counts with the bulk identification results, you can target classes that need expanded coverage. Here are a few ways you can leverage bulk identification:Increase low image counts — Some of your classes may have just barely made the cutoff to be included in the training set, which means you need more examples to improve coverage. Filter for images that have low counts.Replace staged or synthetic images — Some classes may be built entirely using non-real-world images. These pictures may be good enough to get started with, but may cause performance issues down the road because they look different than what typically comes through. Filter for classes that depend on staged images. — A class in your data set may look like another one. For example, let’s say your model can identify an antelope, and that looks like a gazelle which your model cannot identify yet. Setting a filter for antelope and a lower confidence score may reveal gazelle images that you can label. — You may not have known how to identify the dozens of cute wallaby pictures, so you saved them under “Unknown” because it was a good image. Now that you know what it is, you can filter for its look-alike kangaroo and quickly add a new class.Mass removal of low scores — As a way to clean out your large collection of unlabelled images that have nothing worth labelling, set a filter for lowest scores.Recall the decision I made to have image cutoffs from Part 2, which allows us to ensure an adequate number of example images of a class before we train and server a model to the public. The problem is that you may have a number of classes that are  below your cutoff (in my case, 40) and don’t make it into the model.The way I approach this is with a “throw-away” training run that I do not intend to move to production. I will decrease the lower cutoff from 40 to perhaps 35, build my train-validation-test sets, then train and evaluate like I normally do. The most important part of this is the bulk identification at the end!There is a chance that somewhere in the large collection of unlabelled images I will find the few that I need. Doing the bulk identification with this throw-away model helps find them.One very important aspect of any machine learning application is being able to show usage and performance reports. Your manager will likely want to see how many times the application is being used to justify the expense, and you as the ML engineer will want to see how the latest model is performing compared to the previous one.You should build logging into your model serving to record every transaction going through the system. Also, the manual evaluations from Part 3 should be recorded so you can report on performance for such things as accuracy over time, by model version, by confidence scores, by class, etc. You will be able to detect trends and make adjustments to improve the overall solution.There are a lot of reporting tools, so I won’t recommend one over the other. Just make sure you are collecting as much information as you can to build these dashboards. This will justify the time, effort, and cost associated with maintaining the application.We covered a lot of ground across this four-part series on building an image classification project and deploying it in the real world. It all starts with the data, and by investing the time and effort into maintaining the highest quality image library, you can reach impressive levels of model performance that will gain the trust and confidence of your business partners.As a Machine Learning Engineer, you are primarily responsible for building and deploying your model. But it doesn’t stop there — dive into the data. The more familiar you are with the data, the better you will understand the strengths and weaknesses of your model. Take a close look at the evaluations and use them as an opportunity to adjust the data set.I hope these articles have helped you find new ways to improve your own machine learning project. And by the way, don’t let the machine do all the learning — as humans, our job is to continue our own learning, so don’t ever stop!Thank you for taking this deep dive with me into a data-driven approach to model optimization. I look forward to your feedback and how you can apply this to your own application.]]></content:encoded></item><item><title>Use language embeddings for zero-shot classification and semantic search with Amazon Bedrock</title><link>https://aws.amazon.com/blogs/machine-learning/use-language-embeddings-for-zero-shot-classification-and-semantic-search-with-amazon-bedrock/</link><author>Tom Rogers</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:53:32 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[In this post, we discuss what embeddings are, show how to practically use language embeddings, and explore how to use them to add functionality such as zero-shot classification and semantic search. We then use Amazon Bedrock and language embeddings to add these features to a really simple syndication (RSS) aggregator application.Amazon Bedrock is a fully managed service that makes foundation models (FMs) from leading AI startups and Amazon available through an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case. Amazon Bedrock offers a serverless experience, so you can get started quickly, privately customize FMs with your own data, and integrate and deploy them into your applications using Amazon Web Services (AWS) services without having to manage infrastructure. For this post, we use the Cohere v3 Embed model on Amazon Bedrock to create our language embeddings.To demonstrate some of the possible uses of these language embeddings, we developed an RSS aggregator website. RSS is a web feed that allows publications to publish updates in a standardized, computer-readable way. On our website, users can subscribe to an RSS feed and have an aggregated, categorized list of the new articles. We use embeddings to add the following functionalities:This post uses this application as a reference point to discuss the technical implementation of the semantic search and zero-shot classification features.This solution uses the following services:The following diagram illustrates the solution architecture.This section offers a quick primer on what embeddings are and how they can be used.Embeddings are numerical representations of concepts or objects, such as language or images. In this post, we discuss language embeddings. By reducing these concepts to numerical representations, we can then use them in a way that a computer can understand and operate on.Let’s take Berlin and Paris as an example. As humans, we understand the conceptual links between these two words. Berlin and Paris are both cities, they’re capitals of their respective countries, and they’re both in Europe. We understand their conceptual similarities almost instinctively, because we can create a model of the world in our head. However, computers have no built-in way of representing these concepts.To represent these concepts in a way a computer can understand, we convert them into language embeddings. Language embeddings are high dimensional vectors that learn their relationships with each other through the training of a neural network. During training, the neural network is exposed to enormous amounts of text and learns patterns based on how words are colocated and relate to each other in different contexts.Embedding vectors allow computers to model the world from language. For instance, if we embed “Berlin” and “Paris,” we can now perform mathematical operations on these embeddings. We can then observe some fairly interesting relationships. For instance, we could do the following: Paris – France + Germany ~= Berlin. This is because the embeddings capture the relationships between the words “Paris” and “France” and between “Germany” and “Berlin”—specifically, that Paris and Berlin are both capital cities of their respective countries.The following graph shows the word vector distance between countries and their respective capitals.Subtracting “France” from “Paris” removes the country semantics, leaving a vector representing the concept of a capital city. Adding “Germany” to this vector, we are left with something closely resembling “Berlin,” the capital of Germany. The vectors for this relationship are shown in the following graph.For our use case, we use the pre-trained Cohere Embeddings model in Amazon Bedrock, which embeds entire texts rather than a single word. The embeddings represent the meaning of the text and can be operated on using mathematical operations. This property can be useful to map relationships such as similarity between texts.One way in which we use language embeddings is by using their properties to calculate how similar an article is to one of the topics.To do this, we break down a topic into a series of different and related embeddings. For instance, for culture, we have a set of embeddings for sports, TV programs, music, books, and so on. We then embed the incoming title and description of the RSS articles, and calculate the similarity against the topic embeddings. From this, we can assign topic labels to an article.The following figure illustrates how this works. The embeddings that Cohere generates are highly dimensional, containing 1,024 values (or dimensions). However, to demonstrate how this system works, we use an algorithm designed to reduce the dimensionality of the embeddings, t-distributed Stochastic Neighbor Embedding (t-SNE), so that we can view them in two dimensions. The following image uses these embeddings to visualize how topics are clustered based on similarity and meaning.You can use the embedding of an article and check the similarity of the article against the preceding embeddings. You can then say that if an article is clustered closely to one of these embeddings, it can be classified with the associated topic.This is the k-nearest neighbor (k-NN) algorithm. This algorithm is used to perform classification and regression tasks. In k-NN, you can make assumptions around a data point based on its proximity to other data points. For instance, you can say that an article that has proximity to the music topic shown in the preceding diagram can be tagged with the culture topic.The following figure demonstrates this with an ArsTechnica article. We plot against the embedding of an article’s title and description: (The climate is changing so fast that we haven’t seen how bad extreme weather could get: Decades-old statistics no longer represent what is possible in the present day).The advantage of this approach is that you can add custom, user-generated topics. You can create a topic by first creating a series of embeddings of conceptually related items. For instance, an AI topic would be similar to the embeddings for AI, Generative AI, LLM, and Anthropic, as shown in the following screenshot.In a traditional classification system, we’d be required to train a classifier—a supervised learning task where we’d need to provide a series of examples to establish whether an article belongs to its respective topic. Doing so can be quite an intensive task, requiring labeled data and training the model. For our use case, we can provide examples, create a cluster, and tag articles without having to provide labeled examples or train additional models. This is shown in the following screenshot of results page of our website.In our application, we ingest new articles on a schedule. We use EventBridge schedules to periodically call a Lambda function, which checks if there are new articles. If there are, it creates an embedding from them using Amazon Bedrock and Cohere.We calculate the article’s distance to the different topic embeddings, and can then determine whether the article belongs to that category. This is done with Aurora PostgreSQL with pgvector. We store the embeddings of the topics and then calculate their distance using the following SQL query:const topics = await sqlClient.then(it=> it.query(
    `SELECT name, embedding_description, similarity
     FROM (SELECT topic_id as name, embedding_description, (1- ABS( 1 –(embed.embedding <-> $1))) AS "similarity" FROM topic_embedding_link embed)  topics
     ORDER BY similarity desc`,
    [toSql(articleEmbedding)]
  ))
The <-> operator in the preceding code calculates the Euclidean distance between the article and the topic embedding. This number allows us to understand how close an article is to one of the topics. We can then determine the appropriateness of a topic based on this ranking.We then tag the article with the topic. We do this so that the subsequent request for a topic is as computationally as light as possible; we do a simple join rather than calculating the Euclidean distance.const formattedTopicInsert = pgformat(
    `INSERT INTO feed_article_topic_link(topic_id, feed_article_id) VALUES %L ON CONFLICT DO NOTHING`,
    topicLinks
  )We also cache a specific topic/feed combination because these are calculated hourly and aren’t expected to change in the interim.As previously discussed, the embeddings produced by Cohere contain a multitude of features; they embed the meanings and semantics of a word of phrase. We’ve also found that we can perform mathematical operations on these embeddings to do things such as calculate the similarity between two phrases or words.We can use these embeddings and calculate the similarity between a search term and an embedding of an article with the k-NN algorithm to find articles that have similar semantics and meanings to the search term we’ve provided.For example, in one of our RSS feeds, we have a lot of different articles that rate products. In a traditional search system, we’d rely on keyword matches to provide relevant results. Although it might be simple to find a specific article (for example, by searching “best digital notebooks”), we would need a different method to capture multiple product list articles.In a semantic search system, we first transform the term “Product list” in an embedding. We can then use the properties of this embedding to perform a search within our embedding space. Using the k-NN algorithm, we can find articles that are semantically similar. As shown in the following screenshot, despite not containing the text “Product list” in either the title or description, we’ve been able to find articles that contain a product list. This is because we were able to capture the semantics of the query and match it to the existing embeddings we have for each article.In our application, we store these embeddings using pgvector on Aurora PostgreSQL. pgvector is an open source extension that enables vector similarity search in PostgreSQL. We transform our search term into an embedding using Amazon Bedrock and Cohere v3 Embed.After we’ve converted the search term to an embedding, we can compare it with the embeddings on the article that have been saved during the ingestion process. We can then use pgvector to find articles that are clustered together. The SQL code for that is as follows:SELECT *
FROM (
    SELECT feed_articles.id as id, title, feed_articles.feed_id as feed, feedName, slug, description, url, author, image, published_at as published, 1 - ABS(1 - (embedding <-> $2)) AS "similarity"
    FROM feed_articles
    INNER JOIN (select feed_id, name as feedName from feed_user_subscription fus where fus.user_id=$1) sub on feed_articles.feed_id=sub.feed_id
    ${feedId != undefined ? `WHERE feed_articles.feed_id = $4` : ""}
)
WHERE similarity > 0.95
ORDER BY similarity desc
LIMIT $3;
This code calculates the distance between the topics, and the embedding of this article as “similarity.” If this distance is close, then we can assume that the topic of the article is related, and we therefore attach the topic to the article.To deploy this application in your own account, you need the following prerequisites:Model access for Cohere Embed English. On the Amazon Bedrock console, choose  in the navigation pane, then choose . Select the FMs of your choice and request access.When the prerequisite steps are complete, you’re ready to set up the solution:Navigate to the solution directory:In your terminal, export your AWS credentials for a role or user in ACCOUNT_ID. The role needs to have all necessary permissions for AWS CDK deployment: 
  export AWS_REGION=”<region>” – The AWS Region you want to deploy the application toexport AWS_ACCESS_KEY_ID=”<access-key>” – The access key of your role or userexport AWS_SECRET_ACCESS_KEY=”<secret-key>” – The secret key of your role or userIf you’re deploying the AWS CDK for the first time, run the following command:To synthesize the AWS CloudFormation template, run the following command:cdk synth -c vpc_id=<ID Of your VPC>To deploy, use the following command:cdk deploy -c vpc_id=<ID Of your VPC>When deployment is finished, you can check these deployed stacks by visiting the AWS CloudFormation console, as shown in the following screenshot.Run the following command in the terminal to delete the CloudFormation stack provisioned using the AWS CDK:In this post, we explored what language embeddings are and how they can be used to enhance your application. We’ve learned how, by using the properties of embeddings, we can implement a real-time zero-shot classifier and can add powerful features such as semantic search.The code for this application can be found on the accompanying GitHub repo. We encourage you to experiment with language embeddings and find out what powerful features they can enable for your applications!is a Solutions Architect based in Amsterdam, the Netherlands. He has a background in software engineering. At AWS, Thomas helps customers build cloud solutions, focusing on modernization, data, and integrations.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 2: The Data Sets</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:29:39 +0000</pubDate><source url="https://towardsdatascience.com/">Towards Data Science</source><content:encoded><![CDATA[In Part 1, we discussed the importance of collecting good image data and assigning proper labels for your Image Classification project to be successful. Also, we talked about classes and sub-classes of your data. These may seem pretty straight forward concepts, but it’s important to have a solid understanding going forward. So, if you haven’t, please check it out.Now we will discuss how to build the various data sets and the techniques that have worked well for my application. Then in the next part, we will dive into the evaluation of your models, beyond simple accuracy.I will again use the example zoo animals image classification app.As machine learning engineers, we are all familiar with the train-validation-test sets, but when we include the concept of sub-classes discussed in Part 1, and incorporate to concepts discussed below to set a minimum and maximum image count per class, as well as staged and synthetic data to the mix, the process gets a bit more complicated. I had to create a custom script to handle these options.I will walk you through these concepts before we split the data for training: — Too few images and your model performance will suffer. Too many and you spend more time training than it’s worth. — Your model indicates how confident it is in the predictions. Let’s use that to decide when to present results to the user. — Real-world data is messy and the benchmark sets should reflect that. These need to stretch the model to the limit and help us decide when it is ready for production.Staged and synthetic data — Real-world data is king, but sometimes you need to produce the your own or even generate data to get off the ground. Be careful it doesn’t hurt performance. — Repeat data can skew your results and give you a false sense of performance. Make sure your data is diverse. — Combine sub-classes, apply cutoffs, and create your train-validation-test sets. Now we are ready to get the show started.In my experience, using a minimum of 40 images per class provides descent performance. Since I like to use 10% each for the test set and validation set, that means at least 4 images will be used to check the training set, which feels just barely adequate. Using fewer than 40 images per class, I notice my model evaluation tends to suffer.On the other end, I set a maximum of about 125 images per class. I have found that the performance gains tend to plateau beyond this, so having more data will slow down the training run with little to show for it. Having more than the maximum is fine, and these “overflow” can be added to the test set, so they don’t go to waste.There are times when I will drop the minimum cutoff to, say 35, with no intention of moving the trained model to production. Instead, the purpose is to leverage this throw-away model to find more images from my unlabelled set. This is a technique that I will go into more detail in Part 3.You are likely familiar with the softmax score. As a reminder, softmax is the probability assigned to each label. I like to think of it as a confidence score, and we are interested in the class that receives the highest confidence. Softmax is a value between zero and one, but I find it easier to interpret confidence scores between zero and 100, like a percentage.In order to decide if the model is confident enough with its prediction, I have chosen a threshold of 95. I use this threshold when determining if I want to present results to the user.Scores above the threshold have a better changes of being right, so I can confidently provide the results. Scores below the threshold may not be right — in fact it could be “out-of-scope”, meaning it’s something the model doesn’t know how to identify. So, instead of taking the risk of presenting incorrect results, I instead prompt the user to try again and offer suggestions on how to take a “good” picture.Admittedly this is somewhat arbitrary cutoff, and you should decide for your use-case what is appropriate. In fact, this score could probably be adjusted for each trained model, but this would make it harder to compare performance across models.I will refer to this confidence score frequently in the evaluations section in Part 3.Let me introduce what I call the benchmark sets, which you can think of as extended test sets. These are hand-picked images designed to stretch the limits of your model, and provide a measure for specific classes of your data. Use these benchmarks to justify moving your model to production, and for an objective measure to show to your manager. — These are the “extra credit” images, like the bonus questions a professor would add to the quiz to see which students are paying attention. You need a keen eye to spot the difference between the ground truth and a similar looking class. For example, a cheetah sleeping in the shade that could pass as a leopard if you don’t look closely. — These are the “trick question” images. Our model is trained on zoo animals, but people are known for not following the rules. For example, a zoo guest takes a picture of their child wearing cheetah face paint. — These are your “bread and butter” classes that need to get near perfect scores and zero errors. This would be a make-or-break benchmark for moving to production. — These are your “rare but exceptional” classes that again need to be correct, but reach a minimum score like the confidence threshold.When looking for images to add to the benchmarks, you can likely find them in real-world images from your deployed model. See the evaluation in Part 3.For each benchmark, calculate the min, max, median, and mean scores, and also how many images get scores above and below the confidence threshold. Now you can compare these measures against what is currently in production, and against your minimum requirements, to help decide if the new model is production worthy.Perhaps the biggest hurdle to any supervised machine learning application is having data to train the model. Clearly, “real-world” data that comes from actual users of the application is ideal. However you can’t really collect these until the model is deployed. Chicken and egg problem.One way to get started to is to have volunteers collect “staged” images for you, trying to act like real users. So, let’s have our zoo staff go around taking pictures of the animals. This is a good start, but there will be a certain level of bias introduced in these images. For example, the staff may take the photos over a few days, so you may not get the year-round weather conditions.Another way to get pictures is use computer-generated “synthetic” images. I would avoid these at all costs, to be honest. Based on my experience, the model struggles with these because they look…different. The lighting is not natural, the subject may superimposed on a background and so the edges look too sharp, etc. Granted, some of the AI generated images look very realistic, but if you look closely you may spot something unusual. The neural network in your model will notice these, so be careful.The way that I handle these staged or synthetic images is as a sub-class that gets merged into the training set, but only  giving preference to the real-world images. I cap the number of staged images to 60, so if I have 10 real-world, I now only need 50 staged. Eventually, these staged and synthetic images are phased out completely, and I rely entirely on real-world.One problem that can creep into your image set are duplicate images. These can be exact copies of pictures, or they can be extremely similar. You may think that this is harmless, but imagine having 100 pictures of an elephant that are exactly the same — your model will not know what to do with a different angle of the elephant.Now, let’s say you have only  pictures that are nearly the same. Not so bad, right? Well, here is what can happen to them:Both pictures go in the training set — The model doesn’t learn anything from the repeated image and it wastes time processing them.One goes into the training set, the other goes into the test set — Your test score will be higher, but it is not an accurate evaluation.Both are in the test set — Your test score will be compounded either higher or lower than it should be.None of these will help your model.There are a few ways to find duplicates. The approach I have taken is to calculate a hamming distance on all the pictures and identify the ones that are very close. I have an interface that displays the duplicates and I decide which one I like best, and remove the other.Another way (I haven’t tried this yet) is to create a vector representation of your images. Store these a vector database, and you can do a similarity search to find nearly identical images.Whatever method you use, it is important to clean up the duplicates.Now we are ready to build the traditional training, validation, and test sets. This is no longer a straight forward task since I want to:Merge sub-classes into a main class.Prioritize real-world images over staged or synthetic images.Apply a minimum number of images per class.Apply a maximum number of images per class, sending the “overflow” to the test set.This process is somewhat complicated and depends on how you manage your image library. First, I would recommend keeping your images in a folder structure that has sub-class folders. You can get image counts by using a script to simply read the folders. Second is to keep a configuration of how the sub-classes are merged. To really set yourself up for success, put these image counts and merge rules in a database for faster lookups.My train-validation-test set splits are usually 90–10–0. I originally started out using 80–10–10, but with diligence on keeping the entire data set clean, I noticed validation and test scores became pretty even. This allowed me to increase the training set size, and use “overflow” to become the test set, as well as using the benchmark sets.In this part, we’ve built our data sets by merging sub-classes and using the image count cutoffs. Plus we handle staged and synthetic data as well as cleaning up duplicate images. We also created benchmark sets and defined confidence thresholds, which help us decide when to move a model to production.In Part 3, we will discuss how we are going to evaluate the different model performances. And then finally we will get to the actual model training and the techniques to enhance accuracy.]]></content:encoded></item><item><title>Nearly a Year Later, Mozilla is Still Promoting OneRep</title><link>https://krebsonsecurity.com/2025/02/nearly-a-year-later-mozilla-is-still-promoting-onerep/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Thu, 13 Feb 2025 20:14:47 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[In mid-March 2024, KrebsOnSecurity revealed that the founder of the personal data removal service  also founded dozens of people-search companies. Shortly after that investigation was published,  said it would stop bundling Onerep with the  browser and wind down its partnership with the company. But nearly a year later, Mozilla is still promoting it to Firefox users.Mozilla offers Onerep to Firefox users on a subscription basis as part of Mozilla Monitor Plus. Launched in 2018 under the name Firefox Monitor, Mozilla Monitor also checks data from the website Have I Been Pwned? to let users know when their email addresses or password are leaked in data breaches.The ink on that partnership agreement had barely dried before KrebsOnSecurity published a story showing that Onerep’s Belarusian CEO and founder  launched dozens of people-search services since 2010, including a still-active data broker called  that sells background reports on people. This seemed to contradict Onerep’s stated motto, “We believe that no one should compromise personal online security and get a profit from it.”Shelest released a lengthy statement (PDF) wherein he acknowledged maintaining an ownership stake in , a consumer data broker he founded in 2015 — around the same time he started Onerep.Onerep.com CEO and founder Dimitri Shelest, as pictured on the “about” page of onerep.com.Shelest maintained that Nuwber has “zero cross-over or information-sharing with Onerep,” and said any other old domains that may be found and associated with his name are no longer being operated by him.“I get it,” Shelest wrote. “My affiliation with a people search business may look odd from the outside. In truth, if I hadn’t taken that initial path with a deep dive into how people search sites work, Onerep wouldn’t have the best tech and team in the space. Still, I now appreciate that we did not make this more clear in the past and I’m aiming to do better in the future.”When asked to comment on the findings, Mozilla said then that although customer data was never at risk, the outside financial interests and activities of Onerep’s CEO did not align with their values.“We’re working now to solidify a transition plan that will provide customers with a seamless experience and will continue to put their interests first,” Mozilla said.In October 2024, Mozilla published a statement saying the search for a different provider was taking longer than anticipated.“While we continue to evaluate vendors, finding a technically excellent and values-aligned partner takes time,” Mozilla wrote. “While we continue this search, Onerep will remain the backend provider, ensuring that we can maintain uninterrupted services while we continue evaluating new potential partners that align more closely with Mozilla’s values and user expectations. We are conducting thorough diligence to find the right vendor.”Asked for an update, Mozilla said the search for a replacement partner continues.“The work’s ongoing but we haven’t found the right alternative yet,” Mozilla said in an emailed statement. “Our customers’ data remains safe, and since the product provides a lot of value to our subscribers, we’ll continue to offer it during this process.”It’s a win-win for Mozilla that they’ve received accolades for their principled response while continuing to partner with Onerep almost a year later. But if it takes so long to find a suitable replacement, what does that say about the personal data removal industry itself?Onerep appears to be working in partnership with another problematic people-search service: , which has a history of ignoring opt-out requests or failing to honor them. A week before breaking the story about Onerep, KrebsOnSecurity published research showing the co-founders of Radaris were two native Russian brothers who’d built a vast network of affiliate marketing programs and consumer data broker services.Lawyers for the Radaris co-founders threatened to sue KrebsOnSecurity unless that story was retracted in full, claiming the founders were in fact Ukrainian and that our reporting had defamed the brothers by associating them with the actions of Radaris. Instead, we published a follow-up investigation which showed that not only did the brothers from Russia create Radaris, for many years they issued press releases quoting a fictitious CEO seeking money from investors.Several readers have shared emails they received from Radaris after attempting to remove their personal data, and those messages show Radaris has been promoting Onerep.An email from Radaris promoting Onerep.]]></content:encoded></item><item><title>Bcachefs Freezes Its On-Disk Format With Future Updates Optional</title><link>https://www.phoronix.com/news/Bcachefs-On-Disk-Format-Freeze</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 19:18:32 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The latest round of Bcachefs file-system fixes have been submitted today for the in-development Linux 6.14 kernel. Besides fixes for the current kernel, it was announced today that the on-disk format for the file-system is now considered frozen in its latest development "master" branch...]]></content:encoded></item><item><title>First Trump DOJ Assembled “Tiger Team” To Rewrite Key Law Protecting Online Speech</title><link>https://www.eff.org/deeplinks/2025/02/first-trump-doj-assembled-tiger-team-rewrite-key-law-protecting-online-speech</link><author>Aaron Mackey</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/section_230_banner.jpeg" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:51:58 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[As President Donald Trump issued an Executive Order in 2020 to retaliate against online services that fact-checked him, a team within the Department of Justice (DOJ) was finalizing a proposal to substantially weaken a key law that protects internet users’ speech.Documents released to EFF as part of a Freedom of Information Act (FOIA) suit reveal that the DOJ officials—a self-described “Tiger Team”—were caught off guard by Trump’s retaliatory effort, which was aimed at the same online social services they wanted to regulate further by amending 47 U.S.C. § 230 (Section 230).Section 230 protects users’ online speech by protecting the online intermediaries we all rely on to communicate on blogs, social media platforms, and educational and cultural platforms like Wikipedia and the Internet Archive. Section 230 embodies that principle that we should all be responsible for our own actions and statements online, but generally not those of others. The law prevents most civil suits against users or services that are based on what others say.The correspondence among DOJ officials shows that the group delayed unveiling the agency’s official plans to amend Section 230 in light of Trump’s executive order, which was challenged on First Amendment grounds and later rescinded by President Joe Biden. EFF represented the groups who challenged Trump’s Executive Order and filed two FOIA suits for records about the administration’s implementation of the order.In the most recent FOIA case, the DOJ has been slowly releasing records detailing its work to propose amendments to Section 230, which predated Trump’s Executive Order. The DOJ released the text of its proposed amendments to Section 230 in September 2020, and the proposal would have substantially narrowed the law’s protections.For example, the DOJ’s proposal would have allowed federal civil suits and state and federal criminal prosecutions against online services if they learned that users’ content broke the law. It also would have established notice-and-takedown liability for user-generated content that was deemed to be illegal. Together, these provisions would likely result in online services screening and removing a host of legal content, based on a fear that any questionable material might trigger liability later.The DOJ’s proposal had a distinct emphasis on imposing liability on services should they have hosted illegal content posted by their users. That focus was likely the result of the team DOJ assembled to work on the proposal, which included officials from the agency’s cybercrime division and the FBI.The documents also show that DOJ officials met with attorneys who brought lawsuits against online services to get their perspective on Section 230. This is not surprising, as the DOJ had been meeting with multiple groups throughout 2020 while it prepared a report about Section 230.EFF’s FOIA suit is ongoing, as the DOJ has said that it still has thousands of potential pages to review and possibly release. Although these documents reflect DOJ’s activity from Trump’s first term, they are increasingly relevant as the administration appoints officials who have previously threatened online intermediaries for exercising their own First Amendment rights. EFF will continue to publish all documents released in this FOIA suit and push back on attempts to undermine internet users’ rights to speak online.]]></content:encoded></item><item><title>Lawyers Caught Citing AI-Hallucinated Cases Call It a &apos;Cautionary Tale&apos;</title><link>https://www.404media.co/lawyers-caught-citing-ai-hallucinated-cases-call-it-a-cautionary-tale/</link><author>Samantha Cole</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/photo-1620424393934-04e772be09f4.jpeg" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:33:00 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[After a judge called out a law firm for citing fake cases in court documents, the attorneys admitted to AI “hallucinating” the cases. In a court order filed last week, Wyoming District Judge Kelly Rankin demanded the attorneys explain why they shouldn’t be sanctioned or disciplined for citing made-up information, including referencing eight non-existent cases. , first filed in 2023, is against Walmart and Jetson Electronic Bikes, Inc., which makes hoverboards for sale at Walmart. The plaintiffs, including a woman, her husband, and four minor children, claim a Jetson hoverboard’s lithium ion battery malfunctioned and caught fire while they were sleeping and burned their house down, severely injuring several members of the family.In a motion in limine filed by the plaintiff’s attorneys in January, they cited multiple cases (a "motion in limine" is a pretrial motion where attorneys request specific evidence or arguments be excluded from presentation during the trial) that don’t exist. “The cases are not identifiable by their Westlaw cite, and the Court cannot locate the District of Wyoming cases by their case name in its local Electronic Court Filing System,” Rankin wrote. She demanded each of the attorneys “provide a thorough explanation for how the motion and fake cases were generated,” and “explain their role in drafting or supervising the motion.” Four days later, they responded: “Our internal artificial intelligence platform ‘hallucinated’ the cases in question while assisting our attorney in drafting the motion in limine,” the law firm said in a filed response. “This matter comes with great embarrassment and has prompted discussion and action regarding the training, implementation, and future use of artificial intelligence within our firm. This serves as a cautionary tale for our firm and all firms, as we enter this new age of artificial intelligence.” Lawyers increasingly use AI tools for research and analyzing documents. But this isn’t the first time using AI to draft legal cases has gotten lawyers in trouble. In 2022, a man filed an action alleging he was injured by an Avianca airlines metal serving cart during an Avianca Airlines flight. His lawyers cited non-existent cases, and instead of admitting it and apologizing immediately, they doubled down and defended the filings. Eventually, they were fined $5,000 for fabricating the case,  that they “abandoned their responsibilities when they submitted non-existent judicial opinions with fake quotes and citations created by the artificial intelligence tool ChatGPT, then continued to stand by the fake opinions after judicial orders called their existence into question.” And , disbarred former celebrity attorney Michael Cohen gave his own lawyer, David Schwartz fake case citations generated by Google Bard. Cohen and Schwartz weren’t fined, but the judge who let them off without discipline did call the error “embarrassing” for them. ]]></content:encoded></item><item><title>Coding Interviews were HARD Until I Learned These 20 Tips</title><link>https://blog.algomaster.io/p/20-coding-interviews-tips</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/61c3f6c0-4027-4d37-b4a7-a30fc183fa12_1602x1032.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:30:27 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[I gave my first  in 2016—and failed. I failed the next five interviews as well before finally landing my first job at .Since then, I’ve interviewed with many companies and faced my fair share of rejections. However, over the years, my failure rate in coding interviews dropped significantly.By 2022, with just 1.5 months of focused preparation, I successfully cleared interviews at  and .Surprisingly, my success wasn’t due to a dramatic improvement in problem-solving skills. The real game-changer was my approach— and  during the interview.In this article, I’ll share  that made coding interviews significantly easier for me.These tips cover everything you need to know, including:How to systematically approach coding interview problemsKey concepts and patterns you should knowThe type of problems you should practiceHow to choose the right algorithm for a given problemTechniques to optimize your solutionHow to communicate your thought process effectivelyBy applying these strategies, you’ll be able to tackle coding interviews with confidence and massively increase your chances of success.In a coding interview, interviewers want to see how well you , , and  under pressure.Here's a breakdown of what they look for:Understanding the problem: Do you ask clarifying questions instead of making assumptions to ensure you fully understand the problem?: Can you decompose the problem into smaller, manageable parts?: Can you design an optimal solution in terms of time and space complexity?: Do you handle edge cases like empty inputs, duplicates, large values, or special conditions?: Can you explain why one approach is better than another?: Do you have a strong grasp of data structures and algorithms, and can you choose the right one for the problem?Can you quickly compute the time and space complexity of your solution?Explaining your thought process: Can you clearly articulate your approach and why it works?: Are you receptive to hints and able to adjust your approach accordingly?: Do you follow good coding practices (meaningful variable names, proper indentation, modular functions etc..)?Improving the initial solution: Can you optimize and refine your first solution when prompted?Are you able to tackle variations of the original problem?Can you manually walk through your code with sample inputs to verify correctness?Most coding interviews last Depending on the company and interviewer, you may be asked to solve 2-3easy/medium problems or 1 hard problem with follow-ups.Lets assume you are given one problem, with a follow up in a 45-minute interview. Here’s how you can optimally allocate your time:The interviewer may ask you to introduce yourself. Prepare a concise 1-2 minute introduction that highlights your background, experience, and key strengths. Practice it beforehand so that you can deliver it smoothly.Understand the Problem (5-10 mins):  Carefully read the problem statement, ask clarifying questions, and walk through sample inputs and expected outputs.Plan the Approach (10-20 mins): Brainstorm possible solutions, evaluate trade-offs, and discuss time and space complexity.Implement the Code (20-30 mins): Write a clean, modular and readable code.Dry-run your code with sample inputs, debug any issues, and ensure edge cases are handled.Follow-ups and Wrap Up (35-45 mins): Answer follow up questions, and ask thoughtful questions to the interviewer about the company, role, or team.One of the biggest mistakes candidates make in coding interviews is jumping into coding too soon.If you don't fully understand the question, you might end up solving the Here’s how to ensure you grasp the problem before coding:Read the Problem CarefullyTake a moment to absorb the problem statement. Rephrase it in your own words to confirm your understanding. Identify the expected input/output format and any hidden constraints.If anything is unclear, ask questions before diving into the solution. Interviewers appreciate when you seek clarity. Never assume details that aren’t explicitly mentioned in the problem statement.Common clarifications include:Are there duplicate values?Can the input be empty? If so, what should the output be?Should the solution handle negative numbers?Should the output maintain the original order of elements?Is the graph directed or undirected?Does the input contain only lowercase English letters, or can it have uppercase, digits, or special characters?What should happen if multiple solutions exist? Should I return any valid solution, or does the problem have specific requirements?Walk Through Input/Output ExamplesOnce you understand the problem statement and constraints, go over a few input and output examples to make sure you get it.Draw them out if it helps, especially for visual data structures like trees or graphs.Try to take examples that cover different scenarios of the problem. Think about any  that might come up.]]></content:encoded></item><item><title>A Cheap Jammer That Disrupted Airport Communications</title><link>https://www.youtube.com/watch?v=n_izoDLdUW0</link><author>Jack Rhysider</author><category>Darknet diaries</category><category>infosec</category><category>video</category><category>podcast</category><enclosure url="https://www.youtube.com/v/n_izoDLdUW0?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:00:54 +0000</pubDate><source url="https://www.youtube.com/channel/UCMIqrmh2lMdzhlCPK5ahsAg">Jack Rhysider</source><content:encoded><![CDATA[A Cheap Jammer That Disrupted Airport Communications

#podcast #airport #privacy 

Listen to the full episode 🎙 Darknet Diaries Ep: Jammer! He Just Wanted Privacy, But This Little Device Caused Big Trouble]]></content:encoded></item><item><title>Looking back at our Bug Bounty program in 2024</title><link>https://engineering.fb.com/2025/02/13/security/looking-back-at-our-bug-bounty-program-in-2024/</link><author></author><category>dev</category><category>official</category><pubDate>Thu, 13 Feb 2025 17:00:46 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[Ads audience tools designed to help people choose a target audience for their ads: Mixed reality hardware products:Organizing community events and presenting joint research:Providing resources and timely updates for the research community:]]></content:encoded></item><item><title>Terraform Architecture Explained , Terraform Core, State, and Plugins: How Terraform Works Under…</title><link>https://blog.devops.dev/terraform-architecture-explained-terraform-core-state-and-plugins-how-terraform-works-under-a19e4d4dbb09?source=rss----33f8b2d9a328---4</link><author>Kuseh Simon Wewoliamo</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:38 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Terraform Architecture Explained , Terraform Core, State, and Plugins: How Terraform Works Under the Hood.1. Introduction 2. Terraform Architecture4. Terraform Best Practices6. ReferencesInfrastructure as Code (IaC), is an approach to managing and provisioning infrastructure by writing code instead of the manual processes , “ClickOps”. IaC can be described as a mindset where you treat all aspects of operations (servers, databases, networks) as software. When you define your infrastructure using code , it enables you to automate and use all the best practices of software development. IaC eliminates human errors , speeds up infrastructure deployments and ensures infrastructure is version-controlled, just like software code.Terraform is an open-source tool developed by HashiCorp and the most popular and widely used IaC tool used by DevOps, SREs and cloud architects. Terraform is widely used because of it’s declarative syntax, platform agnostic and its simplicity. Understanding how terraform works behind the hood will go along way to help you in write better terraform code.In this article, we will explore Terraform architecture, its core components, and how it orchestrates infrastructure provisioning efficiently.2. Terraform ArchitectureTerraform follows a standard architecture to fulfill the necessary IaC tasks. Terraform architecture mainly consists of the following components: 1 Terraform core 2 Plugins (Providers and Provisioners) Terraform core is the engine/brain behind how terraform works. It is responsible for reading configurations files , building the dependency graphs from resources and data sources, managing state and applying changes. Terraform Core does not directly interact with cloud providers but communicates with plugins via remote procedure calls (RPCs) and the plugins in turn communicates with their corresponding platforms via HTTPs.Plugins (Providers and Provisioners)Terraform ability is enhance by plugins, which enable terraform to interact with cloud services and configure resources dynamically. Plugins acts as connectors or the glue between terraform and external APIs such as AWS, Azure, GCP, Kubernetes, Docker etc. Each plugin is written in the Go programming language and implements a specific interface. Terraform core knows how to install and execute plugins. Provisioners in Terraform are used to execute scripts or commands on a resource after it has been created or modified.State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources it created. It is a costumed JSON file that terraform uses to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By default, state is stored in a local file named “terraform.tfstate”. You can read more about terraform state here There are two ways to manage state: Local State refers to the default way by which Terraform stores state files (terraform.tfstate). It is suitable for small-scale projects or development environments and single person managing Terraform.Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. More on Remote StateTerraform follows a structured execution flow to provision, update, and manage infrastructure. This process ensures that infrastructure is deployed in a controlled and predictable manner. Terraform workflow consist of mainly three steps: The first step is to write your terraform configuration just like any other code using any editor of your choice. This is the step where you review your configurations. Terraform plan will define the infrastructure to be created, modified, or destroyed depending on the current configuration and infrastructure. The final step in the workflow is Apply, where you are ready to provision real infrastructure. Once you approve of the changes ,terraform will go ahead perform the desired actions as defined execution.4. Terraform Best Practices1. You should never edit the Terraform state files by hand or write code that reads them directly. If for some reason you need to manipulate the state file which should be a relatively rare occurrence, use the terraform import or terraform state commands.2. It’s a good practice to store your work in a version controlled repository even when you’re just operating as an individual.3. When working as a team, it’s important to delegate ownership of infrastructure across these teams and empower them to work in parallel without conflicts.4. Never Store your state file in a version controlled repository.5. Always use state locking on your state files to prevent data loss, conflicts and state file corruption.6. Integrate Terraform to your CI/CD pipelines to make your DevOps pipeline efficient.Well well, we have come to the end of this deep dive into terraform Architecture. To learn more about Terraform visit the official Terraform page. Don’t forget to add your comments , till then keep coding.6. Terraform:Up & Running , Third Edition by Yevgeniy Brikman]]></content:encoded></item><item><title>The Micro Frontend Revolution</title><link>https://blog.devops.dev/the-micro-frontend-revolution-29b6eedc8783?source=rss----33f8b2d9a328---4</link><author>Adem KORKMAZ</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:32 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Free AI models: Running Local LLMS with Llama 3.3,</title><link>https://blog.devops.dev/running-local-llms-with-llama-3-3-deepseek-r1-and-other-large-language-models-using-ollama-5d0dc2d09358?source=rss----33f8b2d9a328---4</link><author>Joel Wembo</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:51:03 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Part 4 of 10 Part series on DeepSeek MLOpsFree AI models: Running Local LLMS with Llama 3.3, DeepSeek-R1, and other Large Language Models using OllamaStep-by-Step Guide: Installing a Web UI for Local LLMs on WindowsWith the rise of powerful open-source large language models (LLMs) like , , Phi-4, and Gemma 2, many users want to run these models locally for privacy, performance, and customization. However, interacting with these models via the command line can be limiting. The solution? A web-based user interface (UI) that allows easy interaction with your local LLMs.In this article, we will explore the best web UIs for running LLMs locally on Windows and guide you through the installation process. is a lightweight, high-performance framework designed for running large language models (LLMs) locally with optimized execution. It works by leveraging GGUF (GGML Unified Format), an efficient model storage format that supports quantization, allowing models to run smoothly even on consumer hardware.Why Use a Web UI for Local LLMs?Using a web UI for local LLMs offers several advantages:: No need to work with command-line tools.: Manage multiple models in one place.: Chat history, prompt engineering, and adjustable settings.: Access your models remotely via a web browserStep 1 : Download and Install OllamaDownload Ollama from https://ollama.com/download/windows, then right click on the downloaded OllamaSetup.exe file and run the installer as administrator. Once the installation is complete, Ollama is ready to use on your Windows system. An Ollama icon will be added to the tray area at the bottom of the desktop.To run Ollama and start utilizing its AI models, you’ll need to use a terminal on Windows. We’ll skip it here and let’s see how to install WebUI for a better experience.Now open the browser and type localhost:11434 to check is Ollama is up and runningAlso, Check in your system TrayNext, Open your CMD to pull some free AI modelsStep 2 — Install Ollama WebUIRun the below docker command to deploy ollama-webui docker container on your local machine. If Ollama is on your computer, use this command:docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:mainTo connect to Ollama on another server, change the OLLAMA_BASE_URL to the server’s URL. So if Ollama is on a Different Server, use this command:docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:mainNext, Open your browser and type localhost:3000Ollama utilizes Metal on macOS and CUDA on Windows/Linux for hardware acceleration, enabling faster inference by directly leveraging GPU tensor operations. It runs a persistent server in the background, managing requests via an  that communicates with models using optimized token streaming.Internally, it uses low-level memory-efficient inference kernels, minimizing VRAM and RAM usage while maintaining performance. It also supports LoRA (Low-Rank Adaptation) fine-tuning, allowing users to personalize models on their local machine with minimal compute overhead.Run the following command:ollama run deepseek-r1:671bChoosing the Right Web UI for Your Needs: LM Studio (Simple setup, user-friendly UI): Oobabooga (More features, customization options): Gradio (Custom interface, lightweight solution): Open WebUI (Accessible over the internet)Setting up a web UI for local LLMs on Windows significantly enhances your experience, making it easier to interact with AI models without complex command-line operations. Whether you’re a beginner or an advanced user, the right UI can streamline your workflow and unlock new possibilities with local AI models.Start today with one of these web UIs and bring AI power to your local machine! 🚀Thank you for Reading !! 🙌🏻, don’t forget to subscribe and give it a CLAP, cloud Solutions architect, Back-end developer, and AWS Community Builder, currently working at prodxcloud as a DevOps & Cloud Architect. I bring a powerful combination of expertise in cloud architecture, DevOps practices, and a deep understanding of high availability (HA) principles. For more information about the author ( ]]></content:encoded></item><item><title>Understanding Container Orchestration (AWS ECS, AWS EKS &amp; Kubernetes)</title><link>https://blog.devops.dev/understanding-container-orchestration-aws-ecs-aws-eks-kubernetes-baee401db009?source=rss----33f8b2d9a328---4</link><author>Althaf Hussain</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:55 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Why Do We Need Container Orchestration?1️⃣ We use Docker to create and run containersDocker  using a Dockerfile with :: Packages and compiles the app.: Runs the app and exposes ports.docker run -p 80:80 my-appNow the app is running inside a . 🎉2️⃣ But what if the app crashes due to high traffic?Docker cannot restart or scale the app.If there’s high traffic (e.g., festive season sales), .3️⃣ Solution? We need a tool to manage containers automatically!This is where Container Orchestration Tools come in!Examples: Kubernetes, AWS ECS, AWS EKS, Azure AKS, Google GKE, OpenShift.🚀 Kubernetes — Full Control but Complex Setup✅  When you want  over your cluster.✅ Manages multiple containers (Docker is just for one container). (if traffic increases, it adds more containers). (if an app crashes, Kubernetes restarts it).What is Kubernetes (Self-Managed)?If you want  over your cluster, you can set up Kubernetes manually.How Kubernetes Works (Practical Steps)1️⃣ Create a VM or server (EC2, Azure VM, GCP VM, on-premise server, etc.).2️⃣ Install Kubernetes, kubeadm, kubectl, networking, storage, etc.3️⃣ Set up a  and .4️⃣ Deploy your app using a .5️⃣ Manage scaling, auto-healing, networking, etc. manually.🛠️ Steps to Deploy an App Using Kubernetes:1️⃣ Set up a server (EC2 instance or VM)sudo apt update && sudo apt install -y curl apt-transport-httpscurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.listsudo apt updatesudo apt install -y kubelet kubeadm kubectl2️⃣ Initialize Kubernetes clustermkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configapiVersion: apps/v1kind: Deployment  name: my-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-app        image: my-docker-image:latest        ports:kubectl apply -f deployment.yamlapiVersion: v1kind: Service  name: my-app-service  type: LoadBalancer    app: my-app    - protocol: TCP      targetPort: 80kubectl apply -f service.yaml🎯 Your app is now running inside Kubernetes! 🚀✅ Advantages of Self-Managed Kubernetes✔  → You can configure every part of the cluster.✔  → On-premise, AWS, Azure, GCP, or hybrid cloud.✔  → You’re not tied to AWS, Azure, or any provider.✔  → Most companies use  for flexibility.❌ Disadvantages of Self-Managed Kubernetes✖  → You need to manually configure networking, storage, security, etc.✖  → You have to patch, upgrade, and secure the cluster yourself.✖  → Setting up and managing Kubernetes is .🚀 AWS ECS — AWS Manages Everything✅  Running containers without managing Kubernetes.✅ You don’t need to set up Kubernetes.Just  what app you want to run, and it does everything. over cluster management.🛠️ Steps to Deploy an App in AWS ECS1️⃣  in the AWS Console.2️⃣  (Choose Fargate or EC2).3️⃣ :Go to ECS > Task Definitions > Create new task definition.Choose  (serverless) or  (self-managed).Define  (Docker image, ports, CPU, memory).4️⃣ :Go to ECS > Services > Create Service.Choose the cluster and task definition you created.Define  (number of tasks).5️⃣  🎉🎯 Your app is running inside AWS ECS without managing infrastructure! 🚀✔  → AWS takes care of the infrastructure.✔  → No need to set up Kubernetes manually.✔ Tightly integrated with AWS → Works great with AWS services like ALB, IAM, CloudWatch, etc.✔ Less operational overhead → No need to worry about maintaining a cluster.❌ Disadvantages of AWS ECS✖  → If you want to move your app from AWS to , or , you have to set up everything from scratch.✖  → You don’t have full control over how the cluster is managed.✖  → Most companies prefer  over ECS for multi-cloud strategies.🚀 AWS EKS — AWS Manages Kubernetes for You✅  When you want Kubernetes but don’t want to install it manually.✅ AWS  (no need to install manually).You  to deploy apps. than ECS but  than DIY Kubernetes.🛠️ Steps to Deploy an App in AWS EKS1️⃣  in the AWS Console.2️⃣ :Set Cluster name, VPC, IAM role.AWS will create & configure the Kubernetes control plane.3️⃣ aws eks update-kubeconfig --region your-region --name your-cluster-name4️⃣  (same as Kubernetes DIY)kubectl apply -f deployment.yaml5️⃣ Expose the app using a Kubernetes service (same as before).🎯 Your app is running in AWS EKS with Kubernetes, but AWS helps with the setup! 🚀✔  → Works exactly like Kubernetes, so it’s easy to move to another cloud (Azure AKS, Google GKE, etc.).✔ Fully managed control plane → AWS handles the  (setting up Kubernetes).✔  than ECS → You can tweak networking, security, and scaling.✔  → You can run Kubernetes anywhere (AWS, Azure, GCP, or on-premise).❌ Disadvantages of AWS EKS✖  → You still need to understand Kubernetes concepts.✖ More operational overhead → Though AWS sets up Kubernetes, you still .✖  → You  for the Kubernetes control plane.🎯 Real-World Example of How These Work TogetherImagine you’re running an :1️⃣ You  to package your app into a container.2️⃣ You deploy it to Kubernetes (DIY) if you want full control.3️⃣ If you don’t want to manage Kubernetes, you use  (simplest).4️⃣ If you need Kubernetes but don’t want manual setup, you use .📌 Think of Kubernetes as a powerful machine where you control everything.📌 Think of AWS ECS as a service where AWS does the heavy lifting for you.📌 Think of AWS EKS as Kubernetes, but AWS helps with setup.Conclusion: Which One Should You Use?👉  if you  and don’t care about moving to another cloud.👉  if you  but don’t want to set it up manually.👉 Use Self-Managed Kubernetes if you  and plan to run across multiple clouds (AWS, Azure, GCP, on-premise, etc.).💡 If you’re , start with .If you’re building , go for .If you want , use .]]></content:encoded></item><item><title>Understanding APIs: A Developer’s Guide to Building and Using APIs</title><link>https://blog.devops.dev/understanding-apis-a-developers-guide-to-building-and-using-apis-4253418d18ba?source=rss----33f8b2d9a328---4</link><author>Subbareddysangham</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:21 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[An Application Programming Interface (API) acts as a bridge between different software applications, allowing them to communicate with each other. Think of an API like a waiter in a restaurant — customers (the client application) don’t need to know how the kitchen (the server) prepares their food; they need to know how to place their order through the waiter (the API).I designed and developed an e-commerce web application with HTML, CSS, and JavaScript for the front end,  for the back end, and  for the database. I will use this as an example to explain the core concepts of APIs.E-commerce Web Application API Flow Chart:To check the complete source code:APIs in the E-Commerce ApplicationThis E-Commerce application consists of the following API endpoints:1. Authentication API (auth_routes): /api/auth/login (POST) → Authenticates users and starts a session.: /api/auth/logout (POST) → Clears session and logs out users.2. Product API (product_routes): /api/products (GET) → Returns a list of products.: /api/products/<int:product_id> (GET) → Fetches details of a specific product.3. Cart API (cart_routes): /api/cart (GET) → Returns the current user's cart.: /api/cart (POST) → Adds a product to the cart.: /api/cart/<int:item_id> (DELETE) → Removes an item from the cart.4. Order API (order_routes): /api/orders (POST) → Places an order with the items in the cart.: /api/orders/<int:order_id> (GET) → Fetches details of a specific order.: /api/health (GET) → Provides API uptime, session data, and frontend path.: /<path:filename> (GET) → Serves frontend files.: / (GET) → Serves index.html or API running message.: Handles missing resources.500 Internal Server Error: Handles unexpected issues.: Handles invalid requests.An API consists of several key components that work together:These are the URLs where the  can be accessed. Similar to a , each endpoint serves a specific purpose. For example,📌 https://api.ecommerce.com/products → Retrieves a list of available products.📌 https://api.ecommerce.com/cart → Fetches the current user's shopping cart details.📌 https://api.ecommerce.com/orders → Handles order-related operations.These actions can be performed on the allowed endpoints. They’re like verbs telling the API what to do with the data. → Read data (View products, orders, cart items). → Create new data (Add product, register user, place order). → Update existing data (Update profile, modify cart quantity). → Remove data (Delete cart item, cancel order). Additional data is sent to fine-tune the API request, such as specifying which page of results you want to see.: Parameters are extra details added to an API request to filter or refine the results.Fetch  of products:GET /api/products?category=laptopsSecurity measures ensure that only authorized users can access the API.: Ensures that only authorized users can access the API.When a user logs in, the API gives a :{  "message": "Login successful",  "token": "eyJhbGciOiJIUz..."}To add a product to the cart, the request must include this :POST /api/cart/addAuthorization: Bearer eyJhbGciOiJIUz... Prevents unauthorized access and protects user data. The structure of the data returned by the API, commonly in formats like JSON or XML.: The structure of the data sent back by the API. (because it’s easy to read and use).{  "id": 1,  "price": 799.99,} Frontend uses this data to display products to users.APIs are classified according to their usage patterns and architectures.API Types According to Purposes of Use🔹  — Used within a company, hidden from public access. Helps teams share data securely.🔹  — Available to everyone, can be free or paid. Example: Google Maps API.🔹  — Used between business partners for secure data exchange. Example: E-commerce & shipping company integration.🔹  — Combines multiple APIs into one request for efficiency. Example: Fetching account balance + transaction history in one call.API Types According to Architectural Structure:1. Web APIs (HTTP/HTTPS APIs)These are the most common APIs, operating over the internet using HTTP protocols. They come in several varieties:1.1. REST (Representational State Transfer):The most popular type of web API today. REST APIs follow these principles:Stateless: Each request contains all the information neededResource-based: Everything is treated as a resource with a unique URLUses standard HTTP methods (GET, POST, PUT, DELETE)Supports multiple data formats (usually JSON)Example REST API Request in an E-Commerce Web Application:This request fetches all available products from the online store.✅ Request (Client → Server)GET /api/products HTTP/1.1Host: api.ecommerce.comAuthorization: Bearer <User_Token>Content-Type: application/json✅ Response (Server → Client)[    {"id": 1, "name": "iPhone 15", "price": 999.99, "stock": 20},    {"id": 2, "name": "Samsung Galaxy S24", "price": 899.99, "stock": 15}]catalog.html Fetches and Displays Products1️⃣ User Visits catalog.htmlThe user opens the  page in their browser (http://52.90.222.178:5000/catalog.html).The browser  to fetch product data.2️⃣ Frontend (JavaScript) Sends an API RequestJavaScript code in catalog.html makes a GET request to the  /api/products.3️⃣ Backend API (GET /api/products) Fetches DataThe get_all_products() function runs when the frontend calls /api/products.4️⃣ Database Retrieves Product InformationThe backend queries the  in the databaseExample database response:[    {"id": 1, "name": "Laptop", "price": 799.99},    {"id": 2, "name": "Smartphone", "price": 499.99}]5️⃣ Frontend Renders Product Data in catalog.htmlThe JavaScript loops through the  and dynamically  to display products.<div class="product-card">  <h3>Laptop</h3>  <button onclick="addToCart(1)">Add to Cart</button><div class="product-card">  <p>Price: $499.99</p>  <button onclick="addToCart(2)">Add to Cart</button></div>1.2. SOAP (Simple Object Access Protocol):SOAP has strict rules and rigid messaging standards that can make it more secure than protocols such as REST. These types of APIs are frequently used in enterprise applications, particularly for payment processing and customer management, as they are highly safe in nature.A more rigid, protocol-specific API style used in enterprise environments:Highly structured messaging<soap:Envelope>  <soap:Header>    <Authorization>Bearer abc123</Authorization>  </soap:Header>    <GetUser>    </GetUser></soap:Envelope>A modern API query language that gives clients more control:Clients specify precisely what data they needSingle endpoint for all requestsReduces over-fetching and under-fetching of dataFacebook initially developed GraphQL to simplify endpoint management for REST-based APIs. Instead of maintaining multiple endpoints with small amounts of disjointed data, GraphQL provides a single endpoint that inputs complex queries and outputs only as much information as is needed for the query.query {  user(id: "123") {    email      title  }These are programming interfaces provided by software libraries or frameworks:Used directly in your codeNo network requests are neededUsually specific to a programming languageExample using a Python library API:import pandas as pd# Using pandas API to read a CSV filedf = pd.read_csv('data.csv')These allow applications to interact with the operating system:Example using Python’s OS API:import os# Using OS API to create a directoryos.mkdir('new_folder')The foundation of web APIs, using well-defined methods and status codes:PUT: Update existing dataPATCH: Partially update data2xx: Success (e.g., 200 OK)Enables real-time, two-way communication:Ideal for chat apps and live updatesExample WebSocket connection:const ws = new WebSocket('wss://api.example.com/chat');ws.onmessage = (event) => {    console.log('Received:', event.data);};gRPC (Google Remote Procedure Call) is a  framework for inter-service communication in microservices architecture. Unlike REST APIs that use , gRPC uses Protocol Buffers (Protobuf), making it faster and more efficient.Google’s high-performance RPC framework:Excellent for microservicesExample Protocol Buffer definition:📌 How gRPC Works in a Web ApplicationIn an , gRPC can be used for fast communication between microservices.A  must fetch a  from the backend .1️⃣ Defining gRPC Service (product.proto)gRPC services use Protocol Buffers (Protobuf) to define API contracts.GetAllProducts(): Returns a list of products.GetProductById(): Fetches a single product by ID.Product: Defines the product structure.2️⃣ Implementing gRPC Server (product_server.py)The gRPC server implements the service logic.Implements ProductService methods (GetAllProducts, GetProductById).3️⃣ Implementing gRPC Client (product_client.py)The client  to fetch data.Calls GetAllProducts() to fetch all products.Calls GetProductById() to fetch a single product.4️⃣ Running gRPC Server & Client# 1. Generate Python code from Protobufpython -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. product.protopython product_server.pypython product_client.pyProductService gRPC Server is running on port 50051...Product List: products {  name: "Laptop"}  id: 2  price: 499.991. GET: Used to retrieve dataIt is a request used to retrieve data. Never used to delete, update or insert data.Product API in E-Commerce Web Applicationcurl -X GET http://52.90.222.178:5000/api/products[   {"id": 1, "name": "iPhone 15", "price": 999.99},   {"id": 2, "name": "Samsung Galaxy S24", "price": 899.99}]If the API returns JSON, you can format the response using curl -X GET http://52.90.222.178:5000/api/products | jqDebugging & TroubleshootingIf you’re not getting the expected response, check:Is the Flask server running?curl -X GET http://52.90.222.178:5000/api/healthHow the /api/health Endpoint Works in FlaskThe endpoint is a  that provides the current status of the application, including . It helps in monitoring the system and ensuring that the API is .I have configured this /api/health endpoint in my E-Commerce Web Application.@app.route("/api/health", methods=["GET"])def health_check():    uptime = time.time() - start_time    return jsonify({        "uptime": f"{uptime:.2f} seconds",        "session_active": "username" in session    }), 2002. POST: Creates new resourcesThe  method is a request used to insert data. Posted data type — JSON.✅ Authentication EndpointsThis is handled in auth_routes.py, prefixed with /api/auth. → The frontend sends a request to the backend API (POST /api/auth/login). This is handled in auth_routes.py, prefixed with /api/auth.User sends a  with username & password.If credentials are valid:The user session is stored.API returns a success message.3. If credentials are invalid:API returns 3. PUT : Updates existing resourcesPUT method is used to create or update (replace) a resource. Useful for syncing data.Ex: We can add a “Change Password” feature for an existing user using a PUT /api/auth/change-password API endpoint.Steps to Implement “Change Password” APIPUT request with their old and new password.curl -X PUT http://localhost:5000/api/auth/change-password      -H "Content-Type: app     -d '{"old_password": "currentPass123", "new_password": "newPass456"}'2. API verifies the old password:If incorrect, return an error (401 Unauthorized).3. If correct, update the password in the database.4. Save the new password (after hashing it for security).5. Return a success message.{"message": "Password changed successfully"}In an e-commerce application like yours, a PUT method would typically be used in: → PUT /api/auth/update-profileUpdating Product Information (Admin) → PUT /api/products/<product_id> → PUT /api/cart/<cart_id> → PUT /api/orders/<order_id>4. DELETE: Removes resourcesThe DELETE method deletes the specified resource./api/cart API to Remove a Product from the Cart1️⃣ Endpoint Definition (Flask API):@cart_bp.route('/cart', methods=['DELETE'])def remove_from_cart():    Remove a product from the cart for the logged-in user.    Expects JSON payload: { product_id }.    """        user_id = session.get('user_id')            return jsonify({"message": "User not authenticated"}), 401        product_id = data.get('product_id')            return jsonify({"message": "'product_id' is required"}), 400        connection = get_db_connection()        cursor = connection.cursor()        delete_query = "DELETE FROM cart_items WHERE user_id = %s AND product_id = %s"        cursor.execute(delete_query, (user_id, product_id))            return jsonify({"message": "Product not found in cart"}), 404        return jsonify({"message": "Product removed from cart successfully"}), 200        return jsonify({"message": "Failed to remove product from cart", "error": str(e)}), 500        close_db_connection(connection)User sends a DELETE request with the product_id they want to remove.API verifies if the user is logged in (checks session['user_id']). (if missing, returns 400 Bad Request). to remove the product from the cart_items table.If the product does not exist, it returns 404 Not Found., it commits the transaction and returns a success message (200 OK).Handles database errors and ensures the connection is closed.3️⃣ Example API Request & Response:Now User wanted to delete iPhone 15 Pro from the cart:Click the  button under “iPhone 15 Pro”.The item should disappear, and the cart total should update.Run this command in the terminal:curl -X DELETE http://52.90.222.178:5000/api/cart      -H "Content-Type: application/json"      -H "Cookie: session=00068d4c-4b41-4e3e-8884-7389cabbb9b0"     -d '{"product_id": 4}'{    "message": "Product removed from cart successfully"After deletion of that item:5. PATCH: Partially updates resourcesPATCH method is to request used to update data. Only passed data will be updated. You don’t need to provide all the data set.The  method is used to  a resource. Instead of sending the entire data set, we only send the fields that need to be updated.Use Case: Updating a User’s Profile (PATCH /api/auth/update-profile)Imagine a user wants to update  or  without changing their username.1️⃣ PATCH Endpoint: PATCH /api/auth/update-profile2️⃣ Sending a PATCH RequestIf the user wants to update curl -X PATCH http://52.90.222.178:5000/api/auth/update-profile \     -H "Content-Type: application/json" \     -H "Cookie: session=your_valid_session_id" \     -d '{"email": "newemail@example.com"}'🔹 Only the  field will be updated.✅ {    "message": "Profile updated successfully"❌ If no fields are provided:{    "message": "No valid fields provided for update"{    "message": "User not authenticated"4️⃣ Why Use PATCH Instead of PUT?Conclusion: Understanding APIs, Endpoints, and Methods in Web DevelopmentAPIs (Application Programming Interfaces) allow different systems to  with each other. They define how requests and responses are exchanged between a client (browser, app) and a server.An  acts as a bridge between two applications, enabling data exchange.Example: A shopping website uses an API to fetch product details from a database.An  is a URL that clients use to request or send data.Example: GET /api/products retrieves all products. → Uses HTTP methods (GET, POST, PUT, DELETE) to manage data. → Lets clients request specific data fields, reducing unnecessary data transfer. → Uses XML messaging, mainly in enterprise applications. → Maintains a continuous connection for real-time updates (e.g., live chat).Use  (JWT, API Keys, OAuth) to restrict access.Protect sensitive data with .Implement  to prevent abuse.APIs are the backbone of modern applications, enabling data sharing between different services. Developers create smooth and efficient digital experiences by designing well-structured and secure APIs.I’d love to hear what you think about this article — feel free to share your opinions in the comments below (or above, depending on your device!). If you found this helpful or enjoyable, a clap, a comment, or a highlight of your favourite sections would mean a lot.For more insights into the world of technology and data, visit  There’s plenty of exciting content waiting for you to explore!Thank you for reading, and happy learning! 🚀]]></content:encoded></item><item><title>Most Developers Get This Wrong in Docker Networking!</title><link>https://blog.devops.dev/most-developers-get-this-wrong-in-docker-networking-359dbb3eac16?source=rss----33f8b2d9a328---4</link><author>Gaddam.Naveen</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:15 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Jenkins in Kubernetes: Deployment and Persistent Storage(volume) Setup</title><link>https://blog.devops.dev/jenkins-in-kubernetes-deployment-and-persistent-storage-volume-setup-a70fe0579ac8?source=rss----33f8b2d9a328---4</link><author>th@n@n</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:10 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Jenkins, a popular automation server, becomes even more powerful when deployed in Kubernetes. Ensuring its availability and data persistence is crucial for uninterrupted CI/CD pipelines. In this guide, we’ll walk through deploying Jenkins in Kubernetes, configuring its resources, and setting up persistent storage to safeguard critical data.In this configuration, we have a Deployment resource for deploying Jenkins in Kubernetes, along with associated PersistentVolumeClaim (PVC), PersistentVolume (PV), Service, and StorageClass resources. Let’s break down each partkind: StorageClassapiVersion: storage.k8s.io/v1  name: localstorageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumerThis StorageClass resource defines storage provisioning and management policies.Since provisioner is set to kubernetes.io/no-provisioner, it indicates that no dynamic provisioning is performed by Kubernetes.volumeBindingMode: WaitForFirstConsumer ensures that volume binding waits for the first Pod using the PersistentVolumeClaim to be created.PersistentVolumeClaim (PVC) Resource:apiVersion: v1kind: PersistentVolumeClaim  name: pvc-jenkinsspec:  storageClassName: localstorage  accessModes:  resources:      storage: 2GiThis PVC resource requests storage from a PersistentVolume using the localstorage StorageClass.It requests 2Gi of storage with access mode ReadWriteOnce, meaning it can be mounted as read-write by a single node.PersistentVolume (PV) Resource:apiVersion: v1kind: PersistentVolume  name: pv-jenkins    type: local  claimRef:    namespace: jenkins    storage: 3Gi    - ReadWriteOnce    path: /mnt  storageClassName: localstorageThis PV resource represents the actual storage volume available for use by the PVC.It is bound to the PVC pvc-jenkins within the jenkins namespace.The PV has a capacity of 3Gi and is accessible in ReadWriteOnce mode.The storage is provided by a hostPath /mnt on the host machine, with storage class localstorage.apiVersion: apps/v1kind: Deployment  name: jenkins-deployment    name: jenkinsspec:    matchLabels:  replicas: 1    metadata:      labels:    spec:        - name: deployment-jenkins          image: jenkins/jenkins:lts          resources:              memory: "0.5Gi"            requests:              cpu: "125m"            - name: http-port            - name: jnlp-port          livenessProbe:              path: "/login"            initialDelaySeconds: 60            timeoutSeconds: 5          readinessProbe:              path: "/login"            initialDelaySeconds: 60            timeoutSeconds: 5          volumeMounts:              mountPath: /var/jenkins_home        - name: data-jenkins            claimName: pvc-jenkins        runAsUser: 0        fsGroup: 0This Deployment resource defines how Jenkins is deployed.It specifies a single replica (replicas: 1) of the Jenkins container.The container is based on the jenkins/jenkins:lts image.Resource limits and requests for CPU and memory are set to ensure resource allocation.Ports 8080 and 50000 are exposed for HTTP and JNLP respectively.Liveness and readiness probes are configured to check the health of the container.The Jenkins home directory (/var/jenkins_home) is mounted to a PersistentVolumeClaim (pvc-jenkins) named data-jenkins.SecurityContext ensures that Jenkins runs with the appropriate user and group permissions. securityContext:        runAsUser: 0        fsGroup: 0When you deploy this deployment instance in kubernetes cluster, make sure the user have the right privileges to read and write the host volume For this demo purpose, I am using the root user to do this task, but this is not encouraged to do in real environment.apiVersion: v1kind: Service  name: jenkins-servicespec:    app: jenkins-pod  ports:      port: 8080      nodePort: 32000This Service resource exposes the Jenkins deployment internally within the jenkins namespace.It selects pods with the label app: jenkins-pod.The service type is NodePort, making the service accessible from outside the cluster on each node's IP at a static port (nodePort: 32000).Port 8080 is mapped to the targetPort 8080 where Jenkins is listening.Once you execute all the manifiest file in kubernetes cluster.Check the host volume path ls -al /mntExecute the below command to see the whether the same files are present in the jenkins containerkubectl exec -it POD_NAME /bin/bash -n jenkinsls -al /var/jenkins_homeThis configuration sets up Jenkins deployment in Kubernetes with persistence using a PersistentVolume and PersistentVolumeClaim. It ensures that Jenkins data stored in /var/jenkins_home persists across container restarts and pod rescheduling. Additionally, the Service resource exposes Jenkins for external access within the Kubernetes cluster.For now, that’s it guys, If you like this article don’t forget to give a clap. 👏]]></content:encoded></item><item><title>How to Build and Deploy a Simple Frontend App with Python Backend</title><link>https://blog.devops.dev/how-to-build-and-deploy-a-simple-frontend-app-with-python-backend-108b505be2be?source=rss----33f8b2d9a328---4</link><author>krth1k</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:50:06 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Building a full-stack web application might seem daunting, especially if you’re primarily a backend developer. However, with the right approach, you can create a simple frontend and connect it to a Python backend with ease.In this guide, we’ll walk through the process of:Setting up a basic Python backend with FlaskCreating a simple frontend with HTML, CSS, and JavaScriptConnecting the frontend to the backend using REST APIDeploying the app on a local Kubernetes cluster1. Setting Up the Python BackendWe’ll use , a lightweight Python web framework, to create a REST API that serves data to the frontend.Ensure you have Python installed, then install Flask:Create a new file called app.py:from flask import Flask, jsonify@app.route('/api/message')def get_message():    return jsonify({"message": "Hello from the Python backend!"})if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)This API exposes a single endpoint /api/message that returns a JSON response.For the frontend, we’ll use HTML, CSS, and JavaScript to display the data from our backend.Create an HTML File (index.html)<!DOCTYPE html><html lang="en">    <meta charset="UTF-8">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <title>Frontend App</title>        body {            font-family: Arial, sans-serif;            text-align: center;        }            padding: 10px 20px;        }</head>    <h1>Simple Frontend App</h1>    <button onclick="fetchMessage()">Get Message</button>    <p id="message"></p>        function fetchMessage() {            fetch('http://127.0.0.1:5000/api/message')                .then(response => response.json())                    document.getElementById("message").innerText = data.message;                .catch(error => console.error('Error:', error));    </script></html>This page has a button that fetches and displays a message from the Flask backend.3. Connecting the Frontend to the BackendNow, let’s serve the frontend using  itself so that both frontend and backend are accessible from the same origin.Update app.py to Serve HTMLModify app.py to serve the index.html file:from flask import Flask, jsonify, send_from_directoryapp = Flask(__name__, static_folder='static')@app.route('/api/message')def get_message():    return jsonify({"message": "Hello from the Python backend!"})@app.route('/')def serve_frontend():    return send_from_directory('static', 'index.html')if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)Move index.html to a static FolderYour project structure should now look like this:/project-folder│-- app.py│   └── index.htmlNow, visit http://127.0.0.1:5000/ in your browser, and your frontend will be served!Now, let’s deploy this app using .# Use Python base imageFROM python:3.9# Set the working directoryWORKDIR /app# Copy application filesCOPY . .# Install dependenciesRUN pip install flask# Expose port 5000EXPOSE 5000# Run the appCMD ["python", "app.py"]Build and Run the Docker Containerdocker build -t myapp .docker run -p 5000:5000 myappCreate a Kubernetes Deployment YAML (apiVersion: apps/v1kind: Deployment  name: myapp  replicas: 1    matchLabels:  template:      labels:    spec:        - name: myapp          ports:---kind: Service  name: myapp-service  selector:  ports:      port: 80  type: NodePortkubectl apply -f deployment.yamlminikube service myapp-service --urlVisit the displayed URL in your browser!In this tutorial, we covered: ✅ Creating a Flask backend ✅ Building a simple HTML/JavaScript frontend ✅ Connecting the frontend to the backend ✅ Deploying the app with Docker and KubernetesThis is a basic example, but you can expand it by:Adding user authenticationUsing React or Vue.js for a modern frontendStoring and retrieving data from a databaseIf you found this helpful, let me know in the comments! 🚀]]></content:encoded></item><item><title>Best Practices For Database Authorization In Multi-Tenant Systems</title><link>https://blog.devops.dev/best-practices-for-database-authorization-in-multi-tenant-systems-001a1bcf2568?source=rss----33f8b2d9a328---4</link><author>Noel</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:49:43 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Multi-tenant databases allow multiple companies or organizations (tenants) to securely share the same database infrastructure while ensuring data isolation and integrity. However, this shared structure introduces complexities in managing access and authorization. A robust authorization strategy is essential to ensure that users can only access resources belonging to their tenant without compromising scalability or performance.This article explores the best practices and technical solutions that we adopt at Xinthe, for implementing efficient authorization mechanisms in multi-tenant systems, with a focus on nested resource structures, such as companies, clients, projects, and tasks. After having read this entire article, you will be armed with actionable insights to build secure, efficient, and future-proof authorization strategies for multi-tenant applications.Understanding Multi-Tenant Database AuthorizationDefinition Of Multi-TenancyMulti-tenancy refers to an architectural pattern where a single instance of a software application and its database serves multiple tenants (e.g., companies, organizations, or users). Each tenant’s data remains logically isolated, ensuring that no tenant can access another’s data, while sharing underlying resources for efficiency.Key Multi-Tenancy Models:Each tenant has a dedicated database or schema.Offers strong isolation and security.Higher costs and maintenance complexity due to multiple instances.Multiple tenants share the same database.Logical separation is maintained through identifiers (e.g., tenant_id or company_id).Cost-effective and scalable but requires robust authorization mechanisms.Challenges Of AuthorizationImplementing authorization in multi-tenant systems is a non-trivial task, especially as the scale and complexity of resources grow. Common challenges include -Cross-Tenant Data Leakage:Risk: Improper queries or configurations can expose data to unauthorized tenants.Example: A user from Company A inadvertently accessing tasks belonging to Company B due to a missing or incorrect WHERE clause.Deeply nested resource structures often require joins across multiple tables.Queries with extensive joins can degrade performance as data volume increases.Scalability & Maintainability:The need to balance fast access controls with a maintainable schema.Adding new authorization rules or resource types without overhauling the system.Data Localization & Compliance:For multi-tenant systems spanning regions, ensuring that tenant data complies with regulations like GDPR can complicate authorization logic.Importance Of Nested Resource StructuresIn many applications, resources are interconnected in a hierarchical fashion. Consider the following nested structure -Company → Client → Project → TaskA  has multiple .Each  manages several .Each  contains multiple .Why Nested Structures Matter:Access Control Complexity: Permissions must flow through the hierarchy (e.g., a user’s access to a task must be verified against their company). Hierarchical access often necessitates multiple joins, impacting query efficiency. Hierarchical structures reflect real-world use cases like SaaS platforms, where users must operate within their organization’s boundaries.A user from Company A should only edit tasks within their projects. Authorization must ensure that the task → project → client → company linkage is maintained without exposing data from Company B.Comparing Authorization ApproachesWhen implementing authorization in a multi-tenant database, there are three common strategies to choose from: the , the , and Tenant-Specific Databases or Tables. Each comes with its own set of benefits and trade-offs. Let’s break them down -1. Flat Model (Adding Tenant IDs)In this approach, a tenant_id or company_id is added to every resource table (e.g., tasks, projects, clients), enabling direct filtering for authorization. Queries can directly filter by tenant_id without traversing the hierarchy.SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id; Reduces query complexity by avoiding multiple table joins to enforce access. Straightforward implementation makes it easy to debug and maintain. tenant_id is replicated across multiple tables, introducing redundancy. Adding tenant_id and other metadata can lead to bloated schemas, especially as the number of attributes grows. Schema updates (e.g., adding new relationships) might require extensive changes across multiple tables.Ideal for systems where performance is critical and the schema is relatively stable, such as SaaS platforms with many small tenants.2. Hierarchical Model (Enforcing Relationships)In this approach, the relationships between resources (e.g., task → project → client → company) are strictly enforced through foreign keys. Authorization is achieved by traversing the hierarchy. Avoids redundant fields by relying on inherent relationships.CREATE TABLE tasks (    id SERIAL PRIMARY KEY,    project_id INT REFERENCES projects(id),    ...    id SERIAL PRIMARY KEY,    client_id INT REFERENCES clients(id),    ...    id SERIAL PRIMARY KEY,    company_id INT REFERENCES companies(id),    ... Reduces duplication of metadata like tenant_id.Relationship-Centric Queries: Makes it easier to enforce hierarchical constraints and maintain referential integrity. Queries require multiple joins to verify access, which can impact performance.SELECT t.*FROM tasks tJOIN projects p ON t.project_id = p.idJOIN clients c ON p.client_id = c.idWHERE c.company_id = :tenant_id AND t.id = :task_id; Deep hierarchies with large datasets can significantly increase query execution time. As the hierarchy grows, maintaining performance becomes challenging.Suitable for applications where maintaining strict relationships between resources is essential, such as ERP systems or large enterprise applications.3. Tenant-Specific Databases Or TablesThis approach creates separate databases or tables for each tenant, isolating their data entirely. Each tenant’s data can be managed independently, making it easier to scale horizontally by distributing databases across servers. Ensures complete data isolation, reducing the risk of cross-tenant data leakage. Simplifies adherence to regulations like GDPR by enabling tenant-specific backups, retention policies, and deletions. Managing multiple databases or schemas requires sophisticated deployment and CI/CD pipelines. Schema updates need to be applied consistently across all tenant databases. For tenants with small datasets, the resource consumption of separate databases might be inefficient.Best for large organizations with high regulatory or security requirements, or when dealing with tenants that require dedicated resources (e.g., enterprise customers).Summary Table — Comparing ApproachesCriteria For Choosing An Authorization ModelSelecting the right authorization model for a multi-tenant database is critical for ensuring scalability, performance, and compliance. The decision hinges on a combination of technical, regulatory, and operational factors. Below are the primary criteria to consider -The level of traffic and query complexity your application handles directly impacts the choice of an authorization model.High-Traffic Applications:Benefit from simpler and faster queries, such as those enabled by the .SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;Minimal joins mean lower query latency, ensuring the system performs well under heavy loads.Suitable for SaaS platforms or e-commerce systems with a high volume of tenant interactions.:Can afford the  with more joins, as performance trade-offs are less significant.Allows for cleaner schema designs and strict relational integrity.Suitable for internal enterprise tools or smaller-scale applications.2. Regulatory RequirementsCompliance with data protection and privacy regulations often dictates how data is stored and accessed.Using Tenant-Specific Databases or Tables simplifies compliance for regulations like GDPR or HIPAA.Tenant isolation reduces the risk of data leakage and ensures tenant-specific data retention and deletion policies.An enterprise customer requires dedicated storage with separate backups and audit logs.A  can still meet compliance needs with appropriate access controls and audit mechanisms.Challenges arise in managing and enforcing tenant-specific data governance policies within shared infrastructure.The ability to handle growth in the number of tenants and data volumes is a critical factor.Planning For Tenant Growth:For a rapidly scaling user base, Tenant-Specific Databases or Tables provide the most flexibility -Each tenant can be distributed across servers to balance load.Tenant databases can be independently scaled based on specific needs.A B2B SaaS platform serving both small businesses and large enterprises can allocate resources dynamically based on tenant size.The  can handle larger datasets more efficiently as indexes on tenant_id make filtering faster.The  may struggle as table sizes grow, requiring optimization for complex joins.Ease of schema management and updates is essential for long-term maintainability.Simplified Schema Updates:The  simplifies schema updates by centralizing data attributes like tenant_id.However, redundant fields may increase the risk of errors during updates.The  enforces relational integrity, ensuring data consistency.Complex queries for nested structures may require more effort to maintain and optimize.Automated CI/CD Pipelines:For Tenant-Specific Databases, CI/CD automation becomes critical to manage schema changes across multiple databases.Tools like Octopus Deploy or Liquibase can help automate schema migrations and ensure consistency.Key Considerations SummaryDesigning Authorization Strategies For Multi-TenancyDesigning robust authorization strategies for multi-tenant systems requires careful consideration of schema design, indexing, and data partitioning to ensure scalability, security, and performance. This section outlines best practices for implementing these strategies effectively.The foundation of a successful multi-tenant authorization system lies in a well-thought-out schema.Add a tenant_id column to all relevant tables (e.g., clients, projects, tasks) for direct tenant filtering.CREATE TABLE tasks (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    project_id BIGINT NOT NULL,    status VARCHAR(20),    FOREIGN KEY (project_id) REFERENCES projects(id));Ensure tenant_id is a mandatory field in all write operations to enforce multi-tenancy constraints.Defining Relationships In Hierarchical Structures:Maintain strict referential integrity between hierarchical entities.Example for hierarchical relationships -CREATE TABLE projects (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    client_id BIGINT NOT NULL,    FOREIGN KEY (client_id) REFERENCES clients(id)Flat schema enables quick lookups for tenant-specific data.Hierarchical relationships ensure data consistency and logical separation.2. Indexing Best PracticesIndexes are essential for optimizing queries in multi-tenant systems. However, improper indexing can lead to inefficiencies.Compound Indexes For Tenant-Specific Queries:Use composite indexes combining tenant_id with frequently queried columns.CREATE INDEX idx_tasks_tenant_projectON tasks (tenant_id, project_id, status);This enables efficient filtering and sorting within a tenant’s scope.Balancing Indexing Depth & Query Speed:Avoid over-indexing, which can slow down write operations.Prioritize indexing columns involved in filtering, joining, and sorting operations.Regularly analyze query performance using tools like  in PostgreSQL or  in MySQL.Partitioning improves scalability by dividing data into smaller, more manageable segments, reducing query times for tenant-specific operations.Horizontal Partitioning By Tenants:Partition data within a single database based on tenant_id.CREATE TABLE tasks_1 PARTITION OF tasksFOR VALUES IN (1); -- Partition for tenant_id 1Faster tenant-specific queries as partitions reduce the search space.Simplifies maintenance for large datasets.Database Sharding For High-Scale Systems:Distribute tenant data across multiple databases (shards).Example Sharding Strategy -Use tenant_id % shard_count to assign tenants to shards.Tools like  or  can manage sharding in distributed database systems.Eliminates contention in single-database systems.Enhances fault isolation and scalability.Example Use Case — Applying These StrategiesAn application manages 100,000 tenants, each with thousands of projects and tasks.Add tenant_id to all tables.Use foreign keys to link tasks → projects → clients.Create a compound index on tasks (tenant_id, project_id) for common queries like -SELECT * FROM tasks WHERE tenant_id = 123 AND project_id = 456;For smaller tenants, use horizontal partitioning -CREATE TABLE tasks_tenant_123 PARTITION OF tasks FOR VALUES IN (123);For larger tenants, shard data across multiple databases to scale.A well-designed schema with tenant_id simplifies multi-tenant data filtering.Proper indexing ensures efficient queries, even at scale.Partitioning and sharding prepare the system for growth, reducing query times and enhancing reliability.This section provides concrete examples of implementing different authorization models for multi-tenant systems, including schemas, queries, and tooling. Each approach demonstrates how to enforce tenant-specific access effectively.1. Flat Model ImplementationThe flat model relies on adding a tenant_id column to all relevant tables, ensuring that queries are scoped to the tenant directly.CREATE TABLE tasks (    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    project_id BIGINT NOT NULL,    status VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    FOREIGN KEY (project_id) REFERENCES projects(id)    id BIGINT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    client_id BIGINT NOT NULL,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    FOREIGN KEY (client_id) REFERENCES clients(id));: Access tasks for a user’s company -SELECT * FROM tasks WHERE tenant_id = 123 AND status = 'in_progress';Simplifies authorization logic with direct lookups.Reduces query complexity by avoiding joins.Potential schema bloat with additional tenant_id columns.2. Hierarchical Model ImplementationIn this model, tenant authorization is enforced by traversing relationships between resources (e.g., Company → Client → Project → Task).CREATE TABLE companies (    id BIGINT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT PRIMARY KEY,    company_id BIGINT NOT NULL,    name VARCHAR(255),    FOREIGN KEY (company_id) REFERENCES companies(id));    id BIGINT PRIMARY KEY,    client_id BIGINT NOT NULL,    name VARCHAR(255),    FOREIGN KEY (client_id) REFERENCES clients(id));    id BIGINT PRIMARY KEY,    project_id BIGINT NOT NULL,    description TEXT,    FOREIGN KEY (project_id) REFERENCES projects(id): Check task access by traversing relationships -SELECT t.* FROM tasks tINNER JOIN projects p ON t.project_id = p.idINNER JOIN clients c ON p.client_id = c.idINNER JOIN companies co ON c.company_id = co.idWHERE co.id = 123 AND t.status = 'in_progress';Maintains normalized relationships.Avoids redundant tenant_id columns.Complex joins increase query costs.Requires optimized indexes to maintain performance.3. Tenant-Specific Database/Table ImplementationFor scenarios requiring strict isolation, separate databases or tables for each tenant can be used.Create a separate database or schema for each tenant -CREATE DATABASE company_123;CREATE TABLE company_123.tasks (    project_id BIGINT NOT NULL,    status VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);: Access tasks for a specific tenant -USE company_123;SELECT * WHERE status = 'in_progress';: Use CI/CD tools like  to manage multi-tenant databases:Automate schema changes across databases.Track versioning for each tenant’s database.Complete tenant isolation for security and compliance (e.g., GDPR).Simplifies data archival and backup for individual tenants.Deployment complexity increases with the number of tenants.Resource-intensive for systems with many small tenants.Choosing The Right ImplementationUse the flat model for simplicity in high-traffic environments.Use the hierarchical model when data relationships must be preserved and redundancy minimized.Opt for tenant-specific databases for strict isolation and compliance requirements.Each implementation can be tailored based on application needs, tenant size, and regulatory requirements. Balancing performance, scalability, and maintainability is key to successful multi-tenant authorization systems.Security Best Practices For AuthorizationEnsuring robust security in multi-tenant systems is essential to prevent data breaches, maintain compliance, and build user trust. This section outlines key practices for implementing secure and reliable authorization mechanisms.1. Strict Access ControlsImplementing strong access controls ensures that only authorized users can access or modify resources.Role-Based Access Control (RBAC):Assign roles (e.g., Admin, Manager, User) to users based on their responsibilities.Enforce role-specific permissions at the application and database layers.Example: Use database roles to restrict access to tenant-specific tables.CREATE ROLE company_admin;GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO company_admin;REVOKE ALL ON ALL TABLES FROM PUBLIC; -- Restrict public access:Enforce tenant-level data segregation directly at the database layer.RLS ensures that queries automatically filter data based on the user’s tenant.PostgreSQL Example For RLS:CREATE POLICY tenant_policyON tasksUSING (tenant_id = current_setting('app.current_tenant')::BIGINT);ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;SET app.current_tenant = '123'; -- Simulate tenant contextSELECT * FROM tasks; -- Only tasks with tenant_id = 123 will be visible2. Preventing Cross-Tenant Data LeaksPreventing accidental or intentional cross-tenant data leaks is critical in multi-tenant architectures.Multi-Layer Access Checks:Enforce tenant isolation at both the database and application layers.Validate all queries to ensure they are scoped to the user’s tenant.Always include a tenant_id check in database queries.Use database views or abstractions to simplify tenant-specific filtering.Application-Layer Validation:Add additional validation at the application level as a guardrail.Ensure that APIs restrict data access to the authenticated tenant context.def get_user_tasks(user):    if user.tenant_id != request.tenant_id:        raise PermissionDenied("Cross-tenant access is not allowed.")    return db.query(Tasks).filter(Tasks.tenant_id == user.tenant_id).all()Audit logs are essential for monitoring, compliance, and debugging. They provide visibility into access patterns and help detect unauthorized access attempts.User ID and tenant ID for all queries.Access attempts (successful and failed).Data modification operations (insert, update, delete).Timestamps and IP addresses for requests.SQL Example For Logging Queries:CREATE TABLE audit_logs (    id SERIAL PRIMARY KEY,    tenant_id BIGINT,    table_name VARCHAR(255),    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPINSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)VALUES (123, 456, 'SELECT', 'tasks', 'SELECT * FROM tasks WHERE tenant_id = 456');Integrating Logging Tools:Use database triggers to log operations automatically.Combine with external tools like  (Elasticsearch, Logstash, Kibana) or  for advanced monitoring.CREATE OR REPLACE FUNCTION log_task_changes()RETURNS TRIGGER AS $$    INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)    VALUES (current_user_id(), NEW.tenant_id, TG_OP, TG_TABLE_NAME, current_query());    RETURN NEW;$$ LANGUAGE plpgsql;CREATE TRIGGER audit_task_changesAFTER INSERT OR UPDATE OR DELETE ON tasksFOR EACH ROW EXECUTE FUNCTION log_task_changes();Combine RBAC, RLS, and application-level validation for comprehensive protection.Use multi-layered access checks and robust query scoping to ensure tenant isolation.Maintain detailed audit logs to track access and modifications for accountability and compliance.These practices create a secure foundation for multi-tenant authorization systems, ensuring that each tenant’s data is isolated, protected, and auditable.Designing a robust multi-tenant authorization system involves navigating a set of challenges and trade-offs. Each approach has its own set of complexities that must be carefully managed to ensure scalability, performance, and maintainability.1. Balancing Performance & FlexibilityChoosing between speed and schema cleanliness can significantly impact your database design and performance.Prioritizing Performance (Flat Model):Direct lookups using a tenant_id column ensure fast query execution.Reduced join complexity leads to quicker response times.: May result in data redundancy (e.g., repeating tenant IDs across multiple tables).SQL Example For Optimized Query:SELECT * FROM tasks WHERE tenant_id = 123 AND status = 'pending';Prioritizing Schema Cleanliness (Hierarchical Model):Using a normalized schema ensures a clean and consistent database structure.: Requires more complex joins and increased query times, especially for deeply nested relationships.Hierarchical Query Example:SELECT tasks.*FROM tasksJOIN projects ON tasks.project_id = projects.idJOIN clients ON projects.client_id = clients.idWHERE clients.tenant_id = 123;Managing tenant-specific setups adds complexity, particularly as the number of tenants grows.Tenant-Specific Databases:Each tenant has its own database, simplifying compliance and data isolation.: Maintaining consistency across databases for schema changes.: Use automation tools like  or  to manage schema migrations across tenants.# Liquibase command to apply migrations to multiple tenant databasesliquibase --url="jdbc:mysql://db_host/tenant1" updateliquibase --url="jdbc:mysql://db_host/tenant2" updateSingle Multi-Tenant Database:Shared schema reduces maintenance but requires more sophisticated query scoping and indexing.: Tracking and isolating tenant data effectively without introducing query overhead.3. Handling Schema Updates In Multi-Tenant DatabasesEnsuring all tenants have consistent schemas while minimizing downtime is one of the most significant challenges in multi-tenant systems.Use versioned migrations to apply incremental updates across all tenants.Maintain backward compatibility to prevent disruptions during updates.CREATE TABLE schema_versions (    tenant_id BIGINT,    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPApply updates to a subset of tenants, validate, and then roll out to the rest.Use feature flags to selectively enable new schema features.Code Example For Rolling Updates:tenants = get_tenant_list()for tenant in tenants:    apply_schema_update(tenant_id=tenant.id)Testing & CI/CD For Multi-Tenant Systems:Test migrations on a staging environment with realistic tenant data before deploying.Use CI/CD tools like  to automate and track updates across tenant databases.Performance Vs. Flexibility:Use a flat model for high-speed queries or hierarchical models for cleaner schemas but expect performance trade-offs.Tenant-specific databases simplify compliance but require robust automation for schema management.Implement version control and rolling updates to ensure seamless schema changes across all tenants.Addressing these challenges with well-defined strategies ensures a scalable and maintainable multi-tenant authorization system, capable of adapting to evolving application needs.Real-world applications of multi-tenant database authorization vary depending on the complexity of the resource structure, performance requirements, and compliance needs. Below are three illustrative scenarios demonstrating how different authorization models can be applied effectively.Scenario 1 — Flat Model For A SaaS CRM AppA SaaS customer relationship management (CRM) application needs to store and manage customer interactions for multiple companies, ensuring users can only access data associated with their organization.: Each company has its own sales team, and users need quick access to customer records and sales data.: Use a flat model by adding tenant_id to every table, such as customers, leads, and sales.CREATE TABLE customers (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    name VARCHAR(255),    phone VARCHAR(20),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    tenant_id BIGINT NOT NULL,    customer_id BIGINT,    status VARCHAR(50),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);SELECT * FROM customers Simple queries without joins for tenant-specific data.High performance due to direct lookups.Wider tables due to the inclusion of tenant_id.Potential redundancy if relationships between entities are not properly normalized.Scenario 2 — Hierarchical Model For A Project Management ToolA project management tool with a nested structure: Company → Client → Project → Task. Users need to manage projects while maintaining strict access control based on their organization.: Each company has multiple clients, each with its own projects and tasks. Users must only access tasks related to their company.: Use a hierarchical model to enforce relationships and control access through joins.CREATE TABLE companies (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT AUTO_INCREMENT PRIMARY KEY,    company_id BIGINT NOT NULL,    name VARCHAR(255),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    client_id BIGINT NOT NULL,    name VARCHAR(255),    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);    id BIGINT AUTO_INCREMENT PRIMARY KEY,    project_id BIGINT NOT NULL,    description TEXT,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMPSELECT tasks.*FROM tasksJOIN projects ON tasks.project_id = projects.idJOIN clients ON projects.client_id = clients.idWHERE clients.company_id = 123;Clean, normalized schema without redundant data.Access naturally follows the hierarchy.Complex queries due to multi-level joins.Slower query performance for deep hierarchies.Scenario 3 — Single-Tenant Databases For An Enterprise AppAn enterprise application handles sensitive data, requiring strict data isolation for compliance with GDPR and HIPAA regulations.: Each tenant’s data must be completely isolated to ensure compliance and scalability.: Use a single-tenant database model where each company has its own dedicated database.Database Names:  tenant_1_db  tenant_3_db: Use  to manage database updates across multiple tenants.deploy:  steps:    - name: Update Tenant Databases      script: |        for db in $(list_databases); do          apply_migrations $dbComplete isolation ensures compliance with regulatory requirements.Scalability: Large tenants can have dedicated resources (e.g., separate hardware).Higher operational complexity in managing multiple databases.Requires robust CI/CD pipelines for schema updates.Each use case demonstrates how careful consideration of application requirements, data relationships, and compliance needs can guide the choice of the best authorization model for a multi-tenant database.Efficient, scalable, and secure authorization in multi-tenant databases as we’ve found often at Xinthe, requires a well-thought-out approach tailored to the application’s needs.Authorization in multi-tenant databases isn’t a one-size-fits-all challenge. Developers and database architects must carefully evaluate their application’s structure, expected growth, and regulatory needs to select the most effective approach. Armed with the insights and strategies outlined in this article, you can design multi-tenant database systems that are secure, scalable, and efficient, ensuring both developer productivity and a seamless user experience.]]></content:encoded></item><item><title>Docker, Kubernetes, and NATS — The Backbone of Cloud-Native Apps</title><link>https://blog.devops.dev/docker-kubernetes-and-nats-the-backbone-of-cloud-native-apps-af724f41c17d?source=rss----33f8b2d9a328---4</link><author>Cristhian Ferrufino</author><category>devops</category><pubDate>Thu, 13 Feb 2025 16:48:09 +0000</pubDate><source url="https://blog.devops.dev/?source=rss----33f8b2d9a328---4">Devops.dev blog</source><content:encoded><![CDATA[Docker, Kubernetes, and NATS — The Backbone of Cloud-Native AppsWelcome back to the ! In the last article, we explored the world of message brokers and why NATS is a standout choice for modern microservices. Now, it’s time to dive into the backbone of cloud-native applications:  and . If microservices are the chefs in our restaurant analogy, containers are the kitchen tools that keep everything running smoothly. And Kubernetes? That’s the head chef, making sure everyone works in harmony.In this article, we’ll break down  and , explore how they work together, and even touch on how  fits into the mix. By the end, you’ll have a solid understanding of how to containerize your applications and orchestrate them like a pro. Let’s get cooking!What Is Containerization, and Why Is It Important?Imagine you’re shipping a fragile package across the world. You’d want to pack it in a sturdy container, right? That’s exactly what containerization does for your applications. It packages your app and all its dependencies (libraries, frameworks, etc.) into a lightweight, portable unit called a . This ensures that your app runs consistently across different environments — whether it’s your laptop, a testing server, or a production cluster.Why developers love containers:✅ : “Works on my machine” becomes “Works everywhere.”✅ : No more dependency hell — each app lives in its own bubble.✅ : Deploy to AWS, Azure, or your grandma’s PC (if she’s cool with Kubernetes).✅ : 10x lighter than VMs. Think EVs vs. a gas-guzzling truckIntroduction to Docker: Building, Running, and Managing ContainersDocker is the most popular tool for containerization, and for good reason. It’s simple, powerful, and widely supported. Let’s break it down:To create a container, you start with a  — a text file that defines the steps to build your app’s environment. Here’s a simple example:# Use a lightweight Python image (because nobody likes bloat)FROM python:3.9-slim# Set the stage for your appWORKDIR /app# Install dependencies (Pro tip: Skip the cache to shrink your image)COPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txt# Copy the rest of the codeCOPY . .# Open the app’s “front door”EXPOSE 8080CMD ["python", "app.py"]With this Dockerfile, you can build a container image using the `` command:🚀 Run this: docker build -t my-python-app .Once you’ve built your image, you can run it as a container:🎯 Pro tip: Map ports like a pirate mapping treasure.  docker run -p 8080:8080 my-python-appThis command starts your app and maps port 8080 on your host to port 8080 in the container. Easy, right?Docker also provides tools to manage your containers:: List running containers.docker logs <container_id>: View logs for a specific containerdocker stop <container_id>: Stop a running container.Docker vs. Podman: A Detailed ComparisonWhile Docker is the most popular containerization tool, it’s not the only one.  is a rising star in the container world, and it’s worth understanding how it compares to Docker.+----------------------+-------------------------------------+-----------------------------------+| Feature              | Docker 🐳                           | Podman 📦                         |+----------------------+-------------------------------------+-----------------------------------+| Daemon Requirement   | Requires a daemon (dockerd)         | Daemonless (runs containers       ||                      |                                     | directly)                         |+----------------------+-------------------------------------+-----------------------------------+| Root vs. Rootless    | Runs as root by default              | Supports rootless containers out  ||                      |                                     | of the box                        |+----------------------+-------------------------------------+-----------------------------------+| Compatibility        | Uses Docker CLI and Dockerfiles     | Fully compatible with Docker CLI  ||                      |                                     | and Dockerfiles                   |+----------------------+-------------------------------------+-----------------------------------+| Security             | Requires root privileges, which can | Rootless mode reduces attack      ||                      | be a security risk                  | surface                           |+----------------------+-------------------------------------+-----------------------------------+| Orchestration        | Requires Docker Swarm for           | Integrates with Kubernetes        ||                      | orchestration                       | natively                          |+----------------------+-------------------------------------+-----------------------------------+| Community Support    | Larger community and ecosystem      | Growing community, backed by Red  ||                      |                                     | Hat                               |+----------------------+-------------------------------------+-----------------------------------+You need a mature, widely supported tool with a large ecosystem.You’re already using Docker Swarm for orchestration.You’re okay with running containers as root.You want a daemonless, more secure alternative to Docker.You’re working in environments where root privileges are restricted.You’re already using Kubernetes and want tighter integration.Both tools are excellent choices, so pick the one that best fits your needs.Kubernetes Overview: Orchestration, Scaling, and Self-HealingWhile Docker is great for running containers, managing them at scale can get tricky. Enter  (or K8s for short), the de facto standard for container orchestration. Think of Kubernetes as the conductor of an orchestra — it ensures all your containers play in harmony.Key Features of Kubernetes: Automates deployment, scaling, and management of containers.: Automatically adjusts the number of running containers based on demand.: Restarts failed containers and replaces unhealthy ones.: Automatically assigns IP addresses and DNS names to containers.Kubernetes organizes containers into , which are the smallest deployable units. A pod can contain one or more containers that share resources like storage and networking. Here’s a simple Kubernetes deployment file:apiVersion: apps/v1kind: Deployment  name: my-python-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-python-app        image: my-python-app:latest        ports:his file tells Kubernetes to run three replicas of your app and expose it on port 8080. You can apply it using:🔥 Run this: kubectl apply -f deployment.yamlHow NATS Shines in a Kubernetes EnvironmentNow, let’s talk about . As a lightweight, high-performance messaging system, NATS plays well with Kubernetes. Here’s how it stands out:Use Cases for NATS in KubernetesService-to-Service Communication: NATS excels at enabling fast, reliable communication between microservices. Its lightweight design makes it perfect for Kubernetes’ dynamic environment.Event-Driven Architectures: NATS’s pub/sub, request-reply or streams patterns make it ideal for event-driven systems, where services need to react to events in real time.Scalability: NATS can handle millions of messages per second, making it a great fit for high-throughput applications running on Kubernetes.Resilience: NATS’s built-in fault tolerance ensures that your messaging system remains reliable, even in the face of node failures.Deploying NATS on KubernetesDeploying NATS on Kubernetes is straightforward. Here’s a basic NATS deployment file:apiVersion: apps/v1kind: Deployment  name: nats  replicas: 1    matchLabels:  template:      labels:    spec:      - name: nats        ports:        - containerPort: 4222 # The messaging highway 🛣️Once deployed, NATS can be used by your microservices for seamless communication.Best Practices for Containerizing MicroservicesTo wrap things up, here are some best practices for containerizing your microservices:Keep Containers Lightweight: Use minimal base images (e.g.,  or  versions) to reduce size and improve performance.: Separate the build and runtime environments to keep production images small.Leverage Kubernetes Features: Use ConfigMaps and Secrets to manage configuration and sensitive data.: Integrate tools like Prometheus and Fluentd for monitoring and logging.: Use CI/CD pipelines to automate building, testing, and deploying containers.In the next article, we’ll explore “NATS as a Service Mesh — The Lightweight Superhero Your Microservices Deserve” and how it simplifies communication between microservices. Spoiler alert: it’s like giving your microservices a supercharged walkie-talkie. Stay tuned!Until then, feel free to drop a comment or share your thoughts. What’s your experience with Docker and Kubernetes? Any tips or tricks you’d like to share? Let’s keep the conversation going. 💬 What’s your #1 Kubernetes struggle? Scaling? Debugging? Share below! ❤️ Happy containerizing, and stay tuned for the next chapter in the !]]></content:encoded></item><item><title>Google is on the Wrong Side of History</title><link>https://www.eff.org/deeplinks/2025/02/google-wrong-side-history</link><author>Matthew Guariglia</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/ai-soldiers-3b.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 16:08:57 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Hector Martin Resigns From The Asahi Linux Project</title><link>https://www.phoronix.com/news/Hector-Martin-Resigns-Asahi</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 15:56:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Last week Hector Martin resigned from upstream maintainership of the Apple Silicon code for the Linux kernel. At the time he was still going to contribute to the Asahi Linux project's downstream kernel but in a surprise move today, he has decided to resign as project leader of Asahi Linux...]]></content:encoded></item><item><title>Linux 6.14-rc3 To Fix Platform Profile Support For Newer AMD-Powered ThinkPads</title><link>https://www.phoronix.com/news/Linux-6.14-rc3-Newer-AMD-PP</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 15:41:24 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Submitted today via the x86 platform driver updates ahead of Linux 6.14-rc3 on Sunday are some Lenovo ThinkPad patches that may interest some users...]]></content:encoded></item><item><title>Python vs R for data science: Which should you choose?</title><link>https://www.datasciencecentral.com/python-vs-r-for-data-science-which-should-you-choose/</link><author>Mike Steven</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 15:37:52 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Welcome to another comparison article where you will understand the features, intricacies, pros, and cons of two different stacks of the information technology industry. Today’s comparison blog is especially for data scientists who spend their day and night with datasets, insights, trends, and analysis of many other factors. From a long list of skills that… Read More »]]></content:encoded></item><item><title>A ‘True Crime’ Documentary Series Has Millions of Views. The Murders Are All AI-Generated</title><link>https://www.404media.co/a-true-crime-documentary-series-has-millions-of-views-the-murders-are-all-ai-generated/</link><author>Henry Larson</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/CleanShot-2025-02-12-at-10.36.04@2x.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 15:09:44 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[Elizabeth Hernandez found out about the decade-old  from a flurry of tips sent to her newsroom in August last year.The tips were all reacting to a YouTube video with a shocking title: "Husband's Secret Gay Love Affair with Step Son Ends in Grisly Murder.” It described a gruesome crime that apparently took place in Littleton, Colorado. Almost two million people had watched it.“Some people in fact were saying, ‘Why didn't The Denver Post cover this?’” Hernandez, a reporter at the paper, told me. “Because in the video, it makes it sound like it was a big news event and yet, when you Google it, there is no coverage.”The reason for the lack of coverage was pretty clear to her. In the 26-minute long video, a stilted voice narrated over hazy still images of a neighborhood that really didn’t look like Littleton.Hernandez called several law enforcement officials and quickly confirmed her suspicions. The murder was fake, and the video was made using generative AI.]]></content:encoded></item><item><title>A new tool for visualizing Rust lifetimes</title><link>https://www.youtube.com/watch?v=NV6Xo_el_2o</link><author>Let&apos;s Get Rusty</author><category>dev</category><category>rust</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/NV6Xo_el_2o?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 15:00:34 +0000</pubDate><source url="https://www.youtube.com/channel/UCSp-OaMpsO8K0KkOqyBl7_w">Let&apos;s get Rusty</source><content:encoded><![CDATA[See how RustOwl can help you understand lifetimes in a real Rust codebase. A brand-new tool designed to visualize Rust lifetimes and make learning Rust easier. Check it out and see how it can change the way you write Rust!

Free Rust training: https://letsgetrusty.com/bootcamp

RustOwl: https://github.com/cordx56/rustowl

Corrections:
- Bacon is a CLI tool, not a library. Check it out here: https://github.com/Canop/bacon]]></content:encoded></item><item><title>Bridging the Gap: Democratizing AI for All</title><link>https://www.kdnuggets.com/bridging-gap-democratizing-ai</link><author>Vidhi Chugh</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/chugh_Bridging-the-Gap-Democratizing-AI-for-All_1.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 15:00:03 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Let’s explore how democratizing AI can level the playing field and create opportunities for all, no matter the background or resources.]]></content:encoded></item><item><title>Google Releases AOM-AV1 3.12 With More Performance Optimizations</title><link>https://www.phoronix.com/news/AOM-AV1-3.12</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 14:51:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those preferring the AOM-AV1 open-source AV1 video encoder over SVT-AV1, Rav1e, or other AV1 encoders, Google this week unveiled AOM-AV1 3.12...]]></content:encoded></item><item><title>Rust vs C++ Performance</title><link>https://www.youtube.com/watch?v=WnMin9cf78g</link><author>Anton Putra</author><category>dev</category><category>video</category><enclosure url="https://www.youtube.com/v/WnMin9cf78g?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 14:30:41 +0000</pubDate><source url="https://www.youtube.com/channel/UCeLvlbC754U6FyFQbKc0UnQ">Anton Putra</source><content:encoded><![CDATA[C++ vs Rust Speed.

🔴 To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)

🍿 Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&si=p-UOaVM_6_SFx52H

👋 AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra

▬▬▬▬▬ Experience & Location 💼 ▬▬▬▬▬
►  I’m a Senior Software Engineer at Juniper Networks (13+ years of experience)
►  Located in San Francisco Bay Area, CA (US citizen)

▬▬▬▬▬▬ Connect with me 👋 ▬▬▬▬▬▬
►  LinkedIn: https://www.linkedin.com/in/anton-putra
►  Twitter/X: https://twitter.com/antonvputra
►  GitHub: https://github.com/antonputra
►  Email: me@antonputra.com

▬▬▬▬▬▬▬ Source Code 📚 ▬▬▬▬▬▬▬
► Original Source Code: https://github.com/antonputra/tutorials/tree/245/lessons/245

PR to improve Rust - https://github.com/antonputra/tutorials/pull/429
PR to improve Rust - https://github.com/antonputra/tutorials/pull/431
PR to improve Rust - https://github.com/antonputra/tutorials/pull/433

#rust #golang #devops]]></content:encoded></item><item><title>Are You Ready to Let an AI Agent Use Your Computer?</title><link>https://spectrum.ieee.org/ai-agents-computer-use</link><author>Eliza Strickland</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjM3NTc5Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3MzQzMDIzNX0.q8-_ndmeOkmbC-IEhxZy5uOZIw7195YebXH8q88gz48/image.jpg?width=600" length="" type=""/><pubDate>Thu, 13 Feb 2025 14:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[AI agents from OpenAI, Anthropic, and Google want to lighten your load]]></content:encoded></item><item><title>Wayland Color Management &amp; HDR Protocol Support Merged</title><link>https://www.phoronix.com/news/Wayland-CM-HDR-Merged</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 13:25:18 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As a quick follow-up to the article earlier today... The Wayland Color Management and HDR protocol support is now merged to upstream Wayland Protocols!..]]></content:encoded></item><item><title>Scroll Driven Animations Notebook</title><link>https://css-tricks.com/scroll-driven-animations-notebook/</link><author>Geoff Graham</author><category>frontend</category><pubDate>Thu, 13 Feb 2025 13:24:29 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[One of his latest is a notebook of scroll-driven animations. They’re all impressive as heck, as you’d expect from Adam. But it’s the simplicity of the first few examples that I love most. Here I am recreating two of the effects in a CodePen, which you’ll want to view in the latest version of Chrome for support.This is a perfect example of how a scroll-driven animation is simply a normal CSS animation, just tied to scrolling instead of the document’s default timeline, which starts on render. We’re talking about the same set of keyframes:@keyframes slide-in-from-left {
  from {
    transform: translateX(-100%);
  }
}All we have to do to trigger scrolling is call the animation and assign it to the timeline:li {
  animation: var(--animation) linear both;
  animation-timeline: view();
}Notice how there’s no duration set on the animation. There’s no need to since we’re dealing with a scroll-based timeline instead of the document’s timeline. We’re using the  function instead of the  function, which acts sort of like JavsScript’s Intersection Observer where scrolling is based on where the element comes into view and intersects the scrollable area.It’s easy to drop your jaw and ooo and ahh all over Adam’s demos, especially as they get more advanced. But just remember that we’re still working with plain ol’ CSS animations. The difference is the timeline they’re on.]]></content:encoded></item><item><title>How to Scale Sklearn with Dask</title><link>https://www.kdnuggets.com/how-to-scale-sklearn-dask</link><author>Iván Palomares Carrascosa</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/crVYYGyvTE2Jilsmvukhpw.jpeg" length="" type=""/><pubDate>Thu, 13 Feb 2025 13:00:49 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Here's how Dask applies the building blocks of sklearn to bring ML modeling workflows to the next level of scalability via high-performance parallel computing]]></content:encoded></item><item><title>Better Value Sooner Safer Happier • Simon Rohrer &amp; Eduardo da Silva • GOTO 2025</title><link>https://www.youtube.com/watch?v=8oqT5tjqf-0</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/8oqT5tjqf-0?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 13:00:11 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This interview was recorded for the GOTO Book Club. #GOTOcon #GOTObookclub
http://gotopia.tech/bookclub

Read the full transcription of the interview here:
https://gotopia.tech/episodes/356

Simon Rohrer - Co-Author of "Better Value Sooner Safer Happier" & Senior Director at Saxo Bank @SimonRohrer 
Eduardo da Sliva - Independent Consultant on Organization, Architecture, and Leadership Modernization @emgsilva 

RESOURCES
Simon
https://bsky.app/profile/simon.bvssh.com
https://mastodon.social/@simonr
https://x.com/sirohrer
https://www.linkedin.com/in/simonrohrer
https://github.com/sirohrer
https://www.soonersaferhappier.com

Eduardo
https://bsky.app/profile/esilva.net
https://mastodon.social/@eduardodasilva
https://x.com/emgsilva
https://www.linkedin.com/in/emgsilva
https://github.com/emgsilva
https://esilva.net

Links
https://youtu.be/oOYddleBsuk
https://youtu.be/ESOaDiv3lXA
https://youtu.be/B3b4tremI5o
https://youtu.be/WPCrGYjrJ1Y
https://youtu.be/48g7CRlMUD8
https://youtu.be/86-Dy5U2p5Y
https://youtu.be/l1tyfb5we7I
https://youtu.be/B7JqZh9A2pQ
https://youtu.be/yuEbZYKgZas
https://youtu.be/hBxV3Cr0lXY
https://youtu.be/HKS5sKsIkkc

DESCRIPTION
Eduardo da Silva and Simon Rohrer discuss the core ideas of "Better Value Sooner Safer Happier" diving into the principles of organizational transformation.

Simon shares insights on the shift from output-driven to outcome-focused thinking, emphasizing value over productivity, and the need for continuous improvement in delivery speed, stakeholder satisfaction, and safety.
The conversation explores key concepts like technical excellence, integrating safety into development, and balancing incremental changes with occasional larger steps.

Simon Rohrer discusses organizational patterns and the importance of decentralizing decision-making, recommending a flexible, context-driven approach to transformation. The session concludes with practical advice on how to start implementing these ideas, using the book’s map to guide organizations toward the right transformation strategy based on their specific goals.

TIMECODES
00:00 Intro
01:55 The journey to "Better Value Sooner Safer Happier"
06:14 Antipatterns & patterns for business agility
14:09 Embracing continuous attention to technical excellence
24:43 Embedding safety in continuous delivery
33:31 Balancing incremental change & big leaps in transformation
40:18 Exploring organizational patterns
44:12 Outro

RECOMMENDED BOOKS
Jonathan Smart, Zsolt Berend, Myles Ogilvie & Simon Rohrer • Sooner Safer Happier • https://amzn.to/3Emm9p2
Stephen Fishman & Matt McLarty • Unbundling the Enterprise • https://amzn.to/3W2EcXw
Carliss Y. Baldwin • Design Rules, Vol. 2 • https://amzn.to/4hdllRr
Matthew Skelton & Manuel Pais • Team Topologies • http://amzn.to/3sVLyLQ
Forsgren, Humble & Kim • Accelerate: The Science of Lean Software and DevOps • https://amzn.to/3tCz1xO
Kim, Humble, Debois, Willis & Forsgren • The DevOps Handbook • https://amzn.to/47oAf3l

https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#BetterValue #SoonerSaferHappier #OrganizationalTransformation #BusinessAgility #Agility #ValueOverProductivity #ContinuousDelivery #ContinuousAttention #Safety #TechnicalExcellence #BookClub

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Brain-inspired Computing Is Ready for the Big Time</title><link>https://spectrum.ieee.org/neuromorphic-computing-2671121824</link><author>Edd Gent</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjM3Njk2NC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1MTg1NjUyMH0.WiISOGfkZPanzeo80jgQnL_WWY8h_Ot84nPBMWFvyuY/image.jpg?width=600" length="" type=""/><pubDate>Thu, 13 Feb 2025 13:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Neuromorphic pioneer Steve Furber says it's just awaiting a killer app]]></content:encoded></item><item><title>Is It Time For a Change In GNOME Leadership?</title><link>https://tech.slashdot.org/story/25/02/13/0150244/is-it-time-for-a-change-in-gnome-leadership?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>slashdot</category><category>linux</category><pubDate>Thu, 13 Feb 2025 13:00:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Longtime Slashdot reader BrendaEM writes: Command-line aside, Cinnamon is the most effective keeper of the Linux desktop flame -- by not abandoning desktop and laptop computers. Yes, there are other desktop GUIs, such as MATE, and the lightweight Xfce, which are valuable options when low overhead is important, such as in LinuxCNC. However, among the general public lies a great expanse of office workers who need a full-featured Linux desktop.
 
The programmers who work on GNOME and its family of supporting applications enrich many other desktops do their more than their share. These faithful developers deserve better user-interface leadership. GNOME has tried to steer itself into tablet waters, which is admirable, but GNOME 3.x diminished the desktop experience for both laptop and desktop users. For instance, the moment you design what should be a graphical user interface with words such as "Activities," you ask people to change horses midstream. That is not to say that the command line and GUI cannot coexist -- because they can, as they do in many CAD programs.
 
I remember a time when GNOME ruled the Linux desktop -- and I can remember when GNOME left those users behind. Perhaps in a future, GNOME could return to the Linux desktop and join forces with Cinnamon -- so that we may once again have the year of the Linux desktop.]]></content:encoded></item><item><title>Fostering open source culture (Interview)</title><link>https://changelog.com/podcast/628</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/podcast/628/the-changelog-628.mp3" length="" type=""/><pubDate>Thu, 13 Feb 2025 13:00:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Arun Gupta is back, this time with his latest book in hand titled “Fostering Open Source Culture” to share his wisdom and experiences of fostering open source culture. BTW you can use the code  to get 20% off (both print and e-book). Use this link and enjoy.Changelog++ members save 8 minutes on this episode because they made the ads disappear. Join today!Temporal – Build invincible applications. Manage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail. Register for Replay in London, March 3-5 to break free from the status quo.
Augment Code – Developer AI that uses deep understanding of your large codebase and how you build software to deliver personalized code suggestions and insights. Augment provides relevant, contextualized code right in your IDE or Slack. It transforms scattered knowledge into code or answers, eliminating time spent searching docs or interrupting teammates.
Notion – Notion is a place where any team can write, plan, organize, and rediscover the joy of play. It’s a workspace designed not just for making progress, but getting inspired. Notion is for everyone — whether you’re a Fortune 500 company or freelance designer, starting a new startup or a student juggling classes and clubs.
Fly.io – The home of Changelog.com — Deploy your apps close to your users — global Anycast load-balancing, zero-configuration private networking, hardware isolation, and instant WireGuard VPN connections. Push-button deployments that scale to thousands of instances. Check out the speedrun to get started in minutes.
]]></content:encoded></item><item><title>Peter Bengtsson: get in JavaScript is the same as property in Python</title><link>http://www.peterbe.com/plog/get-in-javascript-is-the-same-as-property-in-python</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 12:41:56 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Prefix a function, in an object or class, with `get` and then that acts as a function call without brackets. Just like Python's `property` decorator.]]></content:encoded></item><item><title>AI Coding Assistants are Not the Solution You Think</title><link>https://devops.com/ai-coding-assistants-are-not-the-solution-you-think/</link><author>Anish Dhar</author><category>devops</category><pubDate>Thu, 13 Feb 2025 12:31:35 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to add a directory to your PATH</title><link>https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/</link><author>Julia Evans</author><category>blog</category><pubDate>Thu, 13 Feb 2025 12:27:56 +0000</pubDate><source url="https://jvns.ca/atom.xml">Julia Evans</source><content:encoded><![CDATA[I was talking to a friend about how to add a directory to your PATH today. It’s
something that feels “obvious” to me since I’ve been using the terminal for a
long time, but when I searched for instructions for how to do it, I actually
couldn’t find something that explained all of the steps – a lot of them just
said “add this to ”, but what if you’re not using bash? What if your
bash config is actually in a different file? And how are you supposed to figure
out which directory to add anyway?So I wanted to try to write down some more complete directions and mention some
of the gotchas I’ve run into over the years.Here’s a table of contents:step 1: what shell are you using?If you’re not sure what shell you’re using, here’s a way to find out. Run this:if you’re using , it’ll print out if you’re using , it’ll print out if you’re using , it’ll print out an error like “In fish, please use
$fish_pid” ( isn’t valid syntax in fish, but in any case the error
message tells you that you’re using fish, which you probably already knew)Also bash is the default on Linux and zsh is the default on Mac OS (as of
2024). I’ll only cover bash, zsh, and fish in these directions.step 2: find your shell’s config filein zsh, it’s probably in bash, it might be , but it’s complicated, see the note in the next sectionin fish, it’s probably ~/.config/fish/config.fish (you can run  if you want to be 100% sure)a note on bash’s config fileBash has three possible config files: , , and .If you’re not sure which one your system is set up to use, I’d recommend
testing this way:add  to your If you see “hi there”, that means  is being used! Hooray!Otherwise remove it and try the same thing with You can also try  if the first two options don’t work.(there are a lot of elaborate flow charts out there that explain how bash
decides which config file to use but IMO it’s not worth it and just testing is
the fastest way to be sure)step 3: figure out which directory to addLet’s say that you’re trying to install and run a program called 
and it doesn’t work, like this:$ npm install -g http-server
$ http-server
bash: http-server: command not found
How do you find what directory  is in? Honestly in general this is
not that easy – often the answer is something like “it depends on how npm is
configured”. A few ideas:Often when setting up a new installer (like , , , etc),
when you first set it up it’ll print out some directions about how to update
your PATH. So if you’re paying attention you can get the directions then.Sometimes installers will automatically update your shell’s config file
to update your  for youSometimes just Googling “where does npm install things?” will turn up the
answerSome tools have a subcommand that tells you where they’re configured to
install things, like:
Node/npm:  (then append )Go:  (then append )asdf: asdf info | grep ASDF_DIR (then append  and )step 3.1: double check it’s the right directoryOnce you’ve found a directory you think might be the right one, make sure it’s
actually correct! For example, I found out that on my machine,  is
in . I can make sure that it’s the right directory by trying to
run the program  in that directory like this:$ ~/.npm-global/bin/http-server
Starting up http-server, serving ./public
It worked! Now that you know what directory you need to add to your ,
let’s move to the next step!step 4: edit your shell configNow we have the 2 critical pieces of information we need:Which directory you’re trying to add to your PATH (like  )Where your shell’s config is (like , , or ~/.config/fish/config.fish)Now what you need to add depends on your shell:Open your shell’s config file, and add a line like this:export PATH=$PATH:~/.npm-global/bin/
(obviously replace  with the actual directory you’re trying to add)You can do the same thing as in bash, but zsh also has some slightly fancier
syntax you can use if you prefer:path=(
  $path
  ~/.npm-global/bin
)
In fish, the syntax is different:set PATH $PATH ~/.npm-global/bin
(in fish you can also use , some notes on that further down)step 5: restart your shellNow, an extremely important step: updating your shell’s config won’t take
effect if you don’t restart it!open a new terminal (or terminal tab), and maybe close the old one so you don’t get confusedRun  to start a new shell (or  if you’re using zsh, or  if you’re using fish)I’ve found that both of these usually work fine.And you should be done! Try running the program you were trying to run and
hopefully it works now.If not, here are a couple of problems that you might run into:problem 1: it ran the wrong programIf the wrong  of a is program running, you might need to add the
directory to the  of your PATH instead of the end.For example, on my system I have two versions of  installed, which I
can see by running :$ which -a python3
/usr/bin/python3
/opt/homebrew/bin/python3
The one your shell will use is the .If you want to use the Homebrew version, you need to add that directory
() to the  of your PATH instead, by putting this in
your shell’s config file (it’s  instead of the usual )export PATH=/opt/homebrew/bin/:$PATH
set PATH ~/.cargo/bin $PATH
problem 2: the program isn’t being run from your shellAll of these directions only work if you’re running the program . If you’re running the program from an IDE, from a GUI, in a cron job,
or some other way, you’ll need to add the directory to your PATH in a different
way, and the exact details might depend on the situation.use the full path to the program you’re running, like /home/bork/bin/my-programput the full PATH you want as the first line of your crontab (something like
PATH=/bin:/usr/bin:/usr/local/bin:….). You can get the full PATH you’re
using in your shell by running .I’m honestly not sure how to handle it in an IDE/GUI because I haven’t run into
that in a long time, will add directions here if someone points me in the right
direction.problem 3: duplicate  entries making it harder to debugIf you edit your path and start a new shell by running  (or , or
), you’ll often end up with duplicate  entries, because the shell
keeps adding new things to your  every time you start your shell.Personally I don’t think I’ve run into a situation where this kind of
duplication breaks anything, but the duplicates can make it harder to debug
what’s going on with your  if you’re trying to understand its contents.Some ways you could deal with this:If you’re debugging your , open a new terminal to do it in so you get
a “fresh” state. This should avoid the duplication.Deduplicate your  at the end of your shell’s config  (for example in
zsh apparently you can do this with )Check that the directory isn’t already in your  when adding it (for
example in fish I believe you can do this with fish_add_path --path /some/directory)How to deduplicate your  is shell-specific and there isn’t always a
built in way to do it so you’ll need to look up how to accomplish it in your
shell.problem 4: losing your history after updating your Here’s a situation that’s easy to get into in bash or zsh:Run  to reload your configPress the up arrow a couple of times to rerun the failed command (or open a new terminal)The failed command isn’t in your history! Why not?This happens because in bash, by default, history is not saved until you exit
the shell.Some options for fixing this:Instead of running  to reload your config, run  (or
 in zsh). This will reload the config inside your current
session.Configure your shell to continuously save your history instead of only saving
the history when the shell exits. (How to do this depends on whether you’re
using bash or zsh, the history options in zsh are a bit complicated and I’m
not exactly sure what the best way is)When you install  (Rust’s installer) for the first time, it gives you
these instructions for how to set up your PATH, which don’t mention a specific
directory at all.This is usually done by running one of the following (note the leading DOT):

. "$HOME/.cargo/env"        	# For sh/bash/zsh/ash/dash/pdksh
source "$HOME/.cargo/env.fish"  # For fish
The idea is that you add that line to your shell’s config, and their script
automatically sets up your  (and potentially other things) for you.This is pretty common (for example Homebrew suggests you eval ), and there are
two ways to approach this:Just do what the tool suggests (like adding  to your shell’s config)Figure out which directories the script they’re telling you to run would add
to your PATH, and then add those manually. Here’s how I’d do that:
Run  in my shell (or the fish version if using fish)Run echo "$PATH" | tr ':' '\n' | grep cargo to figure out which directories it addedSee that it says  and shorten that to Add the directory  to PATH (with the directions in this post)I don’t think there’s anything wrong with doing what the tool suggests (it
might be the “best way”!), but personally I usually use the second approach
because I prefer knowing exactly what configuration I’m changing.fish has a handy function called  that you can run to add a directory to your  like this:fish_add_path /some/directory
This is cool (it’s such a simple command!) but I’ve stopped using it for a couple of reasons:Sometimes  will update the  for every session in the
future (with a “universal variable”) and sometimes it will update the 
just for the current session and it’s hard for me to tell which one it will
do. In theory the docs explain this but I could not understand them.Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if
you there are other major gotchas that have tripped you up when adding a
directory to your PATH, or if you have questions about this post!]]></content:encoded></item><item><title>DOGE as a National Cyberattack</title><link>https://www.schneier.com/blog/archives/2025/02/doge-as-a-national.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Thu, 13 Feb 2025 12:03:26 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[In the span of just weeks, the US government has experienced what may be the most consequential security breach in its history—not through a sophisticated cyberattack or an act of foreign espionage, but through official orders by a billionaire with a poorly defined government role. And the implications for national security are profound.First, it was reported that people associated with the newly created Department of Government Efficiency (DOGE) had accessedtheUSTreasury computer system, giving them the ability to collect data on and potentially control the department’s roughly $5.45 trillion in annual federal payments.Then, we learned that uncleared DOGE personnel had gained access to classified data from the US Agency for International Development, possibly copying it onto their own systems. Next, the Office of Personnel Management—which holds detailed personal data on millions of federal employees, including those with security clearances—wascompromised. After that, Medicaid and Medicare records were compromised.Meanwhile, only partially redacted names of CIA employees were sent over an unclassified email account. DOGE personnel are also reported to be feeding Education Department data into artificial intelligence software, and they have also started working at the Department of Energy.This story is moving very fast. On Feb. 8, a federal judge blocked the DOGE team from accessing the Treasury Department systems any further. But given that DOGE workers have already copied data and possibly installed and modified software, it’s unclear how this fixes anything.In any case, breaches of other critical government systems are likely to follow unless federal employees stand firm on the protocols protecting national security.The systems that DOGE is accessing are not esoteric pieces of our nation’s infrastructure—they are the sinews of government.For example, the Treasury Department systems contain the technical blueprints for how the federal government moves money, while the Office of Personnel Management (OPM) network contains information on who and what organizations the government employs and contracts with.What makes this situation unprecedented isn’t just the scope, but also the method of attack. Foreign adversaries typically spend years attempting to penetrate government systems such as these, using stealth to avoid being seen and carefully hiding any tells or tracks. The Chinese government’s 2015 breach of OPM was a significant US security failure, and it illustrated how personnel data could be used to identify intelligence officers and compromise national security.In this case, external operators with limited experience and minimal oversight are doing their work in plain sight and under massive public scrutiny: gaining the highest levels of administrative access and making changes to the United States’ most sensitive networks, potentially introducing new security vulnerabilities in the process.But the most alarming aspect isn’t just the access being granted. It’s the systematic dismantling of security measures that would detect and prevent misuse—including standard incident response protocols, auditing, and change-tracking mechanisms—by removing the career officials in charge of those security measures and replacing them with inexperienced operators.The Treasury’s computer systems have such an impact on national security that they were designed with the same principle that guides nuclear launch protocols: No single person should have unlimited power. Just as launching a nuclear missile requires two separate officers turning their keys simultaneously, making changes to critical financial systems traditionally requires multiple authorized personnel working in concert.This approach, known as “separation of duties,” isn’t just bureaucratic red tape; it’s a fundamental security principle as old as banking itself. When your local bank processes a large transfer, it requires two different employees to verify the transaction. When a company issues a major financial report, separate teams must review and approve it. These aren’t just formalities—they’re essential safeguards against corruption and error. These measures have been bypassed or ignored. It’s as if someone found a way to rob Fort Knox by simply declaring that the new official policy is to fire all the guards and allow unescorted visits to the vault.The implications for national security are staggering. Sen. Ron Wyden said his office had learned that the attackers gained privileges that allow them to modify core programs in Treasury Department computers that verify federal payments, access encrypted keys that secure financial transactions, and alter audit logs that record system changes. Over at OPM, reports indicate that individuals associated with DOGE connected an unauthorized server into the network. They are also reportedly trainingAI software on all of this sensitive data.This is much more critical than the initial unauthorized access. These new servers have unknown capabilities and configurations, and there’s no evidence that this new code has gone through any rigorous security testing protocols. The AIs being trained are certainly not secure enough for this kind of data. All are ideal targets for any adversary, foreign or domestic, also seeking access to federal data.There’s a reason why every modification—hardware or software—to these systems goes through a complex planning process and includes sophisticated access-control mechanisms. The national security crisis is that these systems are now much more vulnerable to dangerous attacks at the same time that the legitimate system administrators trained to protect them have been locked out.By modifying core systems, the attackers have not only compromised current operations, but have also left behind vulnerabilities that could be exploited in future attacks—giving adversaries such as Russia and China an unprecedentedopportunity. These countries have long targeted these systems. And they don’t just want to gather intelligence—they also want to understand how to disrupt these systems in a crisis.Now, the technical details of how these systems operate, their security protocols, and their vulnerabilities are now potentially exposed to unknown parties without any of the usual safeguards. Instead of having to breach heavily fortified digital walls, these parties  can simply walk through doors that are being propped open—and then erase evidence of their actions.The security implications span three critical areas.First, system manipulation: External operators can now modify operations while also altering audit trails that would track their changes. Second, data exposure: Beyond accessing personal information and transaction records, these operators can copy entire system architectures and security configurations—in one case, the technical blueprint of the country’s federal payment infrastructure. Third, and most critically, is the issue of system control: These operators can alter core systems and authentication mechanisms while disabling the very tools designed to detect such changes. This is more than modifying operations; it is modifying the infrastructure that those operations use.To address these vulnerabilities, three immediate steps are essential. First, unauthorized access must be revoked and proper authentication protocols restored. Next, comprehensive system monitoring and change management must be reinstated—which, given the difficulty of cleaning a compromised system, will likely require a complete system reset. Finally, thorough audits must be conducted of all system changes made during this period.This is beyond politics—this is a matter of national security. Foreign national intelligence organizations will be quick to take advantage of both the chaos and the new insecurities to steal US data and install backdoors to allow for future access.Each day of continued unrestricted access makes the eventual recovery more difficult and increases the risk of irreversible damage to these critical systems. While the full impact may take time to assess, these steps represent the minimum necessary actions to begin restoring system integrity and security protocols.Assuming that anyone in the government still cares.This essay was written with Davi Ottenheimer, and originally appeared in Foreign Policy.]]></content:encoded></item><item><title>Device Trees For Apple T2 SoCs Slated For Upstreaming In Linux 6.15</title><link>https://www.phoronix.com/news/Linux-6.15-Apple-T2-SoC-DTs</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 11:58:31 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While there has been the recent drama over upstream maintainership over Apple Silicon / Asahi Linux code, Sven Peter is continuing to move things forward for the upstream kernel and this week sent out a set of Apple SoC DeviceTree updates intended for the upcoming Linux 6.15 kernel cycle...]]></content:encoded></item><item><title>NVIDIA Wiring Up Autonomous Performance Level Selection To Linux CPPC CPUFreq Driver</title><link>https://www.phoronix.com/news/NVIDIA-Auto-Perf-CPPC-CPUFreq</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 11:40:09 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Similar to the Autonomous Performance Level Selection and Energy Performance Preference (EPP) support already found within the Intel P-State and AMD P-State CPU frequency scaling drivers for their modern processors, NVIDIA engineers are working on similar support for the CPPC CPUFreq driver that can benefit their Grace processor...]]></content:encoded></item><item><title>Wayland Color Management &amp; HDR Protocols Expected To Be Merged Imminently</title><link>https://www.phoronix.com/news/Wayland-Color-HDR-Merging-Soon</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 11:30:10 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Today could finally be the day. In the works for 5+ years, the Wayland color management and HDR protocol additions look like they will finally be merged in the coming hours...]]></content:encoded></item><item><title>Kubernetes History Inspector, with Kakeru Ishii</title><link>http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii</link><author>gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)</author><category>podcast</category><category>k8s</category><category>devops</category><enclosure url="https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674" length="" type=""/><pubDate>Thu, 13 Feb 2025 11:23:00 +0000</pubDate><source url="https://kubernetespodcast.com/">Kubernetes Podcast</source><content:encoded><![CDATA[Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.Do you have something cool to share? Some questions? Let us know: News of the week ]]></content:encoded></item><item><title>Financially motivated hackers are helping their espionage counterparts and vice versa</title><link>https://arstechnica.com/security/2025/02/financially-motivated-hackers-are-helping-their-espionage-counterparts-and-vice-versa/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2018/10/GettyImages-981636794-1152x648.jpg" length="" type=""/><pubDate>Thu, 13 Feb 2025 11:00:40 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[There’s a growing collaboration between hacking groups engaging in espionage on behalf of nation-states and those seeking financial gains through ransomware and other forms of cybercrime, researchers noted this week.There has always been some level of overlap between these two groups, but it has become more pronounced in recent years. On Tuesday, the Google-owned Mandiant security firm said the uptick comes amid tighter purse strings and as a means for concealing nation-state-sponsored espionage by making it blend in with financially motivated cyberattacks.“Modern cybercriminals are likely to specialize in a particular area of cybercrime and partner with other entities with diverse specializations to conduct operations,” Mandiant researchers explained. “The specialization of cybercrime capabilities presents an opportunity for state-backed groups to simply show up as another customer for a group that normally sells to other criminals. Purchasing malware, credentials, or other key resources from illicit forums can be cheaper for state-backed groups than developing them in-house, while also providing some ability to blend in to financially motivated operations and attract less notice."]]></content:encoded></item><item><title>GenAI Patterns: Reranker</title><link>https://martinfowler.com/articles/gen-ai-patterns/#reranker</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Thu, 13 Feb 2025 10:16:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[LLMs struggle with large amounts of context. Bharani
      Subramaniam and I explain how to mitigate this common RAG
      problem with a Reranker which takes the document
      fragments from the retriever, and ranks them according to their usefulness.]]></content:encoded></item><item><title>EuroPython: EuroPython February 2025 Newsletter</title><link>https://blog.europython.eu/europython-february-2025-newsletter/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Thu, 13 Feb 2025 08:36:11 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Hope you&aposre all having a fantastic February. We sure have been busy and got some exciting updates for you as we gear up for EuroPython 2025, which is taking place once again in the beautiful city of Prague. So let&aposs dive right in!EuroPython 2025 is right around the corner and our programme team is hard at work putting together an amazing lineup. But we need your help to shape the conference! We received over 572 fantastic proposals, and now it’s time for Community Voting! 🎉 If you&aposve attended EuroPython before or submitted a proposal this year, you’re eligible to vote.📢 More votes = a stronger, more diverse programme! Spread the word and get your EuroPython friends to cast their votes too.🏃The deadline is , so don’t miss your chance!Want to play a key role in building an incredible conference? Join our review team and help select the best talks for EuroPython 2025! Whether you&aposre a Python expert or an enthusiastic community member, your insights matter.We’d like to also thank the over 100 people who have already signed up to review! For those who haven’t done so yet, please remember to accept your Pretalx link and get your reviews in by You can already start reviewing proposals, and each review takes as little as 5 minutes. We encourage reviewers to go through at least 20-30 proposals, but if you can do more, even better! With almost 600 submissions to pick from, your help ensures we curate a diverse and engaging programme.🏃The deadline is Monday next week, so don’t delay!EuroPython isn’t just present at other Python events—we actively support them too! As a community sponsor, we love helping local PyCons grow and thrive. We love giving back to the community and strengthening Python events across Europe! 🐍💙The EuroPython team had a fantastic time at PyCon + Web in Berlin, meeting fellow Pythonistas, exchanging ideas, and spreading the word about EuroPython 2025. It was great to connect with speakers, organizers, and attendees. Ever wondered how long it takes to walk from Berlin to Prague? A huge thank you to our co-organizers, Cheuk, Artur, and Cristián, for answering that in their fantastic lightning talk about EuroPython!We had some members of the EuroPython team at FOSDEM 2025, connecting with the open-source community and spreading the Python love! 🎉 We enjoyed meeting fellow enthusiasts, sharing insights about the EuroPython Society, and giving away the first EuroPython 2025 stickers. If you stopped by—thank you and we hope to see you in Prague this July.🦒 Speaker Mentorship ProgrammeThe signups for The Speaker Mentorship Programme closed on 22nd January 2025. We’re excited to have matched 43 mentees with 24 mentors from our community. We had an increase in the number of mentees who signed up and that’s amazing! We’re glad to be contributing to the journey of new speakers in the Python community. A massive thank you to our mentors for supporting the mentees and to our mentees; we’re proud of you for taking this step in your journey as a speaker. 26 mentees submitted at least 1 proposal. Out of this number, 13 mentees submitted 1 proposal, 9 mentees submitted 2 proposals, 2 mentees submitted 3 proposals, 1 mentee submitted 4 proposals and lastly, 1 mentee submitted 5 proposals. We wish our mentees the best of luck. We look forward to the acceptance of their proposals.In a few weeks, we will host an online panel session with 2–3 experienced community members who will share their advice with first-time speakers. At the end of the panel, there will be a Q&A session to answer all the participants’ questions.You can watch the recording of the previous year’s workshop here:EuroPython is one of the largest Python conferences in Europe, and it wouldn’t be possible without our sponsors. We are so grateful for the companies who have already expressed interest. If you’re interested in sponsoring EuroPython 2025 as well, please reach out to us at sponsoring@europython.eu.🎤 EuroPython Speakers Share Their ExperiencesWe asked our past speakers to share their experiences speaking at EuroPython. These videos have been published on YouTube as shorts, and we&aposve compiled them into brief clips for you to watch.A big thanks goes to Sebastian Witowski, Jan Smitka, Yuliia Barabash, Jodie Burchell, Max Kahan, and Cheuk Ting Ho for sharing their experiences.Why You Should Submit a Proposal for EuroPython? Part 2Why You Should Submit a Proposal for EuroPython? Part 3📊 EuroPython Society Board Report The EuroPython conference wouldn’t be what it is without the incredible volunteers who make it all happen. 💞 Behind the scenes, there’s also the EuroPython Society—a volunteer-led non-profit that manages the fiscal and legal aspects of running the conference, oversees its organization, and works on a few smaller projects like the grants programme. To keep everyone in the loop and promote transparency, the Board is sharing regular updates on what we’re working on.That&aposs all for now! Keep an eye on your inbox and our website for more news and announcements. We&aposre counting down the days until we can come together in Prague to celebrate our shared love for Python. 🐍❤️Cheers,The EuroPython Team]]></content:encoded></item><item><title>Sandbox environments: Creating efficient and isolated testing realms</title><link>https://www.youtube.com/watch?v=fh7-lQVmX-o</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/fh7-lQVmX-o?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 06:00:33 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>KitOps: AI Model Packaging Standards</title><link>https://www.youtube.com/watch?v=1TD-e_wVe4Q</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/1TD-e_wVe4Q?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 06:00:00 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Chat with us on Discord:  https://discord.gg/Tapeh8agYy

Check out our repos:
KitOps      https://github.com/jozu-ai/kitops
PyKitOps Python Library  https://github.com/jozu-ai/pykitops
KitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin]]></content:encoded></item><item><title>OpenSUSE Tumbleweed Switching From AppArmor To SELinux For New Installations</title><link>https://www.phoronix.com/news/OpenSUSE-Tumble-Goes-SELinux</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 01:47:35 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[SUSE/openSUSE has a long history with the AppArmor Linux security module going back to the Novell days and when AppArmor was originally known as SubDomain. OpenSUSE/SUSE and Ubuntu Linux have been big proponents of AppArmor for Linux security but now moving forward on new installations of openSUSE Tumbleweed it will be defaulting to Security Enhanced Linux (SELinux)...]]></content:encoded></item><item><title>ISC Stormcast For Thursday, February 13th, 2025 https://isc.sans.edu/podcastdetail/9322, (Thu, Feb 13th)</title><link>https://isc.sans.edu/diary/rss/31682</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Thu, 13 Feb 2025 01:26:50 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[
 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>DShield SIEM Docker Updates, (Thu, Feb 13th)</title><link>https://isc.sans.edu/diary/rss/31680</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Thu, 13 Feb 2025 01:23:59 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[Over the past several weeks, I have been testing various enhancements to the DShield SIEM, to process DShield sensor log from local and cloud sensors with Filebeat and Filebeat modules to easily send Zeek and NetFlow logs back to a local network ELK stack via home router natting. This is a list of updates and enhancements:- Upgrade to the current version of Elastic 8.17.2
- A single script to configure the base configuration of all the docker files (change_perms.sh)
- Addition of docker filebeat for cloud DShield sensor collection (Cowrie, Zeek & NetFlow logs)
- Second filebeat to ingest ISC & Rosti Threat Intel IP data [3]
- Separation of GitHub DShield SIEM & DShield sensor scripts
- Addition to docker Metricbeat for ELK Stack metric information
- Updated dashboard that includes Zeek in the tab lists
- Query in one dashboard is linked to the others
- Tested the ELK Stack in a LXC Proxmox container [4]
- The addition of ELK Stack monitoring of all the Beats and Logstash
- Configured Logstash to parse logs with Beats pipelines (Zeek & NetFlow)
- Removed and merged multiple steps to simplify the installation (change_perms.sh)
- Updated some sections of the Troubleshooting document [5]
- Updated some sections of the docker useful commands [6]
- Updated the DShield SIEM network flow [7]
- Docker update steps to current version [8][1] https://github.com/bruneaug/DShield-SIEM/tree/main
[2] https://github.com/bruneaug/DShield-Sensor
[3] https://github.com/bruneaug/DShield-SIEM/blob/main/AddOn/ISC_threatintel.md
[4] https://github.com/bruneaug/DShield-SIEM/blob/main/AddOn/LXC_Container_DShield-SIEM.md
[5] https://github.com/bruneaug/DShield-SIEM/blob/main/Troubleshooting/Troubleshooting_SIEM_and_Sensor.md
[6] https://github.com/bruneaug/DShield-SIEM/blob/main/Troubleshooting/docker_useful_commands..md
[7] https://github.com/bruneaug/DShield-SIEM/blob/main/Troubleshooting/DShield-SIEM-Flow.png
[8] https://github.com/bruneaug/DShield-SIEM/blob/main/Troubleshooting/docker_useful_commands..md#update-dshield-elk-to-the-latest-version
[9] https://www.elastic.co/guide/en/elasticsearch/reference/current/release-notes-8.17.2.html

 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>Ubuntu 24.04.2 LTS Delayed To Next Week</title><link>https://www.phoronix.com/news/Ubuntu-24.04.2-LTS-Delayed</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 13 Feb 2025 00:06:58 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Ubuntu 24.04.2 LTS along with new point releases for its derivatives had been scheduled for release on Thursday. But a last minute issue has delayed this release...]]></content:encoded></item><item><title>Extensible Wasm Applications with Go</title><link>https://go.dev/blog/wasmexport</link><author>Cherry Mui</author><category>dev</category><category>official</category><category>go</category><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Golang Blog</source><content:encoded><![CDATA[
      Cherry Mui
      13 February 2025
      Go 1.24 enhances its WebAssembly (Wasm) capabilities with the
addition of the  directive and the ability to build a reactor
for WebAssembly System Interface (WASI).
These features enable Go developers to export Go functions to Wasm,
facilitating better integration with Wasm hosts and expanding the possibilities
for Go-based Wasm applications.WebAssembly and the WebAssembly System InterfaceWebAssembly (Wasm) is a binary instruction format
that was initially created for web browsers, providing the execution of
high-performance, low-level code at speeds approaching native performance.
Since then, Wasm’s utility has expanded, and it is now used in various
environments beyond the browser.
Notably, cloud providers offer services that directly execute Wasm
executables, taking advantage of the
WebAssembly System Interface (WASI) system call API.
WASI allows these executables to interact with system resources.Go first added support for compiling to Wasm in the 1.11 release, through the
 port.
Go 1.21 added a new port targeting the WASI preview 1 syscall API through the
new  port.Exporting Go Functions to Wasm with Go 1.24 introduces a new compiler directive, , which allows
developers to export Go functions to be called from outside of the
Wasm module, typically from a host application that runs the Wasm runtime.
This directive instructs the compiler to make the annotated function available
as a Wasm export
in the resulting Wasm binary.To use the  directive, simply add it to a function definition://go:wasmexport add
func add(a, b int32) int32 { return a + b }
With this, the Wasm module will have an exported function named  that
can be called from the host.This is analogous to the cgo  directive,
which makes the function available to be called from C,
though  uses a different, simpler mechanism.A WASI reactor is a WebAssembly module that operates continuously, and
can be called upon multiple times to react on events or requests.
Unlike a “command” module, which terminates after its main function finishes,
a reactor instance remains live after initialization, and its exports remain
accessible.With Go 1.24, one can build a WASI reactor with the  build
flag.$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm
The build flag signals to the linker not to generate the  function
(the entry point for a command module), and instead generate an
 function, which performs runtime and package initialization,
along with any exported functions and their dependencies.
The  function must be called before any other exported functions.
The  function will not be automatically invoked.To use a WASI reactor, the host application first initializes it by calling
, then simply invoke the exported functions.
Here is an example using Wazero, a Go-based Wasm runtime
implementation:// Create a Wasm runtime, set up WASI.
r := wazero.NewRuntime(ctx)
defer r.Close(ctx)
wasi_snapshot_preview1.MustInstantiate(ctx, r)

// Configure the module to initialize the reactor.
config := wazero.NewModuleConfig().WithStartFunctions("_initialize")

// Instantiate the module.
wasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)

// Call the exported function.
fn := wasmModule.ExportedFunction("add")
var a, b int32 = 1, 2
res, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))
c := api.DecodeI32(res[0])
fmt.Printf("add(%d, %d) = %d\n", a, b, c)

// The instance is still alive. We can call the function again.
res, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))
fmt.Printf("add(%d, %d) = %d\n", b, c, api.DecodeI32(res[0]))
The  directive and the reactor build mode allow applications to
be extended by calling into Go-based Wasm code.
This is particularly valuable for applications that have adopted Wasm as a
plugin or extension mechanism with well-defined interfaces.
By exporting Go functions, applications can leverage the Go Wasm modules to
provide functionality without needing to recompile the entire application.
Furthermore, building as a reactor ensures that the exported functions can be
called multiple times without requiring reinitialization, making it suitable
for long-running applications or services.Supporting rich types between the host and the clientGo 1.24 also relaxes the constraints on types that can be used as input and
result parameters with  functions.
For example, one can pass a bool, a string, a pointer to an , or a
pointer to a struct which embeds  and contains supported
field types
(see the documentation for detail).
This allows Go Wasm applications to be written in a more natural and ergonomic
way, and removes some unnecessary type conversions.While Go 1.24 has made significant enhancements to its Wasm capabilities,
there are still some notable limitations.Wasm is a single-threaded architecture with no parallelism.
A  function can spawn new goroutines.
But if a function creates a background goroutine, it will not continue
executing when the  function returns, until calling back into
the Go-based Wasm module.While some type restrictions have been relaxed in Go 1.24, there are still
limitations on the types that can be used with  and
 functions.
Due to the unfortunate mismatch between the 64-bit architecture of the client
and the 32-bit architecture of the host, it is not possible to pass pointers in
memory.
For example, a  function cannot take a pointer to a struct that
contains a pointer-typed field.The addition of the ability to build a WASI reactor and export Go functions to
Wasm in Go 1.24 represent a significant step forward for Go’s WebAssembly
capabilities.
These features empower developers to create more versatile and powerful Go-based
Wasm applications, opening up new possibilities for Go in the Wasm ecosystem.]]></content:encoded></item><item><title>2024 State of Rust Survey Results</title><link>https://blog.rust-lang.org/2025/02/13/2024-State-Of-Rust-Survey-results.html</link><author>The Rust Survey Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[The Rust Survey Team is excited to share the results of our 2024 survey on the Rust Programming language, conducted between December 5, 2024 and December 23, 2024.
As in previous years, the 2024 State of Rust Survey was focused on gathering insights and feedback from Rust users, and all those who are interested in the future of Rust more generally.This ninth edition of the survey surfaced new insights and learning opportunities straight from the global Rust language community, which we will summarize below. In addition to this blog post,  containing charts with aggregated results of all questions in the survey.Our sincerest thanks to every community member who took the time to express their opinions and experiences with Rust over the past year. Your participation will help us make Rust better for everyone.There's a lot of data to go through, so strap in and enjoy!As shown above, in 2024, we have received fewer survey views than in the previous year. This was likely caused simply by the fact that the survey ran only for two weeks, while in the previous year it ran for almost a month. However, the completion rate has also dropped, which seems to suggest that the survey might be a bit too long. We will take this into consideration for the next edition of the survey.The State of Rust survey not only gives us excellent insight into how many Rust users around the world are using and experiencing the language but also gives us insight into the makeup of our global community. This information gives us a sense of where the language is being used and where access gaps might exist for us to address over time. We hope that this data and our related analysis help further important discussions about how we can continue to prioritize global access and inclusivity in the Rust community.Same as every year, we asked our respondents in which country they live in. The top 10 countries represented were, in order: United States (22%), Germany (14%), United Kingdom (6%), France (6%), China (5%), Canada (3%), Netherlands (3%), Russia (3%), Australia (2%), and Sweden (2%). We are happy to see that Rust is enjoyed by users from all around the world! You can try to find your country in the chart below:We also asked whether respondents consider themselves members of a marginalized community. Out of those who answered, 74.5% selected no, 15.5% selected yes, and 10% preferred not to say.We have asked the group that selected “yes” which specific groups they identified as being a member of. The majority of those who consider themselves a member of an underrepresented or marginalized group in technology identify as lesbian, gay, bisexual, or otherwise non-heterosexual. The second most selected option was neurodivergent at 46% followed by trans at 35%.Each year, we must acknowledge the diversity, equity, and inclusivity (DEI) related gaps in the Rust community and open source as a whole. We believe that excellent work is underway at the Rust Foundation to advance global access to Rust community gatherings and distribute grants to a diverse pool of maintainers each cycle, which you can learn more about here. Even so, global inclusion and access is just one element of DEI, and the survey working group will continue to advocate for progress in this domain.The number of respondents that self-identify as a Rust user was quite similar to last year, around 92%. This high number is not surprising, since we primarily target existing Rust developers with this survey.Similarly as last year, around 31% of those who did not identify as Rust users cited the perception of difficulty as the primary reason for not using Rust. The most common reason for not using Rust was that the respondents simply haven’t had the chance to try it yet.Of the former Rust users who participated in the 2024 survey, 36% cited factors outside their control as a reason why they no longer use Rust, which is a 10pp decrease from last year. This year, we also asked respondents if they would consider using Rust again if an opportunity comes up, which turns out to be true for a large fraction of the respondents (63%). That is good to hear!Closed answers marked with N/A were not present in the previous version(s) of the survey.Those not using Rust anymore told us that it is because they don't really need it (or the goals of their company changed) or because it was not the right tool for the job. A few reported being overwhelmed by the language or its ecosystem in general or that switching to or introducing Rust would have been too expensive in terms of human effort.Of those who used Rust in 2024, 53% did so on a daily (or nearly daily) basis — an increase of 4pp from the previous year. We can observe an upward trend in the frequency of Rust usage over the past few years, which suggests that Rust is being increasingly used at work. This is also confirmed by other answers mentioned in the Rust at Work section later below.Rust expertise is also continually increasing amongst our respondents! 20% of respondents can write (only) simple programs in Rust (a decrease of 3pp from 2023), while 53% consider themselves productive using Rust — up from 47% in 2023. While the survey is just one tool to measure the changes in Rust expertise overall, these numbers are heartening as they represent knowledge growth for many Rustaceans returning to the survey year over year.Unsurprisingly, the most popular version of Rust is , either the most recent one or whichever comes with the users' Linux distribution. Almost a third of users also use the latest nightly release, due to various reasons (see below). However, it seems that the beta toolchain is not used much, which is a bit unfortunate. We would like to encourage Rust users to use the beta toolchain more (e.g. in CI environments) to help test soon-to-be stabilized versions of Rust.People that use the nightly toolchain mostly do it to gain access to specific unstable language features. Several users have also mentioned that rustfmt works better for them on nightly or that they use the nightly compiler because of faster compilation times.To use Rust, programmers first have to learn it, so we are always interested in finding out how do they approach that. Based on the survey results, it seems that most users learn from Rust documentation and also from The Rust Programming Language book, which has been a favourite learning resource of new Rustaceans for a long time. Many people also seem to learn by reading the source code of Rust crates. The fact that both the documentation and source code of tens of thousands of Rust crates is available on docs.rs and GitHub makes this easier.In terms of answers belonging to the "Other" category, they can be clustered into three categories: people using LLM (large language model) assistants (Copilot, ChatGPT, Claude, etc.), reading the official Rust forums (Discord, URLO) or being mentored while contributing to Rust projects. We would like to extend a big thank you to those making our spaces friendly and welcoming for newcomers, as it is important work and it pays off. Interestingly, a non-trivial number of people "learned by doing" and used rustc error messages and clippy as a guide, which is a good indicator of the quality of Rust diagnostics.In terms of formal education, it seems that Rust has not yet penetrated university curriculums, as this is typically a very slowly moving area. Only a very small number of respondents (around 3%) have taken a university Rust course or used university learning materials.In terms of operating systems used by Rustaceans, Linux was the most popular choice, and it seems that it is getting increasingly popular year after year. It is followed by macOS and Windows, which have a very similar share of usage.As you can see in the wordcloud, there are also a few users that prefer Arch, btw.Rust programmers target a diverse set of platforms with their Rust programs. We saw a slight uptick in users targeting embedded and mobile platforms, but otherwise the distribution of platforms stayed mostly the same as last year. Since the WebAssembly target is quite diverse, we have split it into two separate categories this time. Based on the results it is clear that when using WebAssembly, it is mostly in the context of browsers (23%) rather than other use-cases (7%).We cannot of course forget the favourite topic of many programmers: which IDE (developer environment) they use. Although Visual Studio Code still remains the most popular option, its share has dropped by 5pp this year. On the other hand, the Zed editor seems to have gained considerable traction recently. The small percentage of those who selected "Other" are using a wide range of different tools: from CursorAI to classics like Kate or Notepad++. Special mention to the 3 people using "ed", that's quite an achievement.You can also take a look at the linked wordcloud that summarizes open answers to this question (the "Other" category), to see what other editors are also popular.We were excited to see that more and more people use Rust at work for the majority of their coding, 38% vs 34% from last year. There is a clear upward trend in this metric over the past few years.The usage of Rust within companies also seems to be rising, as 45% of respondents answered that their organisation makes non-trivial use of Rust, which is a 7pp increase from 2023.Once again, the top reason employers of our survey respondents invested in Rust was the ability to build relatively correct and bug-free software. The second most popular reason was Rust’s performance characteristics. 21% of respondents that use Rust at work do so because they already know it, and it's thus their default choice, an uptick of 5pp from 2023. This seems to suggest that Rust is becoming one of the baseline languages of choice for more and more companies.Similarly to the previous year, a large percentage of respondents (82%) report that Rust helped their company achieve its goals. In general, it seems that programmers and companies are quite happy with their usage of Rust, which is great!In terms of technology domains, the situation is quite similar to the previous year. Rust seems to be especially popular for creating server backends, web and networking services and cloud technologies. It also seems to be gaining more traction for embedded use-cases.You can scroll the chart to the right to see more domains. Note that the Automotive domain was not offered as a closed answer in the 2023 survey (it was merely entered through open answers), which might explain the large jump.It is exciting to see the continued growth of professional Rust usage and the confidence so many users feel in its performance, control, security and safety, enjoyability, and more!As always, one of the main goals of the State of Rust survey is to shed light on challenges, concerns, and priorities on Rustaceans’ minds over the past year.We have asked our users about aspects of Rust that limit their productivity. Perhaps unsurprisingly, slow compilation was at the top of the list, as it seems to be a perennial concern of Rust users. As always, there are efforts underway to improve the speed of the compiler, such as enabling the parallel frontend or switching to a faster linker by default. We invite you to test these improvements and let us know if you encounter any issues.Other challenges included subpar support for debugging Rust and high disk usage of Rust compiler artifacts. On the other hand, most Rust users seem to be very happy with its runtime performance, the correctness and stability of the compiler and also Rust's documentation.In terms of specific unstable (or missing) features that Rust users want to be stabilized (or implemented), the most desired ones were async closures and if/let while chains. Well, we have good news! Async closures will be stabilized in the next version of Rust (1.85), and if/let while chains will hopefully follow soon after, once Edition 2024 is released (which will also happen in Rust 1.85).Other coveted features are generators (both sync and async) and more powerful generic const expressions. You can follow the Rust Project Goals to track the progress of these (and other) features.In the open answers to this question, people were really helpful and tried hard to describe the most notable issues limiting their productivity. We have seen mentions of struggles with async programming (an all-time favourite), debuggability of errors (which people generally love, but they are not perfect for everyone) or Rust tooling being slow or resource intensive (rust-analyzer and rustfmt). Some users also want a better IDE story and improved interoperability with other languages.This year, we have also included a new question about the speed of Rust's evolution. While most people seem to be content with the status quo, more than a quarter of people who responded to this question would like Rust to stabilize and/or add features more quickly, and only 7% of respondents would prefer Rust to slow down or completely stop adding new features.Interestingly, when we asked respondents about their main worries for the future of Rust, one of the top answers remained the worry that Rust will become too complex. This seems to be in contrast with the answers to the previous question. Perhaps Rust users still seem to consider the complexity of Rust to be manageable, but they worry that one day it might become too much.We are happy to see that the amount of respondents concerned about Rust Project governance and lacking support of the Rust Foundation has dropped by about 6pp from 2023.Each year, the results of the State of Rust survey help reveal the areas that need improvement in many areas across the Rust Project and ecosystem, as well as the aspects that are working well for our community.If you have any suggestions for the Rust Annual survey, please let us know!We are immensely grateful to those who participated in the 2024 State of Rust Survey and facilitated its creation. While there are always challenges associated with developing and maintaining a programming language, this year we were pleased to see a high level of survey participation and candid feedback that will truly help us make Rust work better for everyone.If you’d like to dig into more details, we recommend you to browse through the full survey report.]]></content:encoded></item><item><title>Giampaolo Rodola: psutil: drop Python 2.7 support</title><link>https://gmpy.dev/blog/2025/psutil-drop-python-27-support</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[About dropping Python 2.7 support in psutil, 3 years ago
I stated:Not a chance, for many years to come. [Python 2.7] currently represents 7-10%
of total downloads, meaning around 70k / 100k downloads per day.Only 3 years later, and to my surprise, downloads for Python 2.7 dropped to
0.36%! As such, as of psutil 7.0.0, I finally decided to drop support for
Python 2.7!These are downloads per month:According to pypistats.org Python 2.7 downloads
represents the 0.28% of the total, around 15.000 downloads per day.Maintaining 2.7 support in psutil had become increasingly difficult, but still
possible. E.g. I could still run tests by using old PYPI
backports.
GitHub Actions could still be
tweaked
to run tests and produce 2.7 wheels on Linux and macOS. Not on Windows though,
for which I had to use a separate service (Appveyor). Still, the amount of
hacks in psutil source code necessary to support Python 2.7 piled up over the
years, and became quite big. Some disadvantages that come to mind:Having to maintain a Python compatibility layers like
  psutil/_compat.py.
  This translated in extra extra code and extra imports.The C compatibility layer to differentiate between Python 2 and 3 (#if
  PY_MAJOR_VERSION <= 3, etc.).Dealing with the string vs. unicode differences, both in Python and in C.Inability to use modern language features, especially f-strings.Inability to freely use s, which created a difference on how CONSTANTS
  were exposed in terms of API.Having to install a specific version of  and other (outdated)
  deps.Relying on the third-party Appveyor CI service to run tests and produce 2.7
  wheels.Running 4 extra CI jobs on every commit (Linux, macOS, Windows 32-bit,
  Windows 64-bit) making the CI slower and more subject to failures (we have
  quite a bit of flaky tests).The distribution of 7 wheels specific for Python 2.7. E.g. in the previous
  release I had to upload:psutil-6.1.1-cp27-cp27m-macosx_10_9_x86_64.whl
psutil-6.1.1-cp27-none-win32.whl
psutil-6.1.1-cp27-none-win_amd64.whl
psutil-6.1.1-cp27-cp27m-manylinux2010_i686.whl
psutil-6.1.1-cp27-cp27m-manylinux2010_x86_64.whl
psutil-6.1.1-cp27-cp27mu-manylinux2010_i686.whl
psutil-6.1.1-cp27-cp27mu-manylinux2010_x86_64.whl
The removal was done in
PR-2841, which removed around
1500 lines of code (nice!). . In doing so, in the doc I
still made the promise that the 6.1.* serie will keep supporting Python 2.7
and will receive  (no new features). It will be
maintained in a specific python2
branch. I explicitly kept
the
setup.py
script compatible with Python 2.7 in terms of syntax, so that, when the tarball
is fetched from PYPI, it will emit an informative error message on . The user trying to install psutil on Python 2.7 will see:$pip2installpsutil
Asofversion.0.0psutilnolongersupportsPython.7.
LatestversionsupportingPython.7ispsutil.1.X.
Installitwith:.
As the informative message states, users that are still on Python 2.7 can still
use psutil with:pip2 install psutil==6.1.*
]]></content:encoded></item><item><title>Kay Hayen: Nuitka Release 2.6</title><link>https://nuitka.net/posts/nuitka-release-26.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 23:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[ Path normalization to native Windows format was required
in more places for the  variant of .The  function doesn’t normalize to native Win32
paths with MSYS2, instead using forward slashes. This required manual
normalization in additional areas. (Fixed in 2.5.1) Fix, give a proper error when extension modules asked to
include failed to be located. instead of a proper error message.
(Fixed in 2.5.1)Fix, files with illegal module names (containing ) in their
basename were incorrectly considered as potential sub-modules for
. These are now skipped. (Fixed in 2.5.1) Improved stability by preventing crashes when stubgen
encounters code it cannot handle. Exceptions from it are now ignored.
(Fixed in 2.5.1) Addressed a crash that occurred when encountering
assignments to non-variables. (Fixed in 2.5.1) Fixed a regression introduced in 2.5 release that could
lead to segmentation faults in exception handling for generators.
(Fixed in 2.5.2) Corrected an issue where dictionary copies of large
split directories could become corrupted. This primarily affected
instance dictionaries, which are created as copies until updated,
potentially causing problems when adding new keys. (Fixed in 2.5.2) Removed the assumption that module dictionaries
always contain only strings as keys. Some modules, like
 on macOS, use non-string keys. (Fixed in 2.5.2) Ensured that the  option correctly
affects the C compilation process. Previously, only individual
disables were applied. (Fixed in 2.5.2) Fixed a crash that could occur during compilation
when unary operations were used within binary operations. (Fixed in
2.5.3) Corrected the handling of
, which could lead to crashes. (Fixed
in 2.5.4) Resolved a segmentation fault occurring at runtime
when calling  with only keyword arguments.
(Fixed in 2.5.5) Harmless warnings generated for x64 DLLs on arm64 with
newer macOS versions are now ignored. (Fixed in 2.5.5) Addressed a crash in Nuitka’s dictionary code that
occurred when copying dictionaries due to internal changes in Python
3.13. (Fixed in 2.5.6) Improved onefile mode signing by applying
 to the signature of binaries, not just
app bundles. (Fixed in 2.5.6) Corrected an issue where too many paths were added as
extra directories from the Nuitka package configuration. This
primarily affected the  package, which currently relies
on the  import hack. (Fixed in 2.5.6) Prevented crashes on macOS when creating onefile
bundles with Python 2 by handling negative CRC32 values. This issue
may have affected other versions as well. (Fixed in 2.5.6) Restored the functionality of code provided in
, which was no longer being applied due to a
regression. (Fixed in 2.5.6) Suppressed the app bundle mode recommendation when it is
already in use. (Fixed in 2.5.6) Corrected path normalization when the output directory
argument includes “~”. GitHub Actions Python is now correctly identified as a
Homebrew Python to ensure proper DLL resolution. (Fixed in 2.5.7) Fixed a reference leak that could occur with
values sent to generator objects. Asyncgen and coroutines were not
affected. (Fixed in 2.5.7) The  scan now correctly handles
cases where both a package init file and competing Python files
exist, preventing compile-time conflicts. (Fixed in 2.5.7) Resolved an issue where handling string constants in
modules created for Python 3.12 could trigger assertions, and modules
created with 3.12.7 or newer failed to load on older Python 3.12
versions when compiled with Nuitka 2.5.5-2.5.6. (Fixed in 2.5.7) Corrected the tuple code used when calling certain
method descriptors. This issue primarily affected a Python 2
assertion, which was not impacted in practice. (Fixed in 2.5.7) Updated resource readers to accept multiple
arguments for , and correctly handle
 and  as keyword-only arguments. The platform encoding is no longer used to decode
 logs. Instead,  is used, as it is sufficient for
matching filenames across log lines and avoids potential encoding
errors. (Fixed in 2.5.7) Requests to statically link libraries for 
are now ignored, as these libraries do not exist. (Fixed in 2.5.7) Fixed a memory leak affecting the results of
functions called via specs. This primarily impacted overloaded hard
import operations. (Fixed in 2.5.7) When multiple distributions for a package are found,
the one with the most accurate file matching is now selected. This
improves handling of cases where an older version of a package (e.g.,
) is overwritten with a different variant (e.g.,
), ensuring the correct version is used for
Nuitka package configuration and reporting. (Fixed in 2.5.8) Prevented a potential crash during onefile
initialization on Python 2 by passing the directory name directly
from the onefile bootstrap, avoiding the use of  which
may not be fully loaded at that point. (Fixed in 2.5.8) Preserved necessary  environment variables on
Windows for packages that require loading DLLs from those locations.
Only  entries not pointing inside the installation prefix are
removed. (Fixed in 2.5.8) Corrected the  check to function
properly when distribution names and package names differ. (Fixed in
2.5.8) Improved package name resolution for Anaconda
distributions by checking conda metadata when file metadata is
unavailable through the usual methods. (Fixed in 2.5.8) Normalized the downloaded gcc path to use native Windows
slashes, preventing potential compilation failures. (Fixed in 2.5.9) Restored static libpython functionality on Linux by
adapting to a signature change in an unexposed API. (Fixed in 2.5.9) Prevented  from being resurrected when a
finalizer is attached, resolving memory leaks that could occur with
 in the presence of exceptions. (Fixed in 2.5.10) Suppressed the gcc download prompt that could appear during
 output on Windows systems without MSVC or with an
improperly installed gcc.Ensured compatibility with monkey patched  or 
functions, which are used in some testing scenarios. Improved the determinism of the JSON statistics
output by sorting keys, enabling reliable build comparisons. Fixed a memory leak in  with finalizers,
which could lead to significant memory consumption when using
 and encountering exceptions. Optimized empty generators (an optimization result) to
avoid generating unused context code, eliminating C compilation
warnings. Fixed a reference leak affecting the  value
in . While typically , this could lead to
observable reference leaks in certain cases. Improved handling of  and 
resurrection, preventing memory leaks with  and
, and ensuring correct execution of  code in
coroutines. Corrected the handling of  objects
resurrecting during deallocation. While not explicitly demonstrated,
this addresses potential issues similar to those encountered with
coroutines, particularly for old-style coroutines created with the
 decorator. Fixed a potential crash during runtime trace collection by
ensuring timely initialization of the output mechanism.]]></content:encoded></item><item><title>Precision agriculture powered by AI for climate-resilient crops</title><link>https://www.datasciencecentral.com/precision-agriculture-powered-by-ai-for-climate-resilient-crops/</link><author>Shanthababu Pandian</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 21:25:50 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[AI in Agriculture Precision Farming AI-Powered Agriculture Climate-Resilient Crops
Sustainable Farming Practices AI for Pest Control AI for Soil Analysis Machine Learning in Agriculture Smart Farming Solutions IoT in Agriculture Crop Monitoring with AI
Predictive Analytics in Farming AI for Weather Prediction in Agriculture
AI-Driven Precision Irrigation AI in Fertilization Optimization Sustainable Agriculture Technology Advanced Farming Techniques Agriculture Data Analysis with AI
AI-Powered Smart Irrigation Agricultural Innovation with AI]]></content:encoded></item><item><title>Show HN: I made my own OS from scratch because I was bored</title><link>https://jotalea.com.ar/misc/jotaleaos/</link><author>Jotalea</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 20:55:12 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitHub Availability Report: January 2025</title><link>https://github.blog/news-insights/company-news/github-availability-report-january-2025/</link><author>Jakub Oleksy</author><category>official</category><pubDate>Wed, 12 Feb 2025 20:49:36 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[In January, we experienced three incidents that resulted in degraded performance across GitHub services.January 09 1:26 UTC (lasting 31 minutes)On January 9, 2025, between 01:26 UTC and 01:56 UTC, GitHub experienced widespread disruption to many services, with users receiving 500 responses when trying to access various functionality. This was due to a deployment which introduced a query that saturated a primary database server. On average, the error rate was 6% and peaked at 6.85% of update requests.We were able to mitigate the incident by identifying the source of the problematic query and rolling back the deployment. The internal tooling and our dashboards surfaced the relevant data that helped us quickly identify the problematic query. It took us a total of 14 minutes from the time to engage to finding the errant query.However, we are investing in tooling to detect problematic queries prior to deployment to prevent and to reduce our time to detection and mitigation of issues like this one in the future.January 13 23:35 UTC (lasting 49 minutes)On January 13, 2025, between 23:35 UTC and 00:24 UTC, all Git operations were unavailable due to a configuration change related to traffic routing and testing that caused our internal load balancer to drop requests between services that Git relies upon.We mitigated the incident by rolling back the configuration change.We are improving our monitoring and deployment practices to improve our time to detection and automated mitigation for issues like this in the future.January 30 14:22 UTC (lasting 26 minutes)On January 30, 2025, between 14:22 UTC and 14:48 UTC, web requests to github.com experienced failures (at peak the error rate was 44%), with the average successful request taking over three seconds to complete.This outage was caused by a hardware failure in the caching layer that supports rate limiting. In addition, the impact was prolonged due to a lack of automated failover for the caching layer. A manual failover of the primary to trusted hardware was performed following recovery to ensure that the issue would not reoccur under similar circumstances.As a result of this incident, we will be moving to a high availability cache configuration and adding resilience to cache failures at this layer to ensure requests are able to be handled should similar circumstances happen in the future.Please follow our status page for real-time updates on status changes and post-incident recaps. To learn more about what we’re working on, check out the GitHub Engineering Blog.]]></content:encoded></item><item><title>Show HN: yknotify – Notify when YubiKey needs touch on macOS</title><link>https://github.com/noperator/yknotify</link><author>noperator</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 20:24:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Yes, You Have the Right to Film ICE</title><link>https://www.eff.org/deeplinks/2025/02/yes-you-have-right-film-ice</link><author>Matthew Guariglia</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/OG-PoliceRecording.png" length="" type=""/><pubDate>Wed, 12 Feb 2025 19:58:33 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[What to Know When Recording Law EnforcementYour First Amendment Right to Record Law Enforcement Officers Exercising Their Official Duties in PublicSpecial Considerations for Recording AudioDo Not Interfere With Law Enforcement]]></content:encoded></item><item><title>Show HN: A no-build fullstack SSR TypeScript web framework</title><link>https://jsr.io/@fullsoak/fullsoak</link><author>thesephi</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 19:54:52 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>IEEE Unveils the 2025–2030 Strategic Plan</title><link>https://spectrum.ieee.org/ieee-2025-2030-strategic-plan</link><author>IEEE</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjM3NzA5My9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2MTI3MTcwNn0.LrFFj3crdpb3QjS2y-oZ-dHy3RYNn9U8HYdHYssKuhE/image.jpg?width=600" length="" type=""/><pubDate>Wed, 12 Feb 2025 19:00:06 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The new strategic goals bolster IEEE’s long-standing mission]]></content:encoded></item><item><title>ARCTIC Freezer 4U-SP5 Provides Effective Cooling For AMD EPYC 9004/9005 CPUs</title><link>https://www.phoronix.com/review/arctic-freezer-4u-sp5</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Feb 2025 19:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Along with the recently reviewed ARCTIC Freezer 4U-M for Ampere Altra, ARCTIC Cooling had also recently sent over their ARCTIC Freezer 4U-SP5 heatsink for cooling AMD EPYC 9004/9005 server processors within 4U rackmount height requirements. This cooler does a very good job at keeping even 400 Watt processors running well.]]></content:encoded></item><item><title>Oracle&apos;s Ellison Calls for Governments To Unify Data To Feed AI</title><link>https://developers.slashdot.org/story/25/02/12/1857208/oracles-ellison-calls-for-governments-to-unify-data-to-feed-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Wed, 12 Feb 2025 18:56:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[Oracle co-founder and chairman Larry Ellison said governments should consolidate all national data for consumption by AI models, calling this step the "missing link" for them to take full advantage of the technology. From a report: Fragmented sets of data about a population's health, agriculture, infrastructure, procurement and borders should be unified into a single, secure database that can be accessed by AI models, Ellison said in an on-stage interview with former British Prime Minister Tony Blair at the World Government Summit in Dubai. 

Countries with rich population data sets, such as the UK and United Arab Emirates, could cut costs and improve public services, particularly health care, with this approach, Ellison said. Upgrading government digital infrastructure could also help identify wastage and fraud, Ellison said. IT systems used by the US government are so primitive that it makes it difficult to identify "vast amounts of fraud," he added, pointing to efforts by Elon Musk's team at the Department of Government Efficiency to weed it out.]]></content:encoded></item><item><title>When Platforms and the Government Unite, Remember What’s Private and What Isn’t</title><link>https://www.eff.org/deeplinks/2025/02/when-platforms-and-government-unite-remember-whats-private-and-what-isnt</link><author>Matthew Guariglia</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/social-media-surveillance-1b_0.jpg" length="" type=""/><pubDate>Wed, 12 Feb 2025 18:07:52 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mesa 25.0-rc3 Released With Numerous RADV &amp; RadeonSI Fixes</title><link>https://www.phoronix.com/news/Mesa-25.0-rc3-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Feb 2025 17:51:35 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Mesa 25.0-rc3 is out today as a rather large weekly release candidate to Mesa 25.0 that will be debuting as stable later this month...]]></content:encoded></item><item><title>Fine-tune LLMs with synthetic data for context-based Q&amp;A using Amazon Bedrock</title><link>https://aws.amazon.com/blogs/machine-learning/fine-tune-llms-with-synthetic-data-for-context-based-qa-using-amazon-bedrock/</link><author>Sue Cha</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 17:44:10 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[There’s a growing demand from customers to incorporate generative AI into their businesses. Many use cases involve using pre-trained large language models (LLMs) through approaches like Retrieval Augmented Generation (RAG). However, for advanced, domain-specific tasks or those requiring specific formats, model customization techniques such as fine-tuning are sometimes necessary. Amazon Bedrock provides you with the ability to customize leading foundation models (FMs) such as Anthropic’s Claude 3 Haiku and Meta’s Llama 3.1.Amazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available through an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case. Amazon Bedrock offers a serverless experience, so you can get started quickly, privately customize FMs with your own data, and integrate and deploy them into your applications using AWS tools without having to manage any infrastructure.Fine-tuning is a supervised training process where labeled prompt and response pairs are used to further train a pre-trained model to improve its performance for a particular use case. One consistent pain point of fine-tuning is the lack of data to effectively customize these models. Gathering relevant data is difficult, and maintaining its quality is another hurdle. Furthermore, fine-tuning LLMs requires substantial resource commitment. In such scenarios, synthetic data generation offers a promising solution. You can create synthetic training data using a larger language model and use it to fine-tune a smaller model, which has the benefit of a quicker turnaround time.In this post, we explore how to use Amazon Bedrock to generate synthetic training data to fine-tune an LLM. Additionally, we provide concrete evaluation results that showcase the power of synthetic data in fine-tuning when data is scarce.The solution comprises two main steps:Generate synthetic data using the Amazon Bedrock InvokeModel API.Fine-tune using an Amazon Bedrock custom model.For synthetic data generation, we use a larger language model (such as Anthropic’s Claude 3 Sonnet on Amazon Bedrock) as the teacher model, and a smaller language model (such as Anthropic’s Claude Instant 1.2 or Claude 3 Haiku on Amazon Bedrock) as the student model for fine-tuning. We use the larger teacher model to generate new data based on its knowledge, which is then used to train the smaller student model. This concept is similar to knowledge distillation used in deep learning, except that we’re using the teacher model to generate a new dataset from its knowledge rather than directly modifying the architecture of the student model.The following diagram illustrates the overall flow of the solution.Finally, we share our experiment results, where we compare the performance of the model fine-tuned with synthetic data to the baseline (not fine-tuned) model and to a model fine-tuned with an equal amount of original training data.To generate synthetic data and fine-tune models using Amazon Bedrock, you first need to create an AWS Identity and Access Management (IAM) service role with the appropriate permissions. This role is used by Amazon Bedrock to access the necessary resources on your behalf.If you’re running this code using an Amazon SageMaker notebook instance, edit the IAM role that’s attached to the notebook (for example, AmazonSageMaker-ExecutionRole-XXX) instead of creating a new role. Follow Create a service role for model customization to modify the trust relationship and add the S3 bucket permission. Additionally, on the role’s tab, create the following inline policies:Policy name: bedrock-customization{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel",
                "bedrock:ListModelCustomizationJobs",
                "bedrock:DeleteCustomModel",
                "bedrock:CreateModelCustomizationJob",
                "bedrock:StopModelCustomizationJob",
                "bedrock:ListCustomModels",
                "bedrock:GetCustomModel",
                "bedrock:GetModelCustomizationJob"
            ],
            "Resource": "*"
        }
    ]
}
Policy name: iam-pass-role{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "iam:PassRole",
            "Resource": [
                "${sagemaker-execution-role-arn}"
            ]
        }
    ]
}The final permission policies for the SageMaker execution role should look like the following, which include AmazonSageMaker-ExecutionPolicy, AmazonSageMakerFullAccess, bedrock-customization, and iam-pass-role.Generate synthetic data using the Amazon Bedrock InvokeModel APIWe use the Amazon Bedrock InvokeModel API to generate synthetic data for fine-tuning. You can use the API to programmatically send an inference (text generation) request to the model of your choice. All you need is a well-crafted prompt tailored for data synthesis. We used the following sample prompt for our use case:PROMPT = """
You are an AI assistant who is an expert in Amazon services. Your task is to understand a system that takes in a list of documents, and based on that, answers a question by providing citations for the documents that it referred the answer from.

Your job is to generate three new Question/Answer pairs, emulating the tone, style, and grammar of the original data provided.

Here is the original data :
Input Documents and Question : {document}\n\nQuestion: {question}
Output Answer : {answer}

Strictly return a jsonl with the keys (question, answer, topic). Every topic should be different. The answers should be in the exact same format as the original. The question and the answer should be different in content from the original data provided, and all questions should be diverse and different from each other. Do not answer in any other format. The response should be parsable as a jsonl.
"""The goal of our use case was to fine-tune a model to generate a relevant and coherent answer based on a given reference document and a question. RAG is a popular technique used for such Q&A tasks; however, one significant challenge with RAG is the potential for retrieving unrelated or irrelevant documents, which can lead to inaccurate responses. You can apply fine-tuning to guide the model to better focus on the relevance of the documents to the question instead of using the provided documents without context to answer the question.Our dataset includes Q&A pairs with reference documents regarding AWS services. Each sample has up to five reference documents as context, and a single-line question follows. The following table shows an example.Step 1: Prepare to work with AWS CodeStar projectsIn this step, you create an AWS CodeStar service role and an Amazon EC2 key pair, so that you can begin creating and working with AWS CodeStar projects. If you have used AWS CodeStar before, skip ahead to Step 2Step 2: Create a Project in AWS CodeStar.For this step, follow the instructions in Setting Up AWS CodeStar in the AWS CodeStar User Guide. Do not create a new AWS account, IAM user, or IAM group as part of those instructions. Use the ones you created or identified in Team Setup for AWS Cloud9. When you finish following those instructions, return to this topic.Before you can start using AWS CodeStar, you must complete the following steps.Step 1: Create an accountStep 2: Create the AWS CodeStar Service RoleStep 3: Configure the User’s IAM PermissionsStep 4: Create an Amazon EC2 Key Pair for AWS CodeStar ProjectsStep 5: Open the AWS CodeStar ConsoleHow Do I Get Started with AWS CodeStar?To get started with AWS CodeStar:Prepare to use AWS CodeStar by following the steps in Setting Up AWS CodeStar.Experiment with AWS CodeStar by following the steps in the Getting Started with AWS CodeStar tutorial.Share your project with other developers by following the steps in Add Team Members to an AWS CodeStar Project.Integrate your favorite IDE by following the steps in Use an IDE with AWS CodeStar.Step 2: Create a project in AWS CodeStarIn this step, you create a project in AWS CodeStar.If you already have a project in AWS CodeStar you want to use, skip ahead to Step 3: Create an AWS Cloud9 Development Environment and Connect It to the Project.For this step, follow the instructions in Create a Project in AWS CodeStar in the AWS CodeStar User Guide. In the AWS CodeStar create project wizard, when you get to the Setup tools page or Connect to your source repository page, choose Skip, and then return to this topic.Create a Project in AWS CodeStarYou use the AWS CodeStar console to create a project. If you use a project template, it sets up the required resources for you. The template also includes sample code that you can use to start coding.To create a project, sign in to the AWS Management Console with an IAM user that has the AWSCodeStarFullAccess policy or equivalent permissions. For more information, see Setting Up AWS CodeStar.Note: You must complete the steps in Setting Up AWS CodeStar before you can complete the procedures in this topic.Create a Project in AWS CodeStar (Console)Create a Project in AWS CodeStar (AWS CLI)Answer the following question to the best of your ability. Use and paraphrase the information in the context, if it answers the question. Otherwise, rely on your knowledge and memory.How do I get started with AWS CodeStar?To get started with AWS CodeStar, follow these steps:1. Prepare to work with AWS CodeStar projects:a. Create an AWS CodeStar service role by following the instructions in “Setting Up AWS CodeStar” (Document 2) in the AWS CodeStar User Guide [1].b. Create an Amazon EC2 key pair for AWS CodeStar projects, as described in the same guide [1].2. Create a project in AWS CodeStar:a. Sign in to the AWS Management Console with an IAM user that has the AWSCodeStarFullAccess policy or equivalent permissions [2].b. In the AWS CodeStar console, select one of the available project templates, which will automatically provision all the required resources for your project [3].c. Once your project is provisioned, you can see your running application from the Application endpoints tile [3].3. Connect to the AWS CodeCommit source repository for your project:a. Use the steps provided in the CodeStar console to connect to the AWS CodeCommit source repository for your project [3].b. Begin coding and making changes to your project\’s source code.4. Track and manage changes in the release process:a. Use the project dashboard to track and manage changes in the release process [3].b. View the most recent project activity on the dashboard.5. Share your project with other developers:a. Follow the steps in “Add Team Members to an AWS CodeStar Project” to share your project with other developers [3].6. Integrate your favorite IDE:a. Follow the steps in “Use an IDE with AWS CodeStar” to integrate your preferred Integrated Development Environment (IDE) with AWS CodeStar [3].[1] https://docs.aws.amazon.com/codestar/latest/userguide/setting-up.html[2] https://docs.aws.amazon.com/codestar/latest/userguide/create-project.html[3] https://docs.aws.amazon.com/codestar/latest/userguide/getting-started.htmlFor data synthesis, we asked the model to generate three new Q&A pairs per reference document. However, you can adjust the number as needed. The crucial part is to make the model think deeply about a variety of topics. Because the purpose of generating synthetic data is to enrich the training dataset, it’s more beneficial to have the model look at different parts of the documents and create Q&A pairs with different topics than the original.The following example shows how to generate synthetic data with the Amazon Bedrock InvokeModel API. We tested the preceding prompt with Anthropic’s Claude 3 Sonnet. If you want to test a different model, retrieve the corresponding model ID from Amazon Bedrock model IDs, and replace the modelId variable in the function.import boto3
import json

bedrock = boto3.client(service_name="bedrock-runtime")

def generate_synthetic_data(document, question, answer):
    
    values = {
        "document": document,
        "question": question,
        "answer": answer
    }
    
    body = {
        "messages": [{
            "role": "user", "content": PROMPT.format(**values)
        }],
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 2048,
        "temperature" : 0.5
    }
    
    response = bedrock.invoke_model(
        body=json.dumps(body),
        modelId="anthropic.claude-3-sonnet-20240229-v1:0",
        accept="application/json",
        contentType="application/json"
    )
    
    response_body = json.loads(response.get('body').read())
    
    return response_body['content'][0]['text']
The preceding function returns three JSONL records in strings with question, answer, and topic as keys. The following parse_llm_output function loads the strings and uses regular expressions to retrieve the generated questions and answers. Then, the create_synthetic_samples function combines those two functionalities to produce the final synthetic training samples.import re
import pd

def parse_llm_output(jsonl_string):
    
    question_pattern = re.compile(r'"question":\s*"([^"]+)"')
    answer_pattern = re.compile(r'"answer":\s*"(.*?)"\s*,\s*"topic"') 
    questions = question_pattern.findall(jsonl_string)
    answers = answer_pattern.findall(jsonl_string)
    
    return questions, answers


def create_synthetic_samples(row: pd.Series) -> pd.DataFrame:

    jsonl_string = generate_synthetic_data(row['document'], row['question'], row['answer'])
    questions, answers = parse_llm_output(jsonl_string)
    
    return pd.DataFrame({
        "document": [row['document']] * len(questions),
        "question": questions,
        "answer": answers
    })


def to_customization_format(row):

    msg = {
        "messages": [
            {"role": "user", "content": f"{row['document']}\n\nQuestion: {row['question']}"},
            {"role": "assistant", "content": row['answer']}
        ]
    }
    
    return msg
The following script combines all of the preceding functions and gives you the final training set with both original and synthetic samples. We convert the samples into the format required by the customization job using the to_customization_format function and save them as train.jsonl. Assume the input data is a CSV file with three columns: document, question, and answer.import pandas as pd

# Load original training samples
original_train = pd.read_csv(input_df_path)

# Create synthetic training samples
synthetic_train = pd.concat(original_train.apply(create_synthetic_samples, axis=1).tolist())

# Combine original and synthetic samples
final_train_df = pd.concat([original_train, synthetic_train])

# Convert to the format required by the customization job
final_train = final_train_df.apply(to_customization_format, axis=1).tolist()

# Write to JSONL file    
with open('train.jsonl', 'w') as file:
    for item in final_train:
        json.dump(item, file)
        file.write('\n')
Fine-tune using an Amazon Bedrock custom modelNow that you have the synthetic data generated by the teacher model along with your original data, it’s time to train the student model. We fine-tune the student model using the Amazon Bedrock custom model functionality.Model customization is the process of providing training data to an FM to improve its performance for specific use cases. Amazon Bedrock offers three model customization methods as of this writing:You can create your own custom model using any of these methods through the Amazon Bedrock console or API. For more information on supported models and AWS Regions with various customization methods, please see User guide for model customization. In this section, we focus on how to fine-tune a model using the API.To create a fine-tuning job in Amazon Bedrock, complete the following prerequisite steps:Upload the jsonl file to the training data bucket.Make sure that you have created an IAM role, as described in the PrerequisitesWhen these steps are complete, run the following code to submit a new fine-tuning job. In our use case, the student model was Anthropic’s Claude Instant 1.2. At the time of writing, Anthropic’s Claude 3 Haiku is generally available, and we recommend following the rest of the code using Anthropic’s Claude 3 Haiku. For the release announcement, see Fine-tuning for Anthropic’s Claude 3 Haiku in Amazon Bedrock is now generally available.If you want to try different models, you must check the model provider’s terms of service yourself. Many providers restrict using their models to train competing models. For the latest model support information, see Supported Regions and models for model customization, and replace baseModelIdentifier accordingly. Different models have different hyperparameters. For more information, see Custom model hyperparameters.import boto3
import json
import time

bedrock = boto3.client(service_name='bedrock')
    
# Set parameters
customizationType = "FINE_TUNING"
baseModelIdentifier = "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k"
roleArn = "${customization-role-arn}"
jobName = "${customization-job-name}"
customModelName = "${customization-model-name}"
hyperParameters = {
    "epochCount": "1",
    "batchSize": "96",
    "learningRateMultiplier": "0.5",
 }
trainingDataConfig = {"s3Uri": "s3://${training-bucket}/train.jsonl"}
outputDataConfig = {"s3Uri": "s3://${output-bucket}/myOutputData"}

# Create job
response_ft = bedrock.create_model_customization_job(
    jobName=jobName, 
    customModelName=customModelName,
    roleArn=roleArn,
    baseModelIdentifier=baseModelIdentifier,
    hyperParameters=hyperParameters,
    trainingDataConfig=trainingDataConfig,
    outputDataConfig=outputDataConfig
)

jobArn = response_ft.get('jobArn')

# Check job status
while True:
    status = bedrock.get_model_customization_job(jobIdentifier=jobArn).get('status')
    if status != 'InProgress':
        print(status)
        break
    else:
        print(status)
    time.sleep(30)
When the status changes to , your fine-tuned student model is ready for use. To run an inference with this custom model, you need to purchase . A flexible option is available for custom models, which can be turned off when not in use and billed by the hour. A cost estimate is provided on the console prior to purchasing provisioned throughput.On the Amazon Bedrock console, choose in the navigation pane. Select the model you fine-tuned and choose Purchase provisioned throughput.The model name and type are automatically selected for you. Select  for . After you make this selection, the estimated cost is shown. If you’re okay with the pricing, choose .When the Provisioned Throughput becomes available, retrieve the ARN of the provisioned custom model and run the inference:import boto3
import json

bedrock = boto3.client(service_name="bedrock-runtime")

def run_student_model(document, question):
    
    values = {
        "document": document,
        "question": question,
    }
    
    body = {
        "messages": [{
            "role": "user", "content": PROMPT.format(**values)
        }],
        "max_tokens": 2048,
        "temperature" : 0.5
    }
    
    response = bedrock.invoke_model(
        body=json.dumps(body),
        modelId="${provisioned_model_arn}",
        accept="application/json",
        contentType="application/json"
    )
    
    response_body = json.loads(response.get('body').read())
    
    return response_body['content'][0]['text']
In this section, we share our experiment results to provide data points on how the synthetic data generated by a teacher model can improve the performance of a student model. For evaluation methods, we used an LLM-as-a-judge approach, where a judge model compares responses from two different models and picks a better response. Additionally, we conducted a manual evaluation on a small subset to assess whether the LLM-as-a-judge and human judges have aligned preferences.We carried out controlled experiments where we compared four different models as follows: 1,500 synthetic training samples for the 4 model were generated by Anthropic’s Claude 3 Sonnet, and we created three synthetic samples per one original reference document (3 samples * 500 original reference documents = 1,500 synthetic samples).Anthropic’s Claude Instant without any customizationAnthropic’s Claude Instant fine-tuned with 500 original training samplesAnthropic’s Claude Instant fine-tuned with 2,000 original training samplesAnthropic’s Claude Instant fine-tuned with 500 original training samples plus 1,500 synthetic training samplesLLM output evaluation is an important step in developing generative AI applications, but it is expensive and takes considerable time if done manually. An alternative solution to systematically evaluate output quality in large volume is the LLM-as-a-judge approach, where an LLM is used to evaluate another LLM’s responses.For our use case, we used Anthropic’s Claude 3 Sonnet and Meta Llama 3 70B as the judges. We asked the LLM judges to compare outputs from two different models and choose one over the other or state a tie. The following chart summarizes the judges’ decisions. Each number represents the percentage of times when the respective model was selected as providing a better answer, excluding tie cases. The test set contained 343 samples.As shown in the preceding chart, the Anthropic’s Claude 3 Sonnet judge preferred the response from the fine-tuned model with synthetic examples over the Anthropic’s Claude Instant base model (84.8% preference) and the fine-tuned model with original 500 samples (72.3% preference). However, the judge concluded that the fine-tuned model with 2,000 original examples was preferred over the fine-tuned model with synthetic examples (32.3% preference). This aligns with the expectation that when large, high-quality original data is available, it’s better to use the large training data that accurately reflects the target data distribution.The Meta Llama judge reached a similar conclusion. As shown in the preceding chart, it preferred the response from the fine-tuned model with synthetic samples over the Anthropic’s Claude Instant base model (75.6% preference) and the fine-tuned model with original 500 examples (76.4% preference), but the fine-tuned model with 2,000 original examples was the ultimate winner.To complement the LLM-as-a-judge result, we conducted manual evaluation with two human judges. We asked the two human evaluators to perform the same pairwise comparison task as the LLM judge, but for 20 examples. The following chart summarizes the results.As shown in the preceding chart, the two human evaluators reached a similar conclusion, reinforcing the LLM-as-a-judge result. The fine-tuned model with synthetic examples produced outputs that were more preferable than the Anthropic’s Claude Instant base model and the fine-tuned model with the original 500 examples; however, it didn’t outperform the fine-tuned model with the 2,000 original examples.These comparative evaluation results from both the LLM judges and human judges strongly demonstrate the power and potential of using data synthesis when training data is scarce. Moreover, by using high-quality data from the teacher model, we can effectively train the student model, which is lightweight and cost-effective for deployment in a production environment.Amazon Bedrock evaluationsRunning LLM-as-a-judge and human evaluation has become much easier with Amazon Bedrock. Model evaluation on Amazon Bedrock allows you to evaluate, compare, and select the best FMs for your use case. Human evaluation workflows can use your own employees or an AWS-managed team as reviewers. For more information on how to set up a human evaluation workflow, see Creating your first model evaluation that uses human workers. The latest feature, LLM-as-a-judge, is now in preview and allows you to assess multiple quality dimensions including correctness, helpfulness, and responsible AI criteria such as answer refusal and harmfulness. For step-by-step instructions, see New RAG evaluation and LLM-as-a-judge capabilities in Amazon Bedrock.Make sure to delete the following resources to avoid incurring cost:Provisioned throughput for the custom modelThe training_bucket and output_bucket S3 bucketsIn this post, we explored how to use Amazon Bedrock to generate synthetic training data using a large teacher language model and fine-tune a smaller student model with synthetic data. We provided instructions on generating synthetic data using the Amazon Bedrock InvokeModel API and fine-tuning the student model using an Amazon Bedrock custom model. Our evaluation results, based on both an LLM-as-a-judge approach and human evaluation, demonstrated the effectiveness of synthetic data in improving the student model’s performance when original training data is limited.Although fine-tuning with a large amount of high-quality original data remains the ideal approach, our findings highlight the promising potential of synthetic data generation as a viable solution when dealing with data scarcity. This technique can enable more efficient and cost-effective model customization for domain-specific or specialized use cases.If you’re interested in working with the AWS Generative AI Innovation Center and learning more about LLM customization and other generative AI use cases, visit Generative AI Innovation Center. is a Deep Learning Architect at the AWS Generative AI Innovation Center, where she specializes in model customization and optimization. She has extensive hands-on experience in solving customers’ business use cases by utilizing generative AI as well as traditional AI/ML solutions. Sujeong holds a M.S. degree in Data Science from New York University.Arijit Ghosh Chowdhury is a Scientist with the AWS Generative AI Innovation Center, where he works on model customization and optimization. In his role, he works on applied research in fine-tuning and model evaluations to enable GenAI for various industries. He has a Master’s degree in Computer Science from the University of Illinois at Urbana Champaign, where his research focused on question answering, search and domain adaptation. is a Senior Applied Scientist at Amazon Generative AI Innovation Center where he helps expedite the variety of use cases of AWS customers. Before joining Amazon, Sungmin was a postdoctoral research fellow at Harvard Medical School. He holds Ph.D. in Computer Science from New York University. Outside of work, Sungmin enjoys hiking, reading and cooking. is an Applied Scientist II at the AWS Generative AI Innovation Center, where she develops generative AI solutions for AWS customers. Her expertise encompasses designing and implementing innovative AI-driven and deep learning techniques, focusing on natural language processing, computer vision, multi-modal learning, and graph learning. Yiyue holds a Ph.D. in Computer Science from the University of Notre Dame, where her research centered on advanced machine learning and deep learning methodologies. Outside of work, she enjoys sports, hiking, and traveling. is a Machine Learning Engineer at the AWS Generative AI Innovation Center, where he works on model customization and optimization for LLMs. He also builds tools to help his team tackle various aspects of the LLM development life cycle—including fine-tuning, benchmarking, and load-testing—that accelerating the adoption of diverse use cases for AWS customers. He holds an M.S. degree in Computer Science from UC Davis. is a Senior Manager of Model Customization at the AWS Generative AI Innovation Center. Her team specializes in helping customers develop differentiating Generative AI solutions using their unique and proprietary data to achieve key business outcomes. She holds a Ph.D in Physics from the University of Iowa, with a focus on astronomical X-ray analysis and instrumentation development. Outside of work, she can be found hiking, mountain biking, and skiing around the mountains in Colorado.]]></content:encoded></item><item><title>Achieve ~2x speed-up in LLM inference with Medusa-1 on Amazon SageMaker AI</title><link>https://aws.amazon.com/blogs/machine-learning/achieve-2x-speed-up-in-llm-inference-with-medusa-1-on-amazon-sagemaker-ai/</link><author>Daniel Zagyva</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 17:41:33 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This blog post is co-written with Moran Beladev, Manos Stergiadis, and Ilya Gusev from Booking.com.Large language models (LLMs) have revolutionized the field of natural language processing with their ability to understand and generate humanlike text. Trained on broad, generic datasets spanning a wide range of topics and domains, LLMs use their parametric knowledge to perform increasingly complex and versatile tasks across multiple business use cases. Furthermore, companies are increasingly investing resources in customizing LLMs through few-shot learning and fine-tuning to optimize their performance for specialized applications.However, the impressive performance of LLMs comes at the cost of significant computational requirements, driven by their large number of parameters and autoregressive decoding process which is sequential in nature. This combination makes achieving low latency a challenge for use cases such as real-time text completion, simultaneous translation, or conversational voice assistants, where subsecond response times are critical.Researchers developed Medusa, a framework to speed up LLM inference by adding extra heads to predict multiple tokens simultaneously. This post demonstrates how to use Medusa-1, the first version of the framework, to speed up an LLM by fine-tuning it on Amazon SageMaker AI and confirms the speed up with deployment and a simple load test. Medusa-1 achieves an inference speedup of around two times without sacrificing model quality, with the exact improvement varying based on model size and data used. In this post, we demonstrate its effectiveness with a 1.8 times speedup observed on a sample dataset.Introduction to Medusa and its benefits for LLM inference speedLLMs generate text in a sequential manner, which involves autoregressive sampling, with each new token conditional on the previous ones. Generating K tokens necessitates K sequential executions of the model. This token-by-token processing introduces an inherent latency and computational overhead because the model needs to perform a separate forward pass for each new token in the output sequence. The following diagram from Role-Play with Large Language Models illustrates this flow.Speculative decoding tackles this challenge by using a smaller, faster draft model to generate multiple potential token continuations in parallel, which are then verified by a larger, more accurate target model. This parallelization speeds up text generation while maintaining the quality of the target model because the verification task is faster than autoregressive token generation. For a detailed explanation of the concept, refer to the paper Accelerating Large Language Model Decoding with Speculative Sampling. The speculative decoding technique can be implemented using the inference optimization toolkit on Amazon SageMaker Jumpstart.The paper Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads introduced Medusa as an alternative to speculative decoding. Instead of adding a separate draft model, it adds extra decoding heads to the LLM that generate candidate continuations simultaneously. These candidates are then evaluated in parallel using a tree-based attention mechanism. This parallel processing reduces the number of sequential steps needed, leading to faster inference times. The main advantage of Medusa over speculative decoding is that it eliminates the need to acquire and maintain a separate draft model while achieving higher speedups. For example, when tested on the MT-Bench dataset, the paper reports that Medusa-2 (the second version of Medusa) speeds up inference time by 2.8 times. This outperforms speculative decoding, which only manages to speed up inference time by 1.5 times on the same dataset.The Medusa framework currently supports Llama and Mistral models. Although it offers significant speed improvements, it does come with a memory trade-off (similar to speculative decoding). For instance, adding five Medusa heads to the 7-billion-parameter Mistral model increases the total parameter count by 750 million (150 million per head), which means these additional parameters must be stored in GPU memory, leading to a higher memory requirement. However, in most cases, this increase doesn’t necessitate switching to a higher GPU memory instance. For example, you can still use an  instance with 24 GB of GPU memory to host your 7-billion-parameter Llama or Mistral model with extra Medusa heads.Training Medusa heads requires additional development time and computational resources, which should be factored into project planning and resource allocation. Another important limitation to mention is that the current framework, when deployed on an Amazon SageMaker AI endpoint, only supports a batch size of one—a configuration typically used for low-latency applications.The following diagram from the original Medusa paper authors’ FasterDecoding repository gives a visual Medusa framework overview.There are two main variants of Medusa: – Requires a two-stage approach where you first fine-tune your LLM and then add Medusa heads and train them on top of your frozen fine-tuned LLM – Introduced later as an improvement, fine-tunes both the additional heads and the backbone LLM parameters together, enabling potentially even further latency speedupsThe Medusa paper reports that across models of varying sizes, you can achieve inference speedups of around two times for Medusa-1 and around three times for Medusa-2. With Medusa-1, the predictions are identical to those of the originally fine-tuned LLM. In contrast, with Medusa-2, we might observe slightly different results compared to simple fine-tuning of the LLM because both the heads and the backbone LLM parameters are updated together. In this post, we focus on Medusa-1.We cover the following steps in our solution:Load and prepare the datasetFine-tune an LLM using a SageMaker AI training jobTrain Medusa heads on top of a frozen fine-tuned LLM using a SageMaker AI training jobDeploy the fine-tuned LLM with Medusa heads on a SageMaker AI endpointDemonstrate LLM inference speedupBy following this solution, you can accelerate LLM inference in your applications, leading to faster response times and improved user experience.To build the solution yourself, there are the following prerequisites:Load and prepare the datasetNow that you have cloned the GitHub repository and opened the  notebook, you will load and prepare the dataset in the notebook. We encourage you to read this post while running the code in the notebook. For this post, we use a dataset called sql-create-context, which contains samples of natural language instructions, schema definitions and the corresponding SQL query. It contains 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL queries answering the question using the CREATE statement as context. For demonstration purposes, we select 3,000 samples and split them into train, validation, and test sets.You need to run the “Load and prepare the dataset” section of the  to prepare the dataset for fine-tuning. We also included a data exploration script to analyze the length of input and output tokens. After data exploration, we prepare the train, validation, and test sets and upload them to Amazon Simple Storage Service (Amazon S3).Fine-tune an LLM using SageMaker AI training jobWe use the Zephyr 7B β model as our backbone LLM. Zephyr is a series of language models trained to act as helpful assistants, and Zephyr 7B β is a fine-tuned version of Mistral-7B-v0.1, trained on a mix of publicly available and synthetic datasets using Direct Preference Optimization.To launch a SageMaker AI training job, we need to use the PyTorch or Hugging Face estimator. SageMaker AI starts and manages all the necessary Amazon Elastic Compute Cloud (Amazon EC2) instances for us, supplies the appropriate containers, downloads data from our S3 bucket to the container and uploads and runs the specified training script, in our case . We select the hyperparameters based on the QLoRA paper, but we encourage you to experiment with your own combinations. To expedite the execution of this code, we set the number of epochs to 1. However, for better results, it’s generally recommended to set the number of epochs to at least 2 or 3.from sagemaker.pytorch.estimator import PyTorch
from sagemaker.debugger import TensorBoardOutputConfig
import time
import os

def get_current_time():
    return time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())

def create_estimator(hyperparameters_dict, job_name, role, sess, train_scipt_path):
    metric=[
        {"Name": "loss", "Regex": r"'loss':\s*([0-9.]+)"},
        {"Name": "epoch", "Regex": r"'epoch':\s*([0-9.]+)"},
    ]

    tensorboard_s3_output_path = os.path.join(
       "s3://", sess.default_bucket(), job_name, 'tensorboard'
    )
    print("Tensorboard output path:", tensorboard_s3_output_path)

    tensorboard_output_config = TensorBoardOutputConfig(
        s3_output_path=tensorboard_s3_output_path,
        container_local_output_path=hyperparameters_dict['logging_dir']
    )
    estimator = PyTorch(
        sagemaker_session    = sess,
        entry_point          = train_scipt_path,    # train script
        source_dir           = 'train',      # directory which includes all the files needed for training
        instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job, "local_gpu" for local mode
        metric_definitions   = metric,
        instance_count       = 1,                 # the number of instances used for training
        role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3
        volume_size          = 300,               # the size of the EBS volume in GB
        framework_version      = '2.1.0',             # the pytorch_version version used in the training job
        py_version           = 'py310',           # the python version used in the training job
        hyperparameters      =  hyperparameters_dict,  # the hyperparameters passed to the training job
        disable_output_compression = True,        # not compress output to save training time and cost
        tensorboard_output_config = tensorboard_output_config
    )
    return estimator
    
# hyperparameters, which are passed into the training job
sft_hyperparameters = {
  ### SCRIPT PARAMETERS ###
  'train_dataset_path': '/opt/ml/input/data/train/train_dataset.json', # path where sagemaker will save training dataset
  'eval_dataset_path': '/opt/ml/input/data/eval/eval_dataset.json', # path where sagemaker will save evaluation dataset
  'model_id': model_id,
  'max_seq_len': 256,                               # max sequence length for model and packing of the dataset
  'use_qlora': True,                                 # use QLoRA model
  ### TRAINING PARAMETERS ###
  'num_train_epochs': 1,                             # number of training epochs
  'per_device_train_batch_size': 1,                  # batch size per device during training
  'gradient_accumulation_steps': 16,                  # number of steps before performing a backward/update pass
  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory
  'optim': "adamw_8bit",                             # use fused adamw 8bit optimizer
  'logging_steps': 15,                               # log every 10 steps
  'save_strategy': "steps",                          # save checkpoint every epoch
  'save_steps': 15,
  'save_total_limit': 2,
  'eval_strategy': "steps",
  'eval_steps': 15,
  'learning_rate': 1e-4,                             # learning rate, based on QLoRA paper
  'bf16': True,                                      # use bfloat16 precision
  'max_grad_norm': 10,                              # max gradient norm based on QLoRA paper
  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper
  'lr_scheduler_type': "constant",                   # use constant learning rate scheduler
  'output_dir': '/opt/ml/checkpoints/',              # Temporary output directory for model checkpoints
  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment
  'report_to': "tensorboard",                        # report metrics to tensorboard
  'logging_dir': "/opt/ml/output/tensorboard"        # tensorboard logging directory
}
 
sft_job_name = f"sft-qlora-text-to-sql-{get_current_time()}"
data = {
    'train': train_dataset_path,
    'eval': eval_dataset_path
}

sft_estimator = create_estimator(sft_hyperparameters, sft_job_name, role, sess, "fine_tune_llm.py")

sft_estimator.fit(job_name=sft_job_name, inputs=data, wait=False)When our training job has completed successfully after approximately 1 hour, we can use the fine-tuned model artifact for the next step, training the Medusa heads on top of it. To visualize the training metrics in Tensorboard, you can follow the guidance in this documentation: Load and visualize output tensors using the TensorBoard applicationTrain Medusa heads on top of frozen fine-tuned LLM using a SageMaker AI training jobFor training Medusa heads, we can reuse the functions previously mentioned to launch the training job. We selected hyperparameters based on a combination of what the Medusa paper reported and what we found to be best performing after a few experiments. We set the number of Medusa heads to 5 and used the 8-bit AdamW optimizer, as recommended by the paper. For simplicity, we maintained a constant learning rate of 1e-4 with a constant scheduler, similar to the previous fine-tuning step. Although the paper recommends an increased learning rate and a cosine scheduler, we found that our chosen combination of hyperparameters performed well on this dataset. However, we encourage you to experiment with your own hyperparameter settings to potentially achieve even better results.# hyperparameters, which are passed into the training job
medusa_hyperparameters = {
  ### SCRIPT PARAMETERS ###
  'train_dataset_path': '/opt/ml/input/data/train/train_dataset.json', # path where sagemaker will save training dataset
  'eval_dataset_path': '/opt/ml/input/data/eval/eval_dataset.json', # path where sagemaker will save evaluation dataset
  'model_path': '/opt/ml/input/data/fine-tuned-model/',
  'max_seq_len': 256,                               # max sequence length for model and packing of the dataset
  'medusa_num_heads': 5,
  ### TRAINING PARAMETERS ###
  'num_train_epochs': 3,                             # number of training epochs
  'per_device_train_batch_size': 1,                  # batch size per device during training
  'gradient_accumulation_steps': 16,                  # number of steps before performing a backward/update pass
  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory
  'optim': "adamw_8bit",                             # use fused adamw 8bit optimizer
  'logging_steps': 15,                               # log every 10 steps
  'save_strategy': "steps",                          # save checkpoint every epoch
  'save_steps': 15,
  'save_total_limit':2,
  'eval_strategy': "steps",
  'eval_steps': 15,
  'learning_rate': 1e-4,                             # learning rate
  'bf16': True,                                      # use bfloat16 precision
  'max_grad_norm': 10,                              # max gradient norm based on QLoRA paper
  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper
  'lr_scheduler_type': "constant",                   # use constant learning rate scheduler
  'output_dir': '/opt/ml/checkpoints/',              # Temporary output directory for model checkpoints
  'report_to': "tensorboard",                        # report metrics to tensorboard
  'logging_dir': "/opt/ml/output/tensorboard"        # tensorboard logging directory
}

medusa_train_job_name = f"medusa-text-to-sql-{get_current_time()}"
data = {
    'train': train_dataset_path,
    'eval': eval_dataset_path,
    'fine-tuned-model': fine_tuned_model_path
}

medusa_estimator = create_estimator(medusa_hyperparameters, medusa_train_job_name, role, sess, "train_medusa_heads.py")

medusa_estimator.fit(job_name=medusa_train_job_name, inputs=data, wait=False)We found that after 3 epochs, the evaluation loss of Medusa heads was converging, which can be observed in the TensorBoard graph in the following image.Besides the hyperparameters, the main difference is that we pass  as the training entrypoint, where we first add Medusa heads, then freeze the fine-tuned LLM, and we create custom MedusaSFTTrainer class, which is a subclass of the transformers SFTTrainer.# Add medusa heads and freeze base model
add_medusa_heads(
    model,
    medusa_num_heads=script_args.medusa_num_heads,
)
freeze_layers(model)
model.config.torch_dtype = torch_dtype
model.config.use_cache = False

logger.info("Finished loading model and medusa heads")

tokenizer = AutoTokenizer.from_pretrained(script_args.model_path, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

################
# Training
################
trainer = MedusaSFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    max_seq_length=script_args.max_seq_length,
    tokenizer=tokenizer,
    dataset_kwargs={
        "add_special_tokens": False,  # We template with special tokens
        "append_concat_token": False,  # No need to add additional separator token
    },
    medusa_num_heads=script_args.medusa_num_heads,
    medusa_heads_coefficient=script_args.medusa_heads_coefficient,
    medusa_decay_coefficient=script_args.medusa_decay_coefficient,
    medusa_scheduler=script_args.medusa_scheduler,
    train_only_medusa_heads=script_args.train_only_medusa_heads,
    medusa_lr_multiplier=script_args.medusa_lr_multiplier
)
trainer.train()In the  function, we add the residual blocks of the Medusa heads, and also override the forward pass for our model to make sure not to train the frozen backbone LLM:def add_medusa_heads(
    model,
    medusa_num_heads,
):
    """
    Args:
        model (nn.Module): The base language model to be used.
        medusa_num_heads (int, optional): Number of additional tokens to predict
    """
    hidden_size = model.lm_head.weight.shape[-1]
    vocab_size = model.lm_head.weight.shape[0]
    model.config.medusa_num_layers = 1
    model.config.medusa_num_heads = medusa_num_heads
    model.medusa_num_heads = medusa_num_heads
    # Create a list of Medusa heads
    model.medusa_heads = nn.ModuleList(
        [
            nn.Sequential(
                ResBlock(hidden_size),
                nn.Linear(hidden_size, vocab_size, bias=False),
            )
            for _ in range(medusa_num_heads)
        ]
    )

    # Ensure medusa_head's dtype and device align with the base_model
    model.medusa_heads.to(model.dtype).to(model.device)
    logger.info(f"Loading medusa heads in {str(model.dtype)} to device {model.device}")

    for i in range(medusa_num_heads):
        # Initialize the weights of each medusa_head using the base model's weights
        model.medusa_heads[i][-1].weight.data[:] = model.lm_head.weight.data[:]

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        train_only_medusa_heads: bool = False,
    ):
        """Forward pass of the MedusaModel.
        Returns:
            torch.Tensor: A tensor containing predictions from all Medusa heads.
            (Optional) Original predictions from the base model's LM head.
        """
        maybe_grad = torch.no_grad() if train_only_medusa_heads else nullcontext()
        with maybe_grad:
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
            )
            hidden_states = outputs[0]
            medusa_logits = [self.lm_head(hidden_states)]
        for i in range(self.medusa_num_heads):
            medusa_logits.append(self.medusa_heads[i](hidden_states))
        return torch.stack(medusa_logits, dim=0)

    model.forward = types.MethodType(forward, model)After the model training is finished (which takes 1 hour), we prepare the model artefacts for deployment and upload it to Amazon S3. Your final model artifact contains both the original fine-tuned model from the previous step under the  prefix and the trained Medusa heads in a file named .Deploy the fine-tuned LLM with Medusa heads on a SageMaker AI endpointFirst, we create a SageMaker AI HuggingFaceModel object and then deploy the model to an endpoint with the following function:import json
from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri


def deploy_model(endpoint_name, instance_type, model_s3_path=None, hf_model_id=None):
    llm_image = get_huggingface_llm_image_uri(
      "huggingface",
      version="2.2.0",
      session=sess,
    )

    print(f"llm image uri: {llm_image}")

    model_data = None
    if model_s3_path:
        model_data = {'S3DataSource': {'S3Uri': model_s3_path, 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}
        hf_model_id = "/opt/ml/model"
    else:
        assert hf_model_id, "You need to provide either pretrained HF model id, or S3 model data to deploy"
    config = {
      'HF_MODEL_ID': hf_model_id,  # path to where sagemaker stores the model
      'SM_NUM_GPUS': json.dumps(1),  # Number of GPU used per replica
      'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text
      'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)
    }

    llm_model = HuggingFaceModel(
      name=endpoint_name,
      role=role,
      image_uri=llm_image,
      model_data=model_data,
      env=config
    )

    deployed_llm = llm_model.deploy(
      endpoint_name=endpoint_name,
      initial_instance_count=1,
      instance_type=instance_type,
      container_startup_health_check_timeout=300,
    )
    return deployed_llmWe deploy three LLMs on three SageMaker AI endpoints:Base LLM which isn’t fine-tunedThe LLM that we fine-tunedThe fine-tuned LLM that also has trained Medusa headsYou can deploy the three models in parallel by using a function that we included in the notebook, or you can deploy the models one by one by running the code below:base_deployed_llm = deploy_model( f"base-{get_current_time()}", instance_type="ml.g5.4xlarge", model_s3_path=None, hf_model_id=model_id )
sft_deployed_llm = deploy_model( f"sft-{get_current_time()}", instance_type="ml.g5.4xlarge", model_s3_path=fine_tuned_model_path )
medusa_deployed_llm = deploy_model( f"medusa-{get_current_time()}", instance_type="ml.g5.4xlarge", model_s3_path=medusa_trained_model_path )
After the status for each endpoint becomes , which should take around 15 minutes, we can invoke them for inference. We send the following input:“You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA. SCHEMA: CREATE TABLE table_name_32 (time VARCHAR, heat VARCHAR, name VARCHAR)
What was Liu Limin's time in heat 4?“We can observe the following responses:The base LLM response contains extra words that aren’t needed:“To retrieve the time of Liu Limin in heat 4 based on the provided SCHEMA, we need to write a SQL query. Since the table name is not provided, let 's assume it's "my_table".  Assuming the table has a primary key (which is not mentioned in the SCHEMA), we can join the table with itself to compare the heat and name of Liu Limin in heat 4 with all the records in the table.  Here's the SQL query:  ``sql SELECT t1.time FROM my_table t1 JOIN my_table t2 ON t1.name = t2.name AND t2.heat = 4 WHERE t1.name = 'Liu Limin' AND t1.heat <> 4; `  Explanation:  1. We're selecting the time` column from the first table (t1) in the FROM clause. 2. We're joining the table with itself (my_table t1 JOIN my_table t2) to compare the name and heat of Liu Limin in heat 4 with all the records in the table. 3. We're filtering the results using the WHERE clause. We're selecting only the records where the name is 'Liu Limin' and the heat is not equal to 4 (i.e., not heat 4). This is to ensure that we're selecting the time of Liu Limin in heat 3.  Note: This query assumes that the table has a unique primary key. If the table doesn't have a primary key, you may need to add additional conditions to the JOIN and WHERE clauses to ensure that we're selecting the correct records.“The fine-tuned LLM response is improved significantly, and contains only the required output:'SELECT time FROM table_name_32 WHERE heat = 4 AND name = "liu limin"'The fine-tuned LLM with trained Medusa heads provides the exact same response as the fine-tuned model, demonstrating that Medusa-1, by design, maintains the output (quality) of the original model:'SELECT time FROM table_name_32 WHERE heat = 4 AND name = "liu limin"'Demonstrate LLM inference speedupTo measure the inference speed improvements, we compare the response times of the deployed fine-tuned LLM and the fine-tuned LLM with Medusa heads on 450 test observations with the following code:import time
import numpy as np
from tqdm import tqdm

def request(sample, deployed_llm):
    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)
    outputs = deployed_llm.predict({
      "inputs": prompt,
      "parameters": {
        "max_new_tokens": 512,
        "do_sample": False,
        "return_full_text": False,
      }
    })
    return {"role": "assistant", "content": outputs[0]["generated_text"].strip()}

def predict(deployed_llm, test_dataset):
    predicted_answers = []
    latencies = []

    for sample in tqdm(test_dataset):
        start_time = time.time()
        predicted_answer = request(sample["messages"][:2], deployed_llm)
        end_time = time.time()

        latency = end_time - start_time
        latencies.append(latency)
        predicted_answers.append(predicted_answer)

    # Calculate p90 and average latencies
    p90_latency = np.percentile(latencies, 90)
    avg_latency = np.mean(latencies)

    print(f"P90 Latency: {p90_latency:.2f} seconds")
    print(f"Average Latency: {avg_latency:.2f} seconds")

    return predicted_answersFirst, we run predictions using the fine-tuned LLM:sft_predictions = predict(sft_deployed_llm, test_dataset)
P90 Latency: 1.28 seconds
Average Latency: 0.95 secondsThen, we run predictions using the fine-tuned LLM with Medusa heads:medusa_predictions = predict(medusa_deployed_llm, test_dataset)
P90 Latency: 0.80 seconds
Average Latency: 0.53 secondsThe prediction runs should take around 8 and 4 minutes respectively. We can observe that the average latency decreased from 950 to 530 milliseconds, which is an improvement of 1.8 times. You can achieve even higher improvements if your dataset contains longer inputs and outputs. In our dataset, we only had an average of 18 input tokens and 30 output tokens.We want to once again highlight that, with this technique, the output quality is fully maintained, and all the prediction outputs are the same. The model responses for the test set of 450 observations are the same for both with Medusa heads and without Medusa heads:match_percentage = sum(a["content"] == b["content"] for a, b in zip(sft_predictions, medusa_predictions)) / len(sft_predictions) * 100
print(f"Predictions with the fine-tuned model with medusa heads are the same as without medusa heads: {match_percentage:.2f}% of test set ")

Predictions with fine-tuned model with medusa heads are the same as without medusa heads: 100.00% of test set You might notice in your run that a few observations aren’t exactly matching, and you might get a 99% match due to small errors in floating point operations caused by optimizations on GPUs.At the end of this experiment, don’t forget to delete the SageMaker AI endpoints you created:base_deployed_llm.delete_model()
base_deployed_llm.delete_endpoint()
sft_deployed_llm.delete_model()
sft_deployed_llm.delete_endpoint()
medusa_deployed_llm.delete_model()
medusa_deployed_llm.delete_endpoint()In this post, we demonstrated how to fine-tune and deploy an LLM with Medusa heads using the Medusa-1 technique on Amazon SageMaker AI to accelerate LLM inference. By using this framework and SageMaker AI scalable infrastructure, we showed how to achieve up to twofold speedups in LLM inference while maintaining model quality. This solution is particularly beneficial for applications requiring low-latency text generation, such as customer service chat assistants, content creation, and recommendation systems.As a next step, you can explore fine-tuning your own LLM with Medusa heads on your own dataset and benchmark the results for your specific use case, using the provided GitHub repository. is a Senior ML Engineer at AWS Professional Services. He specializes in developing scalable, production-grade machine learning solutions for AWS customers. His experience extends across different areas, including natural language processing, generative AI and machine learning operations. is a Senior Data Scientist at AWS Professional Services. She enjoys supporting customers to build innovative AI/ML solutions on AWS and she is excited about business transformations through the power of data. is a Senior ML Manager at Booking.com. She is leading the content intelligence track which is focused on building, training and deploying content models (computer vision, NLP and generative AI) using the most advanced technologies and models. Moran is also a PhD candidate, researching applying NLP models on social graphs. is a Senior ML Scientist at Booking.com. He specializes in generative NLP and has experience researching, implementing and deploying large deep learning models at scale. is a Senior Machine Learning Engineer at Booking.com. He leads the development of the several LLM systems inside Booking.com. His work focuses on building production ML systems that help millions of travelers plan their trips effectively. is a Machine Learning Engineer at AWS Professional Services. He works closely with customers building their machine learning solutions on AWS, specializes in natural language processing, experimentation and responsible AI, and is passionate about using machine learning to drive meaningful change in the world.]]></content:encoded></item><item><title>LLM-as-a-judge on Amazon Bedrock Model Evaluation</title><link>https://aws.amazon.com/blogs/machine-learning/llm-as-a-judge-on-amazon-bedrock-model-evaluation/</link><author>Adewale Akinfaderin</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 17:36:57 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[The evaluation of large language model (LLM) performance, particularly in response to a variety of prompts, is crucial for organizations aiming to harness the full potential of this rapidly evolving technology. The introduction of an  framework represents a significant step forward in simplifying and streamlining the model evaluation process. This approach allows organizations to assess their AI models’ effectiveness using pre-defined metrics, making sure that the technology aligns with their specific needs and objectives. By adopting this method, companies can more accurately gauge the performance of their AI systems, making informed decisions about model selection, optimization, and deployment. This not only enhances the reliability and efficiency of AI applications, but also contributes to a more strategic and informed approach to technology adoption within the organization.Amazon Bedrock, a fully managed service offering high-performing foundation models from leading AI companies through a single API, has recently introduced two significant evaluation capabilities: LLM-as-a-judge under Amazon Bedrock Model Evaluation and RAG evaluation for Amazon Bedrock Knowledge Bases. Both features use the LLM-as-a-judge technique behind the scenes but evaluate different things. This blog post explores LLM-as-a-judge on Amazon Bedrock Model Evaluation, providing comprehensive guidance on feature setup, evaluating job initiation through both the console and Python SDK and APIs, and demonstrating how this innovative evaluation feature can enhance generative AI applications across multiple metric categories including quality, user experience, instruction following, and safety.Before we explore the technical aspects and implementation details, let’s examine the key features that make LLM-as-a-judge on Amazon Bedrock Model Evaluation particularly powerful and distinguish it from traditional evaluation methods. Understanding these core capabilities will help illuminate why this feature represents a significant advancement in AI model evaluation.Key features of LLM-as-a-judgeAutomated intelligent evaluation: LLM-as-a-judge uses pre-trained models to evaluate responses automatically, providing human-like evaluation quality with up to 98% cost savings. The system dramatically reduces evaluation time from weeks to hours while maintaining consistent evaluation standards across large datasets.Comprehensive metric categories: The evaluation system covers four key metric areas: quality assessment (correctness, completeness, faithfulness), user experience (helpfulness, coherence, relevance), instruction compliance (following instructions, professional style), and safety monitoring (harmfulness, stereotyping, refusal handling).: The feature integrates directly with Amazon Bedrock and remains compatible with existing Amazon Bedrock Model Evaluation features. Users can access the functionality through the AWS Management Console for Amazon Bedrock and quickly integrate their custom datasets for evaluation purposes.: The system supports the evaluation of models hosted on Amazon Bedrock, custom fine-tuned models, and imported models. Users can seamlessly connect their evaluation datasets through Amazon Simple Storage Service (Amazon S3) buckets, making the evaluation process streamlined and efficient.: Amazon Bedrock provides pre-selected, high-quality evaluation models with optimized prompt engineering for accurate assessments. Users don’t need to bring external judge models, because the Amazon Bedrock team maintains and updates a selection of judge models and associated evaluation judge prompts.: The feature enables organizations to perform comprehensive model evaluations at scale without the traditional costs and time investments associated with human evaluation. The automated process maintains high-quality assessments while significantly reducing operational overhead.These features create a powerful evaluation framework that helps organizations optimize their AI model performance while maintaining high standards of quality and safety, all within their secure AWS environment.Now that you understand the key features of LLM-as-a-judge, let’s examine how to implement and use this capability within Amazon Bedrock Model Evaluation. This section provides a comprehensive overview of the architecture and walks through each component, demonstrating how they work together to deliver accurate and efficient model evaluations.LLM-as-a-judge on Amazon Bedrock Model Evaluation provides a comprehensive, end-to-end solution for assessing and optimizing AI model performance. This automated process uses the power of LLMs to evaluate responses across multiple metric categories, offering insights that can significantly improve your AI applications. Let’s walk through the key components of this solution as shown in the following diagram:LLM-as-a-judge on Amazon Bedrock Model Evaluation follows a streamlined workflow that enables systematic model evaluation. Here’s how each component works together in the evaluation process:: The process begins with a prepared dataset containing prompts that will be used to test the model’s performance. The evaluation can be conducted with or without ground truth responses—while including ground truth provides additional comparison points, it’s entirely optional and not required for successful evaluation.: The prompt dataset is converted into JSONL format, which is specifically structured for LLM-as-a-judge evaluation jobs. This format promotes proper processing of evaluation data.: The prepared JSONL file is uploaded to an S3 bucket, serving as the secure storage location for the evaluation data.: The Amazon Bedrock LLM-as-a-judge model evaluation job processes the stored data, running comprehensive assessments across the selected metric categories (including quality, user experience, instruction following, and safety).Automated report generation: Upon completion, the system generates detailed evaluation reports containing metrics, scores, and insights at both aggregate and individual response levels.: Data scientists or machine learning engineers analyze the generated reports to derive actionable insights and make informed decisions.With this solution architecture in mind, let’s explore how to implement LLM-as-a-judge model evaluations effectively, making sure that you get the most valuable insights from your assessment process.To use the LLM-as-a-judge model evaluation, make sure that you have satisfied the following requirements:Selected  and  models enabled in Amazon Bedrock. You can confirm that the models are enabled for your account on the  page of the Amazon Bedrock console.If you’re using a custom model instead of an on-demand model for your generator model, make sure that you have sufficient quota for running a Provisioned Throughput during inference. 
  Go to the AWS Service Quotas console, and check the following quotas: 
    Model units no-commitment Provisioned Throughputs across custom models.Model units per provisioned model for [your custom model name].Both of these fields need to have enough quota to support your Provisioned Throughput model unit. Request a quota increase if necessary to accommodate your expected inference workload.When preparing your dataset for LLM-as-a-judge model evaluation jobs, each prompt must include specific key-value pairs. Here are the required and optional fields:: This key indicates the input for various tasks. It can be used for general text generation where the model needs to provide a response, question-answering tasks where the model must answer a specific question, text summarization tasks where the model needs to summarize a given text, or classification tasks where the model must categorize the provided text.referenceResponse (used for specific metrics with ground truth): This key contains the ground truth or correct response. It serves as the reference point against which the model’s responses will be evaluated if it is provided.: This key is used to generate evaluation scores reported by category, helping organize and segment evaluation results for better analysis.Each line must be a valid JSON objectThe file must use JSONL formatThe dataset should be stored in an Amazon S3 bucketExample JSONL format without ground truth ( is optional):{
    "prompt": "What is machine learning?"
    "category": "technical"
}
{
    "prompt": "Summarize climate change impacts",
    "category": "environmental"
}
Example JSONL format with ground truth ( is optional):{
    "prompt": "What is machine learning?",
    "referenceResponse": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses algorithms and statistical models to analyze and draw inferences from patterns in data, allowing computers to perform specific tasks without explicit instructions.",
    "category": "technical"
}
{
    "prompt": "Summarize climate change impacts",
    "referenceResponse": "Climate change leads to rising global temperatures, extreme weather events, sea level rise, and disruption of ecosystems. These changes result in more frequent natural disasters, threats to food security, loss of biodiversity, and various public health challenges. The impacts affect agriculture, coastal communities, and vulnerable populations disproportionately.",
    "category": "environmental"
}Start an LLM-as-a-judge model evaluation job using the consoleYou can use LLM-as-a-judge on Amazon Bedrock Model Evaluation to assess model performance through a user-friendly console interface. Follow these steps to start an evaluation job:In the Amazon Bedrock console, choose and then select . On the page, choose the Choose  and select Automatic: LLM-as-a-judge.Enter a name and description and select an . This model will be used as a judge to evaluate the response of a prompt or model from your generative AI application.Choose  and select the model to be used for generating responses in this evaluation job.Select the metrics you want to use to evaluate the model response (such as helpfulness, correctness, faithfulness, relevance, and harmfulness).Select the  for  and for . You can use theoption.Select or create an IAM service role with the proper permissions. This includes service access to Amazon Bedrock, the S3 buckets in the evaluation job, and the models being used in the job. If you create a new IAM role in the evaluation setup, the service will automatically give the role the proper permissions for the job. Specify the output S3 bucket and choose .You will be able to see the evaluation job is . Wait for the job status to change to .When complete, select the job to see its details. The following is the metrics summary (such as 0.83 for helpfulness, 1.00 for correctness, 1.00 for faithfulness, 1.00 for relevance, and 0.00 for harmfulness).To view generation metrics details, scroll down in the model evaluation report and choose any individual metric (like helpfulness or correctness) to see its detailed breakdown.To see each record’s prompt input, generation output, ground truth, and individual scores, choose a metric and select “Prompt details”. Hover over any individual score to view its detailed explanation.Start an LLM-as-a-judge evaluation job using Python SDK and APIsTo use the Python SDK for creating an LLM-as-a-judge model evaluation job, use the following steps. First, set up the required configurations:import boto3
from datetime import datetime

# Generate unique name for the job
job_name = f"Model-evaluation-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}"

# Configure your knowledge base and model settings
evaluator_model = "mistral.mistral-large-2402-v1:0"
generator_model = "amazon.nova-pro-v1:0"
role_arn = "arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>"

# Specify S3 locations for evaluation data and output
input_data = "s3://<YOUR_BUCKET>/evaluation_data/input.jsonl"
output_path = "s3://<YOUR_BUCKET>/evaluation_output/"

# Create Bedrock client
bedrock_client = boto3.client('bedrock')To create an LLM-as-a-judge model evaluation job:def create_llm_judge_evaluation(
    client,
    job_name: str,
    role_arn: str,
    input_s3_uri: str,
    output_s3_uri: str,
    evaluator_model_id: str,
    generator_model_id: str,
    dataset_name: str = None,
    task_type: str = "General" # must be General for LLMaaJ
):    
    # All available LLM-as-judge metrics
    llm_judge_metrics = [
        "Builtin.Correctness",
        "Builtin.Completeness", 
        "Builtin.Faithfulness",
        "Builtin.Helpfulness",
        "Builtin.Coherence",
        "Builtin.Relevance",
        "Builtin.FollowingInstructions",
        "Builtin.ProfessionalStyleAndTone",
        "Builtin.Harmfulness",
        "Builtin.Stereotyping",
        "Builtin.Refusal"
    ]

    # Configure dataset
    dataset_config = {
        "name": dataset_name or "CustomDataset",
        "datasetLocation": {
            "s3Uri": input_s3_uri
        }
    }

    try:
        response = client.create_evaluation_job(
            jobName=job_name,
            roleArn=role_arn,
            applicationType="ModelEvaluation",
            evaluationConfig={
                "automated": {
                    "datasetMetricConfigs": [
                        {
                            "taskType": task_type,
                            "dataset": dataset_config,
                            "metricNames": llm_judge_metrics
                        }
                    ],
                    "evaluatorModelConfig": {
                        "bedrockEvaluatorModels": [
                            {
                                "modelIdentifier": evaluator_model_id
                            }
                        ]
                    }
                }
            },
            inferenceConfig={
                "models": [
                    {
                        "bedrockModel": {
                            "modelIdentifier": generator_model_id
                        }
                    }
                ]
            },
            outputDataConfig={
                "s3Uri": output_s3_uri
            }
        )
        return response
        
    except Exception as e:
        print(f"Error creating evaluation job: {str(e)}")
        raise
        
 # Create evaluation job
try:
    llm_as_judge_response = create_llm_judge_evaluation(
        client=bedrock_client,
        job_name=job_name,
        role_arn=ROLE_ARN,
        input_s3_uri=input_data,
        output_s3_uri=output_path,
        evaluator_model_id=evaluator_model,
        generator_model_id=generator_model,
        task_type="General"
    )
    print(f"✓ Created evaluation job: {llm_as_judge_response['jobArn']}")
except Exception as e:
    print(f"✗ Failed to create evaluation job: {str(e)}")
    raise
To monitor the progress of your evaluation job:# Get job ARN based on job type
evaluation_job_arn = llm_as_judge_response['jobArn']
# Check job status
check_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn) 
print(f"Job Status: {check_status['status']}")You can also compare multiple foundation models to determine which one works best for your needs. By using the same evaluator model across all comparisons, you’ll get consistent benchmarking results to help identify the optimal model for your use case.# Generator Models
GENERATOR_MODELS = [
    "anthropic.claude-3-haiku-20240307-v1:0",
    "amazon.nova-micro-v1:0"
]

# Consistent Evaluator
EVALUATOR_MODEL = "anthropic.claude-3-haiku-20240307-v1:0"

def run_model_comparison(
    generator_models: List[str],
    evaluator_model: str
) -> List[Dict[str, Any]]:
    evaluation_jobs = []
    
    for generator_model in generator_models:
        job_name = f"llmaaj-{generator_model.split('.')[0]}-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}"
        
        try:
            response = create_llm_judge_evaluation(
                client=bedrock_client,
                job_name=job_name,
                role_arn=ROLE_ARN,
                input_s3_uri=input_data,
                output_s3_uri=f"{output_path}/{job_name}/",
                evaluator_model_id=evaluator_model,
                generator_model_id=generator_model,
                task_type="General"
            )
            
            job_info = {
                "job_name": job_name,
                "job_arn": response["jobArn"],
                "generator_model": generator_model,
                "evaluator_model": evaluator_model,
                "status": "CREATED"
            }
            evaluation_jobs.append(job_info)
            
            print(f"✓ Created job: {job_name}")
            print(f"  Generator: {generator_model}")
            print(f"  Evaluator: {evaluator_model}")
            print("-" * 80)
            
        except Exception as e:
            print(f"✗ Error with {generator_model}: {str(e)}")
            continue
            
    return evaluation_jobs

# Run model comparison
evaluation_jobs = run_model_comparison(GENERATOR_MODELS, EVALUATOR_MODEL)Correlation analysis for LLM-as-a-judge evaluationsYou can use the Spearman’s rank correlation coefficient to compare evaluation results between different generator models using LLM-as-a-judge in Amazon Bedrock. After retrieving the evaluation results from your S3 bucket, containing evaluation scores across various metrics, you can begin the correlation analysis.Using , compute the correlation coefficient between pairs of generator models, filtering out constant values or error messages to have a valid statistical comparison. The resulting correlation coefficients help identify how similarly different models respond to the same prompts. A coefficient closer to 1.0 indicates stronger agreement between the models’ responses, while values closer to 0 suggest more divergent behavior. This analysis provides valuable insights into model consistency and helps identify cases where different models might produce significantly different outputs for the same input.import json
import boto3
import numpy as np
from scipy import stats

def read_and_organize_metrics_from_s3(bucket_name, file_key):
    s3_client = boto3.client('s3')
    metrics_dict = {}
    
    try:
        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)
        content = response['Body'].read().decode('utf-8')
        
        for line in content.strip().split('\n'):
            if line:
                data = json.loads(line)
                if 'automatedEvaluationResult' in data and 'scores' in data['automatedEvaluationResult']:
                    for score in data['automatedEvaluationResult']['scores']:
                        metric_name = score['metricName']
                        if 'result' in score:
                            metric_value = score['result']
                            if metric_name not in metrics_dict:
                                metrics_dict[metric_name] = []
                            metrics_dict[metric_name].append(metric_value)
        return metrics_dict
    
    except Exception as e:
        print(f"Error: {e}")
        return None

def get_spearmanr_correlation(scores1, scores2):
    if len(set(scores1)) == 1 or len(set(scores2)) == 1:
        return "undefined (constant scores)", "undefined"
    
    try:
        result = stats.spearmanr(scores1, scores2)
        return round(float(result.statistic), 4), round(float(result.pvalue), 4)
    except Exception as e:
        return f"error: {str(e)}", "undefined"

# Extract metrics
bucket_name = "<EVALUATION_OUTPUT_BUCKET>"
file_key1 = "<EVALUATION_FILE_KEY1>"
file_key2 = "<EVALUATION_FILE_KEY2>"

metrics1 = read_and_organize_metrics_from_s3(bucket_name, file_key1)
metrics2 = read_and_organize_metrics_from_s3(bucket_name, file_key2)

# Calculate correlations for common metrics
common_metrics = set(metrics1.keys()) & set(metrics2.keys())

for metric_name in common_metrics:
    scores1 = metrics1[metric_name]
    scores2 = metrics2[metric_name]
    
    if len(scores1) == len(scores2):
        correlation, p_value = get_spearmanr_correlation(scores1, scores2)
        
        print(f"\nMetric: {metric_name}")
        print(f"Number of samples: {len(scores1)}")
        print(f"Unique values in Model 1 scores: {len(set(scores1))}")
        print(f"Unique values in Model 2 scores: {len(set(scores2))}")
        print(f"Model 1 scores range: [{min(scores1)}, {max(scores1)}]")
        print(f"Model 2 scores range: [{min(scores2)}, {max(scores2)}]")
        print(f"Spearman correlation coefficient: {correlation}")
        print(f"P-value: {p_value}")
    else:
        print(f"\nMetric: {metric_name}")
        print("Error: Different number of samples between models")Best practices for LLM-as-a-judge implementationYou can also compare multiple foundation models to determine which one works best for your needs. By using the same evaluator model across all comparisons, you’ll get consistent, scalable results. The following best practices will help you establish standardized benchmarking when comparing different foundation models.Create diverse test datasets that represent real-world use cases and edge cases. For large workloads (more than 1,000 prompts), use stratified sampling to maintain comprehensive coverage while managing costs and completion time. Include both simple and complex prompts to test model capabilities across different difficulty levels.Choose evaluation metrics that align with your specific business objectives and application requirements. Balance quality metrics (correctness, completeness) with user experience metrics (helpfulness, coherence). Include safety metrics when deploying customer-facing applications.Maintain consistent evaluation conditions when comparing different models. Use the same evaluator model across comparisons for standardized benchmarking. Document your evaluation configuration and parameters for reproducibility.Schedule regular evaluation jobs to track model performance over time. Monitor trends across different metric categories to identify areas for improvement. Set up performance baselines and thresholds for each metric.Optimize batch sizes based on your evaluation needs and cost constraints. Consider using smaller test sets for rapid iteration and larger sets for comprehensive evaluation. Balance evaluation frequency with resource utilization.Maintain detailed records of evaluation jobs, including configurations and results. Track improvements and changes in model performance over time. Document any modifications made based on evaluation insights. The optional job description field can help you here.Use evaluation results to guide model selection and optimization. Implement feedback loops to continuously improve prompt engineering. Regularly update evaluation criteria based on emerging requirements and user feedback.Design your evaluation framework to accommodate growing workloads. Plan for increased complexity as you add more models or use cases. Consider automated workflows for regular evaluation tasks.These best practices help establish a robust evaluation framework using LLM-as-a-judge on Amazon Bedrock. For deeper insights into the scientific validation of these practices, including case studies and correlation with human judgments, stay tuned for our upcoming technical deep-dive blog post.LLM-as-a-judge on Amazon Bedrock Model Evaluation represents a significant advancement in automated model assessment, offering organizations a powerful tool to evaluate and optimize their AI applications systematically. This feature combines the efficiency of automated evaluation with the nuanced understanding typically associated with human assessment, enabling organizations to scale their quality assurance processes while maintaining high standards of performance and safety.The comprehensive metric categories, flexible implementation options, and seamless integration with existing AWS services make it possible for organizations to establish robust evaluation frameworks that grow with their needs. Whether you’re developing conversational AI applications, content generation systems, or specialized enterprise solutions, LLM-as-a-judge provides the necessary tools to make sure that your models align with both technical requirements and business objectives.We’ve provided detailed implementation guidance, from initial setup to best practices, to help you use this feature effectively. The accompanying code samples and configuration examples in this post demonstrate how to implement these evaluations in practice. Through systematic evaluation and continuous improvement, organizations can build more reliable, accurate, and trustworthy AI applications.We encourage you to explore LLM-as-a-judge capabilities in the Amazon Bedrock console and discover how automatic evaluation can enhance your AI applications. To help you get started, we’ve prepared a Jupyter notebook with practical examples and code snippets that you can find on our GitHub repository. is a Sr. Data Scientist–Generative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering. is a Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building Generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau. is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business.]]></content:encoded></item><item><title>From concept to reality: Navigating the Journey of RAG from proof of concept to production</title><link>https://aws.amazon.com/blogs/machine-learning/from-concept-to-reality-navigating-the-journey-of-rag-from-proof-of-concept-to-production/</link><author>Vivek Mittal</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 17:27:52 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Generative AI has emerged as a transformative force, captivating industries with its potential to create, innovate, and solve complex problems. However, the journey from a proof of concept to a production-ready application comes with challenges and opportunities. Moving from proof of concept to production is about creating scalable, reliable, and impactful solutions that can drive business value and user satisfaction.One of the most promising developments in this space is the rise of Retrieval Augmented Generation (RAG) applications. RAG is the process of optimizing the output of a foundation model (FM), so it references a knowledge base outside of its training data sources before generating a response.The following diagram illustrates a sample architecture.In this post, we explore the movement of RAG applications from their proof of concept or minimal viable product (MVP) phase to full-fledged production systems. When transitioning a RAG application from a proof of concept to a production-ready system, optimization becomes crucial to make sure the solution is reliable, cost-effective, and high-performing. Let’s explore these optimization techniques in greater depth, setting the stage for future discussions on hosting, scaling, security, and observability considerations.The diagram below illustrates the tradeoffs to consider for a production-ready RAG application.The success of a production-ready RAG system is measured by its quality, cost, and latency. Machine learning (ML) engineers must make trade-offs and prioritize the most important factors for their specific use case and business requirements. For example, consider the use case of generating personalized marketing content for a luxury fashion brand. The brand might be willing to absorb the higher costs of using a more powerful and expensive FMs to achieve the highest-quality classifications, because misclassifications could lead to customer dissatisfaction and damage the brand’s reputation. Consider another use case of generating personalized product descriptions for an ecommerce site. The retailer might be willing to accept slightly longer latency to reduce infrastructure and operational costs, as long as the generated descriptions remain reasonably accurate and compelling. The optimal balance of quality, cost, and latency can vary significantly across different applications and industries.Let’s look into practical guidelines on how you can enhance the overall quality of your RAG workflow, including the quality of the retriever and quality of the result generator using Amazon Bedrock Knowledge Bases and other features of Amazon Bedrock. Amazon Bedrock Knowledge Bases provides a fully managed capability that helps you implement the entire RAG workflow from ingestion to retrieval and prompt augmentation without having to build custom integrations to data sources and manage data flows.An effective evaluation framework is crucial for assessing and optimizing RAG systems as they move from proof of concept to production. These frameworks typically include overall metrics for a holistic assessment of the entire RAG pipeline, as well as specific diagnostic metrics for both the retrieval and generation components. This allows for targeted improvements in each phase of the system. By implementing a robust evaluation framework, developers can continuously monitor, diagnose, and enhance their RAG systems, achieving optimal performance across quality, cost, and latency dimensions as the application scales to production levels. Amazon Bedrock Evaluations can help you evaluate your retrieval or end-to-end RAG workflow in Amazon Bedrock Knowledge Bases. In the following sections, we discuss these specific metrics in different phases of the RAG workflow in more detail.For better retrieval performance, the way the data is stored in the vector store has a big impact. For example, your input document might include tables within the PDF. In such cases, using an FM to parse the data will provide better results. You can use advanced parsing options supported by Amazon Bedrock Knowledge Bases for parsing non-textual information from documents using FMs. Many organizations store their data in structured formats within data warehouses and data lakes. Amazon Bedrock Knowledge Bases offers a feature that lets you connect your RAG workflow to structured data stores. This fully managed out-of-the-box RAG solution can help you natively query structured data from where it resides.Another important consideration is the way your source document is split up into chunks. If your document would benefit from inherent relationships within your document, it might be wise to use hierarchical chunking, which allows for more granular and efficient retrieval. Some documents benefit from semantic chunking by preserving the contextual relationship in the chunks, helping make sure that the related information stays together in logical chunks. You can also use your own custom chunking strategy for your RAG application’s unique requirements.RAG applications process user queries by searching across a large set of documents. However, in many situations, you might need to retrieve documents with specific attributes or content. You can use metadata filtering to narrow down search results by specifying inclusion and exclusion criteria. Amazon Bedrock Knowledge Bases now also supports auto generated query filters, which extend the existing capability of manual metadata filtering by allowing you to narrow down search results without the need to manually construct complex filter expressions. This improves retrieval accuracy by making sure the documents are relevant to the query.Writing an effective query is just as important as any other consideration for generation accuracy. You can add a prompt providing instructions to the FM to provide an appropriate answer to the user. For example, a legal tech company would want to provide instructions to restrict the answers to be based on the input documents and not based on general information known to the FM. Query decomposition by splitting the input query into multiple queries is also helpful in retrieval accuracy. In this process, the subqueries with less semantic complexity might find more targeted chunks. These chunks can then be pooled and ranked together before passing them to the FM to generate a response.Reranking, as a post-retrieval step, can significantly improve response quality. This technique uses LLMs to analyze the semantic relevance between the query and retrieved documents, reordering them based on their pertinence. By incorporating reranking, you make sure that only the most contextually relevant information is used for generation, leading to more accurate and coherent responses.Adjusting inference parameters, such as temperature and top-k/p sampling, can help in further refining the output.You can use Amazon Bedrock Knowledge Bases to configure and customize queries and response generation. You can also improve the relevance of your query responses with a reranker model in Amazon Bedrock.The key metrics for retriever quality are context precision, context recall, and context relevance. Context precision measures how well the system ranks relevant pieces of information from the given context. It considers the question, ground truth, and context. Context recall provides the percentage of ground truth claims or key information covered by the retrieved context. Context relevance measures whether the retrieved passages or chunks are relevant for answering the given query, excluding extraneous details. Together, these three metrics offer insight into how effectively the retriever is able to surface the most relevant and focused source material to support a high-quality response.Generator quality can be assessed through several key metrics. Context utilization examines how effectively the generator uses relevant information from the provided source material. Noise sensitivity gauges the generator’s propensity to include inaccurate details from the retrieved content. Hallucination measures the extent to which the generator produces incorrect claims not present in the source data. Self-knowledge reflects the proportion of accurate statements generated that can’t be found in the retrieved chunks. Finally, faithfulness evaluates how closely the generator’s output aligns with the information contained in the source material.For measuring the overall generation quality, the key metrics include measuring the precision, recall, and answer similarity. Precision suggests the proportion of the correct claims in model’s response, whereas recall suggests the proportion of the ground truth claims covered by the model’s response. Answer similarity compares the meaning and content of a generated answer with a reference or ground truth answer. It evaluates how closely the generated answer matches the intended meaning of the ground truth answer.Establishing a feedback loop with an evaluation framework against these quality metrics allows for continuous improvement, where the system can learn from user interactions and refine its performance over time. By optimizing these quality metrics, the RAG system can be designed to deliver reliable, cost-effective, and high-performing results for users.Implementing responsible AI practices is crucial for maintaining ethical and safe deployment of RAG systems. This includes using guardrails to filter harmful content, deny certain topics, mask sensitive information, and ground responses in verified sources to reduce hallucinations.Cost considers the compute resources and infrastructure required to run the system, and latency evaluates the response times experienced by end-users. To optimize cost and latency, implement caching strategies to reduce the need for expensive model inferences. Efficient query batching can also improve overall throughput and reduce resource usage. Balance performance and resource usage to find the ideal configuration that meets your application’s requirements.Use tools like Amazon Bedrock Knowledge Bases so you can take advantage of fully managed support for the end-to-end RAG workflow. It supports many of the advanced RAG capabilities we discussed earlier. By addressing these optimization techniques, you can transition your RAG-powered proof of concept to a robust, production-ready system that delivers high-quality, cost-effective, and low-latency responses to your users.In addition to the server or compute layer, you will also need to consider an orchestration tool, testing environments, and a continuous integration and delivery (CI/CD) pipeline to streamline your application deployment. Having a feedback loop established based on the quality metrics along with a CI/CD pipeline is an important first step to creating self-healing architectures.As your application grows, you will need to make sure your infrastructure can scale to meet the increasing demand. This can involve containerization with Docker or choosing serverless options, implementing load balancing, setting up auto scaling, and choosing between on-premises, cloud, or hybrid solutions. It also includes unique scaling requirements of your frontend application and backend generative AI workflow, as well as the use of content delivery networks (CDNs) and disaster recovery and backup strategies.The following is a sample architecture for a secure and scalable RAG-based web application. This architecture uses Amazon ECS for hosting the service, Amazon CloudFront as a CDN, AWS WAF as a firewall, and Amazon MemoryDB for providing a semantic cache.By carefully considering these aspects of hosting and scaling your infrastructure, you can build a resilient and adaptable system to support your growing web application or service. Stay tuned for more detailed information on these topics in upcoming blog posts.Data privacy, security, and observabilityMaintaining data privacy and security is of utmost importance. This includes implementing security measures at each layer of your application, from encrypting data in transit to setting up robust authentication and authorization controls. It also involves focusing on compute and storage security, as well as network security. Compliance with relevant regulations and regular security audits are essential. Securing your generative AI system is another crucial aspect. By default, Amazon Bedrock Knowledge Bases encrypts the traffic using AWS managed AWS Key Management Service (AWS KMS) keys. You can also choose customer managed KMS keys for more control over encryption keys. For more information on application security, refer to Safeguard a generative AI travel agent with prompt engineering and Amazon Bedrock Guardrails.Comprehensive logging, monitoring, and maintenance are crucial to maintaining a healthy infrastructure. This includes setting up structured logging, centralized log management, real-time monitoring, and strategies for system updates and migrations.By addressing these critical areas, you can build a secure and resilient infrastructure to support your growing web application or service. Stay tuned for more in-depth coverage of these topics in upcoming blog posts.To successfully transition a RAG application from a proof of concept to a production-ready system, you should focus on optimizing the solution for reliability, cost-effectiveness, and high performance. Key areas to address include enhancing retriever and generator quality, balancing cost and latency, and establishing a robust and secure infrastructure.By using purpose-built tools like Amazon Bedrock Knowledge Bases to streamline the end-to-end RAG workflow, organizations can successfully transition their RAG-powered proofs of concept into high-performing, cost-effective, secure production-ready solutions that deliver business value. is a Solution Architect at Amazon Web Services, where he helps organizations architect and implement cutting-edge cloud solutions. With a deep passion for Generative AI, Machine Learning, and Serverless technologies, he specializes in helping customers harness these innovations to drive business transformation. He finds particular satisfaction in collaborating with customers to turn their ambitious technological visions into reality. is a Sr. Enterprise Solutions Architect at AWS, experienced in Software Engineering, Enterprise Architecture, and AI/ML. He is deeply passionate about exploring the possibilities of generative AI. He collaborates with customers to help them build well-architected applications on the AWS platform, and is dedicated to solving technology challenges and assisting with their cloud journey. is a Tech Lead – Generative AI Specialists, author of the book Applied Machine Learning and High-Performance Computing on AWS, and a member of the Board of Directors for Women in Manufacturing Education Foundation Board. She leads machine learning projects in various domains such as computer vision, natural language processing, and generative AI. She speaks at internal and external conferences such AWS re:Invent, Women in Manufacturing West, YouTube webinars, and GHC 23. In her free time, she likes to go for long runs along the beach.]]></content:encoded></item><item><title>Show HN: Game Bub – open-source FPGA retro emulation handheld</title><link>https://eli.lipsitz.net/posts/introducing-gamebub/</link><author>elipsitz</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Feb 2025 17:11:25 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I’m excited to announce the project I’ve been working on for the last year and a half: , an open-source FPGA based retro emulation handheld, with support for Game Boy, Game Boy Color, and Game Boy Advance games.Game Bub can play physical cartridges, as well as emulated cartridges using ROM files loaded from a microSD card. Game Bub also supports the Game Link Cable in both GB and GBA modes for multiplayer games. I designed the hardware with a number of bonus features, like video out (HDMI) via a custom dock, a rumble motor, real-time clock (for certain games). Additionally, the hardware is designed with extensibility in mind, allowing future software improvements to expand its capabilities.Game Bub has a custom-designed 6 layer PCB featuring a Xilinx XC7A100T FPGA with integrated memory,  display, speakers, rechargable battery, GB/GBA cartridge slot, all packaged up in a custom 3D-printed enclosure.Check out the instructions, code, and design files on GitHub. Note that building a Game Bub unit is fairly complex. If you might be interested in buying a complete Game Bub kit, please fill out this form to help me gauge interest.I had a lot of fun implementing a Game Boy at the hardware level, and I started thinking about how far I could take the project. I was using a Pynq-Z2 development board, which was definitely the right way to get started, but it came with a lot of limitations.I had to use an external monitor for audio/video, and an external gamepad for input, but a real Game Boy, of course, is a portable handheld. I also wanted to add Game Boy Advance support, but the memory architecture of the Pynq-Z2 had access latency that was just barely acceptable for the Game Boy, and would have been completely unacceptable for the Game Boy Advance. I also wanted to make something less “hacky”: a real device that I could play and give to people, not just a bare PCB.Furthermore, while there are open-source FPGA retrogaming projects (e.g. MiSTer), there doesn’t appear to be anything open-source that supports physical Game Boy and Game Boy Advance cartridges, let alone an open-source handheld device.Thus, I somewhat naively set out to design what would become by far my most complex electrical engineering and hardware design project to date.I set out some goals for the project:Build a standalone, rechargable battery-powered FPGA handheldMinimize cost and complexity by using off-the-shelf components wherever possibleCapable of playing Game Boy, Game Boy Color, and Game Boy Advance gamesCapable of using physical cartridges, or emulating cartridges (reading ROM files off of a microSD card)Easy to use: graphical menu and in-game overlayIntegrated display and speakers, with headphone supportIntegrated peripherals (rumble, real-time clock, accelerometer) for emulated cartridgesHDMI video output support for playing on a big screenDecent looking design with good ergonomicsExpansion opportunities in the future: support for more systems, Wi-Fi, etc.And finally, since I was building this project for fun and learning, I wanted to be able to fully understand every single component of the system. I wanted to use my own emulator cores (e.g. not just port them from MiSTer), do my own board design, and write my own drivers to interface with peripherals.A brief rant about FPGA retrogaming#There’s a lot of misleading marketing and hype out there around FPGA retrogaming. Some claim that FPGA retrogaming devices are not emulators (because they supposedly “act like [the system] at the gate level”), that they achieve “perfect accuracy”, or that they’re superior to software emulators.In my opinion, this is blatantly wrong and actively harmful. FPGA retrogaming devices are emulators: they pretend to be something they’re not. And they’re only as accurate as they’re programmed to be, since they’re recreations. An FPGA can make certain aspects of accuracy easier to achieve, but it doesn’t guarantee it.Software emulators can be extremely accurate. Furthermore, perfect accuracy (if it’s even possible) is by no means a requirement to play an entire system’s library of games. Some people claim that FPGA emulators are the only way to “preserve” a system, but I’d argue that software emulators are a significantly more accessible (no special hardware needed!) way to further this goal.I believe that FPGA emulators have only one real advantage over software emulators: they can more easily interface with original hardware, such as physical cartridges or other consoles via link cables.I did this project not because I think that FPGA emulators are inherently better than software emulators, but because I think they’re interesting and fun to build.I began work on the project by doing some initial research and sketching out a high level design.My previous FPGA emulator project used a Xilinx Zynq chip, which integrates FPGA fabric (“PL”) with a dual-core ARM processor running Linux (“PS”). I implemented the entire emulator on the FPGA, and used the Linux system to configure the FPGA, render the UI, and load ROM files from the filesystem.I decided to keep this same division of responsibilities: using the FPGA to do the core emulation, with a separate processor to do support tasks. However, to make the overall design easier to reason about, I decided to to use an FPGA-only chip (without any hard processor cores), and an external microcontroller (MCU) to do the tasks that the ARM cores did before.The FPGA would consume input, directly interface to the game cartridges (through level shifters to support both the 3.3 volt GBA and 5 volt Game Boy), and output audio and video to the speakers and display. The MCU would handle the UI, read ROM files from the microSD card, initialize peripherals (display, DAC, IMU), handle power sequencing, and load the FPGA configuration.I wanted to have Wi-Fi and Bluetooth support: Wi-Fi for software updates, and the possibility of emulating the Game Boy Advance Wireless Adapter, and Bluetooth to support wireless game controllers (when connected to an external display). To reduce complexity (and avoid the need for careful RF design), I looked only for complete Wi-Fi/Bluetooth modules with integrated antennas.An early block diagram I sketched outI also drew out rough sketches of what the final device might look like: placement of buttons, screen, speakers, ports, cartridge slot, and battery. I settled on a vertical Game Boy Color-esque design (as opposed to a horizontal Game Boy Advance-style design), because I felt that this would maximize the space in the back of the device for full-size Game Boy Color cartridges and a battery.After sketching out the goals and high level design, I started component selection: picking out each non-trivial component of the system, evaluating features and requirements (e.g. how they communicate, power consumption and voltages needed).Since I intended to have this manufactured and assembled at JLCPCB, I strongly preferred parts that were available in their part library. One technique I even used for narrowing down part choices was finding the relevant category in their part search, and sorting by their stock count.I initially planned to use an RP2040 microcontroller, with a separate ESP32-WROOM module to support Wi-Fi and Bluetooth.The ESP32 supports both Bluetooth Classic and LE, which is essential for supporting a wide range of controllers, and the RP2040 has USB host support, to support wired controllers.During the schematic design process, I ended up simplifying the RP2040 + ESP32 combination to just a single ESP32-S3 module for a few reasons:I started running out of GPIOs on the RP2040, and I was dedicating 4 of them (2 for UART, 1 for reset, 1 for booting in firmware download mode) to communication with the ESP32. Plus, the ESP32-S3 has more GPIOs overall.I wanted to write the MCU firmware in Rust, and the ESP32-S3 had support for the Rust standard library (via ESP-IDF and esp-idf-hal). This seemed like it would be easier to get the software up and running.Fewer components means easier routing and assemblyThe ESP32-S3 has an SDIO module (for interfacing with the microSD card), and FAT filesystem support (via ESP-IDF). It would be possible to do this with the RP2040 PIO, but having a proper peripheral and driver for this makes it a lot easier.The ESP32-S3 is more powerful than the RP2040, and would probably be able to render a smoother UI.However, the ESP32-S3 has one main disadvantage compared to the original ESP32: it doesn’t have Bluetooth Classic support, only LE. This would greatly limit the range of supported wireless controllers, but I believed the compromise was worth it. I also decided to scrap USB host support, because supporting USB-C dual role (switchable device or host) would have added a lot of additional complexity.If the RP2350 microcontroller (the successor to the RP2040) had been available when I started this project, I may very well have chosen it, since it has even more power, PIO blocks, memory, and GPIO pins. I might have paired it with an RM2 radio module for Wi-Fi and Bluetooth.I wanted a display that would support integer scaling for the Game Boy Advance, which has a 240x160 pixel screen. I was also looking for a screen roughly on the order of 3.0-3.5 inches wide (diagonal), to be comfortable to hold in the hand.I found the ER-TFT035IPS-6 LCD module from EastRising, with a 3.5 inch display, and a 320x480 pixel resolution. This allows for a 2x integer scale for the Game Boy Advance (and a 2x scale plus centering for the 160x144 Game Boy display). This checked off almost all of the boxes: integer scaling, a good size, available at a reasonable price, pretty good documentation (for the ILI9488 LCD controller).ER-TFT035IPS-6 LCD moduleThe main issue, which actually ended up being fairly annoying, is that it’s a 320x480 display, not 480x320. Meaning, it’s oriented in portrait mode, not landscape. I rotated the device 90 degrees to fit in a landscape orientation, but this created two issues:In landscape orientation, the bottom of the display (containing the LCD driver chip and the flex cable) faces to the left or the right, which means that larger bazels are required on the left and right of the display to center the “active area” of the LCD within the handheld.In landscape orientation, the display refreshes from left to right, not top to bottom.The problem with refreshing from left to right is that the Game Boy and Game Boy Advance (and almost every other system) refresh from top to bottom. This means that the display can’t be refreshed perfectly in sync with the game (zero buffering), and single buffering leads to unsightly diagonal tearing. Instead, I had to use triple buffering, where the game is writing to one framebuffer, the LCD driver is reading from another buffer, and there’s one spare swap buffer. This increases the amount of memory used – and because it needed to be accessed by both the game and LCD driver simultaneously (dual port), it needed to be stored in internal block RAM in the FPGA, a scarce resource.So, even though the Game Boy emulator uses <10% of the total logic resources of the FPGA, and the Game Boy Advance uses around 30%, I had to use a large (more expensive, and power hungry) FPGA so that I had enough block RAM.I also stuck a standard size HDMI port into the design, connected directly to the FPGA. HDMI has a few additional, non-video signals that need level shifting from 5V to 3.3V (I opted for discrete transistors), and it requires the source (me!) to supply a small amount of power.I had never previously designed anything that used a lithium ion battery, so I had a fair amount of learning to do. Adafruit was a helpful resource. I needed a way to charge the battery from USB power, and a way to measure how charged it is.Lithium ion batteries can be dangerous if misused. Safely charging a battery is non-trivial, and requires a feedback loop and adjustable voltage sources. A dedicated IC seemed like the best way to do this. A lot of hobbyists use the ultra-cheap TP4056 1A battery charger, but I’d read about a lot of issues it has around safely charging the battery while using it. I decided instead to opt for the TI BQ2407x series of battery charger ICs. They seem to be widely used in commercial products, came with a comprehensive datasheet, and had a few critical features: programmable input and charge current limits, safety timers, and “power path management” for safely charging the battery while the device is on.Typical discharge curve for a 3.7V lipo battery (source: Adafruit)There are a few ways to measure the charge level of the battery, which generally relies on the fact that a lithium ion battery’s voltage depends on its charge level. A fully charged battery is about 4.2 volts, a battery with between 80% and 20% charge is about 3.7 volts, and below that a drained battery falls off pretty quickly to under 3.0 volts. If all you want is a coarse estimate of the battery level, you can use an ADC to read the voltage and estimate whether the battery is fully charged or nearly discharged. However, since the voltage curve is nearly flat between 20% and 80% charge (and is also dependent on the load), this can’t give the fine-grained battery percentage that we’re used to on phones and laptops. Instead, I opted for a discrete fuel gauge IC, the MAX17048. It’s simple to integrate and inexpensive.I decided to use a push button for the main power switch, because I needed to be able to do a graceful shutdown, where the microcontroller could save state (e.g. the current save file for an emulated cartridge) before it actually powered off.I briefly considered using an ultra-low power, always on microcontroller to act as a custom PMIC to provide power switch functionality (and perhaps avoid the need for a separate real-time clock IC, and even a battery gauge). While this would have been flexible and really cool, I figured it wasn’t worth the additional complexity.The main system power ranges from about 3.4 V when the battery is discharged, to 4.2 V when the battery is fully charged, up to 5.0 V when the device is plugged in with USB.The ESP32-S3 module required 3.3 V, and most of the other ICs in the system did too. The main exception is the FPGA, which requires a 1.0 V core power rail, a 1.8 V “auxiliary” power rail, and a 3.3 V power rail for I/O. Moreover, according to the Xilinx Artix-7 datasheet (DS181), these power rails need to be powered on in a particular sequence: for my use, this means 1.0 V, then 1.8 V, then 3.3 V. Additionally, I needed a 5.0 V supply to interface with Game Boy / Game Boy Color cartridges.There are multi-rail power regulators available, and a lot of FPGA development boards use them. However, they all seemed to be expensive and difficult to purchase in low quantities. Instead, I opted for separate power regulators for each rail. I used buck converters instead of linear regulators to maximize power efficiency.I used the TLV62585 converter for the 3.3 V, 1.8 V, and 1.0 V rails. This is a simple, performant buck converter with a “power good” output, which is useful for power sequencing: you can connect the  output of one regulator to the  pin of the next regulator, to power on the rails in the desired order.For the 5.0 V rail, I used the TPS61022 boost converter. This converter is way overkill for the 5.0 V rail (which might use 75mA ), but it was readily available, and conveniently compatible with the same 1µH inductor as the buck converters.According to the FPGA datasheet, the XC7A100T consumes more than 100mW of static power. That is, it consumes that as long as it’s connected to power, even if it’s doing absolutely nothing. I figured I might want to support a low power sleep mode, so I decided to split the FPGA into a separate power domain with an explicit power enable signal from the MCU. I also used an AP2191W load switch for the FPGA’s 3.3 V rail to be able to keep the 1.0 V → 1.8 V → 3.3 V sequencing.I wanted the device to have both speakers and a 3.5mm headphone jack. Ultimately, the FPGA generates an I2S digital audio signal, and I needed a DAC to convert it to an analog audio signal, and then an amplifier to drive the speakers (or headphones). I wanted digital volume control (to support volume buttons, rather than a volume knob or slider), and I needed some way to switch the audio output between speakers and the headphones, depending on whether or not headphones are plugged in. With no real audio experience, this seemed like a daunting task.While searching for multiple separate components, I stumbled upon the TLV320DAC3101. It combines a stereo DAC with a speaker amplifier and a headphone driver. Additionally, it supports digital volume control, and headphone detection. I think this chip is a good example of how thoughtful component selection can simplify the overall design. Looking through the datasheet, it required a 1.8 V core voltage (unlike essentially every other component other than the FPGA) and a fair amount of configuration registers to set over I2C, but it had all of the features I needed.I was originally planning to have just a single (mono) speaker, but I figured if I had a stereo DAC, I might as well put two in there. I chose the CES-20134-088PMB, an enclosed microspeaker with a JST-SH connector. Having an enclosed speaker simplified audio design, because as it turns out, you can’t just stick a speaker to a board and expect it to sound okay (Same Sky, the manufacturer of that speaker, has a blog post explaining some of the nuances).I prefer the feeling of clicky, tactile buttons (such as those found in the GBA SP, Nintendo DS (original), Nintendo 3DS, Switch) compared to “mushy” membrane buttons (such as those found in the Game Boy Color, original GBA, and Nintendo DS Lite). I learned that the tactile switches used in the GBA SP are a widely available off-the-shelf part from Alps Alpine. I used similar, but smaller buttons for the Start/Select/Home buttons, and a right-angle button from the same manufacturer for side volume and power buttons.Although I only had plans to support Game Boy and Game Boy Advance (requiring a D-pad, A and B buttons, L and R shoulder buttons, and Start/Select), I opted to add two more “X” and “Y” face buttons to leave the possibility open of supporting more systems in the future.The L and R buttons posed an additional challenge – I found numerous right-angle tactile buttons (to be soldered onto the back, facing towards the top). However, none of them seemed to have the actuator (the part of the button you make contact with) far enough away from the PCB to be easily pressed. At first, I thought about making a separate shoulder button board to move them at the correct distance, but then I started looking at what existing devices do for inspiration. The Game Boy Advance SP actually uses a more complex mechanism for the shoulder buttons: rather than a simple actuator like the face buttons, there’s a hinge with a torsion spring that hits the actuator at an angle. This is actually part of what makes the shoulder buttons pleasant to press: you don’t need to hit them from exactly the right direction, because they pivot. I ended up just going with a standard right-angle tactile button, opting to solve the problem with the mechanism in the enclosure.GBA SP shoulder button mechanismOne of my main goals was to allow ROM files to be loaded from a microSD card, rather than only being able to be played from a physical cartridge. To do this, I’d need dedicated RAM for the FPGA to hold the game. Game Boy Advance games, typically, are a maximum of 32 MB. They don’t make SRAMs that large (and if they did, they’d be very expensive). Instead, I needed to use DRAM.Asynchronous SRAM is very simple: supply a read address to the address pins, and some amount of nanoseconds later, the data you’re reading appears on the data pins. DRAM is more complex: the simplest kind is “single data rate synchronous DRAM” (SDR SDRAM, or just SDRAM, distinguishing it from the significantly more complex DDR SDRAM). However, even SDRAM is non-trivial to use. DRAM is organized into banks, rows, and columns, and accessing DRAM requires sending commands to “activate” (open) a row before reading out “columns”, and then “precharging” (closing) a row. Handling all of this requires a DRAM controller (see this simple description of the state machine required). This isn’t terribly complex, but I was signing myself up for more work.Alternatively, I could have chosen a PSRAM chip (essentially DRAM with an integrated controller to make it have a more SRAM-like interface). However, I couldn’t find a PSRAM part that I was happy with (cost, availability, interface), and so I ended up going with the inexpensive W9825G6KH 32MB 16-bit SDRAM.I also decided to stick a 512 KiB SRAM chip in the design in case I ended up needing some more simple memory later, like for emulating the SRAM used for Game Boy cartridge save files. Despite being 1/64 the capacity, this chip was about 3x the cost of the SDRAM. This ended up being a wise decision, since a lot of my internal FPGA block ram was eaten up by the triple buffer for the display (see above).Cartridge and Link Ports#The cartridge slot and link ports are no-name parts from AliExpress, easily available for cheap. These seem to mostly be GBA SP compatible, and are often used as repair parts.The Game Boy Advance can play both Game Boy [Color] and Game Boy Advance games. These run at different voltages and use different protocols, so the device needed some way of determining which type of cartridge is inserted.GBA cartridge (top) vs GB cartridge (bottom)The cartridges are physically different at the bottom: GBA cartridges (the top cartridge in the image) have a notch on either side. The GBA has a  that senses the absence of a notch on an inserted cartridge and switches the device into Game Boy Color mode.I measured the size and position of this notch, and searched Digi-Key and Mouser for switches that met these constraints. In the end, I was only able to find a single switch that would work.Miscellaneous peripherals#I used the surprisingly cheap LSM6DS3TR-C IMU from ST. This tiny IMU has a 3-axis accelerometer and gyroscope, more than sufficient for emulating the few GB/GBA cartridges that have motion controls.For keeping track of time even when the device was off, I used the PCF8563T real-time clock chip. I chose this because it was 1) I2C (no additional pins required), 2) cheap, and 3) readily available from JLCPCB. Interestingly, all of the real-time clock chips I found count in seconds/minutes/hours/days/months/years. This makes sense for a really simple device with minimal computational power. However, it’s annoying for my purposes, since all I really want is a timestamp I can pass to some other datetime library, and converting between the calendar time and a unix timestamp is non-trivial due to how the chips incompletely handle leap years.I picked up a few cheap coin vibration motors to use for vibration support (for the rare cartridge that had a built-in vibration motor).I also used a TCA9535 I2C I/O expander to connect the face buttons to the MCU. I ran out of pins, and while I  have used the FPGA as a sort of I/O expander, I figured I’d make it simpler for myself (and allow the buttons to be used even if the FPGA was powered off) by letting the MCU read them itself.For this project, as with my previous ones, I used KiCad to create my schematic and do PCB layout. I really can’t recommend KiCad enough: it’s a great program, intuitive to use, and it’s free and open source.This was a very ambitious project for my level of electrical engineering experience, and creating the schematic took a couple of weeks. I spent a lot of time designing the circuit for each component, because I was afraid I’d do something wrong and end up with a stack of useless boards without the skills needed to debug them. A lot of the component selection actually happened in parallel with schematic design, as I found new requirements or problems and had to change components.I gained a lot of experience reading component datasheets. It’s a really valuable skill, both for component selection and for creating designs that use the components. Nearly every datasheet has a “typical application” section, where the manufacturer shows how the component would fit into a circuit. At minimum, this has power supply information (e.g. these voltages to these pins with these decoupling capacitors). For more complex components like the DAC, it also has information about power sequencing, different ways the device could be connected to the rest of the system, a register list, that sort of thing. Some components also included PCB layout recommendations. This information was all really helpful, and gave me a good deal of confidence that my board would work as long as I read through the datasheet and followed the manufacturer’s recommendations.Then I got to the FPGA. Nearly every component has a single datasheet. Some of them have an additional application note or two. Particularly complex chips (like the ESP32-S3 microcontroller) have a separate datasheet, reference manual, and hardware design guide. The Xilinx Series 7 FPGAs have . Overviews, packaging and pinout, configuration guides, BGA design rules, power specifications, clocking resources, I/O specifications, PCB layout guides, design checklists… even a 4MB Excel spreadsheet for estimating power consumption! And believe me, Xilinx didn’t just write documentation for fun: there’s so much documentation because the chip  this much documentation.Designing with the FPGA was overwhelming, and  beyond my experience level. At several points I genuinely considered dropping the project altogether. Fortunately, I persevered, and gradually internalized a lot of the information. I also read through the schematics of any open-source Artix-7 development board I could get my hands on. Seeing what other people were doing gave me more confidence that I was doing the right thing.Eventually, after I placed all of the components, connected them, ensured all of the nets were labeled, and ran KiCad’s electrical rules checker (ERC) to find obvious mistakes, I moved on to layout.I did PCB layout at the same time as some of the initial enclosure CAD. The mechanics of how everything fit together influenced the placement of the display connector, cartridge slot, buttons, speakers, and connectors. After I came up with a plausible enclosure design, I placed some of the first key components onto the PCB and locked them into place while I did the rest of the routing.Rough enclosure design to help with board layoutI first focused on components that would be hardest to route. Primarily, the FPGA: the package I was using (CSG324) is a BGA, 18x18 with 0.8mm pitch between pins. “Fanning out” all of the I/O signals requires careful routing, and at 0.8mm pitch, it’s difficult to do this routing with cheap PCB manufacturing techniques. I ended up being able to do this routing with a 6-layer PCB (three signal, two ground, one power), with 0.1mm track width and spacing, and 0.4/0.25mm vias. Fortunately, this is all within the realm of JLCPCB’s capabilities.BGA fanout with thin traces and small viasAs I routed signals out from the FPGA to other parts, I assigned those signals to the FPGA pins. Similarly, with the MCU, I assigned signals to pins in a way that made routing easier. Certain signals had restrictions (e.g. on the FPGA, the main 50 MHz clock signal can only go into certain pins, or the configuration bitstream can only go to certain pins, or certain pins are differential pairs for HDMI output), but overall, I had a lot of flexibility with pin assignment.KiCad has a feature where it automatically backs up your project as you work on it. I changed the settings to save every 5 minutes and not delete old backups, which allowed me to generate this timelapse of my layout process:Revision 1 board layout timelapseOnce I finished placing and routing all of the components, I ran the design rules checker (DRC) and fixed issues. I hesitated for a while before sending the PCB for manufacturing. I re-read the schematics, reviewed the layout, and eventually felt confident enough that I was done. I submitted the order to JLCPCB, and after a few questions by their engineers about component placement, they started manufacturing it.Board testing and bring-up#After two weeks or so, I received the assembled boards in the mail:An assembled board and an unassembled boardFirst, I probed the power rail test points with a multimeter to check for shorts. Then, I plugged the boards in for the first time, and pressed the power button. To my delight, the green LED turned on, indicating that the power button circuit, power path, and 3.3V regulator worked. The microcontroller USB enumerated, and I could see that it logged some errors (since I hadn’t flashed anything to it yet).I intended to write the MCU firmware in Rust, but I did initial board testing and bring-up with MicroPython. This would let me interactively type in Python and write basic scripts to communicate with the peripherals on the board and make sure I had connected everything correctly. I didn’t have to worry about writing efficient or well-organized code, and could just focus on functionality.I flashed the MicroPython firmware image, and wrote a couple lines of Python to blink the LED. I powered on the FPGA power domain, and checked that the , , and  rails had the correct voltage.Next, I wrote a simple bitstream for the FPGA that read the state of the buttons and produced a pattern on the shared signals between the FPGA and the MCU. I wrote simple Python code to configure the FPGA, loaded up the bitstream, and polled the signals from the FPGA. Pressing buttons changed the state, and confirmed that the FPGA was properly powered, and configurable from the MCU.After I confirmed the FPGA worked, I started writing a simple display driver to initialize the LCD and push some pixels from the MCU over SPI. The initialization sequence uses a number of LCD-specific parameters (voltages, gamma correction, etc.), that I learned from the LCD manufacturer’s example code.(Slowly) pushing pixels to the LCDThe LCD module’s controller, an ILI9488, has a few quirks: despite claiming that it supports 16-bit colors over SPI, it actually only supports 18-bit colors. This unfortunately meant that the MCU’s LCD driver would be more inefficient than I expected, since it has to expand 16-bit colors to 18-bit before sending them over the bus. This didn’t end up being a huge issue, however, because the FPGA is the one driving the display most of the time.Another quirk (hardware bug?) is that the ILI9488 doesn’t stop driving its SPI output line, even when its chip-select signal is inactive. This means that the chip will interfere with any other communication on the bus… including the FPGA, which sits on the same bus. I never actually needed to read any data back from the LCD (and even if I did, it supports three-wire SPI), so I just cut the trace between the LCD’s SDO line and the SPI bus.Debugging the LCD test codeTrouble with power domains#I started trying to communicate with the I2C peripherals (I/O expander, RTC, etc.), and found that nothing was responding. A bit of probing with a logic analyzer revealed that the SCL/SDA lines were being held low, and that powering on the FPGA power domain let the lines be pulled high and communication to happen.I deduced that this was due to the DAC, which had its IOVDD powered by , which likely caused its protection diodes to pull the IO lines (SCL and SDA) low:The problematic portion of the schematicI tested out this theory by cutting the PCB traces connecting the DAC’s IOVDD and  with a knife. After this, I2C worked even with the FPGA power disabled. Then, I tested a possible fix by adding a wire to power the DAC’s IOVDD from the  rail. I confirmed that I could still talk to the other I2C devices, and once enabling FPGA power, that I could talk to the DAC too.While bringing up the LCD, I saw that the FPGA was also pulling down the shared SPI bus lines while it was unpowered. Not enough to prevent communication with the LCD, but it still wasn’t great. Between this and the DAC issue, I learned an important EE lesson: be careful when connecting components in different power domains together. A tristate buffer, such as the 74LVC1G125, could have helped here to isolate the buses.Once I2C was working, I wrote some basic driver code for the fuel gauge, real-time clock, IMU, and I/O expander, just to check that they all worked correctly. I also checked that the MCU could read from and write to the attached microSD card.Audio and video output from the FPGA#Next, I updated my testing FPGA bitstream to output a test pattern over the LCD parallel interface (“DPI”), and a test tone to the DAC over the I2S interface. Then, I began poking on the MCU side to configure the LCD controller and DAC appropriately.With some amount of trial and error, I convinced the LCD to accept input from the FPGA. Most of the trial and error revolved around the rotation of the LCD module. Soon after, I configured the DAC properly, and it played the test tone from the FPGA over the speakers and the headphones.WIP video output from the FPGAAt this point, much of the board was working, so I soldered on the rest of the components (cartridge slot, cartridge switch, link port, shoulder buttons).With the cartridge slot in place, I had everything I needed to port over the Game Boy emulator from my last project. I did a quick-and-dirty port of the emulator, with some hacking around to connect the core to the audio, video, and the physical cartridge. I was able to play the first Game Boy game on the device far sooner than I was expecting:Pokemon Silver running from cartridgeI spent the next month or so implementing things on the FPGA. I started on the SPI receiver implementation, so that the MCU and FPGA could communicate.It was relatively straightforward to write the initial version, which 4x oversampled the SPI signals from the main system clock. For the Game Boy, that was ~8 MHz, for a maximum SPI speed of 2 MHz. The MicroPython ESP32-S3 SPI implementation supported only single SPI, so that allowed for a maximum transfer speed of 256 KB/s. This was sufficient to do most of my initial testing, but I later wrote an improved SPI receiver to run with an internal 200 MHz clock (from a PLL that turned on and off with the chip-select signal to save power), communicating with the rest of the system via a pair of FIFOs. This added a lot of complexity and edge cases, but it greatly improved performance, allowing the bus to run at 40 MHz.I wrote the SPI interface to the FPGA with memory-like semantics: each SPI transfer starts with a command byte, encoding whether it’s a read or write transfer, the size of each word in the transfer (8, 16, or 32 bits), and whether the “target address” should autoincrement as the transfer progresses. Then, a 32-bit address, followed by reading or writing the data. Each thing that the MCU might want to access (control registers, blocks of memory) are mapped into the 32-bit address space.As with my previous FPGA project, I wrote almost all of the FPGA code in Chisel, a Scala-based HDL. The remaining bits were the top-level Verilog. Chisel made it really simple to parametrize, compose, and test the various modules that I wrote.Once I had the SPI receiver working, I wrote controllers for the on-board SRAM and SDRAM. The SRAM was relatively simple (although I still got it slightly wrong at first). The SDRAM was a bit tricky, and even as I write this I’m not quite satisfied with its performance, and intend to rewrite it in the future.I exposed the SRAM and SDRAM interfaces to the MCU via SPI, which allowed me to read and write to these pieces of memory from the MCU. I used this a lot for testing: writing patterns into memory and reading them back to ensure that read and write both worked.Side note: SDRAM has to be continuously refreshed, otherwise the stored data decays over time. It depends on the chip, but typically each row has to be read and written back (or auto-refreshed, which does the same thing) at least once every 64 milliseconds to avoid losing state. What I found interesting, however, is that the data can actually persist for quite a bit longer. I discovered that when I was reconfiguring the FPGA between tests, most of the test data that I had previously written would still stick around even without being refreshed. In the first few seconds some bits would start flipping, and over the course of a few minutes, most of what was written was completely unintelligible.With the SDRAM controller and SPI receiver written, I was then able to implement the “emulated cartridge” part of the Game Boy emulator, where the MCU reads a ROM file off of the microSD card and sends it to the FPGA to be stored in SDRAM. Then, the FPGA “emulates” a cartridge (rather than interfacing with a real physical cartridge). After a few stupid mistakes, I was able to run test ROMs and homebrew. As an added bonus, since I was using my own SDRAM controller directly, I didn’t have any of the performance issues I’d faced before when accessing the ROM stored in memory.Writing the microcontroller firmware in Rust#By this point I had tested, in some form or another, all of the different components of the system. I’m really surprised that everything worked in my first board revision – even the rework I did early on wasn’t actually required for functionality.I decided now was a good time to start building an interactive GUI. Up until this point, I had just been running commands in the MicroPython REPL. However, I didn’t want to build a whole UI in Python just to throw it away later, so I also started working on the “production” Rust firmware.In the last few years, a lot of progress has been made towards making Rust on the ESP32 chips work well, even on the chips that use the Xtensa ISA. I followed the Rust on ESP Book and quickly had an environment set up. I opted for the “Rust with the Standard Library” approach, so that I could benefit from ESP-IDF, especially the built-in support for USB and SD cards with the FAT filesystem.I started porting over the drivers I had written in Python. I found embedded Rust to be a bit verbose in some cases, but overall pleasant to use and worth the (little) trouble.I starting writing my own minimal GUI framework for basic menus. I poked around with the  library, but soon found that the typical patterns I was expecting to use weren’t a great fit for Rust. I also started planning out different screens and realized that I probably actually wanted to use a more comprehensive UI framework.Ultimately, I settled on Slint, a Rust-native declarative GUI framework with excellent support for embedded devices. Slint has a custom DSL to describe the UI and composable components. After a bit of practice I found myself to be really productive with it. I enjoyed using Slint, and I’d use it again in the future. The authors are responsive on GitHub, and the project has steadily improved over the year or so that I’ve been using it.There were a few rough edges for my use case, however:The built-in GUI elements and examples were all heavily oriented around mouse or touchscreen navigation. Game Bub only has buttons for navigation, however, so I had to make my own widgets (buttons, lists) that worked with key navigation. This involved a few hacks, because Slint’s focus handling was a little bit simplistic.The built-in GUI styles looked (in my opinion) bad on a low DPI screen. Text was excessively anti-aliased and hard to read at small sizes. This was also fixed by building my own widgets.Slint doesn’t have a great story around supporting different “screens” – I had to build some of my own infrastructure to be able to support navigation between the main menu, games, rom select, settings, etc.The GUI is rendered on the MCU, and then the rendered framebuffer is sent over to the FPGA. Slint supports partial rendering, where only the parts of the screen that have changed are updated, which improved performance. The FPGA maintains a copy of the framebuffer and ultimately is responsible for driving the display. This has a few advantages over driving the display directly from the MCU:Sending a framebuffer at 40 MHz QSPI to the FPGA is 16x faster than sending it to the LCD controller at 10 MHz (the fastest speed supported by the ILI9488)The UI is rendered at 240x160 to improve performance and maintain the GBA aesthetic, but the LCD controller doesn’t have a scaler, so the MCU would have to send 4x the pixels. The FPGA can easily scale the UI framebuffer itself.The FPGA can composite the emulator output with a semi-transparent “overlay” to support an in-game menu, volume / brightness bars, battery notifications, etc.An external display (e.g. monitor or TV) can be driven by the FPGA via HDMII spent some time making a variety of firmware improvements, mostly polish and quality-of-life. I added a settings screen to set the date and time, whether to use Game Boy (DMG) or Game Boy Color (CGB) mode when playing Game Boy games, etc.Then I improved the ROM select file browser, and added a battery level indicator.I also got sick of having to take the microSD card out of the device and connect it to my computer through a series of adapters (microSD to SD to USB-A to USB-C), so I implemented a basic utility to expose the microSD card as a USB Mass Storage Device, using TinyUSB and the ESP32-S3’s USB-OTG capabilities.It was a little bit more difficult than I expected, because USB Mass Storage requires the device to provide raw block access. This means that the filesystem has to be unmounted by the device, otherwise the device and host could conflict and corrupt the filesystem. The ESP32-S3 also only supports USB Full Speed, for a practical maximum transfer speed of ~600KB/sec. It’s really useful for transferring save files or updating the FPGA bitstreams, but less useful for transferring a large number of ROM files.Later, I implemented MBC7 support in the Game Boy emulator for Kirby Tilt ’n Tumble, using the on-board accelerometer.After I implemented a decent amount of software functionality, I decided to finish the enclosure design. The bare board just wasn’t cutting it anymore, and the taped LCD module/loose speakers/rubber-banded battery contraption was fragile.Game Bub looking rough without an enclosureI came into this project without any CAD or 3D printing experience. I looked at a few different CAD software packages, and I ultimately settled on FreeCAD, primarily because it was free and open source. I learned how to use the software with some video tutorials. FreeCAD, unfortunately, was a little bit rough around the edges and I ended up running into some annoying issues. Nevertheless, I powered through and finished the design.FreeCAD view of the enclosure and some buttonsI found parametric modeling, where the geometry of the model is defined by constraints and dimensions, to be intuitive. However overall, I found 3D CAD to be very time consuming. I think a large part of this is my inexperience, but thinking in three dimensions is a lot more difficult than, say, a 2D PCB layout. Creating a full assembly was even more difficult: I had to visualize how the front and rear pieces would fit together, where the screws would go, and how the buttons, screen, speaker, cartridge slot, battery, and ports would all fit in. This project definitely pushed the boundaries of my (previously non-existent) product design skills.After finishing the design, I printed out the technical drawing at a 1:1 scale and physically placed the board and other components down as a final check. Then, I sent it to JLCPCB for manufacturing. I opted for SLA resin printing, for high precision and a smooth finish.Enclosure technical drawingAfter a couple weeks, I got the finished enclosure and custom buttons back.Front and rear half, outsideI put the buttons, speakers, and screen into the enclosure, screwed on the PCB, and put the whole thing together.Assembling the front sideGame Bub, fully assembled and functionalI wasn’t sure how dimensionally accurate the 3D printing would be, so I added a lot of extra clearance around the buttons and ports. As it turned out, the printing was very precise, so the buttons rattled around a little in the oversized button holes.It’s a little bit chunky (smaller than an original Game Boy, though!) and the ergonomics aren’t ideal, but I was really happy to finally have an enclosure. It actually started (sort of) looking like a real product, and I wasn’t constantly worried about breaking it anymore.Game Boy Advance support#I won’t go into all of the details of how I wrote the emulator here (this article is already long enough!). If you’re interested, my previous article about my Game Boy FPGA emulator goes into detail about the general process of writing an emulator, and for a high-level introduction to the Game Boy Advance (from a technical perspective), I recommend Rodrigo Copetti’s article. In general, I tried to implement the emulator the way it might actually have been implemented in the original hardware: each cycle of the FPGA corresponds to one actual hardware cycle (no cheating!).As with the Game Boy, I did nearly all of my development with a simulator backed by Verilator and SDL. By the end of the development process, the simulator was running at about 8% of the real-time speed (on an M3 MacBook Air with excellent single-core performance), which was a bit painful.The Game Boy Advance CPU, the ARM7TDMI, is significantly more complicated than the Game Boy’s SM83 (a Z80 / 8080-ish hybrid). However, in some ways, it was easier to understand and implement: the ARM7TDMI is much closer to a simple modern processor architecture, and it’s extensively documented by ARM. For example, the ARM7TDMI Technical Reference Manual has block diagrams and detailed cycle-by-cycle instruction timing descriptions.I had a lot of fun implementing the CPU. The architecture has a three-stage pipeline (fetch, decode, execute) – a division that feels natural when you implement it in hardware. The ARM7TDMI has two instruction sets: the standard 32-bit ARM instruction set, and the compressed 16-bit THUMB instruction set. I implemented the CPU the way it works in hardware, where the only difference between ARM and THUMB is the decode stage.As I was implementing the CPU, I wrote test cases for each instruction. Each test checks the functionality of the instruction: processor state, register values after, as well as the cycle-by-cycle behavior and interaction with the memory bus. This was helpful for catching regressions as I implemented more and more control logic. It was also really satisfying to be able to implement individual instructions, then write the tests, and check that everything worked.Chisel made it easy to write out the CPU control logic. The CPU control logic is a state machine that generates microarchitectural control signals (e.g. bus A should hold the value from the first read register, bus B should hold an immediate value, the memory unit should start fetching the computed address, etc.). Chisel allowed me to collect common functionality into functions (e.g.  to set up the signals to dispatch the next decoded instruction, or  to signal that the pipeline should be flushed and a new instruction should be fetched from the current program counter).I found it helpful to draw out timing diagrams with WaveDrom when working through instructions, especially to deal with the pipelined memory bus.My timing diagram of the ARM7TDMI branch instructionsBy mid-May (about a month later), I finished the CPU implementation (with occasional bug fixes after) and moved onto the rest of the system.PPU, MMIO, and everything else#Over the next month and a half, I implemented the majority of the rest of the Game Boy Advance. The CPU interacts with the rest of the system via memory-mapped IO (MMIO) registers. Unlike the Game Boy CPU, which can only access memory a single byte at a time, the ARM7TDMI can make 8-bit, 16-bit, and 32-bit accesses. This complicates MMIO, and the different hardware registers and memory regions in the GBA respond to different access widths in different ways.I started with the Picture Processing Unit (PPU), which produces the video output. The author of NanoBoyAdvance, fleroviux, had helpfully documented the PPU VRAM access patterns, which gave a lot of insight into how the PPU might work internally. Tonc was also immensely helpful for implementing the PPU and testing individual pieces of functionality.(Sort of) running a Tonc PPU demoThe PPU took a few weeks, and then I moved onto DMA, followed by hardware timers, and audio. Of course, as I’d try new tests, demos, and games, I’d uncover bugs and fix them.Kirby  in Dream LandGame Boy and Game Boy Advance cartridges use the same 32-pin connector. However, they work very differently. The Game Boy cartridge bus is asynchronous: the game outputs the 16-bit address (64 KiB address space) on one set of pins and lowers the  pin. Some time later, the 8-bit read data from the ROM stabilizes on a separate set of pins.For the GBA, Nintendo extended the bus data width to 16-bit and the address space to 25-bit (32 MiB). However, they kept roughly the same set of pins, accomplishing this by multiplexing the 24 data/address pins: the console outputs the address (in increments of the data word size of 16-bits, for a 24-bit physical address), then lowers the  signal to “latch” the address in the cartridge. Then, each time the console pulses the  pin, the cartridge increments its latched address and outputs the next data over the same pins. This allows for a continuous read of sequential data without having to send a new address for each access. The GBA also allows games to configure cartridge access timings to support different ROM chips.I had to do a lot of my own research here. Software emulators don’t need to care about the precise timing of the cartridge bus, so there wasn’t much documentation. To figure out the exact cycle-accurate timing, I used a Saleae logic analyzer and connected it to the cartridge bus. I wrote a test program for the GBA to do different types of accesses (reads, writes, sequential, non-sequential, DMA) with different timing configurations.Cartridge bus analysis setupAfter coming up with numerous scenarios (especially around the interaction between DMA and the CPU, and starting and stopping burst accesses), I came up with a consistent model for how cartridge accesses worked. I created some timing diagrams to help:Timing diagram of a non-sequential access followed by a sequential accessFinally, I started implementing the cartridge controller state machine based on my observations, paired with an emulated cartridge implementation. With the emulated cartridge, I was able to properly run real games in the simulator.I quickly implemented physical cartridge support, to be able to finally run it on the actual FPGA. I connected the signals, built a new bitstream, and… it didn’t work at all. The Game Boy Advance boot screen ran, but it didn’t get any further than that. I implemented the emulated cartridge on the FPGA (reading ROM files from the SD card), and it worked! Which was great, but physical cartridges still didn’t.I used the logic analyzer to observe how my emulator was interacting with the cartridge compared to how an actual GBA, and found numerous issues.One of the first things I noticed was short glitches on the  line. I knew these had to be glitches (rather than incorrect logic), because they were 8 nanoseconds long, much shorter than the ~59.6ns clock period. Since the cartridge latches the address on a falling edge of , glitches cause it to latch an address when it shouldn’t, screwing up reads.Glitches on the cartridge busHere, I learned an important lesson in digital design: output signals should come directly from flip-flops, with no logic in between.After each flip-flop outputs a new value (on the rising edge of the clock), the signals propagate through the chip. As they propagate, taking different paths of different lengths throughout the chip, the output from each lookup table (LUT) is unstable. These values only stabilize near the end of the clock cycle (assuming the design met timing closure), and then each flip-flop stores the stable value at the next rising edge. If you output a signal from logic, this instability is visible from outside of the chip, manifesting as glitches in the output signal. If you instead output the signal from a flip-flop, it’ll change only on each clock edge, remaining stable in the middle.And of course, I had written the cartridge controller without thinking about this, and  of the output signals were generated from logic. I rewrote the controller to output everything from flip-flops, which had a series of cascading changes since all of the signals now had to be computed one clock cycle earlier than I expected.There were other issues too – part of the problem was that my emulated cartridge model was too permissive, and didn’t catch some fairly obvious incorrect behavior. After a few days of intensive debugging with the logic analyzer, I got to the point where I could play games from physical cartridges.Metroid: Zero Mission running from the cartridgeCartridge prefetch buffer#The ARM7TDMI has a single shared instruction and data memory bus. As a result, a long series of sequential memory accesses is rare. Even a linear piece of code without branches that includes “load” or “store” instructions would produce a series of non-sequential memory accesses, as the CPU fetches an instruction from one location, loads a register from a different location, and then goes back to fetching the next instruction.This poses a real performance issue on the GBA, because every non-sequential access from the cartridge incurs a multi-cycle penalty. Nintendo attempted to mitigate this somewhat with the “prefetch buffer” (read this post by endrift, the author of mGBA, for more details) which attempts to keep a cartridge read burst active between CPU accesses. Without emulating the prefetch buffer, some games lag (I noticed this the most in Mario Kart Super Circuit, and some rooms of Metroid: Zero Mission).The prefetch buffer, while simple in theory, is not well documented and has a lot of corner cases and weird interactions. Emulator developers often start by taking a shortcut: making all cartridge accesses take a single cycle when the prefetch buffer is enabled. This wouldn’t work for me, since I actually had to interface with the physical cartridge.So, I set out to do some more research to figure out exactly how the prefetch buffer worked. After making some educated guesses and tests, I came up with a reasonable model of how it might work.Notes about the prefetch state machineActually implementing it took a lot of work, and I kept stumbling upon more and more corner cases. Eventually I got to the point where all games appeared to run at full speed, and most importantly, didn’t randomly crash. My implementation isn’t perfect: there are still a few mGBA test suite timing tests I don’t pass, but it’s certainly sufficient to play games.: standard duplex SPI, used for communicating with accessories: custom multi-drop UART-like protocol, used to link up to four GBAs together for multiplayer games: the Nintendo N64 and GameCube controller protocol, used to connect to a GameCube: duplex UART with flow control, : controlling the four pins individually as GPIO, The timing of these isn’t well documented, so I did my own research.A  mode transfer with no attached consolesI did a lot of testing with examples from the gba-link-connection library, intended for homebrew GBA games, but helpful for testing the different transfer modes in a controlled environment.Multiplayer Mario Kart with Game Bub and a GBAGame Bub linked to a GameCube playing Animal CrossingDuring the emulator development, I had used various test ROMs (mentioned before) to test basic functionality in isolation. As my emulator became mature enough to run commercial games, however, I started to shift some of my focus to accuracy-focused test ROMs.These test ROMs (such as the mGBA test suite) generally test really specific hardware quirks and timing. For example, they might test what happens when you run an instruction that ARM calls “unpredictable”, or the exact number of cycles it takes to service an interrupt in specific scenarios, or the value of the “carry” flag after performing a multiplication. These are the kinds of things that don’t actually matter for playing games, but present a fun challenge and a way to “score” your emulator against others. This also highlights the collaborative nature of the emulation development community: people sharing their research and helping each other out.I won’t talk about all of the tests here (for my emulator’s test results, see this page). But I do want to mention the . This is an official test cartridge from Nintendo, likely used as part of a factory test or RMA procedure. Apparently, Nintendo has  used it to test their emulators (e.g. their GBA emulator on the Nintendo Switch). This test has generally been considered to be difficult to pass (it tests some specific hardware quirks), but it’s easier now that the tests have been thoroughly reverse engineered and documented. Still, passing it is a nice milestone:Passing the AGB Aging CartridgeSecond hardware revision#Towards the end of 2024, approximately one year after I originally designed Game Bub, I decided to make a second hardware revision. Over the past year, I had been keeping track of all of the things I would want to change in a future revision. Since the first version of Game Bub miraculously worked without any major issues, this list was primarily minor issues and ergonomics changes.I fixed the minor I2C power issues, removed the reference designators from the PCB silkscreen (they looked messy with the dense board, and I didn’t use them for anything anyway), and changed around some test points. I improved the rumble circuit to be more responsive, and switched to a PCB-mounted vibration motor.The first version of Game Bub was fairly thick, measuring 12.9mm at the top and 21.9mm on the bottom. The thickness of the rear enclosure was dictated by the thickness of Game Boy cartridges, but I made several changes to the front. I moved the  (8.5mm!) link port to the back, and removed the HDMI port (more on that later). I changed the headphone jack (5.0mm tall – no wonder they started getting removed from phones) to a mid-mount one that sunk into the PCB and reduced the overall height.I also switched from an  module (3.1mm depth) to an  (2.4mm depth). I should have done this from the beginning, I just didn’t even know the ESP32-S3-MINI existed. This had the side effect of giving me 3 more GPIOs, which allowed me to put the FPGA and LCD on separate SPI busses, avoiding the minor issue of an unpowered FPGA interfering with LCD communication, and allowed for faster boot because the LCD could be configured at the same time as the FPGA.I switched the speakers, from the fully-enclosed CES-20134-088PMB to the CMS-160903-18S-X8. I made this change primarily for ease of assembly. The first speaker had a wire connector that plugged into the board, and I found it difficult to connect during assembly without having the wire interfere with buttons. The new speaker is smaller and has a spring contact connector, so it just presses against the PCB as the device is assembled. This required some speaker enclosure design – an unenclosed speaker in free air sounds quiet and tinny.I reworked the layout of the face buttons and D-pad to match the spacing of the Nintendo DSi. This allowed me to use the silicone membranes from the DSi for an improved button feel and reduced rattling. I was also hoping to use the plastic buttons from the DSi (which were higher quality compared to my 3D printed buttons), but even with the new thinner design, the buttons weren’t quite tall enough to be easily pressed.I created another timelapse of my modifications to produce the second version of the PCB:Revision 2 board layout timelapseFor the second revision of the enclosure, I switched to Fusion 360 for the CAD work. While I would have preferred to keep using FreeCAD, I found that it was making it harder for me to be productive. Fusion 360 has a free version for hobbyists (with some limitations that have gradually increased over time), and overall I’ve found it very pleasant to use.Fusion 360 view of the second enclosure, fully assembledUnlike with the first revision, I waited until I had a final design for both the enclosure and the PCB before getting anything manufactured. This let me go back and forth, making small modifications to each of them as needed.I wanted to make the end result look more polished and professional, so I contracted a factory to produce custom LCD cover glass, made out of 0.7mm thick tempered glass with a black silkscreen. It was relatively expensive for a low quantity order, but I’m really happy with how it turned out.Custom LCD cover glass with adhesive backingManufacturing and assembly#I got the PCBs manufactured and assembled, this time with black solder mask to look .Assembled PCB, revision 2I had two enclosures made. The first was black PA-12 Nylon, printed with MJF. Nylon is strong and durable, and the MJF 3D printing technology produces a slightly grainy surface that’s really pleasant to hold in your hand.Closeup of the nylon grainy textureThe second one was made of transparent resin (SLA, like before). This lets me show off the PCB that I worked so hard on, and evokes the transparent electronics trend from the 90s.Assembly was a lot easier this time around: the silicone membranes held the face buttons in place, the speakers had a spring contact instead of wires, and the shoulder button assembly was better. In the first revision, I had excessively large tolerances because I wasn’t sure how precise the 3D printing would be. In the second version, I was able to shrink these.The final product looked and felt a lot better, too. The edges were more rounded, and the device was thinner and easier to hold. The buttons felt  better to press and didn’t rattle around, and the cover glass over the LCD added polish.First revision (left), second revision (center and right)I previously mentioned that I removed the full-size HDMI port from the first revision. I had first planned to change it to a mini-HDMI or micro-HDMI port to reduce the size, but I was worried about durability.What I  wanted to do was output video through the USB-C port, avoiding the need for any HDMI port at all. Unfortunately, I had already concluded earlier that I wouldn’t be able to output DisplayPort video signals from the FPGA, which meant that I couldn’t use the standard USB-C DisplayPort alternate mode.However, an idea struck me towards the end of 2024: I didn’t actually  to use the DisplayPort alt-mode. The USB-C connector, in addition to the USB 2.0 D+/D- pins, has four differential pairs (for USB superspeed). Conveniently, HDMI  uses four differential pairs. The USB specification allows for vendor-specific alt-modes, so I could just implement my own, outputting the HDMI signal directly from the FPGA over the additional pins. Then I could build a custom dock that takes those pins and connects them to the data lines of an HDMI port.According to the USB specification, alternate modes must be negotiated by both sides first, using the USB-C Power Delivery (USB-PD) protocol, to prevent them from interfering with devices that aren’t expecting them. I don’t actually have a USB-PD controller in Game Bub (too much added complexity), so I took a shortcut: have a microcontroller in the dock communicate with the Game Bub over regular USB and perform a handshake before enabling HDMI output from the FPGA. Once Game Bub detects that it’s been disconnected from the dock, it can just switch back to using the internal display.I realized that the dock also presents another opportunity for controller support. I originally wanted to build wireless controller support into the handheld, but the ESP32-S3 only supports Bluetooth Low Energy, and the majority of controllers use Bluetooth Classic. Fortunately, the Raspberry Pi Pico W (with an RP2040 MCU) supports both types of Bluetooth, so I just decided to use that as the microcontroller on the dock. Game controllers connect to the dock over Bluetooth, and the Pico sends the controller inputs to the device. I wired up the  and  USB-C pins as a direct connection between the FPGA and the dock for low latency input.The RP2040 acts as the USB host, and Game Bub only needs to be a device. I also added a USB hub chip and some additional USB ports on the back of the dock to allow for wired controller support too. Just like with wireless controllers, the dock handles the direct controller communication, and just passes inputs back to the main Game Bub unit.Since the dock is so simple (comparatively), it only took about a day to design and lay out.I had also hoped to use the dock to solve another problem around HDMI output: HDMI sinks (monitors, TVs) pull the HDMI data lines up to 3.3 volts, and can actually backfeed power to the HDMI source. For Game Bub, this meant that a powered-off unit would turn itself on when connected over HDMI. I used a HDMI buffer chip in the dock to try to alleviate this problem, but the chip I used wasn’t actually properly suited to this use-case and interfered with video output, so I had to carefully rework the board to bypass the chip. I’ll have to fix it in a later revision.Bypassing the HDMI buffer chipAfter the rework, HDMI output worked! The rest of the features are still a work in progress.Game Bub PCB on the dock, connected to an external monitorCongratulations on reading this far! This writeup ended up being incredibly long, even with a lot of details left out.I’m proud of what I accomplished over the last year and a half: I met all of my goals to produce a polished handheld FPGA retrogaming device. I pushed my electrical engineering and product design skills to the limit, and learned a lot in the process. Professional product and hardware designers deserve  respect.I deliberately designed this project with lots of possible extension opportunities to keep me occupied for a long time. I worked hard to get to the point where I’m comfortable sharing Game Bub with the world, but I still have a long list of TODOs for the future.In the near term, I’m going to work on finishing the dock, implementing wireless controller support (and maybe wired). I plan to use the Bluepad32 library to do so.I also want to improve the accuracy of my Game Boy Advance emulator: my goal here is to someday pass the entire mGBA test suite. I hope that I can contribute back to the wonderful  community with my emulator, and I plan to write-up some of my research around the GBA cartridge interface and link port.I have a long list of mostly minor changes to make to the MCU firmware: improving UI render performance, bits of polish like low battery notifications, eliminating display glitching when reloading the FPGA, and that sort of thing. I also plan to add more utilities, like a cartridge dumper and save backup/restore feature.Some day, I want to emulate the Game Boy Advance Wireless Adapter over Wi-Fi, e.g. with ESP-NOW. This won’t be compatible with the original wireless adapter, unfortunately, since that uses raw 2.4 GHz modulation rather than Wi-Fi.I designed Game Bub with extremely low production volumes in mind, using off-the-shelf commodity parts to keep the overall cost down. However, there are a few things I would have liked to be able to do, but are only possible with much higher volumes:A better LCD module (likely custom): native landscape mode to avoid the need for triple-buffering. Ideally a 720x480 resolution display, to allow for 3x GBA scaling and filter effects.High-quality injection molded case and buttons: 3D printing is great for low volume production, but an injection molded case would be great. It would be more precise (allowing for tighter tolerances), stronger, and allow for significantly more color options.Custom battery pack: or at least customizing the length of the connector wire. The current solution is hacky and doesn’t make the best use of internal space, due to limited off-the-shelf battery options.Smaller BGA parts for SRAM and SDRAM to free up board space (and move internal signals to 1.8 volts): this is actually something that would be possible in smaller volumes too, if I were willing to send parts from Mouser or DigiKey to JLCPCB for assembly.]]></content:encoded></item><item><title>How GitHub uses CodeQL to secure GitHub</title><link>https://github.blog/engineering/how-github-uses-codeql-to-secure-github/</link><author>Brandon Stewart</author><category>official</category><pubDate>Wed, 12 Feb 2025 17:00:04 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[GitHub’s Product Security Engineering team writes code and implements tools that help secure the code that powers GitHub. We use GitHub Advanced Security (GHAS) to discover, track, and remediate vulnerabilities and enforce secure coding standards at scale. One tool we rely heavily on to analyze our code at scale is CodeQL.CodeQL is GitHub’s static analysis engine that powers automated security analyses. You can use it to query code in much the same way you would query a database. It provides a much more robust way to analyze code and uncover problems than an old-fashioned text search through a codebase.The following post will detail how we use CodeQL to keep GitHub secure and how you can apply these lessons to your own organization. You will learn why and how we use:Custom query packs (and how we create and manage them).  Variant analysis to uncover potentially insecure programming practices.We employ CodeQL in a variety of ways at GitHub. with the default and security-extended query suites
Default setup with the default and security-extended query suites meets the needs of the vast majority of our over 10,000 repositories. With these settings, pull requests automatically get a security review from CodeQL.  
A few repositories, like our large Ruby monolith, need extra special attention, so we use advanced setup with a query pack containing custom queries to really tailor to our needs.  
To conduct variant analysis and quick auditing, we use MRVA. We also write custom CodeQL queries to detect code patterns that are either specific to GitHub’s codebases or patterns we want a security engineer to manually review.The specific custom Actions workflow step we use on our monolith is pretty simple. It looks like this:- name: Initialize CodeQL
    uses: github/codeql-action/init@v3
    with:
      languages: ${{ matrix.language }}
      config-file: ./.github/codeql/${{ matrix.language }}/codeql-config.yml
Our Ruby configuration is pretty standard, but advanced setup offers a variety of configuration options using custom configuration files. The interesting part is the  option, which is how we enable our custom query pack as part of the CodeQL analysis. This pack contains a collection of CodeQL queries we have written for Ruby, specifically for the GitHub codebase.So, let’s dive deeper into why we did that—and how!Publishing our CodeQL query packInitially, we published CodeQL query files directly to the GitHub monolith repository, but we moved away from this approach for several reasons:It required going through the production deployment process for each new or updated query.  Queries not included in a query pack were not pre-compiled, which slowed down CodeQL analysis in CI.  Our test suite for CodeQL queries ran as part of the monolith’s CI jobs. When a new version of the CodeQL CLI was released, it sometimes caused the query tests to fail because of changes in the query output, even when there were no changes to the code in the pull request. This often led to confusion and frustration among engineers, as the failure wasn’t related to their pull request changes.By switching to publishing a query pack to GitHub Container Registry (GCR), we’ve simplified our process and eliminated many of these pain points, making it easier to ship and maintain our CodeQL queries. So while it’s  to deploy custom CodeQL query files directly to a repository, we recommend publishing CodeQL queries as a query pack to the GCR for easier deployment and faster iteration.When setting up our custom query pack, we faced several considerations, particularly around managing dependencies like the  package.To ensure our custom queries remain maintainable and concise, we extend classes from the default query suite, such as the  library. This allows us to leverage existing functionality rather than reinventing the wheel, keeping our queries concise and maintainable. However, changes to the CodeQL library API can introduce breaking changes, potentially deprecating our queries or causing errors. Since CodeQL runs as part of our CI, we wanted to minimize the chance of this happening, as this can lead to frustration and loss of trust from developers.We develop our queries against the latest version of the  package, ensuring we’re always working with the most up-to-date functionality. To mitigate the risk of breaking changes affecting CI, we pin the  version when we’re ready to release, locking it in the  file. This guarantees that when our queries are deployed, they will run with the specific version of  we’ve tested, avoiding potential issues from unintentional updates.Here’s how we manage this setup:In our qlpack.yml, we set the dependency to use the latest version of During development, this configuration pulls in the latest version) of  when running , ensuring we’re always up to date.
// Our custom query pack's qlpack.yml

library: false
name: github/internal-ruby-codeql
version: 0.2.3
extractor: 'ruby'
dependencies:
  codeql/ruby-all: "*"
tests: 'test'
description: "Ruby CodeQL queries used internally at GitHub"
Before releasing, we lock the version in the  file, specifying the exact version to ensure stability and prevent issues in CI.
// Our custom query pack's codeql-pack.lock.yml

lockVersion: 1.0.0
dependencies:
 ...
 codeql/ruby-all:
   version: 1.0.6
This approach allows us to balance developing against the latest features of the  package while ensuring stability when we release.We also have a set of CodeQL unit tests that exercise our queries against sample code snippets, which helps us quickly determine if any query will cause errors before we publish our pack. These tests are run as part of the CI process in our query pack repository, providing an early check for issues. We strongly recommend writing unit tests for your custom CodeQL queries to ensure stability and reliability.Altogether, the basic flow for releasing new CodeQL queries via our pack is as follows:Open a pull request with the new query.  Write unit tests for the new query.  Increment the pack version in a new pull request.  Run  to resolve dependencies.  Correct unit tests as needed.  Publish the query pack to the GitHub Container Registry (GCR).  Repositories with the query pack in their config will start using the updated queries.We have found this flow balances our team’s development experience while ensuring stability in our published query pack.Configuring our repository to use our custom query packWe won’t provide a general recommendation on configuration here, given that it ultimately depends on how your organization deploys code. We opted against locking our pack to a particular version in our CodeQL configuration file (see above). Instead, we chose to manage our versioning by publishing the CodeQL package in GCR. This results in the GitHub monolith retrieving the latest published version of the query pack. To roll back changes, we simply have to republish the package. In one instance, we released a query that had a high number of false positives and we were able to publish a new version of the pack that removed that query in less than 15 minutes. This is faster than the time it would have taken us to merge a pull request on the monolith repository to roll back the version in the CodeQL configuration file.One of the problems we encountered with publishing the query pack in GCR was how to easily make the package available to multiple repositories within our enterprise. There are several approaches we explored.Grant access permissions for individual repositories. On the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API.   Mint a personal access token for the CodeQL action runner. We could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read  of our private packages rather than ones we explicitly allow it to have access to.  CodeQL query pack queriesWe write a variety of custom queries to be used in our custom query packs. These cover GitHub-specific patterns that aren’t included in the default CodeQL query pack. This allows us to tailor the analysis to patterns and preferences that are specific to our company and codebase. Some of the types of things we alert on using our custom query pack include:High-risk APIs specific to GitHub’s code that can be dangerous if they receive unsanitized user input.  Use of specific built-in Rails methods for which we have safer, custom methods or functions.    Required authorization methods not being used in our REST API endpoint definitions and GraphQL object/mutation definitions.  REST API endpoints and GraphQL mutations that require engineers to define access control methods to determine which actors can access them. (Specifically, the query detects the absence of this method definition to ensure that the actors’ permissions are being checked for these endpoints.)  Use of signed tokens so we can nudge engineers to include Product Security as a reviewer when using them.Custom queries can be used more for educational purposes rather than being blockers to shipping code. For example, we want to alert engineers when they use the ActiveRecord::decrypt method. This method should generally not be used in production code, as it will cause an encrypted column to become decrypted. We use the recommendation severity in the query metadata so these alerts are treated as more of an informational alert. That means this may trigger an alert in a pull request, but it won’t cause the CodeQL CI job to fail. We use this lower severity level to allow engineers to assess the impact of new queries without immediate blocking. Additionally, this alert level isn’t tracked through our Fundamentals program, meaning it doesn’t require immediate action, reflecting the query’s maturity as we continue to refine its relevance and risk assessment./**
 * @id rb/github/use-of-activerecord-decrypt
 * @description Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save
 * them unencrypted, effectively undoing encryption and possibly making the attributes inaccessible.
 * If you need to access the unencrypted value of any attribute, you can do so by calling my_model.attribute_name.
 * @kind problem
 * @severity recommendation
 * @name Use of ActiveRecord decrypt method
 * @tags security
 *      github-internal
 */

import ruby
import DataFlow
import codeql.ruby.DataFlow
import codeql.ruby.frameworks.ActiveRecord

/** Match against .decrypt method calls where the receiver may be an ActiveRecord object */
class ActiveRecordDecryptMethodCall extends ActiveRecordInstanceMethodCall {
  ActiveRecordDecryptMethodCall() { this.getMethodName() = "decrypt" }
}

from ActiveRecordDecryptMethodCall call
select call,
  "Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save them unencrypted.
Another educational query is the one mentioned above in which we detect the absence of the `control_access` method in a class that defines a REST API endpoint. If a pull request introduces a new endpoint without `control_access`,  a comment will appear on the pull request saying that the `control_access` method wasn’t found and it’s a requirement for REST API endpoints. This will notify the reviewer of a potential issue and prompt the developer to fix it./**
 * @id rb/github/api-control-access
 * @name Rest API Without 'control_access'
 * @description All REST API endpoints must call the 'control_access' method, to ensure that only specified actor types are able to access the given endpoint.
 * @kind problem
 * @tags security
 * github-internal
 * @precision high
 * @problem.severity recommendation
 */

import codeql.ruby.AST
import codeql.ruby.DataFlow
import codeql.ruby.TaintTracking
import codeql.ruby.ApiGraphs

// Api::App REST API endpoints should generally call the control_access method
private DataFlow::ModuleNode appModule() {
  result = API::getTopLevelMember("Api").getMember("App").getADescendentModule() and
  not result = protectedApiModule() and
  not result = staffAppApiModule()
}

// Api::Admin, Api::Staff, Api::Internal, and Api::ThirdParty REST API endpoints do not need to call the control_access method
private DataFlow::ModuleNode protectedApiModule() {
  result =
    API::getTopLevelMember(["Api"])
        .getMember(["Admin", "Staff", "Internal", "ThirdParty"])
        .getADescendentModule()
}

// Api::Staff::App REST API endpoints do not need to call the control_access method
private DataFlow::ModuleNode staffAppApiModule() {
  result =
    API::getTopLevelMember(["Api"]).getMember("Staff").getMember("App").getADescendentModule()
}

private class ApiRouteWithoutControlAccess extends DataFlow::CallNode {
  ApiRouteWithoutControlAccess() {
    this = appModule().getAModuleLevelCall(["get", "post", "delete", "patch", "put"]) and
    not performsAccessControl(this.getBlock())
  }
}

predicate performsAccessControl(DataFlow::BlockNode blocknode) {
  accessControlCalled(blocknode.asExpr().getExpr())
}

predicate accessControlCalled(Block block) {
  // the method `control_access` is called somewhere inside `block`
  block.getAStmt().getAChild*().(MethodCall).getMethodName() = "control_access"
}

from ApiRouteWithoutControlAccess api
select api.getLocation(),
  "The control_access method was not detected in this REST API endpoint. All REST API endpoints must call this method to ensure that the endpoint is only accessible to the specified actor types."
Variant analysis (VA) refers to the process of searching for variants of security vulnerabilities. This is particularly useful when we’re responding to a bug bounty submission or a security incident. We use a combination of tools to do this, including GitHub’s code search functionality, custom scripts, and CodeQL.  We will often start by using code search to find patterns similar to the one that caused a particular vulnerability across numerous repositories. This is sometimes not good enough, as code search is not semantically aware, meaning that it cannot determine whether a given variable is an Active Record object or whether it is being used in an `if` expression. To answer those types of questions we turn to CodeQL.When we write CodeQL queries for variant analysis we are much less concerned about false positives, since the goal is to provide results for security engineers to analyze. The quality of the code is also not quite as important, as these queries will only be used for the duration of the VA effort. Some of the types of things we use CodeQL for during VAs are:Where are we using SHA1 hashes?  One of our internal API endpoints was vulnerable to SQLi according to a recent bug bounty report. Where are we passing user input to that API endpoint?   There is a problem with how some HTTP request libraries in Ruby handle the proxy setting. Can we look at places we are instantiating our HTTP request libraries with a proxy setting?One recent example involved a subtle vulnerability in Rails. We wanted to detect when the following condition was present in our code:A parameter was used to look up an Active Record object.  That parameter is later reused after the Active Record object is looked up.The concern with this condition is that it could lead to an insecure direct object reference (IDOR) vulnerability because Active Record finder methods can accept an array. If the code looks up an Active Record object in one call to determine if a given entity has access to a resource, but later uses a different element from that array to find an object reference, that can lead to an IDOR vulnerability. It would be difficult to write a query to detect  vulnerable instances of this pattern, but we were able to write a query that found potential vulnerabilities that gave us a list of code paths to manually analyze. We ran the query against a large number of our Ruby codebases using CodeQL’s MRVA.The query, which is a bit hacky and not quite production grade, is below:/**
 * @name wip array query
 * @description an array is passed to an AR finder object
 */

import ruby
import codeql.ruby.AST
import codeql.ruby.ApiGraphs
import codeql.ruby.frameworks.Rails
import codeql.ruby.frameworks.ActiveRecord
import codeql.ruby.frameworks.ActionController
import codeql.ruby.DataFlow
import codeql.ruby.Frameworks
import codeql.ruby.TaintTracking

// Gets the "final" receiver in a chain of method calls.
// For example, in `Foo.bar`, this would give the `Foo` access, and in
// `foo.bar.baz("arg")` it would give the `foo` variable access
private Expr getUltimateReceiver(MethodCall call) {
  exists(Expr recv |
    recv = call.getReceiver() and
    (
      result = getUltimateReceiver(recv)
      or
      not recv instanceof MethodCall and result = recv
    )
  )
}

// Names of class methods on ActiveRecord models that may return one or more
// instances of that model. This also includes the `initialize` method.
// See https://api.rubyonrails.org/classes/ActiveRecord/FinderMethods.html
private string staticFinderMethodName() {
  exists(string baseName |
    baseName = ["find_by", "find_or_create_by", "find_or_initialize_by", "where"] and
    result = baseName + ["", "!"]
  )
  // or
  // result = ["new", "create"]
}

private class ActiveRecordModelFinderCall extends ActiveRecordModelInstantiation, DataFlow::CallNode
{
  private ActiveRecordModelClass cls;

  ActiveRecordModelFinderCall() {
    exists(MethodCall call, Expr recv |
      call = this.asExpr().getExpr() and
      recv = getUltimateReceiver(call) and
      (
        // The receiver refers to an `ActiveRecordModelClass` by name
        recv.(ConstantReadAccess).getAQualifiedName() = cls.getAQualifiedName()
        or
        // The receiver is self, and the call is within a singleton method of
        // the `ActiveRecordModelClass`
        recv instanceof SelfVariableAccess and
        exists(SingletonMethod callScope |
          callScope = call.getCfgScope() and
          callScope = cls.getAMethod()
        )
      ) and
      (
        call.getMethodName() = staticFinderMethodName()
        or
        // dynamically generated finder methods
        call.getMethodName().indexOf("find_by_") = 0
      )
    )
  }

  final override ActiveRecordModelClass getClass() { result = cls }
}

class FinderCallArgument extends DataFlow::Node {
  private ActiveRecordModelFinderCall finderCallNode;

  FinderCallArgument() { this = finderCallNode.getArgument(_) }
}

class ParamsHashReference extends DataFlow::CallNode {
  private Rails::ParamsCall params;

  // TODO: only direct element references against `params` calls are considered
  ParamsHashReference() { this.getReceiver().asExpr().getExpr() = params }

  string getArgString() {
    result = this.getArgument(0).asExpr().getConstantValue().getStringlikeValue()
  }
}

class ArrayPassedToActiveRecordFinder extends TaintTracking::Configuration {
  ArrayPassedToActiveRecordFinder() { this = "ArrayPassedToActiveRecordFinder" }

  override predicate isSource(DataFlow::Node source) { source instanceof ParamsHashReference }

  override predicate isSink(DataFlow::Node sink) {
    sink instanceof FinderCallArgument
  }

  string getParamsArg(DataFlow::CallNode paramsCall) {
    result = paramsCall.getArgument(0).asExpr().getConstantValue().getStringlikeValue()
  }

  // this doesn't check for anything fancy like whether it's reuse in a if/else
  // only intended for quick manual audit filtering of interesting candidates
  // so remains fairly broad to not induce false negatives
  predicate paramsUsedAfterLookups(DataFlow::Node source) {
    exists(DataFlow::CallNode y | y instanceof ParamsHashReference
    and source.getEnclosingMethod() = y.getEnclosingMethod()
    and source != y
    and getParamsArg(source) = getParamsArg(y)
    // we only care if it's used again AFTER an object lookup
    and y.getLocation().getStartLine() > source.getLocation().getStartLine())
  }
}

from ArrayPassedToActiveRecordFinder config, DataFlow::Node source, DataFlow::Node sink
where config.hasFlow(source, sink) and config.paramsUsedAfterLookups(source)
select source, sink.getLocation()
CodeQL can be very useful for product security engineering teams to detect and prevent vulnerabilities at scale. We use a combination of queries that run in CI using our query pack and one-off queries run through MRVA to find potential vulnerabilities and communicate them to engineers. CodeQL isn’t only useful for finding security vulnerabilities, though; it is also useful for detecting the presence or absence of security controls that are defined in code. This saves our security team time by surfacing certain security problems automatically, and saves our engineers time by detecting them earlier in the development process.Writing custom CodeQL queriesWe have a large number of articles and resources for writing custom CodeQL queries. If you haven’t written custom CodeQL queries before, here are some resources to help get you started:Michael Recachinas, GitHub Staff Security Engineer, also contributed to this blog post.]]></content:encoded></item><item><title>Current state of machine learning and intelligent systems</title><link>https://www.datasciencecentral.com/current-state-of-machine-learning-and-intelligent-systems/</link><author>Dan Wilson</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 16:44:58 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Interview with Dr. Andrea Isoni – SHOW 16 Intelligent systems are evolving faster than ever, and keeping up with the latest advancements requires expertise, foresight, and a deep understanding of both the technological and regulatory landscapes. In AI Think Tank Podcast – Show 16, I had the pleasure of sitting down with Dr. Andrea Isoni, Director and… Read More »]]></content:encoded></item><item><title>Elon Musk&apos;s Waste.gov Is Just a WordPress Theme Placeholder Page</title><link>https://www.404media.co/elon-musks-waste-gov-is-just-a-wordpress-theme-placeholder-page/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/CleanShot-2025-02-12-at-08.15.15@2x.png" length="" type=""/><pubDate>Wed, 12 Feb 2025 16:25:59 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[A  created by the Trump administration to “track government waste” has been left unupdated with a default WordPress sample page that includes language about an imaginary architecture firm. “Waste.gov: Tracking government waste,” the tagline for the website, , says. The rest of the webpage, however, is about an imaginary architecture firm called Études, pulled from a sample webpage for a default WordPress theme called Twenty Twenty-Four. This remains the case a day after Elon Musk, in the Oval Office,  that all of DOGE’s supposed waste-cutting actions are transparent and are available on government websites. Musk is currently in charge of finding and eliminating "waste."“We actually are trying to be as transparent as possible. In fact, our actions—we post our actions to the DOGE handle on X, and to the DOGE website. So all of our actions, which are maximally transparent,” Musk said. “In fact, I don’t think there’s been — I don’t know the case that where [sic] an organization has been more transparent than the DOGE organization.”  contains the line “An official website of the United States government,” a single image of a dollar sign, the words “Department of Government Efficiency. The people voted for major reform,” and nothing else. The website for the US Digital Service, which has been renamed the US DOGE Service, has not been meaningfully updated since Trump was inaugurated. Waste.gov, meanwhile, says “Études is a pioneering firm that seamlessly merges creativity and functionality to redefine architectural excellence,” and various default images and text from the “” WordPress theme, which is also billed as “a flexible default theme.” The theme is “suitable for everyone, from casual bloggers to creative photographers or small businesses,” the theme’s page advertises. Seemingly, the inclusive nature of this theme extends its utility to those seeking to gut the federal government. Specifically, Waste.gov is an exact mirror of the “” for that theme.Notably, Waste.gov does not comply with various executive orders issued by Donald Trump because it contains the word “diverse” in the line “Our comprehensive suite of professional services caters to a diverse clientele, ranging from homeowners to commercial developers.” The imaginary architecture firm also offers “a commitment to innovation and sustainability.” Research on climate change and sustainability has been purged by this administration. The White House registered both waste.gov and DEI.gov—which redirects to waste.gov—last week, .Update: After this article was published, Waste.gov was put behind a password wall.]]></content:encoded></item><item><title>Podcast: AI Is Breaking Our Brains</title><link>https://www.404media.co/podcast-ai-is-breaking-our-brains/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/ai-atrophy-yt-cover.jpg" length="" type=""/><pubDate>Wed, 12 Feb 2025 15:34:47 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[This week we discuss a new Microsoft study that finds using generative AI is "atrophying" people's cognition and critical thinking skills, the right's war on Wikipedia, and, in the subscriber's section, the idea of posting against fascism.Subscribers-only video and embed below:]]></content:encoded></item><item><title>AI Slop of Musk and Trump on TikTok Racks Up 700 Million Views</title><link>https://www.404media.co/ai-slop-of-musk-and-trump-on-tiktok-racks-up-700-million-views/</link><author>Emanuel Maiberg</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/image6.png" length="" type=""/><pubDate>Wed, 12 Feb 2025 15:11:06 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[Videos that use AI generated voices of President Donald Trump and Elon Musk are gaining a massive number of views on TikTok, showing that the platform is also suffering from the onslaught of AI-generated slop that has flooded every corner of the internet. According to Alexios Mantzarlis in his , 400 videos from about two dozen accounts dedicated to posting AI-generated audio of Trump and Musk making quasi-motivational statements have gained more than 700 million views between them.]]></content:encoded></item><item><title>EuroPython Society: Board Report for January 2025</title><link>https://www.europython-society.org/board-report-for-january-2025/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 15:08:37 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[The top priority for the board in January was finishing the hiring of our event manager. We’re super excited to introduce Anežka Müller! Anežka is a freelance event manager and a longtime member of the Czech Python community. She’s a member of the Pyvec board, co-organizes PyLadies courses, PyCon CZ, Brno Pyvo, and Brno Python Pizza. She’ll be working closely with the board and OPS team, mainly managing communication with service providers. Welcome onboard! Our second priority was onboarding teams. We’re happy that we already have the Programme team in place—they started early and launched the Call for Proposals at the beginning of January. We’ve onboarded a few more teams and are in the process of bringing in the rest.Our third priority was improving our grant programme in order to support more events with our limited budget and to make it more clear and transparent. We went through past data, came up with a new proposal, discussed it, voted on it, and have already published it on our blog. Updating onboarding/offboarding checklists for Volunteers and Board MembersVarious infrastructure updates including new website deployment and self-hosted previews for Pull Requests to the website.Setting up EPS AWS account.Working out the Grant Guidelines update for 2025Attending PyConWeb and FOSDEMReviewing updates to the Sponsors setup and packages for 2025More documentation, sharing know-how and reviewing new proposals.Brand strategy: Analysis of social media posts from previous years and web analytics. Call with a European open-source maintainer and a call with a local events organizer about EP content.Comms & design: Call for proposal announcements, EP 2024 video promotions, speaker mentorship, and newsletter. Video production - gathering videos from speakers, video post-production, and scheduling them on YouTube shorts, and social media.Event management coordination: Calls with the event manager and discussions about previous events.Grants: Work on new grant guidelines and related comms.Team onboarding: Calls with potential comms team members and coordination.PR: Delivering a lightning talk at FOSDEM.Offboarding the old boardOnboarding new team membersAdministrative work on GrantsWorked on the Grants proposalFollow-up with team membersCommunity outreach: FOSDEMWorking on various infrastructure updates, mostly related to the website.Reviewing Pull Requests for the website and the internal botWorking on the infrastructure team proposal.Timeline: Discussion with the Programme Team, and planning to do the same with the other teams.Visa Request letter: Setup and Test Visa Request Automation for the current yearTeam selection discussion with past volunteers]]></content:encoded></item><item><title>Python Morsels: Avoid over-commenting in Python</title><link>https://www.pythonmorsels.com/avoid-comments/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 15:05:39 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Documenting instead of commentingHere is a comment I would not write in my code:That comment seems to describe what this code does... so why would I  write it?I do like that comment, but I would prefer to write it as a docstring instead:Documentation strings are for conveying the purpose of function, class, or module, typically at a high level.
Unlike comments, they can be read by Python's built-in  function:Docstrings are also read by other documentation-oriented tools, like Sphinx.Non-obvious variables and valuesHere's a potentially helpful comment:]]></content:encoded></item><item><title>Linux 6.13 Performance For 250Hz vs. 1000Hz Timer Frequency Comparison</title><link>https://www.phoronix.com/news/Linux-250Hz-1000Hz-Kernel-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Feb 2025 15:05:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Given the recent patch proposal to raise the Linux kernel's default timer frequency from 250Hz to 1000Hz, I ran some fresh benchmarks looking at the 250Hz vs. 1000Hz comparison on some modern desktop hardware...]]></content:encoded></item><item><title>Meta’s New AI: Outrageously Good!</title><link>https://www.youtube.com/watch?v=m6aaQoPv5r8</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/m6aaQoPv5r8?version=3" length="" type=""/><pubDate>Wed, 12 Feb 2025 15:03:46 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

📝 The paper "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models" is available here:
https://hila-chefer.github.io/videojam-paper.github.io/

Vs Veo2: https://x.com/TomLikesRobots/status/1888279188336963725

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu]]></content:encoded></item><item><title>5 LLM Prompting Techniques Every Developer Should Know</title><link>https://www.kdnuggets.com/5-llm-prompting-techniques-every-developer-should-know</link><author>Bala Priya C</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/llm-prompting.png" length="" type=""/><pubDate>Wed, 12 Feb 2025 15:00:27 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Want to make the most out of large language models? Check out these prompting techniques you can start using today.]]></content:encoded></item><item><title>White Hat Hackers Expose Iridium Satellite Security Flaws</title><link>https://spectrum.ieee.org/iridium-satellite</link><author>Tereza Pultarova</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjIyMzgzNC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NzYyODU5NX0.HBq5b9Y1gKzuYmBUj9j9OW0i6yHKWc8xIgxlrxWlo2k/image.jpg?width=600" length="" type=""/><pubDate>Wed, 12 Feb 2025 15:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Users' locations and texts can be intercepted, including DoD employees]]></content:encoded></item><item><title>GNU Shepherd 1.0.2 Service Manager Delivers Fixes</title><link>https://www.phoronix.com/news/GNU-Shepherd-1.0.2</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Feb 2025 15:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In addition to the recent release of SysVinit 3.14 and systemd continuing to tack on new features, the GNU Shepherd system/user service manager written in Guile Scheme is out today with a new release...]]></content:encoded></item><item><title>Typecasting and Viewport Transitions in CSS With tan(atan2())</title><link>https://css-tricks.com/typecasting-and-viewport-transitions-in-css-with-tanatan2/</link><author>Juan Diego Rodríguez</author><category>frontend</category><pubDate>Wed, 12 Feb 2025 14:15:28 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[We’ve been able to get the length of the viewport in CSS since… … 2013! Surprisingly, that was more than a decade ago. Getting the viewport width is as easy these days as easy as writing , but what does that translate to, say, in ? What about the other properties, like those that take a percentage, an angle, or an integer?Think about changing an element’s opacity, rotating it, or setting an animation progress based on the screen size. We would first need the  — which isn’t currently possible in CSS, right?What I am about to say isn’t a groundbreaking discovery, it was first described amazingly by Jane Ori in 2023. In short, we can use a weird hack (or feature) involving the  and  trigonometric functions to typecast a length (such as the viewport) to an integer. This opens  new layout possibilities, but my first experience was while writing an Almanac entry in which I just wanted to make an image’s opacity responsive.Resize the CodePen and the image will get more transparent as the screen size gets smaller, of course with some boundaries, so it doesn’t become invisible:This is the simplest we can do, but there is a lot more. Take, for example, this demo I did trying to combine many viewport-related effects. Resize the demo and the page feels alive: objects move, the background changes and the text smoothly wraps in place.I think it’s really cool, but I am no designer, so that’s the best my brain could come up with. Still, it may be too much for an introduction to this typecasting hack, so as a middle-ground, I’ll focus only on the title transition to showcase how all of it works:The idea behind this is to convert  to radians (a way to write angles) using , and then back to its original value using , with the perk of coming out as an integer. It should be achieved like this::root {
  --int-width: tan(atan2(100vw, 1px));
}But! Browsers aren’t too keep on this method, so a lot more wrapping is needed to make it work across all browsers. The following may seem like magic (or nonsense), so I recommend reading Jane’s post to better understand it, but this way it will work in all browsers:@property --100vw {
  syntax: "<length>";
  initial-value: 0px;
  inherits: false;
}

:root {
  --100vw: 100vw;
  --int-width: calc(10000 * tan(atan2(var(--100vw), 10000px)));
}Don’t worry too much about it. What’s important is our precious  variable, which holds the viewport size as an integer!Wideness: One number to rule them allRight now we have the viewport as an integer, but that’s just the first step. That integer isn’t super useful by itself. We oughta convert it to something else next since:different properties have different units, andwe want each property to go from a start value to an end value.Think about an image’s  going from  to , an object rotating from  to , or an element’s  going from  to . We want to interpolate between these values as  gets bigger, but right now it’s just an integer that usually ranges between  to , which is inflexible and can’t be easily converted to any of the end values.The best solution is to turn  into a number that goes from  to . So, as the screen gets bigger, we can multiply it by the desired end value. Lacking a better name, I call this “0-to-1” value . If we have , all the last examples become possible:/* If `--wideness is 0.5 */

.element {
  opacity: var(--wideness); /* is 0.5 */
  translate: rotate(calc(wideness(400px, 1200px) * 360deg)); /* is 180deg */
  offset-distance: calc(var(--wideness) * 100%); /* is 50% */
}So  is a value between  to  that represents how wide the screen is:  represents when the screen is narrow, and  represents when it’s wide. But we still have to set what those values mean in the viewport. For example, we may want  to be  and  to be , our viewport transitions will run between these values. Anything below and above is clamped to  and , respectively.In CSS, we can write that as follows::root {
  /* Both bounds are unitless */
  --lower-bound: 400; 
  --upper-bound: 1200;

  --wideness: calc(
    (clamp(var(--lower-bound), var(--int-width), var(--upper-bound)) - var(--lower-bound)) / (var(--upper-bound) - var(--lower-bound))
  );
}Besides easy conversions, the  variable lets us define the lower and upper limits in which the transition should run. And what’s even better, we can set the transition zone at a middle spot so that the user can see it in its full glory. Otherwise, the screen would need to be  so that  reaches  and who knows how wide to reach .We got the . What’s next?For starters, the title’s markup is divided into spans since there is no CSS-way to select specific words in a sentence:<h1><span>Resize</span> and <span>enjoy!</span></h1>And since we will be doing the line wrapping ourselves, it’s important to unset some defaults:h1 {
  position: absolute; /* Keeps the text at the center */
  white-space: nowrap; /* Disables line wrapping */
}The transition should work without the base styling, but it’s just too plain-looking. They are below if you want to copy them onto your stylesheet:And just as a recap, our current hack looks like this:@property --100vw {
  syntax: "<length>";
  initial-value: 0px;
  inherits: false;
}

:root {
  --100vw: 100vw;
  --int-width: calc(10000 * tan(atan2(var(--100vw), 10000px)));
  --lower-bound: 400;
  --upper-bound: 1200;

  --wideness: calc(
    (clamp(var(--lower-bound), var(--int-width), var(--upper-bound)) - var(--lower-bound)) / (var(--upper-bound) - var(--lower-bound))
  );
}OK, enough with the set-up. It’s time to use our new values and make the viewport transition. We first gotta identify  the title should be rearranged for smaller screens: as you saw in the initial demo, the first  goes up and right, while the second  does the opposite and goes down and left. So, the end position for both spans translates to the following values:h1 {
  span:nth-child(1) {
    display: inline-block; /* So transformations work */
    position: relative;
    bottom: 1.2lh;
    left: 50%;
    transform: translate(-50%);
  }

  span:nth-child(2) {
    display: inline-block; /* So transformations work */
    position: relative;
    bottom: -1.2lh;
    left: -50%;
    transform: translate(50%);
  }
}Before going forward, both formulas are basically the same, but with different signs. We can rewrite them at once bringing one new variable: . It will be either  or  and define which direction to run the transition:h1 {
  span {
    display: inline-block;
    position: relative;
    bottom: calc(1.2lh * var(--direction));
    left: calc(50% * var(--direction));
    transform: translate(calc(-50% * var(--direction)));
    }

  span:nth-child(1) {
    --direction: 1;
  }

  span:nth-child(2) {
    --direction: -1;
  }
}
The next step would be bringing  into the formula so that the values change as the screen resizes. However, we can’t just multiply everything by . Why? Let’s see what happens if we do:span {
  display: inline-block;
  position: relative;
  bottom: calc(var(--wideness) * 1.2lh * var(--direction));
  left: calc(var(--wideness) * 50% * var(--direction));
  transform: translate(calc(var(--wideness) * -50% * var(--direction)));
}As you’ll see, everything is backwards! The words wrap when the screen is too wide, and unwrap when the screen is too narrow:Unlike our first examples, in which the transition ends as  increases from  to , we want to complete the transition as  decreases from  to , i.e. while the screen gets smaller the properties need to reach their end value. This isn’t a big deal, as we can rewrite our formula as a subtraction, in which the subtracting number gets bigger as  increases:span {
  display: inline-block;
  position: relative;
  bottom: calc((1.2lh - var(--wideness) * 1.2lh) * var(--direction));
  left: calc((50% - var(--wideness) * 50%) * var(--direction));
  transform: translate(calc((-50% - var(--wideness) * -50%) * var(--direction)));
}And now everything moves in the right direction while resizing the screen!However, you will notice how words move in a straight line and some words overlap while resizing. We can’t allow this since a user with a specific screen size may get stuck at that point in the transition. Viewport transitions are cool, but not at the expense of ruining the experience for certain screen sizes.Instead of moving in a straight line, words should move in a curve such that they pass around the central word. Don’t worry, making a curve here is easier than it looks: just move the spans twice as fast in the x-axis as they do in the y-axis. This can be achieved by multiplying  by , although we have to cap it at  so it doesn’t overshoot past the final value.span {
 display: inline-block;
 position: relative;
 bottom: calc((1.2lh - var(--wideness) * 1.2lh) * var(--direction));
 left: calc((50% - min(var(--wideness) * 2, 1) * 50%) * var(--direction));
 transform: translate(calc((-50% - min(var(--wideness) * 2, 1) * -50%) * var(--direction)));
}Look at that beautiful curve, just avoiding the central text:This is just the beginning!It’s surprising how powerful having the viewport as an integer can be, and what’s even crazier, the last example is one of the most basic transitions you could make with this typecasting hack. Once you do the initial setup, I can imagine a lot more possible transitions, and  is so useful, it’s like having a new CSS feature right now.I expect to see more about “Viewport Transitions” in the future because they do make websites feel more “alive” than adaptive.]]></content:encoded></item><item><title>Real Python: Python Keywords: An Introduction</title><link>https://realpython.com/python-keywords/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 14:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Python keywords are reserved words with specific functions and restrictions in the language. Currently, Python has thirty-five keywords and four soft keywords. These keywords are always available in Python, which means you don’t need to import them. Understanding how to use them correctly is fundamental for building Python programs.By the end of this tutorial, you’ll understand that:There are  and  in Python.You can get a list of all keywords using  from the  module. in Python act as keywords only in specific contexts. are keywords that have been deprecated and turned into functions in Python 3.In this article, you’ll find a basic introduction to all Python keywords and soft keywords along with other resources that will be helpful for learning more about each keyword. Test your knowledge with our interactive “Python Keywords: An Introduction” quiz. You’ll receive a score upon completion to help you track your learning progress:In this quiz, you'll test your understanding of Python keywords and soft keywords. These reserved words have specific functions and restrictions in Python, and understanding how to use them correctly is fundamental for building Python programs.Python keywords are special reserved words that have specific meanings and purposes and can’t be used for anything but those specific purposes. These keywords are always available—you’ll never have to import them into your code.Python keywords are different from Python’s built-in functions and types. The built-in functions and types are also always available, but they aren’t as restrictive as the keywords in their usage. An example of something you  do with Python keywords is assign something to them. If you try, then you’ll get a . You won’t get a  if you try to assign something to a built-in function or type, but it still isn’t a good idea. For a more in-depth explanation of ways keywords can be misused, check out Invalid Syntax in Python: Common Reasons for SyntaxError.There are thirty-five keywords in Python. Here’s a list of them, each linked to its relevant section in this tutorial:Two keywords have additional uses beyond their initial use cases. The  keyword is also used with loops and with  and  in addition to in conditional statements. The  keyword is most commonly used in  statements, but also used with the  keyword.The list of Python keywords and soft keywords has changed over time. For example, the  and  keywords weren’t added until Python 3.7. Also, both  and  were keywords in Python 2.7 but were turned into built-in functions in Python 3 and no longer appear in the keywords list.As mentioned above, you’ll get an error if you try to assign something to a Python keyword. Soft keywords, on the other hand, aren’t that strict. They syntactically act as keywords only in certain conditions.This new capability was made possible thanks to the introduction of the PEG parser in Python 3.9, which changed how the interpreter reads the source code.Leveraging the PEG parser allowed for the introduction of structural pattern matching in Python. In order to use intuitive syntax, the authors picked , , and  for the pattern matching statements. Notably,  and  are widely used for this purpose in many other programming languages.To prevent conflicts with existing Python code that already used , , and  as variable or function names, Python developers decided to introduce the concept of soft keywords.Currently, there are four  in Python:You can use the links above to jump to the soft keywords you’d like to read about, or you can continue reading for a guided tour.Value Keywords: , , There are three Python keywords that are used as values. These values are singleton values that can be used over and over again and always reference the exact same object. You’ll most likely see and use these values a lot.There are a few terms used in the sections below that may be new to you. They’re defined here, and you should be aware of their meaning before proceeding:]]></content:encoded></item><item><title>EuroPython Society: Changes in the Grants Programme for 2025</title><link>https://www.europython-society.org/changes-in-the-grants-programme-for-2025/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 13:16:30 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[We are increasing transparency and reducing ambiguity in the guidelines.We would like to support more events with our limited budgetWe’ve introduced caps for events in order to make sure all grants are fairly given and we can support more communities.We’ve set aside 10% of our budget for the local community. The EPS introduced a Grant Programme in 2017. Since then, we have granted almost EUR 350k through the programme, partly via EuroPython Finaid and by directly supporting other Python events and projects across Europe. In the last two years, the Grant Programme has grown to EUR 100k per year, with even more requests coming in.With this growth come new challenges in how to distribute funds fairly so that more events can benefit. Looking at data from the past two years, we’ve often been close to or over our budget. The guidelines haven’t been updated in a while. As grant requests become more complex, we’d like to simplify and clarify the process, and better explain it on our website.We would also like to acknowledge that EuroPython, when traveling around Europe, has an additional impact on the host country, and we’d like to set aside part of the budget for the local community.The Grant Programme is also a primary funding source for EuroPython Finaid. To that end, we aim to allocate 30% of the total Grant Programme budget to Finaid, an increase from the previous 25%.We’ve updated the text on our website, and split it into multiple sub-pages to make it easier to navigate. The website now includes a checklist of what we would like to see in a grant application, and a checklist for the Grants Workgroup – so that when you apply for the Grant you already know the steps that it will go through later and when you can expect an answer from us.We looked at the data from previous years, and size and timing of the grant requests. With the growing number and size of the grants, to make it more accessible to smaller conferences and conferences happening later in the year, we decided to introduce max caps per grant and split the budget equally between the first and second half of the year. We would also explicitly split the total budget into three categories – 30% goes to the EuroPython finaid, 10% is reserved for projects in the host country. The remaining 60% of the budget goes to fund other Python Conferences. This is similar to the split in previous years, but more explicit and transparent.Using 2024 data, and the budget available for Community Grants (60% of total), we’ve simulated different budget caps and found a sweet spot at 6000EUR, where we are able to support all the requests with most of the grants being below that limit. For 2025 we expect to receive a similar or bigger number of requests.We are introducing a special 10% pool of money to be used on projects in the host country (in 2025 that’s again Czech Republic). This pool is set aside at the beginning of the year, with one caveat that we would like to deploy it in the first half of the year. Whatever is left unused goes back to the Community Pool to be used in second half of the year.Fairer Funding: By spreading our grants out during the year, conferences that happen later won’t miss out.Easy to Follow: Clear rules and deadlines cut down on confusion about how much you can get and what it’s for.Better Accountability: We ask for simple post-event reports so we can see where the money went and what impact it made.Stronger Community: Funding more events grows our Python network across Europe, helping everyone learn, connect, and collaborate.]]></content:encoded></item><item><title>Apache Kafka Meets Apache Iceberg: Real-Time Data Streaming • Kasun Indrasiri • GOTO 2024</title><link>https://www.youtube.com/watch?v=bzVjBu5nhyM</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/bzVjBu5nhyM?version=3" length="" type=""/><pubDate>Wed, 12 Feb 2025 13:00:58 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at GOTO Chicago 2024. #GOTOcon #GOTOchgo
https://gotochgo.com

Kasun Indrasiri - Author of "Microservices for the Enterprise" @kasunindrasiri 

ORIGINAL TALK TITLE
Kafka Meets Iceberg: Real-Time Data Streaming into Modern Data Lakes & Warehouses

RESOURCES
https://twitter.com/kasunindrasiri
https://medium.com/@kasunindrasiri
https://www.linkedin.com/in/kasun-indrasiri

ABSTRACT
In this talk, we'll explore how Kafka serves as a powerful platform for capturing real-time streaming data and how organizations are increasingly adopting Apache Iceberg table format to store data in data lakes and data warehouses. We'll discuss the key benefits of using Apache Iceberg tables in your data lake such as schema evolution, ACID transactions, hidden partitioning, time traveling and efficient querying.

Next, we'll dive into how to efficiently stream data from Kafka into Iceberg-based data lakes. Confluent Tableflow will be introduced as a potential solution for streamlining the ingestion of Kafka streams into Iceberg tables within your data lake. A live demo will showcase the seamless integration of Kafka with Iceberg, equipping participants with practical knowledge to enhance their data architectures for powerful real-time analytics.

• The role of Kafka in real-time data streaming
• Why Apache Iceberg is essential for data lakes and data warehouses
• Iceberg fundamentals: Core concepts and key features
• Streaming data from Kafka to Iceberg tables in data lakes
• Use case: Leveraging Confluent Tableflow to stream Kafka data into data lakes and warehouses [...]

TIMECODES
00:00 Intro
01:10 Overview
02:06 Kafka is the standard for operational data
03:41 Iceberg for analytical data in data lakes
04:42 Apache Iceberg
05:27 Why Iceberg?
12:24 Structure of an Iceberg table
16:40 Streaming to data lakes is complicated
20:47 Tableflow materialize Kafka topics as Iceberg tables
23:47 Demo
35:37 Outro

Download slides and read the full abstract here:
https://gotochgo.com/2024/sessions/3370

RECOMMENDED BOOKS
Kasun Indrasiri & Sriskandarajah Suhothayan • Design Patterns for Cloud Native Applications • https://amzn.to/3szGx0p
Kasun Indrasiri & Danesh Kuruppu • gRPC: Up and Running • https://amzn.to/3sBGBJJ
Kasun Indrasiri & Prabath Siriwardena • Microservices for the Enterprise • https://amzn.to/40FhxkQ
Kasun Indrasiri • Beginning WSO2 ESB • https://amzn.to/3sx9NF0

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#ApacheKafka #Kafka #ApacheIceberg #Iceberg #AmazonAthena #DataStreaming #DataLake #DataWarehouse #ACIDTransactions #KafkaStreams #Confluent #Tableflow #ConfluentTableflow #KasunIndrasiri

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Top 5 Freelancer Websites Better Than Fiverr and Upwork</title><link>https://www.kdnuggets.com/top-5-freelancer-websites-better-fiverr-upwork</link><author>Abid Ali Awan</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/awan_top_5_freelancer_websites_better_fiverr_upwork_4.png" length="" type=""/><pubDate>Wed, 12 Feb 2025 13:00:35 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Discover freelancing platforms that care about you, not just your money, offering low commission rate, better policies, and higher earning potential.]]></content:encoded></item><item><title>&quot;Thinking&quot; Visually Boosts AI Problem Solving</title><link>https://spectrum.ieee.org/visual-reasoning-in-ai</link><author>Edd Gent</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjM4MjE0OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5NTE3MDA5M30.vId7q-VTLZuXiKu3LB50mcqGpz6lD5N59-GuTYRzltQ/image.jpg?width=600" length="" type=""/><pubDate>Wed, 12 Feb 2025 13:00:05 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[ Giving language models a “mind’s eye” helps them tackle spatial reasoning tasks]]></content:encoded></item><item><title>Training IT Teams for Multi-Cloud DevOps Environments</title><link>https://devops.com/training-it-teams-for-multi-cloud-devops-environments/</link><author>Anne Fernandez</author><category>devops</category><pubDate>Wed, 12 Feb 2025 12:11:06 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Delivering Malware Through Abandoned Amazon S3 Buckets</title><link>https://www.schneier.com/blog/archives/2025/02/delivering-malware-through-abandoned-amazon-s3-buckets.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Wed, 12 Feb 2025 12:09:24 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Here’s a supply-chain attack just waiting to happen. A group of researchers searched for, and then registered, abandoned Amazon S3 buckets for about $400. These buckets contained software libraries that are still used. Presumably the projects don’t realize that they have been abandoned, and still ping them for patches, updates, and etc.The TL;DR is that this time, we ended up discovering ~150 Amazon S3 buckets that had previously been used across commercial and open source software products, governments, and infrastructure deployment/update pipelines—and then abandoned.Naturally, we registered them, just to see what would happen—”how many people are really trying to request software updates from S3 buckets that appear to have been abandoned months or even years ago?”, we naively thought to ourselves.Turns out they got eight million requests over two months.Had this been an actual attack, they would have modified the code in those buckets to contain malware and watch as it was incorporated in different software builds around the internet. This is basically the SolarWinds attack, but much more extensive.But there’s a second dimension to this attack. Because these update buckets are abandoned, the developers who are using them also no longer have the power to patch them automatically to protect them. The mechanism they would use to do so is now in the hands of adversaries. Moreover, often—but not always—losing the bucket that they’d use for it also removes the original vendor’s ability to identify the vulnerable software in the first place. That hampers their ability to communicate with vulnerable installations.Software supply-chain security is an absolute mess. And it’s not going to be easy, or cheap, to fix. Which means that it won’t be. Which is an even worse mess.]]></content:encoded></item><item><title>Real Python: Quiz: Python Keywords: An Introduction</title><link>https://realpython.com/quizzes/python-keywords/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 12:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Python keywords are reserved words with specific functions and restrictions in the language. These keywords are always available in Python, which means you don’t need to import them. Understanding how to use them correctly is fundamental for building Python programs.]]></content:encoded></item><item><title>Linux 6.15 To Bring More Improvements To DRM Panic &quot;Screen of Death&quot;</title><link>https://www.phoronix.com/news/Linux-6.15-More-DRM-Panic</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Feb 2025 11:52:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Now that the Linux 6.14 merge window has passed, new feature material aiming for the Linux 6.15 kernel is beginning to get ready for staging in DRM-Next ahead of that next merge window opening up around the end of March. Sent out today was the first batch of drm-misc-next changes for Linux 6.15 that include more work on DRM Panic for that Linux equivalent to Microsoft Windows' "Blue Screen of Death" as well as changes to the other smaller Direct Rendering Manager drivers...]]></content:encoded></item><item><title>Intel C1 Demotion Knob Proposed For The Linux Kernel To Help Newer Xeon CPUs</title><link>https://www.phoronix.com/news/Intel-C1-Demotion-Knob-Linux</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Feb 2025 11:35:53 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A patch has been proposed for the Linux kernel to add a C1 demotion knob via /sys/devices/system/cpu/cpuidle/c1_demotion for more control over lower power state handling for recent Xeon Scalable processors. This C1 demotion knob can help with the performance of some workloads for Intel Xeon servers but at the cost of increased power consumption...]]></content:encoded></item><item><title>Nping – ping, but with a graph or table view</title><link>https://github.com/hanshuaikang/Nping</link><author>markhan-nping</author><category>hn</category><pubDate>Wed, 12 Feb 2025 10:43:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The difference between metrics and scorers</title><link>https://www.youtube.com/watch?v=67KMDSitw1M</link><author>probabl</author><category>dev</category><category>ai</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/67KMDSitw1M?version=3" length="" type=""/><pubDate>Wed, 12 Feb 2025 10:41:04 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Probabl</source><content:encoded><![CDATA[You can easily write custom metric functions for scikit-learn, but you need to take an extra step if you want to use those metrics in a hyperparameter search. To add a number there, you need to use a scorer instead. The goal of this video is to explain the difference and to show how to leverage the `make_scorer` helper utility here.

Website: https://probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Bluesky: https://bsky.app/profile/probabl.bsky.social
Twitter: https://x.com/probabl_ai
Discord: https://discord.probabl.ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

#probabl]]></content:encoded></item><item><title>The Subsea Cable Network with Josh Dzieza</title><link>https://softwareengineeringdaily.com/2025/02/12/the-subsea-cable-network-with-josh-dzieza/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-subsea-cable-network-with-josh-dzieza</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED7608490746.mp3" length="" type=""/><pubDate>Wed, 12 Feb 2025 10:00:59 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[Subsea cables are high-capacity fiber-optic lines laid along the ocean floor to enable global communication by transmitting data between continents. Spanning thousands of miles, they carry an estimated 95% of international internet, phone, and data transmissions.Critically, these cables are vulnerable to sabotage by state actors, as they form critical infrastructure for global communication and economic stability. Indeed, Russia and China have been implicated in activities targeting subsea cables as recently as November 2024, and experts warn that these networks are likely to be focal points in future conflicts, heightening geopolitical tensions.Josh Dzieza is a reporter for The Verge and has covered the subsea cable industry and the strategic importance of subsea cables. He joins the podcast alongside Gregor Vand to discuss this invisible, and increasingly important, network infrastructure.You can check out Josh’s reporting here.]]></content:encoded></item><item><title>Achieve Transformative Network Security With Cisco Hybrid Mesh Firewall</title><link>https://blogs.cisco.com/security/achieve-transformative-network-security-with-cisco-hybrid-mesh-firewall/</link><author>Rick Miles</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Wed, 12 Feb 2025 08:30:00 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[Hybrid Mesh Firewall addresses 3 forces: Fine-grained composition & distribution of apps in data centers, complex modern networks & sophisticated threats.]]></content:encoded></item><item><title>Cisco and Wiz Collaborate to Enhance Cloud Security: Tackling AI-Generating Threats in Complex IT Infrastructures</title><link>https://blogs.cisco.com/security/cisco-and-wiz-collaborate-to-enhance-cloud-security/</link><author>Rick Miles</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Wed, 12 Feb 2025 08:30:00 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[Cisco is collaborating with Wiz. Together, they aim to improve cloud security for enterprises grappling with AI-generated threats in intricate IT landscapes.]]></content:encoded></item><item><title>Zato Blog: Modern REST API Tutorial in Python</title><link>https://zato.io/en/blog/modern-rest-api-tutorial-in-python.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 08:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[
  2025-02-12, by Dariusz Suchojad
Great APIs don't win theoretical arguments - they just prefer to work reliably and to make developers' lives easier.Here's a tutorial on what building production APIs is really about: creating interfaces that are practical in usage,
while keeping your systems maintainable for years to come.]]></content:encoded></item><item><title>Kushal Das: pass using stateless OpenPGP command line interface</title><link>https://kushaldas.in/posts/pass-using-stateless-openpgp-command-line-interface.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Wed, 12 Feb 2025 05:26:13 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Yesterday I wrote about how
I am using a different tool for  signing and verification. Next, I
replaced my  usage. I have a small
patch to use
stateless OpenPGP command line interface (SOP). It is an implementation
agonostic standard for handling OpenPGP messages. You can read the whole SPEC
here.cargo install rsop rsop-oct
And copied the bash script from my repository to the path somewhere.The  binary from  follows the same SOP standard but uses the
card to signing/decryption. I stored my public key in
~/.password-store/.gpg-key file, which is in turn used for encryption.Here nothing changed related my daily  usage, except the number of time I am typing my  :)]]></content:encoded></item><item><title>This Week in Rust 586</title><link>https://this-week-in-rust.org/blog/2025/02/12/this-week-in-rust-586/</link><author>TWiR Contributors</author><category>This week in Rust</category><category>dev</category><category>rust</category><pubDate>Wed, 12 Feb 2025 05:00:00 +0000</pubDate><source url="https://this-week-in-rust.org/">This Week in Rust</source><content:encoded><![CDATA[This week's crate is esp32-mender-client, a client for ESP32 to execute firmware updates and remote commands.Thanks to Kelvin for the self-suggestion!An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.  The following
RFCs would benefit from user testing before moving forward:No calls for testing were issued this week.No calls for testing were issued this week.No calls for testing were issued this week.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.No Calls for participation were submitted this week.Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.No Calls for papers or presentations were submitted this week.A relatively neutral week, with lots of real changes but most small in
magnitude. Most significant change is rustdoc's move of JS/CSS minification to
build time which cut doc generation times on most benchmarks fairly
significantly.3 Regressions, 5 Improvements, 1 Mixed; 2 of them in rollups
32 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.No Cargo Tracking Issues or PRs entered Final Comment Period this week.No Language Team Proposals entered Final Comment Period this week.No Language Reference RFCs entered Final Comment Period this week.No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.Rusty Events between 2025-02-12 - 2025-03-12 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.Just because things are useful doesn't mean they are magically sound.]]></content:encoded></item><item><title>Microsoft Patch Tuesday, February 2025 Edition</title><link>https://krebsonsecurity.com/2025/02/microsoft-patch-tuesday-february-2025-edition/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Wed, 12 Feb 2025 04:58:37 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[ today issued security updates to fix at least 56 vulnerabilities in its Windows operating systems and supported software, including two zero-day flaws that are being actively exploited.All supported Windows operating systems will receive an update this month for a buffer overflow vulnerability that carries the catchy name CVE-2025-21418. This patch should be a priority for enterprises, as Microsoft says it is being exploited, has low attack complexity, and no requirements for user interaction. senior staff research engineer  noted that since 2022, there have been nine elevation of privilege vulnerabilities in this same Windows component — three each year — including one in 2024 that was exploited in the wild as a zero day (CVE-2024-38193).“CVE-2024-38193 was exploited by the North Korean APT group known as Lazarus Group to implant a new version of the FudModule rootkit in order to maintain persistence and stealth on compromised systems,” Narang said. “At this time, it is unclear if CVE-2025-21418 was also exploited by Lazarus Group.”The other zero-day, CVE-2025-21391, is an elevation of privilege vulnerability in Windows Storage that could be used to delete files on a targeted system. Microsoft’s advisory on this bug references something called “CWE-59: Improper Link Resolution Before File Access,” says no user interaction is required, and that the attack complexity is low., lead software engineer at , said although the advisory provides scant detail, and even offers some vague reassurance that ‘an attacker would only be able to delete targeted files on a system,’ it would be a mistake to assume that the impact of deleting arbitrary files would be limited to data loss or denial of service.“As long ago as 2022, ZDI researchers set out how a motivated attacker could parlay arbitrary file deletion into full SYSTEM access using techniques which also involve creative misuse of symbolic links,”Barnett wrote.One vulnerability patched today that was publicly disclosed earlier is CVE-2025-21377, another weakness that could allow an attacker to elevate their privileges on a vulnerable Windows system. Specifically, this is yet another Windows flaw that can be used to steal NTLMv2 hashes — essentially allowing an attacker to authenticate as the targeted user without having to log in.According to Microsoft, minimal user interaction with a malicious file is needed to exploit CVE-2025-21377, including selecting, inspecting or “performing an action other than opening or executing the file.”“This trademark linguistic ducking and weaving may be Microsoft’s way of saying ‘if we told you any more, we’d give the game away,'” Barnett said. “Accordingly, Microsoft assesses exploitation as more likely.”The SANS Internet Storm Center has a handy list of all the Microsoft patches released today, indexed by severity. Windows enterprise administrators would do well to keep an eye on askwoody.com, which often has the scoop on any patches causing problems.It’s getting harder to buy Windows software that isn’t also bundled with Microsoft’s flagship Copilot artificial intelligence (AI) feature. Last month Microsoft started bundling Copilot with , which Redmond has since rebranded as “.” Ostensibly to offset the costs of its substantial AI investments, Microsoft also jacked up prices from 22 percent to 30 percent for upcoming license renewals and new subscribers.Office-watch.com writes that existing Office 365 users who are paying an annual cloud license do have the option of “Microsoft 365 Classic,” an AI-free subscription at a lower price, but that many customers are not offered the option until they attempt to cancel their existing Office subscription.In other security patch news,  has shipped iOS 18.3.1, which fixes a zero day vulnerability (CVE-2025-24200) that is showing up in attacks. has issued security updates that fix a total of 45 vulnerabilities across , , , , ,  and . at  notes that  is shipping an update today which will trigger updates for Chromium based browsers including , so be on the lookout for Chrome and Edge updates as we proceed through the week.]]></content:encoded></item><item><title>The Murky Ad-Tech World Powering Surveillance of US Military Personnel</title><link>https://www.404media.co/eskimi-2/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/eskimi.png" length="" type=""/><pubDate>Wed, 12 Feb 2025 04:00:52 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[Last year, a revealed that a Florida-based data broker, Datastream Group, was selling highly sensitive location data that tracked United States military and intelligence personnel overseas. At the time, the origin of that data was unknown.Now, a letter sent to US senator Ron Wyden’s office that was obtained by an international collective of media outlets—including WIRED and 404 Media—claims that the ultimate source of that data was Eskimi, a little-known Lithuanian ad-tech company. Eskimi, meanwhile, denies it had any involvement.Eskimi’s alleged role—and its denials—highlight the opaque nature of the location data industry: A data broker in Florida claims a Lithuanian company provided data on US military personnel in Germany. That data could theoretically be sold to essentially anyone. But the exact ways in which the data is collected, compiled, and shared remains unclear. ]]></content:encoded></item><item><title>Risky Business #779 -- DOGE staffer linked to The Com</title><link>https://risky.biz/RB779/</link><author></author><category>Risky Business blog</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/media3.risky.biz/RB779.mp3" length="" type=""/><pubDate>Wed, 12 Feb 2025 03:18:48 +0000</pubDate><source url="https://risky.biz/">Risky Business</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TL;DW: Too Long; Didn&apos;t Watch Distill YouTube Videos to the Relevant Information</title><link>https://tldw.tube/</link><author>pkaeding</author><category>hn</category><pubDate>Wed, 12 Feb 2025 02:15:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>An ontology for threats, cybercrime and digital forensic investigation on Smart City Infrastructure, (Wed, Feb 12th)</title><link>https://isc.sans.edu/diary/rss/31676</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Wed, 12 Feb 2025 02:05:01 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[Blue teams have it hard – they maintain a watchful eye on whatever technology is deployed to detect threats, respond to incidents, perform digital forensics and reverse malware (or make malware happy!) when needed. Hopefully, no one has to handle all these roles alone since there is also the continuous learning aspect of getting up to speed with the latest threat vectors, vulnerabilities and exploit techniques. Adversaries only need one attack to succeed to gain actions on objective. In contrast, defenders have to detect and stop every attack to prevent adversaries from being successful. Let’s now extrapolate to an even bigger problem – what if this happens on emerging/future technologies and adversaries can get away with such crimes?Multiple countries are gradually considering the concept of Smart Cities, a key consideration in the United Nations Development Programme (UNDP). As such technologies are implemented, the responsibility of defending this critical infrastructure again falls on the shoulders of blue teams. Smart Cities have yet to be fully implemented, but it does not mean we should not be proactive in preparing defenders to handle future problems. Current issues, even without Smart Cities in the fray, already cause blue team grief (e.g. different technology platforms, different contexts, information sharing, collaboration and tool interoperability). Given these complexities, an ontology would allow a shared understanding of vocabulary, facilitate data sharing, and even enable automated data reasoning.Wanting to pre-emptively solve future issues of attacks and cybercrime on Smart City Infrastructure (SCI), I (along with my co-authors in the SUTD ASSET Group) set out to create the Smart City Ontological Paradigm Expression (SCOPE). SCOPE was designed to be an ontology for threats, cybercrime and digital forensic investigation on SCI. We did not create the ontology from scratch but chose to adhere to ontology best practices and extended the venerable Unified Cyber Ontology (UCO) [1] and Cyber-investigation Analysis Standard Expression (CASE) [2]. UCO and CASE have gained some traction, and these ontologies have been experimentally adopted in forensic tools such as Cellebrite, Magnet Forensics, and MSAB XRY [3]. However, UCO and CASE did not have any SCI representation, and expecting overwhelmed blue teams to create them from scratch would most certainly be the straw that broke the camel’s back. Smart City Infrastructure Definition (reproduced with permission from the authors) [4]We deliberated on several design factors. Firstly, we defined smart cities using a technology-agnostic approach while adhering to international standards (with reference to Figure 1) that adopted the United Nations (UN) Sustainable Development Goals (SDG) (this was done in a previous work) [4]. By doing so, we ensured that the evolution of technologies or vendors would not affect the fundamental principle of Smart Cities. Secondly, we identified possible threats, cybercrime, and digital forensic evidence sources from the Smart City, which were defined in the first step (also from the same previous work) [4]. Thirdly, we included MITRE ATT&CK techniques and MITRE CAPEC into SCOPE for analysts and investigators to provide additional context to forensic evidence. Finally, we followed the ontological style and design practices when creating SCOPE, an expansion profile from UCO and CASE.We evaluated SCOPE via real-world attack scenarios attributed to publicly reported real-world incidents attributed to Advanced Persistent Threat (APT) groups. With reference to Figure 2, the evaluation process workflow is shown. We successfully represented the attack scenarios, cybercrime committed, incident details, evidence and attack patterns (to name a few). SCOPE Evaluation Process (reproduced with permission from the authors) [3]Will SCOPE ever be helpful? Not yet. I hope it will come in handy in future for digital forensic investigators and law enforcement agencies when cybercrime on SCI becomes prevalent. As mentioned, SCOPE is technology-agnostic while adhering to several ISO standards. Additionally, it contains enough granularity to allow users to pinpoint key information while ensuring it can capture abstract definitions covering emerging technologies. We have made SCOPE publicly available to the digital forensic community to assist future smart city infrastructure investigations. SCOPE’s GitHub project link is https://github.com/scopeProject, and the official ontology website is https://scopeontology.org. If readers want to find out the complete details of SCOPE, you can find the full published paper in Volume 52 of Forensic Science International: Digital Investigation (FSIDI) or at https://doi.org/10.1016/j.fsidi.2025.301883.
1. https://www.scopus.com/record/display.uri?eid=2-s2.0-85021968557&origin=inward
2. https://doi.org/10.1016/j.diin.2017.08.002
3. https://doi.org/10.1016/j.fsidi.2025.301883
4. https://doi.org/10.1016/j.fsidi.2023.301540

 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>ISC Stormcast For Wednesday, February 12th, 2025 https://isc.sans.edu/podcastdetail/9320, (Wed, Feb 12th)</title><link>https://isc.sans.edu/diary/rss/31678</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Wed, 12 Feb 2025 02:00:02 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[
 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>Apache Iceberg now supports geospatial data types natively</title><link>https://wherobots.com/apache-iceberg-and-parquet-now-support-geo/</link><author>Mosarwat</author><category>hn</category><pubDate>Wed, 12 Feb 2025 01:34:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN</source><content:encoded><![CDATA[Geospatial data isn’t special anymore, and that’s a good thing.Geospatial solutions were thought of as “special”, because what modernized the data ecosystem of today, left geospatial data mostly behind. This changes today. Thanks to the efforts of the Apache Iceberg and Parquet communities, we are excited to share that both Iceberg and Parquet now support geometry and geography (collectively the GEO) data types.Geospatial data has been disconnected from the broader data ecosystem that modernized from open file formats like Apache Parquet, and open table formats like Apache Iceberg, Delta Lake, and Apache Hudi.The benefits of these cloud-native open file and table formats fueled widespread adoption of data lake and lakehouse architectures. Organizations moved away from the use of expensive proprietary systems, away from data siloes that coupled compute with storage and didn’t scale, and away from formats that locked them in and stifled innovation. Relative to legacy options, these cloud-native formats fundamentally change how data is stored, managed, and accessed. This in turn lowers costs, increases agency, and unlocks innovation over time. But because geospatial data was different, which led to a number of technical challenges, it wasn’t supported by these formats from the start. As a result developers building solutions with geospatial data struggled with fragmented formats, proprietary file types, and data siloes – making solutions harder and costlier to build.The silos will break downWith native geospatial data type support in Apache Iceberg and Parquet, you can seamlessly run query and processing engines like Wherobots, DuckDB, Apache Sedona, Apache Spark, Databricks, Snowflake, and BigQuery on your data. All the while benefitting from faster queries and lower storage costs from Parquet formatted data.These changes improve short and long term economics for geospatial solutions. Organizations will have a new freedom to innovate with a lower cost, highly interoperable architecture. They get to choose the best tool for the job over time without having to shuttle data between systems. Their costs reduce, productivity improves, innovation accelerates, and the playing field is leveled with respect to who can provide the best solution for their data. The legacy siloes will break down, just like they’ve done for non-geospatial data. And most importantly, these changes will lead to new innovation about our physical world. Benefits of Iceberg and ParquetThese changes make geospatial solutions based on a data lake a lot more attractive. Here are a few benefits. Iceberg and Parquet alone don’t separate compute from storage, but together they make it possible to utilize low cost data lake storage, along with multiple independent high performance computing solutions for different use casesACID transactions and data versioning enable the use of multiple compute engines without conflictsTime travel allows tracking of data changes over timeQuery performance is higher from features like column pruning, row-group filtering, and fast file accessOpen data formats minimize vendor lock-inGeospatial data will be supported across a broader ecosystem of tools and servicesIn the coming weeks, we will be covering these features in detail and demonstrate how they’re beneficial for geospatial solutions. Grassroots efforts made this happenThese changes were the result of grassroots initiatives, investment, and influence from community members at Planet, CARTO, Wherobots, and many others across the Cloud Native Geospatial community. This includes GeoParquet, which was a grassroots project and an extension of Parquet that proved its worth through use and popularity, countless meetups, and discussions. And we also want to give credit to the Iceberg community for working with members of the Wherobots team, to bring a solution forward while also influencing the Parquet community to make a GEO native data type.While Iceberg and Parquet communities led with support for GEO data types, we welcome compatibility and support for GEO data types in all cloud-native formats, including Apache Hudi and Delta Lake.Thoughts from Szehon Ho, Apache Iceberg PMC Member“The long-awaited incorporation of geospatial data types in the Iceberg V3 spec extends a core theme of Iceberg as a project to provide a universal ‘shared warehouse storage’ across many engines and users, and will now allow this huge, growing ecosystem to work on the same geospatial data as well, unlocking many exciting use cases. It is also a demonstration of Iceberg community’s willingness to take the time and ‘do hard things’, engaging in months of very active discussions across companies and OSS communities, finally reaching consensus on a spec that supports the largest variety of use cases in the fast-evolving geospatial data domain.”Thoughts from Chris Holmes, co-creator of Geo-parquet“The community developed and rallied behind GeoParquet to make geospatial data in Parquet fully interoperable and to let the geospatial world tap into all the advantages the big data world has been getting from Parquet. I’m very excited to see Parquet and Iceberg formally support geospatial types, and look forward to the acceleration in geospatial innovation that these changes will activate across industries and for our planet.”Committers are already working to bring support for these changes into Apache Sedona, and will notify the community as they are introduced. At Wherobots, we’ve supported these GEO datatypes in Havasu (our Iceberg fork) which we built to enable geospatial lakehouse architectures with Wherobots, along with GeoParquet. We’ve begun developing native support for Iceberg and Parquet into how Wherobots operates on customer data, and will put our full support behind these native formats moving forward.Over the coming days and weeks on this blog, we will dive deeper into GEO support in Iceberg and Parquet, provide you with examples of how to get started, how to migrate data, and how you can start developing modern solutions on a geospatial lakehouse.  If you need support throughout your journey adopting and utilizing these cloud-native formats for geospatial use, reach out to Apache Iceberg on Slack or Apache Sedona on Discord.Sign up for our newsletter to stay up to date with everything we are doing to enable the spatial community to embrace the modern geospatial lake-house.Jia Yu is the co-founder and Chief Architect of Wherobots. He also serves as the PMC Chair of Apache Sedona.Mo is the co-founder and CEO of Wherobots, and the co-creator of Apache Sedona.]]></content:encoded></item><item><title>This Week In React #221: State of React, React Router, Rsbuild, ProseMirror, Redux, React Hook Form, Base UI, RSC, React Admin | AI &amp; RN, Expo, Strict DOM, Polygen, Ignite, New Arch, Radon, macOS, Universal RSC, Gesture Handler | TypeScript, Prettier, esbuild...</title><link>https://thisweekinreact.com/newsletter/221</link><author></author><category>frontend</category><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><source url="https://thisweekinreact.com/newsletter">This week in react</source><content:encoded><![CDATA[Cyril and Matthieu from Theodo Apps here 👋, standing in for Seb to bring you the latest news from React and React Native!This week we’ve been carefully reading the State of React 2024 survey results, and also found quite a few interesting articles. Things have been quieter when it comes to new releases.Check our partner conf App.js, happening May 28-30 in 🇵🇱 Kraków! The biggest React Native and Expo-focused conference gives us a 15% discount with code "TWIR". Want to submit a talk? Hurry—CFP is closing soon!As always, thanks for supporting us on your favorite platform:When you wake up in the morning how many different yaml files do you want to manage? With EAS Workflows, you have .• Automate builds• Run e2e tests• Submit to stores• GitHub triggers, lint checks, slack notifications...so much moreWith EAS Workflows you can  all your builds/updates/submissions. Package up all the commands in a workflow and go drink a coffee. You deserve it.The results of State of React 2024 are out. It is a gold mine of information and we can’t possibly talk about everything here but here are a few highlights:Of the “new” React features, Server Components are the most popular with 40% of the ~7000 respondents having “used it”, probably thanks to Next.js… is the most disliked React API, and is going away!Have you heard about React’s Taint API? It looks like 80% of React devs have not, which is understandable since it’s experimental.There’s not that much movement in the #1 spot: Next.js, TanStack Query, Axios, Jest, and Zod are still the most frequently used.A few rising stars: Zustand and Vitest got more users, TanStack Start is #1 in interest even though almost no one has used it yet.Deploying Next.js apps on Vercel is easy, but optimizing hosting costs? That’s a different story.Join Chris Lojniewski and Jakub Dakowicz of Pagepro on  for a deep analysis of Vercel cost optimization strategies. Learn how to identify what drives your costs and reduce them with  like adjusting prefetching to avoid excessive function calls, choosing proper rendering methods, and trimming unnecessary data transfers.They’ll also show you how to minimize reliance on Vercel’s premium features with feature externalization - offloading analytics, image optimization, and password protection. Plus, they’ll break down when enterprise pricing is worth considering and when you’re better off with alternatives. .We don't talk too much about LLMs in this newsletter (we guess you already hear about it more than enough 😅) but with all the announcements this week we thought it was a good time to give you an update.The big news:  to add (React-)Native mobile apps support to Bolt. Right now you can preview your AI-generated apps on the web, but you need to download the code to go further. They plan to add an integration with EAS to deploy to production in the future.Some competitors also had things to announce:]]></content:encoded></item><item><title>Go 1.24 arrives</title><link>https://golangweekly.com/issues/542</link><author></author><category>dev</category><category>go</category><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><source url="https://golangweekly.com/">Golang Weekly</source><content:encoded><![CDATA[DB Fiddle is a handy online database 'sandbox' for playing with various versions of MySQL, Postgres, and SQLite direct from the browser.]]></content:encoded></item><item><title>Meta SAM 2.1 is now available in Amazon SageMaker JumpStart</title><link>https://aws.amazon.com/blogs/machine-learning/meta-sam-2-1-is-now-available-in-amazon-sagemaker-jumpstart/</link><author>Marco Punio</author><category>dev</category><category>ai</category><enclosure url="https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-17926/basketball-layup-masked.mp4" length="" type=""/><pubDate>Tue, 11 Feb 2025 23:09:11 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This blog post is co-written with George Orlin from Meta.Today, we are excited to announce that Meta’s Segment Anything Model (SAM) 2.1 vision segmentation model is publicly available through Amazon SageMaker JumpStart to deploy and run inference. Meta SAM 2.1 provides state-of-the-art video and image segmentation capabilities in a single model. This cutting-edge model supports long-context processing, complex segmentation scenarios, and fine-grained analysis, making it ideal for automating processes for various industries such as medical imaging in healthcare, satellite imagery for environment monitoring, and object segmentation for autonomous systems. Meta SAM 2.1 is well suited for zero-shot object segmentation and accurate object detection based on simple prompts such as point coordinates and bounding boxes in a frame for video tracking and image masking.This model was predominantly trained on AWS, and AWS will also be the first cloud provider to make it available to customers. In this post, we walk through how to discover and deploy the Meta SAM 2.1 model using SageMaker JumpStart.Meta SAM 2.1 is a state-of-the-art vision segmentation model designed for high-performance computer vision tasks, enabling advanced object detection and segmentation workflows. Building upon its predecessor, version 2.1 introduces enhanced segmentation accuracy, robust generalization across diverse datasets, and scalability for production-grade applications. These features enable AI researchers and developers in computer vision, image processing, and data-driven research to improve tasks that require detailed analysis segmentation across multiple fields.Meta SAM 2.1 has a streamlined architecture that is optimized for integration with popular model-serving frameworks like TorchServe and can be deployed on Amazon SageMaker AI to power real-time or batch inference pipelines. Meta SAM 2.1 empowers organizations to achieve precise segmentation outcomes in vision-centric workflows with minimal configuration and maximum efficiency.Meta SAM 2.1 offers multiple variants—Tiny, Small, Base Plus, and Large—available now on SageMaker JumpStart, balancing model size, speed, and segmentation performance to cater to diverse application needs.SageMaker JumpStart overviewSageMaker JumpStart offers access to a broad selection of publicly available foundation models (FMs). These pre-trained models serve as powerful starting points that can be deeply customized to address specific use cases. You can now use state-of-the-art model architectures, such as language models, computer vision models, and more, without having to build them from scratch.With SageMaker JumpStart, you can deploy models in a secure environment. Models hosted on JumpStart can be provisioned on dedicated SageMaker Inference instances, including AWS Trainium and AWS Inferentia based instances, and are isolated within your virtual private cloud (VPC). This enforces data security and compliance, because the models operate under your own VPC controls, rather than in a shared public environment. After deploying an FM, you can further customize and fine-tune it using the extensive capabilities of SageMaker AI, including SageMaker Inference for deploying models and container logs for improved observability. With SageMaker AI, you can streamline the entire model deployment process.Make sure you have the following prerequisites to deploy Meta SAM 2.1 and run inference:Discover Meta SAM 2.1 in SageMaker JumpStartSageMaker JumpStart provides FMs through two primary interfaces: SageMaker Studio and the SageMaker Python SDK. This provides multiple options to discover and use hundreds of models for your specific use case.SageMaker Studio is a comprehensive IDE that offers a unified, web-based interface for performing all aspects of the machine learning (ML) development lifecycle. From preparing data to building, training, and deploying models, SageMaker Studio provides purpose-built tools to streamline the entire process. In SageMaker Studio, you can access SageMaker JumpStart to discover and explore the extensive catalog of FMs available for deployment to inference capabilities on SageMaker Inference.You can access the SageMaker JumpStart UI through either Amazon SageMaker Unified Studio or SageMaker Studio. To deploy Meta SAM 2.1 using the SageMaker JumpStart UI, complete the following steps:In SageMaker Unified Studio, on the menu, choose .If you’re already on the SageMaker Studio console, choose  in the navigation pane.You will be prompted to create a project, after which you can begin deployment.Alternatively, you can use the SageMaker Python SDK to programmatically access and use SageMaker JumpStart models. This approach allows for greater flexibility and integration with existing AI/ML workflows and pipelines. By providing multiple access points, SageMaker JumpStart helps you seamlessly incorporate pre-trained models into your AI/ML development efforts, regardless of your preferred interface or workflow.Deploy Meta SAM 2.1 for inference using SageMaker JumpStartOn the SageMaker JumpStart landing page, you can discover the public pre-trained models offered by SageMaker AI. You can choose the Meta model provider tab to discover the Meta models available.If you’re using SageMaker Studio and don’t see the SAM 2.1 models, update your SageMaker Studio version by shutting down and restarting. For more information about version updates, refer to Shut down and Update Studio Classic Apps.You can choose the model card to view details about the model such as license, data used to train, and how to use. You can also find two buttons,  and , which help you use the model.When you choose , you should be prompted to the next screen to choose an endpoint name and instance type to initiate deployment.Upon defining your endpoint settings, you can proceed to the next step to use the model.Deploy Meta SAM 2.1 vision segmentation model for inference using the Python SDKWhen you choose , model deployment will start. Alternatively, you can deploy through the example notebook by choosing . The notebook provides end-to-end guidance on how to deploy the model for inference and clean up resources.To deploy using a notebook, you start by selecting an appropriate model, specified by the . You can deploy any of the selected models on SageMaker AI.You can deploy a Meta SAM 2.1 vision segmentation model using SageMaker JumpStart with the following SageMaker Python SDK code:from sagemaker.jumpstart.model import JumpStartModel 
model = JumpStartModel(model_id = "meta-vs-sam-2-1-hiera-tiny") 
predictor = model.deploy()This deploys the model on SageMaker AI with default configurations, including default instance type and default VPC configurations. You can change these configurations by specifying non-default values in JumpStartModel. After it’s deployed, you can run inference against the deployed endpoint through the SageMaker predictor. There are three tasks that are available with this endpoint: automatic mask generator, image predictor, and video predictor. We provide a code snippet for each later in this post. To use the predictor, a certain payload schema needs to be followed. The endpoint has sticky sessions enabled, so to start inference, you need to send a  payload:def start_session(asset_type, asset_path):

    asset_base64 = None
    
     with open(image_path, 'rb') as f:
            asset_base64 = base64.b64encode(f.read()).decode('utf-8')
    
    response = predictor.invoke_endpoint(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=json.dumps({
                    "type": "start_session",
                    "input_type": asset_type,
                    "path": asset_base64 
                }),
        SessionId="NEW_SESSION",
    )
    
    session_id = response.headers.get("x-amzn-sagemaker-new-session-id")
    
    return session_id
The  invocation needs an input media type of either image or video and the base64 encoded data of the media. This will launch a session with an instance of the model and load the media to be segmented.To close a session, send a  invocation:def close_session(session_id):
    response = predictor.invoke_endpoint(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=json.dumps({
                    "type": "close_session",
                    "session_id": session_id
                }),
        SessionId=session_id,
    )
    
    session_id = response.headers.get("x-amzn-sagemaker-closed-session-id")
    
    return session_id
If x-amzn-sagemaker-closed-session-id exists as a header, then the session has been successfully closed.To continue a session and retrieve the session ID of the existing session, the response header will have the x-amzn-sagemaker-session-id key with the current session ID for any operation that is not  or . Operations that aren’t  or  need to be invoked with a response stream. This is due to the size of the resulting payload being larger than what SageMaker real-time endpoints can return.This is a basic example of interacting with the SAM 2.1 SageMaker JumpStart endpoint with sticky sessions. The following examples for each of the tasks reference these operations without repeating them. The returned data is of mime type JSONL. For more complete examples, refer to the example notebooks for Meta SAM 2.1 on SageMaker Jumpstart.Recommended instances and benchmarksThe following table lists all the Meta SAM 2.1 models available in SageMaker JumpStart along with the , default instance types, and maximum number of total tokens (sum of number of input tokens and number of generated tokens) supported for each of these models. For increased context length, you can modify the default instance type in the SageMaker JumpStart UI.meta-vs-sam-2-1-hiera-tinyml.g6.24xlarge (5.5 MB total image or video size)meta-vs-sam-2-1-hiera-smallml.g6.24xlarge (5.5 MB total image or video size)meta-vs-sam-2-1-hiera-base-plusml.g6.24xlarge (5.5 MB total image or video size)meta-vs-sam-2-1-hiera-largeml.g6.24xlarge (5.5 MB total image or video size)Meta SAM 2.1 use cases: Inference and prompt examplesAfter you deploy the model using SageMaker JumpStart, you should be able to see a reference Jupyter notebook that references the parser and helper functions needed to begin using Meta SAM 2.1. After you follow those cells in the notebook, you should be ready to begin using the model’s vision segmentation capabilities.Meta SAM 2.1 offers support for three different tasks (automatic mask generator, image predictor, video predictor) to generate masks for various objects in images, including object tracking in videos. In the following examples, we demonstrate how to use the automatic mask generator and image predictor on a JPG of a truck. This  file is stored in the  bucket; you can access it with the following code:s3_bucket = f"jumpstart-cache-prod-{region}"
key_prefix = "inference-notebook-assets"

def download_from_s3(key_filenames):
    for key_filename in key_filenames:
        s3.download_file(s3_bucket, f"{key_prefix}/{key_filename}", key_filename)
        
truck_jpg = "truck.jpg"

#Download images.
download_from_s3(key_filenames=[truck_jpg])
display(Image(filename=truck_jpg))
After you have your image and it is encoded, you can create masks for objects in the image. For use cases where you want to generate masks for every object in the image, you can use the automatic mask generator task.The automatic mask generator is great for AI researchers for computer vision tasks and applications such as medical imaging and diagnostics to automatically segment regions of interest like tumors or specific organs to provide more accurate diagnostic support. Additionally, the automatic mask generator can be particularly useful in the autonomous vehicle space, in which it can segment out elements in a camera like pedestrians, vehicles, and other objects. Let’s use the automatic mask generator to generate masks for all the objects in .The following code is the prompt to generate masks for your base64 encoded image:# Start session
session_id = start_session("image", truck_jpg)
    
# Generate and visualize masks with basic parameters
response = runtime_client.invoke_endpoint_with_response_stream(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=json.dumps({
            "type": "generate_automatic_masks",
            "session_id": session_id,
            "points_per_side": 32,
            "min_mask_region_area": 100
        }),
        SessionId=session_id,
        Accept="application/jsonlines"
    )
    
# Parse response stream
parser = StreamParser()
for event in response['Body']:
    parser.write(event)

masks = parser.get_responses()

# End session
end_session(session_id)
We receive the following output (parsed and visualized).Additionally, you can choose which objects in the provided image you want to create a mask for by adding points within that object for Meta SAM 2.1 to create. A use case for the image predictor can be valuable for tasks related to design and modeling by automating processes that typically require manual efforts. For example, the image predictor can automate turning 2D images into 3D models by analyzing 2D images of blueprints, sketches, or floor plans and generating preliminary 3D models. This is one of many examples of how the image predictor can act as a bridge between 2D and 3D construction across many different tasks. We use the following image with the points that we used to prompt Meta SAM 2.1 for masking the object.The following code is used to prompt Meta SAM 2.1 and plot the coordinates:# Start session
session_id = start_session("image", truck_jpg)

points = [
            {"type": "point", "coordinates": [500, 375], "label": 1},
            {"type": "point", "coordinates": [1125, 625], "label": 1}
         ]
    
# Add multiple points
response = runtime_client.invoke_endpoint_with_response_stream(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=json.dumps({
            "type": "add_points",
            "session_id": session_id,
            "points": [p["coordinates"] for p in points],
            "labels": [p["label"] for p in points],
            "clear_old_points": clear_old_point,
        }),
        SessionId=session_id,
        Accept="application/jsonlines"
    )

# Parse response stream
parser = StreamParser()
for event in response['Body']:
    parser.write(event)

# Intermediate Response
masks = parser.get_responses()
    
response = runtime_client.invoke_endpoint_with_response_stream(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=json.dumps({
            "type": "predict",
            "session_id": session_id,
            "multimask_output": True,
            "return_logits": True
        }),
        SessionId=session_id,
        Accept="application/jsonlines"
    )

# Parse response stream
parser = StreamParser()
for event in response['Body']:
    parser.write(event)

masks = parser.get_responses()

# End session
end_session(session_id)
We receive the following output (parsed and visualized).We now demonstrate how to prompt Meta SAM 2.1 for object tracking on video. One use case would be for ergonomic data collection and training purposes. You can use the video predictor to analyze the movement and posture of humans in real time, serving as a way to reduce injury and improve performance by setting alarms for bad posture or movements. Let’s start by accessing the  file [1] from the  S3 bucket defined in the following code:basketball_mp4 = "basketball-layup.mp4"

#Download video
download_from_s3(key_filenames=[basketball_mp4])
display(Video(filename=basketball_mp4))The following code shows how you can set up the prompt format to track objects in the video. The first object will use coordinates to track and not track, and the second object will track one coordinate.# Start session
session_id = start_session("video", basketball_mp4)

# Object 1
prompts1 = [
        {"type": "point", "coordinates": [1478, 649], "label": 1},
        {"type": "point", "coordinates": [1433, 689], "label": 0},
    ]
    
# Extract points and labels
points = []
labels = []
for prompt in prompts1:
    if prompt["type"] == "point":
        points.append(prompt["coordinates"])
        labels.append(prompt["label"])

request = {
        "type": "add_points",
        "session_id": session_id,
        "frame_index": 0,
        "object_id": 1,
        "points": points,
        "labels": labels,
        "clear_old_points": True,
    }
    
# Add multiple points
response = runtime_client.invoke_endpoint_with_response_stream(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=json.dumps(request),
        SessionId=session_id,
        Accept="application/jsonlines"
    )

# Parse response stream
parser = StreamParser()
for event in response['Body']:
    parser.write(event)

# Intermediate Response
masks = parser.get_responses()

# Object 2
prompts2 = [{"type": "point", "coordinates": [1433, 689], "label": 1}]

# Extract points and labels
points = []
labels = []
for prompt in prompts2:
    if prompt["type"] == "point":
        points.append(prompt["coordinates"])
        labels.append(prompt["label"])

request = {
        "type": "add_points",
        "session_id": session_id,
        "frame_index": 0,
        "object_id": 2,
        "points": points,
        "labels": labels,
        "clear_old_points": True,
    }
    
# Add multiple points
response = runtime_client.invoke_endpoint_with_response_stream(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=json.dumps(request),
        SessionId=session_id,
        Accept="application/jsonlines"
    )

# Parse response stream
parser = StreamParser()
for event in response['Body']:
    parser.write(event)

# Intermediate Response
masks = parser.get_responses()
    
response = runtime_client.invoke_endpoint_with_response_stream(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=json.dumps({
            "type": "propagate_in_video",
            "session_id": session_id,
            "start_frame_index": 0,
        }),
        SessionId=session_id,
        Accept="application/jsonlines"
    )

# Parse response stream
parser = StreamParser()
for event in response['Body']:
    parser.write(event)

masks = parser.get_responses()

# End session
end_session(session_id)
We receive the following output (parsed and visualized).Here we can see that Meta SAM 2.1 Tiny was able to successfully track the objects based off the coordinates that were provided in prompt.To avoid incurring unnecessary costs, when you’re done, delete the SageMaker AI endpoints using the following code:predictor.delete_model()
predictor.delete_endpoint()Alternatively, to use the SageMaker AI console, complete the following steps:On the SageMaker AI console, under in the navigation pane, chooseSearch for the embedding and text generation endpoints.On the endpoint details page, choose .Choose  again to confirm.In this post, we explored how SageMaker JumpStart empowers data scientists and ML engineers to discover, access, and deploy a wide range of pre-trained FMs for inference, including Meta’s most advanced and capable models to date. Get started with SageMaker JumpStart and Meta SAM 2.1 models today. For more information about SageMaker JumpStart, see SageMaker JumpStart pretrained models and Getting started with Amazon SageMaker JumpStart.Marco Punio is a Sr. Specialist Solutions Architect focused on generative AI strategy, applied AI solutions, and conducting research to help customers hyper-scale on AWS. As a member of the 3rd Party Model Provider Applied Sciences Solutions Architecture team at AWS, he is a Global Lead for the Meta – AWS Partnership and technical strategy. Based in Seattle, WA, Marco enjoys writing, reading, exercising, and building applications in his free time.Deepak Rupakula is a Principal GTM lead in the specialists group at AWS. He focuses on developing GTM strategy for large language models like Meta across AWS services like Amazon Bedrock and Amazon SageMaker AI. With over 15 years of experience in the tech industry, his experience includes leadership roles in product management, customer success, and analytics.Harish Rao is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He empowers customers to harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices.Baladithya Balamurugan is a Solutions Architect at AWS focused on ML deployments for inference and using AWS Neuron to accelerate training and inference. He works with customers to enable and accelerate their ML deployments on services such as Amazon SageMaker AI and Amazon EC2. Based in San Francisco, Baladithya enjoys tinkering, developing applications, and building his homelab in his free time.Banu Nagasundaram leads product, engineering, and strategic partnerships for Amazon SageMaker JumpStart, SageMaker AI’s machine learning and generative AI hub. She is passionate about building solutions that help customers accelerate their AI journey and unlock business value.Naman Nandan is a software development engineer at AWS, specializing in enabling large-scale AI/ML inference workloads on Amazon SageMaker AI using TorchServe, a project jointly developed by AWS and Meta. In his free time, he enjoys playing tennis and going on hikes.]]></content:encoded></item><item><title>EFF Sues OPM, DOGE and Musk for Endangering the Privacy of Millions</title><link>https://www.eff.org/press/releases/eff-sues-opm-doge-and-musk-endangering-privacy-millions</link><author>Josh Richman</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/opm-eye-3b.jpg" length="" type=""/><pubDate>Tue, 11 Feb 2025 22:34:50 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Mapping the Unix Magic Poster – An Interactive Annotation Project</title><link>https://drio.github.io/unixmagic/</link><author>drio</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 22:22:37 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I built this as a static site that lets us annotate the Unix Magic poster by placing markers on references and adding descriptions to explain them. I've added a few so far, but there's much more to document.What I love about this approach is that contributions happen not just on the site itself but also through PRs, where we can discuss and refine the details of each reference. Feel free to send a PR!Would love feedback, suggestions, and PRs from the community!]]></content:encoded></item><item><title>Falcon 3 models now available in Amazon SageMaker JumpStart</title><link>https://aws.amazon.com/blogs/machine-learning/falcon-3-models-now-available-in-amazon-sagemaker-jumpstart/</link><author>Niithiyn Vijeaswaran</author><category>dev</category><category>ai</category><pubDate>Tue, 11 Feb 2025 22:16:27 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Overview of the Falcon 3 family of modelsThe Falcon 3 family, developed by Technology Innovation Institute (TII) in Abu Dhabi, represents a significant advancement in open source language models. This collection includes five base models ranging from 1 billion to 10 billion parameters, with a focus on enhancing science, math, and coding capabilities. The family consists of Falcon3-1B-Base, Falcon3-3B-Base, Falcon3-Mamba-7B-Base, Falcon3-7B-Base, and Falcon3-10B-Base along with their instruct variants.These models showcase innovations such as efficient pre-training techniques, scaling for improved reasoning, and knowledge distillation for better performance in smaller models. Notably, the Falcon3-10B-Base model achieves state-of-the-art performance for models under 13 billion parameters in zero-shot and few-shot tasks. The Falcon 3 family also includes various fine-tuned versions like Instruct models and supports different quantization formats, making them versatile for a wide range of applications.Currently, SageMaker JumpStart offers the base versions of Falcon3-3B, Falcon3-7B, and Falcon3-10B, along with their corresponding instruct variants, as well as Falcon3-1B-Instruct.Get started with SageMaker JumpStartSageMaker JumpStart is a machine learning (ML) hub that can help accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select pre-trained foundation models (FMs), including Falcon 3 models. These models are fully customizable for your use case with your data.Deploying a Falcon 3 model through SageMaker JumpStart offers two convenient approaches: using the intuitive SageMaker JumpStart UI or implementing programmatically through the SageMaker Python SDK. Let’s explore both methods to help you choose the approach that best suits your needs.Deploy Falcon 3 using the SageMaker JumpStart UIComplete the following steps to deploy Falcon 3 through the JumpStart UI:Deploy Falcon 3 programmatically using the SageMaker Python SDKFor teams looking to automate deployment or integrate with existing MLOps pipelines, you can use the SageMaker Python SDK:from sagemaker.serve.builder.model_builder import ModelBuilder
from sagemaker.serve.builder.schema_builder import SchemaBuilder
from sagemaker.jumpstart.model import ModelAccessConfig
from sagemaker.session import Session
import logging

sagemaker_session = Session()

artifacts_bucket_name = sagemaker_session.default_bucket()
execution_role_arn = sagemaker_session.get_caller_identity_arn()


js_model_id = "huggingface-llm-falcon-3-10B-base"

gpu_instance_type = "ml.g5.12xlarge"  

response = "Hello, I'm a language model, and I'm here to help you with your English."

sample_input = {
    "inputs": "Hello, I'm a language model,",
    "parameters": {"max_new_tokens": 128, "top_p": 0.9, "temperature": 0.6},
}

sample_output = [{"generated_text": response}]

schema_builder = SchemaBuilder(sample_input, sample_output)

model_builder = ModelBuilder(
    model=js_model_id,
    schema_builder=schema_builder,
    sagemaker_session=sagemaker_session,
    role_arn=execution_role_arn,
    log_level=logging.ERROR
)

model= model_builder.build()

predictor = model.deploy(model_access_configs={js_model_id:ModelAccessConfig(accept_eula=True)}, accept_eula=True)Run inference on the predictor:predictor.predict(sample_input)To clean up the model and endpoint, use the following code:predictor.delete_model()
predictor.delete_endpoint() is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is generative AI and AWS AI Accelerators. He holds a Bachelor’s degree in Computer Science and Bioinformatics. is an ML Architect with the Amazon SageMaker Service team. He focuses on helping customers design, deploy, and manage ML workloads at scale. In his spare time, he enjoys traveling and exploring new places. is a Senior ML Solutions Architect with the Amazon SageMaker Service team. He focuses on helping customers build, deploy, and migrate ML production workloads to SageMaker at scale. He specializes in machine learning, AI, and computer vision domains, and holds a master’s degree in Computer Science from UT Dallas. In his free time, he enjoys traveling and photography. leads product, engineering, and strategic partnerships for SageMaker JumpStart, SageMaker’s machine learning and GenAI hub. She is passionate about building solutions that help customers accelerate their AI journey and unlock business value.]]></content:encoded></item><item><title>New hack uses prompt injection to corrupt Gemini’s long-term memory</title><link>https://arstechnica.com/security/2025/02/new-hack-uses-prompt-injection-to-corrupt-geminis-long-term-memory/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2023/12/gemini_header-1152x648.jpg" length="" type=""/><pubDate>Tue, 11 Feb 2025 22:13:42 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[In the nascent field of AI hacking, indirect prompt injection has become a basic building block for inducing chatbots to exfiltrate sensitive data or perform other malicious actions. Developers of platforms such as Google's Gemini and OpenAI's ChatGPT are generally good at plugging these security holes, but hackers keep finding new ways to poke through them again and again.On Monday, researcher Johann Rehberger demonstrated a new way to override prompt injection defenses Google developers have built into Gemini—specifically, defenses that restrict the invocation of Google Workspace or other sensitive tools when processing untrusted data, such as incoming emails or shared documents. The result of Rehberger’s attack is the permanent planting of long-term memories that will be present in all future sessions, opening the potential for the chatbot to act on false information or instructions in perpetuity.More about the attack later. For now, here is a brief review of indirect prompt injections: Prompts in the context of large language models (LLMs) are instructions, provided either by the chatbot developers or by the person using the chatbot, to perform tasks, such as summarizing an email or drafting a reply. But what if this content contains a malicious instruction? It turns out that chatbots are so eager to follow instructions that they often take their orders from such content, even though there was never an intention for it to act as a prompt.]]></content:encoded></item><item><title>The TAKE IT DOWN Act: A Flawed Attempt to Protect Victims That Will Lead to Censorship</title><link>https://www.eff.org/deeplinks/2025/02/take-it-down-act-flawed-attempt-protect-victims-will-lead-censorship</link><author>Joe Mullin</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/tossed-2_0.jpg" length="" type=""/><pubDate>Tue, 11 Feb 2025 21:01:38 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[The Bill Will Lead To Overreach and Censorship Threatens Encrypted Services Victims Of NCII Have Legal Options Under Existing Law]]></content:encoded></item><item><title>GenAI Patterns: Query Rewriting</title><link>https://martinfowler.com/articles/gen-ai-patterns/#query-rewrite</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 11 Feb 2025 20:58:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[Users often have difficulty writing the most effective queries.
       and I explain Query Rewriting:
      getting an LLM to formulate alternative queries to send to a RAG's
      retriever. ]]></content:encoded></item><item><title>Building a virtual meteorologist using Amazon Bedrock Agents</title><link>https://aws.amazon.com/blogs/machine-learning/building-a-virtual-meteorologist-using-amazon-bedrock-agents/</link><author>Salman Ahmed</author><category>dev</category><category>ai</category><pubDate>Tue, 11 Feb 2025 20:53:04 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[The integration of generative AI capabilities is driving transformative changes across many industries. Although weather information is accessible through multiple channels, businesses that heavily rely on meteorological data require robust and scalable solutions to effectively manage and use these critical insights and reduce manual processes. This solution demonstrates how to create an AI-powered virtual meteorologist that can answer complex weather-related queries in natural language. We use various AWS services to deploy a complete solution that you can use to interact with an API providing real-time weather information. In this solution, we use Amazon Bedrock Agents.Amazon Bedrock Agents helps to streamline workflows and automate repetitive tasks. Amazon Bedrock Agents can securely connect to your company’s data sources and augments the user’s request with accurate responses. You can use Amazon Bedrock Agents to architect an action schema tailored to your requirements, granting you control whenever the agent initiates the specified action. This versatile approach equips you to seamlessly integrate and execute business logic within your preferred backend service, fostering a cohesive combination of functionality and flexibility. There is also memory retention across the interaction allowing a more personalized user experience.In this post, we present a streamlined approach to deploying an AI-powered agent by combining Amazon Bedrock Agents and a foundation model (FM). We guide you through the process of configuring the agent and implementing the specific logic required for the virtual meteorologist to provide accurate weather-related responses. Additionally, we use various AWS services, including AWS Amplify for hosting the front end, AWS Lambda functions for handling request logic, Amazon Cognito for user authentication, and AWS Identity and Access Management (IAM) for controlling access to the agent.The diagram gives an overview and highlights the key components. The architecture uses Amazon Cognito for user authentication and Amplify as the hosting environment for our front-end application. Amazon Bedrock Agents forwards the details from the user query to the action groups, which further invokes custom Lambda functions. Each action group and Lambda function handles a specific task: – Processes geographic coordinates (geo-coordinates) to get details about a specific location Gathers weather information for the provided location Obtains the current date and timeYou must have the following in place to complete the solution in this post:Deploy solution resources using AWS CloudFormationWhen you run the AWS CloudFormation template, the following resources are deployed (note that costs will be incurred for the AWS resources used):Lambda resources: 
  Function – <Stack name>-geo-coordinates-<auto-generated>Function – <Stack name>-weather-<auto-generated>Function – <Stack name>-date-time-<auto-generated>Amazon Bedrock Agents: virtual-meteorologist 
  Action groups (1) – obtain-latitude-longitude-from-place-nameAction groups (2) – obtain-weather-information-with-coordinatesAction groups (3) – get-current-date-time-from-timezoneAfter you deploy the CloudFormation template, copy the following from the tab on the CloudFormation console to be used during the configuration of your application after it’s deployed in AWS Amplify.Deploy the AWS Amplify applicationYou need to manually deploy the Amplify application using the front-end code found on GitHub. Complete the following steps:Download the front-end code AWS-Amplify-Frontend.zip from GitHub.Use the .zip file to manually deploy the application in Amplify.Return to the Amplify page and use the domain it automatically generated to access the application.Use Amazon Cognito for user authenticationAmazon Cognito is an identity service that you can use to authenticate and authorize users. We use Amazon Cognito in our solution to verify the user before they can use the application. We also use identity pool to provide temporary AWS credentials for the user while they interact with Amazon Bedrock API.Use Amazon Bedrock Agents to automate application tasksWith Amazon Bedrock Agents, you can build and configure autonomous agents in your application. An agent helps your end users complete actions based on organization data and user input. Agents orchestrate interactions between FMs, data sources, software applications, and user conversations.Use action group to define actions that Amazon Bedrock agents performAn action group defines a set of related actions that an Amazon Bedrock agent can perform to assist users. When configuring an action group, you have options for handling user-provided information, including adding user input to the agent’s action group, passing data to a Lambda function for custom business logic, or returning control directly through the InvokeAgent response. In our application, we created three action groups to give the Amazon Bedrock agent these essential functionalities: retrieving coordinates for specific locations, obtaining current date and time information, and fetching weather data for given locations. These action groups enable the agent to access and process crucial information, enhancing its ability to respond accurately and comprehensively to user queries related to location-based services and weather conditions.Use Lambda for Amazon Bedrock action groupAs part of this solution, three Lambda functions are deployed to support the action groups defined for our Amazon Bedrock agent:Location coordinates Lambda function – This function is triggered by the obtain-latitude-longitude-from-place-name action group. It takes a place name as input and returns the corresponding latitude and longitude coordinates. The function uses a geocoding service or database to perform this lookup.Date and time Lambda function – Invoked by the get-current-date-time-from-timezone action group, this function provides the current date and time information.Weather information Lambda function – This function is called by the obtain-weather-information-with-coordinates action group. It accepts geo-coordinates from the first Lambda function and returns current weather conditions and forecasts for the specified area. This Lambda function used a weather API to fetch up-to-date meteorological data.Each of these Lambda functions receives an input event containing relevant metadata and populated fields from the Amazon Bedrock agent’s API operation or function parameters. The functions process this input, perform their specific tasks, and return a response with the required information. This response is then used by the Amazon Bedrock agent to formulate its reply to the user’s query. By using these Lambda functions, our Amazon Bedrock agent gains the ability to access external data sources and perform complex computations, significantly enhancing its capabilities in handling user requests related to location, time, and weather information.Use AWS Amplify for front-end codeAmplify offers a development environment for building secure, scalable mobile and web applications. Developers can focus on their code rather than worrying about the underlying infrastructure. Amplify also integrates with many Git providers. For this solution, we manually upload our front-end code using the method outlined earlier in this post.Navigate to the URL provided after you created the application in Amplify. Upon accessing the application URL, you’ll be prompted to provide information related to Amazon Cognito and Amazon Bedrock Agents. This information is required to securely authenticate users and allow the front end to interact with the Amazon Bedrock agent. It enables the application to manage user sessions and make authorized API calls to AWS services on behalf of the user.You can enter information with the values you collected from the CloudFormation stack outputs. You’ll be required to enter the following fields, as shown in the following screenshot:You need to sign in with your username and password. A temporary password was automatically generated during deployment and sent to the email address you provided when launching the CloudFormation template. At first sign-in attempt, you’ll be asked to reset your password, as shown in the following video.Now you can start asking questions in the application, for example, “Can we do barbecue today in Dallas, TX?” In a few seconds, the application will provide you detailed results mentioning if you can do barbecue in Dallas, TX. The following video shows this chat.Here are a few sample queries to demonstrate the capabilities of your virtual meteorologist:“What’s the weather like in New York City today?”“Should I plan an outdoor birthday party in Miami next weekend?”“Will it snow in Denver on Christmas Day?”“Can I go swimming on a beach in Chicago today?These queries showcase the agent’s ability to provide current weather information, offer advice based on weather forecasts, and predict future weather conditions. You can even ask a question related to an activity such as swimming, and it will answer based on the weather conditions if that activity is okay to do.If you decide to discontinue using the virtual meteorologist, you can follow these steps to remove it, its associated resources deployed using AWS CloudFormation, and the Amplify deployment:Delete the CloudFormation stack: 
  On the AWS CloudFormation console, choose  in the navigation pane.Locate the stack you created during the deployment process (you assigned a name to it).Select the stack and choose .Delete the Amplify application and its resources. For instructions, refer to Clean Up Resources.This solution demonstrates the power of combining Amazon Bedrock Agents with other AWS services to create an intelligent, conversational weather assistant. By using AI and cloud technologies, businesses can automate complex queries and provide valuable insights to their users.To learn more about Amazon Bedrock, refer to the following resources:To learn more about the Anthropic’s Claude 3.5 Sonnet model, refer to the following resources: is a Senior Technical Account Manager in AWS Enterprise Support. He enjoys helping customers in the travel and hospitality industry to design, implement, and support cloud infrastructure. With a passion for networking services and years of experience, he helps customers adopt various AWS networking services. Outside of work, Salman enjoys photography, traveling, and watching his favorite sports teams. is a Senior Enterprise Support Lead at AWS, helping energy customers design and optimize cloud solutions. With a passion for software development, he guides energy customers through AWS service adoption. Outside work, Sergio is a multi-instrument musician playing guitar, piano, and drums, and he also practices Wing Chun Kung Fu. is a Senior Technical Account Manager in AWS Enterprise Support who helps customers in the travel and hospitality industry to streamline their cloud operations on AWS. He is a results-driven IT professional with over 20 years of experience. In his free time, Ravi enjoys creative activities like painting. He also likes playing cricket and traveling to new places. is a Enterprise Support Lead in AWS Enterprise Support who helps customers streamline their cloud operations on AWS. He is a results-driven IT professional with over 20 years of experience.]]></content:encoded></item><item><title>Microsoft February 2025 Patch Tuesday, (Tue, Feb 11th)</title><link>https://isc.sans.edu/diary/rss/31674</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Tue, 11 Feb 2025 20:02:21 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[This month, Microsoft has released patches addressing a total of 141 vulnerabilities. Among these, 4 are classified as critical, highlighting the potential for significant impact if exploited. Notably, 2 vulnerabilities are currently being exploited in the wild, underscoring the urgency for immediate updates. Additionally, 1 vulnerability has been disclosed prior to this patch cycle, marking it as a zero-day. Users are strongly advised to prioritize these updates to safeguard their systems against potential threats.Significant Vulnerabilities Windows Ancillary Function Driver for WinSock Elevation of Privilege Vulnerability (CVE-2025-21418)
This vulnerability, identified as CVE-2025-21418, has a severity rating of Important with a CVSS score of 7.8. It is currently being exploited in the wild but has not been publicly disclosed, making it a significant concern for affected systems. The vulnerability allows an attacker to gain SYSTEM privileges, thereby elevating their access and control over the compromised system. Immediate attention and remediation are advised to mitigate the risk posed by this vulnerability.
This is a disclosed vulnerability with a severity rating of Important and a CVSS score of 7.1, which is currently being exploited in the wild. This vulnerability allows an attacker to elevate their privileges to delete targeted files on a system, significantly impacting the integrity and availability of the system without compromising confidentiality. The exploitation of this vulnerability can lead to the deletion of critical data, potentially rendering services unavailable. Despite its exploitation, it has not been publicly disclosed as a zero-day, and users are advised to implement appropriate security measures to mitigate its impact.
This is a disclosed zero-day vulnerability with a severity rating of Important and a CVSS score of 6.5, though it is not currently exploited in the wild. This vulnerability can lead to a total loss of confidentiality by allowing an attacker to obtain a user's NTLMv2 hash, which could be used to authenticate as the user. Exploitation requires minimal user interaction, such as selecting or inspecting a malicious file. It affects all supported versions of Microsoft Windows, and despite the retirement of Internet Explorer 11 and the deprecation of Microsoft Edge Legacy, updates are necessary due to the continued use of the MSHTML and EdgeHTML platforms in various applications. To ensure full protection, users are advised to install both Security Only updates and IE Cumulative updates.Microsoft Dynamics 365 Sales Elevation of Privilege Vulnerability (CVE-2025-21177)
This vulnerability, identified as CVE-2025-21177, has not been exploited in the wild nor disclosed publicly, classifying it as a non-zero-day. It carries a severity rating of Critical with a CVSS score of 8.7, indicating a significant risk of elevation of privilege if exploited. Although the vulnerability could potentially allow attackers to gain unauthorized access and elevate their privileges within the Microsoft Dynamics 365 Sales environment, Microsoft has fully mitigated the issue, requiring no action from users. This CVE serves to enhance transparency regarding cloud service vulnerabilities.Windows Lightweight Directory Access Protocol (LDAP) Remote Code Execution Vulnerability (CVE-2025-21376)
This is a critical vulnerability with a CVSS score of 8.1, which has not been exploited in the wild nor disclosed publicly, thus not classified as a zero-day. This vulnerability allows for remote code execution, posing a significant threat if exploited. An unauthenticated attacker could exploit this vulnerability by sending a specially crafted request to a vulnerable LDAP server, potentially causing a buffer overflow. The attack complexity is high, as successful exploitation requires the attacker to win a race condition. Mitigation efforts should focus on securing LDAP servers and monitoring for unusual activity to prevent potential exploitation.
This vulnerability, identified as CVE-2025-21381, has not been exploited in the wild nor disclosed publicly, making it a non-zero-day threat. It carries a severity rating of Critical with a CVSS score of 7.8, indicating a significant risk of remote code execution. Despite the CVSS metric indicating a local attack vector, the vulnerability allows an attacker to execute code remotely by convincing a user, through social engineering, to download and open a specially crafted file. The attack can be executed locally, with the Preview Pane serving as a potential attack vector. Users are advised to exercise caution when opening files from untrusted sources and to apply any available security updates to mitigate this risk.DHCP Client Service Remote Code Execution Vulnerability (CVE-2025-21379)
This vulnerability, identified as CVE-2025-21379, has not been exploited in the wild nor disclosed publicly, classifying it as a non-zero-day threat. It carries a severity rating of Critical with a CVSS score of 7.1, indicating a significant risk of remote code execution. The vulnerability requires a high attack complexity, necessitating a machine-in-the-middle (MITM) attack where the attacker must intercept the logical network path between the target and the resource. The attack vector is adjacent, meaning it is limited to systems on the same network segment, such as those connected to the same network switch or virtual network. This limitation prevents the attack from being executed across multiple networks, such as a WAN.Microsoft High Performance Compute (HPC) Pack Remote Code Execution Vulnerability (CVE-2025-21198)
is a critical security flaw with a CVSS score of 9.0, rated as Important, and is currently neither exploited in the wild nor publicly disclosed. This vulnerability allows for remote code execution, requiring an attacker to have low privileges and access to the network connecting the targeted HPC clusters and nodes. The attack vector is adjacent, meaning it relies on intra-net or private network access rather than exposure to the public internet. Exploitation involves sending a specially crafted HTTPS request to the head node or Linux compute node, potentially allowing the attacker to execute code on other clusters or nodes connected to the targeted head node. The scope of the attack is changed, indicating that successful exploitation could lead to broader impacts beyond the initially compromised system.Windows Telephony Service Remote Code Execution Vulnerability (CVE-2025-21190)
This is a significant security issue with a CVSS score of 8.8, classified as Important. Although it has not been exploited in the wild or disclosed publicly, this vulnerability poses a risk of remote code execution. An attacker could exploit it by deceiving a user into sending a request to a malicious server, which could then return harmful data leading to arbitrary code execution on the user's system. The attack vector is network-based, requiring user interaction, as the attacker needs a client to connect to the malicious server to execute code on the client system.Windows Telephony Service Remote Code Execution Vulnerability (CVE-2025-21200)
This is a significant security issue with a CVSS score of 8.8, rated as Important, though it has not been exploited in the wild nor disclosed publicly, thus not classified as a zero-day. This vulnerability allows for remote code execution, where an attacker could potentially trick a user into sending a request to a malicious server. The server could then return malicious data, leading to arbitrary code execution on the user's system. The attack vector is network-based, requiring user interaction, as the client must connect to a malicious server, which could enable the attacker to execute code on the client machine. Mitigation strategies should focus on user awareness and network security measures to prevent such exploitations.
This summary of Microsoft's monthly updates highlights several critical vulnerabilities, emphasizing the need for immediate attention to certain threats. The Windows Ancillary Function Driver for WinSock vulnerability (CVE-2025-21418) is currently being exploited and poses a significant risk due to its potential for SYSTEM privilege escalation. Users should prioritize patching this vulnerability. Additionally, the Windows Storage vulnerability (CVE-2025-21391) is actively exploited, risking data integrity and availability. The NTLM Hash Disclosure vulnerability (CVE-2025-21377), a zero-day, threatens confidentiality and requires prompt updates. Other critical vulnerabilities, such as those affecting Microsoft Dynamics 365 Sales and Windows LDAP, though not exploited, demand vigilance and timely updates to prevent potential exploitation. Users are advised to prioritize these updates and enhance security measures to mitigate risks effectively.February 2025 Security UpdatesFebruary 2025 Security UpdatesExploitability (old versions)Azure Network Watcher VM Extension Elevation of Privilege VulnerabilityChromium: CVE-2025-0444 Use after free in SkiaChromium: CVE-2025-0445 Use after free in V8Chromium: CVE-2025-0451 Inappropriate implementation in Extensions APIDHCP Client Service Denial of Service VulnerabilityDHCP Client Service Remote Code Execution VulnerabilityHackerOne: CVE-2023-32002 Node.js `Module._load()` policy Remote Code Execution VulnerabilityInternet Connection Sharing (ICS) Denial of Service VulnerabilityKernel Streaming WOW Thunk Service Driver Elevation of Privilege VulnerabilityMicrosoft AutoUpdate (MAU) Elevation of Privilege VulnerabilityMicrosoft Digest Authentication Remote Code Execution VulnerabilityMicrosoft Dynamics 365 Sales Elevation of Privilege VulnerabilityMicrosoft Edge (Chromium-based) Remote Code Execution VulnerabilityMicrosoft Edge (Chromium-based) Spoofing VulnerabilityMicrosoft Edge for IOS and Android Spoofing VulnerabilityMicrosoft Excel Information Disclosure VulnerabilityMicrosoft Excel Remote Code Execution VulnerabilityMicrosoft High Performance Compute (HPC) Pack Remote Code Execution VulnerabilityMicrosoft Message Queuing (MSMQ) Denial of Service VulnerabilityMicrosoft Office Remote Code Execution VulnerabilityMicrosoft Outlook Spoofing VulnerabilityMicrosoft PC Manager Elevation of Privilege VulnerabilityMicrosoft SharePoint Server Remote Code Execution VulnerabilityMicrosoft Surface Security Feature Bypass VulnerabilityNTLM Hash Disclosure Spoofing VulnerabilityVisual Studio Code Elevation of Privilege VulnerabilityVisual Studio Code JS Debug Extension Elevation of Privilege VulnerabilityVisual Studio Installer Elevation of Privilege VulnerabilityWindows Active Directory Domain Services API Denial of Service VulnerabilityWindows Ancillary Function Driver for WinSock Elevation of Privilege VulnerabilityWindows Core Messaging Elevation of Privileges VulnerabilityWindows Deployment Services Denial of Service VulnerabilityWindows Disk Cleanup Tool Elevation of Privilege VulnerabilityWindows Installer Elevation of Privilege VulnerabilityWindows Kerberos Denial of Service VulnerabilityWindows Kernel Security Feature Bypass VulnerabilityWindows Lightweight Directory Access Protocol (LDAP) Remote Code Execution VulnerabilityWindows NTFS Elevation of Privilege VulnerabilityWindows Remote Desktop Configuration Service Tampering VulnerabilityWindows Resilient File System (ReFS) Deduplication Service Elevation of Privilege VulnerabilityWindows Routing and Remote Access Service (RRAS) Remote Code Execution VulnerabilityWindows Setup Files Cleanup Elevation of Privilege VulnerabilityWindows Storage Elevation of Privilege VulnerabilityWindows Telephony Server Remote Code Execution VulnerabilityWindows Telephony Service Remote Code Execution VulnerabilityWindows Win32 Kernel Subsystem Elevation of Privilege Vulnerability

 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>Python 3.14.0 alpha 5 is out</title><link>https://pythoninsider.blogspot.com/2025/02/python-3140-alpha-5-is-out.html</link><author>Hugo</author><category>Python official news</category><category>dev</category><category>official</category><category>python</category><pubDate>Tue, 11 Feb 2025 19:41:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python Insider</source><content:encoded><![CDATA[Here comes the antepenultimate alpha.This is an early developer preview of Python
3.14Python 3.14 is still in development. This release, 3.14.0a5, is the
fifth of seven planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2025-05-06) and, if necessary, may be modified or
deleted up until the release candidate phase (2025-07-22). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.14 are still being planned and
written. Among the new major new features and changes so far:The next pre-release of Python 3.14 will be the penultimate alpha,
3.14.0a6, currently scheduled for 2025-03-14.2025-01-29 marked the start of a new lunar year, the Year of the
Snake 🐍 (and the Year of Python?).For centuries, π was often approximated as 3 in China. Some time
between the years 1 and 5 CE, astronomer, librarian, mathematician and
politician Liu Xin (劉歆) calculated π as 3.154.Around 130 CE, mathematician, astronomer, and geographer Zhang Heng
(張衡, 78–139) compared the celestial circle with the diameter of the
earth as 736:232 to get 3.1724. He also came up with a formula for the
ratio between a cube and inscribed sphere as 8:5, implying the ratio of
a square’s area to an inscribed circle is √8:√5. From this, he
calculated π as √10 (~3.162).Third century mathematician Liu Hui (刘徽) came up with an algorithm
for calculating π iteratively: calculate the area of a polygon inscribed
in a circle, then as the number of sides of the polygon is increased,
the area becomes closer to that of the circle, from which you can
approximate π.This algorithm is similar to the method used by Archimedes in the 3rd
century BCE and Ludolph van Ceulen in the 16th century CE (see 3.14.0a2
  release notes), but Archimedes only went up to a 96-sided polygon
(96-gon). Liu Hui went up to a 192-gon to approximate π as 157/50 (3.14)
and later a 3072-gon for 3.14159.Liu Hu wrote a commentary on the book The Nine Chapters on the
Mathematical Art which included his π approximations.In the fifth century, astronomer, inventor, mathematician,
politician, and writer Zu Chongzhi (祖沖之, 429–500) used Liu Hui’s
algorithm to inscribe a 12,288-gon to compute π between 3.1415926 and
3.1415927, correct to seven decimal places. This was more accurate than
Hellenistic calculations and wouldn’t be improved upon for 900
years.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software
Foundation.Regards from a remarkably snowless Helsinki,Your release team, Hugo van KemenadeSteve Dower]]></content:encoded></item><item><title>EFF Sues DOGE and the Office of Personnel Management to Halt Ransacking of Federal Data</title><link>https://www.eff.org/deeplinks/2025/02/eff-sues-doge-and-office-personnel-management-halt-ransacking-federal-data</link><author>Jason Kelley</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/opm-eye-3b.jpg" length="" type=""/><pubDate>Tue, 11 Feb 2025 19:38:09 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[What’s in OPM’s Databases?How the Privacy Act Protects Americans’ DataOPM’s Data Has Been Under Assault Before]]></content:encoded></item><item><title>FSF talked about education, copyright management, and free machine learning at FOSDEM 2025</title><link>http://www.fsf.org/blogs/licensing/fsf-at-fosdem-2025</link><author></author><category>linux</category><pubDate>Tue, 11 Feb 2025 19:30:00 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[Four FSF staff members had a great time sharing their knowledge and learning at FOSDEM 2025 in Brussels. ]]></content:encoded></item><item><title>PyCoder’s Weekly: Issue #668: NumPy, Compiling Python 1.0, BytesIO, and More (Feb. 11, 2025)</title><link>https://pycoders.com/issues/668</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 19:30:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[ In this video course, you’ll learn how to use NumPy by exploring several interesting examples. You’ll read data from a file into an array and analyze structured arrays to perform a reconciliation. You’ll also learn how to quickly chart an analysis & turn a custom function into a vectorized function. This tutorial will help you master Python string splitting. You’ll learn to use , , and  to effectively handle whitespace, custom delimiters, and multiline text, which will level up your data parsing skills. Python developers use Posit Package Manager to mirror public & internally developed repos within their firewalls. Get reporting on known vulnerabilities to proactively address potential threats. High-security environments can even run air-gapped. The author was recently invited with other senior devs to give a lightning talk on their personal development philosophy. This post captures those thoughts.[ Subscribe to 🐍 PyCoder’s Weekly 💌 – Get the best Python news, articles, and tutorials delivered to your inbox once a week >> Click here to learn more ]]]></content:encoded></item><item><title>Elon Musk attempts hostile takeover of OpenAI…</title><link>https://www.youtube.com/watch?v=tPZauAYgVRQ</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/tPZauAYgVRQ?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 18:58:36 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Elon Musk has launched a hostile takeover bid to take control over the non-profit assets of OpenAI. Let's look into the details of OpenAI's corporate structure and the likelihood Elon's takeover is successful. 

#tech #ai #thecodereport

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

OpenAI Stargate https://youtu.be/YrHsw4Oja7w
o1 First look https://youtu.be/Sf4WqHBCYSY

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Why is Elon trying to buy OpenAI?
- Latest trends in artificial Intelligence
- Elon Musk vs Sam Altman Feud Explained]]></content:encoded></item><item><title>Judge Orders CDC and FDA to Restore Pages Removed by Trump Admin Before Midnight</title><link>https://www.404media.co/federal-judge-orders-cdc-fda-to-bring-back-web-pages-before-midnight/</link><author>Samantha Cole</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/2048px-Trump_Day_1_Executive_Order_Signing.jpg" length="" type=""/><pubDate>Tue, 11 Feb 2025 18:13:00 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[On Tuesday,  the Department of Health and Human Services, Center for Disease Control, and Food and Drug Administration to restore several of the webpages they removed following President Trump’s executive order attacking diversity, equity and inclusion.The health agencies have until 11:59 p.m. on February 11 to restore the pages to how they were on January 30, and “identify any other resources that [Doctors for America] DFA members rely on to provide medical care and that defendants removed or substantially modified on or after January 29, 2025, without adequate notice or reasoned explanation,” U.S. District Judge John Bates wrote in the order.The nonprofit Doctors for America is suing the health agencies following their takedown of these resources in compliance with President Trump’s executive order “Defending Women,” which demanded federal agencies scrub their websites. “Our team’s government affairs firm is advising that as of 5pm today, all U.S. government agency websites will be taken down,” an internal email obtained by 404 Media on February 1 said. “According to reports, agencies are unable to comply fast enough with President Trump’s EO ordering all government entities to remove all DEI references from their websites, so these websites will be taken offline. There is no word on when they will be made available again.”The judge’s decision is in response to Doctors for America’s motion for a temporary restraining order, as part of the organization’s case against federal agencies for removing information from their websites that healthcare workers need for their work. Doctors for America is suing the Office of Personnel Management (OPM), the Centers for Disease Control and Prevention (CDC), the Food and Drug Administration (FDA) and the Department of Health and Human Services (HHS), claiming that their removal of datasets and webpages violates the Administrative Procedure Act, and the Paperwork Reduction Act. “Lack of access to CDC materials on infectious diseases not only harms DFA members’ ability to treat individual patients but also hampers their ability to respond to broader disease outbreaks,” Doctors for America wrote in court documents.“Like many of my colleagues, I am both a doctor who takes care of patients and a researcher. Removing critical clinical information and datasets from the websites of CDC, FDA, and HHS not only puts the health of our patients at risk, but also endangers research that improves the health and health care of the American public,” Reshma Ramachandran, a member of the Doctors for America board of directors, said in a Doctors for America press release.]]></content:encoded></item><item><title>Healthy Competition With GCC 15 vs. LLVM Clang 20 Performance On AMD Zen 5</title><link>https://www.phoronix.com/review/clang20-gcc15-amd-znver5</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Feb 2025 18:03:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In the recent discussion over the GNU Gold linker being deprecated, there was the usual LLVM vs. GCC compiler/toolchain debate. Fortunately, with recently working on some initial benchmarks of the GCC 15 compiler I was following that up with some fresh LLVM Clang compiler comparison metrics on the same AMD Zen 5 hardware.]]></content:encoded></item><item><title>Building a Community Privacy Plan</title><link>https://www.eff.org/deeplinks/2025/02/building-community-privacy-plan</link><author>Paige Collings</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/communityprivacy-kittens.jpg" length="" type=""/><pubDate>Tue, 11 Feb 2025 17:54:44 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[What do we want to protect?Who do we want to protect it from?How much trouble are we willing to go through to try to prevent potential consequences? It also protects your messages from being available for law enforcement should they request it from the platforms you use. In choosing a platform that protects our privacy, we create a space from safety and authenticity away from government and corporate surveillance.  Beyond this, it's important to know the privacy limitations of the platforms you communicate on. For example, while a secure messaging app might prevent government and corporate eavesdroppers from snooping on conversations, that doesn't stop someone you're communicating with from taking screenshots, or the government from attempting to compel you (or your contact) to turn over your messages yourselves. Secure messaging apps also don't protect when someone gets physical access to an unlocked phone with all those messages on it, which is why you may want to consider enabling disappearing message features for certain conversations.Consider The Content You Post On Social Media It’s important to carefully consider the tradeoffs between publicity and privacy when it comes to social media. If you’re promoting something important that needs greater reach, it may be more worth posting to the more popular platforms that undermine user privacy. To do so, it’s vital that you compartmentalize your personal information (registration credentials, post attribution, friends list, etc) away from these accounts.Think About Cloud Servers as Other People’s Computers  Create Incident Response PlansOnly you and your allies can decide what belongs on such a plan, but some strategies might be: ]]></content:encoded></item><item><title>Privacy Loves Company</title><link>https://www.eff.org/deeplinks/2025/02/privacy-loves-company</link><author>Paige Collings</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/communityprivacy-kittens.jpg" length="" type=""/><pubDate>Tue, 11 Feb 2025 17:54:04 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Using Secure Messaging Services For Every Communication At some point, we all need to send a message that’s safe from prying eyes, so the chances of these apps becoming the default for sensitive communications is much higher if we use these platforms for all communications. On an even simpler level, it also means that messages and images sent to family and friends in group chats will be safe from being viewed by automated and human scans on services like Telegram and Facebook Messenger. Consider The Content You Post On Social Media Think About Cloud Servers as Other People’s Computers  Create Incident Response Plans]]></content:encoded></item><item><title>Amazon Q Business simplifies integration of enterprise knowledge bases at scale</title><link>https://aws.amazon.com/blogs/machine-learning/amazon-q-business-simplifies-integration-of-enterprise-knowledge-bases-at-scale/</link><author>Omar Elkharbotly</author><category>dev</category><category>ai</category><pubDate>Tue, 11 Feb 2025 17:11:32 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[In this new era of emerging AI technologies, we have the opportunity to build AI-powered assistants tailored to specific business requirements. Amazon Q Business, a new generative AI-powered assistant, can answer questions, provide summaries, generate content, and securely complete tasks based on data and information in an enterprise’s systems.Large-scale data ingestion is crucial for applications such as document analysis, summarization, research, and knowledge management. These tasks often involve processing vast amounts of documents, which can be time-consuming and labor-intensive. However, ingesting large volumes of enterprise data poses significant challenges, particularly in orchestrating workflows to gather data from diverse sources.In this post, we propose an end-to-end solution using Amazon Q Business to simplify integration of enterprise knowledge bases at scale.Enhancing AWS Support Engineering efficiencyThe AWS Support Engineering team faced the daunting task of manually sifting through numerous tools, internal sources, and AWS public documentation to find solutions for customer inquiries. For complex customer issues, the process was especially time-consuming, laborious, and at times extended the wait time for customers seeking resolutions. To address this, the team implemented a chat assistant using Amazon Q Business. This solution ingests and processes data from hundreds of thousands of support tickets, escalation notices, public AWS documentation, re:Post articles, and AWS blog posts.By using Amazon Q Business, which simplifies the complexity of developing and managing ML infrastructure and models, the team rapidly deployed their chat solution. The Amazon Q Business pre-built connectors like Amazon Simple Storage Service (Amazon S3), document retrievers, and upload capabilities streamlined data ingestion and processing, enabling the team to provide swift, accurate responses to both basic and advanced customer queries.In this post, we propose an end-to-end solution using Amazon Q Business to address similar enterprise data challenges, showcasing how it can streamline operations and enhance customer service across various industries. First we discuss end-to-end large-scale data integration with Amazon Q Business, covering data preprocessing, security guardrail implementation, and Amazon Q Business best practices. Then we introduce the solution deployment using three AWS CloudFormation templates.The following architecture diagram represents the high-level design of a solution proven effective in production environments for AWS Support Engineering. This solution uses the powerful capabilities of Amazon Q Business. We will walk through the implementation of key components, including configuring enterprise data sources to build our knowledge base, document indexing and boosting, and implementing comprehensive security controls. – An end-user who accesses Amazon Q Business applications with permissions granted by their administrator to perform their job duties – A user who manages Amazon Q Business resources and determines feature access for service users within the organization – A user responsible for creating and managing access policies for Amazon Q Business through AWS IAM Identity CenterThe following workflow details how a service user accesses the application:The service user initiates an interaction with the Amazon Q Business application, accessible through the web experience, which is an endpoint URL.The service user’s permissions are authenticated using IAM Identity Center, an AWS solution that connects workforce users to AWS managed applications like Amazon Q Business. It enables end-user authentication and streamlines access management.The authenticated service user submits queries in natural language to the Amazon Q Business application.The Amazon Q Business application generates and returns answers drawing from the enterprise data uploaded to an S3 bucket, which is connected as a data source to Amazon Q Business. This S3 bucket data is continuously refreshed, making sure that Amazon Q Business accesses the most current information for query responses by using a retriever to pull data from the index.Large-scale data ingestionBefore ingesting the data to Amazon Q Business, the data might need transformation into formats supported by Amazon Q Business. Furthermore, it might contain sensitive data or personally identifiable information (PII) requiring redaction. These data ingestion challenges create a need to orchestrate tasks like transformation, redaction, and secure ingestion.To facilitate orchestration, this solution incorporates AWS Step Functions. Step Functions provides a visual workflow service to orchestrate tasks and workloads resiliently and efficiently through built-in AWS integrations and error handling. The solution uses the Step Functions Map state, which allows for parallel processing of multiple items in a dataset, thereby efficiently orchestrating workflows and speeding up overall processing.The following diagram illustrates an example architecture for ingesting data through an endpoint interfacing with a large corpus.Step Functions orchestrates AWS services like AWS Lambda and organization APIs like DataStore to ingest, process, and store data securely. The workflow includes the following steps:The Prepare Map Input Lambda function prepares the required input for the Map state. For example, the Datastore API might require certain input like date periods to query data. This step can be used to define the date periods to be used by the Map state as an input.The Ingest Data Lambda function fetches data from the Datastore API—which can be in or outside of the virtual private cloud (VPC)—based on the inputs from the Map state. To handle large volumes, the data is split into smaller chunks to mitigate Lambda function overload. This enables Step Functions to manage the workload, retry failed chunks, and isolate failures to individual chunks instead of disrupting the entire ingestion process.The fetched data is put into an S3 data store bucket for processing.The Process Data Lambda function redacts sensitive data through Amazon Comprehend. Amazon Comprehend provides real-time APIs, such as DetectPiiEntities and DetectEntities, which use natural language processing (NLP) machine learning (ML) models to identify text portions for redaction. When Amazon Comprehend detects PII, the terms will be redacted and replaced by a character of your choice (such as *). You can also use regular expressions to remove identifiers with predetermined formats.Finally, the Lambda function creates two separate files: 
  A sanitized data document in an Amazon Q Business supported format that will be parsed to generate chat responses.A JSON metadata file for each document containing additional information to customize chat results for end-users and apply boosting techniques to enhance user experience (which we discuss more in the next section).The following is the sample metadata file:{
    "DocumentId": "qbusiness-ug.pdf.txt",
    "Attributes": {
        "_created_at": "2024-10-29T20:27:45+00:00",
        "_last_updated_at": "2024-10-29T20:27:45+00:00",
        "_source_uri": "https://docs.aws.amazon.com/pdfs/amazonq/latest/qbusiness-ug/qbusiness-ug.pdf",
        "author": "AWS",
        "services": ["Q Business"]
    },
    "Title": "Amazon Q Business - User Guide",
    "ContentType": "plain/text"
}In the preceding JSON file, the  for each data document must be unique. All the other attributes are optional; however, the file has additional attributes like , , and  with values defined.The two files are placed in a new S3 folder for Amazon Q to index. Additionally, the raw unprocessed data is deleted from the S3 bucket. You can further restrict access to documents uploaded to an S3 bucket for specific users or groups using Amazon S3 access control lists (ACLs).Using the Amazon Q Business data source connector feature, we integrated the S3 bucket with our application. This connector functionality enables the consolidation of data from multiple sources into a unified index for the Amazon Q Business application. The service offers various integration options, with Amazon S3 being one of the supported data sources.When working with your specific dataset in Amazon Q Business, you can use relevance tuning to enhance the performance and accuracy of search results. This feature allows you to customize how Amazon Q Business prioritizes information within your ingested documents. For example, if your dataset includes product descriptions, customer reviews, and technical specifications, you can use relevance tuning to boost the importance of certain fields. You might choose to prioritize product names in titles, give more weight to recent customer reviews, or emphasize specific technical attributes that are crucial for your business. By adjusting these parameters, you can influence the ranking of search results to better align with your dataset’s unique characteristics and your users’ information needs, ultimately providing more relevant answers to their queries.For the metadata file used in this example, we focus on boosting two key metadata attributes:  and . By assigning higher weights to these attributes, we made sure documents with specific titles or services received greater prominence in the search results, improving their visibility and relevance for the usersThe following code is the sample CloudFormation template snippet to enable higher weights to  and :BoostOverrideConfiguration:
        Fn::Sub: |
          {
            "nativeIndexConfiguration": {
              "indexId": "${QBusinessIndex.IndexId}",
              "boostingOverride": {
                "_document_title": {
                  "stringConfiguration": {
                    "boostingLevel": "MEDIUM"
                  }
                },
                "services": {
                  "stringListConfiguration": {
                    "boostingLevel": "HIGH"
                  }
                }
              }
            }
          }Amazon Q Business guardrailsImplementing robust security measures is crucial to protect sensitive information. In this regard, Amazon Q Business guardrails or chat controls proved invaluable, offering a powerful solution to maintain data privacy and security.Amazon Q Business guardrails provide configurable rules designed to control the application’s behavior. These guardrails act as a safety net, minimizing access, processing, or revealing of sensitive or inappropriate information. By defining boundaries for the application’s operations, organizations can maintain compliance with internal policies and external regulations. You can enable global- or topic-level controls, which control how Amazon Q Business responds to specific topics in chat.The following is the sample CloudFormation template snippet to enable topic-level controls:TopicConfigurations:
        - name: topic
          rules:
            - ruleType: CONTENT_BLOCKER_RULE
              ruleConfiguration:
                contentBlockerRule:
                  systemMessageOverride: This message is blocked as it contains secure content
          exampleChatMessages:
            - arn:*:ec2:us-east-1:123456789012:instance/i-abcdef123
            - arn:*:ec2:us-west-2:123456789012:vpc/bpc-abcdef123
            - arn:*:kms:eu-west-1:123456789012:key/12345678-1234-12345678-abc12345678
            - s3://bucket/prefix/file.csv
            - arn:*:s3::::bucket-nameThis topic-level control blocks the Amazon Q Business chat conversation that has AWS service Amazon Resource Names (ARNs). When similar chat messages have been detected by the Amazon Q Business application, the system will block the responses and return the message “This message is blocked as it contains secure content.”For information about deploying the Amazon Q Business application with sample boosting and guardrails, refer to the GitHub repo.The following screenshot shows an example of the Amazon Q Business assistant chat landing page.The following screenshot illustrates the assistant’s behavior if a user includes text that matches one of the similarity-based examples specified in the guardrail topic control.To enhance data security, you can deploy Amazon Macie classification jobs to scan for sensitive or PII data stored in S3 buckets. The following diagram illustrates a sample notification architecture to alert users on sensitive information that might be inadvertently stored. Macie uses machine learning to automatically discover, classify, and protect sensitive data stored in AWS. It focuses on identifying PII, intellectual property, and other sensitive data types to help organizations meet compliance requirements and protect their data from unauthorized access or breaches.The workflow includes the following steps:Macie reviews the data store S3 bucket for sensitive information before being ingested.An EventBridge rule invokes the Rectify & Notify Lambda function.The Lambda function processes the alert, remediates it by removing the affected files from the S3 bucket, and sends a notification using Amazon Simple Notification Service (Amazon SNS) to the subscribed email addresses.This system enables rapid response to potential security alerts, allowing for immediate action to protect sensitive data.The Macie detection and subsequent notification system can be demonstrated by uploading a new file to the S3 bucket, such as sample-file-with-credentials.txt, containing the PII data types monitored by Macie, such as fake temporary AWS credentials. After the file is uploaded to Amazon S3 and the scheduled Macie detection job discovers it, the Lambda function immediately removes the file and sends the following notification email to the SNS topic subscribers:Amazon Macie published a new Finding: "The S3 object contains credentials data"
Description: "The S3 object contains credentials data such as AWS secret access keys or private keys."
Severity: {'score': 3, 'description': 'High'}
Type: SensitiveData:S3Object/Credentials
Category: CLASSIFICATION
Origin Type: "SENSITIVE_DATA_DISCOVERY_JOB"
Sensitive Data Categories: "['CREDENTIALS']"
Resources affected:
Bucket="<BUCKET_NAME>",
Key="processed/sample-file-with-credentials.txt"
Trying to delete S3 Object:  s3://<BUCKET_NAME>/processed/sample-file-with-credentials.txt
File deletion succeeded.

-------------
Full Macie finding event:
{
   ...
}Additionally, the findings are visible on the Macie console, as shown in the following screenshot.Additional recommendationsTo further enhance the security and reliability of the Amazon Q Business application, we recommend implementing the following measures. These additional security and logging implementations make sure the data is protected, alerts are sent in response to potential warnings, and timely actions can be taken for security incidents.Amazon CloudWatch logging for Amazon Q Business – You can use Amazon CloudWatch logging for Amazon Q Business to save the logs for the data source connectors and document-level errors, focusing particularly on failed ingestion jobs. This practice is vital from a security perspective because it allows monitoring and quick identification of issues in the data ingestion process. By tracking failed jobs, potential data loss or corruption can be mitigated, maintaining the reliability and completeness of the knowledge base.Unauthorized access monitoring on Amazon S3 – You can implement EventBridge rules to monitor mutating API actions on the S3 buckets. These rules are configured to invoke SNS notifications when such actions are performed by unauthorized users. Enable Amazon S3 server access logging to store detailed access records in a designated bucket, which can be analyzed using Amazon Athena for deeper insights. This approach provides real-time alerts for immediate response to potential security breaches, while also maintaining a detailed audit trail for thorough security analysis, making sure that only authorized entities can modify critical data.In the following sections, we walk through implementing the end-to-end solution. For this solution to work, the following prerequisites are needed:A new or existing AWS account that will be the data collection accountConfigure the data ingestionIn this post, we demonstrate the solution using publicly available documentation as our sample dataset. In your implementation, you can adapt this solution to work with your organization’s specific content sources, such as support tickets, JIRA issues, internal wikis, or other relevant documentation.Ingestion Lambda functionProcessing Lambda functionThe data ingestion workflow in this example fetches and processes public data from the Amazon Q Business and Amazon SageMaker official documentation in PDF format. Specifically, the Ingest Data Lambda function downloads the raw PDF documents, temporarily stores them in Amazon S3, and passes their Amazon S3 URLs to the Process Data Lambda function, which performs the PII redaction (if enabled) and stores the processed documents and their metadata to the S3 path indexed by the Amazon Q Business application.You can adapt the Step Functions Lambda code for ingestion and processing according to your own internal data, making sure that the documents and metadata are in a valid format for Amazon Q Business to index, and are properly redacted for PII data.Configure IAM Identity CenterYou can only have one IAM Identity Center instance per account. If your account already has an Identity Center instance, skip this step and proceed to configuring the Amazon Q Business application.You will need to add details for a user such as user name, email, first name, and surname.After deploying the CloudFormation template, you will receive an email where you will need to accept the invitation and change the password for the user.Before logging in, you will need to deploy the Amazon Q Business application.Configure the Amazon Q Business applicationYou will need to add details such as the IAM Identity Center stack name deployed previously and the S3 bucket name provisioned by the data ingestion stack.After you deploy the CloudFormation template, complete the following steps to manage user access:On the Amazon Q Business console, choose  in the navigation pane.Choose the application you provisioned ().Under , choose .On the  tab, choose the user you specified when deploying the CloudFormation stack.Under , choose  or .Choose  and then Now you can log in using the user you have specified. You can find the URL for the web experience under .If you are unable to log in, make sure that the user has been verified.Before you can use the Amazon Q Business application, the data source needs to be synchronized. The application’s data source is configured to sync hourly. It might take some time to synchronize.When the synchronization is complete, you should now be able to access the application and ask questions.After you’re done testing the solution, you can delete the resources to avoid incurring additional charges. See the Amazon Q Business pricing page for more information. Follow the instructions in the GitHub repository to delete the resources and corresponding CloudFormation templates. Make sure to delete the CloudFormation stacks provisioned as follows:Delete the Amazon Q Business application stack.Delete the IAM Identity Center stack.Delete the data ingestionFor each deleted stack, check for any resources that were skipped in the deletion process, such as S3 buckets.Delete any skipped resources on the console.In this post, we demonstrated how to build a knowledge base solution by integrating enterprise data with Amazon Q Business using Amazon S3. This approach helps organizations improve operational efficiency, reduce response times, and gain valuable insights from their historical data. The solution uses AWS security best practices to promote data protection while enabling teams to create a comprehensive knowledge base from various data sources.Whether you’re managing support tickets, internal documentation, or other business content, this solution can handle multiple data sources and scale according to your needs, making it suitable for organizations of different sizes. By implementing this solution, you can enhance your operations with AI-powered assistance, automated responses, and intelligent routing of complex queries.Try this solution with your own use case, and let us know about your experience in the comments section. is a Senior Cloud Support Engineer at AWS, specializing in Data, Machine Learning, and Generative AI solutions. With extensive experience in helping customers architect and optimize their cloud-based AI/ML/GenAI workloads, Omar works closely with AWS customers to solve complex technical challenges and implement best practices across the AWS AI/ML/GenAI service portfolio. He is passionate about helping organizations leverage the full potential of cloud computing to drive innovation in generative AI and machine learning. is a Principal Cloud Support Engineer at AWS, focused on Networking and Generative AI solutions. He has deep expertise in resolving complex, cross-domain technical challenges through systematic problem-solving methodologies. With a customer-obsessed mindset, he leverages emerging technologies to drive innovation and deliver exceptional customer experiences. is a Principal Cloud Support Engineer at AWS. She specializes in solving complex customer issues on the AWS Cloud, focusing on infrastructure-as-code, container orchestration, and generative AI technologies. She collaborates with teams across AWS to design solutions that enhance the customer experience. Outside of work, Bhavani enjoys cooking and traveling. is a Senior Cloud Support Engineer at AWS, specialized in Machine Learning technologies and Generative AI solutions, helping customers operate and optimize their ML workloads. With a deep passion for driving performance improvements, he dedicates himself to empowering both customers and teams through innovative ML-enabled solutions. Away from his technical pursuits, Mattia embraces his passion for travel and adventure. is a Senior Cloud Support Engineer at AWS who specializes in Serverless technologies and development within the AWS cloud. Kevin has a passion for creating solutions through code while ensuring it is built on solid infrastructure. Outside of work, Kevin enjoys art and sport. is a Senior Principal Engineer leading AWS. Tipu supports customers with designing and optimizing their cloud technology strategy as a senior principal engineer in AWS Support & Managed Services. For over 15 years, he has designed, operated and supported diverse distributed systems at scale with a passion for operational excellence. He currently works on generative AI and operational excellence.]]></content:encoded></item><item><title>Faster distributed graph neural network training with GraphStorm v0.4</title><link>https://aws.amazon.com/blogs/machine-learning/faster-distributed-graph-neural-network-training-with-graphstorm-v0-4/</link><author>Theodore Vasiloudis</author><category>dev</category><category>ai</category><pubDate>Tue, 11 Feb 2025 17:03:16 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[GraphStorm is a low-code enterprise graph machine learning (ML) framework that provides ML practitioners a simple way of building, training, and deploying graph ML solutions on industry-scale graph data. Although GraphStorm can run efficiently on single instances for small graphs, it truly shines when scaling to enterprise-level graphs in distributed mode using a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances or Amazon SageMaker.Today, AWS AI released GraphStorm v0.4. This release introduces integration with DGL-GraphBolt, a new graph storage and sampling framework that uses a compact graph representation and pipelined sampling to reduce memory requirements and speed up Graph Neural Network (GNN) training and inference. For the large-scale dataset examined in this post, the inference speedup is 3.6 times faster, and per-epoch training speedup is 1.4 times faster, with even larger speedups possible.To achieve this, GraphStorm v0.4 with DGL-GraphBolt addresses two crucial challenges of graph learning: – GraphStorm v0.4 provides compact and distributed storage of graph structure and features, which may grow in the multi-TB range. For example, a graph with 1 billion nodes with 512 features per node and 10 billion edges will require more than 4 TB of memory to store, which necessitates distributed computation. – In multi-layer GNNs, you need to sample neighbors of each node to propagate their representations. This can lead to exponential growth in the number of nodes sampled, potentially visiting the entire graph for a single node’s representation. GraphStorm v0.4 provides efficient, pipelined graph sampling.In this post, we demonstrate how GraphBolt enhances GraphStorm’s performance in distributed settings. We provide a hands-on example of using GraphStorm with GraphBolt on SageMaker for distributed training. Lastly, we share how to use Amazon SageMaker Pipelines with GraphStorm.GraphBolt: Pipeline-driven graph samplingGraphBolt is a new data loading and graph sampling framework developed by the DGL team. It streamlines the operations needed to sample efficiently from a heterogeneous graph and fetch the corresponding features. GraphBolt introduces a new, more compact graph structure representation for heterogeneous graphs, called fused Compressed Sparse Column (fCSC). This can reduce the memory cost of storing a heterogeneous graph by up to 56%, allowing users to fit larger graphs in memory and potentially use smaller, more cost-efficient instances for GNN model training.GraphStorm v0.4 seamlessly integrates with GraphBolt, allowing users to take advantage of its performance improvements in their GNN workflows. The user just needs to provide the additional argument  when launching graph construction and training jobs.A common model development process is to perform model exploration locally on a subset of your full data, and when you’re satisfied with the results, train the full-scale model. This setup allows for cheaper exploration before training on the full dataset. GraphStorm and SageMaker Pipelines allows you to do that by creating a model pipeline you can run locally to retrieve model metrics, and when you’re ready, run your pipeline on the full data on SageMaker, and produce models, predictions, and graph embeddings to use in downstream tasks. In the next section, we show how to set up such pipelines for GraphStorm.We demonstrate such a setup in the following diagram, where a user can perform model development and initial training on a single EC2 instance, and when they’re ready to train on their full data, hand off the heavy lifting to SageMaker for distributed training. Using SageMaker Pipelines to train models provides several benefits, like reduced costs, auditability, and lineage tracking.Set up the environment for SageMaker distributed trainingSetting up your environment should take around 10 minutes. First, set up your Python environment to run the examples:conda init
eval $SHELL
# Create a new env for the post
conda create --name gsf python=3.10
conda activate gsf

# Install dependencies for local scripts
pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu
pip install sagemaker boto3 ogb pyarrow
# Verify installation, might take a few minutes for first run
python -c "import sagemaker; import torch"

# Clone the GraphStorm repository to access the example code
git clone https://github.com/awslabs/graphstorm.git ~/graphstormBuild a GraphStorm SageMaker CPU imageNext, build and push the GraphStorm PyTorch Docker image that you will use to run the graph construction, training, and inference jobs for smaller-scale data. Your role will need to be able to pull images from the Amazon ECR Public Gallery and create Amazon Elastic Container Registry (Amazon ECR) repositories and push images to your private ECR registry.# Enter you account ID here
ACCOUNT_ID=<aws-account-id>
REGION=us-east-1

cd ~/graphstorm
bash docker/build_graphstorm_image.sh --environment sagemaker --device cpu
bash docker/push_graphstorm_image.sh -e sagemaker -r $REGION -a $ACCOUNT_ID -d cpu
# This will create an ECR repository and push an image to
# ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/graphstorm:sagemaker-cpuDownload and prepare datasetsIn this post, we use two citation datasets to demonstrate the scalability of GraphStorm. The Open Graph Benchmark (OGB) project hosts a number of graph datasets that can be used to benchmark the performance of graph learning systems. For a small-scale demo, we use the ogbn-arxiv dataset, and for a demonstration of GraphStorm’s large-scale learning capabilities, we use the ogbn-papers100M dataset.Prepare the ogbn-arxiv datasetDownload the smaller-scale ogbn-arxiv dataset to run a local test before launching larger-scale SageMaker jobs on AWS. This dataset has approximately 170,000 nodes and 1.2 million edges. Use the following code to download the data and prepare it for GraphStorm:# Provide the S3 bucket to use for output
BUCKET_NAME=<your-s3-bucket>cd ~/graphstorm/examples/sagemaker-pipelines-graphbolt
python convert_arxiv_to_gconstruct.py \
--output-s3-prefix s3://$BUCKET_NAME/ogb-arxiv-inputThis will create the tabular graph data in Amazon S3, which you can verify by running the following code:aws s3 ls s3://$BUCKET_NAME/ogb-arxiv-input/ 
edges/
nodes/
splits/
gconstruct_config_arxiv.jsonFinally, upload GraphStorm training configuration files for arxiv to use for training and inference:# Upload the training configurations to S3
aws s3 cp ~/graphstorm/training_scripts/gsgnn_np/arxiv_nc.yaml \
s3://$BUCKET_NAME/yaml/arxiv_nc_train.yaml
aws s3 cp ~/graphstorm/inference_scripts/np_infer/arxiv_nc.yaml \
s3://$BUCKET_NAME/yaml/arxiv_nc_inference.yamlPrepare the ogbn-papers100M dataset on SageMakerThe papers-100M dataset is a large-scale graph dataset, with 111 million nodes and 3.2 billion edges after adding reverse edges.To download and preprocess the data as an Amazon SageMaker Processing step, use the following code. You can launch and let the job run in the background while proceeding through the rest of the post, and return to this dataset later. The job should take approximately 45 minutes to run.# Navigate to the example code
cd ~/graphstorm/examples/sagemaker-pipelines-graphbolt

# Build and push a Docker image to download and process the papers100M data
bash build_and_push_papers100M_image.sh -a $ACCOUNT_ID -r $REGION

# This creates an ECR repository and pushes an image to
# $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/papers100m-processor

# Run a SageMaker job to do the processing and upload the output to S3
SAGEMAKER_EXECUTION_ROLE_ARN=<your-sagemaker-execution-role-arn>
aws configure set region $REGION
python sagemaker_convert_papers100m.py \
--output-bucket $BUCKET_NAME \
--execution-role-arn $SAGEMAKER_EXECUTION_ROLE_ARN \
--region $REGION \
--instance-type ml.m5.4xlarge \
--image-uri $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/papers100m-processorThis will produce the processed data in s3://$BUCKET_NAME/ogb-papers100M-input, which can then be used as input to GraphStorm. While this job is running, you can create the GraphStorm pipelines.Create a SageMaker pipeline# Navigate to the example code
cd ~/graphstorm/examples/sagemaker-pipelines-graphbolt

PIPELINE_NAME="ogbn-arxiv-gs-pipeline"

bash deploy_arxiv_pipeline.sh \
--account $ACCOUNT_ID\
--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \
--pipeline-name $PIPELINE_NAME \
--use-graphbolt falseRunning the preceding code will create a SageMaker pipeline configured to run three SageMaker jobs in sequence:A GConstruct job that converts the tabular file input to a binary partitioned graph on Amazon S3A GraphStorm training job that trains a node classification model and saves the model to Amazon S3A GraphStorm inference job that produces predictions for all nodes in the test set, and creates embeddings for all nodesTo review the pipeline, navigate to SageMaker AI Studio, choose the domain and user profile you used to create the pipeline, then choose .In the navigation pane, choose . There should be a pipeline named . Choose the pipeline, which will take you to the  tab for the pipeline. Choose  to view the pipeline steps.Run the SageMaker pipeline locally for ogbn-arxivThe ogbn-arxiv dataset is small enough that you can run the pipeline locally. Run the following command to start a local execution of the pipeline:# Allow the local containers to inherit AWS credentials
export USE_SHORT_LIVED_CREDENTIALS=1
python ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \
--pipeline-name ogbn-arxiv-gs-pipeline \
--region us-east-1 \
--local-execution | tee arxiv-local-logs.txtWe save the log output to . You will use that later to analyze the training speed.Running the pipeline should take approximately 5 minutes. When the pipeline is complete, it will print a message like the following:Pipeline execution 655b9357-xxx-xxx-xxx-4fc691fcce94 SUCCEEDEDYou can inspect the mean epoch and evaluation time using the provided  script and the log file you created:python analyze_training_time.py --log-file arxiv-local-logs.txt

Reading logs from file: arxiv-local-logs.txt

=== Training Epochs Summary ===
Total epochs completed: 10
Average epoch time: 4.70 seconds

=== Evaluation Summary ===
Total evaluations: 11
Average evaluation time: 1.90 secondsThese numbers will vary depending on your instance type; in this case, these are values reported on an m6in.4xlarge instance.Create a GraphBolt pipelineNow you have established a baseline for performance, you can create another pipeline that uses the GraphBolt graph representation to compare the performance.You can use the same pipeline creation script, but change two variables, providing a new pipeline name and setting  to :# Deploy a GraphBolt-enabled pipeline
PIPELINE_NAME_GB="ogbn-arxiv-gs-graphbolt-pipeline"
bash deploy_arxiv_pipeline.sh \
--account $ACCOUNT_ID \
--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \
--pipeline-name $PIPELINE_NAME_GB \
--use-graphbolt true

# Execute the pipeline locally
python ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \
--pipeline-name $PIPELINE_NAME_GB \
--region us-east-1 \
--local-execution | tee arxiv-local-gb-logs.txtAnalyzing the training logs, you can see the per-epoch time has dropped somewhat:python analyze_training_time.py --log-file arxiv-local-gb-logs.txt

Reading logs from file: arxiv-local-gb-logs.txt

=== Training Epochs Summary ===
Total epochs completed: 10
Average epoch time: 4.21 seconds

=== Evaluation Summary ===
Total evaluations: 11
Average evaluation time: 1.63 secondsFor such a small graph, the performance gains are modest, around 13% per epoch time. With large data, the potential gains are much greater. In the next section, you will create a pipeline and train a model for papers-100M, a citation graph with 111 million nodes and 3.2 billion edges.Create a SageMaker pipeline for distributed trainingAfter the SageMaker processing job that prepares the papers-100M data has finished processing and the data is stored in Amazon S3, you can set up a pipeline to train a model on that dataset.Build the GraphStorm GPU imageFor this job, you will use large GPU instances, so you will build and push the GPU image this time:cd ~/graphstorm

bash ./docker/build_graphstorm_image.sh --environment sagemaker --device gpu

bash docker/push_graphstorm_image.sh -e sagemaker -r $REGION -a $ACCOUNT_ID -d gpuDeploy and run pipelines for papers-100MBefore you deploy your new pipeline, upload the training YAML configuration for papers-100M to Amazon S3:aws s3 cp \
~/graphstorm/training_scripts/gsgnn_np/papers100M_nc.yaml \
s3://$BUCKET_NAME/yaml/Now you are ready to deploy your initial pipeline for papers-100M:# Navigate to the example code 
cd ~/graphstorm/examples/sagemaker-pipelines-graphbolt 
PIPELINE_NAME="ogb-papers100M-pipeline" 
bash deploy_papers100M_pipeline.sh \ 
    --account $ACCOUNT_ID \
    --bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \
    --pipeline-name $PIPELINE_NAME \ 
    --use-graphbolt falseRun the pipeline on SageMaker and let it run in the background:# Navigate to the example code
cd ~/graphstorm/examples/sagemaker-pipelines-graphbolt

PIPELINE_NAME="ogb-papers100M-pipeline"
bash deploy_papers100M_pipeline.sh \
--account $ACCOUNT_ID \
--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \
--pipeline-name $PIPELINE_NAME \
--use-graphbolt falseYour account needs to meet the required quotas for the requested instances. For this post, the defaults are set to four  for training jobs and one  instance for a processing job. To adjust your SageMaker service quotas, you can use the Service Quotas console. To run both pipelines in parallel, i.e. without GraphBolt and with GraphBolt, you will need 8 x  and 2 x Next, you can deploy and run another pipeline, with GraphBolt enabled:# Deploy the GraphBolt-enabled pipeline
PIPELINE_NAME_GB="ogb-papers100M-graphbolt-pipeline"
bash deploy_papers100M_pipeline.sh \
--account $ACCOUNT_ID\
--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \
--pipeline-name $PIPELINE_NAME_GB \
--use-graphbolt true

# Execute the GraphBolt pipeline on SageMaker
python ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \
--pipeline-name $PIPELINE_NAME_GB \
--region us-east-1 \
--async-executionCompare performance for GraphBolt-enabled trainingAfter both pipelines are complete, which should take approximately 4 hours, you can compare the training times for both cases.On the page of the SageMaker console, there should be two new pipelines named  and ogb-papers100M-graphbolt-pipeline. Choose ogb-papers100M-pipeline, which will take you to the  tab for the pipeline. Copy the name of the latest successful execution and use that to run the training analysis script:python analyze_training_time.py \
--pipeline-name $PIPELINE_NAME\
--execution-name execution-1734404366941Your output will look like the following code:== Training Epochs Summary ===
Total epochs completed: 15
Average epoch time: 73.95 seconds

=== Evaluation Summary ===
Total evaluations: 15
Average evaluation time: 15.07 secondsNow do the same for the GraphBolt-enabled pipeline:python analyze_training_time.py \
--pipeline-name $PIPELINE_NAME_GB \
--execution-name execution-1734463209078You will see the improved per-epoch and evaluation times:== Training Epochs Summary ===
Total epochs completed: 15
Average epoch time: 54.54 seconds

=== Evaluation Summary ===
Total evaluations: 15
Average evaluation time: 4.13 secondsWithout loss in accuracy, the latest version of GraphStorm achieved a speedup of approximately 1.4 times faster per epoch for training, and a speedup of 3.6 times faster in evaluation time! Depending on the dataset, the speedups can be even greater, as shown by the DGL team’s benchmarking.This post showcased how GraphStorm 0.4, integrated with DGL-GraphBolt, significantly speeds up large-scale GNN training and inference, by 1.4 and 3.6 times faster, respectively, as measured on the papers-100M dataset. As shown in the DGL benchmarks, even larger speedups are possible depending on the dataset.We encourage ML practitioners working with large graph data to try GraphStorm. Its low-code interface simplifies building, training, and deploying graph ML solutions on AWS, allowing you to focus on modeling rather than infrastructure. is a Senior Applied Scientist at Amazon Web Services, where he works on distributed machine learning systems and algorithms. He led the development of GraphStorm Processing, the distributed graph processing library for GraphStorm and is a core developer for GraphStorm. He received his PhD in Computer Science from the KTH Royal Institute of Technology, Stockholm, in 2019. is a Senior Applied Scientist at Amazon Web Services, where he develops deep learning frameworks including GraphStorm, DGL, and DGL-KE. He led the development of Amazon Neptune ML, a new capability of Neptune that uses graph neural networks for graphs stored in a Neptune graph database. He is now leading the development of GraphStorm, an open source graph machine learning framework for enterprise use cases. He received his PhD in computer systems and architecture at the Fudan University, Shanghai, in 2014. is a Principal Technical Product Manager at AWS AI/ML research supporting science teams like the graph machine learning group, and ML Systems teams working on large scale distributed training, inference, and fault resilience. Before joining AWS, Florian lead technical product management for automated driving at Bosch, was a strategy consultant at McKinsey & Company, and worked as a control systems and robotics scientist—a field in which he holds a PhD.]]></content:encoded></item><item><title>Creating a Useful Voice-Activated Fully Local RAG System</title><link>https://www.kdnuggets.com/creating-useful-voice-activated-fully-local-rag-system</link><author>Cornellius Yudha Wijaya</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-wijaya-creating-useful-voice-activated-rag.png" length="" type=""/><pubDate>Tue, 11 Feb 2025 17:00:49 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[This article will explore initiating the RAG system and making it fully voice-activated.]]></content:encoded></item><item><title>A Wild Website Where People Lose Millions of Dollars</title><link>https://www.youtube.com/watch?v=_GUYYeVImVY</link><author>Jack Rhysider</author><category>Darknet diaries</category><category>infosec</category><category>video</category><category>podcast</category><enclosure url="https://www.youtube.com/v/_GUYYeVImVY?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 17:00:40 +0000</pubDate><source url="https://www.youtube.com/channel/UCMIqrmh2lMdzhlCPK5ahsAg">Jack Rhysider</source><content:encoded><![CDATA[A Wild Website Where People Lose Millions of Dollars

#cybersecurity #podcast #crypto #memecoin 

Listen to the full episode 🎙 Darknet Diaries Ep. 152: Stacc Attack]]></content:encoded></item><item><title>How to refactor code with GitHub Copilot</title><link>https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/</link><author>Anthony Grutta</author><category>official</category><pubDate>Tue, 11 Feb 2025 17:00:05 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[We’ve all been there—staring at a function that looks like it was written by an over-caffeinated goblin at 3 AM (maybe even your alter ego). You  pretend it doesn’t exist, or you could refactor it. Luckily, GitHub Copilot makes the second option less painful.What is code refactoring?Feel free to breeze past this section if you already know what’s involved with refactoring code, but we wanted to take a moment to cover what we’ll be looking at today.Think of refactoring as giving your project some digital spring cleaning—a glow up for your functions, classes, and modules. But instead of just tidying up, you’re making your code more efficient, maintainable, and readable, all without changing its external behavior.Some standard ways of refactoring include:Simplifying complex conditionals (because no one should need a PhD to read your  statements)  Extracting duplicated logic (so you’re not trying to maintain code in ten different places)  Improving variable and function names (because  is a crime against humanity)  Converting monolithic functions into smaller, modular pieces (to prevent the dreaded “function that spans multiple screens” scenario)Refactoring isn’t just about tidiness—it’s about making your codebase more resilient, scalable, and enjoyable to work with. Let’s find out how GitHub Copilot can help you do it faster and with fewer headaches.Know what your code does before you refactor anythingIt might seem obvious to say this, but before you can refactor any code you need to understand how it works. If you don’t know what your code is doing, you won’t know whether or not the “improvements” you’re making are changing the core functionality of the code.Consider the following method:public String getSound(String animal) {
  if (animal == null) {
      System.out.println("Oops! A null animal?");
  } else if (animal.equalsIgnoreCase("Dog")) {
      return "Bark";
  } else if ( animal.equalsIgnoreCase("Cat")) {
      return "Meow";
  } else if ( animal.equalsIgnoreCase("Bird")) {
      return "Tweet";
  }
  return "Unknown";
}
You might look at this and immediately think “they should use a switch statement,” and that would be one example of refactoring the code. But having that knee jerk reaction requires you to know how if-statements and switch-statements work. You can only make that suggestion if you understand this code will continue cycling through if-statements until it finds an appropriate match. Otherwise, it will return the value of .As codebases get more complex and make calls between multiple files, this gets (much) more complicated. And this is one way Copilot can help you.You can ask Copilot Chat to explain how some code works, either by asking in plain language or using the /explain slash command. To limit the scope of what Copilot looks at, select the code in your IDE before asking your query, or specify specific files for it to consider by using . While you’re at it, you can even ask it to add code comments to help you (or anyone else reading the code) in the future.Here are some sample prompts:Explain what this code does.Add comments to this code to make it more understandable.You should use Copilot Chat to analyze and explain your codebase until you fully understand the code you’re looking to refactor.Try some blanket improvements to refactor your codeLike most things in life, it’s usually best to start small. When you’re first getting started with refactoring code, keep it simple: open up Copilot Chat in your project and ask “how would you improve this?” Just like when you are asking GitHub Copilot to explain some code, you can specify what it looks at by highlighting sections of code or identifying specific files by using .Here are some sample prompts:How would you improve this?Improve the variable names in this function.#file:pageInit.js, #file:socketConnector.js Offer suggestions to simplify this code.Copilot will then offer suggestions to improve the code in the way that you specified. This is great for getting started, but Copilot can do much more if you give it some guidance.When working with any generative AI-powered developer tool, it is often useful to include more context in your prompts—ones that are more likely to get you the results you’re looking for. By being specific about what you want the tool to do, it focuses the efforts toward that end.It’s a bit like if someone tells you to code something, and you have so many ideas and questions that you’re not quite sure where to begin. But if someone tells you they specifically want you to code a class that sorts a list, you can focus on the task at hand.The prompts above don’t offer much specificity or context, which leaves Copilot to explore all the possible ways your code could be improved. The upside? You may see options that you might not have considered. The downside is some of the proposed solutions might not address your specific concerns.Make a plan for refactoring your codebaseWhat do you want to do to your code? Do you want to make it more readable? Or do you want to find redundant code and remove it? Coming up with a plan for the improvements you want to make will help you to hit your goals. This comes back to making sure you understand your code. If you know how it works, you can come up with a plan for the type of improvements that you want to make.Maybe your code base has a bunch of different scripts that all perform the same general function. You could strip out the code into a common module to import into each of the different scripts, making the code easier to change and maintain.To do so, you can direct Copilot to look for these common code sections and to pull them into a single module.Inspect all my js files for GitHub API calls and create a new class that will manage all the GitHub API calls.
Now that we have provided some guidance and additional context, Copilot will provide suggestions targeting this specific improvement that we want to make to our code.You can also provide a laundry list of tasks, or ask Copilot to keep things in mind while it is doing the refactoring.In that vein, here’s a sample prompt to consider:Can you refactor the GitHubController class to:
- remove nested logic structures
- make the code more concise
- while doing this, check if the code is safe and add comments if not
The takeaway here is Copilot is very good at taking directions—and the more specific your directions are, the more it will generate outputs in line with your intended end goal.A real-world example: Using GitHub Copilot to refactor codeSometimes the best way to understand something is to see it in use. To help with this, we wanted to share an experience where we needed to use GitHub Copilot to refactor our own code, interspersed with some general guidance and pointers.It started with a couple of organizations independently requesting help with their individual large migration projects, which involved moving from other data centers into tens of thousands of repositories on GitHub. After building a proof of concept, we started creating individual systems, each tailored to the needs of the individual customer and their unique systems. While we were doing this, a third organization made a similar request for help, and we realized this was a common need.At this point in time, we had two systems that were both suited for large-scale migrations, but each of them was heavily customized for the original company making the request. We quickly realized the best solution involved refactoring our code to improve its modularity so we could use it across different organizations. This would not be easy—the codebase was made up of several different scripts and workflows with a lot of moving parts. And that’s when we turned to Copilot for help.First we came up with a plan. We created a set of high-level goals and used these to guide us during the refactoring process. We wanted to make sure the code was:Modular and reusable for different organizations: The code should be organized into small, self-contained modules that could be reused and combined for new functionality.   The code should be easy to maintain and extend as needed. That meant making it well-organized, well-documented, and easy to understand.  Customizable for different organizations: The code should be flexible enough to meet the unique needs of each organization.  Following coding standards and best practices: The code should be consistent, readable, and easy to understand.With this plan in place, it was time to ask Copilot for help.Step 1: Creating the initial moduleOur first step was creating a common module we could import into each of the different scripts. This would allow us to reuse the same code across many different scripts, making it easier to maintain and extend. We created a skeleton of a module called . Then we used Copilot to fill in the details.Here was the prompt we gave Copilot:I am creating a module, gh-migrations, which will hold common functionality to be used through out my project. I have already created an index.js for the module. Please inspect the other js files for GitHub API calls, create a new class for the module which will manage all GitHub API calls. This class should also account for rate limiting and errors. Do not make edits to the javascript files, rather identify the API calls, and add the functionality to the new GitHub API class.
And here was Copilot’s response:I'll help create a GitHub API management class based on the API calls found in the provided files. Here's a step-by-step solution:

1. From analyzing the files, I identified these common GitHub API operations: 
  * Issues creation and updates
  * Issue search
  * Rate limit handling (403 errors)
  * Common error handling patterns

2. I'll create a class that encapsulates these operations with proper error handling and rate limiting.

.github/lib/gh-migrations/index.js
Here's the GitHub API management class implementation:

<code>

This implementation:

1. Creates a GitHubApiManager class that handles all GitHub API interactions
2. Implements automatic rate limiting retry logic in withRateLimit
3. Provides methods for common operations: 
  * createIssue: Creates a new issue
  * updateIssue: Updates an existing issue
  * searchIssues: Searches for issues using GitHub's search API
  * findIssueByTitle: Helper method to find issues by title
  * getIssue: Gets a specific issue by number

Usage example:

<code>

The class handles:
  * Automatic retry on rate limits
  * Consistent error handling
  * Repository context management
  * Common GitHub API operations used across the codebase

This centralizes all GitHub API interactions and makes the code more maintainable and consistent.
After reviewing the code (as you should always do!), we made the recommended changes. The next step was putting these changes into a separate file while ensuring the  file could reference it.Here was the prompt we entered for Copilot:Rename the class GitHubApiManager to GHApi and put it in a separate file that is referenced in the index.js file.
And here was Copilot’s response:I'll help you split the code into two files. Here's the step by step solution:

1. Create a new file gh-api.js for the GHApi class
2. Update index.js to export the renamed class from the new file

.github/lib/gh-migrations/gh-api.js
Create a new file containing the renamed class:

<code>

.github/lib/gh-migrations/index.js
Update the index file to export the renamed class from the new file:

<code>

The changes:
1. Created a new file gh-api.js with the renamed class
2. Moved all the class implementation to the new file
3. Updated the class name from GitHubApiManager to GHApi
4. Modified index.js to require and export the class from the new file

Now consumers of this module can continue to import it the same way, but the implementation is better organized.
After making these changes, we then had to repeat the process for each of the files that contained GitHub API calls. Be aware, we didn’t want to refactor the code directly at this stage. Instead, we wanted to create a common module we could use for refactoring code later.Here was the prompt we entered for Copilot:The gh-api.js file is a new class that will be used to manage all GitHub API calls. Please review the other javascript files for GitHub API calls to be added to gh-api.js. Do not edit the javascript files, only add new functionality to gh-api.js.
And here was Copilot’s response:I'll analyze the JavaScript files and add the missing API functionality to the GHApi class.

.github/lib/gh-migrations/gh-api.js
Add methods for issue operations, searching, and deletion based on the API calls found in migration.js and application.js:

<code>

This implementation adds all the GitHub API functionality found in the other files:

1. From migration.js:
  * createComment for updating issues with comments
  * getIssue and isMigrationIssue for checking migration status
2. From application.js:
  * searchIssues and searchIssuesByTitle for finding issues
  * getIssueNodeId and deleteIssue for issue deletion via GraphQL
  * findJiraIssue helper method to get Jira ticket information

The methods maintain the same rate limiting and error handling patterns while providing a cleaner interface for the API operations.
Step 2: Adding new classesOnce we had the new  class to handle the GitHub API, we want to create some reusable classes for different entities in our application. Since our application is a migration tool managed via GitHub Issues, we wanted to create classes for the following entities: represent repository migrations from various sources. They contain configuration and metadata about the repository and how the migration should be performed.   represent a batch of migrations. They contain more high-level metadata about groups of individual migration issues such as custom application ids and status of the batch.   represent the migration of a group of users, their associated repositories, and the permissions to those repositories.   represent Azure DevOps pipelines rewire operations. They allow for the reconfiguration of Azure DevOps pipelines to point to the new GitHub repositories after a migration has been completed.Here was the prompt we entered for Copilot:Review the create-migration-issue.js and develop a new class called MigrationIssue that contains the properties associated with a migration. This shoud be added to the gh-migrations module where index.js is located.
And here was Copilot’s response:I'll help create a new MigrationIssue class based on the migration configuration and status shown in the code. Here's the step-by-step solution:

1. Create a new file migration-issue.js in the gh-migrations directory
2. Update the index.js file to expose the new class
3. Extract configuration and status properties from the create-migration-issues.js file into the new class

migration-issue.js
Create a new class to represent a migration issue with all necessary properties and methods.

<code>

index.js
Update the index.js to expose the new MigrationIssue class.

<code>

The new MigrationIssue class encapsulates all the properties that were previously spread across various objects in the create-migration-issues.js file. It provides a clean interface for creating and managing migration issues, with helper methods for generating issue titles and source types.

You can now use this class in create-migration-issues.js by replacing the manual object creation with the MigrationIssue class. For example, the createMigrationConfig function could be replaced with:

<code>
The process for creating new classes for the other entities was similar to the above. We created new classes for , , and , each with their own properties and methods.After we built out our entity classes, we realized they should manage their own state. This is a common pattern in object-oriented programming, where the class is responsible for managing its state and behavior. Remember our goal about following good coding standards?The class should include methods to get and set the state of the object, as well as methods to perform actions on the object. It should also be responsible for utilizing the GitHub API and manage the state of the object rather than the client code. This will help standardize and protect the state of the object, as well as give the client code a simpler interface to work with.Here was the prompt we entered for Copilot:The MigrationIssue class should contain a initializer that uses the gh-api.js to create the resulting GitHub Issue. The initializer should take in an optional parameter called issueNumber, if the issueNumber is omitted the initializer should create a new Migration Issue and return the issue number created, if the issueNumber is passed to the initializer, the gh-api.js should be used to return the title and body of the issue which the class will parse to create the properties of the MigrationIssue.
And here was Copilot’s response:I'll help you modify the code to add initialization logic to the MigrationIssue class. Here's the step-by-step solution:

1. Add a static method to parse issue body into configuration
2. Add initialization methods to handle both new and existing issues
3. Modify constructor to accept GHApi instance
4. Add method to generate issue body
5. Add method to create or load issue

migration-issue.js
Update the MigrationIssue class to handle initialization from GHApi.

<code>

The main changes include:

1. Constructor now accepts a GHApi instance as first parameter
2. Added parseIssueBody static method to parse configuration from issue body
3. Added generateIssueBody method to create the issue body format
4. Added initialize async method that either creates a new issue or loads an existing one
5. The initialize method returns the issue number in both cases

To use this updated class:

<code>
Remember how we said it’s always important to review suggestions from Copilot? In this case, the  method wasn’t connected to the constructor, like we would expect. We asked Copilot to address this issue with an iterative prompt.Here was the prompt we entered for Copilot:The MigrationIssue class has an initialize function that encapsulates functionality that should be included in the constructor. Rewire the constuctor method to include the initialzie function.
And here was Copilot’s response:Let's modify the class to move the initialization logic into the constructor. Here's the step-by-step solution:

1. Make constructors async
2. Move initialization logic into constructor
3. Remove separate initialize method
4. Keep static parseIssueBody method

migration-issue.js
Move initialization logic into constructor.

<code>

The main changes:

1. Removed initialize method from the class
2. Added static create and load factory methods
3. Added save method to handle issue creation
4. Constructor remains synchronous but now only handles property initialization

Usage example:

<code>

This approach follows better object-oriented principles by:

1. Using factory methods for different creation scenarios
2. Keeping the constructor simple and synchronous
3. Separating issue creation into a distinct operation
4. Making the API more intuitive
In this case, Copilot suggested a different approach than we were thinking of with a factory method (which is a great way to handle different scenarios when creating or loading an issue). Even if it wasn’t what we were thinking, it worked and also kept the constructor clean and focused on just initializing the object. Moreover, the  method is a good way to encapsulate the logic for creating a new issue, and it makes the code more readable.Now we just needed to repeat this process with each of the new classes we added.Step 4: Final refactoringAfter we built a common module, we needed to refactor the client code to make it much cleaner and easier to read and maintain. After that, we’d also be able to identify and add any missing functionality in the common module.This was an iterative process, and we needed to go back and forth between the client code and the common module to make sure everything worked properly. Because of the way this project evolved, it afforded us the opportunity to make sweeping changes to the codebase. It’s much more common to make small, incremental changes to the codebase, especially if it’s code that’s actively in use. In both of these cases, don’t forget to write tests to ensure the code works as expected.Here was the prompt we entered for Copilot:Using the gh-migrations module, refactor the create-migration-issues.js
And here was Copilot’s response:I'll help you refactor the create-migration-issues.js file to use the gh-migrations module. Here's the step-by-step solution:

1. Import the gh-migrations module
2. Create GHApi instance
3. Use MigrationIssue class for creating and managing migration issues
4. Simplify the main logic using the new classes

create-migration-issues.js
Refactor to use the gh-migrations module for better code organization and reusability.

<code>

The refactoring:

1. Removes redundant code by using the MigrationIssue class
2. Simplifies issue creation and management
3. Uses GHApi for GitHub API calls with built-in rate limiting
4. Removes duplicate code for different source types
5. Improves code organization and maintainability

The main changes:
  * Removed several helper functions as they're now part of the MigrationIssue class
  * Simplified the main logic flow
  * Added better error handling
  * Used structured configuration object
  * Maintained the same functionality while reducing code complexity
Now we needed to refactor the remaining scripts in the same manner. For example, this is how we used Copilot to help with refactoring the  script.Here was the prompt we entered for Copilot:Using the gh-migrations module refactor the create-team-issues.js
And here was Copilot’s response:I'll help you refactor the create-team-issues.js file to use the gh-migrations module. Here's the step-by-step solution:

1. Import the gh-migrations module
2. Replace manual issue creation with TeamIssue class
3. Simplify team creation logic using the new class
4. Maintain the same JiraIssue tracking functionality

create-team-issues.js
Refactor to use the gh-migrations module for creating team issues.

<code>

The refactoring:

1. Uses the TeamIssue class to manage team issue creation
2. Simplifies the code by removing redundant config and status objects
3. Uses GHApi for GitHub operations with built-in rate limiting
4. Maintains the same JiraIssue tracking functionality
5. Reduces code duplication by using a loop for team creation

The code is now more maintainable and follows the same pattern as the migration issues implementation.
From there, it was a matter of rinsing and repeating this process with each of our scripts.By first creating the common module, we could guide Copilot  and make sure that the final, refactored codebase was something we could use with multiple organizations. It gave us a foundation we could reuse and customize based on the needs of the system we were migrating.Sometimes refactoring code can be really easy, like our first example with the sequential  statements. But the more you dig into it, the more it grows, and you might find yourself facing down the Herculean task of trying to refactor a codebase that stretches across multiple files from a larger team of developers.Speaking from experience: It can be daunting to even know where to start.Remember to take a step back and start with the basics. Your first step should always be improving your  understanding of the codebase you want to refactor. The more you know about what you’re starting out with, the greater chance that you will be able to think of ways to refactor it effectively. And once you come up with some ideas, you can always ask GitHub Copilot to help turn your vision into reality. It’s got your back and is more than willing to help.Start using GitHub Copilot for free
Our free version of GitHub Copilot is included by default in personal GitHub accounts and VS Code to help you start new projects, manage existing repositories, and more.]]></content:encoded></item><item><title>Senator Pushes Zuckerberg on &quot;Perverse Abuse&quot; of Nudify Ads After 404 Media Report</title><link>https://www.404media.co/senator-pushes-zuckerberg-on-perverse-abuse-of-nudify-ads-after-404-media-report/</link><author>Emanuel Maiberg</author><category>tech</category><enclosure url="https://images.unsplash.com/photo-1568378780196-a9a0444a9151?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fG1hcmslMjB6dWNrZXJiZXJnfGVufDB8fHx8MTczOTI4ODE2N3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" length="" type=""/><pubDate>Tue, 11 Feb 2025 16:49:59 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[Senator and Ranking Member of the Senate Judiciary Committee Dick Durbin has sent a letter to Meta CEO Mark Zuckerberg asking about his company’s role in directing traffic to apps that use artificial intelligence to generate nonconsensual nude images. Specifically, Durbin’s letter cites  about Crushmate, a so-called “nudify” app that has repeatedly advertised its services on Meta platforms, often with nonconsensual nude images of women, and that according to traffic analysis firm Similarweb has sent Crushmate 90 percent of its traffic. Alexios Mantzarlis was first to track the traffic Meta was sending Crush in his .“Tech companies should not assist malevolent actors who seek to take advantage of women and children,” Durbin wrote in his letter to Zuckerberg. “I am gravely concerned with Meta’s failure to prevent this perverse abuse of its platforms and I refuse to accept Meta’s facilitation of these crimes. I therefore urge Meta to join us in combatting this threat.”]]></content:encoded></item><item><title>How A Chinese Villager Shook Silicon Valley [DeepSeek Founder]</title><link>https://www.youtube.com/watch?v=hd1-CKDyHXE</link><author>ColdFusion</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/hd1-CKDyHXE?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 16:34:32 +0000</pubDate><source url="https://www.youtube.com/channel/UC4QZ_LsYcvcq7qOsOhpAX4A">Cold Fusion</source><content:encoded><![CDATA[Go to https://ground.news/coldfusion to stay fully informed on every side of every story.

Liang Wenfeng, you may not know his name, but you know his company: DeepSeek. In recent weeks it's sent shockwaves around the world. But who is it's mysterious founder? And what is his philosophy? In this episode we find out.

ColdFusion Podcast: 

https://www.youtube.com/@ThroughTheWeb 

Watch or listen to ColdFusion on Spotify: https://open.spotify.com/show/1YEwCKoRz8fEDqheXB6UJ1


ColdFusion Music: 

https://www.youtube.com/@ColdFusionmusic
http://burnwater.bandcamp.com   

Get my book: 

http://bit.ly/NewThinkingbook 

ColdFusion Socials: 

https://discord.gg/coldfusion
https://facebook.com/ColdFusionTV 
https://twitter.com/ColdFusion_TV 
https://instagram.com/coldfusiontv

Created by: Dagogo Altraide
Producers: Tawsif Akkas, Dagogo Altraide]]></content:encoded></item><item><title>Insights from Cleric: Building an Autonomous AI SRE // Willem Pienaar // #290</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/Insights-from-Cleric-Building-an-Autonomous-AI-SRE--Willem-Pienaar--290-e2unikg</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/98338896/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-11%2F394679162-44100-2-e1c6c10bbd504.mp3" length="" type=""/><pubDate>Tue, 11 Feb 2025 16:32:23 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[ is the Co-Founder and CTO of. He previously worked at Tecton as a Principal Engineer. Willem Pienaar attended the Georgia Institute of Technology.Insights from Cleric: Building an Autonomous AI SRE // MLOps Podcast #289 with Willem Pienaar, CTO & Co-Founder of Cleric.// AbstractIn this MLOps Community Podcast episode, Willem Pienaar, CTO of Cleric, breaks down how they built an autonomous AI SRE that helps engineering teams diagnose production issues. We explore how Cleric builds knowledge graphs for system understanding, and uses existing tools/systems during investigations. We also get into some gnarly challenges around memory, tool integration, and evaluation frameworks, and some lessons learned from deploying to engineering teams.// BioWillem Pienaar, CTO of Cleric, is a builder with a focus on LLM agents, MLOps, and open source tooling. He is the creator of Feast, an open source feature store, and contributed to the creation of both the feature store and MLOps categories.Before starting Cleric, Willem led the open-source engineering team at Tecton and established the ML platform team at Gojek, where he built high-scale ML systems for the Southeast Asian Decacorn.// MLOps Swag/Merchhttps://shop.mlops.community/// Related LinksWebsite: willem.co --------------- ✌️Connect With Us ✌️ -------------Join our slack community:https://go.mlops.community/slackFollow us on Twitter:@mlopscommunitySign up for the next meetup:https://go.mlops.community/registerCatch all episodes, blogs, newsletters, and more:https://mlops.community/Connect with Demetrios on LinkedIn:https://www.linkedin.com/in/dpbrinkm/Connect with Willem on LinkedIn:https://www.linkedin.com/in/willempienaar/]]></content:encoded></item><item><title>Python Insider: Python 3.14.0 alpha 5 is out</title><link>https://pythoninsider.blogspot.com/2025/02/python-3140-alpha-5-is-out.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 16:25:58 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Here comes the antepenultimate alpha.This is an early developer preview of Python
3.14Python 3.14 is still in development. This release, 3.14.0a5, is the
fifth of seven planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2025-05-06) and, if necessary, may be modified or
deleted up until the release candidate phase (2025-07-22). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.14 are still being planned and
written. Among the new major new features and changes so far:The next pre-release of Python 3.14 will be the penultimate alpha,
3.14.0a6, currently scheduled for 2025-03-14.2025-01-29 marked the start of a new lunar year, the Year of the
Snake 🐍 (and the Year of Python?).For centuries, π was often approximated as 3 in China. Some time
between the years 1 and 5 CE, astronomer, librarian, mathematician and
politician Liu Xin (劉歆) calculated π as 3.154.Around 130 CE, mathematician, astronomer, and geographer Zhang Heng
(張衡, 78–139) compared the celestial circle with the diameter of the
earth as 736:232 to get 3.1724. He also came up with a formula for the
ratio between a cube and inscribed sphere as 8:5, implying the ratio of
a square’s area to an inscribed circle is √8:√5. From this, he
calculated π as √10 (~3.162).Third century mathematician Liu Hui (刘徽) came up with an algorithm
for calculating π iteratively: calculate the area of a polygon inscribed
in a circle, then as the number of sides of the polygon is increased,
the area becomes closer to that of the circle, from which you can
approximate π.This algorithm is similar to the method used by Archimedes in the 3rd
century BCE and Ludolph van Ceulen in the 16th century CE (see 3.14.0a2
  release notes), but Archimedes only went up to a 96-sided polygon
(96-gon). Liu Hui went up to a 192-gon to approximate π as 157/50 (3.14)
and later a 3072-gon for 3.14159.Liu Hu wrote a commentary on the book The Nine Chapters on the
Mathematical Art which included his π approximations.In the fifth century, astronomer, inventor, mathematician,
politician, and writer Zu Chongzhi (祖沖之, 429–500) used Liu Hui’s
algorithm to inscribe a 12,288-gon to compute π between 3.1415926 and
3.1415927, correct to seven decimal places. This was more accurate than
Hellenistic calculations and wouldn’t be improved upon for 900
years.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software
Foundation.Regards from a remarkably snowless Helsinki,Your release team, Hugo van KemenadeSteve Dower]]></content:encoded></item><item><title>Dual-Arm HyQReal Puts Powerful Telepresence Anywhere</title><link>https://spectrum.ieee.org/telepresence-robot</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjE1MDg3MC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2MzQyMjI5MH0.P7rZYSa6orpvLiwnLpf8wHNCw_oa4ckp2iPAj5ykgLg/image.jpg?width=600" length="" type=""/><pubDate>Tue, 11 Feb 2025 16:00:05 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[IIT’s hydraulic quadruped can carry a pair of massive arms]]></content:encoded></item><item><title>AMD NASes give us everything we ever wanted</title><link>https://www.youtube.com/watch?v=etayojgChDM</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/etayojgChDM?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 15:00:53 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[Two new NASes go all-in on AMD. But in totally different directions.

The 45Drives HL4 and ASUSTOR Lockerstor 6 Gen 3 were both provided for this video, though neither company paid any money or had any control over what I say about them.

More information and other videos I mentioned in this video (as an Amazon Associate I earn for qualifying purchases):

  - 45Drives / 45Homelab HL4: https://store.45homelab.com/configure/hl4
  - ASUSTOR Lockerstor 6 Gen 3: https://amzn.to/4huZNAn
  - Kioxia XG8 NVMe SSDs: https://amzn.to/42RpI0K
  - Seagate EXOS 20TB HDDs: https://amzn.to/40SifvV
  - RaidOwl's video on the Lockerstor and ADM: https://www.youtube.com/watch?v=VgmDEd_Ny-4
  - Hardware Haven's video on TrueNAS on the Lockerstor: https://www.youtube.com/watch?v=OTWiQs4TLjs
  - CraftComputing's video on the 45Homelab HL8: https://www.youtube.com/watch?v=IitwrXJuKzw
  - Lawrence System's video on 45Drive's Houston UI: https://www.youtube.com/watch?v=07uTKOB1z7E

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff

Contents:

00:00 - Two wildly different, expensive NASes
01:51 - Lockerstor 6 Gen 3 - teardown
04:38 - Rebuild and overview
07:12 - Plugging in HDDs
09:31 - AMD Ryzen Embedded + ECC RAM
10:27 - First boot and setup
12:14 - 45Homelab HL4 - teardown and overview
16:18 - The same, but different
17:43 - First boot and setup
18:37 - Networking nightmares
19:29 - Houston UI and a more Linux-y experience
20:53 - Network storage performance
22:38 - A lot to like (except maybe the price)]]></content:encoded></item><item><title>Wikipedia Prepares for &apos;Increase in Threats&apos; to US Editors From Musk and His Allies</title><link>https://www.404media.co/wikipedia-prepares-for-increase-in-threats-to-us-editors-from-musk-and-his-allies/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/02/CleanShot-2025-02-10-at-13.18.36@2x.png" length="" type=""/><pubDate>Tue, 11 Feb 2025 14:48:19 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[The Wikimedia Foundation is building new tools that it hopes will help Wikipedia editors stay anonymous in part to avoid harassment and legal threats as Elon Musk and the Heritage Foundation ramp up their attacks on people who edit Wikipedia. Some of the tactics have been pioneered by Wikimedia in countries with authoritarian governments and where editing Wikipedia is illegal or extremely dangerous. Last month,  obtained a document created by the Heritage Foundation called “Wikipedia Editor Targeting,” which set a goal to “identify and target Wikipedia editors abusing their position by analyzing text patterns, usernames, and technical data through data breach analysis, fingerprinting, HUMINT (human intelligence), and technical targeting.” The document discusses creating sock puppet accounts to “reveal patterns and provoke reactions,” discusses trying to track users’ geolocation, searching through hacked datasets for username reuse, and using Pimeyes, a facial recognition software, to learn the real identities of Wikipedia editors. Molly White of Citation Needed has an extensive rundown on Elon Musk’s crusade against Wikipedia, and both  and  have written about the right’s war on Wikipedia in recent days. In a series of calls and letters to the Wikimedia community over the last two weeks, Wikimedia executives have told editors that they are trying to figure out how to keep their users safe in an increasingly hostile political environment. “I’m keeping an eye on the rising noise of criticism from Elon Musk and others and I think that’s something we need to grapple with,” Wikimedia founder Jimmy Wales said in a meeting on January 30.“We’re seeing an increase in threats, both regulation and litigation across the world,” Wikimedia Foundation CEO Maryana Iskander told community members during the same January 30 meeting. “We’re all just trying to understand what is happening not only in the United States [but across the world], so the best we can do is monitor, check-in on staff, and try to understand what’s needed … that’s the most honest answer I can give you to an impossible set of questions we’re all grappling with on a daily basis.”Wikimedia lawyers told the community that the project is trying to change how editing Wikipedia for logged-out accounts works. Currently, if a user edits an article while not logged in, their IP address will show publicly, which can provide information to someone looking to file a defamation or libel lawsuit. Wikimedia is launching a “temporary accounts program” which will give editors who are not logged in a temporary username rather than showing an IP address. “It’s a way of ensuring that for logged-out users, their IP address isn’t visible to everyone asunder but rather available only to people who are really engaged in anti-vandalism,” Phil Bradley-Schmieg, a Wikimedia lawyer, said. Bradley-Schmieg also suggested that Wikimedia’s human rights team, which is focused on “helping users stay safe, particularly in countries where freedom of speech and expression is under attack on a regular basis,” may need to play a larger role across the entire project. Jacob Rogers, another Wikimedia lawyer, said during a separate meeting on January 30 that some Wikimedia projects in non-English languages have a feature where users are allowed to create and register a sock-puppet account (a dummy username, basically) to edit controversial articles and to register that account with administrators.“A number of the different language projects have the option to make legitimate sock puppet accounts if you’re going to work on something you know is going to be controversial, you can make a sock puppet and register it with admins on that project so it’s more obscure, kept separate from the rest of your life,” Rogers said. Both Rogers and Bradley-Schmieg said that Wikimedia has worked to limit the amount of data that the foundation has on any given user. IP addresses associated with edits are deleted or anonymized after 90 days, for example.“The foundation has very little data about most users, so if somebody is stepping up their harassment and coming to the foundation, we generally don’t really know anything about users in most cases and there’s not a lot they can get from us,” Rogers said. In the first six months of 2024, the last period for which data is available, Wikimedia received 26 formal requests for information on users; it provided info in two cases. Six of those requests came from the United States, the most of any jurisdiction.Wikimedia has also created a legal defense program that will in some cases fund the defense of Wikipedia editors who are attacked through the legal system as long as that editor or staffer was contributing to a Wikimedia project in good faith, Rogers said. Wikimedia has recently fought cases in both India and Germany.While Musk’s and the Heritage Foundation’s attacks on Wikipedia have escalated in recent days, these general trends are not new, and they were outlined as a threat in the foundation’s 2024 annual plan, which states the following: “Human rights threats are growing. Physical and legal threats against volunteers and staff who fight disinformation continue to grow. Accusations of bias and inaction by those whose preferred narratives do not prevail on Wikipedia may be encouraged and amplified by purveyors of disinformation,” the foundation wrote in an update to users. “Law is weaponized in important jurisdictions. Bad-faith lawsuits, by people who don’t like the verified information appearing on , are succeeding in some European countries. Some incumbent leaders are abusing their powers to silence and intimidate political opponents.”Iskander said in the meeting that the foundation is going to consider the safety of Wikipedians for its in-person events, such as Wikimania, an annual conference and party.“We’re paying very close attention trying to understand what the impacts might be and ensure those might be considered in any decisions we make. I will remind folks part of our processes in any event related to community gatherings is to do a risk assessment for community conferences for Wikimania,” she said. “It’s an imperfect and imprecise exercise but there’s a real intentionality around being thoughtful about the places that we’re selecting to ask people to gather and manage within our control.”It is not clear whether any of these steps will be sufficient, or whether any of them are going to make Wikipedia more resilient to right-wing attacks. What makes Wikipedia so strong is the fact that it has a distributed global base of dedicated volunteer editors, and a governance structure that is not very easy to infiltrate. Wikimedia’s decentralized power base makes it resistant to but not invulnerable from takeover attempts.During one of the meetings, Rogers was asked if Wikimedia would consider moving its headquarters out of the United States because of the political situation here. Rogers said moving “would probably not do very much because the projects would remain accessible in the United States and many things would still be subject to US law even if the foundation moved its headquarters to a different jurisdiction.”“I think a move would be extremely expensive and cost something in the tens to hundreds of millions of dollars,” he said. “I see that as one of the most significant, expensive, and extreme possible options. You would only do that if it was like, the only solution to a major problem where doing that would make sense.” The Wikimedia Foundation did not respond to a request for comment. The Heritage Foundation did not respond to a request for comment.]]></content:encoded></item><item><title>Rubidium Can Be More Than a Lithium Cast-Off</title><link>https://spectrum.ieee.org/rubidium</link><author>Elissa Welle</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjMyMTI2My9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5MjI3MTA3OH0.Ryeo_Q5ykelatJkzIly879sUfQg0LtgR9rSWXhd5QV0/image.jpg?width=600" length="" type=""/><pubDate>Tue, 11 Feb 2025 14:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[New extraction techniques makes the element—essential in high-tech timekeeping—easier to mine]]></content:encoded></item><item><title>Real Python: Building a Python Command-Line To-Do App With Typer</title><link>https://realpython.com/courses/build-command-line-todo-app-typer/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 14:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Building an application to manage your  can be an interesting project when you’re learning a new programming language or trying to take your skills to the next level. In this video course, you’ll build a functional to-do application for the command line using Python and Typer, which is a relatively young library for creating powerful command-line interface (CLI) applications in almost no time.With a project like this, you’ll apply a wide set of core programming skills while building a real-world application with real features and requirements.In this video course, you’ll learn how to:Build a functional  with a  in PythonUse Typer to add , , and  to your to-do appTest your Python to-do application with Typer’s  and ]]></content:encoded></item><item><title>10 Little-Known Python Libraries That Will Make You Feel Like a Data Wizard</title><link>https://www.kdnuggets.com/10-little-known-python-libraries-data-wizard</link><author>Josep Ferrer</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/Ferrer_10_little_known_libraries1.png" length="" type=""/><pubDate>Tue, 11 Feb 2025 13:00:13 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[In this article, I will introduce you to 10 little-known Python libraries every data scientist should know.]]></content:encoded></item><item><title>From XP to TCR &amp; Limbo • Kent Beck &amp; Daniel Terhorst-North • GOTO 2025</title><link>https://www.youtube.com/watch?v=48g7CRlMUD8</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/48g7CRlMUD8?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 13:00:05 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This interview was recorded for GOTO Unscripted. #GOTOcon #GOTOunscripted
https://gotopia.tech

Read the full transcription of this interview here:
https://gotopia.tech/articles/355

Kent Beck - Software Engineer & Creator of Extreme Programming @KentLBeck 
Daniel Terhorst-North - Originator of Behavior Driven Development (BDD) & Principal at Dan North & Associates @daniel-terhorst-north 

RESOURCES
Kent
https://bsky.app/profile/kentbeck.bsky.social
https://www.kentbeck.com
https://github.com/KentBeck
https://twitter.com/KentBeck
https://www.linkedin.com/in/kentbeck

Daniel
https://bsky.app/profile/tastapod.com
https://twitter.com/tastapod
https://linkedin.com/in/danielterhorstnorth
https://github.com/tastapod
https://mastodon.social/@tastapod
http://dannorth.net/blog

Links
https://youtu.be/tgaKAF_eiOg
https://youtu.be/k9lwn-RqEsE
https://youtu.be/Saaz6D1azlU
https://youtu.be/lOcXdXRxFgA
https://youtu.be/klqo1oPdbpM
https://youtu.be/ShAzlhS_TCg
https://youtu.be/86-Dy5U2p5Y
https://youtu.be/yuEbZYKgZas
https://youtu.be/-4Ybn0Cz2oU
https://youtu.be/XShcmCBK93E

DESCRIPTION
Kent Beck and Daniel Terhorst-North reflect on the evolution of Extreme Programming (XP) and its lasting impact on agile software development. They explore the importance of short feedback loops, iterative learning, and adaptability in different phases of product development, as outlined in Beck’s 3X model (Exploration, Expansion, Extraction).

Kent introduces radical techniques like Test && Commit || Revert (TCR) and Limbo, which challenge traditional coding and collaboration practices by emphasizing tiny, safe iterations. The conversation ties in psychological safety, a key factor in high-performing teams, as outlined by Amy C. Edmondson.

Kent envisions a future where software development scales in real-time, pushing teams to embrace faster experimentation, responsibility, and continuous learning in a rapidly evolving tech landscape. [...]

TIMECODES
00:00 Intro
00:53 Extreme Programming (XP)
03:09 The power of feedback & Continuous Delivery
07:44 Evolution of XP: From practices to people
11:39 3X Model: Managing risk through software evolution
25:02 TCR, Limbo & the future of software development
33:28 The importance of psychological safety for high-performing teams
39:32 Outro

RECOMMENDED BOOKS
Kent Beck • Tidy First? • https://amzn.to/4gscjjK
Kent Beck & Cynthia Andres • Extreme Programming Explained • https://amzn.to/3sBASDG
Amy C. Edmondson • The Fearless Organization • https://amzn.to/3Q5QPxu
Amy C. Edmondson • Right Kind of Wrong • https://amzn.to/3CRbiTD
Jez Humble & David Farley • Continuous Delivery • https://amzn.to/452ZRky

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#XP #EmpiricalDesign #ExtremeProgramming #Explore #Expand #Extract #3X #TDD #TCR #Limbo #PsychologicalSafety #Programming #SoftwareEngineering #KentBeck #DanNorth #DanielTerhorstNorth

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Rivian Flexes Software Power: What VW Gets for $5.7B</title><link>https://spectrum.ieee.org/rivian-volkswagen-software-ev</link><author>Lawrence Ulrich</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjI2NDU4My9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5OTA4MTY4OX0.wWq6j5o1EfjSUl0Y2ssr6wcIxkPln54qyXv3_YmvpXM/image.jpg?width=600" length="" type=""/><pubDate>Tue, 11 Feb 2025 13:00:05 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Struggling to crack the code on EVs, VW bets big on Rivian]]></content:encoded></item><item><title>Introducing the CrossValidationReport</title><link>https://www.youtube.com/watch?v=R6dRAE83Y2c</link><author>probabl</author><category>dev</category><category>ai</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/R6dRAE83Y2c?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 12:45:02 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Probabl</source><content:encoded><![CDATA[Skore version 0.6 introduces the `CrossValidationReport` that provides you with an `EstimatorReport` for each fold of your cross-validation, enabling you to inspect your estimator on each fold.

------
Links:

skore v0.6 documentation:
https://skore.probabl.ai/0.6/index.html

skore GitHub repository:
https://github.com/probabl-ai/skore

Website: https://probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Twitter: https://x.com/probabl_ai
Bluesky: https://bsky.app/profile/probabl.bsky.social
Discord: https://discord.probabl.ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

#probabl]]></content:encoded></item><item><title>Trusted Execution Environments</title><link>https://www.schneier.com/blog/archives/2025/02/trusted-encryption-environments.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Tue, 11 Feb 2025 12:08:36 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Really good—and detailed—survey of Trusted Execution Environments (TEEs.)]]></content:encoded></item><item><title>Kushal Das: Using openpgp-card-tool-git with git</title><link>https://kushaldas.in/posts/using-openpgp-card-tool-git-with-git.html</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 11:12:40 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[One of the power of Unix systems comes from the various small tools and how
they work together. One such new tool I am using for some time is for  &  using OpenPGP and my Yubikey for the actual signing
operation via
openpgp-card-tool-git. I
replaced the standard  for this usecase with the  command from this
project.Installation & configurationcargo install openpgp-card-tool-git
Then you will have to configuration your (in my case the global configuration) git configuration.git config --global gpg.program <path to oct-git>
I am assuming that you already had it configured before for signing, otherwise
you have to run the following two commands too.git config --global commit.gpgsign true
git config --global tag.gpgsign true
Before you start using it, you want to save the pin in your system keyring.Use the following command.That is it, now your  will sign the commits using  tool.In the next blog post I will show how to use the other tools from the 
author for various different OpenPGP oeprations.]]></content:encoded></item><item><title>StackGen’s New Migration Engine: A DevOps Game-Changer for Multi-Cloud Transitions</title><link>https://devops.com/stackgens-new-migration-engine-a-devops-game-changer-for-multi-cloud-transitions/</link><author>Tom Smith</author><category>devops</category><pubDate>Tue, 11 Feb 2025 10:17:03 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>LangChain and Agentic AI Engineering with Erick Friis</title><link>https://softwareengineeringdaily.com/2025/02/11/langchain-and-agentic-ai-engineering-with-erick-friis/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=langchain-and-agentic-ai-engineering-with-erick-friis</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED4262555283.mp3" length="" type=""/><pubDate>Tue, 11 Feb 2025 10:00:11 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[LangChain is a popular open-source framework to build applications that integrate LLMs with external data sources like APIs, databases, or custom knowledge bases. It’s commonly used for chatbots, question-answering systems, and workflow automation. Its flexibility and extensibility have made it something of a standard for creating sophisticated AI-driven software.Erick Friis is a Founding Engineer at LangChain and he leads their integrations and open source efforts. Erick joins the podcast to talk about what inspired the creation of LangChain, agentic flows vs. chained flows, emerging patterns of agentic AI design, and much more.]]></content:encoded></item><item><title>Rust vs. Go (Golang): Performance (Fastest Frameworks + PostgreSQL)</title><link>https://www.youtube.com/watch?v=31R8Ef9A0iw</link><author>Anton Putra</author><category>dev</category><category>video</category><enclosure url="https://www.youtube.com/v/31R8Ef9A0iw?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 09:25:18 +0000</pubDate><source url="https://www.youtube.com/channel/UCeLvlbC754U6FyFQbKc0UnQ">Anton Putra</source><content:encoded><![CDATA[Go vs Rust Performance Comparison. Go (1.23.6) vs Rust (1.84.1).

🔴 To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)

🍿 Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&si=p-UOaVM_6_SFx52H

👋 AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra

▬▬▬▬▬ Experience & Location 💼 ▬▬▬▬▬
►  I’m a Senior Software Engineer at Juniper Networks (13+ years of experience)
►  Located in San Francisco Bay Area, CA (US citizen)

▬▬▬▬▬▬ Connect with me 👋 ▬▬▬▬▬▬
►  LinkedIn: https://www.linkedin.com/in/anton-putra
►  Twitter/X: https://twitter.com/antonvputra
►  GitHub: https://github.com/antonputra
►  Email: me@antonputra.com

▬▬▬▬▬▬▬ Source Code 📚 ▬▬▬▬▬▬▬
► Original Source Code: https://github.com/antonputra/tutorials/tree/244/lessons/244

PR to improve Rust - https://github.com/antonputra/tutorials/pull/429
PR to improve Rust - https://github.com/antonputra/tutorials/pull/431
PR to improve Rust - https://github.com/antonputra/tutorials/pull/433

#rust #golang #devops]]></content:encoded></item><item><title>Fusing Security Into the Network Fabric: From Hybrid Mesh Firewalls to Universal ZTNA</title><link>https://blogs.cisco.com/security/fusing-security-into-the-network-fabric-from-hybrid-mesh-firewalls-to-universal-ztna/</link><author>Raj Chopra</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Tue, 11 Feb 2025 08:51:00 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[In the changing landscape of network security, the combination of Universal Zero Trust Network Access and Hybrid Mesh Firewalls offers a powerful defense.]]></content:encoded></item><item><title>Stateful vs. Stateless Architecture</title><link>https://blog.algomaster.io/p/stateful-vs-stateless-architecture</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/7e4801c3-e3aa-4ab6-8fe6-759af4a1f91a_1684x1196.png" length="" type=""/><pubDate>Tue, 11 Feb 2025 08:46:26 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[When a client interacts with a server, there are two ways to handle it: The client includes all necessary data in each request, so the server doesn’t store any prior information. The server retains some data from previous requests, making future interactions dependent on past state.In software systems,  refers to any data that persists across requests, such as user sessions, shopping carts, or authentication details.The choice between stateless and stateful architecture can affect scalability, performance, complexity, and cost.In this article, we’ll break down both the approaches, their advantages and trade-offs, and when to use each—with real-world examples.If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .In a , the system remembers client or process data () across multiple requests.Once a client connects, the server holds on to certain details—like user preferences, shopping cart contents, or authentication sessions—so the client doesn’t need to resend everything with each request.Stateful systems typically store the state data in a database or in-memory storage. During online shopping, when you add items to your cart, the website remembers your selections. If you navigate away to browse more items and then return to your cart, your items are still there, waiting for you to check out.Common Patterns in Stateful ArchitectureIf you use  session storage (i.e., each app server keeps its own sessions locally), you can configure your load balancer for “sticky sessions.” This means: Once a client is assigned to , all subsequent requests from that client are routed to .: If Server A fails, the user’s session data is lost or the user is forced to re-log in. Sticky sessions are also less flexible when scaling because you can’t seamlessly redistribute user traffic to other servers.2. Centralized Session StoreA more robust approach is to store session data in a  or  store (e.g., Redis). : All servers can access and update session data for any user. Any server can handle any request, because the session data is not tied to a specific server’s memory.: You introduce network overhead and rely on an external storage. If the centralized storage fails, you lose session data unless you have a fallback strategy.Personalized Experiences: Stateful systems can deliver highly tailored interactions, as they remember user preferences and past actions. Users can seamlessly resume activities where they left off, even if they disconnect and reconnect. Certain operations can be faster because the server already possesses necessary data. Maintaining state for a large number of users can become resource-intensive and complex, as each server needs to keep track of specific sessions. Managing and synchronizing state across multiple servers (if needed) introduces additional challenges. If a server holding a user's state fails, their session data might be lost.E-commerce Shopping Carts – Stores cart contents and user preferences across multiple interactions, even if the user navigates away and returns.Video Streaming Services (Netflix, YouTube) – Remembers user watch progress, recommendations, and session data for a seamless experience.Messaging Apps (WhatsApp, Slack) – Maintains active user sessions and message history for real-time communication.In a  architecture, the server does  preserve client-specific data between individual requests.Each request is treated as , with no memory of previous interactions.Every request must include all necessary information for processing.Once the server responds, it discards any temporary data used for that request.: Most  follow a stateless design. For instance, when you request weather data from a public API, you must provide all required details (e.g., location) in each request. The server processes it, sends a response, and forgets the interaction.Common Patterns in Stateless Architecture1. Token-Based Authentication (JWT)A very popular way to implement statelessness is through tokens, particularly  (JSON Web Tokens):Client Authenticates Once: The user logs in using credentials (username/password) for the first time, and the server issues a signed .: The client includes JWT token in each request (e.g., Authorization: Bearer <token> header).: The server validates the token’s signature and any embedded claims (e.g., user ID, expiry time).: The server does  need to store session data; it just verifies the token on each request.Many APIs, including OAuth-based authentication systems, use JWTs to enable stateless, scalable authentication.Stateless architectures benefit from , ensuring that repeated requests produce the same result. This prevents inconsistencies due to network retries or client errors. A  request with the same payload  updates the user’s data but doesn’t create duplicates.Idempotent APIsensures consistency and reliability, especially in distributed systems where requests might be retried automatically. Stateless systems are inherently easier to scale horizontally. New servers can be added effortlessly, as they don't need to maintain any specific user sessions. Since servers don't track state, the architecture is generally simpler and easier to manage. The failure of a single server won't disrupt user sessions, as data isn't tied to specific servers.With no session data stored on the server, you free up memory that would otherwise be reserved for session management.Easier to Cache Responses: Since requests are self-contained, caching layers (like CDNs) can more easily store and serve responses. Stateless systems can't provide the same level of personalization or context awareness as stateful systems without additional effort (like using cookies or tokens).The client must keep track of the authentication token or relevant data. If it loses the token, it must re-authenticate. Every request needs to carry all the required information, potentially leading to larger payloads.Microservices Architecture: Each service handles requests independently, relying on external databases or caches instead of maintaining session data.Public APIs (REST, GraphQL): Clients send tokens with each request, eliminating the need for server-side sessions.Tokens are securely stored on the device and sent with every request to authenticate users.Stateless endpoints make caching easier since responses depend only on request parameters, not stored session data. A CDNcan cache and serve repeated requests, improving performance and reducing backend load.There's no one-size-fits-all answer when choosing between stateful and stateless architectures.The best choice depends on your application’s needs, scalability goals, and user experience expectations.When to Choose Stateful ArchitectureStateful systems are ideal when user context and continuity are critical. Consider a stateful approach if your application:Requires personalization (e.g., user preferences, session history)Needs real-time interactions (e.g., chat applications, multiplayer gaming)Manages multi-step workflows (e.g., online banking transactions, checkout processes)Must retain authentication sessions for security and convenience A shopping cart in an e-commerce app should persist, so users don’t have to re-add items after refreshing the page.When to Choose Stateless ArchitectureStateless systems work best when scalability, simplicity, and resilience are top priorities. Use a stateless approach if your application:Handles a high volume of requests and needs to scale efficientlyDoesn’t require storing client-specific data between requestsNeeds fast, distributed processing without server dependenciesMust ensure reliability and failover readiness A weather API doesn’t need to remember previous requests. Each query includes the location, and the response is processed independently.Hybrid Approaches: The Best of Both WorldsMany modern applications  stateful and stateless components for flexibility.This hybrid approach allows:Stateless APIs for core functionality, ensuring high scalabilityStateful sessions for personalization, improving user experienceExternal session stores (e.g., Redis) to manage state while keeping app servers stateless A video streaming platform (e.g., Netflix) uses a stateless backend for streaming but retains stateful user sessions to track watch history and recommendations.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re enjoying this newsletter and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>Gcore Radar report reveals 56% year-on-year increase in DDoS attacks</title><link>https://devops.com/gcore-radar-report-reveals-56-year-on-year-increase-in-ddos-attacks/</link><author>cybernewswire</author><category>devops</category><pubDate>Tue, 11 Feb 2025 07:01:39 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DtSR Episode 640 - A Practitioner View of Security Automation</title><link></link><author></author><category>Down the Security Rabbit Hole Podcast</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/www.buzzsprout.com/2153215/episodes/16600392-dtsr-episode-640-a-practitioner-view-of-security-automation.mp3" length="" type=""/><pubDate>Tue, 11 Feb 2025 05:00:00 +0000</pubDate><source url="https://blogwh1t3rabbit.medium.com/">Down the Security Rabbithole Podcast (DtSR)</source></item><item><title>Django Weblog: DSF member of the month - Lily Foote</title><link>https://www.djangoproject.com/weblog/2025/feb/10/dsf-member-of-the-month-lily-foote/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 04:51:31 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[For February 2025, we welcome Lily Foote (@lilyf) as our DSF member of the month! ⭐Lily Foote is a contributor to Django core for many years, especially on the ORM. She is currently a member of the Django 6.x Steering Council and she has been a DSF member since March 2021. 
You can learn more about Lily by visiting her GitHub profile.Let’s spend some time getting to know Lily better!Can you tell us a little about yourself (hobbies, education, etc)My name is Lily Foote and I’ve been contributing to Django for most of my career. I’ve also recently got into Rust and I’m excited about using Rust in Python projects. When I’m not programming, I love hiking, climbing and dancing (Ceilidh)! I also really enjoying playing board games and role playing games (e.g. Dungeons and Dragons).How did you start using Django?I’d taught myself Python in my final year at university by doing Project Euler problems and then decided I wanted to learn how to make a website. Django was the first Python web framework I looked at and it worked really well for me.What other framework do you know and if there is anything you would like to have in Django if you had magical powers?I’ve done a small amount with Flask and FastAPI. More than any new features, I think the thing that I’d most like to see is more long-term contributors to spread the work of keeping Django awesome.What projects are you working on now?The side project I’m most excited about is Django Rusty Templates, which is a re-implementation of Django’s templating language in Rust.Which Django libraries are your favorite (core or 3rd party)?What are the top three things in Django that you like?Django Conferences, the mentorship program Djangonaut Space and the whole community!You have been a mentor multiple times with GSoC and Djangonaut Space program, what is required according to you to be a good mentor?I think being willing to invest time is really important. Checking in with your mentees frequently and being an early reviewer of their work. I think this helps keep their motivation up and allows for small corrections early on.Any advice for future contributors?Start small and as you get more familiar with Django and the process of contributing you can take on bigger issues. Also be patient with reviewers – Django has high standards, but is mostly maintained by volunteers with limited time.Yes! It’s a huge honour! Since January, we’ve been meeting weekly and it feels like we’ve hardly scratched the surface of what we want to achieve. The biggest thing we’re trying to tackle is how to improve the contribution experience – especially evaluating new feature ideas – without draining everyone’s time and energy.You have a lot of knowledge in the Django ORM, how did you start to contribute to this part?I added the Greatest and Least expressions in Django 1.9, with the support of one of the core team at the time. After that, I kept showing up (especially at conference sprints) and finding a new thing to tackle.Is there anything else you’d like to say?Thank you for doing the interview, Lily!]]></content:encoded></item><item><title>ISC Stormcast For Tuesday, February 11th, 2025 https://isc.sans.edu/podcastdetail/9318, (Tue, Feb 11th)</title><link>https://isc.sans.edu/diary/rss/31672</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Tue, 11 Feb 2025 02:00:02 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[
 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>Go 1.24 is released!</title><link>https://go.dev/blog/go1.24</link><author>Junyang Shao, on behalf of the Go team</author><category>dev</category><category>official</category><category>go</category><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Golang Blog</source><content:encoded><![CDATA[
      Junyang Shao, on behalf of the Go team
      11 February 2025
      Today the Go team is excited to release Go 1.24,
which you can get by visiting the download page.Go 1.24 comes with many improvements over Go 1.23. Here are some of the notable
changes; for the full list, refer to the release notes.Several performance improvements in the runtime have decreased CPU overhead
by 2–3% on average across a suite of representative benchmarks. These
improvements include a new builtin  implementation based on
Swiss Tables, more efficient
memory allocation of small objects, and a new runtime-internal mutex
implementation.The  command now provides a mechanism for tracking tool dependencies for a
module. Use  to add a  directive to the current module. Use
 to run the tools declared with the  directive.
Read more on the go command in the release notes.The new  analyzer in  subcommand reports common mistakes in
declarations of tests, fuzzers, benchmarks, and examples in test packages.
Read more on vet in the release notes.Standard library additionsImproved WebAssembly supportGo 1.24 adds a new  directive for Go programs to export
functions to the WebAssembly host, and supports building a Go program as a WASI
reactor/library.
Read more on WebAssembly in the release notes.Please read the Go 1.24 release notes for the complete and
detailed information. Don’t forget to watch for follow-up blog posts that
will go in more depth on some of the topics mentioned here!Thank you to everyone who contributed to this release by writing code and
documentation, reporting bugs, sharing feedback, and testing the release
candidates. Your efforts helped to ensure that Go 1.24 is as stable as possible.
As always, if you notice any problems, please file an issue.]]></content:encoded></item><item><title>Quansight Labs Blog: PEP 517 build system popularity</title><link>https://labs.quansight.org/blog/pep-517-build-system-popularity</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[Analysis of PEP 517 build backends used in 8000 top PyPI packages]]></content:encoded></item><item><title>Seth Michael Larson: Building software for connection (#2: Consensus)</title><link>https://sethmlarson.dev/building-software-for-connection-consensus?utm_campaign=rss</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[In the previous article we concluded that a persistent always-on internet
connection isn't required for software to elicit feelings of connection between humans.Building on this conclusion: let's explore how Animal Crossing software was able to intercommunicate without requiring
a centralized server and infrastructure and the trade-offs for these design decisions.Distributing digital goods without the internetAnimal Crossing has over 1,000 unique items that need to be collected
for a complete catalog, including furniture, wallpapers, clothing, parasols, and carpets.
Many of these items are quite rare or were only programmed to be accessible
through an official Nintendo-affiliated distribution such as a magazine or online contest.Beyond official distributions, it's clear Animal Crossings' designer, Katsuya Eguchi,
wanted players to  to complete their catalogs.
The game incentivized trading items between towns by assigning
one “native fruit” (Apple, Orange, Cherry, Peach, or Pear) and
randomly making a subset of items harder to find than others depending
on a hidden “item group” variable (either A, B, or C).Items could be exchanged between players when one player visits another town,
but this required physically bringing your memory card to another
players' GameCube. The GameCube might have come with a handle, but the 'cube wasn't exactly a . Sharing a physical space isn't something you can do with everyone or on a regular basis.So what did Katsuya Eguchi design for Animal Crossing? To allow for item distributions from magazines and contests and to make player-to-player item sharing easier Animal Crossing included a feature called “secret codes”.This feature worked by allowing players to exchange 28-character codes with Tom Nook for items. Players could also generate codes for their friends to “send” an item from their own game to a different town. Codes could be shared by writing them on a paper note, instant message, or text message.The forgotten durability of offline software
This Reddit comment thread from the GameCube subreddit was the initial inspiration for this entire series.
The post is about someone's niece who just started playing Animal Crossing for the first time.
The Redditor asked folks to send items to their nieces' town using the secret code system.
This ended up surprising many folks that this system 
 in a game that was over 23 years old!
For reference, Nintendo Wi-Fi Connection and Nintendo Network were only available for 8 and 13 years respectively.
Below are a handful of the comments from the thread:“For real does this still work lol?”It's hard not to take these comments as indicators that something is
 with internet-connected software today. What had to go wrong for a 
system continuing to work to ? Many consumers' 
experience with 
software products
today is that they become useless e-waste after some far-away service is 
discontinued a few years after purchase.My intuition from this is that software that requires centralized servers and infrastructure to function
will have shorter lifetimes than software which is offline or only
opportunistically uses online functionality.I don't think this is particularly insightful,
more dependencies always means less resilience. But if we're building software for human connection then the software
should optimally only be limited by the availability of humans to connect.What is centralization good for?Animal Crossings' secret code system is far from perfect. The system is easily abusable, as the same secret codes can be
reused over-and-over by the same user to duplicate items without ever expiring. The only limit was that 3 codes could be used per day.Not long after Animal Crossing's release
the secret code algorithm was reverse-engineered so secret codes 
for any item could be created for any town and recipient name as if they came from an official Nintendo distribution.
This was possible because the secret code system relied on "security through obscurity".Could  be the answer to preventing these abuses?The most interesting property that a centralized authority approach
provides is : forcing everyone to play by the same rules. By storing
the “single source-of-truth” a central authority is able to prevent abuses
like the ones mentioned above.For example, a centralized “secret code issuing server” could generate
new unique codes per-use and check each code's validity
against a database to prevent users from generating their
own illegitimate codes or codes being re-used multiple times.The problem with
centralized consensus is it tends to be  to cover the entire software state.
A centralized server can generate codes perfectly, but how can that same server
 that the items you're exchanging for codes were obtained legitimately? To know this
the server would also need to track item legitimacy, leading to software which requires
an internet connection to operate.This is optimal from a correctness perspective, but as was noted earlier,
I suspect that if such a server was a mandatory part of the secret code system
in Animal Crossing that the system would likely not be usable today.This seems like a trade-off, which future would you rather have?Redesigning Animal Crossing secret codesIf I were designing Animal Crossings' secret code system with modern hardware, what would it look like?
How can we keep the offline fall-back while providing consensus and being less
abusable, especially for official distributions.I would likely use a public-key cryptographic system for official distributions,
embedding a certificate that could be used to “verify” that specific secret codes
originated from the expected centralized entity. Codes that are accepted would be
recorded to prevent reusing the same code multiple times in the same town.
Using public-key cryptography prevents the
system from being reverse-engineered to distribute arbitrary items until the certificate
private key was cracked.For sharing items between players I would implement a system where each town
generated a public and private key and the public key was shared to other towns
whenever the software was able to, such as when a player visited the other town.
Players would only be able to send items to players that they have visited
(which for Animal Crossing required physical presence, more on this later!)Each sender could store a nonce value for
each potential recipient. Embedding that nonce into the secret code would allow
the recipients' software to verify that the specific code hadn't been used yet.
The nonce wouldn't have to be long to avoid simple reusing of codes.Both above systems would require much more data to be embedded into each “secret
code” compared to the 28-character codes from the GameCube. For this I would
use QR codes to embed over 2KB of data into a single QR code. Funnily enough,
Animal Crossing New Leaf and onwards use QR code technology for players to share design patterns.This design is still abusable if users can modify their software or hardware
but doesn't suffer from the trivial-to-exploit flaws of Animal Crossing's secret code system.Decentralized global consensus?What if we could have the best of both worlds: we want consensus
that is both  and . At least today, we are out of luck.Decentralized global consensus is technologically feasible, but the existing solutions
(mostly blockchains)
are expensive (both in energy and capital) and can't handle throughput on any sort of 
meaningful scale.There are many other decentralized consensus systems that 
are able to form “pockets” of useful peer-to-peer consensus using a fraction of
the resources, such as email, BitTorrent, ActivityPub, and Nostr.
These systems are only possible by adding  or by only guaranteeing .When is global consensus needed?Obviously global consensus is important for certain classes of software like 
financial, civics, and infrastructure, but I wonder how the necessity
of consensus in software changes for software with different risk
profiles.For software which has fewer risks associated with misuse is there as much
need for global consensus?
How can  be designed to reduce risk and require
less consensus to be effective? If global consensus and centralized 
servers become unnecessary, can we expect  to be usable 
on much longer timescales, essentially for as long as there are users?]]></content:encoded></item><item><title>Brave Now Lets You Inject Custom JavaScript To Tweak Websites</title><link>https://tech.slashdot.org/story/25/02/10/2348258/brave-now-lets-you-inject-custom-javascript-to-tweak-websites?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Mon, 10 Feb 2025 23:50:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[Brave Browser version 1.75 introduces "custom scriptlets," a new feature that allows advanced users to inject their own JavaScript into websites for enhanced customization, privacy, and usability. The feature is similar to the TamperMonkey and GreaseMonkey browser extensions, notes BleepingComputer. From the report: "Starting with desktop version 1.75, advanced Brave users will be able to write and inject their own scriptlets into a page, allowing for better control over their browsing experience," explained Brave in the announcement. Brave says that the feature was initially created to debug the browser's adblock feature but felt it was too valuable not to share with users. Brave's custom scriptlets feature can be used to modify webpages for a wide variety of privacy, security, and usability purposes.
 
For privacy-related changes, users write scripts that block JavaScript-based trackers, randomize fingerprinting APIs, and substitute Google Analytics scripts with a dummy version. In terms of customization and accessibility, the scriptlets could be used for hiding sidebars, pop-ups, floating ads, or annoying widgets, force dark mode even on sites that don't support it, expand content areas, force infinite scrolling, adjust text colors and font size, and auto-expand hidden content.
 
For performance and usability, the scriptlets can block video autoplay, lazy-load images, auto-fill forms with predefined data, enable custom keyboard shortcuts, bypass right-click restrictions, and automatically click confirmation dialogs. The possible actions achievable by injected JavaScript snippets are virtually endless. However, caution is advised, as running untrusted custom scriptlets may cause issues or even introduce some risk.]]></content:encoded></item><item><title>I ♥ Free Software Day 2025: Let&apos;s celebrate the people who make and maintain free software</title><link>http://www.fsf.org/blogs/community/i-love-free-software-2025</link><author></author><category>linux</category><pubDate>Mon, 10 Feb 2025 22:10:00 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[Let's celebrate the people behind free software and tell them how much we appreciate their work! ]]></content:encoded></item><item><title>Tech is supposed to make our lives easier (News)</title><link>https://changelog.com/news/131</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/news/131/changelog-news-131.mp3" length="" type=""/><pubDate>Mon, 10 Feb 2025 21:45:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Bill Maher excoriates the software industry for making our lives more difficult, two professors from the University of Washington put together a curriculum to help us manage life in the ChatGPT world, Daniel Delaney thinks deeply on chat as a dev tool UI, Benedict Evans explores our assumptions that computers be ‘correct’ & the Thoughtbot team writes up six cases when not to refactor.Changelog++ members support our work, get closer to the metal, and make the ads disappear. Join today!Augment Code – Developer AI that uses deep understanding of your large codebase and how you build software to deliver personalized code suggestions and insights. Augment provides relevant, contextualized code right in your IDE or Slack. It transforms scattered knowledge into code or answers, eliminating time spent searching docs or interrupting teammates.
]]></content:encoded></item><item><title>OpenAI’s secret weapon against Nvidia dependence takes shape</title><link>https://arstechnica.com/ai/2025/02/openais-secret-weapon-against-nvidia-dependence-takes-shape/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/02/ai_chip_illustration-1152x648.jpg" length="" type=""/><pubDate>Mon, 10 Feb 2025 21:00:33 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[OpenAI is entering the final stages of designing its long-rumored AI processor with the aim of decreasing the company's dependence on Nvidia hardware, according to a Reuters report released Monday. The ChatGPT creator plans to send its chip designs to Taiwan Semiconductor Manufacturing Co. (TSMC) for fabrication within the next few months, but the chip has not yet been formally announced.The OpenAI chip's full capabilities, technical details, and exact timeline are still unknown, but the company reportedly intends to iterate on the design and improve it over time, giving it leverage in negotiations with chip suppliers—and potentially granting the company future independence with a chip design it controls outright.In the past, we've seen other tech companies, such as Microsoft, Amazon, Google, and Meta, create their own AI acceleration chips for reasons that range from cost reduction to relieving shortages of AI chips supplied by Nvidia, which enjoys a near-market monopoly on high-powered GPUs (such as the Blackwell series) for data center use.]]></content:encoded></item><item><title>Transforming credit decisions using generative AI with Rich Data Co and AWS</title><link>https://aws.amazon.com/blogs/machine-learning/transforming-credit-decisions-using-generative-ai-with-rich-data-co-and-aws/</link><author>Daniel Wirjo</author><category>dev</category><category>ai</category><pubDate>Mon, 10 Feb 2025 20:05:34 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This post is co-written with Gordon Campbell, Charles Guan, and Hendra Suryanto from RDC. The mission of Rich Data Co (RDC) is to broaden access to sustainable credit globally. Its software-as-a-service (SaaS) solution empowers leading banks and lenders with deep customer insights and AI-driven decision-making capabilities.Making credit decisions using AI can be challenging, requiring data science and portfolio teams to synthesize complex subject matter information and collaborate productively. To solve this challenge, RDC used generative AI, enabling teams to use its solution more effectively: – Designed for data science teams, this agent assists teams in developing, building, and deploying AI models within a regulated environment. It aims to boost team efficiency by answering complex technical queries across the machine learning operations (MLOps) lifecycle, drawing from a comprehensive knowledge base that includes environment documentation, AI and data science expertise, and Python code generation. – Designed for portfolio managers and analysts, this agent facilitates natural language inquiries about loan portfolios. It provides critical insights on performance, risk exposures, and credit policy alignment, enabling informed commercial decisions without requiring in-depth analysis skills. The assistant is adept at high-level questions (such as identifying high-risk segments or potential growth opportunities) and one-time queries, allowing the portfolio to be diversified.In this post, we discuss how RDC uses generative AI on Amazon Bedrock to build these assistants and accelerate its overall mission of democratizing access to sustainable credit.Solution overview: Building a multi-agent generative AI solutionWe began with a carefully crafted evaluation set of over 200 prompts, anticipating common user questions. Our initial approach combined prompt engineering and traditional Retrieval Augmented Generation (RAG). However, we encountered a challenge: accuracy fell below 90%, especially for more complex questions.To overcome the challenge, we adopted an agentic approach, breaking down the problem into specialized use cases. This strategy equipped us to align each task with the most suitable foundation model (FM) and tools. Our multi-agent framework is orchestrated using LangGraph, and it consisted of: – The orchestrator is responsible for routing user questions to the appropriate agent. In this example, we start with the data science or portfolio agent. However, we envision many more agents in the future. The orchestrator can also use user context, such as the user’s role, to determine routing to the appropriate agent. – The agent is designed for a specialized task. It’s equipped with the appropriate FM for the task and the necessary tools to perform actions and access knowledge. It can also handle multiturn conversations and orchestrate multiple calls to the FM to reach a solution. – Tools extend agent capabilities beyond the FM. They provide access to external data and APIs or enable specific actions and computation. To efficiently use the model’s context window, we construct a tool selector that retrieves only the relevant tools based on the information in the agent state. This helps simplify debugging in the case of errors, ultimately making the agent more effective and cost-efficient.This approach gives us the right tool for the right job. It enhances our ability to handle complex queries efficiently and accurately while providing flexibility for future improvements and agents.The following image is a high-level architecture diagram of the solution.Data science agent: RAG and code generationTo boost productivity of data science teams, we focused on rapid comprehension of advanced knowledge, including industry-specific models from a curated knowledge base. Here, RDC provides an integrated development environment (IDE) for Python coding, catering to various team roles. One role is model validator, who rigorously assesses whether a model aligns with bank or lender policies. To support the assessment process, we designed an agent with two tools: – Amazon Bedrock Knowledge Bases powers our intelligent content retrieval through a streamlined RAG implementation. The service automatically converts text documents to their vector representation using Amazon Titan Text Embeddings and stores them in Amazon OpenSearch Serverless. Because the knowledge is vast, it performs semantic chunking, making sure that the knowledge is organized by topic and can fit within the FM’s context window. When users interact with the agent, Amazon Bedrock Knowledge Bases using OpenSearch Serverless provides fast, in-memory semantic search, enabling the agent to retrieve the most relevant chunks of knowledge for relevant and contextual responses to users. – With code generation, we selected Anthropic’s Claude model on Amazon Bedrock due to its inherent ability to understand and generate code. This tool is grounded to answer queries related to data science and can generate Python code for quick implementation. It’s also adept at troubleshooting coding errors.Portfolio agent: Text-to-SQL and self-correctionTo boost the productivity of credit portfolio teams, we focused on two key areas. For portfolio managers, we prioritized high-level commercial insights. For analysts, we enabled deep-dive data exploration. This approach empowered both roles with rapid understanding and actionable insights, streamlining decision-making processes across teams.Our solution required natural language understanding of structured portfolio data stored in Amazon Aurora. This led us to base our solution on a text-to-SQL model to efficiently bridge the gap between natural language and SQL.To reduce errors and tackle complex queries beyond the model’s capabilities, we developed three tools using Anthropic’s Claude model on Amazon Bedrock for self-correction: – Verifies and corrects SQL queries, addressing common issues such as data type mismatches or incorrect function usage – Validates query results, providing relevance and prompting retries or user clarification when needed – Engages users for additional information when queries are too broad or lack detail, guiding the interaction based on database information and user inputThese tools operate in an agentic system, enabling accurate database interactions and improved query results through iterative refinement and user engagement.To improve accuracy, we tested model fine-tuning, training the model on common queries and context (such as database schemas and their definitions). This approach reduces inference costs and improves response times compared to prompting at each call. Using Amazon SageMaker JumpStart, we fine-tuned Meta’s Llama model by providing a set of anticipated prompts, intended answers, and associated context. Amazon SageMaker Jumpstart offers a cost-effective alternative to third-party models, providing a viable pathway for future applications. However, we didn’t end up deploying the fine-tuned model because we experimentally observed that prompting with Anthropic’s Claude model provided better generalization, especially for complex questions. To reduce operational overhead, we will also evaluate structured data retrieval on Amazon Bedrock Knowledge Bases.Conclusion and next steps with RDCTo expedite development, RDC collaborated with AWS Startups and the AWS Generative AI Innovation Center. Through an iterative approach, RDC rapidly enhanced its generative AI capabilities, deploying the initial version to production in just 3 months. The solution successfully met the stringent security standards required in regulated banking environments, providing both innovation and compliance.“The integration of generative AI into our solution marks a pivotal moment in our mission to revolutionize credit decision-making. By empowering both data scientists and portfolio managers with AI assistants, we’re not just improving efficiency—we’re transforming how financial institutions approach lending.”–Gordon Campbell, Co-Founder & Chief Customer Officer at RDCRDC envisions generative AI playing a significant role in boosting the productivity of the banking and credit industry. By using this technology, RDC can provide key insights to customers, improve solution adoption, accelerate the model lifecycle, and reduce the customer support burden. Looking ahead, RDC plans to further refine and expand its AI capabilities, exploring new use cases and integrations as the industry evolves.For more information about how to work with RDC and AWS and to understand how we’re supporting banking customers around the world to use AI in credit decisions, contact your AWS Account Manager or visit Rich Data Co.For more information about generative AI on AWS, refer to the following resources: is a Solutions Architect at AWS, focused on FinTech and SaaS startups. As a former startup CTO, he enjoys collaborating with founders and engineering leaders to drive growth and innovation on AWS. Outside of work, Daniel enjoys taking walks with a coffee in hand, appreciating nature, and learning new ideas. leads a science team at the AWS Generative AI Innovation Center in the Asia Pacific regions. His team partners with AWS customers on generative AI projects, with the goal of accelerating customers’ adoption of generative AI. is a computer scientist at the Generative AI Innovation Center at Amazon Web Services (AWS) working on Generative AI and complex multi-agents systems. is the Chief Customer Officer and Co-Founder of RDC, where he leverages over 30 years in enterprise software to drive RDC’s leading AI Decisioning platform for business and commercial lenders. With a proven track record in product strategy and development across three global software firms, Gordon is committed to customer success, advocacy, and advancing financial inclusion through data and AI. is the Chief Technology Officer and Co-founder of RDC. With more than 20 years of experience in data analytics and enterprise applications, he has driven technological innovation across both the public and private sectors. At RDC, Charles leads research, development, and product advancement—collaborating with universities to leverage advanced analytics and AI. He is dedicated to promoting financial inclusion and delivering positive community impact worldwide. is the Chief Data Scientist at RDC with more than 20 years of experience in data science, big data, and business intelligence. Before joining RDC, he served as a Lead Data Scientist at KPMG, advising clients globally. At RDC, Hendra designs end-to-end analytics solutions within an Agile DevOps framework. He holds a PhD in Artificial Intelligence and has completed postdoctoral research in machine learning.]]></content:encoded></item><item><title>Harness Merges with Traceable to Provide Integrated DevSecOps Platform</title><link>https://devops.com/harness-merges-with-traceable-to-provide-integrated-devsecops-platform/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 10 Feb 2025 19:36:12 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Build agentic AI solutions with DeepSeek-R1, CrewAI, and Amazon SageMaker AI</title><link>https://aws.amazon.com/blogs/machine-learning/build-agentic-ai-solutions-with-deepseek-r1-crewai-and-amazon-sagemaker-ai/</link><author>Surya Kari</author><category>dev</category><category>ai</category><pubDate>Mon, 10 Feb 2025 19:33:10 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[AI agents are rapidly becoming the next frontier in enterprise transformation, with 82% of organizations planning adoption within the next 3 years. According to a Capgemini survey of 1,100 executives at large enterprises, 10% of organizations already use AI agents, and more than half plan to use them in the next year. The recent release of the DeepSeek-R1 models brings state-of-the-art reasoning capabilities to the open source community. Organizations can build agentic applications using these reasoning models to execute complex tasks with advanced decision-making capabilities, enhancing efficiency and adaptability.In this post, we dive into how organizations can use Amazon SageMaker AI, a fully managed service that allows you to build, train, and deploy ML models at scale, and can build AI agents using CrewAI, a popular agentic framework and open source models like DeepSeek-R1.Agentic design vs. traditional software designAgentic systems offer a fundamentally different approach compared to traditional software, particularly in their ability to handle complex, dynamic, and domain-specific challenges. Unlike traditional systems, which rely on rule-based automation and structured data, agentic systems, powered by large language models (LLMs), can operate autonomously, learn from their environment, and make nuanced, context-aware decisions. This is achieved through modular components including reasoning, memory, cognitive skills, and tools, which enable them to perform intricate tasks and adapt to changing scenarios.Traditional software platforms, though effective for routine tasks and horizontal scaling, often lack the domain-specific intelligence and flexibility that agentic systems provide. For example, in a manufacturing setting, traditional systems might track inventory but lack the ability to anticipate supply chain disruptions or optimize procurement using real-time market insights. In contrast, an agentic system can process live data such as inventory fluctuations, customer preferences, and environmental factors to proactively adjust strategies and reroute supply chains during disruptions.Enterprises should strategically consider deploying agentic systems in scenarios where adaptability and domain-specific expertise are critical. For instance, consider customer service. Traditional chatbots are limited to preprogrammed responses to expected customer queries, but AI agents can engage with customers using natural language, offer personalized assistance, and resolve queries more efficiently. AI agents can significantly improve productivity by automating repetitive tasks, such as generating reports, emails, and software code. The deployment of agentic systems should focus on well-defined processes with clear success metrics and where there is potential for greater flexibility and less brittleness in process management.Generative AI on SageMaker AISageMaker AI, a fully managed service, provides a comprehensive suite of tools designed to deliver high-performance, cost-efficient machine learning (ML) and generative AI solutions for diverse use cases. SageMaker AI empowers you to build, train, deploy, monitor, and govern ML and generative AI models through an extensive range of services, including notebooks, jobs, hosting, experiment tracking, a curated model hub, and MLOps features, all within a unified integrated development environment (IDE).SageMaker AI simplifies the process for generative AI model builders of all skill levels to work with foundation models (FMs):Amazon SageMaker Canvas enables data scientists to seamlessly use their own datasets alongside FMs to create applications and architectural patterns, such as chatbots and Retrieval Augmented Generation (RAG), in a low-code or no-code environment.Amazon SageMaker JumpStart offers a diverse selection of open and proprietary FMs from providers like Hugging Face, Meta, and Stability AI. You can deploy or fine-tune models through an intuitive UI or APIs, providing flexibility for all skill levels.With SageMaker AI, you can build generative AI-powered agentic workflows using a framework of your choice. Some of the key benefits of using SageMaker AI for fine-tuning and hosting LLMs or FMs include: – SageMaker AI offers access to SageMaker JumpStart, a curated model hub where models with open weights are made available for seamless deployment through a few clicks or API calls. Additionally, for Hugging Face Hub models, SageMaker AI provides pre-optimized containers built on popular open source hosting frameworks such as vLLM, NVIDIA Triton, and Hugging Face Text Generation Inference (TGI). You simply need to specify the model ID, and the model can be deployed quickly.Instance-based deterministic pricing – SageMaker AI hosted models are billed based on instance-hours rather than token usage. This pricing model enables you to more accurately predict and manage generative AI inference costs while scaling resources to accommodate incoming request loads.Deployments with quantization – SageMaker AI enables you to optimize models prior to deployment using advanced strategies such as quantized deployments (such as AWQ, GPTQ, float16, int8, or int4). This flexibility allows you to efficiently deploy large models, such as a 32-billion parameter model, onto smaller instance types like ml.g5.2xlarge with 24 GB of GPU memory, significantly reducing resource requirements while maintaining performance.Inference load balancing and optimized routing – SageMaker endpoints support load balancing and optimized routing with various strategies, providing users with enhanced flexibility and adaptability to accommodate diverse use cases effectively.SageMaker fine-tuning recipes – SageMaker offers ready-to-use recipes for quickly training and fine-tuning publicly available FMs such as Meta’s Llama 3, Mistral, and Mixtral. These recipes use Amazon SageMaker HyperPod (a SageMaker AI service that provides resilient, self-healing clusters optimized for large-scale ML workloads), enabling efficient and resilient training on a GPU cluster for scalable and robust performance.CrewAI provides a robust framework for developing multi-agent systems that integrate with AWS services, particularly SageMaker AI. CrewAI’s role-based agent architecture and comprehensive performance monitoring capabilities work in tandem with Amazon CloudWatch.The framework excels in workflow orchestration and maintains enterprise-grade security standards aligned with AWS best practices, making it an effective solution for organizations implementing sophisticated agent-based systems within their AWS infrastructure.In this post, we demonstrate how to use CrewAI to create a multi-agent research workflow. This workflow creates two agents: one that researches on a topic on the internet, and a writer agent takes this research and acts like an editor by formatting it in a readable format. Additionally, we guide you through deploying and integrating one or multiple LLMs into structured workflows, using tools for automated actions, and deploying these workflows on SageMaker AI for a production-ready deployment.The following diagram illustrates the solution architecture.To follow along with the code examples in the rest of this post, make sure the following prerequisites are met:Integrated development environment – This includes the following: 
  (Optional) Access to Amazon SageMaker Studio and the JupyterLab IDE – We will use a Python runtime environment to build agentic workflows and deploy LLMs. Having access to a JupyterLab IDE with Python 3.9, 3.10, or 3.11 runtimes is recommended. You can also set up Amazon SageMaker Studio for single users. For more details, see Use quick setup for Amazon SageMaker AI. Create a new SageMaker JupyterLab Space for a quick JupyterLab notebook for experimentation. To learn more, refer to Boost productivity on Amazon SageMaker Studio: Introducing JupyterLab Spaces and generative AI tools. – You can also follow along in your local IDE (such as PyCharm or VSCode), provided that Python runtimes have been configured for site to AWS VPC connectivity (to deploy models on SageMaker AI).Permission to deploy models – Make sure that your user execution role has the necessary permissions to deploy models to a SageMaker real-time endpoint for inference. For more information, refer to Deploy models for inference. – The code used in this post is available in the following GitHub repo.Simplified LLM hosting on SageMaker AIBefore orchestrating agentic workflows with CrewAI powered by an LLM, the first step is to host and query an LLM using SageMaker real-time inference endpoints. There are two primary methods to host LLMs on SageMaker AI:Deploy from SageMaker JumpStartDeploy from Hugging Face HubDeploy DeepSeek from SageMaker JumpStartSageMaker JumpStart offers access to a diverse array of state-of-the-art FMs for a wide range of tasks, including content writing, code generation, question answering, copywriting, summarization, classification, information retrieval, and more. It simplifies the onboarding and maintenance of publicly available FMs, allowing you to access, customize, and seamlessly integrate them into your ML workflows. Additionally, SageMaker JumpStart provides solution templates that configure infrastructure for common use cases, along with executable example notebooks to streamline ML development with SageMaker AI.The following screenshot shows an example of available models on SageMaker JumpStart.To get started, complete the following steps:Run the following command in a Jupyter cell or the SageMaker Studio terminal:List all available LLMs under the Hugging Face or Meta JumpStart hub. The following code is an example of how to do this programmatically using the SageMaker Python SDK:from sagemaker.jumpstart.filters import (And, Or)
from sagemaker.jumpstart.notebook_utils import list_jumpstart_models

# generate a conditional filter to only select LLMs from HF or Meta
filter_value = Or(
    And("task == llm", "framework == huggingface"), 
    "framework == meta", "framework == deekseek"
)

# Retrieve all available JumpStart models
all_models = list_jumpstart_models(filter=filter_value)For example, deploying the  model directly from SageMaker JumpStart requires only a few lines of code:from sagemaker.jumpstart.model import JumpStartModel

model_id = " deepseek-llm-r1" 
model_version = "*"

# instantiate a new JS meta model
model = JumpStartModel(
    model_id=model_id, 
    model_version=model_version
)

# deploy model on a 1 x p5e instance 
predictor = model.deploy(
    accept_eula=True, 
    initial_instance_count=1, 
    # endpoint_name="deepseek-r1-endpoint" # optional endpoint name
)We recommend deploying your SageMaker endpoints within a VPC and a private subnet with no egress, making sure that the models remain accessible only within your VPC for enhanced security.Deploy DeepSeek from Hugging Face HubAlternatively, you can deploy your preferred model directly from the Hugging Face Hub or the Hugging Face Open LLM Leaderboard to a SageMaker endpoint. Hugging Face LLMs can be hosted on SageMaker using a variety of supported frameworks, such as NVIDIA Triton, vLLM, and Hugging Face TGI. For a comprehensive list of supported deep learning container images, refer to the available Amazon SageMaker Deep Learning Containers. In this post, we use a DeepSeek-R1-Distill-Llama-70B SageMaker endpoint using the TGI container for agentic AI inference. We deploy the model from Hugging Face Hub using Amazon’s optimized TGI container, which provides enhanced performance for LLMs. This container is specifically optimized for text generation tasks and automatically selects the most performant parameters for the given hardware configuration. To deploy from Hugging Face Hub, refer to the  or the following code snippet:import json
import sagemaker
import boto3
from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri
import os
from datetime import datetime

# Model configuration
hub = {'HF_MODEL_ID':'deepseek-ai/DeepSeek-R1-Distill-Llama-70B', #Llama-3.3-70B-Instruct
       'SM_NUM_GPUS': json.dumps(number_of_gpu),
       'HF_TOKEN': HUGGING_FACE_HUB_TOKEN,
       'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',  # Set to INFO level
       'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True'  # configure CUDA memory to use expandable memory segments
}
# Create and deploy model
huggingface_model =   HuggingFaceModel(image_uri=get_huggingface_llm_image_uri("huggingface", 
version="2.3.1"),
env=hub,
role=role,sagemaker_session=sagemaker_session)
predictor = huggingface_model.deploy(
               initial_instance_count=1,
               instance_type="ml.p4d.24xlarge"
               endpoint_name=custom_endpoint_name,
               container_startup_health_check_timeout=900)A new DeepSeek-R1-Distill-Llama-70Bendpoint should be  in under 10 minutes. If you want to change the model from DeepSeek to another model from the hub, simply replace the following parameter or refer to the DeepSeek deploy example in the following GitHub repo. To learn more about deployment parameters that can be reconfigured inside TGI containers at runtime, refer to the following GitHub repo on TGI arguments....
"HF_MODEL_ID": "deepseek-ai/...", # replace with any HF hub models
# "HF_TOKEN": "hf_..." # add your token id for gated models
...For open-weight models deployed directly from hubs, we strongly recommend placing your SageMaker endpoints within a VPC and a private subnet with no egress, making sure that the models remain accessible only within your VPC for a secure deployment.Build a simple agent with CrewAICrewAI offers the ability to create multi-agent and very complex agentic orchestrations using LLMs from several LLM providers, including SageMaker AI and Amazon Bedrock. In the following steps, we create a simple blocks counting agent to serve as an example.Create a blocks counting agentThe following code sets up a simple blocks counter workflow using CrewAI with two main components:Agent creation (blocks_counter_agent) – The agent is configured with a specific role, goal, and capabilities. This agent is equipped with a tool called .Task definition (count_task) – This is a task that we want this agent to execute. The task includes a template for counting how many of each color of blocks are present, where  will be replaced with actual color of the block. The task is assigned to .from crewai import Agent, Task
from pydantic import BaseModel, Field

# 1. Configure agent
blocks_counter_agent = Agent(
    role="Blocks Inventory Manager",
    goal="Maintain accurate block counts",
    tools=[BlocksCounterTool],
    verbose=True
)

# 2. Create counting task
count_task = Task(
    description="Count {color} play blocks in storage",
    expected_output="Exact inventory count for specified color",
    agent=blocks_counter_agent
)As you can see in the preceding code, each agent begins with two essential components: an agent definition that establishes the agent’s core characteristics (including its role, goal, backstory, available tools, LLM model endpoint, and so on), and a task definition that specifies what the agent needs to accomplish, including the detailed description of work, expected outputs, and the tools it can use during execution.This structured approach makes sure that agents have both a clear identity and purpose (through the agent definition) and a well-defined scope of work (through the task definition), enabling them to operate effectively within their designated responsibilities.Tools are special functions that give AI agents the ability to perform specific actions, like searching the internet or analyzing data. Think of them as apps on a smartphone—each tool serves a specific purpose and extends what the agent can do. In our example,  helps the agent count the number of blocks organized by color.Tools are essential because they let agents do real-world tasks instead of just thinking about them. Without tools, agents would be like smart speakers that can only talk—they could process information but couldn’t take actual actions. By adding tools, we transform agents from simple chat programs into practical assistants that can accomplish real tasks.Out-of-the-box tools with CrewAICrew AI offers a range of tools out of the box for you to use along with your agents and tasks. The following table lists some of the available tools.For reading various file formatsFor web content extractionFor searching YouTube channelsFor searching PDF documentsFor Python code interpretationBuild custom tools with CrewAIYou can build custom tools in CrewAI in two ways: by subclassing BaseTool or using the @tool decorator. Let’s look at the following BaseTool subclassing option to create the BlocksCounterTool we used earlier:from crewai.tools import BaseTool

class BlocksCounterTool(BaseTool):
    name = "blocks_counter" 
    description = "Simple tool to count play blocks"

    def _run(self, color: str) -> str:
        return f"There are 10 {color} play blocks available"Build a multi-agent workflow with CrewAI, DeepSeek-R1, and SageMaker AIMulti-agent AI systems represent a powerful approach to complex problem-solving, where specialized AI agents work together under coordinated supervision. By combining CrewAI’s workflow orchestration capabilities with SageMaker AI based LLMs, developers can create sophisticated systems where multiple agents collaborate efficiently toward a specific goal. The code used in this post is available in the following GitHub repo.Let’s build a research agent and writer agent that work together to create a PDF about a topic. We will use a DeepSeek-R1 Distilled Llama 3.3 70B model as a SageMaker endpoint for the LLM inference.Define your own DeepSeek SageMaker LLM (using LLM base class) The following code integrates SageMaker hosted LLMs with CrewAI by creating a custom inference tool that formats prompts with system instructions for factual responses, uses Boto3, an AWS core library, to call SageMaker endpoints, and processes responses by separating reasoning (before </think>) from final answers. This enables CrewAI agents to use deployed models while maintaining structured output patterns.# Calls SageMaker endpoint for DeepSeek inference
def deepseek_llama_inference(prompt: dict, endpoint_name: str, region: str = "us-east-2") -> dict:
    try:
        # ... Response parsing Code...

    except Exception as e:
        raise RuntimeError(f"Error while calling SageMaker endpoint: {e}")

# CrewAI-compatible LLM implementation for DeepSeek models on SageMaker.
class DeepSeekSageMakerLLM(LLM):
    def __init__(self, endpoint: str):
        # <... Initialize LLM with SageMaker endpoint ...>

    def call(self, prompt: Union[List[Dict[str, str]], str], **kwargs) -> str:
        # <... Format and return the final response ...>Name the DeepSeek-R1 Distilled endpoint Set the endpoint name as defined earlier when you deployed DeepSeek from the Hugging Face Hub:deepseek_endpoint = "deepseek-r1-dist-v3-llama70b-2025-01-22"Create a DeepSeek inference tool Just like how we created the BlocksCounterTool earlier, let’s create a tool that uses the DeepSeek endpoint for our agents to use. We use the same BaseTool subclass here, but we hide it in the CustomTool class implementation in sage_tools.py in the tools folder. For more information, refer to the GitHub repo.from crewai import Crew, Agent, Task, Process 

# Create the Tool for LLaMA inference
deepseek_tool = CustomTool(
    name="deepseek_llama_3.3_70B",
    func=lambda inputs: deepseek_llama_inference(
        prompt=inputs,
        endpoint_name=deepseek_endpoint
    ),
    description="A tool to generate text using the DeepSeek LLaMA model deployed on SageMaker."
)
 Just like the simple blocks agent we defined earlier, we follow the same template here to define the research agent. The difference here is that we give more capabilities to this agent. We attach a SageMaker AI based DeepSeek-R1 model as an endpoint for the LLM.This helps the research agent think critically about information processing by combining the scalable infrastructure of SageMaker with DeepSeek-R1’s advanced reasoning capabilities.The agent uses the SageMaker hosted LLM to analyze patterns in research data, evaluate source credibility, and synthesize insights from multiple inputs. By using the deepseek_tool, the agent can dynamically adjust its research strategy based on intermediate findings, validate hypotheses through iterative questioning, and maintain context awareness across complex information it gathers.# Research Agent

research_agent = Agent(
    role="Research Bot",
    goal="Scan sources, extract relevant information, and compile a research summary.",
    backstory="An AI agent skilled in finding relevant information from a variety of sources.",
    tools=[deepseek_tool],
    allow_delegation=True,
    llm=DeepSeekSageMakerLLM(endpoint=deepseek_endpoint),
    verbose=False
)
 The writer agent is configured as a specialized content editor that takes research data and transforms it into polished content. This agent works as part of a workflow where it takes research from a research agent and acts like an editor by formatting the content into a readable format. The agent is used for writing and formatting, and unlike the research agent, it doesn’t delegate tasks to other agents.writer_agent = Agent(
    role="Writer Bot",
    goal="Receive research summaries and transform them into structured content.",
    backstory="A talented writer bot capable of producing high-quality, structured content based on research.",
    tools=[deepseek_tool],
    allow_delegation=False,
    llm=DeepSeekSageMakerLLM(endpoint=deepseek_endpoint),
    verbose=False
)Define tasks for the agents Tasks in CrewAI define specific operations that agents need to perform. In this example, we have two tasks: a research task that processes queries and gathers information, and a writing task that transforms research data into polished content.Each task includes a clear description of what needs to be done, the expected output format, and specifies which agent will perform the work. This structured approach makes sure that agents have well-defined responsibilities and clear deliverables.Together, these tasks create a workflow where one agent researches a topic on the internet, and another agent takes this research and formats it into readable content. The tasks are integrated with the DeepSeek tool for advanced language processing capabilities, enabling a production-ready deployment on SageMaker AI.research_task = Task(
    description=(
        "Your task is to conduct research based on the following query: {prompt}.\n"
    ),
    expected_output="A comprehensive research summary based on the provided query.",
    agent=research_agent,
    tools=[deepseek_tool]
)

writing_task = Task(
    description=(
              "Your task is to create structured content based on the research provided.\n""),
    expected_output="A well-structured article based on the research summary.",
    agent=research_agent,
    tools=[deepseek_tool]
)
 A crew in CrewAI represents a collaborative group of agents working together to achieve a set of tasks. Each crew defines the strategy for task execution, agent collaboration, and the overall workflow. In this specific example, the sequential process makes sure tasks are executed one after the other, following a linear progression. There are other more complex orchestrations of agents working together, which we will discuss in future blog posts.This approach is ideal for projects requiring tasks to be completed in a specific order. The workflow creates two agents: a research agent and a writer agent. The research agent researches a topic on the internet, then the writer agent takes this research and acts like an editor by formatting it into a readable format.Let’s call the crew scribble_bots:# Define the Crew for Sequential Workflow # 

scribble_bots = Crew( agents=[research_agent, writer_agent], 
       tasks=[research_task, writing_task], 
       process=Process.sequential # Ensure tasks execute in sequence)
Use the crew to run a task We have our endpoint deployed, agents created, and crew defined. Now we’re ready to use the crew to get some work done. Let’s use the following prompt:result = scribble_bots.kickoff(inputs={"prompt": "What is DeepSeek?"})Our result is as follows:**DeepSeek: Pioneering AI Solutions for a Smarter Tomorrow**

In the rapidly evolving landscape of artificial intelligence, 
DeepSeek stands out as a beacon of innovation and practical application. 
As an AI company, DeepSeek is dedicated to advancing the field through cutting-edge research and real-world applications, 
making AI accessible and beneficial across various industries.

**Focus on AI Research and Development**

………………….. ………………….. ………………….. …………………..
Complete the following steps to clean up your resources:Delete your GPU DeekSeek-R1 endpoint:import boto3

# Create a low-level SageMaker service client.
sagemaker_client = boto3.client('sagemaker', region_name=<region>)

# Delete endpoint
sagemaker_client.delete_endpoint(EndpointName=endpoint_name)
If you’re using a SageMaker Studio JupyterLab notebook, shut down the JupyterLab notebook instance.In this post, we demonstrated how you can deploy an LLM such as DeepSeek-R1—or another FM of your choice—from popular model hubs like SageMaker JumpStart or Hugging Face Hub to SageMaker AI for real-time inference. We explored inference frameworks like Hugging Face TGI which helps streamline deployment while integrating built-in performance optimizations to minimize latency and maximize throughput. Additionally, we showcased how the SageMaker developer-friendly Python SDK simplifies endpoint orchestration, allowing seamless experimentation and scaling of LLM-powered applications.Beyond deployment, this post provided an in-depth exploration of agentic AI, guiding you through its conceptual foundations, practical design principles using CrewAI, and the seamless integration of state-of-the-art LLMs like DeepSeek-R1 as the intelligent backbone of an autonomous agentic workflow. We outlined a sequential CrewAI workflow design, illustrating how to equip LLM-powered agents with specialized tools that enable autonomous data retrieval, real-time processing, and interaction with complex external systems.Now, it’s your turn to experiment! Dive into our publicly available code on GitHub, and start building your own DeepSeek-R1-powered agentic AI system on SageMaker. Unlock the next frontier of AI-driven automation—seamlessly scalable, intelligent, and production-ready.Special thanks to Giuseppe Zappia, Poli Rao, and Siamak Nariman for their support with this blog post. is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging state-of-the-art foundation models. He has extensive experience working with advanced language models including DeepSeek-R1, the LLama family, and Qwen, focusing on their fine-tuning and optimization for specific scientific applications. His expertise extends to implementing efficient training pipelines and deployment strategies using AWS SageMaker, enabling the scaling of foundation models from development to production. He collaborates with customers to design and implement generative AI solutions, helping them navigate model selection, fine-tuning approaches, and deployment strategies to achieve optimal performance for their specific use cases. is a Machine Learning Specialist at Amazon Web Services. He’s been in technology for over a decade, spanning various technologies and multiple roles. He is currently focused on combining his background in software engineering, DevOps, and machine learning to help customers deliver machine learning workflows at scale. In his spare time, he enjoys reading, research, hiking, biking, and trail running. is a Generative AI Specialist for third-party models at AWS, where he works with top-tier third-party foundation model (FM) providers to develop and execute joint Go-To-Market strategies, enabling customers to effectively train, deploy, and scale FMs to solve industry specific challenges. Karan holds a Bachelor of Science in Electrical and Instrumentation Engineering from Manipal University, a master’s in science in Electrical Engineering from Northwestern University and is currently an MBA Candidate at the Haas School of Business at University of California, Berkeley. is an AI/ML Specialist Solutions Architect at AWS. He focuses on helping customers build, train, deploy and migrate machine learning (ML) workloads to SageMaker. He previously worked in the semiconductor industry developing large computer vision (CV) and natural language processing (NLP) models to improve semiconductor processes using state of the art ML techniques. In his free time, he enjoys playing chess and traveling. You can find Pranav on LinkedIn.]]></content:encoded></item><item><title>How Transformer LLMs Work [Free Course]</title><link>https://newsletter.languagemodels.co/p/how-transformer-llms-work-free-course</link><author>Jay Alammar</author><category>dev</category><category>ai</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8995a108-11cc-4edf-b69d-73006c087981_3050x1922.png" length="" type=""/><pubDate>Mon, 10 Feb 2025 19:00:20 +0000</pubDate><source url="https://newsletter.languagemodels.co/">LLM.newsletter</source><content:encoded><![CDATA[Enroll for free now: https://bit.ly/4aRnn7Z Github Repo: https://github.com/HandsOnLLM/Hands-On-Large-Language-Models We're ecstatic to bring you "How Transformer LLMs Work" -- a free course with ~90 minutes of video, code, and crisp visuals and animations that explain the modern Transformer architecture, tokenizers, embeddings, and mixture-of-expert models. and I have developed a lot of the visual language over the last several years (tens of thousands of iterations for hundreds of figures) for the book. This was informed by many incredible colleagues at Cohere, C4AI, and the open source and open science ML community. But to have an opportunity to collaborate with the legendary Andrew Ng and the team at DeepLearning.ai we took them to the next level with animations and a concise narrative meant to enable technical learners to pick up an ML paper and understand the architecture description. In this course, you'll learn how a transformer network architecture that powers LLMs works. You'll build the intuition of how LLMs process text and work with code examples that illustrate the key components of the transformer architecture. Key topics covered in this course include: The evolution of how language has been represented numerically, from the Bag-of-Words model through Word2Vec embeddings to the transformer architecture that captures word meanings in full context. How LLM inputs are broken down into tokens, which represent words or pieces before they are sent to the language model. The details of a transformer and the three main stages, consisting of tokenization and embedding, the stack of transformer blocks, and the language model head. The details of the transformer block, including attention, which calculates relevance scores followed by the feedforward layer, which incorporates stored information learned in training. How cached calculations make transformers faster, how the transformer block has evolved over the years since the original paper was released, and how they continue to be widely used. Explore an implementation of recent models in the Hugging Face transformer library. By the end of this course, you’ll have a deep understanding of how LLMs process language and you'll be able to read through papers describing models and understand the details that are used to describe these architectures. This intuition will help improve your approach to building LLM applications.]]></content:encoded></item><item><title>Celebrating Steve Jobs’s Impact on Consumer Tech and Design</title><link>https://spectrum.ieee.org/steve-jobs</link><author>San Murugesan</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjM3NzgxOC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1NjIzMTg4N30.TjwFgXs9V-Zy7jC58zYwdVCPFYKyFxurZnzQNs9ICQM/image.jpg?width=600" length="" type=""/><pubDate>Mon, 10 Feb 2025 19:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[A look back at his career on what would have been his 70th birthday]]></content:encoded></item><item><title>Automate bulk image editing with Crop.photo and Amazon Rekognition</title><link>https://aws.amazon.com/blogs/machine-learning/automate-bulk-image-editing-with-crop-photo-and-amazon-rekognition/</link><author>Rahul Bhargava</author><category>dev</category><category>ai</category><pubDate>Mon, 10 Feb 2025 18:50:29 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Evolphin Software, Inc. is a leading provider of digital and media asset management solutions based in Silicon Valley, California. Crop.photo from Evolphin Software is a cloud-based service that offers powerful bulk processing tools for automating image cropping, content resizing, background removal, and listing image analysis.Crop.photo is tailored for high-end retailers, ecommerce platforms, and sports organizations. The solution has created a unique offering for bulk image editing through its advanced AI-driven solutions. In this post, we explore how Crop.photo uses Amazon Rekognition to provide sophisticated image analysis, enabling automated and precise editing of large volumes of images. This integration streamlines the image editing process for clients, providing speed and accuracy, which is crucial in the fast-paced environments of ecommerce and sports.Automation: The way out of bulk image editing challengesBulk image editing isn’t just about handling a high volume of images, it’s about delivering flawless results with speed at scale. Large retail brands, marketplaces, and sports industries process thousands of images weekly. Each image must be catalog-ready or broadcast-worthy in minutes, not hours.The challenge lies not just in the quantity but in maintaining high-quality images and brand integrity. Speed and accuracy are non-negotiable. Retailers and sports organizations expect rapid turnaround without compromising image integrity.This is where Crop.photo’s smart automations come in with an innovative solution for high-volume image processing needs. The platform’s advanced AI algorithms can automatically detect subjects of interest, crop the images, and optimize thousands of images simultaneously while providing consistent quality and brand compliance. By automating repetitive editing tasks, Crop.photo enables enterprises to reduce image processing time from hours to minutes, allowing creative teams to focus on higher-value activities.Challenges in the ecommerce industryThe ecommerce industry often encounters the following challenges:Inefficiencies and delays in manual image editing – Ecommerce companies rely on manual editing for tasks like resizing, alignment, and background removal. This process can be time-consuming and prone to delays and inconsistencies. A more efficient solution is needed to streamline the editing process, especially during platform migrations or large updates.Maintaining uniformity across diverse image types – Companies work with a variety of image types, from lifestyle shots to product close-ups, across different categories. Maintaining uniformity and professionalism in all image types is essential to meet the diverse needs of marketing, product cataloging, and overall brand presentation.Large-scale migration and platform transition – Transitioning to a new ecommerce platform involves migrating thousands of images, which presents significant logistical challenges. Providing consistency and quality across a diverse range of images during such a large-scale migration is crucial for maintaining brand standards and a seamless user experience.For a US top retailer, wholesale distribution channels posed a unique challenge. Thousands of fashion images need to be made for the marketplace with less than a day’s notice for flash sales. Their director of creative operations said,“Crop.photo is an essential part of our ecommerce fashion marketplace workflow. With over 3,000 on-model product images to bulk crop each month, we rely on Crop.photo to enable our wholesale team to quickly publish new products on popular online marketplaces such as Macy’s, Nordstrom, and Bloomingdales. By increasing our retouching team’s productivity by over 70%, Crop.photo has been a game changer for us. Bulk crop images used to take days can now be done in a matter of seconds!”Challenges in the sports industryThe sports industry often contends with the following challenges:Bulk player headshot volume and consistency – Sports organizations face the challenge of bulk cropping and resizing hundreds of player headshots for numerous teams, frequently on short notice. Maintaining consistency and quality across a large volume of images can be difficult without AI.Diverse player facial features – Players have varying facial features, such as different hair lengths, forehead sizes, and face dimensions. Adapting cropping processes to accommodate these differences traditionally requires manual adjustments for each image, which leads to inconsistencies and significant time investment.Editorial time constraints – Tight editorial schedules and resource limitations are common in sports organizations. The time-consuming nature of manual cropping tasks strains editorial teams, particularly during high-volume periods like tournaments, where delays and rushed work can impact quality and timing.An Imaging Manager at Europe’s Premier Football Organization expressed,“We recently found ourselves with 40 images from a top flight English premier league club needing to be edited just 2 hours before kick-off. Using the Bulk AI headshot cropping for sports feature from Crop.photo, we had perfectly cropped headshots of the squad in just 5 minutes, making them ready for publishing in our website CMS just in time. We would never have met this deadline using manual processes. This level of speed was unthinkable before, and it’s why we’re actively recommending Crop.photo to other sports leagues.”Crop.photo uses Amazon Rekognition to power a robust solution for bulk image editing. Amazon Rekognition offers features like object and scene detection, facial analysis, and image labeling, which they use to generate markers that drive a fully automated image editing workflow.The following diagram presents a high-level architectural data flow highlighting several of the AWS services used in building the solution.The solution consists of the following key components: – Amazon Cognito is used for user authentication and user management.Infrastructure deployment – Frontend and backend servers are used on Amazon Elastic Container Service (Amazon ECS) for container deployment, orchestration, and scaling.Content delivery and caching – Amazon CloudFront is used to cache content, improving performance and routing traffic efficiently. – Information about uploaded files and job execution is stored in Amazon Aurora. – AWS Batch processes thousands of images in bulk. – Amazon Simple Queue Service (Amazon SQS) manages and queues jobs for processing, making sure they’re run in the correct order by AWS Batch. – Amazon Rekognition services analyze media files, including: 
  Face Analysis to generate headless crops.Moderation to detect and flag profanity and explicit content.Label Detection to provide context for image processing and focus on relevant objects.Custom Labels to identify and verify brand logos and adhere to brand guidelines.Amazon Rekognition is an AWS computer vision service that powers Crop.photo’s automated image analysis. It enables object detection, facial recognition, and content moderation capabilities:Face detection – The Amazon Rekognition face detection feature automatically identifies and analyzes faces in product images. You can use this feature for face-based cropping and optimization through adjustable bounding boxes in the interface.Image color analysis – The color analysis feature examines image composition, identifying dominant colors and balance. This integrates with Crop.photo’s brand guidelines checker to provide consistency across product images.Object detection – Object detection automatically identifies key elements in images, enabling smart cropping suggestions. The interface highlights detected objects, allowing you to prioritize specific elements during cropping.Custom label detection – Custom label detection recognizes brand-specific items and assets. Companies can train models for their unique needs, automatically applying brand-specific cropping rules to maintain consistency.Text detection (OCR) – The OCR capabilities of Amazon Recognition detect and preserve text within images during editing. The system highlights text areas to make sure critical product information remains legible after cropping.Within the Crop.photo interface, users can upload videos through the standard interface, and the speech-to-text functionality will automatically transcribe any audio content. This transcribed text can then be used to enrich the metadata and descriptions associated with the product images or videos, improving searchability and accessibility for customers. Additionally, the brand guidelines check feature can be applied to the transcribed text, making sure that the written content aligns with the company’s branding and communication style.The Crop.photo service follows a transparent pricing model that combines unlimited automations with a flexible image credit system. Users have unrestricted access to create and run as many automation workflows as needed, without any additional charges. The service includes a range of features at no extra cost, such as basic image operations, storage, and behind-the-scenes processing.For advanced AI-powered image processing tasks, like smart cropping or background removal, users consume image credits. The number of credits required for each operation is clearly specified, allowing users to understand the costs upfront. Crop.photo offers several subscription plans with varying image credit allowances, enabling users to choose the plan that best fits their needs.Results: Improved speed and precisionThe automated image editing capabilities of Crop.photo with the integration of Amazon Rekognition has increased speed in editing, with 70% faster image retouching for ecommerce. With a 75% reduction in manual work, the turnaround time for new product images is reduced from 2–3 days to just 1 hour. Similarly, the bulk image editing process has been streamlined, allowing over 100,000 image collections to be processed per day using AWS Fargate. Advanced AI-powered image analysis and editing features provide consistent, high-quality images at scale, eliminating the need for manual review and approval of thousands of product images.For instance, in the ecommerce industry, this integration facilitates automatic product detection and precise cropping, making sure every image meets specific marketplace and brand standards. In sports, it enables quick identification and cropping of player facial features, including head, eyes, and mouth, adapting to varying backgrounds and maintaining brand consistency.The following images are before and after pictures for an ecommerce use case.For a famous wine retailer in the United Kingdom, the integration of Amazon Rekognition with Crop.photo streamlined the processing of over 1,700 product images, achieving a 95% reduction in bulk image editing time, a confirmation to the efficiency of AI-powered enhancement.Similarly, a top 10 global specialty retailer experienced a transformative impact on their ecommerce fashion marketplace workflow. By automating the cropping of over 3,000 on-model product images monthly, they boosted their retouching team’s productivity by over 70%, maintaining compliance with the varied image standards of multiple online marketplaces.These case studies illustrate the tangible benefits of integrating Crop.photo with Amazon Rekognition, demonstrating how automation and AI can revolutionize the bulk image editing landscape for ecommerce and sports industries.Crop.photo, from AWS Partner Evolphin Software, offers powerful bulk processing tools for automating image cropping, content resizing, and listing image analysis, using advanced AI-driven solutions. Crop.photo is tailored for high-end retailers, ecommerce platforms, and sports organizations. Its integration with Amazon Rekognition aims to streamline the image editing process for clients, providing speed and accuracy in the high-stakes environment of ecommerce and sports. Crop.photo plans additional AI capabilities with Amazon Bedrock generative AI frameworks to adapt to emerging digital imaging trends, so it remains an indispensable tool for its clients.To learn more about Evolphin Software and Crop.photo, visit their website. founder & CTO of Evolphin Software and Crop.photo, is reshaping how brands produce and manage visual content at scale. Through Crop.photo’s AI-powered tools, global names like Lacoste and Urban Outfitters, as well as ambitious Shopify retailers, are rethinking their creative production workflows. By leveraging cutting-edge Generative AI, he’s enabling brands of all sizes to scale their content creation efficiently while maintaining brand consistency. is a Solutions Architect specializing in Cloud Security at AWS based in the San Francisco Bay Area. As a trusted technical advisor, Vaishnavi helps customers to design secure, scalable and innovative cloud solutions that drive both business value and technical excellence. Outside of work, Vaishnavi enjoys traveling and exploring different artisan coffee roasters. is an Account Manager at AWS, who provides guidance to Evolphin Software and other organizations to help accelerate business outcomes leveraging AWS Technologies. John has a degree in Business Administration and Management with a concentration in Finance from Gonzaga University, and enjoys snowboarding in the Sierras in his free time.]]></content:encoded></item><item><title>Revolutionizing business processes with Amazon Bedrock and Appian’s generative AI skills</title><link>https://aws.amazon.com/blogs/machine-learning/revolutionizing-business-processes-with-amazon-bedrock-and-appians-generative-ai-skills/</link><author>Sunil Bemarkar</author><category>dev</category><category>ai</category><pubDate>Mon, 10 Feb 2025 18:37:01 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This blog post is co-written with Louis Prensky and Philip Kang from Appian. The digital transformation wave has compelled enterprises to seek innovative solutions to streamline operations, enhance efficiency, and maintain a competitive edge. Recognizing the growing complexity of business processes and the increasing demand for automation, the integration of generative AI skills into environments has become essential. This strategic move addresses key challenges such as managing vast amounts of unstructured data, adhering to regulatory compliance, and automating repetitive tasks to boost productivity. Using robust infrastructure and advanced language models, these AI-driven tools enhance decision-making by providing valuable insights, improving operational efficiency by automating routine tasks, and helping with data privacy through built-in detection and management of sensitive information. For enterprises, this means achieving higher levels of operational excellence, significant cost savings, and scalable solutions that adapt to business growth. For customers, it translates to improved service quality, enhanced data protection, and a more dynamic, responsive service, ultimately driving better experiences and satisfaction.Appian has led the charge by offering generative AI skills powered by a collaboration with Amazon Bedrock and Anthropic’s Claude large language models (LLMs). This partnership allows organizations to:Enhance decision making with valuable insightsImprove operational efficiency by automating tasksHelp protect data privacy through built-in detection and management of sensitive informationMaintain compliance with HIPAA and FedRAMP compliant AI skillsCritically, by placing AI in the context of a wider environment, organizations can operationalize AI in processes that seamlessly integrate with existing software, pass work between digital workers and humans, and help achieve strong security and compliance.Appian, an AWS Partner with competencies in financial services, healthcare, and life sciences, is a leading provider of low-code automation software to streamline and optimize complex business processes for enterprises. The Appian AI Process Platform includes everything you need to design, automate, and optimize even the most complex processes, from start to finish. The world’s most innovative organizations trust Appian to improve their workflows, unify data, and optimize operations—resulting in accelerated growth and superior customer experiences.Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.Appian uses the robust infrastructure of Amazon Bedrock and Anthropic’s Claude LLMs to offer fully integrated, pre-built generative AI skills that help developers enhance and automate business processes using low-code development. These use case-driven tools automate common tasks in business processes, making AI-powered applications faster and easier to develop.This blog post will cover how Appian AI skills build automation into organizations’ mission-critical processes to improve operational excellence, reduce costs, and build scalable solutions. Additionally, we’ll cover real-world examples of processes such as:A mortgage lender that used AI-driven data extraction to reduce mortgage processing times from 16 weeks to 10 weeks.A financial services company that achieved a four-fold reduction in data extraction time from trade-related emails.A legal institution that used AI to reduce attorney time spent on contract review, enabling them to focus on other, high-value work.Current challenges faced by enterprisesModern enterprises face numerous challenges, including:Managing vast amounts of unstructured data: Enterprises deal with immense volumes of data generated from various sources such as emails, documents, and customer interactions. Organizing, analyzing, and extracting valuable insights from unstructured data can be overwhelming without advanced AI capabilities.Help protect data privacy and compliance: With increasing regulatory requirements around data privacy and protection, organizations must safeguard sensitive information, such as personally identifiable information (PII). Manual processes for data redaction and compliance checks are often error-prone and resource-intensive.Streamlining repetitive and time-consuming tasks: Routine tasks such as data entry, document processing, and content classification consume significant time and effort. Automating these tasks can lead to substantial productivity gains and allow employees to focus on more strategic activities.Adapting to rapidly changing market conditions: In a fast-paced business environment, organizations need to be agile and responsive. This requires real-time data analysis and decision-making capabilities that traditional systems might not provide. AI helps businesses quickly adapt to industry changes and customer demands.Enhancing decision-making with accurate data insights: Making informed decisions requires access to accurate and timely data. However, extracting meaningful insights from large datasets can be challenging without advanced analytical tools. AI-powered solutions can process and analyze data at scale, providing valuable insights that drive better decision-making.Appian AI service architectureThe architecture of the generative AI skills integrates both the Amazon Bedrock and Amazon Textract scalable infrastructure with Appian’s process management capabilities. This generative AI architecture is designed with private AI as the foundation and upholds those principles.The key components of this architecture include:Appian AI Process Platform instances: The frontend serves as the primary application environment where users interact with the system application to upload documents, initiate workflows, and view processed results.: This service functions as an intermediary layer between the Appian instances and AWS AI services (Amazon Textract and Amazon Bedrock). This layer encapsulates the logic required to interact with the AWS AI services to manage API calls, data formatting, and error handling.: This AWS service is used to automate the extraction of text and structured data from scanned documents and images and provide the extracted data in a structured format.: This AWS service provides advanced AI capabilities using FMs for tasks such as text summarization, sentiment analysis, and natural language understanding. This helps enhance the extracted data with deeper insights and contextual understanding.Appian generative AI skills, powered by Amazon Bedrock with Anthropic’s Claude family of LLMs, are designed to jump-start the use of generative AI in your processes. The following figure showcases the diverse capabilities of Appian’s generative AI skills, demonstrating how they enable enterprises to seamlessly automate complex tasks.Each new skill provides a pre-populated prompt template tailored to specific tasks, alleviating the need to start from scratch. Businesses can select the desired action and customize the prompt for a perfect fit, enabling the automation of tasks such as:Content analysis and processing: With Appian’s generative AI skills, businesses can automatically generate, summarize, and classify content across various formats. This capability is particularly useful for managing large volumes of customer feedback, generating reports, and creating content summaries, significantly reducing the time and effort required for manual content processing.: Organizations generate mountains of data and documents. Extracting this information manually can be both burdensome and error-prone. Appian’s AI skills can perform highly accurate text extraction from PDF files and scanned images and pull relevant data from both structured and unstructured data sources such as invoices, forms, and emails. This speeds up data processing and promotes higher accuracy and consistency.PII extraction and redaction: Identifying and managing PII within large datasets is crucial for data governance and compliance. Appian’s AI skills can automatically identify and extract sensitive information from documents and communication channels. Additionally, Appian supports plugins that can redact this content for further privacy. This assists your compliance efforts without extensive manual intervention.: Appian’s AI skills can summarize documents to give users an overview before digging into the details. Whether it’s summarizing research papers, legal documents, or internal reports, AI can generate concise summaries, saving time and making sure that critical information is highlighted for quick review.The following figure shows an example of a prompt-builder skill used to extract unstructured data from a bond certificate.Each AI skill offers pre-populated prompt templates, allowing you to deploy AI without starting from scratch. Each template caters to specific business needs, making implementation straightforward and efficient. Plus, users can customize these prompts to fit their unique requirements and operational needs.In this solution, Appian Cloud seamlessly integrates and customizes Amazon Bedrock and Claude LLMs behind the scenes, abstracting complexity to deliver enterprise-grade AI capabilities tailored to its cloud environment. It provides pre-built, use case specific prompt templates for tasks like text summarization and data extractions, dynamically customized based on user inputs and business context. Using the scalability of the Amazon Bedrock infrastructure, Appian Cloud provides optimal performance and efficient handling of enterprise-scale workflows, all within a fully managed cloud service.By addressing these complexities, Appian Cloud empowers businesses to focus solely on using AI to achieve operational excellence and business outcomes without the burdens of technical setup, integration challenges, or ongoing maintenance efforts.Appian’s AI skills have proven effective across multiple industries. Here are a few real-world examples:: This organization automated the extraction of over 60 data fields from inconsistent document formats, reducing the process timeline from 16 weeks to 10 weeks and achieving 98.33% accuracy. The implementation of Appian’s generative AI skills allowed the mortgage processor to streamline their workflow, significantly cutting down on processing time and improving data accuracy, which led to faster loan approvals and increased customer satisfaction.: A financial service company received over 1,000 loosely structured emails about trades. Manually annotating these emails led to significant human errors. With an Appian generative AI skill, the customer revamped the entity tagging process by automatically extracting approximately 40 data fields from unstructured emails. This resulted in a four-fold reduction in extraction time and achieved over 95% accuracy, improving the user experience compared to traditional ML extraction tools. The automated process not only reduced errors but also enhanced the speed and reliability of data extraction, leading to more accurate and timely trading decisions.: A legal institution had to review negotiated contracts against the original contracts to determine whether the outlined risks had been resolved. This manual process was error prone and labor intensive. By deploying a generative AI skill, they automated the extraction of changes between contracts to find the differences and whether risks had been resolved. This streamlined the attorney review process and provided insights and reasoning into the differences found. The automated solution significantly reduced the time attorneys spent on contract review, allowing them to focus on more strategic tasks and improving the overall efficiency of the legal department.AWS and Appian’s collaboration marks a significant advancement in business process automation. By using the power of Amazon Bedrock and Anthropic’s Claude models, Appian empowers enterprises to optimize and automate processes for greater efficiency and effectiveness. This partnership sets a new standard for AI-driven business solutions, leading to greater growth and enhanced customer experiences. The ability to quickly deploy and customize AI skills allows businesses to stay agile and responsive in a dynamic environment.Appian solutions are available as software as a service (SaaS) offerings in AWS Marketplace. Check out the Appian website to learn more about how to use the AI skills.is a Senior Partner Solutions Architect at Amazon Web Services. He works with various Independent Software Vendors (ISVs) and Strategic customers across industries to accelerate their digital transformation journey and cloud adoption.is a Principal Customer Solution Manager within the AWS Independent Software Vendor (ISV) team. In this role, he programmatically helps ISV customers adopt AWS technologies and services to reach their business goals more quickly.is a Senior Product Manager at Appian. He is responsible for driving product strategy and feature design for AI Skills within Appian’s Cognitive Automation Group.is a Principal Solutions Consultant in Partner Technology & Innovation centers with Appian. In this role, he spearheads technical innovation with a focus on AI/ML and cloud solutions.]]></content:encoded></item><item><title>Why the So-Called AI Action Summit Falls Short</title><link>https://www.eff.org/deeplinks/2025/02/why-so-called-ai-action-summit-falls-short</link><author>Svea Windwehr</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/artificial-intelligence-resized_0.png" length="" type=""/><pubDate>Mon, 10 Feb 2025 18:13:51 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CNL: Optimizing Kyverno policy enforcement performance for large clusters</title><link>https://www.youtube.com/watch?v=DWmCAUCs3bc</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/DWmCAUCs3bc?version=3" length="" type=""/><pubDate>Mon, 10 Feb 2025 18:13:08 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Gyroscope-on-a-Chip Targets GPS’s Dominance</title><link>https://spectrum.ieee.org/optical-gyroscopes-on-chip</link><author>Willie D. Jones</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjEyMjY0MS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4MzM2MzQ3OX0.wBTG52F6192KrziO07XCyLR6oSBISZrkoUBwy5BH0n4/image.jpg?width=600" length="" type=""/><pubDate>Mon, 10 Feb 2025 18:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Centimeter-scale wayfinding accuracy emerges from millimeter-scale tech]]></content:encoded></item><item><title>Show HN: I made a tiny book using a pen-plotter and AI</title><link>https://muffinman.io/blog/the-tiny-book-of-great-joys/</link><author>stankot</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 17:52:31 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[If you are interested in how I over-engineered the process of making a tiny book for my wife, using AI, a pen plotter, a 3D printer, and a lot of time, you are in the right place. The book is titled  , and here is how it turned out:My wife is delighted with it, so it was worth all the effort.This post will take you through the process. It will be a long one, but please stick around - I promise there will be a lot of pretty pictures.Here is the outline of the post:I had this idea for a while after seeing something similar somewhere on the internet.. Since then, I always wanted to make one for my wife - a physically small book with a bunch of small drawings of our memories together, inside jokes, and little things she likes.I wanted the illustrations to be hand-drawn, and I had a plan to ask my friend to do them. But I knew he would refuse any kind of payment, so I felt bad adding more work to his plate. So I shelved the idea, but every now and then, it would pop up in my head.Fast forward a few years - we got a kid, and our routine completely changed. We are enjoying it a lot, but it can be very exhausting, and every day seems identical to the last. That's why I decided I needed to do something for her to break the routine. The book idea seemed perfect - personal and handcrafted - so I gave it a try.To be able to do everything myself, I went to create digital drawings and then draw them on paper using my trusty pen plotter.With the idea in place, I moved on to creating the drawings - which turned out to be a challenge of its own.For pen plotting, one needs vector files, so I started drawing in Figma. Unfortunately, I quickly realized that my drawing skills would not get me the result I had envisioned. Determined to do it this time, I decided to try using AI to generate images.I got myself a Midjourney subscription and started playing with it. It took a lot of failed attempts to figure out how to get drawings that were simple and had a strong hand-drawn feel to them. Even then, I ended up editing every one of them, but more on that later.One of the first images I was satisfied with (it didn't end up in the book, though):It took a lot of time, but it was fun. Failed attempts were often quirky and funny, and I was learning how to use the tool. And it made me feel like a secret agent, doing it next to my wife, who had no idea what I was up to.I may be wrong, but I think Midjourney wasn't built for the kind of illustrations I had in mind. I was after simple, hand-drawn illustrations that felt personal. Luckily, I found a style reference () that worked well for my case. I used it to generate almost all of the drawings that ended up in the book. For those who haven't used Midjourney - you can use images as style references to influence the style of images you want to generate.Most of my images were generated using that  code and a  between 150 and 400 (it can go from 0 to 1000).As for the prompts, these are the key terms I combined with the description and the style reference:isolated on white backgroundIt took me a lot of tries - between 10 and 30 attempts for each image you see in the book.Once I solved the image generation part, I had to figure out how to turn them into vector files for plotting. The first thing I tried was something similar to halftone. As you can see below, in this process, the images completely lost the hand-drawn feel.Then I remembered this plot of Marble Machine X I did a while ago, for which I used AutoTrace to convert the original image to a vector file. The great thing about AutoTrace is that it supports "centerline tracing". And this time, I learned that Inkscape has a great AutoTrace plugin, which made it even easier to convert.What makes centerline tracing differentMost of the tools that convert raster to vector images do it by outlining shapes. This is not suitable for plotting, as each line in the original image becomes a sausage-like shape. Centerline tracing, on the other hand, tries to draw a single line following the middle path through shapes. Don't worry if it sounds confusing; the example below should make things clearer.Here is the image of Link from  generated by Midjourney:After applying a common vectorization technique, we get this. As you can see, each line in the original drawing is now outlined, creating this messy-looking image.But if we use centerline tracing, it suddenly looks a lot more like a drawing. It is not perfect, but don't worry - we are going to clean it up in the next step.In the points where lines touch or cross, AutoTrace is not sure which line to follow and creates these funky-looking joints. Here is an exaggerated example to show you what I'm talking about. Input is the raster image at the top and the vectorized result is at the bottom:But I found out that if I roughly separate these lines, I get a much better result.Let's now apply this technique to the image of Link we've seen above. After separating lines (and some cleaning up) this is the image I ended up with. It is rough, but it is only used as an input for the tracing process, so it doesn't really matter. This was manual and somewhat tedious process, but I enjoyed it overall. It was a sort of meditation for me.And finally, when we trace this image, we get a really nice and clean vector file perfect for plotting. Here is another example. We start with the image I generated using Midjourney:After editing, removing details and separating lines, we get this one:And the traced vector result:You'll notice that in both examples I did  . I did that for pretty much all of the images, to fix things I wasn't able to polish using prompts. I also removed a lot of details to make sure images are crisp and readable at the small size.All of this took a lot of experimentation, but it gave me a pretty solid workflow which I used to generate all of the images. The complete flow looks like this:Generate images using Midjourney.Upscale them two times, because upscaled images were easier to edit and tracing was more precise.Clean up, redraw and separate lines by hand using Gimp.Use Inkscape plugin to run AutoTrace centerline tracing.It took me a while to generate all the images, and the fact that I was trying to keep it a secret from my wife didn't help. I think I did it over the span of two weeks, mostly in the evening after she would go to bed. never stood a chance!Before we continue I just want to show you two funky images of Link that really made me laugh:With the drawings ready, I turned to the next crucial part - the text. I first wanted to write everything by hand, photograph it and then vectorize it in the same way I did with the images. But it was a hassle - I had to do a lot of editing for text to look as my handwriting.Evil Mad Scientist, the maker of my pen plotter, has a fantastic tool called Hershey Text. It contains a bunch of single-line fonts ideal for plotting. I chose the EMS Elfin font as it looked playful and hand-drawn. I used it to write all of the text in the book and I think it turned out great.The tricky part with bookbinding is that pages are not printed in order, but in a way that when you fold the sheets in half, you get the right order. I used Figma to design the layout, with a great care to make sure pages are in order after double-sided plotting.Here is the layout laid out on A4 sized paper. Sorry for blurring the text, but a lot of it is very personal and I want to keep it for our eyes only.Plotting is the part that went the smoothest, but not without hiccups. I usually use Pigma Micron blackliner markers. They use archival quality ink and they are literally indestructible. But this time, even the thinnest one I had was too thick for the book this small.Here you can see the first  using markers of 0.2mm and 0.1mm thickness respectively. Lines got a bit smudged and looked much thicker than I expected. This was also the moment I realized I need to remove  from the images to make them readable at this size.I needed to find a thinner pen.Technical pen to the rescueBlackliner markers were made as a more practical replacement for technical pens. But from what I've read, an old-school technical pen was the only thing capable of achieving super-fine lines I wanted. I went online and ordered Rotring Isograph 0.2mm. As soon as it arrived I sneaked out to my study and did another test plot using it. Oh boy, was I happy when I saw the result:Lines were thin and crisp and at this point I was convinced the project will be a success!All of the first plots were done on 120gsm printer paper. It is somewhat thick paper and drawings looked fantastic. Unfortunately, when I bound the pages together, the drawings and letters would get transferred on the opposite pages. I could probably get away with it, considering the whole hand made feel of the book. But I wanted it to be perfect.A friend advised me to leave ink to dry for a few hours. I left each side to dry for 24 hours, but it smudged again. Next time I tried putting the plot (before cutting the pages) between two sheets of papers and pressing it with heavy books. I did that for more than 24 hours, but still after cutting and bounding the pages, they got smudged again. At this point I was becoming somewhat desperate. As the last resort I ordered different, 100gsm paper and to my relief it worked! Crisis averted!In the final version you can still see tiny traces on a few pages, but these are barely visible and don't really bother me.After plotting and cutting I was left with a stack of somewhat delicate pages. Now, it was finally time to turn them into a book.As you can imagine, I had zero bookbinding experience. There are a lot of resources online, but two of them were crucial for my project as they were on how to bind tiny books:After reading and watching these and a few generic articles on bookbinding, I gathered enough info to try doing it myself. I thought I was super clever because I 3D printed sides and spine of the book. I designed sewing holes in the spine so I can connect the pages directly to it without using glue. It was a decent idea, but it left a gap between two  . Still, I went with it for the first try.I laid everything down on the canvas that the book would be wrapped in and started assembling it. But I made a crucial mistake - I used super glue. It dries quickly, it is stiff, and doesn't glue 3D printed plastic well and it dissolved the paper I used. Long story short, I made a mess. But I didn't stress too much, I just proclaimed that version is a prototype and used it as a learning experience.I ordered proper bookbinding glue (PVA). While I was waiting for it, I focused on properly sewing the pages together.The first time I sewed the pages together, I poked the holes by hand and they were somewhat uneven. Again, it was nothing major, but I didn't like it. So I designed and 3D printed a simple tool to help me drill the holes evenly.The tool has two parts, and the pages fit snugly between them. Both top and bottom parts have holes, so I was able to put the needle through and poke perfectly even holes in the pages. I'm very proud of this silly contraption.Here you can see all of the eight sheets with sewing holes.Fun fact, I designed all 3D parts using JavaScript and Replicad library. Here is a link if you want to play with the model in your browser.But I ditched the 3D printed spine and used the technique called , which works great when you have only two signatures. It made signatures way more tight than when I connected them separately to the 3D printed spine.When the glue arrived, I plotted everything again and took it from the top. I swapped 3D printed sides for cardboard. Using proper glue was a game changer. I had enough time to apply it before it hardened, and when it dried it stayed flexible. And when it got onto my fingers, it was easy to remove. Everything was much cleaner, and I finally managed to put it all together.Unfortunately, I was rushing to finish the book, so I didn't take any photos of the process. But here are a few I do have:If you are an experienced bookbinder and reading this, I'm sorry for the bookbinding crimes I probably committed. I promise I won't use super glue again.It looked great! It was not perfect (more on that below), but I was super happy with how it turned out. It had a distinct handcrafted feel to it, the images turned out fantastic, and I think I really managed to bring out a personal touch with it.On the day I finished the book and gave it to my wife, we were both exhausted (our kid was teething, and we had a very rough night), so I thought she would appreciate a little pick-me-up.When I gave it to her, the first thing she asked was, "Will I cry?". She was brave, but it definitely got her all mushy and made her day. After reading, she carefully put it on the shelf, out of the reach of the little one.Then I asked her if she ever suspected I was preparing a surprise for her, and she said that she had no idea. But she also said that she thought it was weird that I would often plot something and not brag about it to her afterwards. It was true, I love showing her my work, but luckily she didn't give it too much thought, and I was able to finish my secret project.One thing I would like to fixLike I mentioned, the book isn't perfect. The sides are a bit too large, so the pages seem too deep inside when the book is closed. For the same reason, the end pages turned out to be a bit short, which gives it a weird, uneven look. It is purely aesthetic, but I think it is the only thing keeping it from being perfect.Lesson learned if I ever end up doing something similar.It took way longer than it should have—it took me a month and a half to finish it. It took so long because I did it in secrecy, which meant working late in the evenings when my wife and kid were asleep. A bunch of little failures... ehm, I mean  also prolonged the project. And finally, I had to order multiple things, so I was blocked a few times while I was waiting for four different deliveries.But the final assembly took me around two and a half hours from start to finish - plotting, cutting, sewing, and bookbinding. Mostly because I had already practiced all of them and defined the exact process.It was so much fun. I love projects that span across multiple disciplines. This one touched AI, drawing, plotting, modeling, 3D printing, sewing, and bookbinding. I encountered a lot of little hiccups, but I also learned about all of them. Some of the errors I made could have been avoided if I had been more patient. But I hope you'll cut me some slack - I was super excited and eager to see how it would turn out, and I had limited time windows when I could do it in secrecy. Still, I need to take it as a lesson - being patient will help me save time when doing projects like this one.The highlight for me was that I could do it without an illustrator. Love it or hate it, AI ended up being a fantastic tool that filled the gap in my skill set, which was crucial for making the book.I hope you enjoyed this write-up as much as I enjoyed making the book and writing the post. And I do hope I inspired you to try making something of your own. If I did, please reach out on GitHub, I would love to see it.]]></content:encoded></item><item><title>Beginner’s Guide to Subqueries in SQL</title><link>https://www.kdnuggets.com/beginners-guide-subqueries-sql</link><author>Nate Rosidi</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/Rosidi_Beginners_Guide_to_Subqueries_in_SQL_5.png.png" length="" type=""/><pubDate>Mon, 10 Feb 2025 17:00:38 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Subqueries are popular tools for more complex data manipulation in SQL. If you’re a beginner on a quest to understand subqueries, this is the article for you.]]></content:encoded></item><item><title>Show HN: Global 3D topography explorer</title><link>https://topography.jessekv.com/</link><author>jessekv</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 15:54:45 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[
                Click catchements or regions on the map to render them in 3D.
                Learn more]]></content:encoded></item><item><title>Microsoft Study Finds AI Makes Human Cognition “Atrophied and Unprepared”</title><link>https://www.404media.co/microsoft-study-finds-ai-makes-human-cognition-atrophied-and-unprepared-3/</link><author>Emanuel Maiberg</author><category>tech</category><enclosure url="https://images.unsplash.com/photo-1707396172454-31f31aa5f9d9?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHRoaW5raW5nJTIwZW1vaml8ZW58MHx8fHwxNzM5MjAwMDQwfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" length="" type=""/><pubDate>Mon, 10 Feb 2025 15:26:45 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[A new paper from researchers at Microsoft and Carnegie Mellon University finds that as humans increasingly rely on generative AI in their work, they use less critical thinking, which can “result in the deterioration of cognitive faculties that ought to be preserved.”“[A] key irony of automation is that by mechanising routine tasks and leaving exception-handling to the human user, you deprive the user of the routine opportunities to practice their judgement and strengthen their cognitive musculature, leaving them atrophied and unprepared when the exceptions do arise,” the researchers wrote. The researchers recruited 319 knowledge workers for the study, who self reported 936 first-hand examples of using generative AI in their job, and asked them to complete a survey about how they use generative AI (including what tools and prompts), how confident they are the generative AI tools’ ability to do the specific work task, how confident they are in evaluating the AI’s output, and how confident they are in their abilities in completing the same work task without the AI tool. Some tasks cited in the paper include a teacher using the AI image generator DALL-E to create images for a presentation about hand washing at school, a commodities trader using ChatGPT to “generate recommendations for new resources and strategies to explore to hone my trading skills,” and a nurse who “verified a ChatGPT-generated educational pamphlet for newly diagnosed diabetic patients.”]]></content:encoded></item><item><title>Python Morsels: Newlines and escape sequences in Python</title><link>https://www.pythonmorsels.com/newlines-and-escape-sequences/</link><author></author><category>Planet Python blog</category><category>dev</category><category>python</category><pubDate>Mon, 10 Feb 2025 15:17:29 +0000</pubDate><source url="http://planetpython.org/">Planet Python</source><content:encoded><![CDATA[This string contains a newline character:That's what  represents: a newline character.If we print this string, we'll see that  becomes an  newline:Why does Python represent a newline as ?Escape sequences in PythonEvery character in a Python …]]></content:encoded></item><item><title>Micro Frontends with Angular and Native Federation</title><link>https://blog.angular.dev/micro-frontends-with-angular-and-native-federation-7623cfc5f413?source=rss----447683c3d9a3---4</link><author>Manfred Steyer</author><category>dev</category><category>official</category><category>frontend</category><pubDate>Mon, 10 Feb 2025 15:02:31 +0000</pubDate><source url="https://blog.angular.dev/?source=rss----447683c3d9a3---4">Angular official</source><content:encoded><![CDATA[Enterprise-scale software systems are often implemented by several cross-functional teams. To enable such teams to provide new features efficiently, it is desirable to minimize the need for coordination between them. This calls for a modularization that verticalizes the system into low-coupled areas individual teams can take care of.There are several possibilities for implementing such high-level modules (also called verticals). For instance, they can be implemented using a respective folder structure or in the form of several libraries in a Monorepo. Micro Frontends go one step further and designate a separate application for each vertical. This architectural style promises several advantages, such as a high amount of team autonomy, but it also comes with numerous challenges.The first part of this article provides a critical overview of the benefits and drawbacks of Micro Frontends in the space of single-page applications. The second part discusses how such an architecture can be implemented with Native Federation, a community project built upon web standards that provide close integration with the Angular CLI.Motivation Behind Micro FrontendsLike Micro Services, Micro Frontends promise several advantages, both technically and with respect to the organization. As applying Micro Frontend architectures results in several smaller applications, testing, performance tuning, and isolating faults in one part of the overarching system becomes more effortless, according to several sources.However, the increased team autonomy was the main reason for applying this architectural style in the numerous cases I was involved in as a consultant. Individual teams are not blocked by waiting for other teams and can deploy separately anytime. This might not be a significant concern in a vast number of projects. Still, as soon as we talk about multi-team projects in a corporate environment with long communication paths and decision times, this aspect quickly becomes vital for the project’s success.Teams can also make their own decisions that best fit their goals — architecturally and technology-wise. Mixing multiple client-side frameworks in the same application is considered an anti-pattern and should be avoided. However, it can help to create a migration path over to a new stack in the long run. The concern in corporate environments is that we find software solutions that usually outlive the average technology stackSince Micro Frontends result in separate build processes, combining them with incremental builds, where only changed applications need to be rebuilt, has a massive potential for build-time improvements. For instance, the well-known Nx build system provides this option. Interestingly, this feature can also be used without applying other aspects such as aligning teams with individual applications or separate deployments. There is debate about whether leveraging this tempting option automatically leads to micro frontend architectures.A system consisting of several smaller applications can provide further organizational advantages: It’s easier to onboard new members and scale the development by adding further micro frontends. Team autonomy also leads to faster release cycles.Challenges to keep in mindEvery architectural decision has consequences that need to be evaluated, and Micro Frontends are no exception. Besides the positive consequences outlined above, there are also several negative ones to consider.For instance, individually developed Micro Frontends can diverge in UI/UX, leading to an inconsistent appearance. Also, loading several applications increases the number of bundles that need to be downloaded, adversely affecting loading times and increasing the memory pressure.Splitting an application into low-coupled parts might be a best practice in general. However, it is often hard to define the boundaries between verticals clearly enough to implement them as individual applications. Also, while having several small applications at first glance simplifies the implementation, integrating them into an overarching solution brings additional complexity.This leads to one of the biggest challenges I have seen in practice: we are moving away from a compile-time integration towards a runtime integration. This has severe consequences because we cannot easily foresee problems that might arise when individually developed and deployed applications start interacting at runtime. Besides the chance of technical conflicts, we also have to see that the current generation of SPA frameworks has not been built with such an operation mode in mind.Instead, modern SPA frameworks, especially Angular, have been developed to focus on compile time optimizations. A powerful compiler leverages type checks to identify technical conflicts and emits efficient source code optimized for tree-shaking. Furthermore, the CLI in the Angular space provides a highly optimized build process. An off-label usage necessary for implementing Micro Frontends undermines some of these achievements.Angular Does Not Officially Support Micro FrontendsFor all the outlined reasons, the Angular team recommends checking if alternatives, like implementing the individual verticals in Monorepos, which can be compiled together, are suitable. For instance, Google adopted this approach years ago and manages all its products and libraries in a single Monorepo.Of course, there are also ways to compensate for the disadvantages outlined here, and some of them, like establishing a design system to help with a consistent UI/UX or lazy loading individual system parts, might be needed in general. More details on such compensation strategies can be found in this survey of more than 150 Micro Frontend practitioners.All architectural decisions have benefits and drawbacks and should be evaluated with those considerations if you are going to implement a solution. If such an evaluation reveals that Micro Frontends provide more advantages than alternatives for achieving your very goals, the following sections provide you a well-lit path for implementing this architectural pattern with Angular.Micro Frontends with FederationModule Federation is a popular technology for implementing Micro Frontends and sharing dependencies. Shipped initially with webpack 5, it comes with a tooling-agnostic runtime and provides compile-time integration into  and . Besides the usage of the vite dev server, these technologies are currently not supported by the Angular CLI. However, promising community solutions like and AnalogJS allow them to be used with Angular. Nx and my CLI-plugin provide an effortless integration.Module Federation enables an application to load parts of other separately built and deployed applications lazily. The loading application is referred to as the host; the integrated ones are called remotes:Federation, if permitted by the library version, can share dependencies like Angular or RxJS between the host and remotes. There are several configuration options for preventing version mismatches. Since MF can only decide which dependencies to share at runtime, tree-shaking for shared parts is not possible.To inform the host about the remotes and their shared dependencies, Module Federation creates a metadata file, the so-called remote entry, during the build. This file needs to be loaded into the host.To fully decouple the idea of Federation from specific bundlers, I started the project Native Federation several years ago. Its API surface is very similar to that of Module Federation. The focus is on portability and standards like ECMAScript modules and Import Maps. Its compile time acts as a wrapper around existing bundlers. For the communication with the bundler, it uses an exchangeable adapter:The integration into the Angular CLI directly delegates to Angular’s  that leverages the fast bundler esbuild, and is the foundation for several current features like partial hydration. Because of its architecture, Native Federation can also be ported to further builders or other innovations the CLI might provide in the long run.For integrating Micro Frontends built with Angular’s webpack-based builder, there is a bridging solution allowing the loading of such remotes into a Native Federation host. This solution enables the gradual adoption of the CLI’s new  and permits the sharing of dependencies between the two kinds of Federation. One of the features added recently is support for SSR and Hydration, which is vital for performance-critical applications like public portals and web shops.Native Federation for Angular is close to the CLI’s , but its compilation mode is for shared dependencies differs. While it works well for packages that align with Angular’s Package Format, which is the case for all libraries built with the CLI, other libraries might provide some challenges, especially older ones that still use CommonJS or older conventions for providing metadata.Using Native Federation in AngularFor the setup Native Federation provides a schematic:ng add @angular-architects/native-federation --project mfe1 --port 4201 --type remoteThe switch  defines the kind of the application. Possible options are , , and . The latter is a host configured with a configuration file (federation manifest) during application start. This manifest informs the application of the locations of the remotes and can be switched out by another manifest during deployment:{    "mfe1" : "http://localhost:4201/remoteEntry.json"The key, in this case, is a short name the host uses to refer to the Micro Frontend. The value is the location of the remote entry with the metadata mentioned above. Alternatively, the manifest can be replaced by a service that informs the host of the current location of all deployed remotes and acts as a Micro Frontend registry.The schematic configures the Native Federation builder delegation to the  and creates a configuration file :const { withNativeFederation, shareAll }     = require('@angular-architects/native-federation/config');module.exports = withNativeFederation({    './Component': './projects/mfe1/src/app/app.component.ts',    ...shareAll({}),    'rxjs/ajax',    'rxjs/testing',    // Add further packages you don't need at runtimeThe configuration assigns a unique name to the remote or host and defines which dependencies to share. Instead of providing an exhaustive list of all the dependencies to share, the configuration uses the helper function , which adds all dependencies found in the project’s . The skip list is used to opt out of sharing some of them or their secondary entry points.Remotes also define exposed EcmaScript modules that can be loaded into the shell. For this, the  node maps the paths of the modules to short names such as  in the example shown.The schematic also adds code to initialize Native Federation to the . For the host, this code points to the federation manifest:import { initFederation } from '@angular-architects/native-federation';initFederation('federation.manifest.json')  .catch(err => console.error(err))  .then(_ => import('./bootstrap'))  .catch(err => console.error(err));After initializing federation, the file , also created by the schematic, is loaded. It contains the usual code for bootstrapping Angular, e.g., via  when the application uses Standalone Components.To load a component or routing configuration exposed by a remote, traditional lazy loading is combined with Native Federation’s  function:import { loadRemoteModule } from '@angular-architects/native-federation';export const APP_ROUTES: Routes = [  [...]    path: 'flights',      loadRemoteModule('mfe1', './Component').then((m) => m.AppComponent),];Here,  is the key defined in the manifest, and  points to the respective exposed module in the remote’s federation configuration.More information on Native Federation can be found in this blog article and in the project’s readme, which also links to a tutorial.Micro Frontends promise significant advantages for enterprise-scale applications, such as enhanced team autonomy and independent deployment. These benefits make this architectural style particularly appealing in multi-team corporate environments where streamlined communication and rapid development cycles are critical. Additionally, they support gradual migration to new technologies and optimize build times by leveraging incremental builds.However, these advantages come with trade-offs. Micro Frontends can lead to inconsistent UI/UX, increased load times, and complex runtime integrations. Defining clear vertical boundaries and managing inter-application communication add to the challenge. Furthermore, frameworks like Angular, designed for compile-time optimization, face limitations in runtime integration scenarios. The Angular team, therefore, recommends alternatives such as splitting an application into libraries managed within a Monorepo, which aligns better with Angular’s strengths in type safety and efficient compilation.Module Federation has emerged as a popular solution to address some challenges by enabling lazy loading and dependency sharing. Native Federation builds on these concepts with a focus on standards and portability. It provides a seamless integration into the Angular CLI and its performant esbuild-based , which is also the foundation for advanced features like SSR and hydration.Together with this team at , Manfred Steyer helps companies around the globe establish maintainable Angular-based Architectures. He is a trainer, consultant, and Google Developer Expert (GDE) and writes for O’Reilly, the German Java Magazin, windows.developer, and Heise Developer. Furthermore, he regularly speaks at conferences.]]></content:encoded></item><item><title>Data Science Showdown: Which Tools Will Gain Ground in 2025</title><link>https://www.kdnuggets.com/data-science-showdown-tools-gain-ground-2025</link><author>Iván Palomares Carrascosa</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/kdn-tools-battle-2025.png" length="" type=""/><pubDate>Mon, 10 Feb 2025 15:00:02 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[An analysis and discussion of the data science tools expected to gain prominence throughout the present year, and why.]]></content:encoded></item><item><title>Show HN: A unique generated maze to share with your valentine</title><link>https://love.berk.es/</link><author>berkes</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 14:35:55 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[ is an art project. It is so called generative art: I write the software, and the software creates art autonomously. 
            A nice introduction into generative art is this talk by Tim Holman.
          
          In a gallery in London, I came across gorgeaus screenprints by Ricky Byrne. I loved their use of color and hand-produced feel. Attention for color, layout, tension.
          So I started experimenting with maze generation algorithms in Rust, with Nannou.
          In the process, I decided to make it a web app, for valentine, so everyone can create their own maze. And ported the Rust code to TypeScript in a tiny web app. The Coding Train has a great tutorial set on maze generation. I used the same common recursive backtracking algorithm, because the aesthetics are what I was looking for. 
          The names you provide are used to generate unique randomnes. 
          This is used in a maze generation algorithm to create a maze. 
          I deliberately chose to animate the maze generation, to show the process. It is a slow process, but I think it is interesting to watch it carve out your maze. 
          Saving, copying and sharing the maze is disabled until the maze is generated. Once the maze is generated, you can save, copy and share the maze. 
          Sharing, copying won't work on all browsers and is affected by some browser plugins or settings. The best result is on Chrome on Android. 
          , the only parameter you can provide is the names. The maze is generated based on these names, and the algorithm is fixed. This is by design. Only the two names determine the artwork. 
            However, you can change the source code, see below.
          
            The code is available on GitHub. Feel free to fork, change and improve it, or just have fun with it. Nannou for the original maze generationVite for the build and web stuff]]></content:encoded></item><item><title>Organizing Design System Component Patterns With CSS Cascade Layers</title><link>https://css-tricks.com/organizing-design-system-component-patterns-with-css-cascade-layers/</link><author>Ryan Trimble</author><category>frontend</category><pubDate>Mon, 10 Feb 2025 14:06:52 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[I’m trying to come up with ways to make components more customizable, more efficient, and easier to use and understand, and I want to describe a pattern I’ve been leaning into using CSS Cascade Layers.I enjoy organizing code and find cascade layers a fantastic way to organize code explicitly as the cascade looks at it. The neat part is, that as much as it helps with “top-level” organization, cascade layers can be nested, which allows us to author more precise styles based on the cascade.The only downside here is your imagination, nothing stops us from over-engineering CSS. And to be clear, you may very well consider what I’m about to show you as a form of over-engineering. I think I’ve found a balance though, keeping things simple yet organized, and I’d like to share my findings.Let’s explore a pattern for writing components in CSS using a button as an example. Buttons are one of the more popular components found in just about every component library. There’s good reason for that popularity because buttons can be used for a variety of use cases, including:performing actions, like opening a drawer,navigating to different sections of the UI, andholding some form of state, such as  or .And buttons come in several different flavors of markup, like , , and . There are even more ways to make buttons than that, if you can believe it.On top of that, different buttons perform different functions and are often styled accordingly so that a button for one type of action is distinguished from another. Buttons also respond to state changes, such as when they are hovered, active, and focused. If you have ever written CSS with the BEM syntax, we can sort of think  within the context of cascade layers..button {}
.button-primary {}
.button-secondary {}
.button-warning {}
/* etc. */Okay, now, let’s write some code. Specifically, let’s create a few different types of buttons. We’ll start with a  class that we can set on any element that we want to be styled as, well, a button! We already know that buttons come in different flavors of markup, so a generic  class is the most reusable and extensible way to select one or all of them..button {
  /* Styles common to all buttons */
}This is where we can insert our very first cascade layer! Remember, the reason we want a cascade layer in the first place is that it allows us to set the CSS Cascade’s reading order when evaluating our styles. We can tell CSS to evaluate one layer first, followed by another layer, then another — all according to the order we want. This is an incredible feature that grants us superpower control over which styles “win” when applied by the browser.We’ll call this layer  because, well, buttons are a type of component. What I like about this naming is that it is generic enough to support other components in the future as we decide to expand our design system. It scales with us while maintaining a nice separation of concerns with other styles we write down the road that maybe aren’t specific to components./* Components top-level layer */
@layer components {
  .button {
    /* Styles common to all buttons */
  }
}Here is where things get a little weird. Did you know you can  cascade layers  classes? That’s totally a thing. So, check this out, we can introduce a new layer inside the  class that’s already inside its own layer. Here’s what I mean:/* Components top-level layer */
@layer components {

  .button {
    /* Component elements layer */
    @layer elements {
      /* Styles */
    }
  }
}This is how the browser interprets that layer within a layer at the end of the day:@layer components {
  @layer elements {
    .button {
      /* button styles... */
    }
  }
}So far, we’ve established a  class inside of a cascade layer that’s designed to hold any type of  in our design system. Inside that  is another cascade layer, this one for selecting the different types of buttons we might encounter in the markup. We talked earlier about buttons being , , or  and this is how we can individually select style each type.We can use the  pseudo-selector function as that is akin to saying, “If this  an  element, then apply these styles.”/* Components top-level layer */
@layer components {
  .button {
    /* Component elements layer */
    @layer elements {
      /* styles common to all buttons */

      &:is(a) {
        /* <a> specific styles */
      }

      &:is(button) {
        /* <button> specific styles */
      }

      /* etc. */
    }
  }
}Defining default button stylesI’m going to fill in our code with the common styles that apply to all buttons. These styles sit at the top of the  layer so that they are applied to any and all buttons, regardless of the markup. Consider them default button styles, so to speak./* Components top-level layer */
@layer components {
  .button {
    /* Component elements layer */
    @layer elements {
      background-color: darkslateblue;
      border: 0;
      color: white;
      cursor: pointer;
      display: grid;
      font-size: 1rem;
      font-family: inherit;
      line-height: 1;
      margin: 0;
      padding-block: 0.65rem;
      padding-inline: 1rem;
      place-content: center;
      width: fit-content;
    }
  }
}Defining button state stylesWhat should our default buttons do when they are hovered, clicked, or in focus? These are the different  that the button might take when the user interacts with them, and we need to style those accordingly.I’m going to create a new cascade sub-layer directly under the  sub-layer called, creatively, :/* Components top-level layer */
@layer components {
  .button {
    /* Component elements layer */
    @layer elements {
      /* Styles common to all buttons */
    }

    /* Component states layer */
    @layer states {
      /* Styles for specific button states */
    }
  }
}Pause and reflect here. What states should we target? What do we want to change for each of these states?Some states may share similar property changes, such as  and  having the same background color. Luckily, CSS gives us the tools we need to tackle such problems, using the  function to group property changes based on the state. Why  instead of ?  comes with zero specificity, meaning it’s a lot easier to override than , which takes the specificity of the element with the highest specificity score in its arguments. Maintaining low specificity is a virtue when it comes to writing scalable, maintainable CSS./* Component states layer */
@layer states {
  &:where(:hover, :focus-visible) {
    /* button hover and focus state styles */
  }
}But how do we update the button’s styles in a  way? What I mean by that is how do we make sure that the button  like it’s hovered or in focus? We could just slap a new background color on it, but ideally, the color should be related to the  set in the  layer.So, let’s refactor things a bit. Earlier, I set the  element’s  to . I want to reuse that color, so it behooves us to make that into a CSS variable so we can update it once and have it apply everywhere. Relying on variables is yet another virtue of writing scalable and maintainable CSS.I’ll create a new variable called --button-background-color that is initially set to  and then set it on the default button styles:/* Component elements layer */
@layer elements {
  --button-background-color: darkslateblue;

  background-color: var(--button-background-color);
  border: 0;
  color: white;
  cursor: pointer;
  display: grid;
  font-size: 1rem;
  font-family: inherit;
  line-height: 1;
  margin: 0;
  padding-block: 0.65rem;
  padding-inline: 1rem;
  place-content: center;
  width: fit-content;
}Now that we have a color stored in a variable, we can set that same variable on the button’s hovered and focused states in our other layer, using the relatively new  function to convert  to a lighter color when the button is hovered or in focus.Back to our  layer! We’ll first mix the color in a new CSS variable called :/* Component states layer */
@layer states {
  &:where(:hover, :focus-visible) {
    /* custom property only used in state */
    --state-background-color: color-mix(
      in srgb, 
      var(--button-background-color), 
      white 10%
    );
  }
}We can then apply that color as the background color by updating the  property./* Component states layer */
@layer states {
  &:where(:hover, :focus-visible) {
    /* custom property only used in state */
    --state-background-color: color-mix(
      in srgb, 
      var(--button-background-color), 
      white 10%
    );

    /* applying the state background-color */
    background-color: var(--state-background-color);
  }
}Defining modified button stylesAlong with  and  layers, you may be looking for some sort of variation in your components, such as . That’s because not all buttons are going to look like your default button. You might want one with a green background color for the user to confirm a decision. Or perhaps you want a red one to indicate danger when clicked. So, we can take our existing default button styles and modify them for those specific use cases If we think about the order of the cascade — always flowing from top to bottom — we don’t want the modified styles to affect the styles in the states layer we just made. So, let’s add a new  layer in between  and :/* Components top-level layer */
@layer components {

  .button {
  /* Component elements layer */
  @layer elements {
    /* etc. */
  }

  /* Component modifiers layer */
  @layer modifiers {
    /* new layer! */
  }

  /* Component states layer */
  @layer states {
    /* etc. */
  }
}Similar to how we handled , we can now update the --button-background-color variable for each button modifier. We could modify the styles further, of course, but we’re keeping things fairly straightforward to demonstrate how this system works.We’ll create a new class that modifies the  of the default button from  to . Again, we can rely on the  selector because we want the added specificity in this case. That way, we override the default button style with the modifier class. We’ll call this class  (green is a “successful” color) and feed it to :/* Component modifiers layer */
@layer modifiers {
  &:is(.success) {
    --button-background-color: darkgreen;
  }
}If we add the  class to one of our buttons, it becomes  instead  which is exactly what we want. And since we already do some -ing in the  layer, we’ll automatically inherit those hover and focus styles, meaning  is lightened in those states./* Components top-level layer */
@layer components {
  .button {
    /* Component elements layer */
    @layer elements {
      --button-background-color: darkslateblue;

      background-color: var(--button-background-color);
      /* etc. */

    /* Component modifiers layer */
    @layer modifiers {
      &:is(.success) {
        --button-background-color: darkgreen;
      }
    }

    /* Component states layer */
    @layer states {
      &:where(:hover, :focus) {
        --state-background-color: color-mix(
          in srgb,
          var(--button-background-color),
          white 10%
        );

        background-color: var(--state-background-color);
      }
    }
  }
}We can refactor any CSS property we need to modify into a CSS custom property, which gives us a lot of room for customization./* Components top-level layer */
@layer components {
  .button {
    /* Component elements layer */
    @layer elements {
      --button-background-color: darkslateblue;

      --button-border-width: 1px;
      --button-border-style: solid;
      --button-border-color: transparent;
      --button-border-radius: 0.65rem;

      --button-text-color: white;

      --button-padding-inline: 1rem;
      --button-padding-block: 0.65rem;

      background-color: var(--button-background-color);
      border: 
        var(--button-border-width) 
        var(--button-border-style) 
        var(--button-border-color);
      border-radius: var(--button-border-radius);
      color: var(--button-text-color);
      cursor: pointer;
      display: grid;
      font-size: 1rem;
      font-family: inherit;
      line-height: 1;
      margin: 0;
      padding-block: var(--button-padding-block);
      padding-inline: var(--button-padding-inline);
      place-content: center;
      width: fit-content;
    }

    /* Component modifiers layer */
    @layer modifiers {
      &:is(.success) {
        --button-background-color: darkgreen;
      }

      &:is(.ghost) {
        --button-background-color: transparent;
        --button-text-color: black;
        --button-border-color: darkslategray;
        --button-border-width: 3px;
      }
    }

    /* Component states layer */
    @layer states {
      &:where(:hover, :focus) {
        --state-background-color: color-mix(
          in srgb,
          var(--button-background-color),
          white 10%
        );

        background-color: var(--state-background-color);
      }
    }
  }
}P.S. Look closer at that demo and check out how I’m adjusting the button’s background using  — then go read Sara Joy’s “Come to the  Side” for a thorough rundown of how that works!What do you think? Is this something you would use to organize your styles? I can see how creating a system of cascade layers could be overkill for a small project with few components. But even a little toe-dipping into things like we just did illustrates how much power we have when it comes to managing — and even taming — the CSS Cascade. Buttons are deceptively complex but we saw how few styles it takes to handle everything from the default styles to writing the styles for their states and modified versions.]]></content:encoded></item><item><title>It’s Time To Rethink 6G</title><link>https://spectrum.ieee.org/5g-bandwidth</link><author>William Webb</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjEzMDA0MS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc0NDEyMzk5Mn0.jPunQzFQ_dBRCyydxSdaI9lAn88q3l2GB9hsCIKa5BM/image.png?width=600" length="" type=""/><pubDate>Mon, 10 Feb 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[It’s not more bandwidth that users need]]></content:encoded></item><item><title>How to Join Strings in Python</title><link>https://realpython.com/python-join-string/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Mon, 10 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[Python’s built-in string method  lets you combine string elements from an iterable into a single string, using a separator that you specify. You call  on the separator, passing the iterable of strings to join.By the end of this tutorial, you’ll understand that:You use  in Python to  with a .A  is the piece of text you want inserted between each substring.To join list elements, you call  on a separator string, passing the list as the argument.inserts the separator between each list element to form a single string.The  method  that is the concatenation of the elements in the iterable, separated by the specified string.For smaller string concatenation tasks, you can use the concatenation operator () or  instead of .Python’s built-in  method gives you a quick and reliable way to combine multiple strings into a single string. Whether you need to format output or assemble data for storage,  provides a clean and efficient approach for joining strings from an iterable.In the upcoming sections, you’ll learn the basic usage of  to concatenate strings effectively. You’ll then apply that knowledge to real-world scenarios, from building CSV files to constructing custom log outputs. You’ll also discover some surprising pitfalls and learn how to avoid them. Test your knowledge with our interactive “How to Join Strings in Python” quiz. You’ll receive a score upon completion to help you track your learning progress:Test your understanding of Python's .join() string method for combining strings, handling edge cases, and optimizing performance.How to Join Strings in Python Using To use the string method , you call  on a separator string and pass an iterable of other strings as the argument. The method returns a single string, where it has inserted the separator string between each element of the iterable:In this example, you joined a list of words into one sentence, separated by spaces.At first glance, this usage might look a little backward. In many other string operations, you call the method on the main string that you want to manipulate. However, with , you call the method on the separator string, then pass the iterable of strings that you want to combine:This example achieves the same result as the earlier one but splits the process into two steps. Defining  separately makes the code more readable and avoids the potentially odd-looking syntax of calling  directly on a short string literal. Remember that  is a  method, which means that you’ll need to call it on a  string object. Keeping that in mind may help you remember why you need to call it on the separator string.You rarely see code that’s written in multiple steps where you assign the separator string to a variable, like you did in the example above.In typical usage, you call  directly on the separator string, all in one line. This approach is more concise and highlights that any valid string can be your separator, whether it’s whitespace, a dash, or a multicharacter substring.Join With an Empty StringWhat if you don’t want any separator at all, but just want to concatenate the items? One valid approach is to use an empty string () as the separator:This code snippet concatenates the letters in the list, forming a single string . Using an empty string as the separator is a great way to assemble strings without a delimiter between them.Since  can take any iterable of strings—not just lists—you can even pass a string as an argument. Because strings are iterable, Python iterates over each character in that string, considering each character as a separate element:By calling  on  and passing the string , you effectively place a comma between every single character in . This might not always be what you intend, but it’s a neat trick to keep in mind if you ever need to treat each character as a separate element.]]></content:encoded></item><item><title>Make Any File a Template Using This Hidden macOS Tool</title><link>https://css-tricks.com/make-any-file-a-template-using-this-hidden-macos-tool/</link><author>Geoff Graham</author><category>frontend</category><pubDate>Mon, 10 Feb 2025 13:54:00 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[Stationery Pad is a handy way to nix a step in your workflow if you regularly use document templates on your Mac. The long-standing Finder feature essentially tells a file’s parent application to open a copy of it by default, ensuring that the original file remains unedited.This works for any kind of file, including HTML, CSS, JavaScriprt, or what have you. You can get there with + or right-click and select “Get info.”]]></content:encoded></item><item><title>Show HN: HTML visualization of a PDF file&apos;s internal structure</title><link>https://github.com/desgeeko/pdfsyntax/blob/main/docs/browse.md</link><author>desgeeko</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 13:52:53 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Hi,
I've just finished a rebuild of this function and added a lot of new features: info, page index, minimap, inverted index,... 
I think it may be useful for inspection, debugging or just as a learning resource showcasing the PDF file format.
This is a pet project and I would be happy to receive some feedback!
Regards]]></content:encoded></item><item><title>AWS Extends AI Agent Reach into the Realm of Testing Code</title><link>https://devops.com/aws-extends-ai-agent-reach-into-the-realm-of-testing-code/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 10 Feb 2025 13:45:46 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>@osherove about Pipeline Driven Organizations</title><link>https://www.youtube.com/watch?v=xOKEXaxRdfs</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/xOKEXaxRdfs?version=3" length="" type=""/><pubDate>Mon, 10 Feb 2025 13:01:31 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[Check out the full version on our YouTube channel now! #GOTOcon #GOTOams #PipelineDriven #Cooperative #CooperativePipeline #CoOpsPipelines #ContinuousDelivery #CICD #DevOps #DevSecOps #DevTestOps #Programming #DevOps2 #SDLC #RoyOsherove #Viral #ViralShorts #GOTO

Full version available here:
https://youtu.be/zmA5fhV-FGk

Roy Osherove - Author of "The Art Of Unit Testing" and "Elastic Leadership: Growing Self-Organizing Teams" @osherove 

RECOMMENDED BOOK
Roy Osherove • The Art of Unit Testing • https://bit.ly/3obiKNB
Roy Osherove • Elastic Leadership • https://amzn.to/3fxEgID
Roy Osherove • Notes to a Software Team Leader • https://amzn.to/3wjroxJ
Goldratt & Cox • The Goal: A Process of Ongoing Improvement • https://amzn.to/3jqNpZv

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Using Gemini 2.0 Pro Locally</title><link>https://www.kdnuggets.com/using-gemini-2-pro-locally</link><author>Abid Ali Awan</author><category>dev</category><category>ai</category><enclosure url="https://www.kdnuggets.com/wp-content/uploads/awan_gemini_20_pro_locally_7.png" length="" type=""/><pubDate>Mon, 10 Feb 2025 13:00:34 +0000</pubDate><source url="https://www.kdnuggets.com/">KDNuggets blog</source><content:encoded><![CDATA[Learn the easiest way to use a state-of-the-art Google experimental model locally.]]></content:encoded></item><item><title>AI Reveals Hidden Interior Design Rules of the Cell</title><link>https://spectrum.ieee.org/ai-protein-localization</link><author>Elie Dolgin</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjI2NzY5MC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1NDA3NDYzOH0.X1yTNpRDu8eaQKN-SOF9IdMARfQ4nVEn7DAzQ4sE4D8/image.jpg?width=600" length="" type=""/><pubDate>Mon, 10 Feb 2025 13:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[A new tool predicts where proteins fit, opening new frontiers in drug discovery]]></content:encoded></item><item><title>Transformational Power of Marginal Propensity to Reuse Undermined by Siren of One-off AI Projects</title><link>https://www.datasciencecentral.com/transformational-power-of-marginal-propensity-to-reuse-undermined-by-siren-of-one-off-ai-projects/</link><author>Bill Schmarzo</author><category>dev</category><category>ai</category><pubDate>Mon, 10 Feb 2025 12:59:15 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pairwise Authentication of Humans</title><link>https://www.schneier.com/blog/archives/2025/02/pairwise-authentication-of-humans.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Mon, 10 Feb 2025 12:00:41 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Here’s an easy system for two humans to remotely authenticate to each other, so they can be sure that neither are digital impersonations.To mitigate that risk, I have developed this simple solution where you can setup a unique time-based one-time passcode (TOTP) between any pair of persons.Two people, Person A and Person B, sit in front of the same computer and open this page;
They input their respective names (e.g. Alice and Bob) onto the same page, and click “Generate”;
The page will generate two TOTP QR codes, one for Alice and one for Bob;
Alice and Bob scan the respective QR code into a TOTP mobile app (such as Authy or Google Authenticator) on their respective mobile phones;
In the future, when Alice speaks with Bob over the phone or over video call, and wants to verify the identity of Bob, Alice asks Bob to provide the 6-digit TOTP code from the mobile app. If the code matches what Alice has on her own phone, then Alice has more confidence that she is speaking with the real Bob.]]></content:encoded></item><item><title>Five Great DevOps Job Opportunities</title><link>https://devops.com/five-great-devops-job-opportunities-125/</link><author>Mike Vizard</author><category>devops</category><pubDate>Mon, 10 Feb 2025 09:18:21 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>&amp;#x26;#xa;Reminder: 7-Zip &amp;#x26; MoW, (Mon, Feb 10th)</title><link>https://isc.sans.edu/diary/rss/31668</link><author></author><category>Sans Edu Diaries</category><category>infosec</category><pubDate>Mon, 10 Feb 2025 07:27:53 +0000</pubDate><source url="https://isc.sans.edu/">SANS Internet Storm Center, InfoCON: green</source><content:encoded><![CDATA[CVE-2025-0411 is a vulnerability in 7-zip that has been reported to be exploited in recent attacks. The problem is that Mark-of-Web (MoW) isn't propagated correctly: when extracted, a file inside a ZIP file inside another ZIP file will not have the MoW propagated from the outer ZIP file.That's good to know, but what I personally consider more important to know, is that MoW isn't propagated at all by 7-zip in its default configuration.I wrote about this a couple years ago in diary entry "7-Zip & MoW", when this new feature was introduced.You have to enable MoW propagation in the GUI or via the registry. And that is still the case with the latest versions of 7-zip.

 
 (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.]]></content:encoded></item><item><title>Show HN: Infinite horizontal arrays of text editors</title><link>https://zeminary.com/arrays/app.html</link><author>tsydenzhap</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 01:25:05 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TypeScript: extracting parts of composite types via infer</title><link>https://2ality.com/2025/02/typescript-infer-operator.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In this blog post, we explore how we can extract parts of composite types via the  operator.It helps if you are loosely familiar with conditional types. You can check out section “Conditional types” in “Tackling TypeScript” to read up on them.]]></content:encoded></item><item><title>Show HN: Searchable library of free audiobooks</title><link>https://booksearch.party/</link><author>libridev</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 21:52:33 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[
                If you're seeking free audiobooks and an efficient book finder, booksearch.party is an excellent resource. This platform aggregates a vast collection of audiobooks for free from various sources, including LibriVox, Project Gutenberg, and Lit2Go, into a user-friendly, searchable database.LibriVox offers a wide range of public domain audiobooks, read by volunteers worldwide. Their catalog includes thousands of titles across various genres, all available for free.Project Gutenberg provides over 60,000 free eBooks, many of which have been converted into audiobooks. This extensive collection includes classic literature and historical texts, all accessible without cost.Lit2Go is a free online collection of public domain fiction, poetry, and nonfiction audiobooks. It offers a user-friendly interface where users can browse texts and listen to individual chapters.By compiling these resources, booksearch.party serves as a comprehensive book finder, allowing users to easily search and access a wide array of free audiobooks. Whether you're interested in classic literature, historical documents, or educational materials, this platform simplifies the process of discovering and enjoying audiobooks at no cost.]]></content:encoded></item><item><title>This Was CS50: Crying Poor, Yale To Stop Offering Harvard&apos;s Famed CS50 Course</title><link>https://developers.slashdot.org/story/25/02/09/1922236/this-was-cs50-crying-poor-yale-to-stop-offering-harvards-famed-cs50-course?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 9 Feb 2025 20:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[Slashdot has been covering Harvard's legendary introductory programming course "CS50" since it began setting attendance records in 2014. 

But now long-time Slashdot reader theodp brings some news about the course's fate over at Yale. From Yale's student newspaper:


After a decade of partnership with Harvard, Yale's CS50 course will no longer be offered starting in fall 2025.... One of Yale's largest computer science courses, jointly taught with Harvard University, was canceled during a monthly faculty meeting after facing budgetary challenges. [Yale's endowment is $40+ billion]... Since Yale started offering the course in 2015, CS50 has consistently seen enrollment numbers in the hundreds and was often the department's largest class.... According to [Yale instructor Ozan] Erat, the original [anonymous] donation that made CS50 possible ended in June 2024, and the cost of employing so many undergraduate learning assistants for the course had become unsustainable. 

theodp reminds us that CS50 and its progeny "will continue to live on in all their glory in-person and online at Harvard and edX."]]></content:encoded></item><item><title>What Do Linux Kernel Developers Think of Rust?</title><link>https://developers.slashdot.org/story/25/02/08/0455231/what-do-linux-kernel-developers-think-of-rust?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>slashdot</category><category>linux</category><pubDate>Sun, 9 Feb 2025 17:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Keynotes at this year's FOSDEM included free AI models and systemd, reports Heise.de — and also a progress report from Miguel Ojeda, supervisor of the Rust integration in the Linux kernel.

Only eight people remain in the core team around Rust for Linux... Miguel Ojeda therefore launched a survey among kernel developers, including those outside the Rust community, and presented some of the more important voices in his FOSDEM talk. The overall mood towards Rust remains favorable, especially as Linus Torvalds and Greg Kroah-Hartman are convinced of the necessity of Rust integration. This is less about rapid progress and more about finding new talent for kernel development in the future. 
The reaction was mostly positive, judging by Ojeda's slides: 

- "2025 will be the year of Rust GPU drivers..." — Daniel Almedia 

- "I think the introduction of Rust in the kernel is one of the most exciting development experiments we've seen in a long time." — Andrea Righi

 - "[T]he project faces unique challenges. Rust's biggest weakness, as a language, is that relatively few people speak it. Indeed, Rust is not a language for beginners, and systems-level development complicates things even more. That said, the Linux kernel project has historically attracted developers who love challenging software — if there's an open source group willing to put the extra effort for a better OS, it's the kernel devs." — Carlos Bilbao 

- "I played a little with [Rust] in user space, and I just absolutely hate the cargo concept... I hate having to pull down other code that I do not trust. At least with shared libraries, I can trust a third party to have done the build and all that... [While Rust should continue to grow in the kernel], if a subset of C becomes as safe as Rust, it may make Rust obsolete..." Steven Rostedt 

Rostedt wasn't sure if Rust would attract more kernel contributors, but did venture this opinion. "I feel Rust is more of a language that younger developers want to learn, and C is their dad's language." 

But still "contention exists within the kernel development community between those pro-Rust and -C camps," argues The New Stack, citing the latest remarks from kernel maintainer Christoph Hellwig (who had earlier likened the mixing of Rust and C to cancer). Three days later Hellwig reiterated his position again on the Linux kernel mailing list:

"Every additional bit that another language creeps in drastically reduces the maintainability of the kernel as an integrated project. The only reason Linux managed to survive so long is by not having internal boundaries, and adding another language completely breaks this. You might not like my answer, but I will do everything I can do to stop this. This is NOT because I hate Rust. While not my favourite language it's definitively one of the best new ones and I encourage people to use it for new projects where it fits. I do not want it anywhere near a huge C code base that I need to maintain." 

But the article also notes that Google "has been a staunch supporter of adding Rust to the kernel for Linux running in its Android phones."
The use of Rust in the kernel is seen as a way to avoid memory vulnerabilities associated with C and C++ code and to add more stability to the Android OS. "Google's wanting to replace C code with Rust represents a small piece of the kernel but it would have a huge impact since we are talking about billions of phones," Ojeda told me after his talk. 

In addition to Google, Rust adoption and enthusiasm for it is increasing as Rust gets more architectural support and as "maintainers become more comfortable with it," Ojeda told me. "Maintainers have already told me that if they could, then they would start writing Rust now," Ojeda said. "If they could drop C, they would do it...." 

Amid the controversy, there has been a steady stream of vocal support for Ojeda. Much of his discussion also covered statements given by advocates for Rust in the kernel, ranging from lead developers of the kernel and including Linux creator Linus Torvalds himself to technology leads from Red Hat, Samsung, Google, Microsoft and others.]]></content:encoded></item><item><title>Flipper Culture Is Wilder Than You Think</title><link>https://www.youtube.com/watch?v=GFB8IuWXK68</link><author>Jack Rhysider</author><category>Darknet diaries</category><category>infosec</category><category>video</category><category>podcast</category><enclosure url="https://www.youtube.com/v/GFB8IuWXK68?version=3" length="" type=""/><pubDate>Sun, 9 Feb 2025 17:00:15 +0000</pubDate><source url="https://www.youtube.com/channel/UCMIqrmh2lMdzhlCPK5ahsAg">Jack Rhysider</source><content:encoded><![CDATA[Flipper Culture Is Wilder Than You Think

#flipping #truecrime #podcast 

Listen to the full episode 🎙 Darknet Diaries Ep. 153: Bike Index]]></content:encoded></item><item><title>Advanced Magnet Manufacturing Begins in the United States</title><link>https://spectrum.ieee.org/advanced-magnet-manufacturing-in-us</link><author>Glenn Zorpette</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjIyNDAzNC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2MTcyODI4N30.sY8_LtyTvEG0Uq1-4DjT5HjiDRMf2L-i7sD3Jaq2b50/image.jpg?width=600" length="" type=""/><pubDate>Sun, 9 Feb 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Rare-earths maker MP Materials leads a tiny charge against the Chinese colossus]]></content:encoded></item><item><title>Show HN: My first side project, streamlined book clubs on Slack</title><link>https://booktalk.club/</link><author>Papamanolis</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 13:28:10 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[Get personalized suggestions and find theperfect reads for your team]]></content:encoded></item><item><title>Lessons From the Design &amp; Implementation of a CSV Library • George Wilson • YOW! 2018</title><link>https://www.youtube.com/watch?v=5nZ1AiWWnUE</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/5nZ1AiWWnUE?version=3" length="" type=""/><pubDate>Sun, 9 Feb 2025 13:00:13 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at YOW! 2018. #GOTOcon #YOW
https://yowcon.com

George Wilson - Functional Programming Engineer at CSIRO's Data61

RESOURCES
https://github.com/gwils
https://wils.online
https://www.linkedin.com/in/george-wilson-49a54457

RECOMMENDED BOOKS
Daniel Higginbotham • Clojure for the Brave and True • https://amzn.to/38tZjsF
Ulisses Almeida • Learn Functional Programming with Elixir • https://amzn.to/389grn6
Neal Ford • Functional Thinking • https://amzn.to/3DdP35B
Petricek & Skeet • Real-World Functional Programming • https://amzn.to/38diF4M
Venkat Subramaniam • Functional Programming in Java • https://amzn.to/2WlOXIt
Cristian Salcescu • Functional Programming in JavaScript • https://amzn.to/3y75jBS
Richard Feldman • Elm in Action • https://amzn.to/387kujI
Jeremy Fairbank • Programming Elm • https://amzn.to/2WhZCE8
Wolfgang Loder • Web Applications with Elm • https://amzn.to/3jblQ3q

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#CSVLibrary CSV #Haskell #FunctionalProgramming #Programming #Coding #FunctionalTraining #GeorgeWilson #Functional #HaskellTutorial #YOWcon

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>C++ on Steroids: Bjarne Stroustrup Presents Guideline-Enforcing &apos;Profiles&apos; For Resource and Type Safety</title><link>https://developers.slashdot.org/story/25/02/09/0636247/c-on-steroids-bjarne-stroustrup-presents-guideline-enforcing-profiles-for-resource-and-type-safety?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 9 Feb 2025 12:04:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA["It is now 45+ years since C++ was first conceived," writes 74-year-old C++ creator Bjarne Stroustrup in an article this week for Communications of the ACM. But he complains that many developers "use C++ as if it was still the previous millennium," in an article titled 21st Century C++ that promises "the key concepts on which performant, type safe, and flexible C++ software can be built: resource management, life-time management, error-handling, modularity, and generic programming... 

"At the end, I present ways to ensure that code is contemporary, rather than relying on outdated, unsafe, and hard-to-maintain techniques: guidelines and profiles."

To help developers focus on effective use of contemporary C++ and avoid outdated "dark corners" of the language, sets of guidelines have been developed. Here I focus on the C++ Core guidelines that I consider the most ambitious... My principal aim is a type-safe and resource-safe use of ISO standard C++. That is: 

- Every object is exclusively used according to its definition
- No resource is leaked 
This encompasses what people refer to as memory safety and much more. It is not a new goal for C++. Obviously, it cannot be achieved for every use of C++, but by now we have years of experience showing that it can be done for modern code, though so far enforcement has been incomplete... When thinking about C++, it is important to remember that C++ is not just a language but part of an ecosystem consisting of implementations, libraries, tools, teaching, and more. 
WG21 (and others) are working on "profiles" to enforce guidelines (though they're "not yet available, except for experimental and partial versions"). But Stroustrup writes that the C++ Core Guidelines "use a strategy known as subset-of-superset."

 First: extend the language with a few library abstractions: use parts of the standard library and add a tiny library to make use of the guidelines convenient and efficient (the Guidelines Support Library, GSL).
 Next: subset: ban the use of low-level, inefficient, and error-prone features. 
What we get is "C++ on steroids": Something simple, safe, flexible, and fast; rather than an impoverished subset or something relying on massive run-time checking. Nor do we create a language with novel and/or incompatible features. The result is 100% ISO standard C++. Messy, dangerous, low-level features can still be enabled and used when needed. 
Stroustrup writes that the C++ Core Guidelines focus on rules "we hope that everyone eventually could benefit from."

No uninitialized variables
No range or nullptr violations
No resource leaks
No dangling pointers
No type violations
No invalidation


Bjarne Stroustrup answered questions from Slashdot readers in 2014...
]]></content:encoded></item><item><title>#493: Quarto: Open-source technical publishing</title><link>https://talkpython.fm/episodes/show/493/quarto-open-source-technical-publishing</link><author></author><category>Talk Python</category><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://talkpython.fm/episodes/download/493/quarto-open-source-technical-publishing.mp3" length="" type=""/><pubDate>Sun, 9 Feb 2025 08:00:00 +0000</pubDate><source url="https://talkpython.fm/">Talk Python To Me</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Daily-notes.nvim – fuzzy time journal and planning plugin</title><link>https://github.com/fdavies93/daily-notes.nvim</link><author>fdavies93</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 06:42:10 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I wrote an nvim plugin that does fuzzy time parsing on plain english dates to help you create + organise periodic notes. I use it daily at work and home. Hope it's helpful to others. :)note: not using NLP, LLMs or 'true' fuzzy parsing as per academic literature; just normal recursive descent parsing]]></content:encoded></item><item><title>TypeDoc: testing code examples in doc comments</title><link>https://2ality.com/2025/02/testing-typedoc-examples.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sun, 9 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[TypeDoc now lets us refer to parts of other files via . In this blog post, I explain how that works and why it’s useful.]]></content:encoded></item><item><title>Show HN: Chez Scheme txtar port from Go</title><link>https://git.sr.ht/~egtann/txtar/</link><author>hellcow</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 18:24:39 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[txtar enables you to work with a simple text archive format compatible with
Go's txtar. It concatenates files together
and allows for a top-level comment.Math implementation
-- math.h --
#ifndef MATH_H
#define MATH_H

int add(int a, int b);

#endif
-- math.c --
#include "math.h"

int add(int a, int b) {
    return a + b;
}
This format is easy for humans to read and write by hand and is perfect for
test data.
$makeinstall


$If you want to remove the library from your system, simply run .You can obtain these libraries and many more via Thunderchez:$gitclonehttps://github.com/ovenpasta/thunderchez


$/path/to/thunderchez
All the exports of this library are documented with type expectations. I
encourage you to scan the implementation.A few common example usecases are presented below for convenience:To use this library, simply import :#Construct an archive from a list of filenames#Write text to an archive file#Retrieve a file from an archiveCopyright (C) 2025 Evan Tann, ParaVoce LLCThis program is free software: you can redistribute it and/or modify it under
the terms of the GNU Affero General Public License as published by the Free
Software Foundation, either version 3 of the License, or (at your option) any
later version.This program is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.]]></content:encoded></item><item><title>Show HN: FlashSpace – fast, open-source, macOS Spaces replacement</title><link>https://github.com/wojciech-kulik/FlashSpace</link><author>wojciech-kulik</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 17:19:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">Show HN</source><content:encoded><![CDATA[I've recently launched a new open-source project aimed at enhancing the experience of switching between Spaces/workspaces on macOS. The built-in Spaces feature is often slow and unfriendly. This project is designed to boost your productivity :). Enjoy!]]></content:encoded></item><item><title>UK Is Ordering Apple to Break Its Own Encryption</title><link>https://www.schneier.com/blog/archives/2025/02/uk-is-ordering-apple-to-break-its-own-encryption.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Sat, 8 Feb 2025 15:56:32 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[The  is reporting that the UK government has served Apple with a “technical capability notice” as defined by the 2016 Investigatory Powers Act, requiring it to break the Advanced Data Protection encryption in iCloud for the benefit of law enforcement.This is a big deal, and something we in the security community have worried was coming for a while now.The law, known by critics as the Snoopers’ Charter, makes it a criminal offense to reveal that the government has even made such a demand. An Apple spokesman declined to comment.Apple can appeal the U.K. capability notice to a secret technical panel, which would consider arguments about the expense of the requirement, and to a judge who would weigh whether the request was in proportion to the government’s needs. But the law does not permit Apple to delay complying during an appeal.In March, when the company was on notice that such a requirement might be coming, it told Parliament: “There is no reason why the U.K. [government] should have the authority to decide for citizens of the world whether they can avail themselves of the proven security benefits that flow from end-to-end encryption.”Apple is likely to turn the feature off for UK users rather than break it for everyone worldwide. Of course, UK users will be able to spoof their location. But this might not be enough. According to the law, Apple would not be able to offer the feature to anyone who is in the UK at any point: for example, a visitor from the US.And what happens next? Australia has a law enabling it to ask for the same thing. Will it? Will even more countries follow?]]></content:encoded></item><item><title>Supersonic Passenger Jet Prototype Surpasses Mach 1</title><link>https://spectrum.ieee.org/supersonic-jet</link><author>Shannon Cuthrell</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjIyNzYwMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1ODQ5NjYwOX0.8Ncj9Lpb_GOWVrdnR24YMP88h9kH527ywpfTH1d1eDI/image.jpg?width=600" length="" type=""/><pubDate>Sat, 8 Feb 2025 14:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The Overture could be the first commercial supersonic plane since the Concorde]]></content:encoded></item><item><title>Digest #159: SQLite vs. PostgreSQL, Kubernetes Cloud Savings, DeepSeek Data Leaks and Terraform Tools</title><link>https://www.devopsbulletin.com/p/digest-159-sqlite-vs-postgresql-kubernetes</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://i.scdn.co/image/ab6765630000ba8a9b07806763f18923cf69da89" length="" type=""/><pubDate>Sat, 8 Feb 2025 13:41:17 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!Ever wonder if SQLite or PostgreSQL is the better choice? This week, we share how Kubernetes cut our cloud spend, saved $3.9M a year, and revealed how the Wiz research team uncovered a leaking database. We look at a new CI/CD built for developers, see how Google keeps its cloud safe, and introduce Flox—a new tool to replace Docker with Kubernetes libraries.In our podcast selection, listen to the wild tale of a penetration tester who accidentally robbed the wrong bank. Our tutorials cover topics like keeping your Git credentials safe, using AI tools as a staff engineer, and avoiding huge Terraform state files. We also highlight cool projects like Mathesar for managing Postgres data, a tool to explore SQLite pages, a web honeypot to fool attackers, an AI agent called Goose, and TerraConstructs, a new helper for Terraform.All this and more in this week’s DevOps Bulletin—don’t miss out!A penetration tester accidentally robs the wrong bank:Highlighting cool DevOps projects to keep an eye on: is an intuitive spreadsheet-like interface that lets users of all technical skill levels view, edit, query, and collaborate on Postgres data directly. is a visual tool that explores SQLite databases page-by-page, how they're stored on disk, and how SQLite sees them. is a web honeypot library that creates vulnerable-looking endpoints to detect and mislead attackers. is an open-source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM. is a library of classes and interfaces inspired by AWS CDK but designed to leverage the power and flexibility of Terraform.If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>The Perfect Language • Bodil Stokke • YOW! 2017</title><link>https://www.youtube.com/watch?v=EaQ3qMa_8UI</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/EaQ3qMa_8UI?version=3" length="" type=""/><pubDate>Sat, 8 Feb 2025 13:00:23 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at YOW! 2017. #GOTOcon #YOW
https://yowcon.com

Bodil Stokke - Functional Programming Hipster & Computer Programmer at Sinopia Olive @bodilpwnz 

ABSTRACT
There’s no such thing as the perfect programming language. I’m sorry, but there isn’t, and I’ll explain why.

On the other hand, if you were set the task of designing the perfect language, how close could you get, how would you go about it, and what would it look like in the end? We’re going to explore this problem space with a tour along the bleeding edge of modern language design, giving you some insight into what’s already out there, what tradeoffs each new idea comes with, and what sort of tools you might be working with in a future where programming is slightly less awful than it is today. [...]

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#SoftwareEngineering #Complexity #SoftwareDevelopment #Programming #BodilStokke #ProgrammingLanguages #FunctionalProgramming #YOWcon

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Rust vs. Go (Golang): Performance 2025</title><link>https://www.youtube.com/watch?v=CsKNTwS9kic</link><author>Anton Putra</author><category>dev</category><category>video</category><enclosure url="https://www.youtube.com/v/CsKNTwS9kic?version=3" length="" type=""/><pubDate>Sat, 8 Feb 2025 11:36:09 +0000</pubDate><source url="https://www.youtube.com/channel/UCeLvlbC754U6FyFQbKc0UnQ">Anton Putra</source><content:encoded><![CDATA[Go vs Rust Performance Comparison. Go (1.23.6) vs Rust (1.84.1).

🔴 To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)

🍿 Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&si=p-UOaVM_6_SFx52H

👋 AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra

▬▬▬▬▬ Experience & Location 💼 ▬▬▬▬▬
►  I’m a Senior Software Engineer at Juniper Networks (13+ years of experience)
►  Located in San Francisco Bay Area, CA (US citizen)

▬▬▬▬▬▬ Connect with me 👋 ▬▬▬▬▬▬
►  LinkedIn: https://www.linkedin.com/in/anton-putra
►  Twitter/X: https://twitter.com/antonvputra
►  GitHub: https://github.com/antonputra
►  Email: me@antonputra.com

▬▬▬▬▬▬▬ Source Code 📚 ▬▬▬▬▬▬▬
► Original Source Code: https://github.com/antonputra/tutorials/tree/243/lessons/243

#rust #golang #devops]]></content:encoded></item><item><title>Google&apos;s 7-Year Slog To Improve Chrome Extensions Still Hasn&apos;t Satisfied Developers</title><link>https://developers.slashdot.org/story/25/02/07/2246202/googles-7-year-slog-to-improve-chrome-extensions-still-hasnt-satisfied-developers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Sat, 8 Feb 2025 01:25:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[The Register's Thomas Claburn reports: Google's overhaul of Chrome's extension architecture continues to pose problems for developers of ad blockers, content filters, and privacy tools. [...] While Google's desire to improve the security, privacy, and performance of the Chrome extension platform is reasonable, its approach -- which focuses on code and permissions more than human oversight -- remains a work-in-progress that has left extension developers frustrated.
 
Alexei Miagkov, senior staff technology at the Electronic Frontier Foundation, who oversees the organization's Privacy Badger extension, told The Register, "Making extensions under MV3 is much harder than making extensions under MV2. That's just a fact. They made things harder to build and more confusing." Miagkov said with Privacy Badger the problem has been the slowness with which Google addresses gaps in the MV3 platform. "It feels like MV3 is here and the web extensions team at Google is in no rush to fix the frayed ends, to fix what's missing or what's broken still." According to Google's documentation, "There are currently no open issues considered a critical platform gap," and various issues have been addressed through the addition of new API capabilities.
 
Miagkov described an unresolved problem that means Privacy Badger is unable to strip Google tracking redirects on Google sites. "We can't do it the correct way because when Google engineers design the [chrome.declarativeNetRequest API], they fail to think of this scenario," he said. "We can do a redirect to get rid of the tracking, but it ends up being a broken redirect for a lot of URLs. Basically, if the URL has any kind of query string parameters -- the question mark and anything beyond that -- we will break the link." Miagkov said a Chrome developer relations engineer had helped identify a workaround, but it's not great. Miagkov thinks these problems are of Google's own making -- the company changed the rules and has been slow to write the new ones. "It was completely predictable because they moved the ability to fix things from extensions to themselves," he said. "And now they need to fix things and they're not doing it."]]></content:encoded></item><item><title>Teen on Musk’s DOGE Team Graduated from ‘The Com’</title><link>https://krebsonsecurity.com/2025/02/teen-on-musks-doge-team-graduated-from-the-com/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Sat, 8 Feb 2025 00:32:53 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[ reported this week that a 19-year-old working for ‘s so-called Department of Government Efficiency (DOGE) was given access to sensitive US government systems even though his past association with cybercrime communities should have precluded him from gaining the necessary security clearances to do so. As today’s story explores, the DOGE teen is a former denizen of ‘,’ an archipelago of Discord and Telegram chat channels that function as a kind of distributed cybercriminal social network for facilitating instant collaboration.Since President Trump’s second inauguration, Musk’s DOGE team has gained access to a truly staggering amount of personal and sensitive data on American citizens, moving quickly to seize control over databases at the , the Office of Personnel Management, the , and the Department of Health and Human Resources, among others. first reported on Feb. 2 that one of the technologists on Musk’s crew is a 19-year-old high school graduate named , who reportedly goes by the nickname “Big Balls” online. One of the companies Coristine founded, , was set up in 2021, when he would have been around 16 years old.“Tesla.Sexy LLC controls dozens of web domains, including at least two Russian-registered domains,”  reported. “One of those domains, which is still active, offers a service called Helfie, which is an AI bot for Discord servers targeting the Russian market. While the operation of a Russian website would not violate US sanctions preventing Americans doing business with Russian companies, it could potentially be a factor in a security clearance review.”Mr. Coristine has not responded to requests for comment. In a follow-up story this week,  found that someone using a Telegram handle tied to Coristine solicited a DDoS-for-hire service in 2022, and that he worked for a short time at a company that specializes in protecting customers from DDoS attacks.A profile photo from Coristine’s WhatsApp account.Internet routing records show that Coristine runs an Internet service provider called  (AS400495). Also known as “,” Packetware currently hosts tesla[.]sexy and diamondcdn[.]com, among other domains.DiamondCDN was advertised and claimed by someone who used the nickname “” on several Com-based Discord channels over the years. A review of chat logs from some of those channels show other members frequently referred to Rivage as “Edward.”From late 2020 to late 2024, Rivage’s conversations would show up in multiple Com chat servers that are closely monitored by security companies. In November 2022, Rivage could be seen requesting recommendations for a reliable and powerful DDoS-for-hire service.Rivage made that request in the cybercrime channel “,” a core Com hub where users could buy and sell attack services. Dstat’s website dstat[.]cc was seized in 2024 as part of “Operation PowerOFF,” an international law enforcement action against DDoS services.Coristine’s LinkedIn profile said that in 2022 he worked at an anti-DDoS company called , which  generously described as a “network monitoring firm known for hiring reformed blackhat hackers.”  wrote:“At Path Network, Coristine worked as a systems engineer from April to June of 2022, according to his now-deleted LinkedIn résumé. Path has at times listed as employees , also known as Cosmo the God, a well-known former cybercriminal and member of the hacker group UGNazis, as well as Matthew Flannery, an Australian convicted hacker whom police allege was a member of the hacker group LulzSec. It’s unclear whether Coristine worked at Path concurrently with those hackers, and WIRED found no evidence that either Coristine or other Path employees engaged in illegal activity while at the company.”The other founder of BackConnect Security LLC was , a Georgia man who pleaded guilty in 2020 to paying a DDoS-for-hire service to launch attacks against others.The aforementioned Path employee Eric Taylor pleaded guilty in 2017 to charges including an attack on our home in 2013. Taylor was among several men involved in making a false report to my local police department about a supposed hostage situation at our residence in Virginia. In response, a heavily-armed police force surrounded my home and put me in handcuffs at gunpoint before the police realized it was all a dangerous hoax known as “swatting.”CosmoTheGod rocketed to Internet infamy in 2013 when he and a number of other hackers set up the Web site exposed[dot]su, which “doxed” dozens of public officials and celebrities by publishing the address, Social Security numbers and other personal information on the former First Lady Michelle Obama, the then-director of the FBI and the U.S. attorney general, among others. The group also swatted many of the people they doxed. noted that Coristine only worked at Path for a few months in 2022, but the story didn’t mention why his tenure was so short. A screenshot shared on the website pathtruths.com includes a snippet of conversations in June 2022 between Path employees discussing Coristine’s firing.According to that record, Path founder Marshal Webb dismissed Coristine for leaking internal documents to a competitor. Not long after Coristine’s termination, someone leaked an abundance of internal Path documents and conversations. Among other things, those chats revealed that one of Path’s technicians was a Canadian man named  who was convicted in 2017 of perpetrating dozens of swatting attacks and fake bomb threats — including at least two attempts against our home in 2014.A snippet of text from an internal Path chat room, wherein members discuss the reason for Coristine’s termination: Allegedly, leaking internal company information. Source: Pathtruths.com.On May 11, 2024, Rivage posted on a Discord channel for a DDoS protection service that is chiefly marketed to members of The Com. Rivage expressed frustration with his time spent on Com-based communities, suggesting that its profitability had been oversold.“I don’t think there’s a lot of money to be made in the com,” Rivage lamented. “I’m not buying Heztner [servers] to set up some com VPN.”Rivage largely stopped posting messages on Com channels after that.  reports that Coristine subsequently spent three months last summer working at , Elon Musk’s brain implant startup.The trouble with all this is that even if someone sincerely intends to exit The Com after years of consorting with cybercriminals, they are often still subject to personal attacks, harassment and hacking long after they have left the scene.That’s because a huge part of Com culture involves harassing, swatting and hacking other members of the community. These internecine attacks are often for financial gain, but just as frequently they are perpetrated by cybercrime groups to exact retribution from or assert dominance over rival gangs.Experts say it is extremely difficult for former members of violent street gangs to gain a security clearance needed to view sensitive or classified information held by the U.S. government. That’s because ex-gang members are highly susceptible to extortion and coercion from current members of the same gang, and that alone presents an unacceptable security risk for intelligence agencies.And make no mistake: The Com is the English-language cybercriminal hacking equivalent of a violent street gang. KrebsOnSecurity has published numerous stories detailing how feuds within the community periodically spill over into real-world violence.When Coristine’s name surfaced in ‘s report this week, members of The Com immediately took notice. In the following segment from a February 5, 2025 chat in a Com-affiliated hosting provider, members criticized Rivage’s skills, and discussed harassing his family and notifying authorities about incriminating accusations that may or may not be true.2025-02-05 16:29:44 UTC vperked#0 they got this nigga on indiatimes man
2025-02-05 16:29:46 UTC alexaloo#0 Their cropping is worse than AI could have done
2025-02-05 16:29:48 UTC hebeatsme#0 bro who is that
2025-02-05 16:29:53 UTC hebeatsme#0 yalla re talking about
2025-02-05 16:29:56 UTC xewdy#0 edward
2025-02-05 16:29:56 UTC .yarrb#0 rivagew
2025-02-05 16:29:57 UTC vperked#0 Rivarge
2025-02-05 16:29:57 UTC xewdy#0 diamondcdm
2025-02-05 16:29:59 UTC vperked#0 i cant spell it
2025-02-05 16:30:00 UTC hebeatsme#0 rivage
2025-02-05 16:30:08 UTC .yarrb#0 yes
2025-02-05 16:30:14 UTC hebeatsme#0 i have him added
2025-02-05 16:30:20 UTC hebeatsme#0 hes on discord still
2025-02-05 16:30:47 UTC .yarrb#0 hes focused on stroking zaddy elon
2025-02-05 16:30:47 UTC vperked#0 https://en.wikipedia.org/wiki/Edward_Coristine
2025-02-05 16:30:50 UTC vperked#0 no fucking way
2025-02-05 16:30:53 UTC vperked#0 they even made a wiki for him
2025-02-05 16:30:55 UTC vperked#0 LOOOL
2025-02-05 16:31:05 UTC hebeatsme#0 no way
2025-02-05 16:31:08 UTC hebeatsme#0 hes not a good dev either
2025-02-05 16:31:14 UTC hebeatsme#0 like????
2025-02-05 16:31:22 UTC hebeatsme#0 has to be fake
2025-02-05 16:31:24 UTC xewdy#0 and theyre saying ts
2025-02-05 16:31:29 UTC xewdy#0 like ok bro
2025-02-05 16:31:51 UTC .yarrb#0 now i wanna know what all the other devs are like…
2025-02-05 16:32:00 UTC vperked#0 “`Coristine used the moniker “bigballs” on LinkedIn and @Edwardbigballer on Twitter, according to The Daily Dot.[“`
2025-02-05 16:32:05 UTC vperked#0 LOL
2025-02-05 16:32:06 UTC hebeatsme#0 lmfaooo
2025-02-05 16:32:07 UTC vperked#0 bro
2025-02-05 16:32:10 UTC hebeatsme#0 bro
2025-02-05 16:32:17 UTC hebeatsme#0 has to be fake right
2025-02-05 16:32:22 UTC .yarrb#0 does it mention Rivage?
2025-02-05 16:32:23 UTC xewdy#0 He previously worked for NeuraLink, a brain computer interface company led by Elon Musk
2025-02-05 16:32:26 UTC xewdy#0 bro what
2025-02-05 16:32:27 UTC alexaloo#0 I think your current occupation gives you a good insight of what probably goes on
2025-02-05 16:32:29 UTC hebeatsme#0 bullshit man
2025-02-05 16:32:33 UTC xewdy#0 this nigga got hella secrets
2025-02-05 16:32:37 UTC hebeatsme#0 rivage couldnt print hello world
2025-02-05 16:32:42 UTC hebeatsme#0 if his life was on the line
2025-02-05 16:32:50 UTC xewdy#0 nigga worked for neuralink
2025-02-05 16:32:54 UTC hebeatsme#0 bullshit
2025-02-05 16:33:06 UTC Nashville Dispatch ##0000 ||@PD Ping||
2025-02-05 16:33:07 UTC hebeatsme#0 must have killed all those test pigs with some bugs
2025-02-05 16:33:24 UTC hebeatsme#0 ur telling me the rivage who failed to start a company
2025-02-05 16:33:28 UTC hebeatsme#0 https://cdn.camp
2025-02-05 16:33:32 UTC hebeatsme#0 who didnt pay for servers
2025-02-05 16:33:34 UTC hebeatsme#0 ?
2025-02-05 16:33:42 UTC hebeatsme#0 was too cheap
2025-02-05 16:33:44 UTC vperked#0 yes
2025-02-05 16:33:50 UTC hebeatsme#0 like??
2025-02-05 16:33:53 UTC hebeatsme#0 it aint adding up
2025-02-05 16:33:56 UTC alexaloo#0 He just needed to find his calling idiot.
2025-02-05 16:33:58 UTC alexaloo#0 He found it.
2025-02-05 16:33:59 UTC hebeatsme#0 bro
2025-02-05 16:34:01 UTC alexaloo#0 Cope in a river dude
2025-02-05 16:34:04 UTC hebeatsme#0 he cant make good money right
2025-02-05 16:34:08 UTC hebeatsme#0 doge is about efficiency
2025-02-05 16:34:11 UTC hebeatsme#0 he should make $1/he
2025-02-05 16:34:15 UTC hebeatsme#0 $1/hr
2025-02-05 16:34:25 UTC hebeatsme#0 and be whipped for better code
2025-02-05 16:34:26 UTC vperked#0 prolly makes more than us
2025-02-05 16:34:35 UTC vperked#0 with his dad too
2025-02-05 16:34:52 UTC hebeatsme#0 time to report him for fraud
2025-02-05 16:34:54 UTC hebeatsme#0 to donald trump
2025-02-05 16:35:04 UTC hebeatsme#0 rivage participated in sim swap hacks in 2018
2025-02-05 16:35:08 UTC hebeatsme#0 put that on his wiki
2025-02-05 16:35:10 UTC hebeatsme#0 thanks
2025-02-05 16:35:15 UTC hebeatsme#0 and in 2021
2025-02-05 16:35:17 UTC hebeatsme#0 thanks
2025-02-05 16:35:19 UTC chainofcommand#0 i dont think they’ll care tbhGiven the speed with which Musk’s DOGE team was allowed access to such critical government databases, it strains credulity that Coristine could have been properly cleared beforehand. After all, he’d recently been dismissed from a job for allegedly leaking internal company information to outsiders.According to the national security adjudication guidelines (PDF) released by the Director of National Intelligence (DNI), eligibility determinations take into account a person’s stability, trustworthiness, reliability, discretion, character, honesty, judgment, and ability to protect classified information.The DNI policy further states that “eligibility for covered individuals shall be granted only when facts and circumstances indicate that eligibility is clearly consistent with the national security interests of the United States, and any doubt shall be resolved in favor of national security.”On Thursday, 25-year-old DOGE staff member  resigned after being linked to a deleted social media account that advocated racism and eugenics. Elez resigned after asked the White House about his connection to the account.“Just for the record, I was racist before it was cool,” the account posted in July. “You could not pay me to marry outside of my ethnicity,” the account wrote on X in September. “Normalize Indian hate,” the account wrote the same month, in reference to a post noting the prevalence of people from India in Silicon Valley.Elez’s resignation came a day after the Department of Justice agreed to limit the number of DOGE employees who have access to federal payment systems. The DOJ said access would be limited to two people, Elez and , the CEO of a company called Cloud Software Group.Earlier today, Musk said he planned to rehire Elez after  and  reportedly endorsed the idea. Speaking at The White House today, Trump said he wasn’t concerned about the security of personal information and other data accessed by DOGE, adding that he was “very proud of the job that this group of young people” are doing.A White House official told on Wednesday that Musk and his engineers have appropriate security clearances and are operating in “full compliance with federal law, appropriate security clearances, and as employees of the relevant agencies, not as outside advisors or entities.”reports Trump added that his administration’s cost-cutting efforts would soon turn to the Education Department and the Pentagon, “where he suggested without evidence that there could be ‘trillions’ of dollars in wasted spending within the $6.75 trillion the federal government spent in fiscal year 2024.”GOP leaders in the Republican-controlled House and Senate have largely shrugged about Musk’s ongoing efforts to seize control over federal databases, dismantle agencies mandated by Congress, freeze federal spending on a range of already-appropriated government programs, and threaten workers with layoffs.Meanwhile, multiple parties have sued to stop DOGE’s activities. ABC News says a federal judge was to rule today on whether DOGE should be blocked from accessing Department of Labor records, following a lawsuit alleging Musk’s team sought to illegally access highly sensitive data, including medical information, from the federal government.At least 13 state attorneys general say they plan to file a lawsuit to stop DOGE from accessing federal payment systems containing Americans’ sensitive personal information, reports.Reuters reported Thursday that the U.S. Treasury Department had agreed not to give Musk’s team access to its payment systems while a judge is hearing arguments in a lawsuit by employee unions and retirees alleging Musk illegally searched those records.writes that The Department of Education (DoE) was sued Friday by a California student association demanding an “immediate stop” to DOGE’s “unlawfully” digging through student loan data to potentially dismantle the DoE.]]></content:encoded></item><item><title>TypeScript: the satisfies operator</title><link>https://2ality.com/2025/02/satisfies-operator.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sat, 8 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[TypeScript’s  operator lets us check the type of a value (mostly) without influencing it. In this blog post, we examine how exactly it works and where it’s useful.]]></content:encoded></item><item><title>Seeking Purity</title><link>http://lucumr.pocoo.org/2025/2/8/seeking-purity</link><author>Armin Ronacher</author><category>dev</category><category>blog</category><pubDate>Sat, 8 Feb 2025 00:00:00 +0000</pubDate><source url="https://lucumr.pocoo.org/feed.atom">Armin Ronacher Blog</source><content:encoded><![CDATA[The concept of purity — historically a guiding principle in social and
moral contexts — is also found in passionate, technical discussions.  By
that I mean that purity in technology translates into adherence to a set
of strict principles, whether it be functional programming, test-driven
development, serverless architectures, or, in the case of Rust, memory
safety.Rust positions itself as a champion of memory safety, treating it as a
non-negotiable foundation of good software engineering.  I love Rust: it's
probably my favorite language.  It probably won't surprise you that I have
no problem with it upholding memory safety as a defining feature.Rust aims to achieve the goal of memory safety via safe abstractions, a
compile time borrow checker and a type system that is in service of those
safe abstractions.  It comes as no surprise that the Rust community is
also pretty active in codifying a new way to reason about pointers.  In many ways,
Rust pioneered completely new technical approaches and it it widely
heralded as an amazing innovation.However, as with many movements rooted in purity, what starts as a
technical pursuit can evolve into something more ideological.  Similar to
how moral purity in political and cultural discourse can become charged,
so does the discourse around Rust, which has been dominated by the pursuit
of memory safety.  Particularly within the core Rust community itself,
discussion has moved beyond technical merits into something akin to
ideological warfare.  The fundamental question of “Is this code memory
safe?”, has shifted to “Was it made memory safe in the  way?”.
This distinction matters because it introduces a purity test that values
methodology over outcomes.  Safe C code, for example, is often dismissed
as impossible, not necessarily because it  impossible, but because it
lacks the strict guarantees that Rust's borrow checker enforces.
Similarly, using Rust’s  blocks is increasingly frowned upon,
despite their intended purpose of enabling low-level optimizations when
necessary.This ideological rigidity creates significant friction when Rust
interfaces with other ecosystems (or gets introduced there), particularly
those that do not share its uncompromising stance.  For instance, the role
of Rust in the Linux kernel has been a hot topic.  The Linux kernel
operates under an entirely different set of priorities.  While memory
safety is important there is insufficient support for adopting Rust in
general.  The kernel is an old project and it aims to remain maintainable
for a long time into the future.  For it to even consider a rather young
programming language should be seen as tremendous success for Rust and
also for how open Linus is to the idea.Yet that introduction is balanced against performance, maintainability,
and decades of accumulated engineering expertise.  Many of the kernel
developers, who have found their own strategies to write safe C for
decades, are not accepting the strongly implied premise that their work is
inherently flawed simply because it does not adhere to Rust's strict
purity rules.Tensions rose when a kernel developer advocating for Rust's inclusion took
to social media to push for changes in the Linux kernel development
process.  The public shaming tactic failed, leading the developer to
conclude:
“If shaming on social media does not work, then tell me what does,
because I'm out of ideas.”It's not just the kernel where Rust's memory safety runs up against the
complexities of the real world.  Very similar feelings creep up in the
gaming industry where people love to do wild stuff with pointers.  You do
not need large disagreements to see the purist approach create some
friction.  A recent post of mine for instance
triggered some discussions about the trade-offs between more dependencies,
and moving unsafe to centralized crates.I really appreciate that Rust code does not crash as much.  That part of
Rust, among many others, makes it very enjoyable to work with.  Yet I am
entirely unconvinced that memory safety should trump everything, at least
at this point in time.What people want in the Rust in Linux situation is for the project leader
to come in to declare support for Rust's call for memory safety above all.
To make the detractors go away.Python's Migration LessonHearing this call and discussion brings back memories.  I have lived
through a purity driven shift in a community before.  The move from Python
2 to Python 3 started out very much the same way.  There was an almost
religious movement in the community to move to Python 3 in a ratcheting
motion.  The idea that you could maintain code bases that support both 2
and 3 were initially very
loudly rejected.  I took a lot of flak at the time (and for years after)
for advocating for a more pragmatic migration which burned me out a lot.
That feedback came both in person and online and it largely pushed me away
from Python for a while.  Not getting behind the Python 3 train was seen
as sabotaging the entire project.  However, a decade later, I feel
somewhat vindicated that it was worth being pragmatic about that
migration.At the root of that discourse was a idealistic view of how Unicode could
work in the language and that you can move an entire ecosystem at once.
Both those things greatly clashed with the lived realities in many
projects and companies.I am a happy user of Python 3 today.  This migration has also taught me
the important lesson not be too stuck on a particular idea.  It would have
been very easy to pick one of the two sides of that debate.  Be stuck on
Python 2 (at the risk of forking), or go all in on Python 3 no questions
asked.  It was the path in between that was quite painful to advocate for,
but it was ultimately the right path.  I wrote about my lessons of that
migration a in 2016 and
I think most of this still rings true.  That was motivated by even years
later people still reaching out to me who did not move to Python 3, hoping
for me to embrace their path.  Yet Python 3 has changed!  Python 3 is a
much better language than it was when it first released.  It is a great
language because it's used by people solving real, messy problems and
because it over time found answers for what to do, if you need to have
both Python 2 and 3 code in the wild.  While the world of Python 2 is
largely gone, we are still in a world where Unicode and bytes mix in
certain contexts.Fully committing to a single worldview can be easier because you stop
questioning everything — you can just go with the flow.  Yet truths often
reside on both sides.  Allowing yourself to walk the careful middle path
enables you to learn from multiple perspectives.  You will face doubts and
open yourself up to vulnerability and uncertainty.  The payoff, however,
is the ability to question deeply held beliefs and push into the unknown
territory where new things can be found.  You can arrive at a solution
that isn't a complete rejection of any side.  There is genuine value in
what Rust offers—just as there was real value in what Python 3 set out to
accomplish.  But the Python 3 of today isn't the Python 3 of those early,
ideological debates; it was shaped by a messy, slow, often contentious,
yet ultimately productive transition process.I am absolutely sure that in 30 years from now we are going to primarily
program in memory safe languages (or the machines will do it for us) in
environments where C and C++ prevail.  That glimpse of a future I can
visualize clearly.  The path to there however?  That's a different story
altogether.  It will be hard, it will be impure.  Maybe the solution will
not even involve Rust at all — who knows.We also have to accept that not everyone is ready for change at the same
pace. Forcing adoption when people aren't prepared only causes the
pendulum to swing back hard.  It's tempting to look for a single authority
to declare “the one true way,” but that won't smooth out the inevitable
complications.  Indeed, those messy, incremental challenges are part of how
real progress happens.  In the long run, these hard-won refinements tend
to produce solutions that benefit all sides—if we’re patient enough to let
them take root.  The painful and messy transition is here to stay, and
that's exactly why, in the end, it works.]]></content:encoded></item><item><title>The state of homelab tech (2025) (Friends)</title><link>https://changelog.com/friends/79</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/friends/79/changelog--friends-79.mp3" length="" type=""/><pubDate>Fri, 7 Feb 2025 23:00:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Techno Tim joins Adam to catch up on the state of Homelab for 2025, the state of AI at home and on-prem (AI Homelab) and where that’s heading, building a creator PC, choosing the parts for your build, GPU availability, Windows being user hostile, and why Tim is happy to be using Windows, Mac AND Linux.Changelog++ members save 8 minutes on this episode because they made the ads disappear. Join today!Retool – The low-code platform for developers to build internal tools — Some of the best teams out there trust Retool…Brex, Coinbase, Plaid, Doordash, LegalGenius, Amazon, Allbirds, Peloton, and so many more – the developers at these teams trust Retool as the platform to build their internal tools. Try it free at retool.com/changelogTemporal – Build invincible applications. Manage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail. Register for Replay in London, March 3-5 to break free from the status quo.
DeleteMe – Text  to 64000 to join DeleteMe. They make it quick, easy, and safe to remove your personal data online.
Fly.io – The home of Changelog.com — Deploy your apps close to your users — global Anycast load-balancing, zero-configuration private networking, hardware isolation, and instant WireGuard VPN connections. Push-button deployments that scale to thousands of instances. Check out the speedrun to get started in minutes.
]]></content:encoded></item><item><title>Friday Squid Blogging: The Colossal Squid</title><link>https://www.schneier.com/blog/archives/2025/02/friday-squid-blogging-the-colossal-squid.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 7 Feb 2025 22:02:37 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The UK&apos;s Demands for Apple to Break Encryption Is an Emergency for Us All</title><link>https://www.eff.org/deeplinks/2025/02/uks-demands-apple-break-encryption-emergency-us-all</link><author>Thorin Klosowski</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/OG-Encryption-DefendEncryption.png" length="" type=""/><pubDate>Fri, 7 Feb 2025 20:44:03 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Governing the ML lifecycle at scale, Part 4: Scaling MLOps with security and governance controls</title><link>https://aws.amazon.com/blogs/machine-learning/governing-the-ml-lifecycle-at-scale-part-4-scaling-mlops-with-security-and-governance-controls/</link><author>Jia (Vivian) Li</author><category>dev</category><category>ai</category><pubDate>Fri, 7 Feb 2025 20:25:13 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Data science teams often face challenges when transitioning models from the development environment to production. These include difficulties integrating data science team’s models into the IT team’s production environment, the need to retrofit data science code to meet enterprise security and governance standards, gaining access to production grade data, and maintaining repeatability and reproducibility in machine learning (ML) pipelines, which can be difficult without a proper platform infrastructure and standardized templates.This post, part of the “Governing the ML lifecycle at scale” series (Part 1, Part 2, Part 3), explains how to set up and govern a multi-account ML platform that addresses these challenges. The platform provides self-service provisioning of secure environments for ML teams, accelerated model development with predefined templates, a centralized model registry for collaboration and reuse, and standardized model approval and deployment processes.An enterprise might have the following roles involved in the ML lifecycles. The functions for each role can vary from company to company. In this post, we assign the functions in terms of the ML lifecycle to each role as follows: – Provision accounts for ML development teams, govern access to the accounts and resources, and promote standardized model development and approval process to eliminate repeated engineering effort. Usually, there is one lead data scientist for a data science group in a business unit, such as marketing. – Perform data analysis, model development, model evaluation, and registering the models in a model registry. – Develop model deployment pipelines and control the model deployment processes. – Review the model’s performance including documentation, accuracy, bias and access, and provide final approval for models to be deployed. – Define a standardized process for creating development accounts that conform to the company’s security, monitoring, and governance standards; create templates for model development; and manage the infrastructure and mechanisms for sharing model artifacts.This ML platform provides several key benefits. First, it enables every step in the ML lifecycle to conform to the organization’s security, monitoring, and governance standards, reducing overall risk. Second, the platform gives data science teams the autonomy to create accounts, provision ML resources and access ML resources as needed, reducing resource constraints that often hinder their work.Additionally, the platform automates many of the repetitive manual steps in the ML lifecycle, allowing data scientists to focus their time and efforts on building ML models and discovering insights from the data rather than managing infrastructure. The centralized model registry also promotes collaboration across teams, enables centralized model governance, increasing visibility into models developed throughout the organization and reducing duplicated work.Finally, the platform standardizes the process for business stakeholders to review and consume models, smoothing the collaboration between the data science and business teams. This makes sure models can be quickly tested, approved, and deployed to production to deliver value to the organization.Overall, this holistic approach to governing the ML lifecycle at scale provides significant benefits in terms of security, agility, efficiency, and cross-functional alignment.In the next section, we provide an overview of the multi-account ML platform and how the different roles collaborate to scale MLOps.The following architecture diagram illustrates the solutions for a multi-account ML platform and how different personas collaborate within this platform.There are five accounts illustrated in the diagram:ML Shared Services Account – This is the central hub of the platform. This account manages templates for setting up new ML Dev Accounts, as well as SageMaker Projects templates for model development and deployment, in AWS Service Catalog. It also hosts a model registry to store ML models developed by data science teams, and provides a single location to approve models for deployment. – This is where data scientists perform their work. In this account, data scientists can create new SageMaker notebooks based on the needs, connect to data sources such as Amazon Simple Storage Service (Amazon S3) buckets, analyze data, build models and create model artifacts (for example, a container image), and more. The SageMaker projects, provisioned using the templates in the ML Shared Services Account, can speed up the model development process because it has steps (such as connecting to an S3 bucket) configured. The diagram shows one ML Dev Account, but there can be multiple ML Dev Accounts in an organization. – This is the test environment for new ML models, where stakeholders can review and approve models before deployment to production. – This is the production account for new ML models. After the stakeholders approve the models in the ML Test Account, the models are automatically deployed to this production account. – This account hosts data governance services for data lake, central feature store, and fine-grained data access.Key activities and actions are numbered in the preceding diagram. Some of these activities are performed by various personas, whereas others are automatically triggered by AWS services.ML engineers create the pipelines in Github repositories, and the platform engineer converts them into two different Service Catalog portfolios: ML Admin Portfolio and SageMaker Project Portfolio. The ML Admin Portfolio will be used by the lead data scientist to create AWS resources (for example, SageMaker domains). The SageMaker Project Portfolio has SageMaker projects that data scientists and ML engineers can use to accelerate model training and deployment.The platform engineer shares the two Service Catalog portfolios with workload accounts in the organization.Data engineer prepares and governs datasets using services such as Amazon S3, AWS Lake Formation, and Amazon DataZone for ML.The lead data scientist uses the ML Admin Portfolio to set up SageMaker domains and the SageMaker Project Portfolio to set up SageMaker projects for their teams.Data scientists subscribe to datasets, and use SageMaker notebooks to analyze data and develop models.Data scientists use the SageMaker projects to build model training pipelines. These SageMaker projects automatically register the models in the model registry.The lead data scientist approves the model locally in the ML Dev Account.This step consists of the following sub-steps: 
   After the data scientists approve the model, it triggers an event bus in Amazon EventBridge that ships the event to the ML Shared Services Account.The event in EventBridge triggers the AWS Lambda function that copies model artifacts (managed by SageMaker, or Docker images) from the ML Dev Account into the ML Shared Services Account, creates a model package in the ML Shared Services Account, and registers the new model in the model registry in the ML Shared Services account.ML engineers review and approve the new model in the ML Shared Services account for testing and deployment. This action triggers a pipeline that was set up using a SageMaker project.The approved models are first deployed to the ML Test Account. Integration tests will be run and endpoint validated before being approved for production deployment.After testing, the governance officer approves the new model in the CodePipeline.After the model is approved, the pipeline will continue to deploy the new model into the ML Prod Account, and creates a SageMaker endpoint.The following sections provide details on the key components of this diagram, how to set them up, and sample code.Set up the ML Shared Services AccountThe ML Shared Services Account helps the organization standardize management of artifacts and resources across data science teams. This standardization also helps enforce controls across resources consumed by data science teams.The ML Shared Services Account has the following features:Service Catalog portfolios – This includes the following portfolios: – This is intended to be used by the project admins of the workload accounts. It is used to create AWS resources for their teams. These resources can include SageMaker domains, Amazon Redshift clusters, and more.SageMaker Projects Portfolio – This portfolio contains the SageMaker products to be used by the ML teams to accelerate their ML models’ development while complying with the organization’s best practices. – This is the centralized place for ML models developed and approved by different teams. For details on setting this up, refer to Part 2 of this series.The following diagram illustrates this architecture.As the first step, the cloud admin sets up the ML Shared Services Account by using one of the blueprints for customizations in AWS Control Tower account vending, as described in Part 1.In the following sections, we walk through how to set up the ML Admin Portfolio. The same steps can be used to set up the SageMaker Projects Portfolio.Bootstrap the infrastructure for two portfoliosAfter the ML Shared Services Account has been set up, the ML platform admin can bootstrap the infrastructure for the ML Admin Portfolio using sample code in the GitHub repository. The code contains AWS CloudFormation templates that can be later deployed to create the SageMaker Projects Portfolio.Complete the following steps:Clone the GitHub repo to a local directory:git clone https://github.com/aws-samples/data-and-ml-governance-workshop.gitChange the directory to the portfolio directory:cd data-and-ml-governance-workshop/module-3/ml-admin-portfolioInstall dependencies in a separate Python environment using your preferred Python packages manager:python3 -m venv env
source env/bin/activate pip 
install -r requirements.txtBootstrap your deployment target account using the following command:cdk bootstrap aws://<target account id>/<target region> --profile <target account profile>If you already have a role and AWS Region from the account set up, you can use the following command instead:Lastly, deploy the stack:cdk deploy --all --require-approval neverWhen it’s ready, you can see the MLAdminServicesCatalogPipeline pipeline in AWS CloudFormation.Navigate to AWS CodeStar Connections of the Service Catalog page, you can see there’s a connection named “codeconnection-service-catalog”. If you click the connection, you will notice that we need to connect it to GitHub to allow you to integrate it with your pipelines and start pushing code. Click the ‘Update pending connection’ to integrate with your GitHub account.Once that is done, you need to create empty GitHub repositories to start pushing code to. For example, you can create a repository called “ml-admin-portfolio-repo”. Every project you deploy will need a repository created in GitHub beforehand.Trigger CodePipeline to deploy the ML Admin PortfolioComplete the following steps to trigger the pipeline to deploy the ML Admin Portfolio. We recommend creating a separate folder for the different repositories that will be created in the platform.Get out of the cloned repository and create a parallel folder called :cd ../../.. # (as many .. as directories you have moved in)
mkdir platform-repositoriesClone and fill the empty created repository:cd platform-repositories
git clone https://github.com/example-org/ml-admin-service-catalog-repo.git
cd ml-admin-service-catalog-repo
cp -aR ../../ml-platform-shared-services/module-3/ml-admin-portfolio/. .Push the code to the Github repository to create the Service Catalog portfolio:git add .
git commit -m "Initial commit"
git push -u origin mainAfter it is pushed, the Github repository we created earlier is no longer empty. The new code push triggers the pipeline named cdk-service-catalog-pipeline to build and deploy artifacts to Service Catalog.It takes about 10 minutes for the pipeline to finish running. When it’s complete, you can find a portfolio named ML Admin Portfolio on the Portfolios page on the Service Catalog console.Repeat the same steps to set up the SageMaker Projects Portfolio, make sure you’re using the sample code (sagemaker-projects-portfolio) and create a new code repository (with a name such as sm-projects-service-catalog-repo).Share the portfolios with workload accountsYou can share the portfolios with workload accounts in Service Catalog. Again, we use ML Admin Portfolio as an example.Confirm available portfolios in workload accountsIf the sharing was successful, you should see both portfolios available on the Service Catalog console, on the  page under .Now that the service catalogs in the ML Shared Services Account have been shared with the workloads OU, the data science team can provision resources such as SageMaker domains using the templates and set up SageMaker projects to accelerate ML models’ development while complying with the organization’s best practices.We demonstrated how to create and share portfolios with workload accounts. However, the journey doesn’t stop here. The ML engineer can continue to evolve existing products and develop new ones based on the organization’s requirements.The following sections describe the processes involved in setting up ML Development Accounts and running ML experiments.Set up the ML Development AccountThe ML Development account setup consists of the following tasks and stakeholders:The team lead requests the cloud admin to provision the ML Development Account.The cloud admin provisions the account.The team lead uses shared Service Catalog portfolios to provisions SageMaker domains, set up IAM roles and give access, and get access to data in Amazon S3, or Amazon DataZone or AWS Lake Formation, or a central feature group, depending on which solution the organization decides to use.Part 3 in this series described multiple ways to share data across the organization. The current architecture allows data access using the following methods:Option 1: Train a model using Amazon DataZone – If the organization has Amazon DataZone in the central governance account or data hub, a data publisher can create an Amazon DataZone project to publish the data. Then the data scientist can subscribe to the Amazon DataZone published datasets from Amazon SageMaker Studio, and use the dataset to build an ML model. Refer to the sample code for details on how to use subscribed data to train an ML model.Option 2: Train a model using Amazon S3 – Make sure the user has access to the dataset in the S3 bucket. Follow the sample code to run an ML experiment pipeline using data stored in an S3 bucket.Option 3: Train a model using a data lake with Athena – Part 2 introduced how to set up a data lake. Follow the sample code to run an ML experiment pipeline using data stored in a data lake with Amazon Athena.Option 4: Train a model using a central feature group – Part 2 introduced how to set up a central feature group. Follow the sample code to run an ML experiment pipeline using data stored in a central feature group.You can choose which option to use depending on your setup. For options 2, 3, and 4, the SageMaker Projects Portfolio provides project templates to run ML experiment pipelines, steps including data ingestion, model training, and registering the model in the model registry.In the following example, we use option 2 to demonstrate how to build and run an ML pipeline using a SageMaker project that was shared from the ML Shared Services Account.It takes a few minutes to create the project.After the project is created, a SageMaker pipeline is triggered to perform the steps specified in the SageMaker project. Choose  in the navigation pane to see the pipeline.You can choose the pipeline to see the Directed Acyclic Graph (DAG) of the pipeline. When you choose a step, its details show in the right pane.The last step of the pipeline is registering the model in the current account’s model registry. As the next step, the lead data scientist will review the models in the model registry, and decide if a model should be approved to be promoted to the ML Shared Services Account.The lead data scientist should review the trained ML models and approve the candidate model in the model registry of the development account. After an ML model is approved, it triggers a local event, and the event buses in EventBridge will send model approval events to the ML Shared Services Account, and the artifacts of the models will be copied to the central model registry. A model card will be created for the model if it’s a new one, or the existing model card will update the version.The following architecture diagram shows the flow of model approval and model promotion.After the previous step, the model is available in the central model registry in the ML Shared Services Account. ML engineers can now deploy the model.If you had used the sample code to bootstrap the SageMaker Projects portfolio, you can use the Deploy real-time endpoint from ModelRegistry – Cross account, test and prod option in SageMaker Projects to set up a project to set up a pipeline to deploy the model to the target test account and production account.The pipeline for deployment is ready. The ML engineer will review the newly promoted model in the ML Shared Services Account. If the ML engineer approves model, it will trigger the deployment pipeline. You can see the pipeline on the CodePipeline console.The pipeline will first deploy the model to the test account, and then pause for manual approval to deploy to the production account. ML engineer can test the performance and Governance officer can validate the model results in the test account. If the results are satisfactory, Governance officer can approve in CodePipeline to deploy the model to production account.This post provided detailed steps for setting up the key components of a multi-account ML platform. This includes configuring the ML Shared Services Account, which manages the central templates, model registry, and deployment pipelines; sharing the ML Admin and SageMaker Projects Portfolios from the central Service Catalog; and setting up the individual ML Development Accounts where data scientists can build and train models.The post also covered the process of running ML experiments using the SageMaker Projects templates, as well as the model approval and deployment workflows. Data scientists can use the standardized templates to speed up their model development, and ML engineers and stakeholders can review, test, and approve the new models before promoting them to production.This multi-account ML platform design follows a federated model, with a centralized ML Shared Services Account providing governance and reusable components, and a set of development accounts managed by individual lines of business. This approach gives data science teams the autonomy they need to innovate, while providing enterprise-wide security, governance, and collaboration. is a Senior Solutions Architect in AWS, with specialization in AI/ML. She currently supports customers in financial industry. Prior to joining AWS in 2022, she had 7 years of experience supporting enterprise customers use AI/ML in the cloud to drive business results. Vivian has a BS from Peking University and a PhD from University of Southern California. In her spare time, she enjoys all the water activities, and hiking in the beautiful mountains in her home state, Colorado. is a Principal ML Solutions Architect at AWS. He has over 3 decades of experience architecting and building distributed, hybrid, and cloud applications. He is passionate about building secure, scalable, reliable AI/ML and big data solutions to help enterprise customers with their cloud adoption and optimization journey to improve their business outcomes. In his spare time, he enjoys riding motorcycle and walking with his dogs. is a GenAI Evaluation Specialist and Solutions Architect at AWS. He assists customers across industries and regions in operationalizing and governing their generative AI systems at scale, ensuring they meet the highest standards of performance, safety, and ethical considerations. Bringing a unique perspective to the field of AI, Alessandro has a background in quantum physics and research experience in quantum communications and quantum memories. In his spare time, he pursues his passion for landscape and underwater photography. is a DevOps Consultant in Professional Services at AWS. He helps accelerate customers’ journeys to the cloud and achieve their digital transformation goals. In his free time, he enjoys playing sports, especially basketball and padel, spending time with family and friends, and learning about technology. is an AI/ML and Generative AI senior solution architect with AWS. He has extensive experience designing end-to-end machine learning and business analytics solutions in finance, operations, marketing, healthcare, supply chain management, and IoT. He has double masters degrees from the University of South Florida, University of Fribourg, Switzerland, and a bachelors degree from the Indian Institute of Technology, Kharagpur. Outside of work, Sovik enjoys traveling, taking ferry rides, and watching movies. is a Senior Machine Learning Engineer within AWS Professional Services, leading teams to build advanced machine learning solutions in the cloud. He’s passionate about making AI impactful, overseeing the entire process from modeling to production. In his spare time, he enjoys surfing, cycling, and traveling.]]></content:encoded></item><item><title>EFF to Ninth Circuit: Young People Have a First Amendment Right to Use Social Media (and All of Its Features)</title><link>https://www.eff.org/deeplinks/2025/02/eff-ninth-circuit-young-people-have-first-amendment-right-use-social-media-and-all</link><author>Emma Leeds Armstrong</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/facebook-thumbs_0.png" length="" type=""/><pubDate>Fri, 7 Feb 2025 20:20:51 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>UK demands backdoor for encrypted Apple user data...</title><link>https://www.youtube.com/watch?v=ozkg_iW9mNU</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/ozkg_iW9mNU?version=3" length="" type=""/><pubDate>Fri, 7 Feb 2025 20:20:21 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Try Brilliant free for 30 days https://brilliant.org/fireship You’ll also get 20% off an annual premium subscription.

The United Kingdom is demanding Apple build a backdoor to access encrypted iCloud user data. Learn how end-to-end-encryption works and other tools that protect your digital privacy. 

#apple #tech #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Mainstream Source https://www.washingtonpost.com/technology/2025/02/07/apple-encryption-backdoor-uk/
Full Cryptography Tutorial https://youtu.be/NuyzuNBFWxQ
Apple Intelligence gone Wild https://youtu.be/7rXgVsIGvGQ
Tails OS in 100 Seconds https://youtu.be/mVKAyw0xqxw

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Why does the UK want Apple iCloud data?
- What is a double rachet algorithm?
- How does signal e2ee protocol work?
- How do I keep browsing data private?
- Best Linux distro for privacy]]></content:encoded></item><item><title>The Story Behind Bike Index</title><link>https://www.youtube.com/watch?v=W6-3s4OH8Gs</link><author>Jack Rhysider</author><category>Darknet diaries</category><category>infosec</category><category>video</category><category>podcast</category><enclosure url="https://www.youtube.com/v/W6-3s4OH8Gs?version=3" length="" type=""/><pubDate>Fri, 7 Feb 2025 19:30:03 +0000</pubDate><source url="https://www.youtube.com/channel/UCMIqrmh2lMdzhlCPK5ahsAg">Jack Rhysider</source><content:encoded><![CDATA[The Story Behind Bike Index

#podcast #bike #bicycle 

Listen to the full episode 🎙 Darknet Diaries Ep. 153: Bike Index]]></content:encoded></item><item><title>New IEEE Standard for Securing Biomedical Devices and Data</title><link>https://spectrum.ieee.org/ieee-standard-biomedical-devices-data</link><author>Kathy Pretz</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjI2ODI3Mi9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc0NDQzMjI5NX0._48EBGJrJMAj3jbHZn_JZFlc4gg3q5Kfk6SYQwDK1xY/image.jpg?width=600" length="" type=""/><pubDate>Fri, 7 Feb 2025 19:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[It covers electronic health records, devices, and hospital systems]]></content:encoded></item><item><title>The case for sans-io</title><link>https://fasterthanli.me/articles/the-case-for-sans-io</link><author>Amos Wenger</author><category>Faster than time blog</category><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 7 Feb 2025 18:53:01 +0000</pubDate><source url="https://fasterthanli.me/index.xml">fasterthanli.me</source><content:encoded><![CDATA[The most popular option to decompress ZIP files from the Rust programming
language is a crate simply named zip — At the time of this writing, it has 48
million downloads. It’s fully-featured, supporting various compression methods,
encryption, and even supports writing zip files.However, that’s not the crate  uses to read ZIP files. Some
applications benefit from using asynchronous I/O, especially if they decompress
archives that they download from the network.]]></content:encoded></item><item><title>Accelerate your Amazon Q implementation: starter kits for SMBs</title><link>https://aws.amazon.com/blogs/machine-learning/accelerate-your-amazon-q-implementation-starter-kits-for-smbs/</link><author>Nneoma Okoroafor</author><category>dev</category><category>ai</category><pubDate>Fri, 7 Feb 2025 17:29:14 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Whether you’re a small or medium-sized business (SMB) or a managed service provider at the beginning of your cloud journey, you might be wondering how to get started. Questions like “Am I following best practices?”, “Am I optimizing my cloud costs?”, and “How difficult is the learning curve?” are quite common. AWS is here to provide a concept called starter kits.Starter kits are complete, deployable solutions that address common, repeatable business problems. They deploy the services that make up a solution according to best practices, helping you optimize costs and become familiar with these kinds of architectural patterns without a large investment in training. Most of all, starter kits save you time—time that can be better spent on your business or with your customers.In this post, we showcase a starter kit for Amazon Q Business. If you have a repository of documents that you need to turn into a knowledge base quickly, or simply want to test out the capabilities of Amazon Q Business without a large investment of time at the console, then this solution is for you.Amazon Q Business is a generative AI-powered assistant that can answer questions, provide summaries, generate content, and securely complete tasks based on data and information in your enterprise systems. It empowers employees to be more creative, data-driven, efficient, prepared, and productive.The following diagram illustrates the solution architecture.The workflow involves the following steps:Upon successful authentication, the user can access the Amazon Q web UI and ask a question.Amazon Q retrieves relevant information from its index, which is populated using data from the connected data sources (Amazon S3 and a web crawler).Amazon Q then generates a response using its internal large language model (LLM) and presents it to the user through the Amazon Q web UI.The user can provide feedback on the response through the Amazon Q web UI.Before deploying the solution, make sure you have the following in place:– You will need an active AWS account with the necessary permissions to deploy CloudFormation stacks and create the required resources. – Make sure you have an existing S3 bucket that will be used as the data source for Amazon Q. To create a S3 bucket, refer to Create your first S3 bucket. – Configure AWS IAM Identity Center in your AWS environment. You will need to provide the necessary details, such as the IAM Identity Center instance Amazon Resource Name (ARN), during the deployment process.Deploy the solution using AWS CloudFormationComplete the following steps to deploy the CloudFormation template:On the  page, leave everything as default, select I acknowledge that AWS CloudFormation might create IAM resources and and choose .To validate the Amazon Q solution is functioning as expected, perform the following tests:Test data ingestion: 
  Upload a test file to the S3 bucket.Verify that the file is successfully ingested and processed by Amazon Q.Check the Amazon Q web experience UI for the processed data.Test web crawler functionality:Verify that the web crawler is able to retrieve and ingest the data from the website.Make sure the data is displayed correctly in the Amazon Q web experience UI.To clean up, delete the CloudFormation stack and the S3 bucket you created.The Amazon Q starter kit provides a streamlined solution for SMBs to use the power of generative AI and intelligent question-answering. By automating the deployment and integration with key data sources, this kit eases the complexity of setting up Amazon Q, empowering businesses to quickly unlock insights and improve productivity.If your SMB has a repository of documents that need to be transformed into a valuable knowledge base, or you simply want to explore the capabilities of Amazon Q, we encourage you to take advantage of this starter kit. Get started today and experience the transformative benefits of enterprise-grade question-answering tailored for your business needs, and let us know what you think in the comments. To explore more generative AI use cases, refer to AI Use Case Explorer. is a Partner Solutions Architect focused on AI/ML and generative AI. Nneoma is passionate about providing guidance to AWS Partners on using the latest technologies and techniques to deliver innovative solutions to customers. is a Partner Solutions Architect with Amazon Web Services. He primarily serves consulting partners, providing architectural guidance and recommendations for new and existing workloads. Outside of work, he enjoys playing soccer, golf, and spending time with family and friends. is a Partner Solutions Architect focused on helping AWS Distribution Partners and their Seller Partners build and grow their AWS practices. Jason is passionate about building solutions for MSPs and VARs in the small business space. Outside the office, Jason is an avid traveler and enjoys offshore fishing.]]></content:encoded></item><item><title>Building the future of construction analytics: CONXAI’s AI inference on Amazon EKS</title><link>https://aws.amazon.com/blogs/machine-learning/building-the-future-of-construction-analytics-conxais-ai-inference-on-amazon-eks/</link><author>Tim Krause</author><category>dev</category><category>ai</category><pubDate>Fri, 7 Feb 2025 17:21:34 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[This is a guest post co-written with Tim Krause, Lead MLOps Architect at CONXAI.CONXAI Technology GmbH is pioneering the development of an advanced AI platform for the Architecture, Engineering, and Construction (AEC) industry. Our platform uses advanced AI to empower construction domain experts to create complex use cases efficiently.Construction sites typically employ multiple CCTV cameras, generating vast amounts of visual data. These camera feeds can be analyzed using AI to extract valuable insights. However, to comply with GDPR regulations, all individuals captured in the footage must be anonymized by masking or blurring their identities.Our AI solution is offered in two forms:Model as a service (MaaS) – Our AI model is accessible through an API, enabling seamless integration. Pricing is based on processing batches of 1,000 images, offering flexibility and scalability for users.Software as a service (SaaS) – This option provides a user-friendly dashboard, acting as a central control panel. Users can add and manage new cameras, view footage, perform analytical searches, and enforce GDPR compliance with automatic person anonymization.Our AI model, fine-tuned with a proprietary dataset of over 50,000 self-labeled images from construction sites, achieves significantly greater accuracy compared to other MaaS solutions. With the ability to recognize more than 40 specialized object classes—such as cranes, excavators, and portable toilets—our AI solution is uniquely designed and optimized for the construction industry.Initially, CONXAI started with a small cloud provider specializing in offering affordable GPUs. However, it lacked essential services required for machine learning (ML) applications, such as frontend and backend infrastructure, DNS, load balancers, scaling, blob storage, and managed databases. At that time, the application was deployed as a single monolithic container, which included Kafka and a database. This setup was neither scalable nor maintainable.After migrating to AWS, we gained access to a robust ecosystem of services. Initially, we deployed the all-in-one AI container on a single Amazon Elastic Compute Cloud (Amazon EC2) instance. Although this provided a basic solution, it wasn’t scalable, necessitating the development of a new architecture.Our top reasons for choosing AWS were primarily driven by the team’s extensive experience with AWS. Additionally, the initial cloud credits provided by AWS were invaluable for us as a startup. We now use AWS managed services wherever possible, particularly for data-related tasks, to minimize maintenance overhead and pay only for the resources we actually use.At the same time, we aimed to remain cloud-agnostic. To achieve this, we chose Kubernetes, enabling us to deploy our stack directly on a customer’s edge—such as on construction sites—when needed. Some customers are potentially very compliance-restrictive, not allowing data to leave the construction site. Another opportunity is federated learning, training on the customer’s edge and only transferring model weights, without sensitive data, into the cloud. In the future, this approach might lead to having one model fine-tuned for each camera to achieve the best accuracy, which requires hardware resources on-site. For the time being, we use Amazon EKS to offload the management overhead to AWS, but we could easily deploy on a standard Kubernetes cluster if needed.Our previous model was running on TorchServe. With our new model, we first tried performing inference in Python with Flask and PyTorch, as well as with BentoML. Achieving high inference throughput with high GPU utilization for cost-efficiency was very challenging. Exporting the model to ONNX format was particularly difficult because the OneFormer model lacks strong community support. It took us some time to identify why the OneFormer model was so slow in the ONNX Runtime with NVIDIA Triton. We ultimately resolved the issue by converting ONNX to TensorRT.Defining the final architecture, training the model, and optimizing costs took approximately 2–3 months. Currently, we improve our model by incorporating increasingly accurate labeled data, a process that takes around 3–4 weeks of training on a single GPU. Deployment is fully automated with GitLab CI/CD pipelines, Terraform, and Helm, requiring less than an hour to complete without any downtime. New model versions are typically rolled out in shadow mode for 1–2 weeks to provide stability and accuracy before full deployment.The following diagram illustrates the solution architecture.The architecture consists of the following key components:The S3 bucket (1) is the most important data source. It is cost-effective, scalable, and provides almost unlimited blob storage. We encrypt the S3 bucket, and we delete all data with privacy concerns after processing took place. Almost all microservices read and write files from and to Amazon S3, which ultimately triggers (2) Amazon EventBridge (3). The process begins when a customer uploads an image on Amazon S3 using a presigned URL provided by our API handling user authentication and authorization through Amazon Cognito.The S3 bucket is configured in such a way that it forwards (2) all events into EventBridge.TriggerMesh is a Kubernetes controller where we use  (6). It abstracts the infrastructure automation and automatically creates an Amazon Simple Queue Service (Amazon SQS) (5) processing queue, which acts as a processing buffer. Additionally, it creates an EventBridge rule (4) to forward the S3 event from the event bus into the SQS processing queue. Finally, TriggerMesh creates a Kubernetes Pod to poll events from the processing queue to feed it into the Knative broker (7). The resources in the Kubernetes cluster are deployed in a private subnet.The Knative trigger (9) polls the Knative broker based on a specific  and forwards it accordingly to the KServe  (10).KServe is a standard model inference platform on Kubernetes that uses Knative Serving as its foundation and is fully compatible with Knative Eventing. It also pulls models from a model repository into the container before the model server starts, eliminating the need to build a new container image for each model version.We use KServe’s “Collocate transformer and predictor in same pod” feature to maximize inference speed and throughput because containers within the same pod can communicate over localhost and the network traffic never leaves the CPU.After many performance tests, we achieved best performance with the NVIDIA Triton Inference Server (11) after converting our model first into ONNX and then into TensorRT.Our transformer (12) uses Flask with Gunicorn and is optimized for the number of workers and CPU cores to maintain GPU utilization over 90%. The transformer gets a  with the reference of the image Amazon S3 path, downloads it, and performs model inference over HTTP. After getting back the model results, it performs preprocessing and finally uploads the processed model results back to Amazon S3.We use Karpenter as the cluster auto scaler. Karpenter is responsible for scaling the inference component to handle high user request loads. Karpenter launches new EC2 instances when the system experiences increased demand. This allows the system to automatically scale up computing resources to meet the increased workload.All this divides our architecture mainly in AWS managed data service and the Kubernetes cluster:The S3 bucket, EventBridge, and SQS queue as well as Amazon MSK are all fully managed services on AWS. This keeps our data management effort low.We use Amazon EKS for everything else. TriggerMesh, , Knative Broker, Knative Trigger, KServe with our Python transformer, and the Triton Inference Server are also within the same EKS cluster on a dedicated EC2 instance with a GPU. Because our EKS cluster is just used for processing, it is fully stateless.From initially having our own highly customized model, transitioning to AWS, improving our architecture, and introducing our new Oneformer model, CONXAI is now proud to provide scalable, reliable, and secure ML inference to customers, enabling construction site improvements and accelerations. We achieved a GPU utilization of over 90%, and the number of processing errors has dropped almost to zero in recent months. One of the major design choices was the separation of the model from the preprocessing and postprocessing code in the transformer. With this technology stack, we gained the ability to scale down to zero on Kubernetes using the Knative serverless feature, while our scale-up time from a cold state is just 5–10 minutes, which can save significant infrastructure costs for potential batch inference use cases.The next important step is to use these model results with proper analytics and data science. These results can also serve as a data source for generative AI features such as automated report generation. Furthermore, we want to label more diverse images and train the model on additional construction domain classes as part of a continuous improvement process. We also work closely with AWS specialists to bring our model in AWS Inferentia chipsets for better cost-efficiency.To learn more about the services in this solution, refer to the following resources: is Lead MLOps Architect at CONXAI. He takes care of all activities when AI meets infrastructure. He joined the company with previous Platform, Kubernetes, DevOps, and Big Data knowledge and was training LLMs from scratch. is a Solutions Architect at AWS, working with startup customers, and leveraging his expertise to help startup customers design their workloads on AWS.]]></content:encoded></item><item><title>How Untold Studios empowers artists with an AI assistant built on Amazon Bedrock</title><link>https://aws.amazon.com/blogs/machine-learning/how-untold-studios-empowers-artists-with-an-ai-assistant-built-on-amazon-bedrock/</link><author>Olivier Vigneresse</author><category>dev</category><category>ai</category><pubDate>Fri, 7 Feb 2025 17:06:09 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[Untold Studios is a tech-driven, leading creative studio specializing in high-end visual effects and animation. Our commitment to innovation led us to a pivotal challenge: how to harness the power of machine learning (ML) to further enhance our competitive edge while balancing this technological advancement with strict data security requirements and the need to streamline access to our existing internal resources.To give our artists access to technology, we need to create good user interfaces. This is a challenge, especially if the pool of end-users are diverse in terms of their needs and technological experience. We saw an opportunity to use large language models (LLMs) to create a natural language interface, which makes this challenge easier and takes care of a lot of the heavy lifting.This post details how we used Amazon Bedrock to create an AI assistant (Untold Assistant), providing artists with a straightforward way to access our internal resources through a natural language interface integrated directly into their existing Slack workflow.The Untold Assistant serves as a central hub for artists. Besides the common AI functionalities like text and image generation, it allows them to interact with internal data, tools, and workflows through natural language queries.For the UI, we use Slack’s built-in features rather than building custom frontends. Slack already provides applications for workstations and phones, message threads for complex queries, emoji reactions for feedback, and file sharing capabilities. The implementation uses Slack’s event subscription API to process incoming messages and Slack’s Web API to send responses. Users interact with the Untold Assistant through private direct messages or by mentioning it (@-style tagging) in channels for everybody to see. Because our teams already use Slack throughout the day, this eliminates context switching and the need to adopt new software. Every new message is acknowledged by a gear emoji for immediate feedback, which eventually changes to a check mark if the query was successful or an X if an error occurred. The following screenshot shows an example.With the use of Anthropic’s Claude 3.5 Sonnet model on Amazon Bedrock, the system processes complex requests and generates contextually relevant responses. The serverless architecture provides scalability and responsiveness, and secure storage houses the studio’s vast asset library and knowledge base. Key AWS services used include:The following diagram illustrates the solution architecture.The main components for this application are the Slack integration, the Amazon Bedrock integration, the Retrieval Augmented Generation (RAG) implementation, user management, and logging.We use a two-function approach to meet Slack’s 3-second acknowledgment requirement. The incoming event from Slack is sent to an endpoint in API Gateway, and Slack expects a response in less than 3 seconds, otherwise the request fails. The first Lambda function, with reserved capacity, quickly acknowledges the event and forwards the request to the second function, where it can be handled without time restrictions. The setup handles time-sensitive responses while allowing for thorough request processing. We call the second function directly from the first function without using an event with Amazon Simple Notification Service (Amazon SNS) or a queue with Amazon Simple Queue Service (Amazon SQS) in between to keep the latency as low as possible.Amazon Bedrock integrationOur Untold Assistant uses Amazon Bedrock with Anthropic’s Claude 3.5 Sonnet model for natural language processing. We use the model’s function calling capabilities, enabling the application to trigger specific tools or actions as needed. This allows the assistant to handle both general queries and complex specialized queries or run tasks across our internal systems.Our RAG setup uses Amazon Bedrock connectors to integrate with Confluence and Salesforce, tapping into our existing knowledge bases. For other data sources without a pre-built connector available, we export content to Amazon S3 and use the Amazon S3 connector. For example, we export pre-chunked asset metadata from our asset library to Amazon S3, letting Amazon Bedrock handle embeddings, vector storage, and search. This approach significantly decreased development time and complexity, allowing us to focus on improving user experience.We map Slack user IDs to our internal user pool, currently in DynamoDB (but designed to work with Amazon Cognito). This system tailors the assistant’s capabilities to each user’s role and clearance level, making sure that it operates within the bounds of each user’s authority while maintaining functionality. The access to data sources is controlled using tools. Every tool encapsulates a data source and the LLM’s access to tools is restricted by the user and their role.Additionally, if a user tells the assistant something that should be remembered, we store this piece of information in a database and add it to the context every time the user initiates a request. This could be, for example, “Keep all your replies as short as possible” or “If I ask for code it’s always Python.”We use the built-in integration with Amazon CloudWatch in Lambda to track system performance and error states. For monitoring critical errors, we’ve set up direct notifications to a dedicated Slack channel, allowing for immediate awareness and response. Every query and tool invocation is logged to DynamoDB, providing a rich dataset that we use to analyze usage patterns and optimize the system’s performance and functionality.Function calling with Amazon BedrockLike Anthropic’s Claude, most modern LLMs support function calling, which allows us to extend the capabilities of LLMs beyond merely generating text. We can provide a set of function specifications with a description of what the function is going to do and the names and descriptions of the function’s parameters. Based on this information, the LLM decides if an incoming request can be solved directly or if the best next step to solve the query would be a function call. If that’s the case, the model returns the name of the function to call, as well as the parameters and values. It’s then up to us to run the function and initiate the next step. Agents use this system in a loop to call functions and process their output until a success criterion is reached. In our case, we only implement a single pass function call to keep things simple and robust. However, in certain cases, the function itself uses the LLM to process data and format it nicely for the end-user.Function calling is a very useful feature that helps us convert unstructured user input into structured automatable instructions. We anticipate that over the next couple of months, we will add many more functions to extend the AI assistant’s capabilities and increase its usefulness. Although frameworks like LangChain offer comprehensive solutions for implementing function calling systems, we opted for a lightweight, custom approach tailored to our specific needs. This decision allowed us to maintain a smaller footprint and focus on the essential features for our use case.The following is a code example of using the AiTool base class for extendability.All that’s required to add a new function is creating a class like the one in our example. The class will automatically be discovered and the respective specification added to the request to the LLM if the user has access to the function. All the required information to create the function specification is extracted from the code and docstrings: – The ID of the function – A message that’s sent to the user through Slack for immediate feedback before the function is runEXCLUSIVE_ACCESS_DEPARTMENTS – If set, only users of the specified departments have access to this toolThe tool in this example updates the user memory. For example, the query “Remember to always use Python as a programming language” will trigger the execution of this tool. The LLM will extract the info string from the request, for example, “code should always be Python.” If the existing user memory that is always added to the context already contains a memory about the same topic (for example, “code should always be Java”), the LLM will also provide the memory ID and the existing memory will be overwritten. Otherwise, a new memory with a new ID is created.Key features and benefitsSlack serves as a single entry point, allowing artists to query diverse internal systems without leaving their familiar workflow. The following features are powered by function calling using Anthropic’s Claude:Various knowledge bases for different user roles (Confluence, Salesforce)Internal asset library (Amazon S3)Image generation powered by Stable DiffusionUser-specific memory and preferences (for example, default programming languages, default dimensions for image generation, detail level of responses)By eliminating the need for additional software or context switching, we’ve drastically reduced friction in accessing critical resources. The system is available around the clock for artist queries and tasks, and our framework for function calling with Anthropic’s Claude allows for future expansion of features.The LLM’s natural language interface is a game changer for user interaction. It’s inherently more flexible and forgiving compared to traditional interfaces, capable of interpreting unstructured input, asking for missing information, and performing tasks like date formatting, unit conversion, and value extraction from natural language descriptions. The system adeptly handles ambiguous queries, extracting relevant information and intent. This means artists can focus on their creative process rather than worrying about precise phrasing or navigating complex menu structures.Security and control are paramount in our AI adoption strategy. By keeping all data within the AWS ecosystem, we’ve eliminated dependencies on third-party AI tools and mitigated associated risks. This approach allows us to maintain tight control over data access and usage. Additionally, we’ve implemented comprehensive usage analytics, providing insights into adoption patterns and areas for improvement. This data-driven approach makes sure we’re continually refining the tool to meet evolving artist needs.The Untold Assistant currently handles up to 120 queries per day, with about 10–20% of them calling additional tools, like image generation or knowledge base search. Especially for new users who aren’t too familiar with internal workflows and applications yet, it can save a lot of time. Instead of searching in several different Confluence spaces and Slack channels or reaching out to the technology team, they can just ask the Untold Assistant, which acts as a virtual member of the support team. This can cut down the time from minutes to only a few seconds.Overall, the Untold Assistant, rapidly developed and deployed using AWS services, has delivered several benefits:Enhanced discoverability and usage of previously underutilized internal resourcesSignificant reduction in time spent searching for informationStreamlined access to multiple internal systems with an authorization system from a central entry pointReduced load on the support and technology teamIncreased speed of adoption of new technologies by providing a framework for user interactionBuilding on this success, we’re expanding functionality through additional function calls. A key planned feature is render job error analysis for artists. This tool will automatically fetch logs from recent renders, analyze potential errors using the capabilities of Anthropic’s Claude, and provide users with explanations and solutions by using both internet resources and our internal knowledge base of known errors.Additionally, we plan to analyze the saved queries using Amazon Titan Text Embeddings and agglomerative clustering to identify semantically similar questions. When the cluster frequency exceeds our defined threshold (for example, more than 10 similar questions from different users within a week), we enhance our knowledge base or update onboarding materials to address these common queries proactively, reducing repetitive questions and improving the assistant’s efficiency.These initial usage metrics and the planned technical improvements demonstrate the system’s positive impact on our workflows. By automating common support tasks and continuously improving our knowledge base through data-driven analysis, we reduce the technology team’s support load while maintaining high-quality assistance. The modular architecture allows us to quickly integrate new tools as needs arise, to keep up with the astonishing pace of the progress made in AI and ML.The Untold Assistant demonstrates how Amazon Bedrock enables rapid development of sophisticated AI applications without compromising security or control. Using function calling and pre-built connectors in Amazon Bedrock eliminated the need for complex vector store integrations and custom embedding pipelines, reducing our development time from months to weeks. The modular architecture using Python classes for tools makes the system highly maintainable and extensible.By automating routine technical tasks and information retrieval, we’ve freed our artists to focus on creative work that drives business value. The solution’s clean separation between the LLM interface and business logic, built entirely within the AWS ecosystem, enables quick integration of new capabilities while maintaining strict data security. The LLM’s ability to interpret unstructured input and handle ambiguous queries creates a more natural and forgiving interface compared to traditional menu-driven systems. This foundation of technical robustness and improved artist productivity positions us to rapidly adopt emerging AI capabilities while keeping our focus on creative innovation.To explore how to streamline your company’s workflows using Amazon Bedrock, see Getting started with Amazon Bedrock. If you have questions or suggestions, please leave a comment. is a Solutions Architect at AWS. Based in England, he primarily works with SMB Media an&d Entertainment customers. With a background in security and networking, Olivier helps customers achieve success on their cloud journey by providing architectural guidance and best practices; he is also passionate about helping them bring value with Machine Learning and Generative AI use-cases. is a Lead R&D Developer at Untold Studios with a focus on cloud infrastructure and emerging technologies. After earning his PhD in Germany, where he collaborated with industry leaders like BMW and Audi, he has spent the past decade implementing software solutions, with a particular emphasis on cloud technology in recent years. At Untold Studios, he leads infrastructure optimisation and AI/ML initiatives, leveraging his technical expertise and background in research to drive innovation in the Media & Entertainment space. is an Account Manager at AWS who specialises in accelerating the cloud journey of Media & Entertainment customers. He has been helping customers at AWS for the past 4.5 years. Max has been particularly involved with customers in the visual effect space, guiding them as they explore generative AI.]]></content:encoded></item><item><title>From finding to fixing: GitHub Advanced Security integrates Endor Labs SCA</title><link>https://github.blog/security/from-finding-to-fixing-github-advanced-security-integrates-endor-labs-sca/</link><author>Mario Rodriguez</author><category>official</category><pubDate>Fri, 7 Feb 2025 17:00:17 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[It’s no wonder developers are increasingly overwhelmed. The number of new CVEs published each year has increased by nearly 500% in the last decade. And the average project, with just 10 direct dependencies, can have hundreds of indirect dependencies. Put simply, developers are often buried under a mountain of security alerts and unable to prioritize which ones to remediate first.While high-profile supply chain attacks like last year’s XZ Utils backdoor tend to capture attention, the danger they pose is just a fraction of the overall threat landscape. The bigger risk often comes from unpatched vulnerabilities in lesser-known open source dependencies.GitHub’s partnership with Endor Labs cuts through the noise to help developers  identify, remediate, and fix the most critical vulnerabilities—without ever leaving GitHub.With Endor Labs software composition analysis (SCA) integrated into GitHub Advanced Security and Dependabot, development teams can dismiss up to 92% of low-risk dependency security alerts to focus instead on the vulnerabilities that matter most.Endor Labs SCA brings context into open source vulnerability detectionEndor Labs SCA helps identify and prioritize dependency vulnerabilities by their potential impact, according to factors like reachability, exploitability, and more. For example, Endor Labs checks if the vulnerable function of a given dependency is actually reachable by your application or if it is just sitting on an unused corner of a transitive dependency. Security teams can also configure risk, licensing, and permission profiles to ensure developers are not bothered unless the risk is truly warranted.Prioritize and fix open source vulnerabilities with GitHubGitHub Advanced Security integrates crucial security practices directly into the development workflow, offering developers a streamlined way to secure their code. Its features are free for open source maintainers, including dependency review, secret scanning, code scanning, and Copilot Autofix.Dependabot, available for free to all GitHub users, automates dependency updates, so you can spend more time building. Developers can remediate vulnerabilities by merging Dependabot-authored pull requests with the click of a button or by applying Endor Patches.Secure your automated workflowsGitHub Actions makes it easy to automate all your software workflows, whether you want to build a container, deploy a web service, or welcome new users to your open source project. These actions are often updated with bug fixes and new features, which can take time to maintain.Endor Labs automatically discovers in-use actions and their dependencies to ensure they fit your risk, licensing, and permission profiles. Dependabot automatically updates your dependencies, and code scanning helps identify existing workflow configuration vulnerabilities and prevent new ones.]]></content:encoded></item><item><title>Video Friday: Agile Humanoids</title><link>https://spectrum.ieee.org/video-friday-agile-humanoids</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjI5NzUwOS9vcmlnaW4uZ2lmIiwiZXhwaXJlc19hdCI6MTc0MzI1MjgxMX0.E544b1VXCKnbQMF2MZF3pmS7vTPUB1Ki4FcOrSVf-so/image.gif?width=600" length="" type=""/><pubDate>Fri, 7 Feb 2025 16:30:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your weekly selection of awesome robot videos]]></content:encoded></item><item><title>Google’s Quantum Simulator Reveals New Facet of Magnetism</title><link>https://spectrum.ieee.org/quantum-simulation</link><author>Charles Q. Choi</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjIyNjk4OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NjI0MDc0OH0.pwak6owtImBsEt2ExAy5KQmJF0CWetgQd5IO5amKSPc/image.jpg?width=600" length="" type=""/><pubDate>Fri, 7 Feb 2025 16:00:15 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[A hybrid digital-analog quantum computer made a surprising discovery]]></content:encoded></item><item><title>Screenshot-Reading Malware</title><link>https://www.schneier.com/blog/archives/2025/02/screenshot-reading-malware.html</link><author>Bruce Schneier</author><category>Schneider on Security</category><category>infosec</category><category>blog</category><pubDate>Fri, 7 Feb 2025 15:26:11 +0000</pubDate><source url="https://www.schneier.com/">Schneier on Security</source><content:encoded><![CDATA[Kaspersky is reporting on a new type of smartphone malware.The malware in question uses optical character recognition (OCR) to review a device’s photo library, seeking screenshots of recovery phrases for crypto wallets. Based on their assessment, infected Google Play apps have been downloaded more than 242,000 times. Kaspersky says: “This is the first known case of an app infected with OCR spyware being found in Apple’s official app marketplace.”That’s a tactic I have not heard of before.]]></content:encoded></item><item><title>Digma Adds Ability to Predict Coding Issues to Observability Platform</title><link>https://devops.com/digma-adds-ability-to-predict-coding-issues-to-observability-platform/</link><author>Mike Vizard</author><category>devops</category><pubDate>Fri, 7 Feb 2025 15:11:00 +0000</pubDate><source url="https://devops.com/">DevOps.com</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Robustness, Detectability, and Data Privacy in AI // Vinu Sankar Sadasivan // #289</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/Robustness--Detectability--and-Data-Privacy-in-AI--Vinu-Sankar-Sadasivan--289-e2ui4si</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/98160978/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-7%2F394458992-44100-2-ab419176af6b.mp3" length="" type=""/><pubDate>Fri, 7 Feb 2025 14:44:23 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[is a CS PhD ... Currently, I am working as a full-time  on jailbreaking multimodal AI models.

Robustness, Detectability, and Data Privacy in AI // MLOps Podcast #289 with Vinu Sankar Sadasivan, Student Researcher at Google DeepMind.

// Abstract
Recent rapid advancements in Artificial Intelligence (AI) have made it widely applicable across various domains, from autonomous systems to multimodal content generation. However, these models remain susceptible to significant security and safety vulnerabilities. Such weaknesses can enable attackers to jailbreak systems, allowing them to perform harmful tasks or leak sensitive information. As AI becomes increasingly integrated into critical applications like autonomous robotics and healthcare, the importance of ensuring AI safety is growing. Understanding the vulnerabilities in today’s AI systems is crucial to addressing these concerns.

// Bio
Vinu Sankar Sadasivan is a final-year Computer Science PhD candidate at The University of Maryland, College Park, advised by Prof. Soheil Feizi. His research focuses on Security and Privacy in AI, with a particular emphasis on AI robustness, detectability, and user privacy. Currently, Vinu is a full-time Student Researcher at Google DeepMind, working on jailbreaking multimodal AI models. Previously, Vinu was a Research Scientist intern at Meta FAIR in Paris, where he worked on AI watermarking.

Vinu is a recipient of the 2023 Kulkarni Fellowship and has earned several distinctions, including the prestigious Director’s Silver Medal. He completed a Bachelor’s degree in Computer Science & Engineering at IIT Gandhinagar in 2020. Prior to their PhD, Vinu gained research experience as a Junior Research Fellow in the Data Science Lab at IIT Gandhinagar and through internships at Caltech, Microsoft Research India, and IISc.

// MLOps Swag/Merch
https://shop.mlops.community/

// Related Links
Website: https://vinusankars.github.io/
 
--------------- ✌️Connect With Us ✌️ -------------
Join our slack community: https://go.mlops.community/slack
Follow us on Twitter: @mlopscommunity
Sign up for the next meetup: https://go.mlops.community/register
Catch all episodes, blogs, newsletters, and more: https://mlops.community/

Connect with Demetrios on LinkedIn: https://www.linkedin.com/in/dpbrinkm/
Connect with Richard on LinkedIn: https://www.linkedin.com/in/vinusankars/

]]></content:encoded></item><item><title>Asahi Linux Lead Developer Hector Martin Resigns From Linux Kernel</title><link>https://linux.slashdot.org/story/25/02/07/1332241/asahi-linux-lead-developer-hector-martin-resigns-from-linux-kernel?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>slashdot</category><category>linux</category><pubDate>Fri, 7 Feb 2025 14:40:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Asahi lead developer Hector Martin, writing in an email: I no longer have any faith left in the kernel development process or community management approach. 

Apple/ARM platform development will continue downstream. If I feel like sending some patches upstream in the future myself for whatever subtree I may, or I may not. Anyone who feels like fighting the upstreaming fight themselves is welcome to do so. 

The Register points out that the action follows this interaction with Linux Torvalds. 
Hector Martin: If shaming on social media does not work, then tell me what does,
because I'm out of ideas. 
Linus Torvalds: How about you accept the fact that maybe the problem is you. You think you know better. But the current process works. It has problems, but problems are a fact of life. There is no perfect. However, I will say that the social media brigading just makes me not
want to have anything at all to do with your approach. Because if we have issues in the kernel development model, then social media sure as hell isn't the solution.]]></content:encoded></item><item><title>The Most Important Programming Invention In 20 Years • Dave Farley • GOTO 2025</title><link>https://www.youtube.com/watch?v=6Il6jLf49BM</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/6Il6jLf49BM?version=3" length="" type=""/><pubDate>Fri, 7 Feb 2025 13:00:08 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[We’re so pleased to having teamed up with Dave Farley, author of “Continuous Delivery” and frequent GOTO Conferences speaker, for a monthly video series featuring ideas about continuous delivery, DevOps, test-driven development, BDD, software engineering and software development in general.

Find plenty more from Dave on his @ContinuousDelivery channel.

Dave Farley - Continuous Delivery & DevOps Pioneer, Award-winning Author, Founder & Director of Continuous Delivery Ltd.

RESOURCES
Dave
https://bsky.app/profile/davefarley77.bsky.social
https://www.continuous-delivery.co.uk
https://linkedin.com/in/dave-farley-a67927
https://twitter.com/davefarley77
http://www.davefarley.net

ABSTRACT
We asked thousands of software developers what they thought was the best invention or discovery in software development in the last 20 years, what have been the software development trends and software engineering technologies that have set the scene for software engineering in 2024?

In this episode, Dave Farley, best-selling author of “Continuous Delivery” and “Modern Software Engineering" explores the top 5 new software engineering technologies that have shaped how we work and gives his impressions on why and how these things have influenced the world of software development.

CD TRAINING COURSES
If you want to learn Continuous Delivery and DevOps skills, check out Dave Farley's courses
https://bit.ly/DFTraining 

RECOMMENDED BOOKS & LINKS
You can grab Dave Farley's new book 'Continuous Delivery Pipelines' here:
https://leanpub.com/cd-pipelines
David Farley • Modern Software Engineering • https://amzn.to/3GI468M
Dave Farley & Jez Humble • Continuous Delivery • https://amzn.to/3ocIHwd
Forsgren, Humble & Kim • Accelerate • https://amzn.to/367RI5o
Gojko Adzic & David Evans • 50 Quick Ideas to Improve your User Stories • https://leanpub.com/50quickideas

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#GOTOxDaveFarley #SRE #Git #GitHub #VersionControl #TDD #LLMs #Programming #DaveFarley #GOTO #GOTOcon #Complexity  #ProgrammingTutorial #SoftwareEngineering

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>The Real Python Podcast – Episode #238: Charlie Marsh: Accelerating Python Tooling With Ruff and uv</title><link>https://realpython.com/podcasts/rpp/238/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Fri, 7 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[Are you looking for fast tools to lint your code and manage your projects? How is the Rust programming language being used to speed up Python tools? This week on the show, we speak with Charlie Marsh about his company, Astral, and their tools, uv and Ruff.]]></content:encoded></item><item><title>Your YouTube Channel to Learn System Design 🔥</title><link>https://newsletter.systemdesign.one/p/system-design-youtube-channel</link><author>Neo Kim</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/83516d21-e16e-4561-861e-e83b9bf60224_1280x720.gif" length="" type=""/><pubDate>Fri, 7 Feb 2025 11:35:39 +0000</pubDate><source url="https://newsletter.systemdesign.one/">Dev - System Design Newsletter</source><content:encoded><![CDATA[I created a YouTube channel to help you learn system design.• System design fundamentals.• System design interview preparation tips.• Simplified engineering case studies with visuals.• System design deep dives with real-world software architecture.You'll receive a new video every 10 days.I'll make it your main video channel for system design over time.And I want to help you become good at work + ace system design interviews:Please show your support,]]></content:encoded></item><item><title>Innovations in Serverless &amp; Event-Driven Solutions • Ben Ellerby &amp; Julian Wood</title><link>https://www.youtube.com/watch?v=ZY52xzWkZFU</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/ZY52xzWkZFU?version=3" length="" type=""/><pubDate>Fri, 7 Feb 2025 11:34:00 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This interview was recorded for GOTO Unscripted.
https://gotopia.tech

Read the full transcription of this interview here
 (https://gotopia.tech/articles/338) 
Ben Ellerby - Founder of aleio & AWS Serverless Hero
Julian Wood - Serverless Developer Advocate at AWS

RESOURCES
Ben
https://twitter.com/EllerbyBen
https://www.linkedin.com/in/benjaminellerby
https://github.com/BenEllerby
https://medium.com/@bene_37069

Julian
https://twitter.com/julian_wood
http://www.wooditwork.com
https://www.linkedin.com/in/julianrwood

DESCRIPTION
Julian Wood and Ben Ellerby explore the challenges and innovations in event-driven architectures, generative AI, and serverless technologies. They emphasize the importance of well-structured event schemas and the role of platform teams in reducing cognitive load for developers.

Ben Ellerby highlights the potential of generative AI in modernizing legacy codebases and discusses the resurgence of event-driven architectures, driven by improved tools and frameworks that promote decoupling and efficiency.

The conversation also touches on the future of serverless computing, edge computing, and the significance of data management in global applications, underscoring a transformative shift toward more scalable and flexible cloud solutions.

RECOMMENDED BOOKS
David Anderson, Marck McCann & Michael O'Reilly • The Value Flywheel Effect
 (https://amzn.to/3VcHxC7) Adam Bellemare • Building Event-Driven Microservices
 (https://amzn.to/3WfNKfM) Lauren Maffeo • Designing Data Governance from the Ground Up
 (https://amzn.to/3QhIlnV) Vaughn Vernon & Tomasz Jaskula • Strategic Monoliths & Microservices
 (https://amzn.to/3AcUscj) Gregor Hohpe • Platform Strategy
 (https://amzn.to/4cxfYdb) Peter Sbarski • Serverless Architectures on AWS
 (https://amzn.to/3hJzEUM) Michael Stack • Event-Driven Architecture in Golang
 (https://amzn.to/3G5e8ST) James Urquhart • Flow Architectures (https://amzn.to/3Tyz8cY) 


Bluesky (https://bsky.app/profile/gotocon.com) 
Twitter (https://twitter.com/GOTOcon) 
Instagram (https://www.instagram.com/goto_con) 
LinkedIn (https://www.linkedin.com/company/goto-) 
Facebook (https://www.facebook.com/GOTOConferences) 

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket: gotopia.tech (https://gotopia.tech) 

SUBSCRIBE TO OUR YOUTUBE CHANNEL (https://www.youtube.com/user/GotoConferences/?sub_confirmation=1)  - new videos posted daily!]]></content:encoded></item><item><title>ChatLoopBackOff Episode 45 (CRI-O)</title><link>https://www.youtube.com/watch?v=--eJZu3Zkbw</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/--eJZu3Zkbw?version=3" length="" type=""/><pubDate>Fri, 7 Feb 2025 06:19:59 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[CRI-O, a CNCF Graduated project, is designed to be lightweight, focusing solely on Kubernetes needs’, reducing resource overhead compared to general-purpose container runtimes. 

It improves security by leveraging OCI (Open Container Initiative) standards, integrating seamlessly with tools like seccomp, SELinux, and AppArmor for enhanced isolation and compliance. With robust support for OCI-compliant image formats and runtimes like runc, CRI-O ensures compatibility with a wide range of container images. Join CNCF Ambassador Marvin Beckers test CRI-O’s efficiency, as it minimizes the feature set to what Kubernetes requires, leading to faster startup times and reduced complexity.]]></content:encoded></item><item><title>Protect your DeepSeek model deployments with Amazon Bedrock Guardrails</title><link>https://aws.amazon.com/blogs/machine-learning/protect-your-deepseek-model-deployments-with-amazon-bedrock-guardrails/</link><author>Satveer Khurpa</author><category>dev</category><category>ai</category><pubDate>Fri, 7 Feb 2025 02:29:52 +0000</pubDate><source url="https://aws.amazon.com/blogs/machine-learning/">AWS AI blog</source><content:encoded><![CDATA[The rapid advancement of generative AI has brought powerful publicly available large language models (LLMs), such as DeepSeek-R1, to the forefront of innovation. The DeepSeek-R1 models are now accessible through Amazon Bedrock Marketplace and Amazon SageMaker JumpStart, and distilled variants are available through Amazon Bedrock Custom Model Import. According to DeepSeek AI, these models offer strong capabilities in reasoning, coding, and natural language understanding. However, their deployment in production environments—like all models—requires careful consideration of data privacy requirements, appropriate management of bias in output, and the need for robust monitoring and control mechanisms.Organizations adopting open source, open weights models such as DeepSeek-R1 have important opportunities to address several key considerations:Making sure to protect sensitive informationFostering responsible content generation practicesStriving for compliance with relevant industry regulationsThese concerns become particularly critical in highly regulated industries such as healthcare, finance, and government services, where data privacy and content accuracy are paramount.This blog post provides a comprehensive guide to implementing robust safety protections for DeepSeek-R1 and other open weight models using Amazon Bedrock Guardrails. We’ll explore:How to use the security features offered by Amazon Bedrock to protect your data and applicationsPractical implementation of guardrails to prevent prompt attacks and filter harmful contentImplementing a robust defense-in-depth strategyBy following this guide, you’ll learn how to use the advanced capabilities of DeepSeek models while maintaining strong security controls and promoting ethical AI practices. Whether developing customer-facing generative AI applications or internal tools, these implementation patterns will help you meet your requirements for secure and responsible AI. By following this step-by-step approach, organizations can deploy open weights LLMs such as DeepSeek-R1 in line with best practices for AI safety and security.DeepSeek models and deployment on Amazon BedrockDeepSeek AI, a company specializing in open weights foundation AI models, recently launched their DeepSeek-R1 models, which according to their paper have shown outstanding reasoning abilities and performance in industry benchmarks. According to third-party evaluations, these models consistently achieve top three rankings across various metrics, including quality index, scientific reasoning and knowledge, quantitative reasoning, and coding (HumanEval).The company has further developed their portfolio by releasing six dense models derived from DeepSeek-R1, built on Llama and Qwen architectures, which they’ve made open weight models. These models are now accessible through AWS generative AI solutions: DeepSeek-R1 is available through Amazon Bedrock Marketplace and SageMaker Jumpstart, while the Llama-based distilled versions can be implemented through Amazon Bedrock Custom Model Import.Amazon Bedrock offers comprehensive security features to help secure hosting and operation of open source and open weights models while maintaining data privacy and regulatory compliance. Key features include data encryption at rest and in transit, fine-grained access controls, secure connectivity options, and various compliance certifications. Additionally, Amazon Bedrock provides guardrails for content filtering and sensitive information protection to support responsible AI use. AWS enhances these capabilities with extensive platform-wide security and compliance measures:Organizations should customize these security settings based on their specific compliance and security needs when deploying to production environments. AWS conducts vulnerability scanning of all model containers as part of its security process and accepts only models in Safetensors format to help prevent unsafe code execution.Amazon Bedrock GuardrailsAmazon Bedrock Guardrails provides configurable safeguards to help safely build generative AI applications at scale. Amazon Bedrock Guardrails can also be integrated with other Amazon Bedrock tools including Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases to build safer and more secure generative AI applications aligned with responsible AI policies. To learn more, see the AWS Responsible AI page.Amazon Bedrock Guardrails can be used in two ways. First, it can be integrated directly with the InvokeModel and Converse API call, where guardrails are applied to both input prompts and model outputs during the inference process. This method is suitable with models hosted on Amazon Bedrock through the Amazon Bedrock Marketplace and Amazon Bedrock Custom Model Import. Alternatively, the ApplyGuardrail API offers a more flexible approach, allowing for independent evaluation of content without invoking a model. This second method is useful for assessing inputs or outputs at various stages of an application, working with custom or third-party models outside of Amazon Bedrock. Both approaches enable developers to implement safeguards customized to their use cases and aligned with responsible AI policies, ensuring secure and compliant interactions in generative AI applications.Key Amazon Bedrock Guardrails policiesAmazon Bedrock Guardrails provides the following configurable guardrail policies to help safely build generative AI applications at scale:Content filters 
  Adjustable filtering intensity for harmful contentPredefined categories: Hate, Insults, Sexual Content, Violence, Misconduct, and Prompt AttacksMulti-modal content including text and images (preview)Topic filters 
  Capability to restrict specific topicsPrevention of unauthorized topics in both queries and responsesWord filters 
  Blocks specific words, phrases, and profanityCustom filters for offensive language or competitor referencesSensitive information filters 
  Personally identifiable information (PII) blocking or maskingSupport for custom regex patternsProbabilistic detection for standard formats (such as SSN, DOB, and addresses)Contextual grounding checks 
  Hallucination detection through source groundingQuery relevance validationAutomated Reasoning checks for hallucination prevention (gated preview)Model-agnostic implementation:Compatible with all Amazon Bedrock foundation modelsSupports fine-tuned modelsExtends to external custom and third-party models through the ApplyGuardrail APIThis comprehensive framework helps customers implement responsible AI, maintaining content safety and user privacy across diverse generative AI applications.Create a guardrail with specific policies tailored to your use case and configure the policies. Integration with InvokeModel APICall the Amazon Bedrock InvokeModel API with the guardrail identifier in your request.When you make the API call, Amazon Bedrock applies the specified guardrail to both the input and output. Guardrail evaluation process: Before sending the prompt to the model, the guardrail evaluates the user input against the configured policies.: For improved latency, the input is evaluated in parallel for each configured policy.: If the input violates any guardrail policies, a pre-configured blocked message is returned, and the model inference is discarded.: If the input passes the guardrail checks, the prompt is sent to the specified model for inference.: After the model generates a response, the guardrail evaluates the output against the configured policies.: If the model response violates any guardrail policies, it will be either blocked with a pre-configured message or have sensitive information masked, depending on the policy.: If the output passes all guardrail checks, the response is returned to the application without modificationsBefore setting up guardrails for models imported using the Amazon Bedrock Custom Model Import feature, make sure you meet these prerequisites:An AWS account with access to Amazon Bedrock along with the necessary IAM role with the required permissions. For centralized access management, we recommend that you use AWS IAM Identity Center.Make sure that a custom model is already imported using the Amazon Bedrock Custom Model Import service. For illustration, we’ll use DeepSeek-R1-Distill-Llama-8B, which can be imported using Amazon Bedrock Custom Model Import. You have two options for deploying this model: 
  You can create the guardrail using the AWS Management Console as explained in this blog post. Alternatively, you can follow this notebook for a programmatic example of how to create the guardrail in this solution. This notebook does the following :Install the required dependenciesCreate a guardrail using the boto3 API and filters to meet the use case mentioned previously.Configure the tokenizer for the imported model.Test Amazon Bedrock Guardrails using prompts that show various Amazon Bedrock guardrail filters in action.This approach integrates guardrails into both the user inputs and the model outputs. This makes sure that any potentially harmful or inappropriate content is intercepted during both phases of the interaction. For open weight distilled models imported using Amazon Bedrock Custom Model Import, Amazon Bedrock Marketplace, and Amazon SageMaker JumpStart, critical filters to implement include those for prompt attacks, content moderation, topic restrictions, and sensitive information protection.Implementing a defense-in-depth strategy with AWS servicesWhile Amazon Bedrock Guardrails provides essential content and prompt safety controls, implementing a comprehensive defense-in-depth strategy is crucial when deploying any foundation model, especially open weights models such as DeepSeek-R1. For detailed guidance on defense-in-depth approaches aligned with OWASP Top 10 for LLMs, see our previous blog post on architecting secure generative AI applications.Developing organizational resiliency by starting with security in mindBuilding on a secure cloud foundation using AWS servicesApplying a layered defense strategy across multiple trust boundariesAddressing the OWASP Top 10 risks for LLM applicationsImplementing security best practices throughout the AI/ML lifecycleUsing AWS security services in conjunction with AI and machine learning (AI/ML)-specific featuresConsidering diverse perspectives and aligning security with business objectivesPreparing for and mitigating risks such as prompt injection and data poisoningThe combination of model-level controls (guardrails) with a defense-in-depth strategy creates a robust security posture that can help protect against:Data exfiltration attemptsUnauthorized access to fine-tuned models or training dataPotential vulnerabilities in model implementationMalicious use of AI agents and integrationsWe recommend conducting thorough threat modeling exercises using AWS guidance for generative AI workloads before deploying any new AI/ML solutions. This helps align security controls with specific risk scenarios and business requirements.Implementing safety protection for LLMs, including DeepSeek-R1 models, is crucial for maintaining a secure and ethical AI environment. By using Amazon Bedrock Guardrails with the Amazon Bedrock InvokeModel API and the ApplyGuardrails API, you can help mitigate the risks associated with advanced language models while still harnessing their powerful capabilities. However, it’s important to recognize that model-level protections are just one component of a comprehensive security strategy.The strategies outlined in this post address several key security concerns that are common across various open weights models hosted on Amazon Bedrock using Amazon Bedrock Custom Model Import, Amazon Bedrock Marketplace, and through Amazon SageMaker JumpStart. These include potential vulnerabilities to prompt injection attacks, the generation of harmful content, and other risks identified in recent assessments. By implementing these guardrails alongside a defense-in-depth approach, organizations can significantly reduce the risk of misuse and better align their AI applications with ethical standards and regulatory requirements.As AI technology continues to evolve, it’s essential to prioritize safety and responsible use of generative AI. Amazon Bedrock Guardrails provides a configurable and robust framework for implementing these safeguards, allowing developers to customize protection measures according to their specific use cases and organizational policies. We strongly recommend conducting thorough threat modeling of your AI workloads using AWS guidance to evaluate security risks and implementing appropriate controls across your entire technology stack.Remember to regularly review and update not only your guardrails but all security controls to address new potential vulnerabilities and help maintain protection against emerging threats in the rapidly evolving landscape of AI security. While today we focus on DeepSeek-R1 models, the AI landscape is continuously evolving with new models emerging regularly. Amazon Bedrock Guardrails, combined with AWS security services and best practices, provides a consistent security framework that can adapt to protect your generative AI applications across various open weights models, both current and future. By treating security as a continuous process of assessment, improvement, and adaptation, organizations can confidently deploy innovative AI solutions while maintaining robust security controls. is a Sr. WW Specialist Solutions Architect, Bedrock at Amazon Web Services. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer’s deep understanding of generative AI technologies allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value. is a Sr. Data Scientist–Generative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering. is a Principal Generative AI Specialist Solutions Architect at Amazon Web Services. He helps companies of all sizes solve their challenges, embrace innovation, and create new business opportunities with Amazon Bedrock. Apart from work, he loves to spend time with his family and play sports with his friends.]]></content:encoded></item><item><title>Oracle dabbles in some JavaScript comedy</title><link>https://javascriptweekly.com/issues/722</link><author></author><category>dev</category><category>frontend</category><pubDate>Fri, 7 Feb 2025 00:00:00 +0000</pubDate><source url="https://javascriptweekly.com/">Javascript Weekly</source><content:encoded><![CDATA[🍪 CookieConsent 3.1 – A lightweight, pure JS, GDPR-compliant cookie consent mechanism with which to  meet regulatory requirements.Happy DOM 17.0 – Cross-runtime JS implementation of a web browser  UI. Now supports ES modules.get-value 4.0 – Use property paths () get a nested value from an object.mp4-muxer 5.2 – MP4 multiplexer in pure TypeScript with support for the WebCodecs API, video and audio.Wasp 0.16 – Wasp is a Rails-like framework using Node, React & Prisma.RxDB 16.5 – Offline-first, reactive database for JS apps.]]></content:encoded></item><item><title>From PDFs to Insights: Structured Outputs from PDFs with Gemini 2.0</title><link>https://www.philschmid.de/gemini-pdf-to-data</link><author></author><category>dev</category><category>ai</category><category>blog</category><pubDate>Fri, 7 Feb 2025 00:00:00 +0000</pubDate><source url="https://www.philschmid.de/">Phil Shmid</source><content:encoded><![CDATA[Learn how to extract structured data from PDFs with Gemini 2.0 and Pydantic.]]></content:encoded></item><item><title>EFF Applauds Little Rock, AR for Cancelling ShotSpotter Contract</title><link>https://www.eff.org/deeplinks/2025/02/eff-applauds-little-rock-ak-cancelling-shotspotter-contract</link><author>Sarah Hamid</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/shotspotter.jpg" length="" type=""/><pubDate>Thu, 6 Feb 2025 23:56:20 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DeepSeek iOS app sends data unencrypted to ByteDance-controlled servers</title><link>https://arstechnica.com/security/2025/02/deepseek-ios-app-sends-data-unencrypted-to-bytedance-controlled-servers/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/01/GettyImages-2195894561-1152x648.jpg" length="" type=""/><pubDate>Thu, 6 Feb 2025 22:06:17 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[A little over two weeks ago, a largely unknown China-based company named DeepSeek stunned the AI world with the release of an open source AI chatbot that had simulated reasoning capabilities that were largely on par with those from market leader OpenAI. Within days, the DeepSeek AI assistant app climbed to the top of the iPhone App Store's "Free Apps" category, overtaking ChatGPT.On Thursday, mobile security company NowSecure reported that the app sends sensitive data over unencrypted channels, making the data readable to anyone who can monitor the traffic. More sophisticated attackers could also tamper with the data while it's in transit. Apple strongly encourages iPhone and iPad developers to enforce encryption of data sent over the wire using ATS (App Transport Security). For unknown reasons, that protection is globally disabled in the app, NowSecure said.Basic security protections MIAWhat’s more, the data is sent to servers that are controlled by ByteDance, the Chinese company that owns TikTok. While some of that data is properly encrypted using transport layer security, once it's decrypted on the ByteDance-controlled servers, it can be cross-referenced with user data collected elsewhere to identify specific users and potentially track queries and other usage.]]></content:encoded></item><item><title>Experts Flag Security, Privacy Risks in DeepSeek AI App</title><link>https://krebsonsecurity.com/2025/02/experts-flag-security-privacy-risks-in-deepseek-ai-app/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Thu, 6 Feb 2025 21:12:30 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[New mobile apps from the Chinese artificial intelligence (AI) company  have remained among the top three “free” downloads for Apple and Google devices since their debut on Jan. 25, 2025. But experts caution that many of DeepSeek’s design choices — such as using hard-coded encryption keys, and sending unencrypted user and device data to Chinese companies — introduce a number of glaring security and privacy risks.Public interest in the DeepSeek AI chat apps swelled following widespread media reports that the upstart Chinese AI firm had managed to match the abilities of cutting-edge chatbots while using a fraction of the specialized computer chips that leading AI companies rely on. As of this writing, DeepSeek is the third most-downloaded “free” app on the Apple store, and #1 on Google Play.DeepSeek’s rapid rise caught the attention of the mobile security firm , a Chicago-based company that helps clients screen mobile apps for security and privacy threats. In a teardown of the DeepSeek app published today, NowSecure urged organizations to remove the DeepSeek iOS mobile app from their environments, citing security concerns.NowSecure founder  said they haven’t yet concluded an in-depth analysis of the DeepSeek app for  devices, but that there is little reason to believe its basic design would be functionally much different.Hoog told KrebsOnSecurity there were a number of qualities about the DeepSeek iOS app that suggest the presence of deep-seated security and privacy risks. For starters, he said, the app collects an awful lot of data about the user’s device.“They are doing some very interesting things that are on the edge of advanced device fingerprinting,” Hoog said, noting that one property of the app tracks the device’s name — which for many iOS devices defaults to the customer’s name followed by the type of iOS device.The device information shared, combined with the user’s Internet address and data gathered from mobile advertising companies, could be used to deanonymize users of the DeepSeek iOS app, NowSecure warned. The report notes that DeepSeek communicates with , a cloud platform developed by  (the makers of ), although NowSecure said it wasn’t clear if the data is just leveraging ByteDance’s digital transformation cloud service or if the declared information share extends further between the two companies.Perhaps more concerning, NowSecure said the iOS app transmits device information “in the clear,” without any encryption to encapsulate the data. This means the data being handled by the app could be intercepted, read, and even modified by anyone who has access to any of the networks that carry the app’s traffic.“The DeepSeek iOS app globally disables App Transport Security (ATS) which is an iOS platform level protection that prevents sensitive data from being sent over unencrypted channels,” the report observed. “Since this protection is disabled, the app can (and does) send unencrypted data over the internet.”Hoog said the app does selectively encrypt portions of the responses coming from DeepSeek servers. But they also found it uses an insecure and now deprecated encryption algorithm called 3DES (aka Triple DES), and that the developers had hard-coded the encryption key. That means the cryptographic key needed to decipher those data fields can be extracted from the app itself.There were other, less alarming security and privacy issues highlighted in the report, but Hoog said he’s confident there are additional, unseen security concerns lurking within the app’s code.“When we see people exhibit really simplistic coding errors, as you dig deeper there are usually a lot more issues,” Hoog said. “There is virtually no priority around security or privacy. Whether cultural, or mandated by China, or a witting choice, taken together they point to significant lapse in security and privacy controls, and that puts companies at risk.”Apparently, plenty of others share this view. reported on January 30 that U.S. congressional offices are being warned not to use the app.“[T]hreat actors are already exploiting DeepSeek to deliver malicious software and infect devices,” read the notice from the chief administrative officer for the House of Representatives. “To mitigate these risks, the House has taken security measures to restrict DeepSeek’s functionality on all House-issued devices.”reports that Italy and Taiwan have already moved to ban DeepSeek over security concerns. writes that  has blocked access to DeepSeek. says also banned employees from using the service, as did the .Beyond security concerns tied to the DeepSeek iOS app, there are indications the Chinese AI company may be playing fast and loose with the data that it collects from and about users. On January 29, researchers at said they discovered a publicly accessible database linked to DeepSeek that exposed “a significant volume of chat history, backend data and sensitive information, including log streams, API secrets, and operational details.”“More critically, the exposure allowed for full database control and potential privilege escalation within the DeepSeek environment, without any authentication or defense mechanism to the outside world,” Wiz wrote. [Full disclosure: Wiz is currently an advertiser on this website.]KrebsOnSecurity sought comment on the report from DeepSeek and from Apple. This story will be updated with any substantive replies.]]></content:encoded></item><item><title>The right to repair supports more than just sustainability and affordability</title><link>http://www.fsf.org/blogs/community/the-right-to-repair-supports-software-freedom</link><author></author><category>linux</category><pubDate>Thu, 6 Feb 2025 20:40:00 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[The right to repair is one of four pillars supporting software freedom  ]]></content:encoded></item><item><title>Protecting Free Speech in Texas: We Need To Stop SB 336</title><link>https://www.eff.org/deeplinks/2025/02/protecting-free-speech-texas-we-need-stop-sb-336</link><author>Joe Mullin</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/anti-slapp-banner-2.jpg" length="" type=""/><pubDate>Thu, 6 Feb 2025 20:39:09 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[SB 336 Undermines Free Speech ProtectionsFighting To Protect Free Speech For Texans ]]></content:encoded></item><item><title>Mixing Rust and C in Linux Likened To Cancer By Kernel Maintainer</title><link>https://linux.slashdot.org/story/25/02/06/1830233/mixing-rust-and-c-in-linux-likened-to-cancer-by-kernel-maintainer?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>slashdot</category><category>linux</category><pubDate>Thu, 6 Feb 2025 19:22:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[A heated dispute has erupted in the Linux kernel community over the integration of Rust code, with kernel maintainer Christoph Hellwig likening multiple programming languages to "cancer" for the project's maintainability. The conflict centers on a proposed patch enabling Rust-written device drivers to access the kernel's DMA API, which Hellwig strongly opposed. While the dispute isn't about Rust itself, Hellwig argues that maintaining cross-language codebases severely compromises Linux's integrated nature. From a report: "Don't force me to deal with your shiny language of the day," he [Hellwig] wrote. "Maintaining multi-language projects is a pain I have no interest in dealing with. If you want to use something that's not C, be that assembly or Rust, you write to C interfaces and deal with the impedance mismatch yourself as far as I'm concerned." This resistance follows the September departure of Microsoft engineer Wedson Almeida Filho from the Rust for Linux project, citing "nontechnical nonsense."]]></content:encoded></item><item><title>Google finally shipped some fire…</title><link>https://www.youtube.com/watch?v=k9xbh9LUYn0</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/k9xbh9LUYn0?version=3" length="" type=""/><pubDate>Thu, 6 Feb 2025 18:26:18 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Deploy your app without complexity using Sevalla http://bit.ly/4hDju8W

Take a first look at Google Gemini 2.0 and see how it compares to models like DeepSeek R1 and OpenAI o3-mini. Learn how to use AI models like Gemini Flash to solve real-world problems. 

#tech #ai #thecodereport

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Gemini for PDF processing https://www.sergey.fyi/articles/gemini-flash-2
OpenAI o3 First Look https://youtu.be/PoeFxGzPpXE
DeepSeek R1 Fallout https://youtu.be/Nl7aCUsWykg

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Is Google Gemini better than OpenAI?
- How to use Gemini Flash
- Comparison of top LLMs
- o3 vs Gemini vs DeepSeek
- Latest trends in Google technology]]></content:encoded></item><item><title>GitHub Copilot: The agent awakens</title><link>https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/</link><author>Thomas Dohmke</author><category>official</category><pubDate>Thu, 6 Feb 2025 17:04:30 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[When we introduced GitHub Copilot back in 2021, we had a clear goal: to make developers’ lives easier with an AI pair programmer that helps them write better code. The name reflects our belief that artificial intelligence (AI) isn’t replacing the developer. Instead, it’s always on their side. And like any good first officer, Copilot can also fly by itself: for example, when providing pull request feedback, autofixing security vulnerabilities, or brainstorming on how to implement an issue.Today, we are upgrading GitHub Copilot with the force of even more agentic AI – introducing agent mode and announcing the General Availability of Copilot Edits, both in VS Code. We are adding Gemini 2.0 Flash to the model picker for all Copilot users. And we unveil a first look at Copilot’s new autonomous agent, codenamed Project Padawan. From code completions, chat, and multi-file edits to workspace and agents, Copilot puts the human at the center of the creative work that is software development. AI helps with the things you don’t want to do, so you have more time for the things you do.Agent mode available in preview 🤖GitHub Copilot’s new agent mode is capable of iterating on its own code, recognizing errors, and fixing them automatically. It can suggest terminal commands and ask you to execute them. It also analyzes run-time errors with self-healing capabilities.In agent mode, Copilot will iterate on not just its own output, but the result of that output. And it will iterate until it has completed all the subtasks required to complete your prompt. Instead of performing just the task you requested, Copilot now has the ability to infer additional tasks that were not specified, but are also necessary for the primary request to work. Even better, it can catch its own errors, freeing you up from having to copy/paste from the terminal back into chat.Here’s an example where GitHub Copilot builds a web app to track marathon training:To get started, you’ll need to download VS Code Insiders and then enable the agent mode setting for GitHub Copilot Chat:Then, when in the Copilot Edits panel, switch from Edit to Agent right next to the model picker:Agent mode will change the way developers work in their editor; and as such, we will bring it to all IDEs that Copilot supports. We also know that today’s Insiders build isn’t perfect, and welcome your feedback as we improve both VS Code and the underlying agentic technology in the coming months.Copilot Edits, now GA in VS Code 🎉Announced at GitHub Universe in October last year, Copilot Edits combines the best of Chat and Inline Chat with a conversational flow and the ability to make inline changes across a set of files that you manage. The feedbackyou provided in the past was instrumental in shipping this feature as GA in VS Code today. Thank you!In Copilot Edits you specify a set of files to be edited, and then use natural language to ask GitHub Copilot for what you need. Copilot Edits makes inline changes in your workspace, across multiple files, using a UI designed for fast iteration. You stay in the flow of your code while reviewing the suggested changes, accepting what works, and iterating with follow-up asks.Behind the scenes, Copilot Edits leverages a dual-model architecture to enhance editing efficiency and accuracy. First, a foundation language model considers a full context of the Edits session to generate initial edit suggestions. You can choose the foundation language model that you prefer between: OpenAI’s GPT-4o, o1, o3-mini, Anthropic’s Claude 3.5 Sonnet, and now, Google’s Gemini 2.0 Flash. For the optimal experience, we developed a speculative decoding endpoint, optimized for fast application of changes in files. The proposed edits from the foundation model are sent to the speculative decoding endpoint that will then propose those changes inline in the editor.Copilot Edits works because it puts you in control, from setting the right context to accepting changes. The experience is iterative: when the model gets it wrong, you can review changes across multiple files, accept good ones and iterate until, together with Copilot, you arrive at the right solution. After accepting changes, you can run the code to verify the changes and, when needed, undo in Copilot Edits to get back to a previous working state. Copilot Edits is in the Secondary Side Bar (default on the right) so that you can interact with views in the Primary Side Bar, such as the Explorer, Debug, or Source Control view, while you’re reviewing proposed changes. For example, you can have unit tests running in the Testing view on the left, while using the Copilot Edits view on the right, so that in every iteration you can verify if the changes Copilot Edits proposed are passing your unit tests.Using your voice is a natural experience while using Copilot Edits. Just talking to Copilot makes the back and forth smooth and conversational. It almost feels like interacting with a colleague with area expertise, using the same kind of iterative flow that you would use in real-life pair programming.Next on our roadmap is to improve the performance of the apply changes speculative decoding endpoint, support transitions into Copilot Edits from Copilot Chat by preserving context, suggest files to the working set, and allow you to undo suggested chunks. If you want to be among the first to get your hands on these improvements, make sure to use VS Code Insiders and the pre-release version of the GitHub Copilot Chat extension. To help improve the feature, please file issues in our repo.Beyond the GA in VS Code, Copilot Edits is now in preview for Visual Studio 2022.Project Padawan: SWE agents on GitHubWe’re excited to share a first look at our autonomous SWE agent and how we envision these types of agents will fit into the GitHub user experience. When the product we are building under the codename Project Padawan ships later this year, it will allow you to directly assign issues to GitHub Copilot, using any of the GitHub clients, and have it produce fully tested pull requests. Once a task is finished, Copilot will assign human reviewers to the PR, and work to resolve feedback they add. In a sense, it will be like onboarding Copilot as a contributor to every repository on GitHub. ✨Behind the scenes, Copilot automatically spins up a secure cloud sandbox for every task it’s assigned. It then asynchronously clones the repository, sets up the environment, analyzes the codebase, edits the necessary files, and builds, tests, and lints the code. Additionally, Copilot takes into account any discussion within the issue or PR, and any custom instruction within the repository, so it understands the full intent of its task, as well as the guidelines and conventions of the project.And just as we did with Copilot Extensions and the model picker in Copilot, we will also provide opportunities to integrate into this AI-native workflow and work closely with partners and customers in a tight feedback loop. We believe the end-state of Project Padawan will result in transforming how teams manage critical-yet-mundane tasks, such as fixing bugs or creating and maintaining automated tests. Because ultimately, it’s all about empowering developers by allowing them to focus on what matters, and letting copilots do the rest. And don’t worry. We will have patience, so the agent won’t turn to the dark side. 😉]]></content:encoded></item><item><title>5 tips for promoting your open source project</title><link>https://github.blog/open-source/maintainers/5-tips-for-promoting-your-open-source-project/</link><author>Klint Finley</author><category>official</category><pubDate>Thu, 6 Feb 2025 17:00:07 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[After months or years of hard work, you’ve just pushed your open source project to GitHub and made it public. Now it’s time to tell the world about it.Chances are you’d rather spend time writing code than getting the word out about your project. Maybe your project will go viral and you won’t have to spend much time on marketing. But chances are you’re going to need to do some work to build awareness, at least in the early days. Fortunately, there are plenty of people who have been down this path and are willing to help. In this article, experienced maintainers offer their advice on sharing open source projects with the world.Start with the obvious. Post to social media about your project. Submit it to Hacker News, Reddit, Product Hunt, and similar sites. Then keep an eye peeled for people who have the problem that you’re trying to solve. Respond to their posts and let them know you have a potential solution. Reach out to podcasts and YouTube channels. Submit talks to conferences. Offer to speak at meetups.Keep promoting your work as you improve the project. Remember that people want to hear about helpful tools that solve real problems, as long as you’re genuinely trying to help, and not just spamming your followers. You might not be comfortable with self promotion, but you need to promote your work to get it out there. “You shouldn’t feel icky about it,” Sidecar maintainer Aaron Francistold us in a Q&A. “You put a lot of time into making something helpful.”Focus on the problem your project solvesWhat should you say when you’re promoting your work? First and foremost, you need to know what problem your project solves and be able to communicate that to potential users as simply as possible. “One of the biggest mistakes I see is the use of too much technical terminology,” says Chakra UI maintainer Segun Adebayo. It might be tempting to talk about the technologies you’ve built your solution upon, or the latest buzzwords you think users might be interested in. Open source users are, after all, often your fellow developers and technical people. But it’s easy to go overboard and obscure the value of the project.For example, your project might make clever use of decentralized computing principles, points out Tasha Drew, co-chair for Kubernetes’ Working Group for Multi-tenancy, but what people really care about is why they should use it. “What’s the message you want people to take away from your webpage or your README? It’s probably not related to the theory behind the code,” she says.Use that core message everywhere: Social media posts and profiles, blog posts, tutorials, etc.Document, document, document!Getting someone’s attention is only one part of the battle. If you want people to actually use, share, and contribute to your project, you need clear, up-to-date documentation. “Write as much as you can stand to write,” Francis says. Not only will it make your user experience better, it might even improve your code. “If you find it’s hard to document a particular feature, that’s probably a sign that it’s too complicated and you need to simplify it,” he explains.Think beyond just documenting the code. You should provide things like quick starts, tutorials, and screencasts. “Video is really helpful for a lot of people,” Adebayo says. “People learn in different ways so it’s important to provide different types of content.”No matter how good your documentation is, people are still going to have questions—and, if you’re lucky, pull requests. It’s important to be responsive, especially when you’re just starting out. “Time is finite, we only get one life, so value those people who are willing to spend some of their precious resources on you,” Francis says. “That applies not just to people sending pull requests, but to people pointing out problems or making suggestions on social media as well.”That doesn’t mean you have to be on call 24/7 to provide an immediate reply to every single question and comment. But it does mean you shouldn’t let pull requests, issues, and comments sit for too long without a response. You have to let people know your project is active, and that you value their input. “It might be intimidating at first to interact with people you don’t know, but you have to do it if you want to grow,” says Adebayo. “This is a sure way to meet new people and make new friends that might be helpful to you in the future.”Invest time in onboarding contributorsYou need to document both how to use your project, and how to contribute to it. Create CONTRIBUTING.md and CODE_OF_CONDUCT.md files with your contribution guidelines and code of conduct. These let potential contributors know that you’re open to contributions and that you’ve put some thought into working with others. It’s especially helpful to provide a list of what you would, and would not, like potential contributors to help with.Remember that non-code contributions, like documentation, support, and graphic design, are a big part of any successful project. While these aren’t necessarily non-technical, you shouldn’t assume too much technical knowledge. “You want to make your language and project easy to understand so that people of various technical skill levels will be interested,” Drew says.Also be sure to take advantage of the “Help wanted” and “Good first issue” labels. These can help people who are looking for ways to contribute find your project.]]></content:encoded></item><item><title>Container query units: cqi and cqb</title><link>https://css-tricks.com/container-query-units-cqi-and-cqb/</link><author>Geoff Graham</author><category>frontend</category><pubDate>Thu, 6 Feb 2025 15:29:35 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[A little gem from Kevin Powell’s “HTML & CSS Tip of the Week” website, reminding us that using container queries opens up container query units for sizing things based on the size of the queried container. and  are similar to  and , but instead of caring about the viewport, they care about their containers size. is your inline-size unit (usually width in horizontal writing modes), while handles block-size (usually height).So,  is equivalent to 1% of the container’s  size, and  is equal to 1% of the container’s  size. I’d be remiss not to mention the  and  units, which evaluate either the container’s inline  block size. So, we could say  and that equals 50% of the container’s size, but it will look at both the container’s inline and block size, determine which is greater, and use that to calculate the final computed value.That’s a nice dash of conditional logic. It can help maintain proportions if you think the writing mode might change on you, such as moving from horizontal to vertical.]]></content:encoded></item><item><title>Underpinning advanced machine learning models with GPUs</title><link>https://www.datasciencecentral.com/underpinning-advanced-machine-learning-models-with-gpus/</link><author>Jelani Harper</author><category>dev</category><category>ai</category><pubDate>Thu, 6 Feb 2025 15:20:05 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Although it’s rarely publicized in the media, not everything about deploying—and certainly not about training or fine-tuning—advanced machine learning models are readily accessible through an API. For certain implementations, the success of enterprise-scale applications of language models hinges on hardware, supporting infrastructure, and other practicalities that require more than just a cloud service provider. Graphics… Read More »]]></content:encoded></item><item><title>Optical Approach Creates Fast-Switching Spintronics</title><link>https://spectrum.ieee.org/spintronics-2670991844</link><author>Sidney Perkowitz</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjEyODU3NC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc0NTU5MjE5Nn0.kAYyd-V3ntwwqHF0Bs8BoTts_4lUv6BSUj6QPPpC7rU/image.jpg?width=600" length="" type=""/><pubDate>Thu, 6 Feb 2025 15:00:06 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The technique created stable magnetic states measured in milliseconds]]></content:encoded></item><item><title>Design a Real-Time Gaming Leaderboard - System Design Interview</title><link>https://blog.algomaster.io/p/design-real-time-gaming-leaderboard</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc07561-4165-42b9-a375-caaa33a2b6e3_2208x1668.png" length="" type=""/><pubDate>Thu, 6 Feb 2025 14:42:35 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[A  is a ranked list of players, typically sorted by a specific metric such as score, points or level. In a  leaderboard, updates happen almost instantly:A player’s score changes (e.g., after scoring a point or defeating an opponent).The system updates that player’s rank immediately.Other players can see the updated position without waiting or refreshing.This real-time aspect makes the user experience more dynamic and engaging. However, it also introduces significant , such as:Efficiently retrieving Top-N players (e.g., Top 10 or Top 100).Allowing players to quickly find their own rank without scanning the entire leaderboard.In this article, we will explore how to design a , , and  that can support above queries and enhance the user experience.Before diving into the design, lets clearly define the functional and non-functional requirements of our real-time gaming leaderboard..: Display top N players (e.g., top 10, top 100) on the leaderboard and update it in real-time.: Allow a player to query their current rank without scanning the entire leaderboard. Provide the ability to retrieve a “slice” of the leaderboard around a specific player (e.g., ranks 45 to 55 if the player is rank 50). Players can view past game scores and historical leaderboards for previous matches.Non-Functional Requirements Score changes should reflect immediately in the leaderboard.: Leaderboard queries should return the results in milliseconds.: System should support thousands of concurrent players submitting scores and fetching rankings.Approach to Designing the SystemThe most challenging aspects of building a real-time leaderboard is . Choosing the right storage system is critical to ensuring that queries can be executed efficiently without performance bottlenecks.To simplify the design process, we will follow below approach:Clearly define the input/output structure of leaderboard queries and updates.Define the High-Level Architecture: Identify core system components and their interactions.Choose the appropriate storage model optimized for fast leaderboard lookups and real-time updates.To support real-time leaderboard operations, we define a set of  that allow players to update scores, retrieve rankings, and query nearby ranks efficiently.Updates a player's score incrementally: POST /leaderboard/score/update{
  "playerId": "player123",
  "scoreDelta": 50
}We will use relative score updates () rather than absolute updates.{
  "playerId": "player123",
  "updatedScore": 1000,
  "currentRank": 10
}Retrieves the top-N players from the leaderboard, ranked by their scores.: GET /leaderboard/top?n=10:  = number of top players to fetch (default 10, max 100, etc.){
  "leaderboardId": "global",
  "topPlayers": [
    { "playerId": "playerA", "score": 1500, "rank": 1 },
    { "playerId": "playerB", "score": 1490, "rank": 2 },
    // ...
  ]
}Allows a player to retrieve their current rank without scanning the entire leaderboard.: GET /leaderboard/rank/{playerId}{
  "playerId": "player123",
  "score": 1000,
  "rank": 10
}Retrieves players ranked around a given player, allowing for comparison with competitors of similar skill levels.: GET /leaderboard/nearby/{playerId}?range=5:  indicates how many ranks above and below to fetch (e.g., 5 above, 5 below).{
  "playerId": "player123",
  "startRank": 45,
  "endRank": 55,
  "players": [
    { "playerId": "playerX", "score": 1020, "rank": 44 },
    { "playerId": "player123", "score": 1000, "rank": 45 },
    { "playerId": "playerZ", "score": 995,  "rank": 46 },
    // ...
  ]
} If a player is ranked 50th, this API allows them to see players ranked 45-55 for a competitive comparison.2.5 WebSockets for Real-Time UpdatesWhile REST APIs are good for on-demand queries, WebSockets or Server-Sent Events (SSEs) can push real-time leaderboard updates to subscribed users.Players would subscribe to leaderboard updates, and receive updates instantly without polling.]]></content:encoded></item><item><title>OpenAI’s Deep Research: Unexpected Game Changer!</title><link>https://www.youtube.com/watch?v=Mhk-otOlo1c</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/Mhk-otOlo1c?version=3" length="" type=""/><pubDate>Thu, 6 Feb 2025 14:39:17 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

📝 Deep research:
https://openai.com/index/introducing-deep-research/

Lambda DeepSeek instructions: https://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/

- Open Deep Research: https://opendeepresearch.vercel.app/
- Revisiting the McKinley Tariff of 1890 through the Lens of Modern Trade Theory
https://kevinbryanecon.com/o3McKinley.pdf
- Deep Research by  Tyler Cowen February 4, 2025
https://marginalrevolution.com/marginalrevolution/2025/02/deep-research.html

Complex tax situation: https://x.com/PatriceBTC/status/1886529037474127951
Daily briefing: https://x.com/mckaywrigley/status/1886521397767467302
Paper: https://x.com/Afinetheorem/status/1886206439582015870/photo/1

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu]]></content:encoded></item><item><title>Ransomware payments declined in 2024 despite massive. well-known hacks</title><link>https://arstechnica.com/security/2025/02/ransomware-payments-declined-in-2024-despite-well-known-massive-hacks/</link><author>Lily Hay Newman, wired.com</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2024/06/ransomware-lock-1152x648.jpg" length="" type=""/><pubDate>Thu, 6 Feb 2025 14:21:08 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Yet beneath those headlines, the numbers tell a surprising story: Ransomware payments actually fell overall in 2024—and in the second half of the year dropped more precipitously than in any six-month period on record.Cryptocurrency tracing firm Chainalysis today released a portion of its annual crime report focused on tracking the ransomware industry, which found that ransomware victims’ extortion payments totaled $814 million in 2024, a drop of 35 percent compared to the record $1.25 billion that hackers extracted from ransomware victims the previous year. Breaking down the payments over the course of 2024 shows an even more positive trend: Hackers collected just $321 million from July through December compared to $492 million the previous half year, the biggest falloff in payments between two six-month periods that Chainalysis has ever seen.]]></content:encoded></item><item><title>The DeepSeek Series: A Technical Overview</title><link>https://martinfowler.com/articles/deepseek-papers.html</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Thu, 6 Feb 2025 14:17:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[The appearance of DeepSeek Large-Language Models has caused a lot of
      discussion and angst since their latest versions appeared at the beginning
      of 2025. But much of the value of DeepSeek's work comes from the papers
      they have published over the last year. 
      provides an overview of these papers, highlighting three main arcs in this
      research: a focus on improving cost and memory efficiency, the use of HPC
      Co-Design to train large models on limited hardware, and the development
      of emergent reasoning from large-scale reinforcement learning]]></content:encoded></item><item><title>@stevemcconnell578: Should Engineering Managers Code?</title><link>https://www.youtube.com/watch?v=iUraWhJUXHI</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/iUraWhJUXHI?version=3" length="" type=""/><pubDate>Thu, 6 Feb 2025 13:01:03 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[Check out the full version on our YouTube channel now! #GOTOcon #GOTObookclub #SoftwareEngineering #CodeComplete #Complexity #Creativity #Construx #Refactoring #Debugging #ComputerScience #SteveMcConnell #Programming #SoftwareConstruction #Agile #Waterfall #Viral #ViralShorts #GOTO

Full version available here:
https://youtu.be/STpbPXW9-pA

Steve McConnell - Author of “Code Complete” & CEO at Construx Software @stevemcconnell578 

RECOMMENDED BOOK
Steve McConnell • Code Complete • https://amzn.to/3Zgu9gr
Steve McConnell • Software Estimation • https://amzn.to/41ZbGHn
Steve McConnell • Rapid Development • https://amzn.to/3J6e5rl
Steve McConnell • Professional Software Development • https://amzn.to/3IVKUa8

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Quantum Key Distribution and the Path to Post-Quantum Computing</title><link>https://blogs.cisco.com/security/quantum-key-distribution-and-the-path-to-post-quantum-computing/</link><author>Mike Luken</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Thu, 6 Feb 2025 13:00:00 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[Learn what's available today for quantum-safe solutions, along with the viability of QKD, an emerging tech that spans the gap between the present and future. ]]></content:encoded></item><item><title>Quiz: Python &quot;for&quot; Loops: The Pythonic Way</title><link>https://realpython.com/quizzes/python-for-loop/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Thu, 6 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[By working through this quiz, you’ll revisit how to iterate over items in a data collection, how to use  for a predefined number of iterations, and how to use  for index-based iteration.]]></content:encoded></item><item><title>Quiz: How to Join Strings in Python</title><link>https://realpython.com/quizzes/python-join-string/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Thu, 6 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[By working through this quiz, you’ll review how to use  to combine strings with a specified separator and handle non-string data in iterables. You’ll also revisit how  compares to other concatenation methods and how it’s optimized in CPython.]]></content:encoded></item><item><title>Rust vs. Go (Golang): Performance (Only Standard Library)</title><link>https://www.youtube.com/watch?v=2e9U5sQ835Y</link><author>Anton Putra</author><category>dev</category><category>video</category><enclosure url="https://www.youtube.com/v/2e9U5sQ835Y?version=3" length="" type=""/><pubDate>Thu, 6 Feb 2025 10:08:02 +0000</pubDate><source url="https://www.youtube.com/channel/UCeLvlbC754U6FyFQbKc0UnQ">Anton Putra</source><content:encoded><![CDATA[Go vs Rust Performance Comparison.

🔴 To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)

🍿 Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&si=p-UOaVM_6_SFx52H

👋 AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra

▬▬▬▬▬ Experience & Location 💼 ▬▬▬▬▬
►  I’m a Senior Software Engineer at Juniper Networks (13+ years of experience)
►  Located in San Francisco Bay Area, CA (US citizen)

▬▬▬▬▬▬ Connect with me 👋 ▬▬▬▬▬▬
►  LinkedIn: https://www.linkedin.com/in/anton-putra
►  Twitter/X: https://twitter.com/antonvputra
►  GitHub: https://github.com/antonputra
►  Email: me@antonputra.com

▬▬▬▬▬▬▬ Source Code 📚 ▬▬▬▬▬▬▬
► Original Source Code: https://github.com/antonputra/tutorials/tree/242/lessons/242

PR to improve Rust - https://github.com/antonputra/tutorials/pull/422
PR to improve Rust - https://github.com/antonputra/tutorials/pull/423
PR to improve Rust - https://github.com/antonputra/tutorials/pull/424

#deno #bun #nodejs]]></content:encoded></item><item><title>Remembering the order of the train_test_split</title><link>https://www.youtube.com/watch?v=wb4bKSHiDik</link><author>probabl</author><category>dev</category><category>ai</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/wb4bKSHiDik?version=3" length="" type=""/><pubDate>Thu, 6 Feb 2025 10:06:54 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Probabl</source><content:encoded><![CDATA[The page for the `train_test_split` function is the most visited page on the scikit-learn website. The reason, we suspect, is because folks often forget the order of arrays that come out. We know that `X_train` comes first, but then what? Is it `y_train` or `X_test`? This video explains the reasoning and also does a deep dive on the implementation.

Website: https://probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Twitter: https://x.com/probabl_ai
Bluesky: https://bsky.app/profile/probabl.bsky.social
Discord: https://discord.probabl.ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

#probabl]]></content:encoded></item><item><title>Secure Communications in Embedded Systems with Ismael Valenzuela and John Wall</title><link>https://softwareengineeringdaily.com/2025/02/06/secure-communications-in-embedded-systems-with-ismael-valenzuela-and-john-wall/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=secure-communications-in-embedded-systems-with-ismael-valenzuela-and-john-wall</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED6211023223.mp3" length="" type=""/><pubDate>Thu, 6 Feb 2025 10:00:47 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[BlackBerry is a Canadian company known for its pivotal role in the smartphone market during the 2000s. Today, BlackBerry has adopted a major focus on cybersecurity.John Wall is the Chief Operating Officer and Head of Products, Engineering and Services at QNX, which is a division of BlackBerry. Ismael Valenzuela is the former Vice President of Threat Research and Intelligence at BlackBerry, where he led threat research, intelligence, and defensive innovation. John and Ismael join the podcast to talk about cybersecurity at Blackberry, including secure communications in embedded systems.]]></content:encoded></item><item><title>Cloud Native Live: What&apos;s latest in KubeArmor v1.5</title><link>https://www.youtube.com/watch?v=OUNEu3h2V3c</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/OUNEu3h2V3c?version=3" length="" type=""/><pubDate>Thu, 6 Feb 2025 06:04:17 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>AI for observability</title><link>https://www.youtube.com/watch?v=IIz8Xpyebug</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/IIz8Xpyebug?version=3" length="" type=""/><pubDate>Thu, 6 Feb 2025 06:00:19 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Closing the Gap in Encryption on Mobile</title><link>https://www.eff.org/deeplinks/2025/02/closing-gap-encryption-mobile</link><author>Alexis Hancock</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/banner-encrypttheweb2.png" length="" type=""/><pubDate>Thu, 6 Feb 2025 02:53:38 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Oracle Starts Laying Mines In JavaScript Trademark Battle</title><link>https://developers.slashdot.org/story/25/02/05/2345236/oracle-starts-laying-mines-in-javascript-trademark-battle?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Thu, 6 Feb 2025 01:30:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[The Register's Thomas Claburn reports: Oracle this week asked the US Patent and Trademark Office (USPTO) to partially dismiss a challenge to its JavaScript trademark. The move has been criticized as an attempt to either stall or water down legal action against the database goliath over the programming language's name. Deno Land, the outfit behind the Deno JavaScript runtime, filed a petition with the USPTO back in November in an effort to make the trademarked term available to the JavaScript community. This legal effort is led by Node.js creator and Deno Land CEO Ryan Dahl, summarized on the JavaScript.tm website, and supported by more than 16,000 members of the JavaScript community. It aims to remove the fear of an Oracle lawsuit for using the term "JavaScript" in a conference title or business venture.
 
"Programmers working with JavaScript have formed innumerable community organizations," the website explains. "These organizations, like the standards bodies, have been forced to painstakingly avoid naming the programming language they are built around -- for example, JSConf. Sadly, without risking a legal trademark challenge against Oracle, there can be no 'JavaScript Conference' nor a 'JavaScript Specification.' The world's most popular programming language cannot even have a conference in its name." [...] In the initial trademark complaint, Deno Land makes three arguments to invalidate Oracle's ownership of "JavaScript." The biz claims that JavaScript has become a generic term; that Oracle committed fraud in 2019 when it applied to renew its trademark; and that Oracle has abandoned its trademark because it does not offer JavaScript products or services.
 
Oracle's motion on Monday focuses on the dismissal of the fraud claim, while arguing that it expects to prevail on the other two claims, citing corporate use of the trademarked term "in connection with a variety of offerings, including its JavaScript Extension Toolkit as well as developer's guides and educational resources, and also that relevant consumers do not perceive JavaScript as a generic term." The fraud claim follows from Deno Land's assertion that the material Oracle submitted in support of its trademark renewal application has nothing to do with any Oracle product. "Oracle, through its attorney, submitted specimens showing screen captures of the Node.js website, a project created by Ryan Dahl, Petitioner's Chief Executive Officer," the trademark cancellation petition says. "Node.js is not affiliated with Oracle, and the use of screen captures of the 'nodejs.org' website as a specimen did not show any use of the mark by Oracle or on behalf of Oracle."
 
Oracle contends that in fact it submitted two specimens to the USPTO -- a screenshot from the Node.js website and another from its own Oracle JavaScript Extension Toolkit. And this, among other reasons, invalidates the fraud claim, Big Red's attorneys contend. "Where, as here, Registrant 'provided the USPTO with [two specimens]' at least one of which shows use of the mark in commerce, Petitioner cannot plausibly allege that the inclusion of a second, purportedly defective specimen, was material," Oracle's motion argues, adding that no evidence of fraudulent intent has been presented. Beyond asking the court to toss the fraud claim, Oracle has requested an additional thirty days to respond to the other two claims.]]></content:encoded></item><item><title>Read-only accessibility in TypeScript</title><link>https://2ality.com/2025/02/typescript-readonly.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Thu, 6 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In this blog post, we look at how can make things “read-only” in TypeScript – mainly via the keyword .]]></content:encoded></item><item><title>Running sklearn models in Zig</title><link>https://zig.news/vinybrasil/running-sklearn-models-in-zig-2do1</link><author>Vinícyus Brasil</author><category>dev</category><category>zig</category><pubDate>Wed, 5 Feb 2025 22:51:00 +0000</pubDate><source url="https://zig.news/">Zig News</source><content:encoded><![CDATA[Scikit-learn models became the industry standard for creating machine learning models. Although the library offers many conveniences, one becomes tied to Python APIs for serving such models. These frameworks (such as FastAPI, for example) have various scalability issues, as Anton demonstrates in this video.
To overcome this limitation, one solution is to use Python's C library to run the models, while the rest of the API can be built using another framework.In this blogpost we'll implement this strategy. We'll create an application that uses Python's C API to run the models with the Zap library handling the requests, which is a Zig blazing fast microframework for web applications. The full code to the post can be found in this repository and this post was originally posted on my blog.
  
  
  Creating the model and the shared object
So let's say we got this simple script called  that creates a Logistic Regression, in which the model takes a numpy array with two dimensions and returns the value 1 or 0.  The model is saved in a pickle file called .The full library we are going to create just loads the pickle file and returns it in a list. It will attend by the name of  and it  module have the following code:Quite simple, right? The next step is to compile it to a shared object, the kind of file that contains the code we just wrote. 
Theoretically this step is not needed but I think its way easier to work with the library this way. To compile the library we can use Cython. The following  file will translate our library to C and generate the shared object.Note that both of the files generated, libpredict.so and model.pkl, need to be moved to the same folder where we'll build the zig executable.
  
  
  Running Python code in Zig
To interact with the Python C API, we gotta load the header file that includes the Python functions. The  file starts with:The syntax to call Python is in this documentation. For exemple,
to print a string, we first need to initialize the interpreter and use the function PyRun_SimpleString The main.zig file will start this way. First, we initialize the interpreter. Then, load the module  and extracts from it a pointer to the function  from the . To call the function we need to convert the zig array  to a Python List also using the C API and that's the reason for the function  to exist. Finally, the function can be called using the converted list.Note the loadMod() function appends the current path and from it import the  file so the loadFunc function can link a pointer to the predict function.The prepareList function just creates a Python list with PyList_New and appends to it the values from the  arrayh.The evaluteResult function iterates through all of the values of the Python function's response and the returns the last one. I've written the code this way because maybe what you are interested is the return of the predict_proba() function of the sklearn model, so it's easier to adapt the code.
  
  
  Creating the API with Zap
I've chosen Zap because it's a  micro webframework that can handle a lot of requests simultaneously and also it's quite intuitive.To create the API, we need a Router object to map the routes to the functions that handle the requests. Our only route here is the  that receives a json payload with the fields  and  to send to the Python function. We'll also need a HttpListener object to listen to the requests on the port 3000. The struct PredictorPackage is where all of the handling of the request is made;Inside the PredictorPackage struct, the function predictValue captures the body of the request, parses it with std.json.Parsed and then calls the Python function with it. Then, it stringify it and responds the request with a resultPrediction object.After building the project () and moving the libpredict.so and the model.pkl to the same folder as the executable,
we can test the API with cURL:curl  POST http://localhost:3000/predict It should return the following response:And that's it. Fell free to contact me via Linkedin or to open a PR on the GitHub repo if you find something wrong. Keep on learning :D]]></content:encoded></item><item><title>Docker Bake is Now Generally Available in Docker Desktop 4.38!</title><link>https://www.docker.com/blog/ga-launch-docker-bake/</link><author>Colin Hemmings</author><category>docker</category><category>devops</category><pubDate>Wed, 5 Feb 2025 21:47:14 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[We’re excited to announce the General Availability of  with Docker Desktop 4.38! This powerful build orchestration tool takes the hassle out of managing complex builds and offers simplicity, flexibility, and performance for teams of all sizes.Docker Bake is an orchestration tool that streamlines Docker builds, similar to how Compose simplifies managing runtime environments. With Bake, you can define build stages and deployment environments in a declarative file, making complex builds easier to manage. It also leverages BuildKit’s parallelization and optimization features to speed up build times.While Dockerfiles are excellent for defining image build steps, teams often need to build multiple images and execute helper tasks like testing, linting, and code generation. Traditionally, this meant juggling numerous  commands with their own options and arguments – a tedious and error-prone process.Bake changes the game by introducing a declarative file format that encapsulates all options and image dependencies, referred to as . Additionally, Bake’s ability to parallelize and deduplicate work ensures faster and more efficient builds.Challenges with complex Docker Build configuration:Managing long, complex build commands filled with countless flags and environment variables.Tedious workflows for building multiple images.Difficulty declaring builds for specific targets or environments.Requires a script or 3rd-party tool to make things manageableDocker Bake tackles these challenges with a better way to manage complex builds with a simple, declarative approach.Key benefits of Docker Bake: Replace complex chains of Docker build commands and scripts with a single  command while maintaining clear, version-controlled configuration files that are easy to understand and modify.: Express sophisticated build logic through HCL syntax and matrix builds, enabling dynamic configurations that adapt to different environments and requirements while supporting custom functions for advanced use cases.: Maintain standardized build configurations across teams and environments through version-controlled files and inheritance patterns, eliminating environment-specific build issues and reducing configuration drift.: Automatically parallelize independent builds and eliminate redundant operations through context deduplication and intelligent caching, dramatically reducing build times for complex multi-image workflows.: One simple Docker buildx bake command to replace all the flags and environment variables.Use cases for Docker Bake1. Monorepo and Image BakeryDocker Bake can help developers efficiently manage and build multiple related Docker images from a single source repository. Plus, they can leverage shared configurations and automated dependency handling to enforce organizational standards. Teams can maintain consistent build logic across dozens or hundreds of microservices in a single repository, reducing configuration drift and maintenance overhead. Shared base images and contexts are automatically deduplicated, dramatically reducing build times and storage costs. Enforce organizational standards through inherited configurations, ensuring all services follow security, tagging, and testing requirements. A single source of truth for build configurations makes it easier to implement organization-wide changes like base image updates or security patches.Docker Bake provides seamless compatibility with existing docker-compose.yml files, allowing direct use of your current configurations. Existing Compose users are able to get started using Bake with minimal effort. Teams can incrementally adopt advanced build features while still leveraging their existing compose workflows and knowledge. Use the same configuration for both local development (via compose) and production builds (via Bake), eliminating “works on my machine” issues.: Access powerful features like matrix builds and HCL expressions while maintaining compatibility with familiar compose syntax.: Seamlessly integrate with existing CI/CD pipelines that already understand compose files while adding Bake’s advanced build capabilities.3. Complex build configurationsCross-Platform Compatibility: Matrix builds enable teams to efficiently manage builds across multiple architectures, OS versions, and dependency combinations from a single configuration. HCL expressions allow builds to adapt to different environments, git branches, or CI variables without maintaining multiple configurations. Custom functions enable sophisticated logic for things like version calculation, tag generation, and conditional builds based on git history. Variable validation and inheritance ensure consistent configuration across complex build scenarios, reducing errors and maintenance burden. Groups and targets help organize large-scale build systems with dozens or hundreds of permutations, making them manageable and maintainable.With Bake-optimized builds as the foundation, developers can achieve more efficient Docker Build Cloud performance and faster builds.Enhanced Docker Build Cloud Performance: Instantly parallelize matrix builds across cloud infrastructure, turning hour-long build pipelines into minutes without managing build infrastructure. Leverage Build Cloud’s distributed caching and deduplication to dramatically reduce bandwidth usage and build times, which is especially valuable for remote teams. Save cost with DBC  Bake’s precise target definitions mean you only consume cloud resources for exactly what needs to be built. Teams can run complex multi-architecture builds without powerful local machines, enabling development from any device while maintaining build performance. Offload resource-intensive builds from CI runners to Build Cloud, reducing CI costs and queue times while improving reliability.What’s New in Bake for GA?Docker Bake has been an experimental feature for several years, allowing us to refine and improve it based on user feedback. So, there is already a strong set of ingredients that users love, such as targets and groups, variables, HCL Expression Support, inheritance capabilities, matrix targets, and additional contexts. With this GA release, Bake is now ready for production use, and we’ve added several enhancements to make it more efficient, secure, and easier to use:Deduplicated Context Transfers Significantly speeds up build pipelines by eliminating redundant file transfers when multiple targets share the same build context. Enhances security and resource management by providing fine-grained control over what capabilities and resources builders can access during the build process. Simplifies configuration management by allowing teams to define reusable attribute sets that can be mixed, matched, and overridden across different targets. Prevents wasted time and resources by catching configuration errors before the actual build process begins.Deduplicate context transfersWhen you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This can significantly impact build time, depending on your build configuration.Previously, the workaround required users to define a named context that loads the context files and then have each target reference the named context. But with Bake, this will be handled automatically now.Bake can automatically deduplicate context transfers from targets sharing the same context. When you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This more efficient approach leads to much faster build time. Read more about how to speed up your build time in our docs. Bake now includes entitlements to control access to privileged operations, aligning with Build. This prevents unintended side effects and security risks. If Bake detects a potential issue — like a privileged access request or an attempt to access files outside the current directory — the build will fail unless explicitly allowed.To be consistent, the Bake command now supports the flag to grant access to additional entitlements. The following entitlements are currently supported for Bake.Build equivalents
 Allows executions with host networking.--allow security.insecure Allows executions without sandbox. (i.e. —privileged)File system: Grant filesystem access for builds that need access files outside the working directory. This will impact context, output, cache-from, cache-to, dockerfile, secret Grant read and write access to files outside the working directory. Grant read access to files outside the working directory.--allow fs.write=<path|*>  Grant write access to files outside the working directory.ssh
– Allows exposing SSH agent.Several attributes previously had to be defined in CSV (e.g. ). These were challenging to read and couldn’t be easily overridden. The following can now be defined as structured objects:target "app" {
		attest = [
			{ type = "provenance", mode = "max" },
			{ type = "sbom", disabled = true}
		]

		cache-from = [
			{ type = "registry", ref = "user/app:cache" },
			{ type = "local", src = "path/to/cache"}
		]

		cache-to = [
			{ type = "local", dest = "path/to/cache" },
		]

		output = [
			{ type = "oci", dest = "../out.tar" },
			{ type = "local", dest="../out"}
		]

		secret = [
			{ id = "mysecret", src = "path/to/secret" },
			{ id = "mysecret2", env = "TOKEN" },
		]

		ssh = [
			{ id = "default" },
			{ id = "key", paths = ["path/to/key"] },
		]
}As such, the attributes are now composable. Teams can mix, match, and override attributes across different targets which simplifies configuration management. target "app-dev" {
    attest = [
			{ type = "provenance", mode = "min" },
			{ type = "sbom", disabled = true}
		]
  }

  target "app-prod" {
    inherits = ["app-dev"]

    attest = [
			{ type = "provenance", mode = "max" },
		]
  }
Bake now supports validation for variables similar to Terraform to help developers catch and resolve configuration errors early. The GA for Bake also supports the following use cases.To verify that the value of a variable conforms to an expected type, value range, or other condition, you can define custom validation rules using the  block.variable "FOO" {
  validation {
    condition = FOO != ""
    error_message = "FOO is required."
  }
}

target "default" {
  args = {
    FOO = FOO
  }
}
To evaluate more than one condition, define multiple  blocks for the variable. All conditions must be true.variable "FOO" {
  validation {
    condition = FOO != ""
    error_message = "FOO is required."
  }
  validation {
    condition = strlen(FOO) > 4
    error_message = "FOO must be longer than 4 characters."
  }
}

target "default" {
  args = {
    FOO = FOO
  }
}
Dependency on other variablesYou can reference other Bake variables in your condition expression, enabling validations that enforce dependencies between variables. This ensures that dependent variables are set correctly before proceeding.variable "FOO" {}
variable "BAR" {
  validation {
    condition = FOO != ""
    error_message = "BAR requires FOO to be set."
  }
}

target "default" {
  args = {
    BAR = BAR
  }
}
In addition to updating the Bake configuration, we’ve added a new –list option. Previously, if you were unfamiliar with a project or wanted a reminder of the supported targets and variables, you would have to read through the file. Now, the list option will allow you to quickly query a list of them. It also supports the JSON format option if you need programmatic access.Quickly get a list of the targets available in your Bake configuration.docker buildx bake --list targetsdocker buildx bake --list type=targets,format=jsonGet a list of variables available for your Bake configuration.docker buildx bake --list variablesdocker buildx bake --list type=variables,format=jsonThese improvements build on a powerful feature set, ensuring Bake is both reliable and future-ready.Get started with Docker BakeReady to simplify your builds? Update to Docker Desktop 4.38 today and start using Bake. With its declarative syntax and advanced features, Docker Bake is here to help you build faster, more efficiently, and with less effort.Explore the documentation to learn how to create your first Bake file and experience the benefits of streamlined builds firsthand.Let’s bake something amazing together!]]></content:encoded></item><item><title>Docker Desktop 4.38: New AI Agent, Multi-Node Kubernetes, and Bake in GA</title><link>https://www.docker.com/blog/docker-desktop-4-38/</link><author>Yiwen Xu</author><category>docker</category><category>devops</category><pubDate>Wed, 5 Feb 2025 21:42:31 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[At Docker, we’re committed to simplifying the developer experience and empowering enterprises to scale securely and efficiently. With the Docker Desktop 4.38 release, teams can look forward to improved developer productivity and enterprise governance. We’re excited to announce the General Availability of Bake, a powerful feature for optimizing build performance and multi-node Kubernetes testing to help teams “shift left.” We’re also expanding availability for several enterprise features designed to boost operational efficiency. And last but not least, Docker AI Agent (formerly Project: Agent Gordon) is now in Beta, delivering intelligent, real-time Docker-related suggestions across Docker CLI, Desktop, and Hub. It’s here to help developers navigate Docker concepts, fix errors, and boost productivity.Docker’s AI Agent boosts developer productivity  We’re thrilled to introduce Docker AI Agent (also known as Project: Agent Gordon) — an embedded, context-aware assistant seamlessly integrated into the Docker suite. Available within Docker Desktop and CLI, this innovative agent delivers real-time, tailored guidance for tasks like container management and Docker-specific troubleshooting — eliminating disruptive context-switching. Docker AI agent can be used for every Docker-related concept and technology, whether you’re getting started, optimizing an existing Dockerfile or Compose file, or understanding Docker technologies in general. By addressing challenges precisely when and where developers encounter them, Docker AI Agent ensures a smoother, more productive workflow. The first iteration of Docker’s AI Agent is now available in Beta for all signed-in usersThe agent is disabled by default, so user activation is required. Read more about Docker’s New AI Agent and how to use it to accelerate developer velocity here. Figure 1: Asking questions to Docker AI Agent in Docker DesktopSimplify build configurations and boost performance with Docker BakeDocker Bake is an orchestration tool that simplifies and speeds up Docker builds. After launching as an experimental feature, we’re thrilled to make it generally available with exciting new enhancements.While Dockerfiles are great for defining build steps, teams often juggle docker build commands with various options and arguments — a tedious and error-prone process. Bake changes the game by introducing a declarative file format that consolidates all options and image dependencies (also known as targets) in one place. No more passing flags to every build command! Plus, Bake’s ability to parallelize and deduplicate work ensures faster and more efficient builds.Key benefits of Docker Bake Abstract complex build configurations into one simple command. Write build configurations in a declarative syntax, with support for custom functions, matrices, and more. Share and maintain build configurations effortlessly across your team. Bake parallelizes multi-image workflows, enabling faster and more efficient builds.Developers can simplify multi-service builds by integrating Bake directly into their Compose files — Bake supports Compose files natively. It enables easy, efficient building of multiple images from a single repository with shared configurations. Plus, it works seamlessly with Docker Build Cloud locally and in CI. With Bake-optimized builds as the foundation, developers can achieve more efficient Docker Build Cloud performance and faster builds.Shift Left with Multi-Node Kubernetes testing in Docker DesktopIn today’s complex production environments, “shifting left”  is more essential than ever. By addressing concerns earlier in the development cycle, teams reduce costs and simplify fixes, leading to more efficient workflows and better outcomes. That’s why we continue to bring new features and enhancements to integrate feedback directly into the developer’s inner loopDocker Desktop now includes Multi-Node Kubernetes integration, enabling easier and extensive testing directly on developers’ machines. While single-node clusters allow for quick verification of app deployments, they fall short when it comes to testing resilience and handling the complex, unpredictable issues of distributed systems. To tackle this, we’re updating our Kubernetes distribution with  — a lightweight, fast, and user-friendly solution for local test and multi-node cluster simulations.Figure 2: Selecting Kubernetes version and cluster number for testingMulti-node cluster support: Replicate a more realistic production environment to test critical features like node affinity, failover, and networking configurations.Multiple Kubernetes versions: Easily test across different Kubernetes versions, which is a must for validating migration paths. Since  is an actively maintained open-source project, developers can update to the latest version on demand without waiting for the next Docker Desktop release.Head over to our documentation to discover how to use multi-node Kubernetes clusters for local testing and simulation.General availability of administration features for Docker Business subscriptionWith the Docker Desktop 4.36 release, we introduced Beta enterprise admin tools to streamline administration, improve security, and enhance operational efficiency. And the feedback from our Early Access Program customers has been overwhelmingly positive. For instance, enforcing sign-in with macOS configuration files and across multiple organizations makes deployment easier and more flexible for large enterprises. Also, the PKG installer simplifies managing large-scale Docker Desktop deployments on macOS by eliminating the need to convert DMG files into PKG first.Today, the features below are now available to all Docker Business customers.  Looking ahead, Docker is dedicated to continue expanding enterprise administration capabilities. Stay tuned for more announcements!Docker Desktop 4.38 reinforces our commitment to simplifying the developer experience while equipping enterprises with robust tools. With Bake now in GA, developers can streamline complex build configurations into a single command. The new Docker AI Agent offers real-time, on-demand guidance within their preferred Docker tools. Plus, with Multi-node Kubernetes testing in Docker Desktop, they can replicate realistic production environments and address issues earlier in the development cycle. Finally, we made a few new admin tools available to all our Business customers, simplifying deployment, management, and monitoring. We look forward to how these innovations accelerate your workflows and supercharge your operations! ]]></content:encoded></item><item><title>Paraguay’s Broadband Providers Continue to Struggle to Attain Best Practices at Protecting Users’ Data</title><link>https://www.eff.org/deeplinks/2025/02/paraguays-broadband-providers-continue-struggle-attain-best-practices-protecting</link><author>Karen Gullo</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/qttd-banner_0.png" length="" type=""/><pubDate>Wed, 5 Feb 2025 21:38:29 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Introducing the Beta Launch of Docker’s AI Agent, Transforming Development Experiences</title><link>https://www.docker.com/blog/beta-launch-docker-ai-agent/</link><author>Jean-Laurent de Morlhon</author><category>docker</category><category>devops</category><pubDate>Wed, 5 Feb 2025 21:36:29 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[For years, Docker has been an essential partner for developers, empowering everyone from small startups to the world’s largest enterprises. Today, AI is transforming organizations across industries, creating opportunities for those who embrace it to gain a competitive edge. Yet, for many teams, the question of where to start and how to effectively integrate AI into daily workflows remains a challenge. True to its developer-first philosophy, Docker is here to bridge that gap.We’re thrilled to introduce the beta launch of Docker AI Agent (also known as Project: Gordon)—an embedded, context-aware assistant seamlessly integrated into the Docker suite. Available within Docker Desktop and CLI, this innovative agent delivers tailored guidance for tasks like building and running containers, authoring Dockerfiles and Docker-specific troubleshooting—eliminating disruptive context-switching. By addressing challenges precisely when and where developers encounter them, Docker AI Agent ensures a smoother, more productive workflow.As the AI Agent evolves, enterprise teams will unlock even greater capabilities, including customizable features that streamline collaboration, enhance security, and help developers work smarter. With the Docker AI Agent, we’re making Docker even easier and more effective to use than it has ever been — AI accessible, actionable, and indispensable for developers everywhere.How Docker’s AI Agent Simplifies Development Challenges  Developing in today’s fast-paced tech landscape is increasingly complex, with developers having to learn an ever growing number of tools, libraries and technologies.By integrating a GenAI Agent into Docker’s ecosystem, we aim to provide developers with a powerful assistant that can help them navigate these complexities. The Docker AI Agent helps developers accelerate their work, providing real-time assistance, actionable suggestions, and automations that remove many of the manual tasks associated with containerized application development. Delivering the most helpful, expert-level guidance on Docker-related questions and technologies, Gordon serves as a powerful support system for developers, meeting them exactly where they are in their workflow. If you’re a developer who favors graphical interfaces, Docker Desktop AI UI will help you navigate container running issues, image size management and more generic Dockerfile oriented questions. If you’re a command line interface user, you can call, and share context with the agent directly in your favorite terminal.So what can Docker’s AI Agent do today? We’re delivering an expert assistant for every Docker-related concept and technology, whether it’s getting started, optimizing an existing Dockerfile or Compose file, or understanding Docker technologies in general. With Docker AI Agent, you also have the ability to delegate actions while maintaining full control and review over the process.A first example, if you want to run a container from an image, our agent can suggest the most appropriate  command tailored to your needs. This eliminates the guesswork or the need to search Docker Hub, saving you time and effort. The result combines a custom prompt, live data from Docker Hub, Docker container expertise and private usage insights, unique to Docker Inc.We’ve intentionally designed the output to be concise and actionable, avoiding the overwhelming verbosity often associated with AI-generated commands. We also provide sources for most of the AI agent recommendations, pointing directly to our documentation website. Our goal is to continuously refine this experience, ensuring that Docker’s AI Agent always provides the best possible command based on your specific local context.Beside helping you run containers, the Docker AI Agent can today:Explain, Rate and optimize Dockerfile leveraging the latest version of Docker.Help you run containers in an effective, concise way, leveraging the local context (checking port already used or volumes).Answers any docker related questions with the latest version of our documentations for our whole tool suite, and as such is able to answer any kind of questions on Docker tools and technologies.Containerize a software project helping you run your software in containers.Helps on Docker related Github Actions.Suggest fix when a container is failing to start in Docker Desktop.Provides contextual help for containers, images and volumes.Can augment its answer with per directory MCP servers (see doc).For the node expert, in the above screenshot the AI is recommending node 20.12 which is not the latest version but the one the AI found in the .With every future version of Docker Desktop and thanks to the feedback that you provide, the agent will be able to do so much more in the future.How can you try Docker AI Agent? This first beta release of Docker AI Agent is now progressively available for By default, the Docker AI agent is disabled. To enable it you will need to follow the steps below. Here’s how to get started:Install or update to the latest release of Docker Desktop 4.38Enable Docker AI into Docker Desktop Settings -> Features in DevelopmentFor the best experience, ensure the Docker terminal is enabled by going to Settings → General* If you’re a business subscriber, your Administrator needs to enable the Docker AI Agent for the organization first. This can be done through the Settings Management. If this is your case, feel free to contact us through the support  for further information.Docker Agent’s Vision for 2025By 2025, we aim to expand the agent’s capabilities with features like customizing your experience with more context from your registry, enhanced GitHub Copilot integrations, and deeper presence across the development tools you already use. With regular updates and your feedback, Docker AI Agent is being built to become an indispensable part of your development process.For now this beta is the start of an exciting evolution in how we approach developer productivity. Stay tuned for more updates as we continue to shape a smarter, more streamlined way to build, secure, and ship applications. We want to hear from you, if you like or want more information you can contact us.]]></content:encoded></item><item><title>7-Zip 0-day was exploited in Russia’s ongoing invasion of Ukraine</title><link>https://arstechnica.com/security/2025/02/7-zip-0-day-was-exploited-in-russias-ongoing-invasion-of-ukraine/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2023/09/code-vulnerability-security-1000x648.jpg" length="" type=""/><pubDate>Wed, 5 Feb 2025 21:05:37 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Researchers said they recently discovered a zero-day vulnerability in the 7-Zip archiving utility that was actively exploited as part of Russia's ongoing invasion of Ukraine.The vulnerability allowed a Russian cybercrime group to override a Windows protection designed to limit the execution of files downloaded from the Internet. The defense is commonly known as MotW, short for Mark of the Web. It works by placing a “Zone.Identifier” tag on all files downloaded from the Internet or from a networked share. This tag, a type of NTFS Alternate Data Stream and in the form of a ZoneID=3, subjects the file to additional scrutiny from Windows Defender SmartScreen and restrictions on how or when it can be executed.There’s an archive in my archiveThe 7-Zip vulnerability allowed the Russian cybercrime group to bypass those protections. Exploits worked by embedding an executable file within an archive and then embedding the archive into another archive. While the outer archive carried the MotW tag, the inner one did not. The vulnerability, tracked as CVE-2025-0411, was fixed with the release of version 24.09 in late November.]]></content:encoded></item><item><title>Victory! EFF Helps Defeat Meritless Lawsuit Against Journalist</title><link>https://www.eff.org/deeplinks/2025/02/victory-eff-helps-defeat-meritless-lawsuit-against-journalist</link><author>Tori Noble</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/press_freedom_img_v3.png" length="" type=""/><pubDate>Wed, 5 Feb 2025 20:37:28 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DDoSed by Policy: Website Takedowns and Keeping Information Alive</title><link>https://www.eff.org/deeplinks/2025/02/ddosed-policy-website-takedowns-and-keeping-information-alive</link><author>Rory Mir</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/icaan-censor-1.jpg" length="" type=""/><pubDate>Wed, 5 Feb 2025 20:21:51 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Build software that lasts! (Interview)</title><link>https://changelog.com/podcast/627</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/podcast/627/the-changelog-627.mp3" length="" type=""/><pubDate>Wed, 5 Feb 2025 20:15:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[After 30+ years in the software industry, Bert Hubert has experienced a lot. He founded PowerDNS, published articles for places like IETF / IEEE, and built his own parliament monitoring system. That just scratches the surface.Recently, Bert wrote about what it takes to build software for the long term. Let’s dig in.Changelog++ members get a bonus 10 minutes at the end of this episode and zero ads. Join today!Retool – The low-code platform for developers to build internal tools — Some of the best teams out there trust Retool…Brex, Coinbase, Plaid, Doordash, LegalGenius, Amazon, Allbirds, Peloton, and so many more – the developers at these teams trust Retool as the platform to build their internal tools. Try it free at retool.com/changelogTemporal – Build invincible applications. Manage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail. Register for Replay in London, March 3-5 to break free from the status quo.
DeleteMe – Text  to 64000 to join DeleteMe. They make it quick, easy, and safe to remove your personal data online.
]]></content:encoded></item><item><title>&apos;I&apos;m Done With Ubuntu&apos;</title><link>https://news.slashdot.org/story/25/02/05/1819218/im-done-with-ubuntu?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>slashdot</category><category>linux</category><pubDate>Wed, 5 Feb 2025 20:01:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Software developer and prolific blogger Herman Ounapuu, writing in a blog post: I liked Ubuntu. For a very long time, it was the sensible default option. Around 2016, I used the Ubuntu GNOME flavor, and after they ditched the Unity desktop environment, GNOME became the default option. 

I was really happy with it, both for work and personal computing needs. Estonian ID card software was also officially supported on Ubuntu, which made Ubuntu a good choice for family members. 

But then something changed. Ounapuu recounts how Ubuntu's bi-annual long-term support releases consistently broke functionality, from minor interface glitches to catastrophic system failures that left computers unresponsive. His breaking point came after multiple problematic upgrades affecting family members' computers, including one that rendered a laptop completely unusable during an upgrade from Ubuntu 20.04 to 22.04. Another incident left a relative's system with broken Firefox shortcuts and duplicate status bar icons after updating Lubuntu 18.04. 

Canonical's aggressive push of Snap packages has drawn particular criticism. The forced migration of system components from traditional Debian packages to Snaps resulted in compatibility issues, broken desktop shortcuts, and government ID card authentication failures. In one instance, he writes, a Snap-related bug in the GNOME desktop environment severely disrupted workplace productivity, requiring multiple system restarts to resolve. The author has since switched to Fedora, praising its implementation of Flatpak as a superior alternative to Snaps.]]></content:encoded></item><item><title>Free Software Awards: Choose your nominations by March 5</title><link>http://www.fsf.org/blogs/community/2025-awards-nominations-open</link><author></author><category>linux</category><pubDate>Wed, 5 Feb 2025 18:45:00 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[The time has come for free software community members to nominate individuals and projects for a Free Software Award.]]></content:encoded></item><item><title>Revolutionizing software testing: Introducing LLM-powered bug catchers</title><link>https://engineering.fb.com/2025/02/05/security/revolutionizing-software-testing-llm-powered-bug-catchers-meta-ach/</link><author></author><category>dev</category><category>official</category><pubDate>Wed, 5 Feb 2025 18:30:51 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Largest Bike Theft Network in the US</title><link>https://www.youtube.com/watch?v=is5me0BJwDQ</link><author>Jack Rhysider</author><category>Darknet diaries</category><category>infosec</category><category>video</category><category>podcast</category><enclosure url="https://www.youtube.com/v/is5me0BJwDQ?version=3" length="" type=""/><pubDate>Wed, 5 Feb 2025 17:00:57 +0000</pubDate><source url="https://www.youtube.com/channel/UCMIqrmh2lMdzhlCPK5ahsAg">Jack Rhysider</source><content:encoded><![CDATA[Largest Bike Theft Network in the US

#podcast #theft 

Listen to the full episode 🎙 Darknet Diaries Ep. 153: Bike Index]]></content:encoded></item><item><title>Some terminal frustrations</title><link>https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/</link><author>Julia Evans</author><category>blog</category><pubDate>Wed, 5 Feb 2025 16:57:00 +0000</pubDate><source url="https://jvns.ca/atom.xml">Julia Evans</source><content:encoded><![CDATA[What’s the most frustrating thing about using the terminal for you?1600 people answered, and I decided to spend a few days categorizing all the
responses. Along the way I learned that classifying qualitative data is not
easy but I gave it my best shot. I ended up building a custom
tool to make it faster to categorize
everything.As with all of my surveys the methodology isn’t particularly scientific. I just
posted the survey to Mastodon and Twitter, ran it for a couple of days, and got
answers from whoever happened to see it and felt like responding.Here are the top categories of frustrations!I think it’s worth keeping in mind while reading these comments that40% of people answering this survey have been using the terminal for 95% of people answering the survey have been using the terminal for at least 4 yearsThese comments aren’t coming from total beginners.Here are the categories of frustrations! The number in brackets is the number
of people with that frustration. I’m mostly writing this up for myself because
I’m trying to write a zine about the terminal and I wanted to get a sense for
what people are having trouble with.People talked about struggles remembering:the syntax for CLI tools like awk, jq, sed, etckeyboard shortcuts for tmux, text editing, etcThere are just so many little “trivia” details to remember for full
functionality. Even after all these years I’ll sometimes forget where it’s 2
or 1 for stderr, or forget which is which for  and .switching terminals is hard (91)People talked about struggling with switching systems (for example home/work
computer or when SSHing) and running into:OS differences in keyboard shortcuts (like Linux vs Mac)systems which don’t have their preferred text editor (“no vim” or “only vim”)different versions of the same command (like Mac OS grep vs GNU grep)a shell they aren’t used to (“the subtle differences between zsh and bash”)as well as differences inside the same system like pagers being not consistent
with each other (git diff pagers, other pagers).I got used to fish and vi mode which are not available when I ssh into
servers, containers.Lots of problems with color, like:programs setting colors that are unreadable with a light background colorfinding a colorscheme they like (and getting it to work consistently across different apps)color not working inside several layers of SSH/tmux/etcnot wanting color at all and struggling to turn it offThis comment felt relatable to me:Getting my terminal theme configured in a reasonable way between the terminal
emulator and fish (I did this years ago and remember it being tedious and
fiddly and now feel like I’m locked into my current theme because it works
and I dread touching any of that configuration ever again).Half of the comments on keyboard shortcuts were about how on Linux/Windows, the
keyboard shortcut to copy/paste in the terminal is different from in the rest
of the OS.Some other issues with keyboard shortcuts other than copy/paste:using  in a browser-based terminal and closing the windowthe terminal only supports a limited set of keyboard shortcuts (no
, no , no , lots of  shortcuts aren’t
possible like )the OS stopping you from using a terminal keyboard shortcut (like by default
Mac OS uses  for something else)issues using emacs in the terminalbackspace not working (2)other copy and paste issues (75)Aside from “the keyboard shortcut for copy and paste is different”, there were
a lot of OTHER issues with copy and paste, like:how tmux and the terminal emulator both do copy/paste in different waysdealing with many different clipboards (system clipboard, vim clipboard, the
“middle click” clipboard on Linux, tmux’s clipboard, etc) and potentially
synchronizing themrandom spaces added when copying from the terminalpasting multiline commands which automatically get run in a terrifying waywanting a way to copy text without using the mouseThere were lots of comments about this, which all came down to the same basic
complaint – it’s hard to discover useful tools or features! This comment kind of
summed it all up:How difficult it is to learn independently. Most of what I know is an
assorted collection of stuff I’ve been told by random people over the years.steep learning curve (44)A lot of comments about it generally having a steep learning curve. A couple of
example comments:After 15 years of using it, I’m not much faster than using it than I was 5 or
maybe even 10 years ago.That I know I could make my life easier by learning more about the shortcuts
and commands and configuring the terminal but I don’t spend the time because it
feels overwhelming.Some issues with shell history:history not being shared between terminal tabs (16)limits that are too short (4)history not being restored when terminal tabs are restoredlosing history because the terminal crashednot knowing how to search historyIt wasted a lot of time until I figured it out and still annoys me that
“history” on zsh has such a small buffer;  I have to type “history 0” to get
any useful length of history.documentation being generally opaquelack of examples in man pagesprograms which don’t have man pagesHere’s a representative comment:Finding good examples and docs. Man pages often not enough, have to wade
through stack overflowA few issues with scrollback:programs printing out too much data making you lose scrollback historyresizing the terminal messes up the scrollbackGUI programs that you start in the background printing stuff out that gets in
the way of other programs’ outputsWhen resizing the terminal (in particular: making it narrower) leads to
broken rewrapping of the scrollback content because the commands formatted
their output based on the terminal window width.Lots of comments about how the terminal feels hampered by legacy decisions and
how users often end up needing to learn implementation details that feel very
esoteric. One example comment:Most of the legacy cruft, it would be great to have a green field
implementation of the CLI interface.Lots of complaints about POSIX shell scripting. There’s a general feeling that
shell scripting is difficult but also that switching to a different less
standard scripting language (fish, nushell, etc) brings its own problems.Shell scripting. My tolerance to ditch a shell script and go to a scripting
language is pretty low. It’s just too messy and powerful. Screwing up can be
costly so I don’t even bother.Some more issues that were mentioned at least 10 times:(31) inconsistent command line arguments: is it -h or help or –help?(24) keeping dotfiles in sync across different systems(23) performance (e.g. “my shell takes too long to start”)(20) window management (potentially with some combination of tmux tabs, terminal tabs, and multiple terminal windows. Where did that shell session go?)(17) generally feeling scared/uneasy (“The debilitating fear that I’m going
to do some mysterious Bad Thing with a command and I will have absolutely no
idea how to fix or undo it or even really figure out what happened”)(16) terminfo issues (“Having to learn about terminfo if/when I try a new terminal emulator and ssh elsewhere.”)(16) lack of image support (sixel etc)(15) SSH issues (like having to start over when you lose the SSH connection)(15) various tmux/screen issues (for example lack of integration between tmux and the terminal emulator)(13) the terminal getting messed up for various reasons (pressing , ing a binary, etc)(12) quoting/escaping in the shell(11) various Windows/PowerShell issuesThere were also 122 answers to the effect of “nothing really” or “only that I
can’t do EVERYTHING in the terminal”Think I’ve found work arounds for most/all frustrationsI’m not going to make a lot of commentary on these results, but here are a
couple of categories that feel related to me:remembering syntax & history (often the thing you need to remember is something you’ve run before!)discoverability & the learning curve (the lack of discoverability is definitely a big part of what makes it hard to learn)“switching systems is hard” & “it feels outdated” (tools that haven’t really
changed in 30 or 40 years have many problems but they do tend to be always
 no matter what system you’re on, which is very useful and makes them
hard to stop using)Trying to categorize all these results in a reasonable way really gave me an
appreciation for social science researchers’ skills.]]></content:encoded></item><item><title>Gemini 2.0 is now available to everyone</title><link>https://deepmind.google/discover/blog/gemini-2-0-is-now-available-to-everyone/</link><author></author><category>dev</category><category>ai</category><enclosure url="https://lh3.googleusercontent.com/jOZg_YMBxrm-cz0kTZPehtQ29D90UZ21AXq9E54u3RZ67VEoXNy2KEsmD3oPgCDCIBw39ydczc8VQHH3QlUzIjdsgSPwIFdAEHXiHhsnUIKqmhppSQ=w528-h297-n-nu-rw" length="" type=""/><pubDate>Wed, 5 Feb 2025 16:00:00 +0000</pubDate><source url="https://deepmind.google/discover/blog/">Google Deep Mind blog</source><content:encoded><![CDATA[We’re announcing new updates to Gemini 2.0 Flash, plus introducing Gemini 2.0 Flash-Lite and Gemini 2.0 Pro Experimental.]]></content:encoded></item><item><title>GenAI Patterns: RAG Limitations and Hybrid Retriever</title><link>https://martinfowler.com/articles/gen-ai-patterns/#RagInPractice</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 5 Feb 2025 15:03:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[Today  and I outline four
      limitations to the simple RAG from yesterday, and the pattern that
      addresses the first of these: Hybrid Retriever. This tackles the
      inefficiencies of embeddings-based search by combining it with other
      search techniques.]]></content:encoded></item><item><title>Baseline Status in a WordPress Block</title><link>https://css-tricks.com/baseline-status-in-a-wordpress-block/</link><author>Geoff Graham</author><category>frontend</category><enclosure url="https://css-tricks.com/wp-content/uploads/2024/11/Screen-Recording-2024-11-05-at-2.56.06 PM.mp4" length="" type=""/><pubDate>Wed, 5 Feb 2025 14:58:18 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[Of course, we could simply drop the HTML component into the page. But I never know where we’re going to use something like this. The Almanac, obs. But I’m sure there are times where embedded it in other pages and posts makes sense.That’s exactly what WordPress blocks are good for. We can take an already reusable component and make it repeatable when working in the WordPress editor. So that’s what I did! That component you see up there is the  web component formatted as a WordPress block. Let’s drop another one in just for kicks.Pretty neat! I saw that Pawel Grzybek made an equivalent for Hugo. There’s an Astro equivalent, too. Because I’m fairly green with WordPress block development I thought I’d write a bit up on how it’s put together. There are still rough edges that I’d like to smooth out later, but this is a good enough point to share the basic idea.I used the  package to bootstrap and initialize the project. All that means is I ‘d into the  directory from the command line and ran the install command to plop it all in there.npm install @wordpress/create-blockThe  file is where the plugin is registered. And yes, it’s looks completely the same as it’s been for years, just not in a  file like it is for themes. The difference is that the  package does some lifting to register the widget so I don’t have to:<?php
/**
 * Plugin Name:       Baseline Status
 * Plugin URI:        https://css-tricks.com
 * Description:       Displays current Baseline availability for web platform features.
 * Requires at least: 6.6
 * Requires PHP:      7.2
 * Version:           0.1.0
 * Author:            geoffgraham
 * License:           GPL-2.0-or-later
 * License URI:       https://www.gnu.org/licenses/gpl-2.0.html
 * Text Domain:       baseline-status
 *
 * @package CssTricks
 */

if ( ! defined( 'ABSPATH' ) ) {
  exit; // Exit if accessed directly.
}

function csstricks_baseline_status_block_init() {
  register_block_type( __DIR__ . '/build' );
}
add_action( 'init', 'csstricks_baseline_status_block_init' );

?>The real meat is in  directory.The  package also did some filling of the blanks in the  file based on the onboarding process:{
  "$schema": "https://schemas.wp.org/trunk/block.json",
  "apiVersion": 2,
  "name": "css-tricks/baseline-status",
  "version": "0.1.0",
  "title": "Baseline Status",
  "category": "widgets",
  "icon": "chart-pie",
  "description": "Displays current Baseline availability for web platform features.",
  "example": {},
  "supports": {
    "html": false
  },
  "textdomain": "baseline-status",
  "editorScript": "file:./index.js",
  "editorStyle": "file:./index.css",
  "style": "file:./style-index.css",
  "render": "file:./render.php",
  "viewScript": "file:./view.js"
}Going off some tutorials published right here on CSS-Tricks, I knew that WordPress blocks render twice — once on the front end and once on the back end — and there’s a file for each one in the  folder: Handles the front-end view Handles the back-end viewThe front-end and back-end markupCool. I started with the  web component’s markup:<script src="https://cdn.jsdelivr.net/npm/baseline-status@1.0.8/baseline-status.min.js" type="module"></script>
<baseline-status featureId="anchor-positioning"></baseline-status>I’d hate to inject that  every time the block pops up, so I decided to enqueue the file conditionally based on the block being displayed on the page. This is happening in the main  file which I treated sorta the same way as a theme’s  file. It’s just where helper functions go.// ... same code as before

// Enqueue the minified script
function csstricks_enqueue_block_assets() {
  wp_enqueue_script(
    'baseline-status-widget-script',
    'https://cdn.jsdelivr.net/npm/baseline-status@1.0.4/baseline-status.min.js',
    array(),
    '1.0.4',
    true
  );
}
add_action( 'enqueue_block_assets', 'csstricks_enqueue_block_assets' );

// Adds the 'type="module"' attribute to the script
function csstricks_add_type_attribute($tag, $handle, $src) {
  if ( 'baseline-status-widget-script' === $handle ) {
    $tag = '<script type="module" src="' . esc_url( $src ) . '"></script>';
  }
  return $tag;
}
add_filter( 'script_loader_tag', 'csstricks_add_type_attribute', 10, 3 );

// Enqueues the scripts and styles for the back end
function csstricks_enqueue_block_editor_assets() {
  // Enqueues the scripts
  wp_enqueue_script(
    'baseline-status-widget-block',
    plugins_url( 'block.js', __FILE__ ),
    array( 'wp-blocks', 'wp-element', 'wp-editor' ),
    false,
  );

  // Enqueues the styles
  wp_enqueue_style(
    'baseline-status-widget-block-editor',
    plugins_url( 'style.css', __FILE__ ),
    array( 'wp-edit-blocks' ),
    false,
  );
}
add_action( 'enqueue_block_editor_assets', 'csstricks_enqueue_block_editor_assets' );The final result bakes the script directly into the plugin so that it adheres to the WordPress Plugin Directory guidelines. If that wasn’t the case, I’d probably keep the hosted script intact because I’m completely uninterested in maintaining it. Oh, and that csstricks_add_type_attribute() function is to help import the file as an ES module. There’s a  action available to hook into that should handle that, but I couldn’t get it to do the trick.With that in hand, I can put the component’s markup into a template. The  file is where all the front-end goodness resides, so that’s where I dropped the markup:<baseline-status
  <?php echo get_block_wrapper_attributes(); ?> 
  featureId="[FEATURE]">
</baseline-status>That  thing is recommended by the WordPress docs as a way to output all of a block’s information for debugging things, such as which features it ought to support.is a placeholder that will eventually tell the component which web platform to render information about. We may as well work on that now. I can register attributes for the component in :"attributes": { "showBaselineStatus": {
  "featureID": {
  "type": "string"
  }
},Now we can update the markup in  to echo the  when it’s been established.<baseline-status
  <?php echo get_block_wrapper_attributes(); ?> 
  featureId="<?php echo esc_html( $featureID ); ?>">
</baseline-status>There will be more edits to that markup a little later. But first, I need to put the markup in the  file so that the component renders in the WordPress editor when adding it to the page.<baseline-status { ...useBlockProps() } featureId={ featureID }></baseline-status> is the JavaScript equivalent of get_block_wrapper_attibutes() and can be good for debugging on the back end.At this point, the block is fully rendered on the page when dropped in! The problems are:It’s not passing in the feature I want to display.I’ll work on the latter first. That way, I can simply plug the right variable in there once everything’s been hooked up.One of the nicer aspects of WordPress DX is that we have direct access to the same controls that WordPress uses for its own blocks. We import them and extend them where needed.I started by importing the stuff in :import { InspectorControls, useBlockProps } from '@wordpress/block-editor';
import { PanelBody, TextControl } from '@wordpress/components';
import './editor.scss';This gives me a few handy things: are good for debugging. are what can be debugged. is the main wrapper for the block settings. is the field I want to pass into the markup where  currently is. provides styles for the controls.Before I get to the controls, there’s an  function needed to use as a wrapper for all the work:export default function Edit( { attributes, setAttributes } ) {
  // Controls
}First is  and the :export default function Edit( { attributes, setAttributes } ) {
  // React components need a parent element
  <>
    <InspectorControls>
      <PanelBody title={ __( 'Settings', 'baseline-status' ) }>
      // Controls
      </PanelBody>
    </InspectorControls>
  </>
}export default function Edit( { attributes, setAttributes } ) {
  <>
    <InspectorControls>
      <PanelBody title={ __( 'Settings', 'baseline-status' ) }>
        // Controls
        <TextControl
          label={ __(
            'Feature', // Input label
            'baseline-status'
          ) }
          value={ featureID || '' }
          onChange={ ( value ) =>
            setAttributes( { featureID: value } )
          }
        />
     </PanelBody>
    </InspectorControls>
  </>
}Block settings with a text inputAll the logic for handling stateOh yeah! Can’t forget to define the  variable because that’s what populates in the component’s markup. Back in :const { featureID } = attributes;In short: The feature’s ID is what constitutes the block’s attributes. Now I need to register that attribute so the block recognizes it. Back in  in a new section:"attributes": {
  "featureID": {
    "type": "string"
  }
},Pretty straightforward, I think. Just a single text field that’s a string. It’s at this time that I can  wire it up to the front-end markup in :<baseline-status
  <?php echo get_block_wrapper_attributes(); ?>
  featureId="<?php echo esc_html( $featureID ); ?>">
</baseline-status>I struggled with this more than I care to admit. I’ve dabbled with styling the Shadow DOM but only academically, so to speak. This is the first time I’ve attempted to style a web component with Shadow DOM parts on something being used in production.If you’re new to Shadow DOM, the basic idea is that it prevents styles and scripts from “leaking” in or out of the component. This is a big selling point of web components because it’s so darn easy to drop them into any project and have them “just” work.A few other more articles I used:First off, I knew I could select the  element directly without any classes, IDs, or other attributes:baseline-status {
  /* Styles! */
}I peeked at the script’s source code to see what I was working with. I had a few light styles I could use right away on the type selector:baseline-status {
  background: #000;
  border: solid 5px #f8a100;
  border-radius: 8px;
  color: #fff;
  display: block;
  margin-block-end: 1.5em;
  padding: .5em;
}I noticed a CSS color variable in the source code that I could use in place of hard-coded values, so I redefined them and set them where needed:baseline-status {
  --color-text: #fff;
  --color-outline: var(--orange);

  border: solid 5px var(--color-outline);
  border-radius: 8px;
  color: var(--color-text);
  display: block;
  margin-block-end: var(--gap);
  padding: calc(var(--gap) / 4);
}Now for a tricky part. The component’s markup looks close to this in the DOM when fully rendered:<baseline-status class="wp-block-css-tricks-baseline-status" featureid="anchor-positioning"></baseline-status>
<h1>Anchor positioning</h1>
<details>
  <summary aria-label="Baseline: Limited availability. Supported in Chrome: yes. Supported in Edge: yes. Supported in Firefox: no. Supported in Safari: no.">
    <baseline-icon aria-hidden="true" support="limited"></baseline-icon>
    <div class="baseline-status-title" aria-hidden="true">
      <div>Limited availability</div>
        <div class="baseline-status-browsers">
        <!-- Browser icons -->
        </div>
    </div>
  </summary><p>This feature is not Baseline because it does not work in some of the most widely-used browsers.</p><p><a href="https://github.com/web-platform-dx/web-features/blob/main/features/anchor-positioning.yml">Learn more</a></p></details>
<baseline-status class="wp-block-css-tricks-baseline-status" featureid="anchor-positioning"></baseline-status>I wanted to play with the idea of hiding the  element in some contexts but thought twice about it because  displaying the title only really works for Almanac content when you’re on the page for the same feature as what’s rendered in the component. Any other context and the heading is a “need” for providing context as far as what feature we’re looking at. Maybe that can be a future enhancement where the heading can be toggled on and off.This is freely available in the WordPress Plugin Directory as of today! This is my very first plugin I’ve submitted to WordPress on my own behalf, so this is really exciting for me!This is far from fully baked but definitely gets the job done for now. In the future it’d be nice if this thing could do a few more things: The widget does not update on the back end until the page refreshes. I’d love to see the final rendering before hitting Publish on something. I got it where typing into the text input is instantly reflected on the back end. It’s just that the component doesn’t re-render to show the update. As in “large” and “small”.  Toggle to hide or show, depending on where the block is used.]]></content:encoded></item><item><title>How to Split a String in Python</title><link>https://realpython.com/python-split-string/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Wed, 5 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[Python’s  method lets you divide a string into a list of substrings based on a specified delimiter. By default,  separates at whitespace, including spaces, tabs, and newlines. You can customize  to work with specific delimiters using the  parameter, and control the amount of splits with .By the end of this tutorial, you’ll understand that:You  in Python using  without arguments.Python’s  method can split on  when you pass a character or string as an argument.You limit splits using  to control the number of substrings Python extracts. into individual lines, excluding or including line breaks with the  parameter. uses  for splitting strings based on complex patterns.Exploring these methods will level up your text-processing capabilities and enable you to confidently tackle real-world data parsing challenges. Test your knowledge with our interactive “How to Split a String in Python” quiz. You’ll receive a score upon completion to help you track your learning progress:In this quiz, you'll test your understanding of Python's .split() method. This method is useful for text-processing and data parsing tasks, allowing you to divide a string into a list of substrings based on a specified delimiter.How to Split a String in Python Using The  method in Python is a versatile tool that allows you to divide a string into a list of substrings based on a specified delimiter. By default,  separates a string at each occurrence of whitespace, which includes spaces, tabs, and newlines. This makes it particularly handy when dealing with plain text where words are separated by spaces:As you can see,  divides the sentence into individual words, removing the spaces in the process. It’s important to note that  also strips newlines by default, which might not be immediately obvious:The  method treats the newline characters as whitespace, and splits the text into individual words rather than lines. This is a crucial point to remember when working with multiline strings.The simplicity of  makes it a powerful tool for string manipulation. Whether you’re processing text files or parsing user input, using  can streamline your workflow.When you’re faced with the task of cleaning up messy text, you may want to pair  with . You can learn more about using  by expanding the collapsible section below:You may encounter code that uses  in combination with another string method, :In this example, you’re dealing with text that contains whitespace characters at the beginning and end of . You may see results like this when working with data that you received through web scraping, copy-paste artifacts from PDFs, or even regular user input.Combining  and  is a common and straightforward way to break potentially messy text into manageable tokens.The idea is that you first remove superfluous whitespace characters from the beginning and end of the string using . This operation returns a new string object without that whitespace. You then chain to this cleaned output, allowing it to break the text into elements—in this case, individual words.If you’re only dealing with messy whitespace, however, then you don’t need to use  to get to a clean result:Because  splits on any whitespace and ignores extra leading and trailing whitespace, the result is the same as when you first explicitly remove leading and trailing whitespace with .Nevertheless, there may be advantages to adding  into the mix: You clearly communicate to other developers that you intend to apply data cleaning—and readability counts! You may later need to update your code to split on a different delimiter, in which case  won’t remove extra whitespace automatically. A well-designed additional step of explicitly removing whitespace with  can prevent surprises.If you’re only splitting on whitespace, then you likely won’t need to use . However, due to developer habits and a preference for explicitness, you may still come across this combination of string methods in the wild.When you use , it returns a list of substrings. This means that you can iterate over the result, access individual elements using indexing, or unpack the iterable into separate variables.For example, when you’re dealing with user input, you might want to extract specific pieces of information:In this code snippet, you take a string containing a name, surname, and age, and split it into a list of three separate strings. Then, you unpack the list into three descriptive variables. Finally, you use an f-string to format the output. Extracting data pieces like this is useful when you’re dealing with structured data where you know the position of each element beforehand, and you can rely on consistent use of whitespace.In this section, you’ve learned how to use Python’s  method to divide strings into smaller parts based on whitespace. This method is invaluable when working with plain text data, allowing you to extract and manipulate information. By understanding the default behavior of , including its treatment of newlines, you’ll be well-equipped to handle a variety of string manipulation tasks in your Python projects.In the next section, you’ll explore how to customize the behavior of  by specifying different delimiters, enabling you to tackle more complex string splitting scenarios.Split With Different Delimiters Using By default,  uses any whitespace as a delimiter to separate a string into a list of substrings. However, many real-world scenarios require splitting strings using other delimiters. This is where the  parameter comes into play.Imagine that you’re parsing data from a CSV (comma-separated values) file. CSV files are a popular format for storing tabular data, where each line represents a row and each value within a line is separated by a comma.]]></content:encoded></item><item><title>Mastering GenAI contextual continuity – Part 2: Farming example</title><link>https://www.datasciencecentral.com/mastering-genai-contextual-continuity-part-2-farming-example/</link><author>Bill Schmarzo</author><category>dev</category><category>ai</category><pubDate>Wed, 5 Feb 2025 13:23:22 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[In the blog “Driving Relevant GenAI / LLM Outcomes with Contextual Continuity,” I introduced the concept of contextual continuity as a technique for getting your Generative AI tools like ChatGPT or CoPilot to deliver more relevant and accurate responses. Contextual Continuity refers to the ability of a Generative AI (GenAI) system, such as ChatGPT, to… Read More »]]></content:encoded></item><item><title>Living on the Edge • Erica Pisani • GOTO 2024</title><link>https://www.youtube.com/watch?v=K6c29Gtlpjs</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/K6c29Gtlpjs?version=3" length="" type=""/><pubDate>Wed, 5 Feb 2025 13:00:24 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at GOTO Chicago 2024. #GOTOcon #GOTOchgo
https://gotochgo.com

Erica Pisani - Senior Software Enginee at Float Financial @ericapisani2789 

RESOURCES
https://bsky.app/profile/ericapisani.dev
https://hachyderm.io/@ericapisani
https://twitter.com/ericapisani
https://linkedin.com/in/erica-pisani-5b77ab84
https://github.com/ericapisani
https://www.ericapisani.dev

ABSTRACT
Edge computing is not a new concept but in the past few years we're seeing powerful tools made more easily available at the edge.
This talk will go into what the edge is, why its increasing availability and adoption is exciting, and ways we can start incorporating the edge into our applications. [...]

TIMECODES
00:00 Intro
01:01 What is "the edge"?
06:54 Ada's pet supplies
14:42 Planetscale
15:55 Cloudflare
21:40 AWS Snowball 
34:29 Limitations
35:50 Lever-edge-ing the edge
38:04 Green software resources
39:18 Outro

Download slides and read the full abstract here:
https://gotochgo.com/2024/sessions/3374

RECOMMENDED BOOKS
Sergio Mendez • Edge Computing Systems with Kubernetes • https://amzn.to/42P5hzl
Robert Robey & Yuliana Zamora • Parallel and High Performance Computing • https://amzn.to/42K9Uef
 Ashok Iyengar & Joseph Pearson • Edge Computing Patterns for Solution Architects • https://amzn.to/4bHUjzP
Perry Lea • IoT and Edge Computing for Architects • https://amzn.to/4bMtb2u

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#EdgeComputing #HighPerformanceComputing #ParallelComputing #AvailabilityZone #Cloudflare #Planetscale #AWSSnowball #CloudflareDurableObjects #CloudflareHyperdrive #GreenSoftware #SustainableSoftware #EricaPisani

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Go Module Mirror served backdoor to devs for 3+ years</title><link>https://arstechnica.com/security/2025/02/backdoored-package-in-go-mirror-site-went-unnoticed-for-3-years/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2023/07/exploit-vulnerability-security.jpg" length="" type=""/><pubDate>Wed, 5 Feb 2025 12:25:55 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[A mirror proxy Google runs on behalf of developers of the Go programming language pushed a backdoored package for more than three years until Monday, after researchers who spotted the malicious code petitioned for it to be taken down twice.The service, known as the Go Module Mirror, caches open source packages available on GitHub and elsewhere so that downloads are faster and to ensure they are compatible with the rest of the Go ecosystem. By default, when someone uses command-line tools built into Go to download or install packages, requests are routed through the service. A description on the site says the proxy is provided by the Go team and “run by Google.”Since November 2021, the Go Module Mirror has been hosting a backdoored version of a widely used module, security firm Socket said Monday. The file uses “typosquatting,” a technique that gives malicious files names similar to widely used legitimate ones and plants them in popular repositories. In the event someone makes a typo or even a minor variation from the correct name when fetching a file with the command line, they land on the malicious file instead of the one they wanted. (A similar typosquatting scheme is common with domain names, too.)]]></content:encoded></item><item><title>Caves of Qud with Brian Bucklew</title><link>https://softwareengineeringdaily.com/2025/02/05/caves-of-qud-with-brian-bucklew/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=caves-of-qud-with-brian-bucklew</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED9626101996.mp3" length="" type=""/><pubDate>Wed, 5 Feb 2025 10:00:53 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[Caves of Qud is a roguelike game set in a richly detailed, post-apocalyptic world blending science fiction and fantasy. The game is known for its deep lore, emergent gameplay, and wildly creative character customization. It is a massive indie success, and recently hit a major milestone with the release of version 1.0 after 15 years of development.Brian Bucklew is the cofounder of Freehold games which develops Caves of Qud. Brian joins the show to talk about his engineering background and the development of his game.Joe Nash is a developer, educator, and award-winning community builder, who has worked at companies including GitHub, Twilio, Unity, and PayPal. Joe got his start in software development by creating mods and running servers for Garry’s Mod, and game development remains his favorite way to experience and explore new technologies and concepts.]]></content:encoded></item><item><title>This Week in Rust 585</title><link>https://this-week-in-rust.org/blog/2025/02/05/this-week-in-rust-585/</link><author>TWiR Contributors</author><category>This week in Rust</category><category>dev</category><category>rust</category><pubDate>Wed, 5 Feb 2025 05:00:00 +0000</pubDate><source url="https://this-week-in-rust.org/">This Week in Rust</source><content:encoded><![CDATA[This week's crate is ratzilla, a library for building terminal-themed web applications with Rust and WebAssembly.An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.  The following
RFCs would benefit from user testing before moving forward:No calls for testing were issued this week.No calls for testing were issued this week.No calls for testing were issued this week.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.No Calls for papers or presentations were submitted this week.A very quiet week with performance of primary benchmarks showing no change over all.Improvements ✅  (secondary)5 Regressions, 2 Improvements, 5 Mixed; 6 of them in rollups
49 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.No RFCs entered Final Comment Period this week.No Cargo Tracking Issues or PRs entered Final Comment Period this week.No Language Team Proposals entered Final Comment Period this week.No Language Reference RFCs entered Final Comment Period this week.No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.Rusty Events between 2025-02-05 - 2025-03-05 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.If your rust code compiles and you don't use "unsafe", that is a pretty good certification.Thanks to ZiCog for the suggestion!]]></content:encoded></item><item><title>Risky Business #778 -- Musk&apos;s child soldiers seize control of FedGov IT systems</title><link>https://risky.biz/RB778/</link><author></author><category>Risky Business blog</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/media3.risky.biz/RB778.mp3" length="" type=""/><pubDate>Wed, 5 Feb 2025 03:24:50 +0000</pubDate><source url="https://risky.biz/">Risky Business</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>This Week In React #220: Next.js, Astro, Nuqs, CRA, React Scan, Fumadocs, Fusion, Radix, Inertia | Sortables, Contributors Summit, Windows, Deep linking, Oklab, Audio, BottomTabs, Metro, OTA | TypeScript, Angular, Node.js, Vite</title><link>https://thisweekinreact.com/newsletter/220</link><author></author><category>frontend</category><pubDate>Wed, 5 Feb 2025 00:00:00 +0000</pubDate><source url="https://thisweekinreact.com/newsletter">This week in react</source><content:encoded><![CDATA[This week we have a good variety of React articles and Next.js is finally adding support for Node.js runtime middleware!Mobile developers will also discover future plans for React Native, including desktop support from Microsoft. A new sortables drag & drop library should unlock new interactions in your app, considering it's quite hard to build all this yourself.TypeScript continues to progress with a new beta including a flag to support the newly added native Node.js support, and the upcoming browser support. A polyfill for native browser support is already available!I'm trying something new this week, splitting the newsletter title keywords with a , following the pattern React keywords | React Native keywords | Other keywords. Do you think this is useful?As always, thanks for supporting us on your favorite platform:If your authentication isn't , it's .With PropelAuth, you get everything you need to launch and scale your B2B product:UIs that cover all your use cases - signup, login, MFA, organization management and more. so you can provide top-notch support to your users. and advanced security features so you can be upmarket-ready. to help you strategize and expand.A much-awaited feature for the Next.js community. The latest v15.2 canary has a new experimental.nodeMiddleware flag letting you opt-in for , enabling you to use Node.js APIs in middleware, and remove the constraints of being limited to APIs compatible with the edge runtime.Sentry's new bi-weekly live demos will show you how to catch, fix and prevent issues fast. Join one of their engineers Thursdays at 1 PM ET for 25 minute tour—no slides, no fluff, just code-level visibility from pre-release to production. RSVPA new powerful React Native library for re-ordering items with drag & drop gestures, including features such as:Support for Grid and Flex layoutsSupport for different item sizes, spacing, and arrangementsAuto-scrolling when dragging outside screenSeamless layout animationsSimple API, full TypeScript supportCompatible with Expo and both architectures]]></content:encoded></item><item><title>crates.io: development update</title><link>https://blog.rust-lang.org/2025/02/05/crates-io-development-update.html</link><author>Tobias Bieniek</author><category>dev</category><category>official</category><category>rust</category><pubDate>Wed, 5 Feb 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[Back in July 2024, we published a blog post about the ongoing development of crates.io. Since then, we have made a lot of progress and shipped a few new features. In this blog post, we want to give you an update on the latest changes that we have made to crates.io.In RFC #3660 we proposed a new feature that allows crate owners to delete their crates from crates.io under certain conditions. This can be useful if you have published a crate by mistake or if you want to remove a crate that is no longer maintained. After the RFC was accepted by all team members at the end of August, we began implementing the feature.We created a new API endpoint DELETE /api/v1/crates/:name that allows crate owners to delete their crates and then created the corresponding user interface. If you are the owner of a crate, you can now go to the crate page, open the "Settings" tab, and find the "Delete this crate" button at the bottom. Clicking this button will lead you to a confirmation page telling you about the potential impact of the deletion and requirements that need to be met in order to delete the crate:As you can see from the screenshot above, a crate can only be deleted if either: the crate has been published for less than 72 hours or the crate only has a single owner, and the crate has been downloaded less than 500 times for each month it has been published, and the crate is not depended upon by any other crate on crates.io.These requirements were put in place to prevent abuse of the deletion feature and to ensure that crates that are widely used by the community are not deleted accidentally. If you have any feedback on this feature, please let us know!Around the holiday season we started experimenting with generating an OpenAPI description for the crates.io API. This was a long-standing request from the community, and we are happy to announce that we now have an experimental OpenAPI description available at https://crates.io/api/openapi.json!Please note that this is still considered work-in-progress and e.g. the stability guarantees for the endpoints are not written down and the response schemas are also not fully documented yet.You can view the OpenAPI description in e.g. a Swagger UI at https://petstore.swagger.io/ by putting https://crates.io/api/openapi.json in the top input field. We decided to not ship a viewer ourselves for now due to security concerns with running it on the same domain as crates.io itself. We may reconsider whether to offer it on a dedicated subdomain in the future if there is enough interest.The OpenAPI description is generated by the utoipa crate, which is a tool that can be integrated with the axum web framework to automatically generate OpenAPI descriptions for all of your endpoints. We would like to thank Juha Kukkonen for his great work on this tool!Support form and "Report Crate" buttonSince the crates.io team is small and mostly consists of volunteers, we do not have the capacity to manually monitor all publishes. Instead, we rely on you, the Rust community, to help us catch malicious crates and users. To make it easier for you to report suspicious crates, we added a "Report Crate" button to all the crate pages. If you come across a crate that you think is malicious or violates the code of conduct or our usage policy, you can now click the "Report Crate" button and fill out the form that appears. This will send an email to the crates.io team, who will then review the crate and take appropriate action if necessary. Thank you to crates.io team member @eth3lbert who worked on the majority of this.If you have any issues with the support form or the "Report Crate" button, please let us know. You can also always email us directly at help@crates.io if you prefer not to use the form.We have added a new feature that allows you to receive email notifications when a new version of your crate is published. This can be useful in detecting unauthorized publishes of your crate or simply to keep track of publishes from other members of your team.This feature was another long-standing feature request from our community, and we were happy to finally implement it. If you'd prefer not to receive publish notifications, then you can go to your account settings on crates.io and disable these notifications.These were some of the more visible changes to crates.io over the past couple of months, but a lot has happened "under the hood" as well.RFC #3691 was opened and accepted to implement "Trusted Publishing" support on crates.io, similar to other ecosystems that adopted it. This will allow you to specify on crates.io which repository/system is allowed to publish new releases of your crate, allowing you to publish crates from CI systems without having to deal with API tokens anymore.Slightly related to the above: API tokens created on crates.io now expire after 90 days by default. It is still possible to disable the expiry or choose other expiry durations though.The crates.io team was one of the first projects to use the diesel database access library, but since that only supported synchronous execution it was sometimes a little awkward to use in our codebase, which was increasingly moving into an async direction after our migration to axum a while ago. The maintainer of diesel, Georg Semmler, did a lot of work to make it possible to use diesel in an async way, resulting in the diesel-async library. Over the past couple of months we incrementally ported crates.io over to  queries, which now allows us to take advantage of the internal query pipelining in  that resulted in some of our API endpoints getting a 10-15% performance boost. Thank you, Georg, for your work on these crates!Whenever you publish a new version or yank/unyank existing versions a couple of things need to be updated. Our internal database is immediately updated, and then we synchronize the sparse and git index in background worker jobs. Previously, yanking and unyanking a high number of versions would each queue up another synchronization background job. We have now implemented automatic deduplication of redundant background jobs, making our background worker a bit more efficient.The final big, internal change that was just merged last week is related to the testing of our frontend code. In the past we used a tool called Mirage to implement a mock version of our API, which allowed us to run our frontend test suite without having to spin up a full backend server. Unfortunately, the maintenance situation around Mirage had lately forced us to look into alternatives, and we are happy to report that we have now fully migrated to the "Industry standard API mocking" package msw. If you want to know more, you can find the details in the "small" migration pull request.We hope you enjoyed this update on the development of crates.io. If you have any feedback or questions, please let us know on Zulip or GitHub. We are always happy to hear from you and are looking forward to your feedback!]]></content:encoded></item><item><title>sync/v2 and the &apos;v2&apos;-ification of Go&apos;s standard library</title><link>https://golangweekly.com/issues/541</link><author></author><category>dev</category><category>go</category><pubDate>Wed, 5 Feb 2025 00:00:00 +0000</pubDate><source url="https://golangweekly.com/">Golang Weekly</source><content:encoded><![CDATA[ — It uses Linux-only network namespaces for now, but this is a Go-powered process-scoped HTTP tracer you can run  root privileges. It even decrypts TLS traffic by generating a CA on the fly. Handy for debugging or seeing if dependencies or apps are ‘phoning home,’ perhaps..Monastic Academy for the Preservation of Life on Earth ]]></content:encoded></item><item><title>European Commission Gets Dinged for Unlawful Data Transfer, Sending a Big Message About Accountability</title><link>https://www.eff.org/deeplinks/2025/02/courts-fine-against-european-commission-unlawful-personal-data-transfer-small</link><author>Karen Gullo</author><category>tech</category><pubDate>Tue, 4 Feb 2025 22:46:07 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[The European Commission was caught failing to comply with its own data protection regulations and, in a first, ordered to pay damages to a user for the violation. The €400 ($415) award may be tiny compared to fines levied against Big Tech by European authorities, but it’s still a win for users and considerably more than just a blip for the “talk about embarrassing” file at the commission.The case, Bindl vs. EC, underscores the principle that when people’s data is lost, stolen, or shared without promised safeguards—which can lead to identity theft, cause uncertainty about who has access to the data and for what purpose, or place our names and personal preferences in the hands of data brokers —they’ve been harmed and have the right to hold those responsible accountable and seek damages.Some corporations, courts, and lawmakers in the U.S. need to learn a thing or two about this principle. Victims of data breaches are subject to anxiety and panic that their social security numbers and other personal information, even their passport numbers, are being bought and sold on the dark web to criminals who will use the information to drain their bank accounts or demand a ransom not to.But when victims try to go to court, the companies that failed to protect their data in the first place sometimes say tough luck—unless you actually lose money, they say you’re not really harmed and can’t sue. And courts in many cases go along with this.The EC debacle arose when a German citizen using the commission’s website to register for a conference was offered to sign in using Facebook, which he did—a common practice that, surprise, surprise, can and does give U.S.-based Facebook access to signees’ personal information.Here’s the problem: In the EU, the General Data Privacy Regulations (GDPR), a comprehensive and far-reaching data privacy law that came into effect in 2018, and a related law that applies to EU institutions, Regulation (EU) 2018/1725, requires entities that handle personal data to abide by certain rules for collecting and transferring it. They must, for instance, ensure that transfers of someone’s personal information, such as their IP address, to countries outside the EU are adequately protected.The GDPR also give users significant control over their data, such as requiring data processors to obtain users’ clear consent to handle their personal data and allowing users to seek compensation if their privacy rights are infringed—although the regulations are silent on how damages should be assessed.In what it called a “sufficiently serious breach,” a condition for awarding damages, the European General Court, which hears actions against EU institutions, found that the EC violated EU privacy protections by facilitating in 2022 the transfer of German citizen Thomas Bindl’s IP address and other personal data to Meta, owner of Facebook. The transfer was unlawful because there were no agreements at the time that adequately protected EU users’ data from U.S. government surveillance and weak data privacy laws.“…personal data may be transferred to a third country or to an international organisation only if the controller or processor has provided appropriate safeguards, and on condition that enforceable data subject rights and effective legal remedies for data subjects are available,” the court said. “In the present case, the Commission has neither demonstrated nor claimed that there was an appropriate safeguard, in particular a standard data protection clause or contractual clause…”(The EC in 2023 adopted the EU-US Data Privacy Framework to facilitate mechanisms for  personal data transfers between the U.S. and EU states, Great Britain, and Switzerland with protections that are supposed to be consistent with EU, UK, and Swiss law and limit US intelligence services’ access to personal data transferred to America.)Bindl sought compensation for non-material—that is, not involving direct financial loss—damages because the transfer caused him to lose control of his data and deprived him of his rights and freedoms.Applying standards it had set in a data mishandling case from Austria involving non-material damage claims, the court said he was entitled to such damages because the commission had violated the GDPR-like regulation 2018/1725 and the damages he suffered were caused by the infringement.Importantly, the court specified that the right to compensation doesn’t hinge on an assessment of whether the harms are serious enough to take to court, a condition that some EU member state courts have used to dismiss non-material damage claims.Rather, it was enough that the data transfer put Bindl “in a position of some uncertainty as regards the processing of his personal data, in particular of his IP address,” the court said. This is criterion that could benefit other plaintiffs seeking non-material damages for the mishandling of their data, said Tilman Herbrich, Bindl’s attorney.Noting the ease with which IP addresses can be used to connect a person to an existing online profile and exploit their data, Bindl, in conversation with The International Association of Privacy Professionals (IAPP), said “it’s totally clear that this was more than just this tiny little piece of IP address, where people even tend to argue whether its PII (personal identifiable information) or not.”  Bindl is the founder of EuGD European Society for Data Protection, a Munich-based litigation funder that supports complainants in data protection lawsuits.The court’s decision recognizes that losing control of your data causes real non-material harm, and shines a light on why people are entitled to seek compensation for emotional damage, probably without the need to demonstrate a minimum threshold of damage.stood up for this principle in U.S. courts against corporate giants who—after data thieves penetrate their inadequate security systems, exposing millions of people’s private information—claim in court that victims haven’t really been injured unless they can prove a specific economic harm on top of the obvious privacy harm.In fact, negligent data breaches inflict grievous privacy harms in and of themselves, and so the victims have “standing” to sue in federal court—without the need to prove more.Once data has been disclosed, it is often pooled with other information, some gathered consensually and legally and some gathered from other data breaches or through other illicit means. That pooled information is then used to create inferences about the affected individuals for purposes of targeted advertising, various kinds of risk evaluation, identity theft, and more.In the EU, the Bindl case could bring more legal certainty to individuals and companies about damages for data protection violations and perhaps open the door to collective-action lawsuits. To the extent that the case was brought to determine whether the EC follows its own rules, the outcome was decisive.The commission “should set the standard in terms of implementation of how they are doing it,” Bindl said. “If anyone is looking at somebody who is doing it perfectly right, it should be the commission, right?”]]></content:encoded></item><item><title>TOC Meeting 2025-02-04</title><link>https://www.youtube.com/watch?v=R3H_ceqR6Us</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/R3H_ceqR6Us?version=3" length="" type=""/><pubDate>Tue, 4 Feb 2025 22:26:39 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Red Hat Plans to Add AI to Fedora and GNOME</title><link>https://linux.slashdot.org/story/25/02/04/2047240/red-hat-plans-to-add-ai-to-fedora-and-gnome?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>slashdot</category><category>linux</category><pubDate>Tue, 4 Feb 2025 20:50:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[In his post about the future of Fedora Workstation, Christian F.K. Schaller discusses how the Red Hat team plans to integrate AI with IBM's open-source Granite engine to enhance developer tools, such as IDEs, and create an AI-powered Code Assistant. He says the team is also working on streamlining AI acceleration in Toolbx and ensuring Fedora users have access to tools like RamaLama. From the post: One big item on our list for the year is looking at ways Fedora Workstation can make use of artificial intelligence. Thanks to IBMs Granite effort we know have an AI engine that is available under proper open source licensing terms and which can be extended for many different usecases. Also the IBM Granite team has an aggressive plan for releasing updated versions of Granite, incorporating new features of special interest to developers, like making Granite a great engine to power IDEs and similar tools. We been brainstorming various ideas in the team for how we can make use of AI to provide improved or new features to users of GNOME and Fedora Workstation. This includes making sure Fedora Workstation users have access to great tools like RamaLama, that we make sure setting up accelerated AI inside Toolbx is simple, that we offer a good Code Assistant based on Granite and that we come up with other cool integration points. "I'm still not sure how I feel about this approach," writes designer/developer and blogger, Bradley Taunt. "While IBM Granite is an open source model, I still don't enjoy so much artificial 'intelligence' creeping into core OS development. This also isn't something optional on the end-users side, like a desktop feature or package. This sounds like it's going to be built directly into the core system."
 
"Red Hat has been pushing hard towards AI and my main concern is having this influence other operating system dev teams. Luckily things seems AI-free in BSD land. For now, at least."]]></content:encoded></item><item><title>Data logs: The latest evolution in Meta’s access tools</title><link>https://engineering.fb.com/2025/02/04/security/data-logs-the-latest-evolution-in-metas-access-tools/</link><author></author><category>dev</category><category>official</category><pubDate>Tue, 4 Feb 2025 20:00:18 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[Lessons learned from building data logsMaking data consistently understandable and explainable]]></content:encoded></item><item><title>Python 3.13.2 and 3.12.9 now available!</title><link>https://pythoninsider.blogspot.com/2025/02/python-3132-and-3129-now-available.html</link><author>Thomas Wouters</author><category>Python official news</category><category>dev</category><category>official</category><category>python</category><pubDate>Tue, 4 Feb 2025 19:58:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python Insider</source><content:encoded><![CDATA[A small release day today! That is to say the releases are relatively 
small; the day itself was of average size, as most days are.Python 3.13’s second maintenance release. About 250 changes went into 
this update, and can be yours for free if you just upgrade now.Python 3.12’s  maintenance release already. Just 180 changes for 3.12, but it’s still worth upgrading.Thanks to all of the many volunteers who help make Python Development
 and these releases possible! Please consider supporting our efforts by 
volunteering yourself or through organization contributions to the 
Python Software Foundation.Regards from your tireless, tireless release team,
Thomas Wouters 
Steve Dower ]]></content:encoded></item><item><title>Hijacked Line</title><link>https://www.youtube.com/watch?v=353ECpo4JIo</link><author>Jack Rhysider</author><category>Darknet diaries</category><category>infosec</category><category>video</category><category>podcast</category><enclosure url="https://www.youtube.com/v/353ECpo4JIo?version=3" length="" type=""/><pubDate>Tue, 4 Feb 2025 18:48:51 +0000</pubDate><source url="https://www.youtube.com/channel/UCMIqrmh2lMdzhlCPK5ahsAg">Jack Rhysider</source><content:encoded><![CDATA[Conor Freeman (x.com/conorfrmn) stole money online. Lot’s of it. In this episode we talk with him, and hear how he did it, why he did, and what he spent it on.

Conor’s website: https://conorfreeman.ie

Conor’s X: https://x.com/conorfrmn

Visit https://darknetdiaries.com/episode/154/ for a list of sources, full transcripts, and to listen to all episodes.]]></content:encoded></item><item><title>OpenAI o3 tries to curb stomp DeepSeek...</title><link>https://www.youtube.com/watch?v=PoeFxGzPpXE</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/PoeFxGzPpXE?version=3" length="" type=""/><pubDate>Tue, 4 Feb 2025 18:48:26 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Stay updated with the best dev content at https://daily.dev/fireship

Let's take a first look at OpenAI o3-mini and its new deep research feature. In addition, we will examine the recent bans on DeepSeek R1 and it compare it to other AI tools.  

#tech #ai #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

OpenAI Deep Research https://openai.com/index/introducing-deep-research/
OpenAI accuses DeepSeek of IP theft https://youtu.be/hpwoGjpYygI
Tech Trends 2025 https://youtu.be/v4H2fTgHGuc
Godot in 100 Seconds https://youtu.be/QKgTZWbwD1U

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- OpenAI Deep Research First Look
- Why is DeepSeek banned?
- Latest AI features in tech
- o3-mini vs DeepSeek R1 vs Claude Sonnet]]></content:encoded></item><item><title>AI &amp; Aliens: New Eyes on Ancient Questions // Richard Cloete // #288</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/AI--Aliens-New-Eyes-on-Ancient-Questions--Richard-Cloete--288-e2udn14</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/98015716/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-4%2F394277793-44100-2-45d73bbd25586.mp3" length="" type=""/><pubDate>Tue, 4 Feb 2025 17:31:32 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[ is a computer scientist and a Laukien-Oumuamua Postdoctoral Research Fellow at the Center for Astrophysics, Harvard University. He is a member of the Galileo Project working under the supervision of Professor Avi, having recently held a postdoctoral position at the University of Cambridge, UK.

AI & Aliens: New Eyes on Ancient Questions // MLOps Podcast #288 with Richard Cloete, 
Laukien-Oumuamua Postdoctoral Research Fellow at Harvard University.

// Abstract
Demetrios speaks with Dr. Richard Cloete, a Harvard computer scientist and founder of SEAQR Robotics, about his AI-driven work in tracking Unidentified Aerial Phenomena (UAPs) through the Galileo Project. Dr. Cloete explains their advanced sensor setup and the challenges of training AI in this niche field, leading to the creation of AeroSynth, a synthetic data tool.

He also discusses his collaboration with the Minor Planet Center on using AI to classify interstellar objects and upcoming telescope data. Additionally, he introduces Seeker Robotics, applying similar AI techniques to oceanic research with unmanned vehicles for marine monitoring. The conversation explores AI’s role in advancing our understanding of space and the ocean.

// Bio
Richard is a computer scientist and Laukien-Oumuamua Postdoctoral Research Fellow at the Center for Astrophysics, Harvard University. As a member of the Galileo Project under Professor Avi Loeb's supervision, he develops AI models for detecting and tracking aerial objects, specializing in Unidentified Anomalous Phenomena (UAP). 

Beyond UAP research, he collaborates with astronomers at the Minor Planet Center to create AI models for identifying potential interstellar objects using the upcoming Vera C. Rubin Observatory. 

Richard is also the CEO and co-founder of SEAQR Robotics, a startup developing advanced unmanned surface vehicles to accelerate the discovery of novel life and phenomena in Earth's oceans and atmosphere. 

Before joining Harvard, he completed a postdoctoral fellowship at the University of Cambridge, UK, where his research explored the intersection of emerging technologies and law.Grew up in Cape Town, South Africa, where I used to build Tesla Coils, plasma globes, radio stethoscopes, microwave guns, AM radios, and bombs...

// MLOps Swag/Merch
https://shop.mlops.community/

// Related Links
Website: www.seaqr.net
https://itc.cfa.harvard.edu/people/richard-cloete
 
--------------- ✌️Connect With Us ✌️ -------------
Join our slack community: https://go.mlops.community/slack
Follow us on Twitter: @mlopscommunity
Sign up for the next meetup: https://go.mlops.community/register
Catch all episodes, blogs, newsletters, and more: https://mlops.community/

Connect with Demetrios on LinkedIn: https://www.linkedin.com/in/dpbrinkm/
Connect with Richard on LinkedIn: https://www.linkedin.com/in/richard-cloete/

]]></content:encoded></item><item><title>Who’s Behind the Seized Forums ‘Cracked’ &amp; ‘Nulled’?</title><link>https://krebsonsecurity.com/2025/02/whos-behind-the-seized-forums-cracked-nulled/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Tue, 4 Feb 2025 17:09:16 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[The  joined authorities across Europe last week in seizing domain names for  and , English-language cybercrime forums with millions of users that trafficked in stolen data, hacking tools and malware. An investigation into the history of these communities shows their apparent co-founders quite openly operate an Internet service provider and a pair of e-commerce platforms catering to buyers and sellers on both forums.In this 2019 post from Cracked, a forum moderator told the author of the post (Buddie) that the owner of the RDP service was the founder of Nulled, a.k.a. “Finndev.” Image: Ke-la.com.On Jan. 30, the U.S. Department of Justicesaid it seized eight domain names that were used to operate Cracked, a cybercrime forum that sprang up in 2018 and attracted more than four million users. The DOJ said the law enforcement action, dubbed Operation Talent, also seized domains tied to , Cracked’s payment processor.In addition, the government seized the domain names for two popular anonymity services that were heavily advertised on Cracked and Nulled and allowed customers to rent virtual servers: StarkRDP[.]io, and rdp[.]sh.Those archived webpages show both RDP services were owned by an entity called . According to corporate records compiled by , 1337 Services GmbH is also known as  and is incorporated in Hamburg, Germany.The Cracked forum administrator went by the nicknames “” and “” on multiple cybercrime forums. Meanwhile, a LinkedIn profile for a . from Germany refers to this person as the co-founder of Sellix and founder of 1337 Services GmbH.Northdata’s business profile for 1337 Services GmbH shows the company is controlled by two individuals: 32-year-old  and , 28.An organization chart showing the owners of 1337 Services GmbH as Florian Marzahl and Finn Grimpe. Image: Northdata.com.Neither Marzahl nor Grimpe responded to requests for comment. But Grimpe’s first name is interesting because it corresponds to the nickname chosen by the founder of Nulled, who goes by the monikers “Finn” and “Finndev.” NorthData reveals that Grimpe was the founder of a German entity called , which rented out high-end sports cars and motorcycles.The email address used for those accounts was . DomainTools.com reports f.grimpe@gmail.com was used to register at least nine domain names, including  and . Neither of these domains were among those seized in Operation Talent.Intel471 finds the user FlorainN registered across multiple cybercrime forums using the email address . The breach tracking service Constella Intelligence says this email address used the same password (and slight variations of it) across many accounts online — including at hacker forums — and that the same password was used in connection with dozens of other email addresses, such as florianmarzahl@hotmail.de, and .The Justice Department said the Nulled marketplace had more than five million members, and has been selling stolen login credentials, stolen identification documents and hacking services, as well as tools for carrying out cybercrime and fraud, since 2016.Perhaps fittingly, both Cracked and Nulled have been hacked over the years, exposing countless private messages between forum users. A review of those messages archived by Intel 471 showed that dozens of early forum members referred privately to Finndev as the owner of , an e-commerce platform that caters to the same clientele as Sellix.Shoppy was not targeted as part of Operation Talent, and its website remains online. Northdata reports that Shoppy’s business name —  — is registered at an address in Gan-Ner, Israel, but there is no ownership information about this entity. Shoppy did not respond to requests for comment.Constella found that a user named Shoppy registered on Cracked in 2019 using the email address . Constella says that email address is tied to a Twitter/X account for Shoppy Ecommerce in Israel.The DOJ said one of the alleged administrators of Nulled, a 29-year-old Argentinian national named , was arrested in Spain. The government has not announced any other arrests or charges associated with Operation Talent.Indeed, both StarkRDP and FloraiN have posted to their accounts on  that there were no charges levied against the proprietors of 1337 Services GmbH. FlorainN told former customers they were in the process of moving to a new name and domain for StarkRDP, where existing accounts and balances would be transferred.“StarkRDP has always been operating by the law and is not involved in any of these alleged crimes and the legal process will confirm this,” the StarkRDP Telegram account wrote on January 30. “All of your servers are safe and they have not been collected in this operation. The only things that were seized is the website server and our domain. Unfortunately, no one can tell who took it and with whom we can talk about it. Therefore, we will restart operation soon, under a different name, to close the chapter [of] ‘StarkRDP.'”]]></content:encoded></item><item><title>Updating the Frontier Safety Framework</title><link>https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/</link><author></author><category>dev</category><category>ai</category><enclosure url="https://lh3.googleusercontent.com/0fu18H8X3miSAuwcVJ7Zulis_LZAL7F4bIFU7FYFA2dGx3Rm3HHlm5N202B0dtKBuS7iI5SD1QgpFPuU-O3TPzb7iG1Ns-loZzinRB3M3X3W-MAgIQ=w528-h297-n-nu-rw" length="" type=""/><pubDate>Tue, 4 Feb 2025 16:41:00 +0000</pubDate><source url="https://deepmind.google/discover/blog/">Google Deep Mind blog</source><content:encoded><![CDATA[Our next iteration of the FSF sets out stronger security protocols on the path to AGI]]></content:encoded></item><item><title>Injecting Domain Expertise Into Your AI System</title><link>https://www.topbots.com/injecting-domain-expertise-into-ai-system/</link><author>Janna Lipenkova</author><category>dev</category><category>ai</category><enclosure url="https://www.topbots.com/wp-content/uploads/2025/02/figure4.png" length="" type="text/html"/><pubDate>Tue, 4 Feb 2025 15:37:37 +0000</pubDate><source url="https://www.topbots.com/">TopBots</source><content:encoded><![CDATA[When starting their AI initiatives, many companies are trapped in silos and treat AI as a purely technical enterprise, sidelining domain experts or involving them too late. They end up with generic AI applications that miss industry nuances, produce poor recommendations, and quickly become unpopular with users. By contrast, AI systems that deeply understand industry-specific processes, constraints, and decision logic have the following benefits: — The more domain knowledge AI incorporates, the less manual effort is required from human experts. — Experts disengage from AI systems that feel too generic. AI must speak their language and align with real workflows to gain trust.A sustainable competitive moat — As AI becomes a commodity, embedding proprietary expertise is the most effective way to build defensible AI systems (cf. this article to learn about the building blocks of AI’s competitive advantage).Domain experts can help you connect the dots between the technicalities of an AI system and its real-life usage and value. Thus, they should be key stakeholders and co-creators of your AI applications. This guide is the first part of my series on expertise-driven AI. Following my mental model of AI systems, it provides a structured approach to embedding deep domain expertise into your AI.Throughout the article, we will use the use case of supply chain optimisation (SCO) to illustrate these different methods. Modern supply chains are under constant strain from geopolitical tensions, climate disruptions, and volatile demand shifts, and AI can provide the kind of dynamic, high-coverage intelligence needed to anticipate delays, manage risks, and optimise logistics. However, without domain expertise, these systems are often disconnected from the realities of life. Let’s see how we can solve this by integrating domain expertise across the different components of the AI application.AI is only as domain-aware as the data it learns from. Raw data isn’t enough — it must be curated, refined, and contextualised by experts who understand its meaning in the real world.Data understanding: Teaching AI what mattersWhile data scientists can build sophisticated models to analyse patterns and distributions, these analyses often stay at a theoretical, abstract level. Only domain experts can validate whether the data is complete, accurate, and representative of real-world conditions.In supply chain optimisation, for example, shipment records may contain missing delivery timestamps, inconsistent route details, or unexplained fluctuations in transit times. A data scientist might discard these as noise, but a logistics expert could have real-world explanations of these inconsistencies. For instance, they might be caused by weather-related delays, seasonal port congestion, or supplier reliability issues. If these nuances aren’t accounted for, the AI might learn an overly simplified view of supply chain dynamics, resulting in misleading risk assessments and poor recommendations.Experts also play a critical role in assessing the completeness of data. AI models work with what they have, assuming that all key factors are already present. It takes human expertise and judgment to identify blind spots. For example, if your supply chain AI isn’t trained on customs clearance times or factory shutdown histories, it won’t be able to predict disruptions caused by regulatory issues or production bottlenecks. Run joint Exploratory Data Analysis (EDA) sessions with data scientists and domain experts to identify missing business-critical information, ensuring AI models work with a complete and meaningful dataset, not just statistically clean data.One common pitfall when starting with AI is integrating too much data too soon, leading to complexity, congestion of your data pipelines, and blurred or noisy insights. Instead, start with a couple of high-impact data sources and expand incrementally based on AI performance and user needs. For instance, an SCO system may initially use historical shipment data and supplier reliability scores. Over time, domain experts may identify missing information — such as port congestion data or real-time weather forecasts — and point engineers to those data sources where it can be found. Start with a minimal, high-value dataset (normally 3–5 data sources), then expand incrementally based on expert feedback and real-world AI performance.AI models learn by detecting patterns in data, but sometimes, the right learning signals aren’t yet present in raw data. This is where data annotation comes in — by labelling key attributes, domain experts help the AI understand what matters and make better predictions. Consider an AI model built to predict supplier reliability. The model is trained on shipment records, which contain delivery times, delays, and transit routes. However, raw delivery data alone doesn’t capture the full picture of supplier risk — there are no direct labels indicating whether a supplier is “high risk” or “low risk.”Without more explicit learning signals, the AI might make the wrong conclusions. It could conclude that all delays are equally bad, even when some are caused by predictable seasonal fluctuations. Or it might overlook early warning signs of supplier instability, such as frequent last-minute order changes or inconsistent inventory levels.Domain experts can enrich the data with more nuanced labels, such as supplier risk categories, disruption causes, and exception-handling rules. By introducing these curated learning signals, you can ensure that AI doesn’t just memorise past trends but learns meaningful, decision-ready insights.You shouldn’t rush your annotation efforts — instead, think about a structured annotation process that includes the following components: Establish clear, standardized rules for labeling data to ensure consistency. For example, supplier risk categories should be based on defined thresholds (e.g., delivery delays over 5 days + financial instability = high risk). Involve several domain experts to reduce bias and ensure objectivity, particularly for subjective classifications like risk levels or disruption impact. Capture both direct and contextual factors, such as annotating not just shipment delays but also the cause (customs, weather, supplier fault). Regularly audit and refine annotations based on AI performance — if predictions consistently miss key risks, experts should adjust labelling strategies accordingly. Define an annotation playbook with clear labelling criteria, involve at least two domain experts per critical label for objectivity, and run regular annotation review cycles to ensure AI is learning from accurate, business-relevant insights.So far, our AI models learn from real-life historical data. However, rare, high-impact events — like factory shutdowns, port closures, or regulatory shifts in our supply chain scenario — may be underrepresented. Without exposure to these scenarios, AI can fail to anticipate major risks, leading to overconfidence in supplier stability and poor contingency planning. Synthetic data solves this by creating more datapoints for rare events, but expert oversight is crucial to ensure that it reflects plausible risks rather than unrealistic patterns.Let’s say we want to predict supplier reliability in our supply chain system. The historical data may have few recorded supplier failures — but that’s not because failures don’t happen. Rather, many companies proactively mitigate risks before they escalate. Without synthetic examples, AI might deduce that supplier defaults are extremely rare, leading to misguided risk assessments.Experts can help generate synthetic failure scenarios based on: — Simulating supplier collapses triggered by economic downturns, regulatory shifts, or geopolitical tensions. — Training AI on unrecorded early warning signs, like financial instability or leadership changes. — Creating “what-if” events, such as a semiconductor supplier suddenly halting production or a prolonged port strike. Work with domain experts to define high-impact but low-frequency events and scenarios, which can be in focus when you generate synthetic data.Data makes domain expertise shine. An AI initiative that relies on clean, relevant, and enriched domain data will have an obvious competitive advantage over one that takes the “quick-and-dirty” shortcut to data. However, keep in mind that working with data can be tedious, and experts need to see the outcome of their efforts — whether it’s improving AI-driven risk assessments, optimising supply chain resilience, or enabling smarter decision-making. The key is to make data collaboration intuitive, purpose-driven, and directly tied to business outcomes, so experts remain engaged and motivated.Once AI has access to high-quality data, the next challenge is ensuring it generates useful and accurate outputs. Domain expertise is needed to:Define  aligned with business prioritiesEnsure AI  industry-specific dataContinuously validate AI’s outputs and recommendationsLet’s look at some common AI approaches and see how they can benefit from an extra shot of domain knowledge.Training predictive models from scratchFor structured problems like supply chain forecasting, predictive models such as classification and regression can help anticipate delays and suggest optimisations. However, to make sure these models are aligned with business goals, data scientists and knowledge engineers need to work together. For example, an AI model might try to minimise shipment delays at all costs, but a supply chain expert knows that fast-tracking every shipment through air freight is financially unsustainable. They can formulate additional constraints on the model, making it prioritise critical shipments while balancing cost, risk, and lead times. Define clear objectives and constraints with domain experts before training AI models, ensuring alignment with real business priorities.While predictive models trained from scratch can excel at very specific tasks, they are also rigid and will “refuse” to perform any other task. GenAI models are more open-minded and can be used for highly diverse requests. For example, an LLM-based conversational widget in an SCO system can allow users to interact with real-time insights using natural language. Instead of sifting through inflexible dashboards, users can ask, “Which suppliers are at risk of delays?” or “What alternative routes are available?” The AI pulls from historical data, live logistics feeds, and external risk factors to provide actionable answers, suggest mitigations, and even automate workflows like rerouting shipments.But how can you ensure that a huge, out-of-the-box model like ChatGPT or Llama understands the nuances of your domain? Let’s walk through the LLM triad — a progression of techniques to incorporate domain knowledge into your LLM system.As you progress from left to right, you can ingrain more domain knowledge into the LLM — however, each stage also adds new technical challenges (if you are interested in a systematic deep-dive into the LLM triad, please check out chapters 5–8 of my book The Art of AI Product Management). Here, let’s focus on how domain experts can jump in at each of the stages: out-of-the-box LLMs might seem like a generic approach, but with the right intuition and skill, domain experts can fine-tune prompts to extract the extra bit of domain knowledge out of the LLM. Personally, I think this is a big part of the fascination around prompting — it puts the most powerful AI models directly into the hands of domain experts without any technical expertise. Some key prompting techniques include: Incorporate examples to guide the model’s responses. Instead of just asking “What are alternative shipping routes?”, a well-crafted prompt includes sample scenarios, such as “Example of past scenario: A previous delay at the Port of Shenzhen was mitigated by rerouting through Ho Chi Minh City, reducing transit time by 3 days.”Chain-of-thought prompting: Encourage step-by-step reasoning for complex logistics queries. Instead of “Why is my shipment delayed?”, a structured prompt might be “Analyse historical delivery data, weather reports, and customs processing times to determine why shipment #12345 is delayed.”Providing further background information: Attach external documents to improve domain-specific responses. For example, prompts could reference real-time port congestion reports, supplier contracts, or risk assessments to generate data-backed recommendations. Most LLM interfaces already allow you to conveniently attach additional files to your prompt.2. RAG (Retrieval-Augmented Generation): While prompting helps guide AI, it still relies on pre-trained knowledge, which may be outdated or incomplete. RAG allows AI to retrieve real-time, company-specific data, ensuring that its responses are grounded in current logistics reports, supplier performance records, and risk assessments. For example, instead of generating generic supplier risk analyses, a RAG-powered AI system would pull real-time shipment data, supplier credit ratings, and port congestion reports before making recommendations. Domain experts can help select and structure these data sources and are also needed when it comes to testing and evaluating RAG systems. Work with domain experts to curate and structure knowledge sources — ensuring AI retrieves and applies only the most relevant and high-quality business information.: While prompting and RAG inject domain knowledge on-the-fly, they do not inherently embed supply domain-specific workflows, terminology, or decision logic into your LLM. Fine-tuning adapts the LLM to think like a logistics expert. Domain experts can guide this process by creating high-quality training data, ensuring AI learns from real supplier assessments, risk evaluations, and procurement decisions. They can refine industry terminology to prevent misinterpretations (e.g., AI distinguishing between “buffer stock” and “safety stock”). They also align AI’s reasoning with business logic, ensuring it considers cost, risk, and compliance — not just efficiency. Finally, they evaluate fine-tuned models, testing AI against real-world decisions to catch biases or blind spots.: In LLM fine-tuning, data is the crucial success factor. Quality goes over quantity, and fine-tuning on a small, high-quality dataset can give you excellent results. Thus, give your experts enough time to figure out the right structure and content of the fine-tuning data and plan for plenty of end-to-end iterations of your fine-tuning process.Encoding expert knowledge with neuro-symbolic AIEvery machine learning algorithm gets it wrong from time to time. To mitigate errors, it helps to set the “hard facts” of your domain in stone, making your AI system more reliable and controllable. This combination of machine learning and deterministic rules is called neuro-symbolic AI.For example, an explicit knowledge graph can encode supplier relationships, regulatory constraints, transportation networks, and risk dependencies in a structured, interconnected format.Instead of relying purely on statistical correlations, an AI system enriched with knowledge graphs can:Validate predictions against domain-specific rules (e.g., ensuring that AI-generated supplier recommendations comply with regulatory requirements).Infer missing information (e.g., if a supplier has no historical delays but shares dependencies with high-risk suppliers, AI can assess its potential risk).Improve explainability by allowing AI decisions to be traced back to logical, rule-based reasoning rather than black-box statistical outputs.How can you decide which knowledge should be encoded with rules ( AI), and which should be learned dynamically from the data ( AI)? Domain experts can help youpick those bits of knowledge where hard-coding makes the most sense:Knowledge that is relatively stable over timeKnowledge that is hard to infer from the data, for example because it is not well-representedKnowledge that is critical for high-impact decisions in your domain, so you can’t afford to get it wrongIn most cases, this knowledge will be stored in separate components of your AI system, like decision trees, knowledge graphs, and ontologies. There are also some methods to integrate it directly into LLMs and other statistical models, such as Lamini’s memory fine-tuning.Compound AI and modular workflowsGenerating insights and turning them into actions is a multi-step process. Experts can help you model workflows and decision-making pipelines, ensuring that the process followed by your AI system aligns with their tasks. For example, the following pipeline shows how the AI components we considered so far can be combined into a modular workflow for the mitigation of shipment risks:Experts are also needed to calibrate the “labor distribution” between humans in AI. For example, when modelling decision logic, they can set thresholds for automation, deciding when AI can trigger workflows versus when human approval is needed. Involve your domain experts in mapping your processes to AI models and assets, identifying gaps vs. steps that can already be automated.Especially in B2B environments, where workers are deeply embedded in their daily workflows, the user experience must be seamlessly integrated with existing processes and task structures to ensure efficiency and adoption. For example, an AI-powered supply chain tool must align with how logistics professionals think, work, and make decisions. In the development phase, domain experts are the closest “peers” to your real users, and picking their brains is one of the fastest ways to bridge the gap between AI capabilities and real-world usability. Involve domain experts early in UX design to ensure AI interfaces are intuitive, relevant, and tailored to real decision-making workflows.Ensuring transparency and trust in AI decisionsAI thinks differently from humans, which makes us humans skeptical. Often, that’s a good thing since it helps us stay alert to potential mistakes. But distrust is also one of the biggest barriers to AI adoption. When users don’t understand why a system makes a particular recommendation, they are less likely to work with it. Domain experts can define how AI should explain itself — ensuring users have visibility into confidence scores, decision logic, and key influencing factors.For example, if an SCO system recommends rerouting a shipment, it would be irresponsible on the part of a logistics planner to just accept it. She needs to see the “why” behind the recommendation — is it due to supplier risk, port congestion, or fuel cost spikes? The UX should show a breakdown of the decision, backed by additional information like historical data, risk factors, and a cost-benefit analysis.Mitigate overreliance on AI: Excessive dependence of your users on AI can introduce bias, errors, and unforeseen failures. Experts should find ways to calibrate AI-driven insights vs. human expertise, ethical oversight, and strategic safeguards to ensure resilience, adaptability, and trust in decision-making. Work with domain experts to define key explainability features — such as confidence scores, data sources, and impact summaries — so users can quickly assess AI-driven recommendations.Simplifying AI interactions without losing depthAI tools should make complex decisions easier, not harder. If users need deep technical knowledge to extract insights from AI, the system has failed from a UX perspective. Domain experts can help strike a balance between simplicity and depth, ensuring the interface provides actionable, context-aware recommendations while allowing deeper analysis when needed.For instance, instead of forcing users to manually sift through data tables, AI could provide pre-configured reports based on common logistics challenges. However, expert users should also have on-demand access to raw data and advanced settings when necessary. The key is to design AI interactions that are efficient for everyday use but flexible for deep analysis when required. Use domain expert feedback to define default views, priority alerts, and user-configurable settings, ensuring AI interfaces provide both efficiency for routine tasks and depth for deeper research and strategic decisions.Continuous UX testing and iteration with expertsAI UX isn’t a one-and-done process — it needs to evolve with real-world user feedback. Domain experts play a key role in UX testing, refinement, and iteration, ensuring that AI-driven workflows stay aligned with business needs and user expectations.For example, your initial interface may surface too many low-priority alerts, leading to alert fatigue where users start ignoring AI recommendations. Supply chain experts can identify which alerts are most valuable, allowing UX designers to prioritize high-impact insights while reducing noise.: Conduct think-aloud sessions and have domain experts verbalize their thought process when interacting with your AI interface. This helps AI teams uncover hidden assumptions and refine AI based on how experts actually think and make decisions.Vertical AI systems must integrate domain knowledge at every stage, and experts should become key stakeholders in your AI development:They refine data selection, annotation, and synthetic data.They guide AI learning through prompting, RAG, and fine-tuning.They support the design of seamless user experiences that integrate with daily workflows in a transparent and trustworthy way.An AI system that “gets” the domain of your users will not only be useful and adopted in the short- and middle-term, but also contribute to the competitive advantage of your business.Now that you have learned a bunch of methods to incorporate domain-specific knowledge, you might be wondering how to approach this in your organizational context. Stay tuned for my next article, where we will consider the practical challenges and strategies for implementing an expertise-driven AI strategy!Note: Unless noted otherwise, all images are the author’s.This article was originally published on Towards Data Science and re-published to TOPBOTS with permission from the author.Enjoy this article? Sign up for more AI updates.We’ll let you know when we release more articles like this one.]]></content:encoded></item><item><title>Panel at goto Copenhagen: &quot;Where is SW development Going</title><link>https://www.youtube.com/watch?v=86-Dy5U2p5Y</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 4 Feb 2025 15:23:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[ was on a panel at goto Copenhagen last September with Holly Cummings,
      Trisha Gee, Dave Farley, and Daniel Terhorst-North. We discussed the
      current state of software development and where it was heading. Given the
      timing, there was much discussion about the role AI would play in our
      profession's future.]]></content:encoded></item><item><title>Top 10 web hacking techniques of 2024</title><link>https://portswigger.net/research/top-10-web-hacking-techniques-of-2024</link><author></author><category>infosec</category><pubDate>Tue, 4 Feb 2025 15:01:48 +0000</pubDate><source url="https://portswigger.net/research">Dev - PortSwitgger Research</source><content:encoded><![CDATA[Welcome to the Top 10 Web Hacking Techniques of 2024, the 18th edition of our annual community-powered effort to identify the most innovative must-read web security research published in the last year]]></content:encoded></item><item><title>GenAI Patterns: Retrieval Augmented Generation (RAG)</title><link>https://martinfowler.com/articles/gen-ai-patterns/#rag</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 4 Feb 2025 14:57:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[A pre-trained GenAI model lacks recent and specific information about a
      domain.  and I explain how Retrieval
      Augmented Generation (RAG) can fill that gap.]]></content:encoded></item><item><title>1 Simple Technique to Scale Microservices Architecture 🚀</title><link>https://newsletter.systemdesign.one/p/how-to-scale-microservices</link><author>Neo Kim</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/84c96d29-cf27-47f0-9e1c-78ca1fa5f461_1280x720.gif" length="" type=""/><pubDate>Tue, 4 Feb 2025 14:04:08 +0000</pubDate><source url="https://newsletter.systemdesign.one/">Dev - System Design Newsletter</source><content:encoded><![CDATA[Get the powerful template to approach system design for FREE on newsletter sign-up:You will find references at the bottom of this page if you want to go deeper.Once upon a time, there was a tech startup.They tracked food deliveries in real-time.Yet they had only a few customers; so, a simple monolith architecture was enough.Until one day, an influencer with a massive following shared their app, and traffic skyrocketed.So they moved to microservices architecture for scale.Although it temporarily solved their scalability issues, there were newer problems.Implementing new features became difficult because of the dependencies between microservices.The operational complexity increased due to different programming languages across microservices.Authorization can make or break your application’s security and scalability. From managing dynamic permissions to implementing fine-grained access controls, the challenges grow as your requirements and users scale.This eBook will guide you through the 6 key requirements all authorization layers should include to avoid technical debt:Architectural and design considerations for building a scalable and secure authorization layer.20+ technologies, approaches, and standards to consider for your permission management systems.Practical insights, based on 500+ interviews with engineers, architects, and IAM professionals.Learn how to create an authorization solution that evolves with your business needs, while avoiding technical debt.How to Scale MicroservicesThey wanted to reduce the maintenance effort and scale microservices quickly.So they relied on automation.A microservice must be scaled to handle changing traffic.Yet scaling manually is slow and causes errors.So they use infrastructure as code to keep things simple; it’s used to deploy new instances based on demand.  means managing and provisioning infrastructure using code.Ready for the next technique?It’s difficult to understand and maintain APIs without a proper naming convention.Yet maintaining consistent naming for APIs across microservices is hard. So they generate APIs using code and store API definitions as JSON in a separate Git repository.Here are the benefits of code-generated APIs:API paths can be validated using linters.API naming conventions and request-response structures are standardized.API version changes can be tracked through tagging.It ensures API operations are defined.User-friendly API paths can be set up as default.Also it standardizes APIs built using different programming languages. Put simply, code generation makes it look like a single person wrote APIs across the system.Besides code generation ensures health checks are included in each microservice.A health check URL is included in the API specification of the microservice.An unhealthy service is removed from production to avoid service interruptions.A change to database schema shouldn’t break microservices depending on it. Yet sharing a single database across microservices introduces tight coupling and failure risk.So they set up separate databases for each microservice.But maintaining many databases is a huge effort; so, they create databases and tables using code.Here are its main benefits:Consistent database names.Ensure proper database indexes exist from the start.Easy to maintain and operate the database.Besides a microservice shouldn't talk to a database owned by another microservice. Instead, communication happens only via the microservice’s API owning the database. Otherwise, the data changes aren't safe as they bypass business logic.: use code generation and automation to scale microservices quickly. It reduces maintenance efforts and helps with version control.👋  -Do you want to level up at work and better understand technical discussions?My mission is to help you go from 0 to 1 in system design by spending less than 37 minutes a month.I launched the newsletter deep dives last month.Yet future deep dives, starting this month, are available only to paid subscribers.And subscription fees will be higher than the current fees.So pledge now to get access at a very low price.“This newsletter is an amazing resource for learning system design.” AlexSubscribe to get simplified case studies delivered straight to your inbox:Want to advertise in this newsletter? 📰Thank you for supporting this newsletter.You are now 123,001+ readers strong, very close to 124k. Let’s try to reach 124k readers by 10 February. Consider sharing this post with your friends and get rewards.]]></content:encoded></item><item><title>NumPy Techniques and Practical Examples</title><link>https://realpython.com/courses/numpy-techniques-practical-examples/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Tue, 4 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[The NumPy library is a Python library used for scientific computing. It provides you with a multidimensional array object for storing and analyzing data in a wide variety of ways. In this video course, you’ll see examples of some features NumPy provides that aren’t always highlighted in other tutorials. In this video course, you’ll learn how to:Create  from data stored in filesIdentify and remove  from a NumPy arrayUse  to reconcile the differences between datasetsAnalyze and chart specific parts of  dataCreate  versions of your own functions]]></content:encoded></item><item><title>22-year-old math wiz indicted for alleged DeFI hack that stole $65M</title><link>https://arstechnica.com/information-technology/2025/02/man-indicted-for-two-alleged-defi-hacks-that-stole-65-million/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2024/07/GettyImages-1230723421-1152x648.jpg" length="" type=""/><pubDate>Tue, 4 Feb 2025 13:25:11 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Federal prosecutors have indicted a man on charges he stole $65 million in cryptocurrency by exploiting vulnerabilities in two decentralized finance platforms and then laundering proceeds and attempting to extort swindled investors.The scheme, alleged in an indictment unsealed on Monday, occurred in 2021 and 2023 against the DeFI platforms KyberSwap and Indexed Finance. Both platforms provide automated services known as “liquidity pools” that allow users to move cryptocurrencies from one to another. The pools are funded with user-contributed cryptocurrency and are managed by smart contracts enforced by platform software.“Formidable mathematical prowess”The prosecutors said Andean Medjedovic, now 22 years old, exploited vulnerabilities in the KyberSwap and Indexed Finance smart contracts by using “manipulative trading practices.” In November 2023, he allegedly used hundreds of millions of dollars in borrowed cryptocurrency to cause artificial prices in the KyberSwap liquidity pools. According to the prosecutors, he then calculated precise combinations of trades that would induce the KyberSwap smart contract system—known as the AMM, or automated market makers—to “glitch,” as he wrote later.]]></content:encoded></item><item><title>3 Ways You’re Screwing up Platform Engineering – And How to Fix It • Steve Smith • GOTO 2024</title><link>https://www.youtube.com/watch?v=vQteH0qcVjE</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/vQteH0qcVjE?version=3" length="" type=""/><pubDate>Tue, 4 Feb 2025 13:01:19 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[This presentation was recorded at GOTO Copenhagen 2024. #GOTOcon #GOTOcph
https://gotocph.com

Steve Smith - Head of Scale Equal Experts

RESOURCES
https://bsky.app/profile/stevesmithtech.bsky.social
https://mastodon.social/@steve_smith@mastodon.social
https://www.linkedin.com/in/stevesmithtech
https://www.stevesmith.tech

ABSTRACT
Why is it so easy to screw up platform engineering, and how do you undo the damage?

Platform engineering… it’s the trendy new buzzword. For a thing we’ve been doing for years, ever since someone said “what if we re-did Solaris zones, but called it Docker instead?” It means building an internal engineering platform for your digital services or data pipelines. It allows you to scale teams up and down, and supercharge their abilities to deliver outcomes.

But here’s a thing that nobody likes to talk about. It’s easy to totally screw up platform engineering. When that happens, there’s a huge negative impact on your engineering culture, and your teams aren’t able to achieve their goals. So how do you avoid screwing up platform engineering, and if it does happen to you, can you actually fix it?

I’ve spent years in platform leadership roles, building internal engineering platforms at different scaleups and enterprise organizations. I’ve had successes, and I’ve had failures. I’ll cover the following irreversible ways to screw up, and how to start reversing out of them:

• Power tools - teams spend all their time configuring Kafka, Kubernetes, Istio etc., because the platform is based on overpowered tech
• Technology anarchy - N teams do the same task in N different ways, because the platform has no opinions on tech choices, ways of working, or path to prod
• Teams as tickets - making teams interact with the platform via tickets

And I’ll explain why all of these can be traced back to a scaling problem, the granddaddy screw up of platform engineering - your mindset is platform as a project, not as a product.

Takeaways
• Platform engineering is about creating an enabler of business value, so your teams can move faster and safer than ever before
• Avoid using platform engineering power tools e.g. Kafka, Kubernetes, Istio. If you’re using them, start measuring unplanned work and plan to migrate away
• Avoid being unopinionated on tech stack, ways of working, and path to prod. If you’re doing that, capture your leadership team’s expectations and track team commitments
• Avoid only interacting with teams through a ticketing system. If you’re stuck there, start collaborating with teams on self-service paved roads
• The key to platform engineering success is a platform as a product mindset [...]

TIMECODES
00:00 Intro
03:36 What is platform engineering?
05:37 No. 1: Power tools
18:32 No. 2: Technology anarchy
31:18 No. 3: Ticketing hell
40:04 Takeaways
42:24 Outro

Download slides and read the full abstract here:
https://gotocph.com/2024/sessions/3235

RECOMMENDED BOOKS
Steve Smith • Measuring Continuous Delivery • https://leanpub.com/measuringcontinuousdelivery
Steve Smith & Denise Yu • A Children's A to Z of Continuous Delivery • https://leanpub.com/achildrensatozofcontinuousdelivery
Steve Smith & Matthew Skelton • Build Quality In • https://leanpub.com/buildqualityin

https://bsky.app/profile/gotocon.com
https://twitter.com/GOTOcon
https://www.linkedin.com/company/goto-
https://www.instagram.com/goto_con
https://www.facebook.com/GOTOConferences
#PlatformEngineering #PlatformLeadership #Kubernetes #ApacheKafka #Istio #TechnologyAnarchy #CICD #SteveSmith

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Cybersecurity for Businesses of All Sizes: A Blueprint for Protection</title><link>https://blogs.cisco.com/security/cybersecurity-for-businesses-of-all-sizes-a-blueprint-for-protection/</link><author>Yuri Kramarz</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Tue, 4 Feb 2025 13:00:10 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[Developing a robust cybersecurity practice involves implementing multiple layers of security measures that are interconnected and continually monitored.]]></content:encoded></item><item><title>Popular Linux Orgs Freedesktop, Alpine Linux Are Scrambling For New Web Hosting</title><link>https://linux.slashdot.org/story/25/02/04/0044210/popular-linux-orgs-freedesktop-alpine-linux-are-scrambling-for-new-web-hosting?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>slashdot</category><category>linux</category><pubDate>Tue, 4 Feb 2025 13:00:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: In what is becoming a sadly regular occurrence, two popular free software projects, X.org/Freedesktop.org and Alpine Linux, need to rally some of their millions of users so that they can continue operating. Both services have largely depended on free server resources provided by Equinix (formerly Packet.net) and its Metal division for the past few years. Equinix announced recently that it was sunsetting its bare-metal sales and services, or renting out physically distinct single computers rather than virtualized and shared hardware. As reported by the Phoronix blog, both free software organizations have until the end of April to find and fund new hosting, with some fairly demanding bandwidth and development needs.
 
An issue ticket on Freedesktop.org's GitLab repository provides the story and the nitty-gritty needs of that project. Both the X.org foundation (home of the 40-year-old window system) and Freedesktop.org (a shared base of specifications and technology for free software desktops, including Wayland and many more) used Equinix's donated space. [...] Alpine Linux, a small, security-minded distribution used in many containers and embedded devices, also needs a new home quickly. As detailed in its blog, Alpine Linux uses about 800TB of bandwidth each month and also needs continuous integration runners (or separate job agents), as well as a development box. Alpine states it is seeking co-location space and bare-metal servers near the Netherlands, though it will consider virtual machines if bare metal is not feasible.]]></content:encoded></item><item><title>Quiz: How to Split a String in Python</title><link>https://realpython.com/quizzes/python-split-string/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Tue, 4 Feb 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[Python’s  method lets you divide a string into a list of substrings based on a specified delimiter. By default,  separates at whitespace, including spaces, tabs, and newlines. You can customize  to work with specific delimiters using the  parameter, and control the amount of splits with .]]></content:encoded></item><item><title>OpenAI o3-mini - Thinking AI for Free…For Everyone!</title><link>https://www.youtube.com/watch?v=oKx2xQQl--Q</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/oKx2xQQl--Q?version=3" length="" type=""/><pubDate>Tue, 4 Feb 2025 11:30:13 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Weights & Biases and sign up for a free demo here: https://wandb.me/papers

o3 mini: https://openai.com/index/openai-o3-mini/
OpenAI Deep Research: https://openai.com/index/introducing-deep-research/

🤝 Interested in sponsoring us? Click here: https://www.jotform.com/241831324457354

Sources:
https://x.com/hxiao/status/1885522459329520089?s=46
https://x.com/techikansh/status/1885429093862187008?s=46
https://x.com/rlancemartin/status/1885748894220554445?s=46
https://x.com/buccocapital/status/1885792154129219959?s=46
https://x.com/_akhaliq/status/1885733581651050586?s=46
https://x.com/_akhaliq/status/1885833163764646267?s=46
https://x.com/aidan_mclau/status/1886078444855034055?s=46
https://x.com/cj_zzzz/status/1885740906034196725?s=46
https://x.com/bbssppllvv/status/1886136914446630978?s=46
https://x.com/hxiao/status/1885700308720091280

Open source Deep Research-ish thing (unofficial, of course):
https://x.com/nickscamara_/status/1886287956291338689?s=46

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu]]></content:encoded></item><item><title>Maximizing Cloud Efficiency with Jerzy Grzywinski and Brent Segner</title><link>https://softwareengineeringdaily.com/2025/02/04/maximizing-cloud-efficiency-with-jerzy-grzywinski-and-brent-segner/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=maximizing-cloud-efficiency-with-jerzy-grzywinski-and-brent-segner</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED1045788818.mp3" length="" type=""/><pubDate>Tue, 4 Feb 2025 10:00:08 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[Compute optimization in a cloud environment is a common challenge because of the need to balance performance, cost, and resource availability. The growing use of GPUs for workloads, including AI, is also increasing the complexity and importance of optimization given the relatively high cost of GPU cloud computation.Jerzy Grzywinski is a Senior Director of Software Engineering and leads FinOps at Capital One. Brent Segner is a distinguished engineer at Capital One and is focused on performance engineering and cloud cost optimization. Jerzy and Brent joined the show with Sean Falconer to talk about methods to measure compute efficiency, horizontal versus vertical scaling, how to think about adopting new instance types, the effect of different languages on compute efficiency, and much more.Full Disclosure: This episode is sponsored by .]]></content:encoded></item><item><title>#419 Is your back end popular?</title><link>https://pythonbytes.fm/episodes/show/419/is-your-back-end-popular</link><author></author><category>Python bytes</category><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://pythonbytes.fm/episodes/download/419/is-your-back-end-popular.mp3" length="" type=""/><pubDate>Tue, 4 Feb 2025 08:00:00 +0000</pubDate><source url="https://pythonbytes.fm/">Python Bytes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What are ACID Transactions in Databases?</title><link>https://blog.algomaster.io/p/what-are-acid-transactions-in-databases</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9ceb65c-70e6-4f3e-9511-f6bc5da93d13_1308x1086.png" length="" type=""/><pubDate>Tue, 4 Feb 2025 05:31:07 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[Imagine you’re running an e-commerce application. A customer places an order, and your system needs to deduct the item from inventory, charge the customer’s credit card, and record the sale in your accounting system—all at once. What happens if the payment fails but your inventory count has already been reduced? Or if your application crashes halfway through the process?This is where  come into play. They ensure that all the steps in such critical operations happen reliably and consistently.ACID is an acronym that refers to the set of 4 key properties that define a transaction: Atomicity, Consistency, Isolation, and In this article, we’ll dive into what each of the ACID properties mean, why they are important, and how they are implemented in databases.If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .A  in the context of databases is a sequence of one or more operations (such as inserting, updating, or deleting records) that the database treats as . It either fully succeeds or fully fails, with no in-between states.When you send money to a friend, two things happen:Money is deducted from your account.Money is added to their account.These two steps form . If either step fails, both are canceled.Without transactions, databases could end up in inconsistent states. : Your money is deducted, but your friend never receives it.: Two people booking the last movie ticket at the same time.Transactions solve these problems by enforcing rules like  (Atomicity, Consistency, Isolation, Durability).Now, lets looks at each of the ACID properties.Atomicity ensures that a transaction—comprising multiple operations—executes as a unit of work: it either  succeeds (commits) or  fails (rolls back). If any part of the transaction fails, the entire transaction is rolled back, and the database is restored to a state exactly as it was before the transaction began.In a money transfer transaction, if the credit step fails, the debit step cannot be allowed to stand on its own. This prevents inconsistent states like “money disappearing” from one account without showing up in another.Atomicity abstracts away the complexity of manually undoing changes if something goes wrong.How Databases Implement AtomicityDatabases use two key mechanisms to guarantee atomicity.1. Transaction Logs (Write-Ahead Logs)Every operation is recorded in a  before it’s applied to the actual database table.If a failure occurs, the database uses this log to  incomplete changes.Once the WAL entry is safely on disk, the database proceeds with modifying the in-memory pages that contain rows for  and .When the operations succeed:The database marks  as  in the transaction log.The newly updated balances for A and B will eventually get flushed from memory to their respective data files on disk.If the database crashes  the log entry is written but  the data files are fully updated, the WAL provides a way to recover:On restart, the database checks the WAL.It sees  was committed.It reapplies the  operations to ensure the final balances are correct in the data files.If the transaction had not committed (or was marked as “in progress”) at the time of the crash, the database would  those changes using information in the log, leaving the table as if the transaction never happened.2. Commit/Rollback ProtocolsDatabases provide commands like , , and Any changes made between  and  are considered “in-progress” and won’t be permanently applied unless the transaction commits successfully.If any step fails, or if you explicitly issue a , all changes since the start of the transaction are undone. in the context of ACID transactions ensures that any transaction will bring the database from one valid state to another valid state—never leaving it in a broken or “invalid” state.It means that all the data integrity constraints, such as  (no duplicate IDs),  (related records must exist in parent tables), and (age can’t be negative), are satisfied before and after the transaction.If a transaction tries to violate these rules, it will not be committed, and the database will revert to its previous state.You have two tables in an e-commerce database: (with columns: , , etc.) (with columns: , , , etc.): You can’t place an order for a product if  is greater than the  in the  table.If the product’s  was 8 (less than what we’re trying to order), the database sees that the new value would be  which breaks the consistency rule (it should not go negative).The transaction fails or triggers a rollback, preventing the database from ending in an invalid state.How to Implement ConsistencyDatabase Schema Constraints, , , ,  constraints, and other schema definitions ensure no invalid entries are allowed.Triggers and Stored ProceduresTriggers can automatically check additional rules whenever rows are inserted, updated, or deleted.Stored procedures can contain logic to validate data before committing.Application-Level SafeguardsWhile the database enforces constraints at a lower level, applications often add extra checks—like ensuring business rules are followed or data is validated before it even reaches the database layer. ensures that concurrently running transactions do not interfere with each other’s intermediate states.Essentially, while a transaction is in progress, its updates (or intermediate data) remain invisible to other ongoing transactions—giving the illusion that each transaction is running sequentially, one at a time.Without isolation, two or more transactions could read and write partial or uncommitted data from each other, causing incorrect or inconsistent results.With isolation, developers can reason more reliably about how data changes will appear to other transactions.To understand how isolation works, it helps to see what can go wrong without proper isolation.  Common concurrency anomalies include:Transaction A reads data that Transaction B has modified but not yet committed.If Transaction B then rolls back, Transaction A ends up holding an invalid or “dirty” value that never truly existed in the committed state.Transaction A reads the same row(s) multiple times during its execution but sees different data because another transaction updated or deleted those rows in between A’s reads.Transaction A performs a query that returns a set of rows. Another transaction inserts, updates, or deletes rows that match A’s query conditions.If A re-runs the same query, it sees a different set of rows (“phantoms”).Databases typically allow you to choose an , which balances data correctness with performance.Higher isolation levels provide stronger data consistency but can reduce system performance by increasing the wait times for transactions. Let's explore the four common isolation levels:Allows dirty reads; transactions can see uncommitted changes.Rarely used, as it can lead to severe anomalies.A transaction sees only data that has been committed at the moment of reading.Prevents dirty reads, but non-repeatable reads and phantom reads can still occur.Ensures if you read the same rows multiple times within a transaction, you’ll get the same values (unless you explicitly modify them).Prevents dirty reads and non-repeatable reads, but phantom reads may still happen (depending on the database engine).The highest level of isolation, acting as if all transactions happen sequentially one at a time.Prevents dirty reads, non-repeatable reads, and phantom reads.Most expensive in terms of performance and concurrency because it can require more locking or more conflict checks.How Databases Enforce IsolationPessimistic Concurrency ControlRows or tables are locked so that no other transaction can read or write them until the lock is released.Can lead to blocking or deadlocks if multiple transactions compete for the same locks.2. MVCC (Multi-Version Concurrency Control)Optimistic Concurrency ControlInstead of blocking reads, the database keeps multiple versions of a row.Readers see a consistent snapshot of data (like a point-in-time view), while writers create a new version of the row when updating.This approach reduces lock contention but requires carefully managing row versions and cleanup (vacuuming in PostgreSQL, for example).A form of MVCC where each transaction sees data as it was at the start (or a consistent point) of the transaction.Prevents non-repeatable reads and dirty reads. Phantom reads may still occur unless the isolation level is fully serializable. ensures that once a transaction has been committed, the changes it made will survive, even in the face of power failures, crashes, or other catastrophic events. In other words, once a transaction says “done,” the data is permanently recorded and cannot simply disappear.How Databases Ensure Durability1. Transaction Logs (Write-Ahead Logging)Most relational databases rely on a  to preserve changes before they’re written to the main data files:: The intended operations (updates, inserts, deletes) are recorded in the WAL on durable storage (disk).: Once the WAL entry is safely persisted, the database can mark the transaction as committed.Apply Changes to Main Data Files: The updated data eventually gets written to the main files—possibly first in memory, then flushed to disk.If the database crashes, it uses the WAL during :: Any committed transactions not yet reflected in the main files are reapplied.: Any incomplete (uncommitted) transactions are rolled back to keep the database consistent.2. Replication / RedundancyIn addition to WAL, many systems use replication to ensure data remains durable even if hardware or an entire data center fails.: Writes are immediately copied to multiple nodes or data centers. A transaction is marked committed only if the primary and at least one replica confirm it’s safely stored.: Changes eventually sync to other nodes, but there is a (small) window where data loss can occur if the primary fails before the replica is updated.Regular  provide a safety net beyond logs and replication. In case of severe corruption, human error, or catastrophic failure:: Capture the entire database at a point in time.Incremental/Differential Backups: Store changes since the last backup for faster, more frequent backups.: Ensures backups remain safe from localized disasters, allowing you to restore data even if hardware is damaged.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re finding this newsletter helpful and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>DtSR Episode 639 - Richard Bird Famous With 12 People</title><link></link><author></author><category>Down the Security Rabbit Hole Podcast</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/www.buzzsprout.com/2153215/episodes/16549969-dtsr-episode-639-richard-bird-famous-with-12-people.mp3" length="" type=""/><pubDate>Tue, 4 Feb 2025 05:00:00 +0000</pubDate><source url="https://blogwh1t3rabbit.medium.com/">Down the Security Rabbithole Podcast (DtSR)</source></item><item><title>Key Issues Shaping State-Level Tech Policy</title><link>https://www.eff.org/deeplinks/2025/02/key-issues-shaping-state-level-tech-policy</link><author>Rindala Alajaji</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/protest-2024-2.jpg" length="" type=""/><pubDate>Tue, 4 Feb 2025 00:27:26 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How State Tech Policies in 2024 Set the Stage for 2025</title><link>https://www.eff.org/deeplinks/2025/02/how-state-tech-policies-2024-set-stage-2025</link><author>Rindala Alajaji</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/congress-action_0.jpg" length="" type=""/><pubDate>Tue, 4 Feb 2025 00:17:24 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[238 technology-related bills passed across 46 states, marking a 163% increase from the previous year.41 states passed 107 bills regulating AI.22 states passed laws addressing Non-Consensual Intimate Images (NCII) and child sexual abuse material (CSAM) generated or altered by AI or digital means.17 states enacted 22 laws regulating the use of generative AI in political campaigns.6 states created 19 new commissions, task forces, and legislative committees to assess the impact of AI and explore its regulation or beneficial use. For example, California created a working group to guide the safe use of AI in education.15 states passed 18 bills related to funding AI research or initiatives. For example, Nebraska allocated funds to explore how AI can assist individuals with dyslexia.3 states made incremental changes to antitrust laws, while 6 states joined federal regulators in pursuing 6 significant cases against tech companies for anticompetitive practices.California passed the most tech-related legislation in 2024, with 26 bills, followed by Utah, which passed 13 bills.]]></content:encoded></item><item><title>Tutorial: publishing ESM-based npm packages with TypeScript</title><link>https://2ality.com/2025/02/typescript-esm-packages.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Tue, 4 Feb 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[During the last two years, ESM support in TypeScript, Node.js and browsers has made a lot of progress. In this blog post, I explain my modern setup that is relatively simple – compared to what we had to do in the past:]]></content:encoded></item><item><title>Fat Rand: How Many Lines Do You Need To Generate A Random Number?</title><link>http://lucumr.pocoo.org/2025/2/4/fat-rand</link><author>Armin Ronacher</author><category>dev</category><category>blog</category><pubDate>Tue, 4 Feb 2025 00:00:00 +0000</pubDate><source url="https://lucumr.pocoo.org/feed.atom">Armin Ronacher Blog</source><content:encoded><![CDATA[I recently wrote about dependencies in Rust.  The feedback, both within and outside
the Rust community, was very different.  A lot of people, particularly
some of those I greatly admire expressed support.  The Rust community, on
the other hand, was very dismissive on on Reddit and Lobsters.Last time, I focused on the  crate, but I also want to
show you a different one that I come across once more: .  It has a
similarly out-of-whack value-to-dependency ratio, but in a slightly
different way.  More than , you are quite likely to use
it.  If for instance if you want to generate a random UUID, the 
crate will depend on it.  Due to its nature it also has a high security
exposure.I don't want to frame this as “ is a bad crate”.  It's not a bad
crate at all!  It is however a crate that does not appear very concerned
about how many dependencies it has, and I want to put this in perspective:
of all the dependencies and lines of codes it pulls in, how many does it
actually use?As the name implies, the  crate is capable of calculating random
numbers.  The crate itself has seen a fair bit of churn: for instance 0.9
broke backwards compatibility with 0.8.  So, as someone who used that
crate, I did what a responsible developer is supposed to do, and upgraded
the dependency.  After all, I don't want to be the reason there are two
versions of  in the dependency tree.  After the upgrade, I was
surprised how fat that dependency tree has become over the last nine
months.Today, this is what the dependency tree looks like for the default feature
set on macOS and Linux:x v0.1.0 (/private/tmp/x)
└── rand v0.9.0
    ├── rand_chacha v0.9.0
    │   ├── ppv-lite86 v0.2.20
    │   │   └── zerocopy v0.7.35
    │   │       ├── byteorder v1.5.0
    │   │       └── zerocopy-derive v0.7.35 (proc-macro)
    │   │           ├── proc-macro2 v1.0.93
    │   │           │   └── unicode-ident v1.0.16
    │   │           ├── quote v1.0.38
    │   │           │   └── proc-macro2 v1.0.93 (*)
    │   │           └── syn v2.0.98
    │   │               ├── proc-macro2 v1.0.93 (*)
    │   │               ├── quote v1.0.38 (*)
    │   │               └── unicode-ident v1.0.16
    │   └── rand_core v0.9.0
    │       ├── getrandom v0.3.1
    │       │   ├── cfg-if v1.0.0
    │       │   └── libc v0.2.169
    │       └── zerocopy v0.8.14
    ├── rand_core v0.9.0 (*)
    └── zerocopy v0.8.14
About a year ago, it looked like this:x v0.1.0 (/private/tmp/x)
└── rand v0.8.5
    ├── libc v0.2.169
    ├── rand_chacha v0.3.1
    │   ├── ppv-lite86 v0.2.17
    │   └── rand_core v0.6.4
    │       └── getrandom v0.2.10
    │           ├── cfg-if v1.0.0
    │           └── libc v0.2.169
    └── rand_core v0.6.4 (*)
So, let's investigate what all these dependencies do. The current version
pulls in quite a lot.If you want to audit the entire dependency chain, you end up with
maintainers that form eight distinct groups:: rust core + various externals: rust core + Alex Crichton and : rust nursery + rust-random and : Google (via two ICs there, Google
does not publish): Andrew Gallant, , , : David TolnayIf I also cared about WASM targets, I'd have to consider even more
dependencies.So let's vendor it.  How much code is there?  After removing all tests, we
end up with  vendored taking up  disk
space.  Tokei reports .Now this is a bit misleading, because like many times most of this is
within .  But how much of  does 
need?  A single function:: : -> For that single function (and the information which DLL it needs link
into), we are compiling and downloading megabytes of .
Longer term this might not be necessary, but today
it is.On Unix, it's harder to avoid  because it tries multiple APIs.
These are mostly single-function APIs, but some non-portable constants
make  difficult to avoid.Beyond the platform dependencies, what else is there? (the 's picked default randon number generator)
alone comes to 3,587 lines of code including 168 unsafe blocks.  If
the goal of using  was to avoid , there is still
a ton of  remaining.The combination of , , , and
 comes to 49,114 lines of code. clocks in at 3,000 lines of code.The pair of  and  together?  14,004 lines
of code.All of these are great crates, but do I need all of this just to generate a random number?The Rust developer community on Reddit
doesn't seem very concerned.  The main sentiment is that  now uses less
 so that's benefit enough.  While the total amount of unsafe
probably did not go down, that moved unsafe is is now in a common crate
written by people that know how to use unsafe ().  There is
also the sentiment that all of this doesn't matter anyways, because we
will will all soon depend on  everywhere anyways, as more and
more dependencies are switching over to it.Maybe this points to Rust not having a large enough standard library.
Perhaps features like terminal size detection and random number generation
should be included.  That at least is what people pointed out on Twitter.We already treat crates like , , and  as if they
were part of the standard library.  The difference is that I can trust the
standard library as a whole—it comes from a single set of authors, making
auditing easier.  If these external, but almost standard crates were more
cautious about dependencies and make it more of a goal to be auditable, we
would all benefit.Or maybe this is just how Rust works now.  That would make me quite sad. it looks like there is some appetite in  to improve on
this.a stripped down version of  (which does not require 
or most of the rust-crypto ecosystem) might replace :
PR #934.if you use Rust 1.71 or later,  becomes mostly a
no-op if you compile with .Edit: This post originally incorrectly said that getrandom depends on
windows-sys.  That is incorrect, it only depends on windows-targets.]]></content:encoded></item><item><title>January 2025 GNU Spotlight with Amin Bandali: Seventeen new GNU releases!</title><link>http://www.fsf.org/blogs/community/january-2025-gnu-spotlight-with-amin-bandali</link><author></author><category>linux</category><pubDate>Mon, 3 Feb 2025 21:21:53 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[Seventeen new GNU releases in the last month (as of January 31, 2025):
]]></content:encoded></item><item><title>NetChoice Sues To Block Maryland&apos;s Kids Code, Saying It Violates the First Amendment</title><link>https://yro.slashdot.org/story/25/02/03/2028246/netchoice-sues-to-block-marylands-kids-code-saying-it-violates-the-first-amendment?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Mon, 3 Feb 2025 21:00:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[NetChoice has filed (PDF) its 10th lawsuit challenging state internet regulations, this time opposing Maryland's Age-Appropriate Design Code Act. The Verge's Lauren Feiner reports: NetChoice has become one of the fiercest -- and most successful -- opponents of age verification, moderation, and design code laws, all of which would put new obligations on tech platforms and change how users experience the internet. [...] NetChoice's latest suit opposes the Maryland Age-Appropriate Design Code Act, a rule that echoes a California law of a similar name. In the California litigation, NetChoice notched a partial win in the Ninth Circuit Court of Appeals, which upheld the district court's decision to block a part of the law requiring platforms to file reports about their services' impact on kids. (It sent another part of the law back to the lower court for further review.)
 
A similar provision in Maryland's law is at the center of NetChoice's complaint. The group says that Maryland's reporting requirement lets regulators subjectively determine the "best interests of children," inviting "discriminatory enforcement." The reporting requirement on tech companies essentially mandates them "to disparage their services and opine on far-ranging and ill-defined harms that could purportedly arise from their services' 'design' and use of information," NetChoice alleges. NetChoice points out that both California and Maryland have passed separate online privacy laws, which NetChoice Litigation Center director Chris Marchese says shows that "lawmakers know how to write laws to protect online privacy when what they want to do is protect online privacy."
 
Supporters of the Maryland law say legislators learned from California's challenges and "optimized" their law to avoid questions about speech, according to Tech Policy Press. In a blog analyzing Maryland's approach, Future of Privacy Forum points out that the state made some significant changes from California's version -- such as avoiding an "express obligationâ to determine users' ages and defining the "best interests of children." The NetChoice challenge will test how well those changes can hold up to First Amendment scrutiny. NetChoice has consistently maintained that even well-intentioned attempts to protect kids online are likely to backfire. Though the Maryland law does not explicitly require the use of specific age verification tools, Marchese says it essentially leaves tech platforms with a no-win decision: collect more data on users to determine their ages and create varied user experiences or cater to the lowest common denominator and self-censor lawful content that might be considered inappropriate for its youngest users. And similar to its arguments in other cases, Marchese worries that collecting more data to identify users as minors could create a "honey pot" of kids' information, creating a different problem in attempting to solve another.]]></content:encoded></item><item><title>Everyone knows your location (News)</title><link>https://changelog.com/news/130</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/news/130/changelog-news-130.mp3" length="" type=""/><pubDate>Mon, 3 Feb 2025 20:30:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Tim Sh tracked himself down through in-app ads, Sniffnet comfortably monitors your Internet traffic, Cate Huston opines on what makes a good team, Victor Shepelev draws on 25 years of coding to share seven things he now knows & Grant Slatton tells you how to write a good design document.Changelog++ members support our work, get closer to the metal, and make the ads disappear. Join today!Augment Code – Developer AI that uses deep understanding of your large codebase and how you build software to deliver personalized code suggestions and insights. Augment provides relevant, contextualized code right in your IDE or Slack. It transforms scattered knowledge into code or answers, eliminating time spent searching docs or interrupting teammates.
]]></content:encoded></item><item><title>Open Licensing Promotes Culture and Learning. That&apos;s Why EFF Is Upgrading its Creative Commons Licenses.</title><link>https://www.eff.org/deeplinks/2025/02/open-licensing-promotes-culture-and-learning-thats-why-eff-upgrading-its-creative</link><author>Mitch Stoltz</author><category>tech</category><pubDate>Mon, 3 Feb 2025 19:49:46 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[At EFF, we’re big fans of the Creative Commons project, which makes copyright work in empowering ways for people who want to share their work widely. EFF uses Creative Commons licenses on nearly all of our public communications. To highlight the importance of open licensing as a tool for building a shared culture, we are upgrading the license on our website to the latest version, Creative Commons Attribution 4.0 International.Open licenses like Creative Commons are an important tool for sharing culture and learning. They allow artists and creators a simple way to encourage widespread, free distribution of their work while keeping just the rights they want for themselves—such as the right to be credited as the work’s author, the right to modify the work, or the right to control commercial uses.Without tools like Creative Commons, copyright is frequently a roadblock to sharing and preserving culture. Copyright is ubiquitous, applying automatically to most kinds of creative work from the moment they are “fixed in a tangible medium.” Copyright carries draconian penalties unknown in most areas of U.S. law, like “statutory damages” with no proof of harm and the possibility of having to pay the rightsholder’s attorney fees. And it can be hard to learn who owns a copyright in any given work, given that copyrights can last a century or more. All of these make it risky and expensive to share and re-use creative works, or sometimes even to preserve them and make them accessible to future generations.Open licensing helps culture and learning flourish. With many millions of works now available under Creative Commons licenses, creators and knowledge-seekers have reassurance that these works of culture and learning can be freely shared and built upon without risk.The current suite of Creative Commons licenses has thoughtful, powerful features. It’s written to work effectively in many countries, using language that can be understood in the context of different copyright laws around the world. It addresses legal regimes other than copyright that can interfere with free re-use of creative materials, like database rights, anti-circumvention laws, and rights of publicity or personality.And importantly, the 4.0 licenses also make clear that giving credit to the author (something all of the Creative Commons licenses require) can be done in various ways, and that technical failures don't expose users to lawsuits by copyright trolls.At EFF, we want our work to be seen and shared widely. That’s why we’ve made our content available under Creative Commons licenses for many years. Today, in that spirit, we are updating the license for most materials on our website, www.eff.org, to Creative Commons Attribution 4.0 International.]]></content:encoded></item><item><title>How Precision Time Protocol handles leap seconds</title><link>https://engineering.fb.com/2025/02/03/production-engineering/how-precision-time-protocol-ptp-handles-leap-seconds/</link><author></author><category>dev</category><category>official</category><pubDate>Mon, 3 Feb 2025 17:00:34 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Compiling CSS With Vite and Lightning CSS</title><link>https://css-tricks.com/compiling-css-with-vite-and-lightning-css/</link><author>Ryan Trimble</author><category>frontend</category><pubDate>Mon, 3 Feb 2025 15:23:37 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[Suppose you follow CSS feature development as closely as we do here at CSS-Tricks. In that case, you may be like me and eager to use many of these amazing tools but find browser support sometimes lagging behind what might be considered “modern” CSS (whatever that means).Even if browser vendors all have a certain feature released, users might not have the latest versions!We can certainly plan for this a number of ways:feature detection with  designsFor even extra help, we turn to build tools. Chances are, you’re already using some sort of build tool in your projects today. CSS developers are  familiar with CSS pre-processors (such as Sass or Less), but if you don’t know, these are tools capable of compiling many CSS files into one stylesheet. CSS pre-processors help make organizing CSS a lot easier, as you can move parts of CSS into related folders and import things as needed.Pre-processors do not  provide organizational superpowers, though. Sass gave us a crazy list of features to work with, including:For a while, this big feature set provided a means of filling gaps missing from CSS, making Sass (or whatever preprocessor you fancy) feel like a necessity when starting a new project. CSS has evolved a lot since the release of Sass — we have so many of those features in CSS today — so it doesn’t quite feel that way anymore, especially now that we have native CSS nesting and custom properties.Along with CSS pre-processors, there’s also the concept of -processing. This type of tool usually helps transform compiled CSS in different ways, like auto-prefixing properties for different browser vendors, code minification, and more. PostCSS is the big one here, giving you tons of ways to manipulate and optimize your code, another step in the build pipeline.In many implementations I’ve seen, the build pipeline typically runs roughly like this:CSS is usually handled in that first part, which includes running CSS pre- and post-processors (though post-processing might also happen after Step 2). As mentioned, the continued evolution of CSS makes it less necessary for a tool such as Sass, so we might have an opportunity to save some time.Awarded “” and “” from the State of JavaScript 2024 survey, Vite certainly seems to be one of the more popular build tools available. Vite is mainly used to build reactive JavaScript front-end frameworks, such as Angular, React, Svelte, and Vue (made by the same developer, of course). As the name implies, Vite is crazy fast and can be as simple or complex as you need it, and has become one of my favorite tools to work with.Vite is mostly thought of as a  tool for  projects, but you can use it without writing any JavaScript at all. Vite works with Sass, though you still need to install Sass as a dependency to include it in the build pipeline. On the other hand, Vite also automatically supports compiling CSS with no extra steps. We can organize our CSS code how we see fit, with no or very minimal configuration necessary. Let’s check that out.We will be using Node and npm to install Node packages, like Vite, as well as commands to run and build the project. If you do not have  or  installed, please check out the download page on their website.Navigate a terminal to a safe place to create a new project, then run:The command line interface will ask a few questions, you can keep it as simple as possible by choosing  and  which will provide you with a starter template including some no-frameworks-attached HTML, CSS, and JavaScript files to help get you started.Before running other commands, open the folder in your IDE (integrated development environment, such as VSCode) of choice so that we can inspect the project files and folders.If you would like to follow along with me,  the following files that are unnecessary for demonstration:We should only have the following files left in out project folder:Let’s also replace the contents of  with an empty HTML template:<!doctype html>

<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
	
    <title>CSS Only Vite Project</title>
  </head>
  <body>
    <!-- empty for now -->
  </body>
</html>One last piece to set up is Vite’s dependencies, so let’s run the npm installation command:A short sequence will occur in the terminal. Then we’ll see a new folder called  and a  file added in our file viewer. is used to house all package files installed through node package manager, and allows us to import and use installed packages throughout our applications. is a file usually used to make sure a development team is all using the same versions of packages and dependencies.We most likely won’t need to touch these things, but they are necessary for Node and Vite to process our code during the build. Inside the project’s root folder, we can create a  folder to contain the CSS we will write. Let’s create one file to begin with, , which we can use to test out Vite.├── public/
├── styles/
|   └── main.css
└──index.htmlIn our  file, inside the  section, we can include a  tag pointing to the CSS file:<head>
  <meta charset="UTF-8" />
  <link rel="icon" type="image/svg+xml" href="/vite.svg" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
	
  <title>CSS Only Vite Project</title>

  <!-- Main CSS -->
  <link rel="stylesheet" href="styles/main.css">
</head>Let’s add a bit of CSS to :body {
  background: green;
}It’s not much, but it’s all we’ll need at the moment! In our terminal, we can now run the Vite build command using :With everything linked up properly, Vite will build things based on what is available within the  file, including our linked CSS files. The build will be  fast, and you’ll be returned to your terminal prompt.The newly generated  folder is Vite’s default output directory, which we can open and see our processed files. Checking out  (the filename will include a unique hash for cache busting), and you’ll see the code we wrote, minified here.Now that we know how to make Vite aware of our CSS, we will probably want to start writing more CSS for it to compile.As quick as Vite is with our code, constantly re-running the build command would still get very tedious. Luckily, Vite provides its own development server, which includes a live environment with hot module reloading, making changes appear instantly in the browser. We can start the Vite development server by running the following terminal command:Vite uses the default network port  for the development server. Opening the  address in your browser will display a blank screen with a green background.Adding any HTML to the  or CSS to , Vite will reload the page to display changes. To stop the development server, use the keyboard shortcut + or close the terminal to kill the process.At this point, you pretty much know all you need to know about how to compile CSS files with Vite. Any CSS file you link up will be included in the built file.Organizing CSS into Cascade LayersOne of the items on my 2025 CSS Wishlist is the ability to apply a cascade layer to a  tag. To me, this might be helpful to organize CSS in a meaningful ways, as well as fine control over the cascade, with the benefits cascade layers provide. Unfortunately, this is a rather difficult ask when considering the way browsers paint styles in the viewport. This type of functionality is being discussed between the CSS Working Group and TAG, but it’s unclear if it’ll move forward.With Vite as our build tool, we can replicate the concept as a way to organize our built CSS. Inside the  file, let’s add the @layer at-rule to set the cascade order of our layers. I’ll use a couple of layers here for this demo, but feel free to customize this setup to your needs./* styles/main.css */
@layer reset, layouts;This is all we’ll need inside our , let’s create another file for our reset. I’m a fan of my friend Mayank‘s modern CSS reset, which is available as a Node package. We can install the reset by running the following terminal command:npm install @acab/reset.cssNow, we can import Mayank’s reset into our newly created  file, as a cascade layer:/* styles/reset.css */
@import '@acab/reset.css' layer(reset);If there are any other reset layer stylings we want to include, we can open up another  block inside this file as well./* styles/reset.css */
@import '@acab/reset.css' layer(reset);

@layer reset {
  /* custom reset styles */
}This  statement is used to pull packages from the  folder. This folder is not  available in the built, public version of a website or application, so referencing this might cause problems if not handled properly.Now that we have two files ( and ), let’s link them up in our  file. Inside the  tag, let’s add them after :<head>
  <meta charset="UTF-8" />
  <link rel="icon" type="image/svg+xml" href="/vite.svg" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
	
  <title>CSS Only Vite Project</title>
	
  <link rel="stylesheet" href="styles/main.css">
  <link rel="stylesheet" href="styles/reset.css">
</head>The idea here is we can add each CSS file, in the order we need them parsed. In this case, I’m planning to pull in each file named after the cascade layers setup in the main.css file. This may not work for every setup, but it is a helpful way to keep in mind the precedence of how cascade layers affect computed styles when rendered in a browser, as well as grouping similarly relevant files.Since we’re in the  file, we’ll add a third CSS  for .<head>
  <meta charset="UTF-8" />
  <link rel="icon" type="image/svg+xml" href="/vite.svg" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
	
  <title>CSS Only Vite Project</title>
	
  <link rel="stylesheet" href="styles/main.css">
  <link rel="stylesheet" href="styles/reset.css">
  <link rel="stylesheet" href="styles/layouts.css">
</head>Create the  file with the new  declaration block, where we can add layout-specific stylings./* styles/layouts.css */
@layer layouts {
  /* layouts styles */
}For some quick, easy, and awesome CSS snippets, I tend to refer to Stephanie Eckles‘ SmolCSS project. Let’s grab the “Smol intrinsic container” code and include it within the  cascade layer:/* styles/layouts.css */
@layer layouts {
  .smol-container {
    width: min(100% - 3rem, var(--container-max, 60ch));
    margin-inline: auto;
  }
}This powerful little, two-line container uses the CSS  function to provide a responsive width, with  set to horizontally center itself and contain its child elements. We can also dynamically adjust the width using the  custom property.Now if we re-run the build command  and check the  folder, our compiled CSS file should contain:Our cascade layer declarations from Mayank’s CSS reset fully imported from The  class added from As you can see, we can get quite far with Vite as our build tool without writing any JavaScript. However, if we choose to, we can extend our build’s capabilities even further by writing just a  of JavaScript.Post-processing with LightningCSSLightning CSS is a CSS parser and post-processing tool that has a lot of nice features baked into it to help with cross-compatibility among browsers and browser versions. Lightning CSS can transform a lot of modern CSS into backward-compatible styles  you.We can install Lightning CSS in our project with :npm install --save-dev lightningcssThe  flag means the package will be installed as a development dependency, as it won’t be included with our built project. We can include it within our Vite build process, but first, we will need to write a tiny bit of JavaScript, a configuration file for Vite. Create a new file called:  and inside add the following code:// vite.config.mjs
export default {
  css: {
    transformer: 'lightningcss'
  },
  build: {
    cssMinify: 'lightningcss'
  }
};Vite will now use LightningCSS to transform and minify CSS files. Now, let’s give it a test run using an  color. Inside  let’s add the following code:/* main.css */
body {
  background-color: oklch(51.98% 0.1768 142.5);
}Then re-running the Vite build command, we can see the  property added in the compiled CSS:/* dist/index.css */
body {
  background-color: green;
  background-color: color(display-p3 0.216141 0.494224 0.131781);
  background-color: lab(46.2829% -47.5413 48.5542);
}Browserslist will give us a way to specify browsers by matching certain conditions (try it out online!)npm install -D browserslistInside our  file, we can configure Lightning CSS further. Let’s import the  package into the Vite configuration, as well as a module from the Lightning CSS package to help us use  in our config:// vite.config.mjs
import browserslist from 'browserslist';
import { browserslistToTargets } from 'lightningcss';We can add configuration settings for , containing the browser targets based on specified browser versions to Vite’s  configuration:// vite.config.mjs
import browserslist from 'browserslist';
import { browserslistToTargets } from 'lightningcss';

export default {
  css: {
    transformer: 'lightningcss',
    lightningcss: {
      targets: browserslistToTargets(browserslist('>= 0.25%')),
    }
  },
  build: {
    cssMinify: 'lightningcss'
  }
};There are lots of ways to extend Lightning CSS with Vite, such as enabling specific features, excluding features we won’t need, or writing our own custom transforms.// vite.config.mjs
import browserslist from 'browserslist';
import { browserslistToTargets, Features } from 'lightningcss';

export default {
  css: {
    transformer: 'lightningcss',
    lightningcss: {
      targets: browserslistToTargets(browserslist('>= 0.25%')),
      // Including `light-dark()` and `colors()` functions
      include: Features.LightDark | Features.Colors,
    }
  },
  build: {
    cssMinify: 'lightningcss'
  }
};Is any of this necessary?Reading through all this, you may be asking yourself if all of this is really necessary. The answer:  But I think you can see the benefits of having access to partialized files that we can compile into unified stylesheets.I doubt I’d go to these lengths for smaller projects, however, if building something with more complexity, such as a design system, I might reach for these tools for organizing code, cross-browser compatibility, and thoroughly optimizing compiled CSS.]]></content:encoded></item><item><title>Python for Loops: The Pythonic Way</title><link>https://realpython.com/python-for-loop/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Mon, 3 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[Python’s  loop allows you to iterate over the items in a collection, such as lists, tuples, strings, and dictionaries. The  loop syntax declares a loop variable that takes each item from the collection in each iteration. This loop is ideal for repeatedly executing a block of code on each item in the collection. You can also tweak  loops further with features like , , and .By the end of this tutorial, you’ll understand that: iterates over items in a data collection, allowing you to execute code for each item.To , you use the  construct.To repeat code a number of times without processing the data of an iterable, use the  construct.To do , you can use for index, value in enumerate(iterable): to access both index and item.In this tutorial, you’ll gain practical knowledge of using  loops to traverse various collections and learn Pythonic looping techniques. Additionally, you’ll learn how to handle exceptions and how to use asynchronous iterations to make your Python code more robust and efficient. Test your knowledge with our interactive “Python "for" Loops: The Pythonic Way” quiz. You’ll receive a score upon completion to help you track your learning progress:In this quiz, you'll test your understanding of Python's for loop. You'll revisit how to iterate over items in a data collection, how to use range() for a predefined number of iterations, and how to use enumerate() for index-based iteration.Getting Started With the Python  LoopIn programming, loops are control flow statements that allow you to repeat a given set of operations a number of times. In practice, you’ll find two main types of loops: loops are mostly used to iterate a  number of times, which is common when you’re processing data collections with a specific number of data items. loops are commonly used to iterate an  number of times, which is useful when the number of iterations depends on a given condition.Python has both of these loops and in this tutorial, you’ll learn about . In Python, you’ll generally use  loops when you need to iterate over the items in a data collection. This type of loop lets you traverse different data collections and run a specific group of statements  or  each item in the input collection.In Python,  loops are compound statements with a  and a  that runs a predefined number of times. The basic syntax of a  loop is shown below:In this syntax,  is the loop variable. In each iteration, this variable takes the value of the current item in , which represents the data collection you need to iterate over. The loop body can consist of one or more statements that must be indented properly.Here’s a more detailed breakdown of this syntax: is the keyword that initiates the loop header. is a variable that holds the current item in the input iterable. is a keyword that connects the loop variable with the iterable. is a data collection that can be iterated over. consists of one or more statements to execute in each iteration.Here’s a quick example of how you can use a  loop to iterate over a list:In this example,  is the loop variable, while the  list is the target collection. Each time through the loop,  takes on a successive item from . In this loop, the body consists of a call to  that displays the value on the screen. This loop runs once for each item in the target iterable. The way the code above is written is the Pythonic way to write it.However, what’s an iterable anyway? In Python, an  is an object—often a data collection—that can be iterated over. Common examples of iterables in Python include lists, tuples, strings, dictionaries, and sets, which are all built-in data types. You can also have custom classes that support iteration. Python has both iterables and iterators. Iterables support the  consisting of the special method. Similarly, iterators support the  that’s based on the  and  special methods.Both iterables and iterators can be iterated over. All iterators are iterables, but not all iterables are iterators. Python iterators play a fundamental role in  loops because they drive the iteration process.You can also have a loop with multiple loop variables:In this loop, you have two loop variables,  and . Note that to use this syntax, you just need to provide a tuple of loop variables. Also, you can have as many loop variables as you need as long as you have the correct number of items to unpack into them. You’ll also find this pattern useful when iterating over dictionary items or when you need to do parallel iteration.Sometimes, the input iterable may be empty. In that case, the loop will run its header once but won’t execute its body:]]></content:encoded></item><item><title>Deno vs. Node.js vs. Bun: Performance Comparison 2025</title><link>https://www.youtube.com/watch?v=DpDHPoStZZ8</link><author>Anton Putra</author><category>dev</category><category>video</category><enclosure url="https://www.youtube.com/v/DpDHPoStZZ8?version=3" length="" type=""/><pubDate>Mon, 3 Feb 2025 13:02:05 +0000</pubDate><source url="https://www.youtube.com/channel/UCeLvlbC754U6FyFQbKc0UnQ">Anton Putra</source><content:encoded><![CDATA[Node.js vs Deno vs Bun Performance

🔴 To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)

🍿 Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&si=p-UOaVM_6_SFx52H

👋 AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra

▬▬▬▬▬ Experience & Location 💼 ▬▬▬▬▬
►  I’m a Senior Software Engineer at Juniper Networks (13+ years of experience)
►  Located in San Francisco Bay Area, CA (US citizen)

▬▬▬▬▬▬ Connect with me 👋 ▬▬▬▬▬▬
►  LinkedIn: https://www.linkedin.com/in/anton-putra
►  Twitter/X: https://twitter.com/antonvputra
►  GitHub: https://github.com/antonputra
►  Email: me@antonputra.com

▬▬▬▬▬▬▬ Timestamps ⏰  ▬▬▬▬▬▬▬
0:00 Intro
0:42 1st Test - Static
4:26 2nd Test - PostgreSQL

▬▬▬▬▬▬▬ Source Code 📚 ▬▬▬▬▬▬▬
► Original Source Code: https://github.com/antonputra/tutorials/tree/241/lessons/241
► Bun's memory leak fix - https://github.com/oven-sh/bun/issues/17063

#deno #bun #nodejs]]></content:encoded></item><item><title>@krausefx about a scary #iPhone Camera Feature, that has now been resolved</title><link>https://www.youtube.com/watch?v=sO6epHFkG_Y</link><author>GOTO Conferences</author><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/sO6epHFkG_Y?version=3" length="" type=""/><pubDate>Mon, 3 Feb 2025 13:01:13 +0000</pubDate><source url="https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA">GOTO Conferences yt</source><content:encoded><![CDATA[Check out the full version on our YouTube channel now! #GOTOcon #GOTOcph #Privacy #Mobile #HowIsFelixToday #FelixKrause #KrauseFX #Fastlane #Security #InAppBrowsers #SFSafari #iOS #Apple #Safari #Android #JavaScript #TikTok #Instagram #Viral #ViralShorts #GOTO

Full version available here:
https://youtu.be/W8PmMDePgf8

Felix Krause - Creator of Fastlane & howisfelix.today & Security & Privacy Researcher @krausefx 

RECOMMENDED BOOK
Nishant Bhajaria • Data Privacy • https://amzn.to/3lGjv4g
Michael Bazzell • Extreme Privacy • https://amzn.to/3KeLwKA
Liz Rice • Container Security • https://amzn.to/3oU4iJe

CHANNEL MEMBERSHIP BONUS
Join this channel to get early access to videos & other perks:
https://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join

Looking for a unique learning experience?
Attend the next GOTO conference near you! Get your ticket at https://gotopia.tech
Sign up for updates and specials at https://gotopia.tech/newsletter

SUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.
https://www.youtube.com/user/GotoConferences/?sub_confirmation=1]]></content:encoded></item><item><title>Glibc 2.41 corrupting Discord installation</title><link>https://archlinux.org/news/glibc-241-corrupting-discord-installation/</link><author>Frederik Schwan</author><category>linux</category><pubDate>Mon, 3 Feb 2025 11:24:25 +0000</pubDate><source url="https://archlinux.org/news/">Dev - Arch Linux News</source><content:encoded><![CDATA[We plan to move  and its friends to stable later today, Feb 3. After installing the update, the Discord client will show a red warning that the installation is corrupt.This issue has been fixed in the Discord canary build. If you rely on audio connectivity, please use the canary build, login via browser or the flatpak version until the fix hits the stable Discord release.There have been no reports that (written) chat connectivity is affected.UPDATE: The issue has been fixed in Discord .]]></content:encoded></item><item><title>Story: One Million Checkboxes - The Viral Game That Defied Convention</title><link>https://corecursive.com/one-million-checkboxes-with-nolen-royalty/</link><author>Adam@corecursive.com (Adam@corecursive.com)</author><category>podcast</category><enclosure url="https://traffic.libsyn.com/secure/corecursive/omcb.mp3?dest-id=628353" length="" type=""/><pubDate>Mon, 3 Feb 2025 11:00:00 +0000</pubDate><source url="http://corecursive.com/">Podcast - CoRecursive</source><content:encoded><![CDATA[What if internet trolls could become your greatest collaborators? Nolen Royalty discovered this unexpected truth when his simple checkbox game went viral.It began with a school email prank that crashed servers but sparked a philosophy: creative constraints breed innovation. From "Flappy Dird" to "One Million Checkboxes," Nolen built games that turned limitations into playgrounds.But when his checkbox project became a battleground of organized chaos, Nolen faced a choice: suppress the chaos or lean into it.Discover how systematic constraints can channel creative anarchy - and why one developer now believes the best user experiences emerge when you code for controlled chaos of strangers meeting on the internet.]]></content:encoded></item><item><title>Should We Sing the Praises of Agile, or Bury It?</title><link>https://it.slashdot.org/story/25/02/02/1948240/should-we-sing-the-praises-of-agile-or-bury-it?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 2 Feb 2025 20:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA["Stakeholders must be included" throughout an agile project "to ensure the evolving deliverables meet their expectations," according to an article this week in Communications of the ACM. 

But long-time Slashdot reader theodp complains it's a "gushing how-to-make-Agile-even-better opinion piece."

Like other pieces by Agile advocates, it's long on accolades for Agile, but short on hard evidence justifying why exactly Agile project management "has emerged as a critical component for firms looking to improve project delivery speed and flexibility" and the use of Agile approaches is being expanded across other departments beyond software development. Indeed, among the three examples of success offered in the piece to "highlight the effectiveness of agile methods in navigating complex stakeholder dynamics and achieving project success" is Atlassian's use of agile practices to market and develop its products, many of which are coincidentally designed to support Agile practices and teams (including Jira). How meta. 

Citing "recent studies," the piece concludes its call for stakeholder engagement by noting that "59% of organizations measure Agile success by customer or user satisfaction." But that is one of those metrics that can create perverse incentives. Empirical studies of user satisfaction and engagement have been published since the 1970's, and sadly one of the cruel lessons learned from them is that the easiest path to having satisfied users is to avoid working on difficult problems. Keep that in mind when you ponder why difficult user stories seem to languish forever in the Kanban and Scrum Board "Ice Box" column, while the "Complete" column is filled with low-hanging fruit. Sometimes success does come easy! So, are you in the Agile-is-Heaven or Agile-is-Hell camp?]]></content:encoded></item><item><title>Facebook Admits Linux-Post Crackdown Was &apos;In Error&apos;, Fixes Moderation Error</title><link>https://tech.slashdot.org/story/25/02/02/1837253/facebook-admits-linux-post-crackdown-was-in-error-fixes-moderation-error?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>slashdot</category><category>linux</category><pubDate>Sun, 2 Feb 2025 19:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Tom's Hardware reports:

Facebook's heavy-handed censorship of Linux groups and topics was "in error," the social media juggernaut has admitted. Responding to reports earlier this week, sparked by the curious censorship of the eminently wholesome DistroWatch, Facebook contacted PCMag to say that it had made a mistake and that the underlying issue had been rectified. 

"This enforcement was in error and has since been addressed. Discussions of Linux are allowed on our services," said a Meta rep to PCMag. That is the full extent of the statement reproduced by the source... Copenhagen-hosted DistroWatch says it has appealed against the Community Standards-triggered ban shortly after it noticed it was in effect (January 19). PCMag received the Facebook admission of error on January 28. The latest statement from DistroWatch, which now prefers posting on Mastodon, indicates that Facebook has lifted the DistroWatch links ban. 


More details from PCMag:

Meta didn't say what caused the crackdown in the first place. But the company has been revamping some of its content moderation and plans to replace its fact-checking methodology with a user-driven Community Notes, similar to X. "We're also going to change how we enforce our policies to reduce the kind of mistakes that account for the vast majority of the censorship on our platforms," the company said earlier this month, in another irony. 
"Up until now, we have been using automated systems to scan for all policy violations, but this has resulted in too many mistakes and too much content being censored that shouldn't have been," Meta added in the same post.
]]></content:encoded></item><item><title>Build a Dice-Rolling Application With Python</title><link>https://realpython.com/python-dice-roll/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Sun, 2 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[In this tutorial, you’ll learn how to create a Python dice roll simulator. The tutorial guides you through building a text-based user interface (TUI) application that simulates rolling dice using Python’s  module. You’ll learn to gather and validate user input, use  for dice rolling, and display results with ASCII art.By the end of this tutorial, you’ll understand that:To simulate  events, you can use  .To get the , you use the built-in  function.To display dice in Python, you generate ASCII art representations of dice faces and use .To manipulate , you use methods such as  and .Building small projects, like a text-based user interface (TUI) dice-rolling application, will help you level up your Python programming skills. You’ll learn how to gather and validate the user’s input, import code from modules and packages, write functions, use  loops and conditionals, and neatly display output by using strings and the  function.Click the link below to download the entire code for this dice-rolling application and follow along while you build the project yourself:In this step-by-step project, you’ll build an application that runs dice-rolling simulations. The app will be able to roll up to six dice, with each die having six faces. After every roll, the application will generate an ASCII diagram of dice faces and display it on the screen. The following video demonstrates how the app works:When you run your dice-rolling simulator app, you get a prompt asking for the number of dice you want to roll. Once you provide a valid integer from 1 to 6, inclusive, then the application simulates the rolling event and displays a diagram of dice faces on the screen.Your dice-rolling simulator app will have a minimal yet user-friendly text-based user interface (TUI), which will allow you to specify the number of six-sided dice that you’d like to roll. You’ll use this TUI to roll the dice at home without having to fly to Las Vegas.Here’s a description of how the app will work internally:Prompt the user to choose how many six-sided dice to roll, then read the user’s inputA call to  with appropriate argumentsParse and validate the user’s inputA user-defined function called Run the dice-rolling simulationA user-defined function called Generate an ASCII diagram with the resulting dice facesA user-defined function called generate_dice_faces_diagram()Display the diagram of dice faces on the screenA call to  with appropriate argumentsKeeping these internal workings in mind, you’ll code three custom functions to provide the app’s main features and functionalities. These functions will define your code’s public API, which you’ll call to bring the app to life.To organize the code of your dice-rolling simulator project, you’ll create a single file called  in a directory of your choice in your file system. Go ahead and create the file to get started!You should be comfortable with the following concepts and skills before you start building this dice-rolling simulation project:If you don’t have all of the prerequisite knowledge before starting this coding adventure, then that’s okay! You might learn more by going ahead and getting started! You can always stop and review the resources linked here if you get stuck.Step 1: Code the TUI of Your Python Dice-Rolling AppIn this step, you’ll write the required code to ask for the user’s input of how many dice they want to roll in the simulation. You’ll also code a Python function that takes the user’s input, validates it, and returns it as an integer number if the validation was successful. Otherwise, the function will ask for the user’s input again.To download the code for this step, click the following link and navigate to the  folder:]]></content:encoded></item><item><title>Build a Quiz Application With Python</title><link>https://realpython.com/python-quiz-application/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Sun, 2 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[In this tutorial, you’ll build a Python quiz application for the terminal. You’ll start by developing a basic app capable of asking questions, collecting answers, and checking correctness. As you progress, you’ll enhance the app by adding features like user-friendly interfaces, storing questions in external files, handling multiple correct answers, and providing hints and explanations.By the end of this tutorial, you’ll understand that: is a nice beginner project as it involves basic Python concepts like data structures and control flow.The  helps you to interact with the user in the terminal.You can structure questions and answers in data files. are suitable for storing data in small games due to their readability and ease of integration with Python.The word  was first used in 1781 to mean . Nowadays, it’s mostly used to describe short tests of trivia or expert knowledge with questions like the following:When was the first known use of the word ?By following along in this step-by-step project, you’ll build an application that can test a person’s expertise on a range of topics. You can use this project to strengthen your own knowledge or to challenge your friends to a fun battle of wits.The quiz application is a comprehensive project for anyone comfortable with the basics of Python. Throughout the tutorial, you’ll get all the code you need in separate, bite-size steps. You can also find the full source code of the application by clicking on the link below:Whether you’re an eccentric person or not, read on to learn how to create your own quiz.Demo: Your Python Quiz ApplicationIn this step-by-step project, you’ll build a terminal application that can quiz you and your friends on a range of topics:You first choose a topic for your questions. Then, for each question, you’ll choose an answer from a set of alternatives. Some questions may have multiple correct answers. You can access a hint to help you along the way. After answering a question, you’ll read an explanation that can provide more context for the answer.You’ll start by creating a basic Python quiz application that’s only capable of asking a question, collecting an answer, and checking whether the answer is correct. From there, you’ll add more and more features in order to make your app more interesting, user-friendly, and fun.You’ll build the quiz application iteratively by going through the following steps:Create a basic application that can ask multiple-choice questions.Make the app more user-friendly by improving how it looks and how it handles user errors.Refactor the code to use functions.Separate question data from source code by storing questions in a dedicated data file.Expand the app to handle multiple correct answers, give hints, and provide explanations.Add interest by supporting different quiz topics to choose from.As you follow along, you’ll gain experience in starting with a small script and expanding it. This is an important skill in and of itself. Your favorite program, app, or game probably started as a small proof of concept that later grew into what it is today.In this tutorial, you’ll build a quiz application using Python’s basic building blocks. While working through the steps, it’s helpful if you’re comfortable with the following concepts:If you’re not confident in your knowledge of these prerequisites, then that’s okay too! In fact, going through this tutorial will help you practice these concepts. You can always stop and review the resources linked above if you get stuck.In this step, you’ll learn how to create a program that can ask questions and check answers. This will be the foundation of your quiz application, which you’ll improve upon in the rest of the tutorial. At the end of this step, your program will look like this:]]></content:encoded></item><item><title>Top Python Game Engines</title><link>https://realpython.com/top-python-game-engines/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Sun, 2 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[You can use several Python game engines for crafting video games using your existing Python skills. Popular Python game engines are Pygame, Arcade, and Ren’Py, each offering unique features. In this tutorial, you’ll learn how to install and use them, and how these engines differ from traditional stand-alone game engines.By the end of this tutorial, you’ll understand that:Pygame, Arcade, adventurelib, and Ren’Py are some of the top Python game engines, each with unique features.Python game engines are  for Python developers but may require more effort for  support compared to other engines. is designed for beginners, offering simplified game development requiring less manual code than Pygame. with Python game engines isn’t straightforward and requires additional effort for optimization and compatibility.You can use Python to  using frameworks like .Using Python, and a host of great Python game engines, makes crafting great computer games much easier than in the past. In this tutorial, you’ll explore several of these game engines, learning what you need to start crafting your own Python video games!To get the most out of this tutorial, you should be well-versed in Python programming, including object-oriented programming. An understanding of basic game concepts is helpful, but not necessary.Ready to dive in? Click the link below to download the source code for all the games that you’ll be creating:Python Game Engines OverviewGame engines for Python most often take the form of Python libraries, which can be installed in a variety of ways. Most are available on PyPI and can be installed with . However, a few are available only on GitHub, GitLab, or other code sharing locations, and they may require other installation steps. This article will cover installation methods for all the engines discussed.Python is a general purpose programming language, and it’s used for a variety of tasks other than writing computer games. In contrast, there are many different stand-alone game engines that are tailored specifically to writing games. Some of these include:These stand-alone game engines differ from Python game engines in several key aspects: Languages like C++, C#, and JavaScript are popular for games written in stand-alone game engines, as the engines themselves are often written in these languages. Very few stand-alone engines support Python.Proprietary scripting support: In addition, many stand-alone game engines maintain and support their own scripting languages, which may not resemble Python. For example, Unity uses C# natively, while Unreal works best with C++.  Many modern stand-alone game engines can produce games for a variety of platforms, including mobile and dedicated game systems, with very little effort. In contrast, porting a Python game across various platforms, especially mobile platforms, can be a major undertaking. Games written using a stand-alone game engine may have different licensing options and restrictions, based on the engine used.So why use Python to write games at all? In a word, Python. Using a stand-alone game engine often requires you to learn a new programming or scripting language. Python game engines leverage your existing knowledge of Python, reducing the learning curve and getting you moving forward quickly.There are many game engines available for the Python environment. The engines that you’ll learn about here all share the following criteria:They’re relatively popular engines, or they cover aspects of gaming that aren’t usually covered.They’re currently maintained.They have good documentation available.For each engine, you’ll learn about:Basic concepts, as well as assumptions that the engine makesMajor features and capabilitiesTwo game implementations, to allow for comparisonWhere appropriate, you should install these game engines in a virtual environment. Full source code for the games in this tutorial is available for download at the link below and will be referenced throughout the article:With the source code downloaded, you’re ready to begin.When people think of Python game engines, the first thought many have is Pygame. In fact, there’s already a great primer on Pygame available at Real Python.Written as a replacement for the stalled PySDL library, Pygame wraps and extends the SDL library, which stands for . SDL provides cross-platform access to your system’s underlying multimedia hardware components, such as sound, video, mouse, keyboard, and joystick. The cross-platform nature of both SDL and Pygame means that you can write games and rich multimedia Python programs for every platform that supports them!]]></content:encoded></item><item><title>Develop Data Visualization Interfaces in Python With Dash</title><link>https://realpython.com/python-dash/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Sun, 2 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[Dash is a popular Python framework for creating interactive data visualization interfaces. With Dash, you build web applications using only Python, without needing advanced web development skills. It integrates seamlessly with technologies like Flask, React.js, and Plotly.js to render user interfaces and generate charts.By the end of this tutorial, you’ll understand that: is an open-source framework for building data visualization interfaces using Python. include interactive dashboards for data analysis and visualization tasks.You can  of a Dash app using CSS, either inline or with external files.You can  on PythonAnywhere, a platform offering free hosting for Python web apps.Dash gives data scientists the ability to showcase their results in interactive web applications. You don’t need to be an expert in web development. In this tutorial, you’ll explore how to create, style, and deploy a Dash application, transforming a basic dashboard into a fully interactive tool.You can download the source code, data, and resources for the sample application that you’ll make in this tutorial by clicking the link below:Dash is an open-source framework for building data visualization interfaces. Released in 2017 as a Python library, it’s grown to include implementations for R, Julia, and F#. Dash helps data scientists build analytical web applications without requiring advanced web development knowledge.Three technologies constitute the core of Dash: supplies the web server functionality. renders the user interface of the web page. generates the charts used in your application.But you don’t have to worry about making all these technologies work together. Dash will do that for you. You just need to write Python, R, Julia, or F# and sprinkle in a bit of CSS.Plotly, a Canada-based company, built Dash and supports its development. You may know the company from the popular graphing libraries that share its name. The company released Dash as open source under an MIT license, so you can use Dash at no cost.Plotly also offers a commercial companion to Dash called Dash Enterprise. This paid service provides companies with support services such as hosting, deploying, and handling authentication on Dash applications. But these features live outside of Dash’s open-source ecosystem.Dash will help you build dashboards quickly. If you’re used to analyzing data or building data visualizations using Python, then Dash will be a useful addition to your toolbox. Here are a few examples of what you can make with Dash:This is just a tiny sample. If you’d like to see other interesting use cases, then go check out the Dash App Gallery.If you feel comfortable with the requirements and want to learn how to use Dash in your next project, then continue to the following section!Get Started With Dash in PythonIn this tutorial, you’ll go through the end-to-end process of building a dashboard using Dash. If you follow along with the examples, then you’ll go from a bare-bones dashboard on your local machine to a styled dashboard deployed on PythonAnywhere.How to Set Up Your Local EnvironmentTo develop your app, you’ll need a new directory to store your code and data. You’ll also need a clean Python virtual environment. To create those, execute the commands below, choosing the version that matches your operating system:]]></content:encoded></item><item><title>Slashdot Asks: Do You Remember Your High School&apos;s &apos;Computer Room&apos;?</title><link>https://developers.slashdot.org/story/25/02/02/0233216/slashdot-asks-do-you-remember-your-high-schools-computer-room?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 2 Feb 2025 02:36:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[Bill Gates' blog has been updated with short videos about his upcoming book, including one about how his school ended up with an ASR-33 teletype that could connect their Seattle classroom to a computer in California. "The teachers faded away pretty quickly," Gates adds, "But about six of us stayed hardcore. One was Paul Allen..." — the future co-founder of Microsoft. And the experience clearly meant a lot to Gates. "Microsoft just never would've happened without Paul — and this teletype room." 

In a longer post thanking his "brilliant" teachers, Gates calls his teletype experience "an encounter that would shape my entire future" and "opened up a whole new world for me." Gates also thanks World War II Navy pilot and Boeing engineer Bill Dougall, who "was instrumental in bringing computer access to our school, something he and other faculty members pushed for after taking a summer computer class... The fascinating thing about Mr. Dougall was that he didn't actually know much about programming; he exhausted his knowledge within a week. But he had the vision to know it was important and the trust to let us students figure it out." 

Gates shared a similar memory about the computer-room's 20-something overseer Fred Wright, who "intuitively understood that the best way to get students to learn was to let us explore on our own terms. There was no sign-up sheet, no locked door, no formal instruction."

Instead, Mr. Wright let us figure things out ourselves and trusted that, without his guidance, we'd have to get creative... Some of the other teachers argued for tighter regulations, worried about what we might be doing in there unsupervised. But even though Mr. Wright occasionally popped in to break up a squabble or listen as someone explained their latest program, for the most part he defended our autonomy... 
Mr. Wright gave us something invaluable: the space to discover our own potential.
 

Any Slashdot readers have a similarly impactful experience? Share your own thoughts and memories in the comments. 

Do you remember your high school's computer room?]]></content:encoded></item><item><title>Grafana is the goat... Let&apos;s deploy the LGTM stack</title><link>https://www.youtube.com/watch?v=1X3dV3D5EJg</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/1X3dV3D5EJg?version=3" length="" type=""/><pubDate>Sat, 1 Feb 2025 15:24:39 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Get up to 67% off VPS at Hostinger. Use code FIRESHIP for an extra discount at https://hostinger.com/fireship

#linux #programming #softwareengineer 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Docker 101 https://youtu.be/rIrNIzy6U_g
Full Deno Course https://fireship.io/courses/deno
Grafana on GitHub https://github.com/grafana/grafana

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

What is Grafana? 
How to get started with software observability?
What is Open Telemetry?
How to deploy LGTM stack?
How monitor Linux server performance?]]></content:encoded></item><item><title>A Guide to Modern Python String Formatting Tools</title><link>https://realpython.com/python-formatted-output/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Sat, 1 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[In modern Python, you have f-strings and the  method to approach the tasks of interpolating and formatting strings.
These tools help you embed variables and expressions directly into strings, control text alignment, and use custom format specifiers to modify how values appear. You can apply these techniques to create well-structured and readable Python code.By the end of this tutorial, you’ll understand that: in Python involves embedding variables and expressions into strings.You create an  in Python by prepending a string literal with an  or  and using  to include variables or expressions.You can use variables in Python’s  by placing them inside curly braces and passing them as . in Python control how values appear when formatted, using components like fill, align, sign, width, and type.You align text in Python string formatting using the  component, which can justify text to the  within a specified width.When working with strings in Python, you can leverage these formatting techniques to create dynamic and readable output. To get the most out of this tutorial, you should know the basics of Python programming and the string data type. Test your knowledge with our interactive “A Guide to Modern Python String Formatting Tools” quiz. You’ll receive a score upon completion to help you track your learning progress:Python has developed different string interpolation and formatting tools over the years. If you’re getting started with Python and looking for a quick way to format your strings, then you should use Python’s f-strings.If you need to work with older versions of Python or legacy code, then it’s a good idea to learn about the other formatting tools, such as the  method.In this tutorial, you’ll learn how to format your strings using f-strings and the  method. You’ll start with f-strings to kick things off, which are quite popular in modern Python code.Using F-Strings for String InterpolationPython has a string formatting tool called , which stands for formatted string literals. F-strings are string literals that you can create by prepending an  or  to the literal. They allow you to do string interpolation and formatting by inserting variables or expressions directly into the literal.Creating F-String LiteralsHere you’ll take a look at how you can create an f-string by prepending the string literal with an  or :Using either  or  has the same effect. However, it’s a more common practice to use a lowercase  to create f-strings.Just like with regular string literals, you can use single, double, or triple quotes to define an f-string:Up to this point, your f-strings look pretty much the same as regular strings. However, if you create f-strings like those in the examples above, you’ll get complaints from your code linter if you have one.The remarkable feature of f-strings is that you can embed Python variables or expressions directly inside them. To insert the variable or expression, you must use a , which you create using a pair of curly braces.Interpolating Variables Into F-StringsThe variable that you insert in a replacement field is evaluated and converted to its string representation. The result is interpolated into the original string at the replacement field’s location:In this example, you’ve interpolated the  variable into your string. Note that Python treats anything outside the curly braces as a regular string.]]></content:encoded></item><item><title>Build a Tic-Tac-Toe Game With Python and Tkinter</title><link>https://realpython.com/tic-tac-toe-python/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Sat, 1 Feb 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[Developing a tic-tac-toe game in Python using Tkinter combines programming logic with graphical user interface design (GUI). This tutorial guides you through creating the game logic and a Tkinter-based GUI to produce a fully functional tic-tac-toe game. You’ll learn how to manage player moves, detect winning combinations, and build an interactive interface with Tkinter widgets.By the end of this tutorial, you’ll understand that: can be used to implement the  for a tic-tac-toe game. provides tools to build a game  using labels and buttons.You can connect the  and  to create a fully interactive application.Playing computer games is a great way to unwind or challenge yourself, and it’s also fun and educational to build your own. In this project, you’ll use the Tkinter GUI framework from Python’s standard library to create a game interface, while applying the model-view-controller pattern and an object-oriented approach to organize your code. For more on these concepts, check out the links in the prerequisites.To download the entire source code for this project, click the link in the box below:Demo: A Tic-Tac-Toe Game in PythonIn this step-by-step project, you’ll build a tic-tac-toe game in Python. You’ll use the Tkinter tool kit from the Python standard library to create the game’s GUI. In the following demo video, you’ll get a sense of how your game will work once you’ve completed this tutorial:Your tic-tac-toe game will have an interface that reproduces the classic three-by-three game board. The players will take turns making their moves on a shared device. The game display at the top of the window will show the name of the player who gets to go next.If a player wins, then the game display will show a winning message with the player’s name or mark ( or ). At the same time, the winning combination of cells will be highlighted on the board.Finally, the game’s  menu will have options to reset the game if you want to play again or to exit the game when you’re done playing.If this sounds like a fun project to you, then read on to get started!Your goal with this project is to create a tic-tac-toe game in Python. For the game interface, you’ll use the Tkinter GUI tool kit, which comes in the standard Python installation as an included battery. The tic-tac-toe game is for two players. One player plays  and the other plays . The players take turns placing their marks on a grid of three-by-three cells. If a given player gets three marks in a row horizontally, vertically, or diagonally, then that player wins the game. The game will be tied if no one gets three in a row by the time all the cells are marked.With these rules in mind, you’ll need to put together the following game components:The game’s , which you’ll build with a class called The game’s , which you’ll manage using a class called The game board will work as a mix between  and  in a model-view-controller design. To build the board, you’ll use a Tkinter window, which you can create by instantiating the  class. This window will have two main components: Shows information about the game’s status Represents previous moves and available spaces or cellsYou’ll create the game display using a  widget, which allows you to display text and images.For the grid of cells, you’ll use a series of  widgets arranged in a grid. When a player clicks one of these buttons, the game logic will run to process the player’s move and determine whether there’s a winner. In this case, the game logic will work as the , which will manage the data, logic, and rules of your game.Now that you have a general idea of how to build your tic-tac-toe game, you should check out a few knowledge prerequisites that’ll allow you to get the most out of this tutorial.To complete this tic-tac-toe game project, you should be comfortable or at least familiar with the concepts and topics covered in the following resources:]]></content:encoded></item><item><title>AI Cyber Threat Intelligence Roundup: January 2025</title><link>https://blogs.cisco.com/security/ai-cyber-threat-intelligence-roundup-january-2025/</link><author>Adam Swanda</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Sat, 1 Feb 2025 13:00:13 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[AI threat research is a fundamental part of Cisco’s approach to AI security. Our roundups highlight new findings from both original and third-party sources.]]></content:encoded></item><item><title>Android 16&apos;s Linux Terminal Runs Doom</title><link>https://linux.slashdot.org/story/25/01/31/2317212/android-16s-linux-terminal-runs-doom?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>slashdot</category><category>linux</category><pubDate>Sat, 1 Feb 2025 01:40:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Google is enhancing Android 16's Linux Terminal app to support graphical Linux applications, so Android Authority decided to put it to the test by running Doom. From the report: The Terminal app first appeared in the Android 15 QPR2 beta as a developer option, and it still remains locked behind developer settings. Since its initial public release, Google pushed a few changes that fixed issues with the installation process and added a settings menu to resize the disk, forward ports, and backup the installation. However, the biggest changes the company has been working on, which include adding hardware acceleration support and a full graphical environment, have not been pushed to any public releases.
 
Thankfully, since Google is working on this feature in the open, it's possible to simply compile a build of AOSP with these changes added in. This gives us the opportunity to trial upcoming features of the Android Linux Terminal app before a public release. To demonstrate, we fired up the Linux Terminal on a Pixel 9 Pro, tapped a new button on the top right to enter the Display activity, and then ran the 'weston' command to open up a graphical environment. (Weston is a reference implementation of a Wayland compositor, a modern display server protocol.)
 
We also went ahead and enabled hardware acceleration beforehand as well as installed Chocolate Doom, a source port of Doom, to see if it would run. Doom did run, as you can see below. It ran well, which is no surprise considering Doom can run on literal potatoes. There wasn't any audio because an audio server isn't available yet, but audio support is something that Google is still working on.]]></content:encoded></item><item><title>Over the top auth strategies (Friends)</title><link>https://changelog.com/friends/78</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/friends/78/changelog--friends-78.mp3" length="" type=""/><pubDate>Fri, 31 Jan 2025 20:30:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Dan Moore from FusionAuth joins us for a wide-ranging discussion about modern auth strategies. We talk magic links, OTP, MFA, passkeys, password managers & so much more.Changelog++ members get a bonus 9 minutes at the end of this episode and zero ads. Join today!Retool – The low-code platform for developers to build internal tools — Some of the best teams out there trust Retool…Brex, Coinbase, Plaid, Doordash, LegalGenius, Amazon, Allbirds, Peloton, and so many more – the developers at these teams trust Retool as the platform to build their internal tools. Try it free at retool.com/changelogTemporal – Build invincible applications. Manage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail. Register for Replay in London, March 3-5 to break free from the status quo.
Notion – Notion is a place where any team can write, plan, organize, and rediscover the joy of play. It’s a workspace designed not just for making progress, but getting inspired. Notion is for everyone — whether you’re a Fortune 500 company or freelance designer, starting a new startup or a student juggling classes and clubs.
A few more links from Dan]]></content:encoded></item><item><title>3D Printed Mini Rack Drawer #shorts</title><link>https://www.youtube.com/watch?v=tyLI8usvLJM</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/tyLI8usvLJM?version=3" length="" type=""/><pubDate>Fri, 31 Jan 2025 20:18:45 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[More on Project MINI RACK: https://mini-rack.jeffgeerling.com

Drawer STLs are here: https://github.com/JaredC01/LabStack-Rack/tree/main/LabStack%20Rack%20Mini/STLs

I used these magnets: https://www.amazon.com/dp/B0BW91ZTKZ]]></content:encoded></item><item><title>Copyright is a Civil Liberties Nightmare</title><link>https://www.eff.org/deeplinks/2025/01/copyright-civil-liberties-nightmare</link><author>Kit Walsh</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/fixcopyright-graphic-banner.jpg" length="" type=""/><pubDate>Fri, 31 Jan 2025 19:07:20 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>FBI, Dutch Police Disrupt ‘Manipulaters’ Phishing Gang</title><link>https://krebsonsecurity.com/2025/01/fbi-dutch-police-disrupt-manipulaters-phishing-gang/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Fri, 31 Jan 2025 18:35:32 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[The FBI and authorities in The Netherlands this week seized dozens of servers and domains for a hugely popular spam and malware dissemination service operating out of Pakistan. The proprietors of the service, who use the collective nickname “,” have been the subject of three stories published here since 2015. The FBI said the main clientele are organized crime groups that try to trick victim companies into making payments to a third party.One of several current Fudtools sites run by the principals of The Manipulators.On January 29, the FBI and the Dutch national police seized the technical infrastructure for a cybercrime service marketed under the brands ,  and  (and many other “fud” variations). The “fud” bit stands for “Fully Un-Detectable,” and it refers to cybercrime resources that will evade detection by security tools like antivirus software or anti-spam appliances.The Dutch authorities said 39 servers and domains abroad were seized, and that the servers contained millions of records from victims worldwide — including at least 100,000 records pertaining to Dutch citizens.A statement from the U.S. Department of Justice refers to the cybercrime group as , after a pseudonym The Manipulaters communally used to promote their spam, malware and phishing services on social media.“The Saim Raza-run websites operated as marketplaces that advertised and facilitated the sale of tools such as phishing kits, scam pages and email extractors often used to build and maintain fraud operations,” the DOJ explained.The core Manipulaters product is , a spam delivery service whose homepage openly advertised phishing kits targeting users of various Internet companies, including , , , ,  and , to name a few.The government says transnational organized crime groups that purchased these services primarily used them to run business email compromise (BEC) schemes, wherein the cybercrime actors tricked victim companies into making payments to a third party.“Those payments would instead be redirected to a financial account the perpetrators controlled, resulting in significant losses to victims,” the DOJ wrote. “These tools were also used to acquire victim user credentials and utilize those credentials to further these fraudulent schemes. The seizure of these domains is intended to disrupt the ongoing activity of these groups and stop the proliferation of these tools within the cybercriminal community.”Manipulaters advertisement for “Office 365 Private Page with Antibot” phishing kit sold via Heartsender. “Antibot” refers to functionality that attempts to evade automated detection techniques, keeping a phish deployed and accessible as long as possible. Image: DomainTools.KrebsOnSecurity first wrote about The Manipulaters in May 2015, mainly because their ads at the time were blanketing a number of popular cybercrime forums, and because they were fairly open and brazen about what they were doing — even who they were in real life.We caught up with The Manipulaters again in 2021, with a story that found the core employees had started a web coding company in Lahore called  — presumably as a way to account for their considerable Heartsender income. That piece examined how WeCodeSolutions employees had all doxed themselves on Facebook by posting pictures from company parties each year featuring a large cake with the words  written in icing.A follow-up story last year about The Manipulaters prompted messages from various WeCodeSolutions employees who pleaded with this publication to remove stories about them. The Saim Raza identity told KrebsOnSecurity they were recently released from jail after being arrested and charged by local police, although they declined to elaborate on the charges.The Manipulaters never seemed to care much about protecting their own identities, so it’s not surprising that they were unable or unwilling to protect their own customers. In an analysis released last year,  found the web-hosted version of Heartsender leaked an extraordinary amount of user information to unauthenticated users, including customer credentials and email records from Heartsender employees.Almost every year since their founding, The Manipulaters have posted a picture of a FudCo cake from a company party celebrating its anniversary.DomainTools also uncovered evidence that the computers used by The Manipulaters were all infected with the same password-stealing malware, and that vast numbers of credentials were stolen from the group and sold online.“Ironically, the Manipulaters may create more short-term risk to their own customers than law enforcement,” DomainTools wrote. “The data table ‘User Feedbacks’ (sic) exposes what appear to be customer authentication tokens, user identifiers, and even a customer support request that exposes root-level SMTP credentials–all visible by an unauthenticated user on a Manipulaters-controlled domain.”Police in The Netherlands said the investigation into the owners and customers of the service is ongoing.“The Cybercrime Team is on the trail of a number of buyers of the tools,” the Dutch national police said. “Presumably, these buyers also include Dutch nationals. The investigation into the makers and buyers of this phishing software has not yet been completed with the seizure of the servers and domains.”U.S. authorities this week also joined law enforcement in Australia, France, Greece, Italy, Romania and Spain in seizing a number of domains for several long-running cybercrime forums and services, including  and . According to a statement from the European police agency , the two communities attracted more than 10 million users in total.Other domains seized as part of “” included , an e-commerce platform that was frequently used by cybercrime forum members to buy and sell illicit goods and services.]]></content:encoded></item><item><title>Evaluating Security Risk in DeepSeek and Other Frontier Reasoning Models</title><link>https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models/</link><author>Paul Kassianik</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Fri, 31 Jan 2025 18:30:00 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[The performance of DeepSeek models has made a clear impact, but are these models safe and secure? We use algorithmic AI vulnerability testing to find out.]]></content:encoded></item><item><title>How Hackers Target a Victim</title><link>https://www.youtube.com/watch?v=XwbabtA03ns</link><author>Jack Rhysider</author><category>Darknet diaries</category><category>infosec</category><category>video</category><category>podcast</category><enclosure url="https://www.youtube.com/v/XwbabtA03ns?version=3" length="" type=""/><pubDate>Fri, 31 Jan 2025 18:04:26 +0000</pubDate><source url="https://www.youtube.com/channel/UCMIqrmh2lMdzhlCPK5ahsAg">Jack Rhysider</source><content:encoded><![CDATA[How Hackers Target a Victim

#cybersecurity #podcast 

Listen to the full episode 🎙 Darknet Diaries Ep. 151: Chris Rock]]></content:encoded></item><item><title>Dell risks employee retention by forcing all teams back into offices full-time</title><link>https://arstechnica.com/information-technology/2025/01/dell-risks-employee-retention-by-forcing-all-teams-back-into-offices-full-time/</link><author>Scharon Harding</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/01/GettyImages-833524236-1024x648.jpg" length="" type=""/><pubDate>Fri, 31 Jan 2025 17:40:52 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Dell is calling much of its workforce back into the office five days a week starting on March 3. The technology giant is framing the mandate as a business strategy, but there’s reason to believe the policy may drive employee turnover.Business Insider detailed an internal memo today from CEO and Chairman Michael Dell informing workers that if they live within an hour of a Dell office, they’ll have to go in five days a week."What we're finding is that for all the technology in the world, nothing is faster than the speed of human interaction,” Dell wrote, per Business Insider. "A thirty-second conversation can replace an email back-and-forth that goes on for hours or even days."]]></content:encoded></item><item><title>Digest #158: Kubernetes Security, PostgreSQL Upgrades, AWS Tagging, FinOps vs. DevOps, and Helm’s Future</title><link>https://www.devopsbulletin.com/p/digest-158-kubernetes-security-postgresql</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ff08844-2219-4b9e-8c15-7c85207f0c1d_1024x605.jpeg" length="" type=""/><pubDate>Fri, 31 Jan 2025 16:48:31 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!Ever wondered what Uber’s MySQL architecture looks like at scale? Or how the EU’s new DORA regulation will impact DevOps and cloud security? This week’s edition explores resilience engineering, Kubernetes controllers, and the next era of Helm.This edition also includes hands-on tutorials on securing Maven proxy repositories, the risks of running Kubernetes containers as root, upgrading PostgreSQL with zero downtime, building a client VPN on AWS with Terraform, and automating Git Bisect with ephemeral environments. There’s also a closer look at AWS tagging best practices, Kubernetes API server proxies, and heap exploitation mechanisms.On the open-source front, we’re featuring a browser extension analysis tool for security, a graph database designed for testing new techniques, a CLI to clean up Terraform configurations, and a project that simplifies AI model deployment using OCI containers. Plus, a discussion on the ongoing debate between DevOps and FinOps, and whether these two disciplines can truly coexist or if their priorities will always clash.All this and more in this week’s DevOps Bulletin—don’t miss out!Highlighting cool DevOps projects to keep an eye on: is a Python tool for analyzing browser extensions through a risk management lens. is a graph database system for testing new database and graph techniques, supporting RDF/SPARQL and Property Graphs. simplifies AI model deployment using OCI containers, automatically managing dependencies and GPU/CPU execution. is a CLI tool that alphabetically sorts Terraform variables and outputs while fixing spacing and formatting issues is a versatile tool manager that handles dev tools, environment variables, and task automation across projectsIf you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>Chrome 133 Goodies</title><link>https://css-tricks.com/chrome-133-goodies/</link><author>Geoff Graham</author><category>frontend</category><pubDate>Fri, 31 Jan 2025 15:27:50 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[I often wonder what it’s like working for the Chrome team. You must get issued some sort of government-level security clearance for the latest browser builds that grants you permission to bash on them ahead of everyone else and come up with these rad demos showing off the latest features. No, I’m, not jealous, why are you asking?Totally unrelated, did you see the release notes for Chrome 133? It’s currently in beta, but the Chrome team has been publishing a slew of new articles with pretty incredible demos that are tough to ignore. I figured I’d round those up in one place.We’ve been able to use HTML attributes in CSS for some time now, but it’s been relegated to the  property and only parsed strings.<h1 data-color="orange">Some text</h1>h1::before {
  content: ' (Color: ' attr(data-color) ') ';
}Bramus demonstrates how we can now use it on any CSS property, including custom properties, in Chrome 133. So, for example, we can take the attribute’s value and put it to use on the element’s  property:h1 {
  color: attr(data-color type(<color>), #fff)
}This is a trite example, of course. But it helps illustrate that there are three moving pieces here:the attribute ()the fallback value ()We make up the attribute. It’s nice to have a wildcard we can insert into the markup and hook into for styling. The  is a new deal that helps CSS know what sort of value it’s working with. If we had been working with a numeric value instead, we could ditch that in favor of something less verbose. For example, let’s say we’re using an attribute for the element’s font size:<div data-size="20">Some text</div>Now we can hook into the  attribute and use the assigned value to set the element’s  property, based in  units:h1 {
  color: attr(data-size px, 16);
}The fallback value is optional and might not be necessary depending on your use case.This is a mind-blowing one. If you’ve ever wanted a way to style a sticky element when it’s in a “stuck” state, then you already know how cool it is to have something like this. Adam Argyle takes the classic pattern of an alphabetical list and applies styles to the letter heading when it sticks to the top of the viewport. The same is true of elements with scroll snapping and elements that are scrolling containers.In other words, we can style elements when they are “stuck”, when they are “snapped”, and when they are “scrollable”.Quick little example that you’ll want to open in a Chromium browser:The general idea (and that’s all I know for now) is that we register a container… you know, a container that we can query. We give that container a  that is set to the type of scrolling we’re working with. In this case, we’re working with sticky positioning where the element “sticks” to the top of the page..sticky-nav {
  container-type: scroll-state;
}A container can’t query itself, so that basically has to be a wrapper around the element we want to stick. Menus are a little funny because we have the  element and usually stuff it with an unordered list of links. So, our  can be the container we query since we’re effectively sticking an unordered list to the top of the page.<nav class="sticky-nav">
  <ul>
    <li><a href="#">Home</a></li>
    <li><a href="#">About</a></li>
    <li><a href="#">Blog</a></li>
  </ul>
</nav>We can put the sticky logic directly on the  since it’s technically holding what gets stuck:.sticky-nav {
  container-type: scroll-state; /* set a scroll container query */
  position: sticky; /* set sticky positioning */
  top: 0; /* stick to the top of the page */
}I supposed we could use the  shorthand if we were working with multiple containers and needed to distinguish one from another with a . Either way, now that we’ve defined a container, we can query it using ! In this case, we declare the type of container we’re querying:@container scroll-state() { }And we tell it the state we’re looking for:@container scroll-state(stuck: top) {If we were working with a sticky footer instead of a menu, then we could say  instead. But the kicker is that once the  element sticks to the top, we get to apply styles to it in the  block, like so:.sticky-nav {
  border-radius: 12px;
  container-type: scroll-state;
  position: sticky;
  top: 0;

  /* When the nav is in a "stuck" state */
  @container scroll-state(stuck: top) {
    border-radius: 0;
    box-shadow: 0 3px 10px hsl(0 0 0 / .25);
    width: 100%;
  }
}It seems to work when nesting other selectors in there. So, for example, we can change the links in the menu when the navigation is in its stuck state:.sticky-nav {
  /* Same as before */

  a {
    color: #000;
    font-size: 1rem;
  }

  /* When the nav is in a "stuck" state */
  @container scroll-state(stuck: top) {
    /* Same as before */

    a {
      color: orangered;
      font-size: 1.5rem;
    }
  }
}So, yeah. As I was saying, it must be pretty cool to be on the Chrome developer team and get ahead of stuff like this, as it’s released. Big ol’ thanks to Bramus and Adam for consistently cluing us in on what’s new and doing the great work it takes to come up with such amazing demos to show things off.]]></content:encoded></item><item><title>Real LLM Success Stories: How They Actually Work // Alex Strick van Linschoten // #287</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/Real-LLM-Success-Stories-How-They-Actually-Work--Alex-Strick-van-Linschoten--287-e2tvfr2</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/97549602/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-25%2F393695538-44100-2-283541e0a6446.mp3" length="" type=""/><pubDate>Fri, 31 Jan 2025 15:15:54 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Beyond DeepSeek: An Overview of Chinese AI Tigers and Their Cutting-Edge Innovations</title><link>https://www.topbots.com/chinese-ai-tigers-overview/</link><author>Kate Koidan</author><category>dev</category><category>ai</category><pubDate>Fri, 31 Jan 2025 14:53:52 +0000</pubDate><source url="https://www.topbots.com/">TopBots</source><content:encoded><![CDATA[The recent disruption caused by DeepSeek’s R1 model sent shockwaves through the AI community, demonstrating that Chinese AI advancements may have been underestimated. The model’s performance, rivaling some of the most advanced offerings from OpenAI and Anthropic at a fraction of the cost, signaled a new era of competition in artificial intelligence.However, DeepSeek is not the only Chinese company making waves in AI. While industry giants like Alibaba, Tencent, Baidu, and ByteDance continue to lead the charge, a new generation of AI startups – often referred to as the “Chinese AI Tigers” – is emerging as formidable players. These startups are pushing the limits of generative AI, challenging global incumbents with state-of-the-art models and breakthrough innovations.In this article, we will explore DeepSeek and five of the most influential Chinese AI startups: Moonshot AI, Zhipu AI, Baichuan AI, MiniMax, and 01.AI. Each of these companies has developed cutting-edge AI models and solutions that are shaping the future of artificial intelligence, both in China and beyond.DeepSeek: A Research-Driven AI Powerhouse in ChinaFounded in May 2023, DeepSeek is an AI company based in Hangzhou, operating as an independent entity under High-Flyer, a leading Chinese quantitative hedge fund. Unlike many AI startups chasing commercialization, DeepSeek prioritizes technical innovation, running more like a research lab than a traditional business. The company is focused on developing artificial general intelligence (AGI) through breakthroughs in mathematics, coding, and multimodal AI capabilities.Technical Innovation & Model EfficiencyDeepSeek has distinguished itself through cost-effective AI solutions and pioneering architectural advancements. Its research efforts have led to the deployment of novel techniques, including Multi-Head Latent Attention (MLA), sparse Mixture-of-Experts (MoE), and FP8 mixed precision training. These innovations significantly reduce memory requirements and computational costs, allowing DeepSeek to achieve state-of-the-art performance with fewer resources than industry giants like OpenAI, Meta, and Anthropic.DeepSeek V3, its most advanced model, boasts 671B parameters and was trained in just 55 days at a cost of $5.58 million – an order of magnitude more efficient than its Western counterparts. The company has also committed to open-source development, furthering its influence in the AI research community. (May 2024): Introduced the MLA architecture, reducing inference costs and intensifying competition in China’s AI market. (December 2024): A 671B-parameter model trained in record time, outperforming Llama 3.1 and Qwen 2.5 while matching GPT-4o and Claude 3.5 Sonnet. (January 2025): A reasoning model based on DeepSeek-V3 that rivals OpenAI’s o1 in mathematics and coding benchmarks. Open-sourced and free for commercial use, it delivers performance comparable to GPT models at just 3% of the cost. (January 2025): A vision model capable of understanding and generating images from text prompts, outperforming OpenAI’s DALL-E 3 and Stable Diffusion 3 in multiple image generation benchmarks.: Optimized smaller-scale versions that retain high efficiency and strong performance.Leadership & Talent StrategyDeepSeek is led by CEO Liang Wenfeng, a former AI researcher and founder of High-Flyer Quant. Known for his technical idealism and open-source advocacy, Liang believes AGI could be achieved within the next decade. The company’s talent acquisition strategy is unconventional, focusing on hiring fresh graduates and early-career researchers driven by curiosity rather than experience. This approach fosters a culture of bottom-up innovation, with flexible resource allocation to encourage groundbreaking research.Unlike many AI startups dependent on external venture capital, DeepSeek is entirely funded by High-Flyer Quant. The hedge fund’s strategic investment in over 10,000 GPUs before U.S. sanctions has provided DeepSeek with a significant computational advantage.While primarily research-driven, DeepSeek has achieved profitability through API revenue. Its aggressive pricing – offering API access at just 1/53rd the cost of Claude 3.5 Sonnet – has triggered a price war in the AI industry, challenging established players.DeepSeek’s chatbot technology demonstrates strong performance, delivering factual responses with linked resources. By combining technical breakthroughs, open-source accessibility, and an unconventional approach to AI development, DeepSeek is emerging as one of China’s most formidable AI contenders.Moonshot AI: Pushing the Boundaries of Long-Context LLMsMoonshot AI (YueZhiAnMian) is a Beijing-based artificial intelligence company founded in March 2023 by AI researchers Yang Zhilin, Zhou Xinyu, and Wu Yuxin. The company’s name draws inspiration from Pink Floyd’s album The Dark Side of the Moon, reflecting CEO Yang Zhilin’s admiration for the band. Moonshot AI specializes in developing LLMs capable of processing extensive text inputs, with a strong emphasis on long-form context and response capabilities. (October 2023): A breakthrough AI assistant capable of handling up to 200,000 Chinese characters in a single input, making it a leader in long-form text processing. Available via chat interface and API. (January 2025): The latest o1-level multimodal model, achieving state-of-the-art performance on short-chain-of-thought (CoT) benchmarks. It significantly outperforms GPT-4o and Claude 3.5 Sonnet on AIME, MATH-500, and LiveCodeBench, with improvements reaching up to +550%.Moonshot-V1-Vision-Preview (January 2025): A multimodal image understanding model that excels in OCR text extraction, image recognition, and advanced visual processing.CEO Yang Zhilin, a co-founder of Recurrent AI, previously worked at Meta AI and Google Brain. He earned a PhD in computer science from Carnegie Mellon University under the advisement of Ruslan Salakhutdinov. A key contributor to Transformer-XL, Yang’s expertise in extending language model context windows has been instrumental in shaping Moonshot AI’s platform.Moonshot AI generates revenue through subscription-based services, pay-per-use API access, and licensing its AI technologies.Moonshot AI’s chatbot delivers high-quality responses with factual accuracy and linked sources. With its expertise in long-context language processing and a strong funding base, Moonshot AI is poised to become a dominant player in China’s AI landscape, challenging global incumbents with its cutting-edge models.Zhipu AI: Advancing AI with Multimodal and Enterprise SolutionsFounded in 2019 as a spin-off from Tsinghua University, Zhipu AI (Beijing Zhipu Huazhang Technology) has emerged as a key player in China’s AI landscape. The company focuses on developing advanced LLMs and multimodal AI applications for both consumer and enterprise use cases, with a mission to “teach machines to think like humans.”: An open-source, end-to-end speech large language model, released in October 2024, capable of human-like interactions with customizable tone, emotion, and dialect.: A 130B-parameter code-generation model.: A text-to-image model that generates high-quality, realistic, and novel images from textual descriptions.: A voice-command-driven AI agent designed for smartphone task automation, capable of handling complex requests like ordering from local stores based on user shopping history.: A cloud-based AI service providing enterprise access to Zhipu’s suite of AI models, including GLP-4-Plus, CogView-3-Plus, CogVideoX, and GLM-4V-Plus.Zhipu AI’s leadership team is anchored by CEO Zhang Peng, a Tsinghua University alumnus. The company’s foundation is deeply rooted in academia, having been co-founded by Tang Jie and Li Juanzi, both esteemed professors at Tsinghua University. Tang Jie, who serves as the Chief Scientist, is also the vice director of the Beijing Academy of Artificial Intelligence, highlighting his significant influence in China’s AI research community.Zhipu AI has attracted significant investment from major Chinese technology firms and venture capitalists. In 2023, the company raised RMB 2.5 billion (≈$350 million) in funding, with backers including Meituan, Ant Group, Alibaba, Tencent, Xiaomi, Sequoia Capital, and GL Ventures. A December 2024 funding round secured an additional $420 million, valuing the company at over $2.8 billion.The company’s primary revenue stream comes from API access through the Zhipu MaaS Platform. In 2024, API revenue surged more than 30-fold year-over-year, with daily token consumption increasing 150-fold. The MaaS platform has attracted over 700,000 enterprise and developer users.Challenges & U.S. Trade BlacklistIn October 2024, Zhipu AI was added to the U.S. trade blacklist, limiting its access to American technology due to concerns about its ties to the Chinese military. Despite these restrictions, Zhipu AI has remained resilient, maintaining strong domestic growth and continued support from major Chinese investors.While Zhipu AI faces stiff competition from other Chinese AI tigers, it continues to solidify its position in the Chinese AI ecosystem through a strong enterprise focus and cutting-edge multimodal models. However, its chatbot’s performance lags behind competitors, often struggling with precise, nuanced responses and occasionally mixing English and Chinese in its outputs. Nonetheless, its rapid growth and financial backing make it a formidable contender in China’s AI race.Baichuan AI: China’s OpenAI ChallengerFounded in April 2023, Baichuan AI (Beijing Baichuan Intelligent Technology Co., Ltd.) is a Beijing-based artificial intelligence company led by Wang Xiaochuan, the former CEO of Sogou. The company positions itself as China’s equivalent of OpenAI, focusing on AI-driven solutions in healthcare, education, and finance. One of its core missions is leveraging AI to tackle systemic issues such as the global shortage of skilled doctors.: Baichuan-7B, Baichuan-13B, Baichuan2-7B, and Baichuan2-13B.:
: A multimodal model with extended context and integrated search capabilities.: Uses a MoE (Mixture of Experts) architecture to optimize costs and significantly enhance speed.: Tailored for high-frequency enterprise use, improving response time and efficiency.: Competing with local AI offerings like Baidu’s Ernie Bot and Moonshot AI’s Kimi, Baixiaoying has gained recognition for its superior search capabilities, functioning “like a professional” according to company claims.Baichuan AI is led by founder and CEO Wang Xiaochuan, who previously helmed Sogou, once China’s second-largest search engine before its acquisition by Tencent. The company’s core team comprises AI experts from leading global tech firms, including Google, Tencent, Baidu, Huawei, Microsoft, and ByteDance, bringing together deep expertise in AI development and deployment.Baichuan AI has received strong backing from major Chinese technology investors. In October 2023, the company raised $300 million in a Series A1 funding round, led by Alibaba, Tencent, and Xiaomi. This was followed by a Series A round in July 2024, which secured an additional $691 million, bringing its valuation to $2.7 billion.Baichuan AI’s business model revolves around API monetization, offering enterprise access to its proprietary models. Additionally, it provides domain-specific AI solutions across healthcare, finance, education, and entertainment. The company also customizes AI solutions for enterprise clients, leveraging its foundational models to address specific industry needs.With a clear ambition to rival OpenAI, Baichuan AI is rapidly expanding its technological footprint in China. Its robust funding, seasoned leadership, and advanced AI models position it as a key player in China’s AI landscape. MiniMax: Advancing Multimodal AI SolutionsFounded in December 2021, MiniMax is a cutting-edge artificial intelligence company dedicated to developing large-scale AI models and multimodal technologies. The company integrates text, voice, and vision capabilities into its AI solutions, positioning itself as a key player in China’s AI landscape. (June 2023): An AI companion app allowing users to engage with virtual characters, including digital recreations of celebrities. The app gained significant traction in international markets, particularly among U.S. teenagers, ranking as the fifth most downloaded free entertainment app in the U.S. (March 2024): A multimodal large language model consumer platform offering AI-generated text and music features. Expanded in September 2024 with , a text-to-video model producing high-resolution videos from text prompts.:
: A 456B-parameter text-only model that surpasses Google’s Gemini 2.0 Flash on MMLU and SimpleQA benchmarks. With a 4M-token context window, it processes over 3 million words in a single input – approximately five times the length of .: A multimodal model integrating text and visual inputs, excelling in ChartQA benchmarks. While it rivals Anthropic’s Claude 3.5 Sonnet, it falls short of OpenAI’s GPT-4o, Google’s Gemini 2.0 Flash, and InternVL2.5 in some evaluations.: An advanced speech synthesis model supporting 17 languages, including English and Chinese. It enables voice cloning from a mere 10-second sample and is available exclusively via MiniMax’s API and Hailuo AI platform.MiniMax was founded by Yan Junjie and Zhou Yucong, both former SenseTime employees. Yan led deep learning toolchain development at SenseTime, while Zhou spearheaded its algorithms R&D team. The leadership team also includes Yang Bin, a former Uber AI researcher known for his work in autonomous driving technologies.MiniMax has attracted substantial investment from leading Chinese technology firms. In June 2023, the company raised over $250 million, backed by a Tencent-led entity, bringing its valuation to $1.2 billion. A subsequent funding round in March 2024, led by Alibaba Group, secured $600 million, raising its valuation to $2.5 billion. Investors also include Hillhouse Investment, HongShan, IDG Capital, and Tencent.MiniMax operates under a hybrid business model that balances open-source development, API services, and product innovation. The company releases select AI models with licensing restrictions to prevent their use in developing competing AI systems. It also provides API access to its AI capabilities, allowing third-party developers to integrate MiniMax’s technologies into their applications. Additionally, MiniMax develops both B2B solutions and consumer-facing applications, such as AI-powered role-playing platforms and text-to-video generators, positioning itself as a versatile player in the AI ecosystem.MiniMax has faced regulatory challenges, including the removal of Talkie from Apple’s App Store in December 2024 due to concerns over unauthorized AI avatars of public figures. Additionally, the company has been accused of using copyrighted content in its training data by British TV channels and Chinese streaming service iQIY. Despite these hurdles, MiniMax continues to innovate and expand its AI capabilities, solidifying its position in China’s AI sector.01.AI: Industry-Specific AI Company Led by Kai-Fu LeeFounded in March 2023, 01.AI is a Beijing-based artificial intelligence startup led by Kai-Fu Lee. The company focuses on developing smaller, industry-specific AI models while balancing both open-source and proprietary solutions. (November 2023): Released three model variants – Yi-6B, Yi-9B, and Yi-34B – designed for text generation, understanding, and complex reasoning. These models serve as the foundation for 01.AI’s large-scale language model offerings. (Early 2024): The company’s first closed-source model, built for high-performance reasoning and content creation.: A specialized coding model excelling in code completion, editing, and long-context modeling, supporting up to 128K tokens. It also demonstrates strong mathematical reasoning. (May 2024): A personal AI workspace and productivity tool aimed at enhancing user efficiency.01.AI is led by Kai-Fu Lee, a renowned AI pioneer and former executive at Microsoft and Google. Lee is also the co-founder of Sinovation Ventures, a leading Chinese venture capital firm. The company has a team of over 100 AI specialists and experienced business professionals, many of whom previously worked with Lee at top tech firms.01.AI has secured significant investments, raising $300 million in November 2023 from Tencent, Xiaomi, Alibaba Cloud, and Sinovation Ventures, which brought its valuation to $1 billion. In August 2024, the company reportedly secured additional funding worth hundreds of millions from an international strategic investor, a Southeast Asian consortium, and other institutions.The company operates under a dual approach, balancing open-source accessibility with proprietary solutions. Its smaller models are open-source, allowing for academic and commercial use while maintaining strong performance across various AI benchmarks. In parallel, 01.AI offers API access to its closed-source models, providing six enterprise-optimized APIs, including Yi-Large, Yi-Large-Turbo API, Yi-Medium API, Yi-Medium-200K API, Yi-Vision API, and Yi-Spark API. Additionally, the company develops consumer-facing AI applications such as Wanzhi, an AI productivity tool, and RuYi, a digital human solution, further expanding its reach in both enterprise and individual user markets.In August 2024, 01.AI announced that its Wanzhi productivity app had reached 10 million users, generating over 100 million yuan ($13.8 million) in revenue. With a strong balance between open-source development and enterprise-driven AI solutions, 01.AI is positioning itself as a key player in China’s AI sector, offering scalable and specialized AI tools for a range of industries.Conclusion: The Future of China’s AI InnovatorsThe rapid advancements made by DeepSeek and the Chinese AI Tigers highlight the country’s growing prominence in artificial intelligence. These startups are not only competing with global tech giants but also setting new benchmarks in efficiency, multimodal capabilities, and enterprise applications. As China continues to invest heavily in AI research and development, the next wave of innovations from these companies could reshape the global AI landscape.Enjoy this article? Sign up for more AI updates.We’ll let you know when we release more overview articles like this one.]]></content:encoded></item><item><title>Deepgram builds and hosts ‘Voice AI’ models</title><link>https://www.datasciencecentral.com/deepgram-builds-and-hosts-voice-ai-models/</link><author>Jelani Harper</author><category>dev</category><category>ai</category><pubDate>Fri, 31 Jan 2025 14:25:44 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Most enterprise applications of Artificial Intelligence are predicated on understanding, and generating, natural language in the form of text. Deepgram, an AI platform for generating and understanding spoken and written language, is looking to expand this paradigm by making applications just as accessible—and adept at understanding natural language—for vocal deployments of speech. The startup’s platform… Read More »]]></content:encoded></item><item><title>Keeping the page interactive while a View Transition is running</title><link>https://css-tricks.com/keeping-the-page-interactive-while-a-view-transition-is-running/</link><author>Geoff Graham</author><category>frontend</category><pubDate>Fri, 31 Jan 2025 14:11:00 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[When using View Transitions you’ll notice the page becomes unresponsive to clicks while a View Transition is running. […] This happens because of the  pseudo element – the one that contains all animated snapshots – gets overlayed on top of the document and captures all the clicks.::view-transition /* 👈 Captures all the clicks! */
└─ ::view-transition-group(root)
   └─ ::view-transition-image-pair(root)
      ├─ ::view-transition-old(root)
      └─ ::view-transition-new(root)The trick? It’s that sneaky little  property! Slapping it directly on the  allows us to click “under” the pseudo-element, meaning the full page is interactive even while the view transition is running.::view-transition {
  pointer-events: none;
}I always, always,  forget about , so thanks to Bramus for posting this little snippet. I also appreciate the additional note about removing the  element from participating in the view transition::root {
  view-transition-name: none;
}He quotes the spec noting the reason why snapshots do not respond to hit-testing:Elements participating in a transition need to skip painting in their DOM location because their image is painted in the corresponding  pseudo-element instead. Similarly, hit-testing is skipped because the element’s DOM location does not correspond to where its contents are rendered.]]></content:encoded></item><item><title>Harnessing AI to Simplify Cloud Configuration Drift Detection</title><link>https://blogs.cisco.com/security/harnessing-ai-to-simplify-cloud-configuration-drift-detection/</link><author>Gogulakrishnan Thiyagarajan</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Fri, 31 Jan 2025 13:00:00 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[Explore how AI-driven solutions can proactively identify configuration drifts, enhance cloud security, and ensure compliance efficiently.]]></content:encoded></item><item><title>The Real Python Podcast – Episode #237: Testing Your Python Code Base: Unit vs. Integration</title><link>https://realpython.com/podcasts/rpp/237/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Fri, 31 Jan 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[What goes into creating automated tests for your Python code? Should you focus on testing the individual code sections or on how the entire system runs? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.]]></content:encoded></item><item><title>ChatLoopBackOff Episode 44 (k3s)</title><link>https://www.youtube.com/watch?v=6vYfJ6MM9_o</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/6vYfJ6MM9_o?version=3" length="" type=""/><pubDate>Fri, 31 Jan 2025 05:50:51 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[K3s, a CNCF Sandbox project, is a lightweight, certified Kubernetes distribution designed for simplicity, minimal resource consumption, and ease of use. Its small binary size, pre-bundled dependencies, and single-command installation make it ideal for edge computing, IoT, and development environments. 

K3s feature low hardware requirements, support for ARM devices, and optimized performance, so it runs seamlessly on resource-constrained systems. Join CNCF Ambassador Aditya Soni while he explores how K3s ensures compatibility with Kubernetes tools while lowering the complexity and cost of deployment.]]></content:encoded></item><item><title>A WebAssembly compiler in 192 bytes</title><link>https://javascriptweekly.com/issues/721</link><author></author><category>dev</category><category>frontend</category><pubDate>Fri, 31 Jan 2025 00:00:00 +0000</pubDate><source url="https://javascriptweekly.com/">Javascript Weekly</source><content:encoded><![CDATA[ — Take your knowledge to the next level with the most loved JavaScript course in the industry. Deepen your understanding of the most important aspects of JavaScript. This highly rated video course goes under the hood, looking at callbacks, higher-order functions, object-oriented JS, and more.]]></content:encoded></item><item><title>Executive Order to the State Department Sideswipes Freedom Tools, Threatens Censorship Resistance, Privacy, and Anonymity of Millions</title><link>https://www.eff.org/deeplinks/2025/01/executive-order-state-department-sideswipes-freedom-tools-threatens-censorship</link><author>Cindy Cohn</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/white-house-1.jpg" length="" type=""/><pubDate>Thu, 30 Jan 2025 21:35:28 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Internet Never Forgets: Fighting the Memory Hole</title><link>https://www.eff.org/deeplinks/2025/01/internet-never-forgets-fighting-memory-hole</link><author>Katharine Trendacosta</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/fixcopyright-graphic-banner.jpg" length="" type=""/><pubDate>Thu, 30 Jan 2025 21:16:21 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>gRPC vs REST vs GraphQL: Comparison &amp; Performance</title><link>https://www.youtube.com/watch?v=uH0SxYdsjv4</link><author>Anton Putra</author><category>dev</category><category>video</category><enclosure url="https://www.youtube.com/v/uH0SxYdsjv4?version=3" length="" type=""/><pubDate>Thu, 30 Jan 2025 19:54:43 +0000</pubDate><source url="https://www.youtube.com/channel/UCeLvlbC754U6FyFQbKc0UnQ">Anton Putra</source><content:encoded><![CDATA[gRPC vs REST vs GraphQL Performance Benchmark.

🔴 To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)

🍿 Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&si=p-UOaVM_6_SFx52H

👋 AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra

▬▬▬▬▬ Experience & Location 💼 ▬▬▬▬▬
►  I’m a Senior Software Engineer at Juniper Networks (13+ years of experience)
►  Located in San Francisco Bay Area, CA (US citizen)

▬▬▬▬▬▬ Connect with me 👋 ▬▬▬▬▬▬
►  LinkedIn: https://www.linkedin.com/in/anton-putra
►  Twitter/X: https://twitter.com/antonvputra
►  Instagram: https://www.instagram.com/aputrabay
►  GitHub: https://github.com/antonputra
►  Email: me@antonputra.com

▬▬▬▬▬▬▬ Source Code 📚 ▬▬▬▬▬▬▬
► Original Source Code: https://github.com/antonputra/tutorials/tree/240/lessons/240

#python #golang #nodejs]]></content:encoded></item><item><title>Infrastructure Laundering: Blending in with the Cloud</title><link>https://krebsonsecurity.com/2025/01/infrastructure-laundering-blending-in-with-the-cloud/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Thu, 30 Jan 2025 17:10:08 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[Image: Shutterstock, ArtHead.In an effort to blend in and make their malicious traffic tougher to block, hosting firms catering to cybercriminals in China and Russia increasingly are funneling their operations through major U.S. cloud providers. Research published this week on one such outfit — a sprawling network tied to Chinese organized crime gangs and aptly named “” — highlights a persistent whac-a-mole problem facing cloud services.In October 2024, the security firm  published a lengthy analysis of how  and  were providing services to Funnull, a two-year-old Chinese content delivery network that hosts a wide variety of fake trading apps, pig butchering scams, gambling websites, and retail phishing pages.Funnull made headlines last summer after it acquired the domain name , previously the home of a widely-used open source code library that allowed older browsers to handle advanced functions that weren’t natively supported. There were still tens of thousands of legitimate domains linking to the Polyfill domain at the time of its acquisition, and Funnull soon after conducted a supply-chain attack that redirected visitors to malicious sites.Silent Push’s October 2024 report found a vast number of domains hosted via Funnull promoting gambling sites that bear the logo of the , a Chinese entity named in a 2024 UN report (PDF) for laundering millions of dollars for the North Korean Lazarus Group.It is likely the gambling sites coming through Funnull are abusing top casino brands as part of their money laundering schemes. In reporting on Silent Push’s October report, obtained a comment from Bwin, one of the casinos being advertised en masse through Funnull, and Bwin said those websites did not belong to them.Gambling is illegal in China except in Macau, a special administrative region of China. Silent Push researchers say Funnull may be helping online gamblers in China evade the Communist party’s “Great Firewall,” which blocks access to gambling destinations.Silent Push’s  said that upon revisiting Funnull’s infrastructure again this month, they found dozens of the same Amazon and Microsoft cloud Internet addresses still forwarding Funnull traffic through a dizzying chain of auto-generated domain names before redirecting malicious or phishous websites.Edwards said Funnull is a textbook example of an increasing trend Silent Push calls “infrastructure laundering,” wherein crooks selling cybercrime services will relay some or all of their malicious traffic through U.S. cloud providers.“It’s crucial for global hosting companies based in the West to wake up to the fact that extremely low quality and suspicious web hosts based out of China are deliberately renting IP space from multiple companies and then mapping those IPs to their criminal client websites,” Edwards told KrebsOnSecurity. “We need these major hosts to create internal policies so that if they are renting IP space to one entity, who further rents it to host numerous criminal websites, all of those IPs should be reclaimed and the CDN who purchased them should be banned from future IP rentals or purchases.”A Suncity gambling site promoted via Funnull. The sites feature a prompt for a Tether/USDT deposit program.Reached for comment, Amazon referred this reporter to a statement Silent Push included in a report released today. Amazon said AWS was already aware of the Funnull addresses tracked by Silent Push, and that it had suspended all known accounts linked to the activity.Amazon said that contrary to implications in the Silent Push report, it has every reason to aggressively police its network against this activity, noting the accounts tied to Funnull used “fraudulent methods to temporarily acquire infrastructure, for which it never pays. Thus, AWS incurs damages as a result of the abusive activity.”“When AWS’s automated or manual systems detect potential abuse, or when we receive reports of potential abuse, we act quickly to investigate and take action to stop any prohibited activity,” Amazon’s statement continues. “In the event anyone suspects that AWS resources are being used for abusive activity, we encourage them to report it to AWS Trust & Safety using the report abuse form. In this case, the authors of the report never notified AWS of the findings of their research via our easy-to-find security and abuse reporting channels. Instead, AWS first learned of their research from a journalist to whom the researchers had provided a draft.”Microsoft likewise said it takes such abuse seriously, and encouraged others to report suspicious activity found on its network.“We are committed to protecting our customers against this kind of activity and actively enforce acceptable use policies when violations are detected,” Microsoft said in a written statement. “We encourage reporting suspicious activity to Microsoft so we can investigate and take appropriate actions.” is threat intelligence lead at . Hummel said it used to be that “noisy” and frequently disruptive malicious traffic — such as automated application layer attacks, and “brute force” efforts to crack passwords or find vulnerabilities in websites — came mostly from botnets, or large collections of hacked devices.But he said the vast majority of the infrastructure used to funnel this type of traffic is now proxied through major cloud providers, which can make it difficult for organizations to block at the network level.“From a defenders point of view, you can’t wholesale block cloud providers, because a single IP can host thousands or tens of thousands of domains,” Hummel said.In May 2024, KrebsOnSecurity published a deep dive on Stark Industries Solutions, an ISP that materialized at the start of Russia’s invasion of Ukraine and has been used as a global proxy network that conceals the true source of cyberattacks and disinformation campaigns against enemies of Russia. Experts said much of the malicious traffic  traversing Stark’s network (e.g. vulnerability scanning and password brute force attacks) was being bounced through U.S.-based cloud providers.Stark’s network has been a favorite of the Russian hacktivist group called , which frequently launches huge distributed denial-of-service (DDoS) attacks against a variety of targets seen as opposed to Moscow. Hummel said NoName’s history suggests they are adept at cycling through new cloud provider accounts, making anti-abuse efforts into a game of whac-a-mole.“It almost doesn’t matter if the cloud provider is on point and takes it down because the bad guys will just spin up a new one,” he said. “Even if they’re only able to use it for an hour, they’ve already done their damage. It’s a really difficult problem.”Edwards said Amazon declined to specify whether the banned Funnull users were operating using compromised accounts or stolen payment card data, or something else.“I’m surprised they wanted to lean into ‘We’ve caught this 1,200+ times and have taken these down!’ and yet didn’t connect that each of those IPs was mapped to [the same] Chinese CDN,” he said. “We’re just thankful Amazon confirmed that account mules are being used for this and it isn’t some front-door relationship. We haven’t heard the same thing from Microsoft but it’s very likely that the same thing is happening.”Funnull wasn’t always a bulletproof hosting network for scam sites. Prior to 2022, the network was known as , based in the Philippines. One of Anjie’s properties was a website called . Loading that domain reveals a pop-up message by the original Anjie CDN owner, who said their operations had been seized by an entity known as  and , the parent company of Funnull.A machine-translated message from the former owner of Anjie CDN, a Chinese content delivery network that is now Funnull.“After I got into trouble, the company was managed by my family,” the message explains. “Because my family was isolated and helpless, they were persuaded by villains to sell the company. Recently, many companies have contacted my family and threatened them, believing that Fangneng CDN used penetration and mirroring technology through customer domain names to steal member information and financial transactions, and stole customer programs by renting and selling servers. This matter has nothing to do with me and my family. Please contact Fangneng CDN to resolve it.”In January 2024, the U.S. Department of Commerce issued a proposed rule that would require cloud providers to create a “Customer Identification Program” that includes procedures to collect data sufficient to determine whether each potential customer is a foreign or U.S. person.According to the law firm , the Commerce rule also would require “infrastructure as a service” (IaaS) providers to report knowledge of any transactions with foreign persons that might allow the foreign entity to train a large AI model with potential capabilities that could be used in malicious cyber-enabled activity.“The proposed rulemaking has garnered global attention, as its cross-border data collection requirements are unprecedented in the cloud computing space,” Crowell wrote. “To the extent the U.S. alone imposes these requirements, there is concern that U.S. IaaS providers could face a competitive disadvantage, as U.S. allies have not yet announced similar foreign customer identification requirements.”It remains unclear if the new White House administration will push forward with the requirements. The Commerce action was mandated as part of an executive order President Trump issued a day before leaving office in January 2021.]]></content:encoded></item><item><title>4 steps to building a natural language search tool</title><link>https://github.blog/open-source/social-impact/4-steps-to-building-a-natural-language-search-tool/</link><author>Ryan Fallon</author><category>official</category><pubDate>Thu, 30 Jan 2025 17:00:40 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[“We have a problem. Our current search method for sifting through PDFs is extremely manual and time consuming. Is there an easier way?”As a developer, this is one of those questions that really gets me excited. I was tasked with finding a way to transform a cumbersome, archival process into an efficient, intuitive search experience. It’s a way to make a group of people’s lives easier, and because of the organizations they work for, help them be more effective in providing humanitarian assistance to people in need around the world. I couldn’t imagine a better project to be working on.Unlocking the United Nations’ legacy for rapid actionSince 1945, the United Nations has produced resolutions and other documents that guide international peace and security efforts. Yet accessing this wealth of knowledge remains a challenge, including for organizations such as the International Committee of the Red Cross (ICRC). Currently, delegates at ICRC’s permanent observer mission to the UN advise member states and other stakeholders on international humanitarian law and humanitarian issues. When states negotiate relevant resolutions and other UN products, leaning on pre-existing humanitarian language from UN resolutions can provide precedence. This often requires sifting through PDFs to find relevant content within documents—a time-intensive, manual process ill-suited to the fast-paced world of humanitarian diplomacy.To solve this, I built a single-page application (SPA) that enables users to input natural language queries and instantly retrieve relevant UN resolutions. The solution is live now at resolutions.projectrefuge.io and serves as a robust example of how technology can simplify access to critical information.Text extraction and structuring
Using Amazon Textract, I extracted raw text from decades’ worth of UN Security Council Resolutions and Presidential Statements and six years of UN General Assembly Resolutions. A Go script then parsed this text using Regex matching, segmenting it into individual resolutions for easier indexing.
Search-ready database with MongoDB Atlas
I adapted a Node.js script from MongoDB to upload the parsed resolutions as embeddings into a MongoDB Atlas database. This step ensures the content is structured for fast and relevant searches.User interface built with Vue.js
The front end is an intuitive SPA created with Vue.js. Users simply enter semantic search queries—such as “resolutions on humanitarian access in armed conflicts”—and receive results in seconds.
The backend relies on AWS Lambda and API Gateway, ensuring scalability and seamless performance. The entire application is hosted as a subdomain on AWS Amplify, combining reliability with ease of access.Broader implications: a blueprint for impactThe implications of this project go far beyond the ICRC’s use case with UN Resolutions. With slight modifications, the tool could index and search any collection of legal and policy documents. This approach is a blueprint for organizations aiming to leverage technology for better decision-making and more effective action. For nonprofits, this demonstrates the power of owning your code and building tailored solutions. For developers, it’s a reminder of how open source can accelerate progress in humanitarian and public policy sectors.Build together with open sourceProjects like resolutions.projectrefuge.io highlight the potential of open source to transform how we access and use information. If you’re a nonprofit, explore GitHub for Nonprofits to discover tools and resources that can help you build your own solutions. Developers eager to contribute to impactful work can browse the For Good First Issue program to find projects that align with their skills and values.Finally, stay tuned as we work to identify other opportunities with humanitarian actors such as the ICRC to bridge the technology and humanitarian space. Together, we can build a future where knowledge is more accessible and tools are built with collaboration in mind, ensuring that humanitarian efforts are supported by cutting-edge technology.Let’s code for good—and make a lasting impact.If you’d like to lend your developer skills for good, check out For Good First Issue, a curated platform of open source projects that contribute to a better future for everyone.]]></content:encoded></item><item><title>Oracle Faces Java Customer Revolt After &apos;Predatory&apos; Pricing Changes</title><link>https://developers.slashdot.org/story/25/01/30/1617210/oracle-faces-java-customer-revolt-after-predatory-pricing-changes?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Thu, 30 Jan 2025 16:41:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[Nearly 90% of Oracle Java customers are looking to abandon the software maker's products following controversial licensing changes made in 2023, according to research firm Dimensional Research. 

The exodus reflects growing frustration with Oracle's shift to per-employee pricing for its Java platform, which critics called "predatory" and could increase costs up to five times for the same software, Gartner found. The dissatisfaction runs deepest in Europe, where 92% of French and 95% of German users want to switch to alternative providers like Bellsoft Liberica, IBM Semeru, or Azul Platform Core.]]></content:encoded></item><item><title>Organize a community meetup in your area</title><link>http://www.fsf.org/blogs/community/organize-a-community-meetup-in-your-area</link><author></author><category>linux</category><pubDate>Thu, 30 Jan 2025 15:55:00 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[Let's claim the month of May as LibreLocal month!]]></content:encoded></item><item><title>Deaf and Hard of Hearing WG Meeting - 2025-01-28</title><link>https://www.youtube.com/watch?v=Rndt_zS6v_w</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/Rndt_zS6v_w?version=3" length="" type=""/><pubDate>Thu, 30 Jan 2025 15:09:58 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Data strategy for sustainable ESG compliance</title><link>https://www.datasciencecentral.com/data-strategy-for-sustainable-esg-compliance/</link><author>Rob Turner</author><category>dev</category><category>ai</category><pubDate>Thu, 30 Jan 2025 14:54:06 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Image: pexels Businesses face mounting pressure to meet ESG, i.e. Environmental, Social, and Governance expectations. Compliance is no longer optional. It shapes reputations, investor trust, and long-term growth. How do data strategies fit in? They provide a backbone for tracking progress while ensuring companies align with legal and ethical benchmarks. This article unpacks how data… Read More »]]></content:encoded></item><item><title>The Mistakes of CSS</title><link>https://css-tricks.com/the-mistakes-of-css/</link><author>Juan Diego Rodríguez</author><category>frontend</category><pubDate>Thu, 30 Jan 2025 14:31:08 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[Surely you have seen a CSS property and thought “Why?” For example:Why doesn’t  work on all elements, and why is it “-” anyways?Why do we need  to animate to ?You are not alone. CSS was born in 1996 (it can legally order a beer, you know!) and was initially considered a way to style documents; I don’t think anyone imagined everything CSS would be expected to do nearly 30 years later. If we had a time machine, many things would be done differently to match conventions or to make more sense. Heck, even the CSS Working Group admits to wanting a time-traveling contraption… in the specifications! If we had a time machine, this property wouldn’t need to exist.If by some stroke of opportunity, I was given free rein to rename some things in CSS, a couple of ideas come to mind, but if you want more, you can find an ongoing list of mistakes made in CSS… by the CSS Working Group! Take, for example, :Not quite a mistake, because it was a reasonable default for the 90s, but it would be more helpful since then if  defaulted to .Why not fix them? Sadly, it isn’t as easy as  something. People already built their websites with these quirks in mind, and changing them would break those sites. Consider it technical debt.This is why I think the CSS Working Group deserves an onslaught of praise. Designing new features that are immutable once shipped has to be a nerve-wracking experience that involves inexact science. It’s not that we haven’t seen the specifications change or evolve in the past — they most certainly have — but the value of getting things right the first time is a beast of burden.]]></content:encoded></item><item><title>Slow, flaky, and failing</title><link>https://bitfieldconsulting.com/posts/slow-flaky-failing</link><author>John Arundel</author><category>dev</category><category>blog</category><category>go</category><pubDate>Thu, 30 Jan 2025 13:15:00 +0000</pubDate><source url="https://bitfieldconsulting.com/posts/">Bitfield</source><content:encoded><![CDATA[If you find yourself working on a project with quite a few broken
windows, it’s all too easy to slip into the mindset of “All the rest of
this code is crap, I’ll just follow suit.”
—David Thomas & Andrew Hunt, “The
Pragmatic Programmer: Your Journey to Mastery”It’s one minute to ship time, and you hit “push” on the very last
commit. There it goes: the build is running. Every second counts now,
and you watch the test output with increasing impatience. Why do the
tests take so darned ?And then, to your horror, the first red lights start to appear. “But
these were passing before, and I haven’t touched that code!” you wail.
It’s no good: your co-workers are already giving you the stink eye for
breaking the build and holding them up.You’re not going to ship today, for one simple reason: your tests are
slow, flaky, and failing. So what the hell?Flaky tests sometimes fail, sometimes pass, regardless of whether the
system is correct. There are many reasons for flaky tests, so let’s look
at a couple of them, with some possible solutions. can be a source of flakiness, as you probably
know from experience. In particular, fixed sleeps in tests are a bad
idea (see the next section for more about these). Eliminate these
wherever possible and replace them with code that only waits as long as
strictly necessary.When you need to  timing itself, use the shortest
possible interval. For example, don’t test a timer function with a
one-second duration when one millisecond would work just as well.In some tests, as crazy as it sounds, the  can
affect the test. One way to eliminate this cause of flakiness is by
turning , and if necessary injecting a fake
 function to return a canned time of day.Flakiness can also sometimes arise from .
Some data structures in Go are inherently unordered: maps, for example.
Comparing these needs special care.For example, iterating over a map comparing its elements is not good
enough: the iteration order of maps is unspecified in Go. Instead, we
can use the  function to compare maps regardless
of iteration order:On the other hand, slices  inherently ordered, and so
 requires this:But sometimes we don’t actually care about the order. Maybe we get
these results from some concurrent computations, and we don’t know what
order they will show up in. We just want to know that we 
the right results.To compare two slices for equal , then, regardless
of order, we can use  to sort them before the
comparison:Whatever the cause of a flaky test suite, it’s a serious problem.
Left untreated, it will continuously erode value from the tests, until
eventually they become useless and ignored by all. It should be a red
flag to hear something like “Oh yeah, that test just fails
sometimes.”As soon as you hear that, you know that the test has become useless.
Delete it, if the flakiness really can’t be fixed. Thou shalt not suffer
a flaky test to live. As soon as it starts flaking, it stops being a
useful source of feedback, and bad tests are worse than no tests.A  test is not the same thing as a flaky test: a
brittle test fails when you change something unrelated, whereas a flaky
test fails when it feels like it. Fixing brittle tests is usually a
matter of decoupling entangled components, or simply reducing the scope
(and thus sharpening the focus) of the test.On the other hand, flaky tests can require some time and effort to
find the underlying cause and address it. Only do this if the test is
really worth it; if not, just delete it.What if some tests aren’t just flaky, but fail all the time, because
bugs aren’t being fixed? This is a very dangerous situation, and without
prompt action the tests will rapidly become completely useless.Why? Because if tests are allowed to fail for a while without being
fixed, people soon stop trusting them, or indeed paying 
attention to them: “Oh yeah, that test always fails.”We can never have any failing tests, just as we can never have any
bugs:As soon as any test starts failing, fixing it should be everyone’s
top priority. No one is allowed to deploy any code change that’s not
about fixing this bug. Once you let one failing test slip through the
net, all the other tests become worthless.This so-called  sounds radical, but
it really isn’t. After all, what’s the alternative?The very first version of Microsoft Word for Windows was
considered a “death march” project. Managers were so insistent on
keeping to the schedule that programmers simply rushed through the
coding process, writing extremely bad code, because bug-fixing was not a
part of the formal schedule.Indeed, the schedule became merely a checklist of features waiting
to be turned into bugs. In the post-mortem, this was referred to as
“infinite defects methodology”.
—Joel Spolsky, “The
Joel Test: 12 Steps to Better Code”Fixing bugs now is cheaper, quicker, and makes more business sense
than fixing them later. The product should be ready to ship at all
times, without bugs.If you already  a large backlog of bugs, or failing
tests, but the company’s still in business, then maybe those bugs aren’t
really that critical after all. The best way out may be to declare
voluntary : just close all old bugs, or delete
all failing tests. Bugs that people  care about will pretty
soon be re-opened.My book The Power of Go: Tests is all
about how to write  tests: not just box-ticking
exercises to satisfy some bureaucratic manager, but tests that really
add value to the code, and make your work easier and more enjoyable.Even the world’s greatest test suite does us no good, though, if it
takes too long to run. How long is too long? Well, if we’re running
tests every few minutes, clearly even a few minutes is too long. We
simply won’t run the tests often enough to get the fast feedback we need
from them.By running the test suite frequently, at least several times a
day, you’re able to detect bugs soon after they are introduced, so you
can just look in the recent changes, which makes it much easier to find
them.
—Martin Fowler, “Self-Testing
Code”One way or the other, then, we don’t want to be more than about five
minutes away from passing tests. So, again, how long is 
long for a test suite to run?Kent Beck suggests that ten minutes is a psychologically significant
length of time:The equivalent of 9.8 m/s² is the ten-minute test suite. Suites
that take longer than ten minutes inevitably get trimmed, or the
application tuned up, so the suite takes ten minutes again.
—Kent Beck, “Test-Driven Development
by Example”We may perhaps call this psychological limit the .
Beyond the ten-minute mark, the problem is so obvious to everybody that
people are willing to put effort into speeding up the test suite. Below
that time, people will probably grumble but put up with it.That certainly doesn’t mean that a ten-minute test suite is okay:
it’s not, for the reasons we’ve discussed. Let’s look at a few simple
ways to reduce the overall run-time of the test suite to something more
manageable.. The inability to run certain
tests in parallel is usually a design smell. Refactor so that each test
has its own world, touches no global state, and can thus run in
parallel. Adding parallelism to a suite that doesn’t have it should
speed it up by about an order of magnitude.Eliminate unnecessary I/O. Once you go off the
chip, things get slow. Do everything on the chip as far as possible,
avoiding I/O operations such as network calls or accessing disk files.
For example, you could use an  as an in-memory
filesystem, and memory-backed s and
s instead of real files.. Instead of calling some
remote API, call a local fake instead. Local networking happens right in
the kernel, and while it’s still not , it’s a lot faster
than actually going out onto the wire.Share fixtures between tests. Any time you have
some expensive fixture setup to do, such as loading data into a
database, try to share its cost between as many tests as possible, so
that they can all use it. If necessary, do the setup in a single test
and then run a bunch of subtests against it.However, we need to be careful that the tests don’t then become flaky
as a result of too much fixture sharing. A flaky test is worse than a
slow test.. A test that can’t proceed until
some concurrent operation has completed should use the “wait for
success” pattern (loop and retry, with a tiny delay, until the operation
has completed). This minimises wasted time, whereas a long fixed sleep
maximises it (or causes flaky tests, which is also bad).Throw hardware at the problem. When you’ve made
the test suite as fast as it can go and it’s still slow, just run it on
a faster computer. If the tests are mostly CPU-bound, rent a 256-core
cloud machine and have it pull and run the tests on demand. CPU time
costs a lot less than programmer time, especially since hiring cheap
programmers is a false economy.. This is a last resort,
but it might come to that. If you have a few tests that simply
 be speeded up any more, and they’re dragging down the
rest of the suite, extract them to a separate “slow test” suite, and run
it on a schedule. Every night, perhaps; certainly no less frequently
than that. Even nightly isn’t great, but it’s better than not running
tests at all.]]></content:encoded></item><item><title>Top Threat Tactics and How to Address Them</title><link>https://blogs.cisco.com/security/top-threat-tactics-and-how-to-address-them/</link><author>Jennifer Golden</author><category>Cisco Security Blog</category><category>infosec</category><pubDate>Thu, 30 Jan 2025 13:00:31 +0000</pubDate><source url="https://blogs.cisco.com/security">Security @ Cisco Blogs</source><content:encoded><![CDATA[See the key takeaways for the most recent Cisco Talos Incident Response report and learn how you can use Cisco Security products to help defend against these.]]></content:encoded></item><item><title>Turso is rewriting SQLite in Rust (Interview)</title><link>https://changelog.com/podcast/626</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/podcast/626/the-changelog-626.mp3" length="" type=""/><pubDate>Thu, 30 Jan 2025 13:00:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Glauber Costa, co-founder and CEO of Turso, joins us to discuss libSQL, Limbo, and how they’re rewriting SQLite in Rust. We discuss their efforts with libSQL, the challenge of SQLite being in the public domain but not being open for contribution, their choice to rewrite everything with Limbo, how this all plays into the future of the Turso platform, how they test Limbo with Deterministic Simulation Testing (DST), and their plan to replace SQLite.Changelog++ members save 6 minutes on this episode because they made the ads disappear. Join today!Retool – The low-code platform for developers to build internal tools — Some of the best teams out there trust Retool…Brex, Coinbase, Plaid, Doordash, LegalGenius, Amazon, Allbirds, Peloton, and so many more – the developers at these teams trust Retool as the platform to build their internal tools. Try it free at retool.com/changelogTemporal – Build invincible applications. Manage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail. Register for Replay in London, March 3-5 to break free from the status quo.
Notion – Notion is a place where any team can write, plan, organize, and rediscover the joy of play. It’s a workspace designed not just for making progress, but getting inspired. Notion is for everyone — whether you’re a Fortune 500 company or freelance designer, starting a new startup or a student juggling classes and clubs.
Fly.io – The home of Changelog.com — Deploy your apps close to your users — global Anycast load-balancing, zero-configuration private networking, hardware isolation, and instant WireGuard VPN connections. Push-button deployments that scale to thousands of instances. Check out the speedrun to get started in minutes.
]]></content:encoded></item><item><title>Protect Your Privacy on Bumble</title><link>https://www.eff.org/deeplinks/2025/01/protect-your-privacy-bumble</link><author>Paige Collings</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/online-dating-1.jpg" length="" type=""/><pubDate>Thu, 30 Jan 2025 12:01:03 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[: Select the option for “Limited Access.”: Open  >  >  > >  > If you decide you’re done with Bumble for good, then you should delete your account before deleting the app off your phone. In the Bumble app, tap the Profile option, then tap the gear icon. Scroll down to the bottom of that page, tap “Delete Account” and follow the on-screen directions. Once complete, go ahead and delete the app.]]></content:encoded></item><item><title>Local Diffusion Inference with Stable-Diffusion.cpp and Flux.1</title><link>https://medium.com/@kyodo-tech/local-diffusion-inference-with-stable-diffusion-cpp-and-flux-1-5d1841fff9cf?source=rss-ac02ab142942------2</link><author>Kyodo Tech</author><category>dev</category><category>blog</category><pubDate>Thu, 30 Jan 2025 11:58:04 +0000</pubDate><source url="https://medium.com/@kyodo-tech?source=rss-ac02ab142942------2">Kyodo-Tech</source><content:encoded><![CDATA[Stable-Diffusion.cpp is designed for efficient, high-performance inference of Stable Diffusion models, especially on low-resource environments where traditional deep learning pipelines struggle. We provide an examination of SDcpp, its components, and the technical concepts that enable efficient inference of Stable Diffusion models on lower-end hardware such as CPUs and lightweight GPUs, such as Apple M-series chips. In particular, we’re looking at Flux.1, a text-to-image model developed by Black Forest Labs.Introduction to Diffusion Models and Stable DiffusionDiffusion models, specifically Denoising Diffusion Probabilistic Models (DDPMs), have emerged as powerful generative frameworks capable of producing high-fidelity images from random noise through iterative denoising processes. Stable Diffusion extends the DDPM framework by incorporating text conditioning, allowing for the generation of images based on descriptive prompts. However, the computational demands of such models typically necessitate high-memory, high-bandwidth GPU resources, limiting their accessibility and scalability.SDcpp addresses these limitations by providing an optimized inference framework that leverages techniques such as quantization, memory-efficient sampling, and optimized latent space representations to facilitate the deployment of Stable Diffusion models on devices with constrained computational resources. This paper delves into the architectural and algorithmic components of SDcpp, highlighting the methodologies that underpin its efficiency and performance.The architecture is composed of several interdependent modules, each contributing to the inference process. First, it is loading the required models (1), including text encoders, the diffusion model, and the VAE decoder. It then performs  (2), tokenizing the input prompt and generating text embeddings using a CLIP-based model and an additional T5XXL text conditioning model. Next, it computes the conditioning graph (3), transforming embeddings to create a guidance vector for the diffusion process. The system then  (4) by iteratively refining a randomly initialized latent tensor using a noise schedule and a chosen sampling method, progressively steering it towards a structured latent representation in a number of . This step is the most compute-intensive, consuming roughly 90% of the total processing time. Once sampling completes, the pipeline decodes the latent representation (5) using a VAE model to reconstruct pixel-space information. Finally, it  (6) as an image file, completing the text-to-image generation process.Diffusion Model Core ComponentsThe diffusion model within SDcpp is the principal generative engine, responsible for transforming Gaussian noise into coherent images through a series of denoising . More steps allow finer details and smoother outputs but increase inference time, as each step requires additional computation. This process is governed by the DDPM framework, wherein the model iteratively refines the image by removing noise in a controlled manner. The diffusion model encompasses three primary components:UNet (Denoising Network): the UNet architecture employs an encoder-decoder structure with skip connections, enabling multi-scale feature extraction and reconstruction. The network predicts and removes noise at each denoising step, progressively enhancing image quality.Text Encoder (Conditioning Module): The text encoder converts textual prompts into latent embeddings that guide the denoising process. SDcpp supports multiple text encoders, including CLIP-L, CLIP-G, and T5XXL, each tailored for different model versions and memory considerations.Scheduler (Noise Controller): The scheduler orchestrates the noise removal strategy, dictating the temporal progression of denoising steps. It determines the rate and manner in which noise is reduced, balancing computational efficiency and image fidelity.A recent model,  , is optimized for low-resource environments, featuring a compact UNet architecture that accelerates inference by reducing the number of denoising steps. Flux.1 is designed for memory-efficient quantization and compatibility with the GGUF format, making it suitable for deployment on lightweight hardware. It reduces inference steps to only 4 steps and can achieve fast inference.Variational Autoencoder (VAE)VAEs facilitating the encoding of images into a lower-dimensional latent space. This reduction significantly diminishes computational and memory overhead while retaining high-resolution image details. Latent space is a compact mathematical representation of an image. Think of it as a highly efficient shorthand where only the most essential details are stored. Instead of storing every pixel, the image is encoded as numbers that describe its features. This makes it possible to work with images at a much lower computational cost. Stable Diffusion doesn’t generate images pixel by pixel. Instead, it creates images in latent space, where information is stored in a more compact and manageable way. Once the model finishes generating an image in this space, the VAE  it back into a high-resolution format that we can see.Different VAEs can influence how well the final image looks. A generic VAE works for most images, but a model trained for a specific style, like anime or realistic photography, may use a custom VAE to better match its training data. A poorly matched VAE can cause color shifts, blurriness, or unwanted artifacts.Technically, any model can use any VAE, but results vary. A VAE trained on cartoon-style images won’t reconstruct a photorealistic image correctly. This is why some models provide their own VAEs — they have been optimized to maintain details that matter most for a specific style.LoRA (Low-Rank Adaptation)LoRA facilitates model customization without the need to adjust the entire diffusion model. By modifying only a small subset of parameters (~10 million), LoRA significantly reduces memory usage compared to full model fine-tuning. Multiple LoRAs can be combined with weighted influences on the final image, enabling diverse stylistic and feature modifications. For instance, combining two LoRAs with different weights can be executed using the syntax <lora:styleA:0.6> <lora:styleB:0.8>. To optimize performance, it is advisable to minimize the number and strength of LoRAs, especially in memory-limited environments.Training and Inference File FormatsThe  is the standard PyTorch checkpoint format, that stores full precision weights and optimizer states. Supports training and fine-tuning but can execute arbitrary Python code, posing security risks. SafeTensors (.safetensors) is a safer alternative to CKPT, with structured, immutable storage and no executable code. Loads faster but still used mainly for training.GGUF (GPT-Generated Unified Format) is a binary model format introduced by llama.cpp, succeeding GGML. Initially designed for language models (like LLaMA models), GGUF has been adopted in diffusion models to enable efficient, quantized inference without reformatting model files into proprietary formats (e.g., .ggml or .gguf conversions required previously). Traditional formats (.safetensors, .ckpt, .bin) store full-precision FP16/FP32 model weights, consuming significant VRAM. GGUF introduces quantization at multiple levels, storing models in 2-bit to 16-bit precision while maintaining compatibility with SDcpp.Use GGUF for efficient inference on low-resource devices, prefer .safetensors for training.In our experiments,  demonstrates an extremely fast denoising process, with images at 1 and 2 steps appearing nearly identical, suggesting that most structural information is determined in the very first iterations. By 4 steps, images diverge significantly, and by 6 steps, they change even more, indicating that additional steps introduce large variations rather than just refining details. Interestingly, Euler and Euler_a produce the best results, likely because they follow a balanced noise reduction trajectory, allowing structure to form naturally without over-smoothing or excessive deviations. Euler introduces noticeable changes between 3, 4, and 6 steps, including shifts in pose, face, and hair color, implying that it follows a latent space trajectory where small step changes can drastically impact the final result. Euler_a, being an ancestral sampler, adds controlled randomness at each step, further amplifying these changes. In contrast, LCM performs poorly with Flux 1, excessively blurring images, likely due to its tuning for models that require more iterative refinement, making it unsuitable for Flux 1’s fast-converging architecture. We observed that 4 steps often produce better images than 5 or 6 steps, as extra iterations introduce unnecessary alterations instead of improving details, reinforcing that Flux 1 is optimized for extremely low-step inference. This is a stark contrast to Stable Diffusion models like SD 1.5 or SDXL, which typically require 20–50 steps for high-quality outputs. These findings highlight that Flux 1’s unique training favors fewer steps with the right sampler, making Euler and Euler_a better choices at around 4 steps, while models like LCM fail due to excessive denoising.Denoising Schedules and Sampling IntegrationDenoising schedules define a series of noise levels that guide reverse diffusion. These schedules set discrete or continuous noise decay steps, essentially parameterizing the stochastic differential equation used to reverse the diffusion process. Discrete schedules apply abrupt noise transitions, forcing the model to rapidly shift between latent states. In contrast, smooth schedules such as karras or exponential distribute noise reduction evenly, supporting more accurate latent updates.Sampling methods numerically integrate these latent trajectories. Euler-type samplers update the latent state with fixed time step approximations, benefiting from stable transitions when paired with smooth noise decay. Latent Consistency Models (LCM) depend on gradual noise reduction; abrupt schedules can induce over-smoothing and detail loss. Our observations indicate that matching a sampling method’s sensitivity to noise decay with an appropriate denoising schedule is key to preserving image structure while ensuring efficient inference.Flux.1’s behavior indicates that its training regime emphasizes rapid structure formation in early steps. Euler’s robustness across discrete, exponential, and karras schedules suggests that its integration of abrupt noise transitions still aligns well with Flux.1’s latent dynamics. LCM, being more sensitive to the noise decay, only works well with smoother schedules (exponential and karras) that provide gradual latent updates.The failures with AYS and GITS imply that these schedules impose noise trajectories incompatible with Flux.1’s fast-convergence design. Their latent paths likely diverge from the model’s learned reverse diffusion trajectory, causing the state to collapse into uniform or noisy outputs rather than structured images. This sensitivity reinforces that Flux.1 is optimized for few-step denoising with specific noise decay characteristics, and departures from that — especially with schedules that enforce alternative trajectory regularity — can disrupt the image generation process.Flux.1 departs from prior diffusion models by leveraging a T5-style text encoder instead of CLIP, allowing it to process full sentences fluently rather than relying on discrete token hierarchies. Unlike Stable Diffusion, where Booru-style tags and keyword emphasis are required for precision, Flux.1 understands natural language natively, reducing reliance on rigid token structures. Long, descriptive prompts yield better scene coherence, material accuracy, and relational awareness than fragmented keyword lists.Trigger words are optional rather than mandatory, as the model activates concepts contextually rather than requiring explicit token matching. However, structured elements — short modifiers or category cues — can refine specificity without overpowering Flux.1’s broader language comprehension. The model is particularly strong at multi-concept blending, which traditionally caused token interference in CLIP-based architectures. Negative prompting is also more effective, reducing the unintended blending of attributes.Flux.1 performs best with contextual and relational descriptions, leveraging semantic depth over isolated tokens. While Booru tags still function, hybrid prompting — mixing structured elements with full sentences — offers the best balance between control and generative adaptability.LoRA functions differently in Flux.1 compared to Stable Diffusion. Since Flux.1 uses a T5-based encoder, LoRA activation does not require fixed trigger words but instead responds dynamically to natural descriptions. This allows LoRAs to integrate seamlessly into prompts without rigid dependencies.Key parameters such as network dimension, alpha scaling, and captioning density influence performance. Higher network dimension strengthens adaptation but risks overfitting, while alpha scaling balances LoRA integration with the base model. Flux.1’s LoRAs benefit from mixed captioning strategies — varying between short tags and natural descriptions — ensuring activation without over-constraining the generative process. Unlike SDXL, which often requires explicit weight scaling (1.2–1.5x) to activate LoRAs, Flux.1 adapts LoRA strength more organically within context, reducing the need for manual weight adjustments.To maximize LoRA performance in Flux.1, descriptive phrasing should replace rigid token triggers, using context-driven prompts to activate learned modifications naturally. Captions should balance specificity with generalization, ensuring LoRAs enhance rather than dominate the generative output.Optimization Strategies for Low-Resource InferenceQuantization is a pivotal technique in reducing model size and memory usage by lowering the precision of model weights. SDcpp supports various quantization levels, each offering a different balance between memory efficiency and image quality: Reduces memory usage by approximately 75% compared to full precision (FP16) while maintaining good image quality. It introduces a slight computational overhead due to dequantization but is optimal for low-end hardware. Offers slightly better precision than Q4 at the cost of requiring twice the memory. It is suitable for systems with more VRAM where higher image fidelity is desired.For lower-end hardware, Q4_K provides a good balance of speed and quality, while  is better for systems with more VRAM (>16GB).Quantization Impact of Model PartsWe’re loading a model, vae, clip, and t5, and can choose quantized parts for each. The model itself must be quantized due to memory constraints, but the VAE, CLIP, and T5 can remain in higher precision. We tested a q4_K model with safetensors in higher precision, and a fully quantized version where all parts were q4_0.We observed a 2.5x slowdown with the fully quantized run, despite reduced memory usage. Likely due to increased dequantization overhead, precision mismatch-induced stalls, and reduced SIMD efficiency on the Apple M2. While background processes may have contributed to higher involuntary context switches (3.3M vs. 1.48M), they do not fully explain the slowdown. The quantization format (q4_K vs. q4_0) influences computational efficiency rather than raw inference speed, with q4_K employing grouped quantization, which improves data locality and reduces dequantization frequency, while q4_0 likely applies simpler per-tensor quantization, increasing compute overhead. The hybrid approach—quantizing only the diffusion model while keeping VAE, CLIP, and T5 in higher precision—seems to better utilize hardware by avoiding unnecessary precision conversion costs and leveraging FP computation optimizations. These results suggest that full quantization may degrade performance when not optimized for architectural strengths, particularly on FP-optimized hardware like the M2.The sampling method determines the strategy by which noise is removed during the denoising process. Ancestral samplers introduce random variations at each step, leading to more diverse outputs, while fixed-step samplers follow a deterministic trajectory, yielding consistent results.SDcpp integrates several sampling algorithms optimized for different performance criteria: A simple and fast method effective with a lower number of denoising steps, suitable for rapid inference. Prioritizes image quality, requiring more steps and memory. It is ideal for applications where high detail is essential.Latent Consistency Models (LCM): Optimizes speed by drastically reducing the number of required denoising steps (e.g., 4–8 instead of 20–30), making it highly suitable for low-end systems. Users can specify the sampling method using the --sampling-method flag, such as --sampling-method lcm.Memory management is critical in ensuring that diffusion models run efficiently on constrained hardware. SDcpp employs several techniques to optimize memory usage: By processing the VAE decoder in smaller, manageable segments, VAE tiling minimizes peak memory usage. Flash Attention optimizes memory allocation during UNet computations. On CUDA-enabled GPUs, it offers significant memory savings and speed improvements. Flash Attention can be activated via the --diffusion-fa flag.In our experiments on an Apple M2, enabling --vae-tiling reduced memory usage but increased execution time, suggesting a trade-off between efficiency and speed. When --diffusion-fa (Flash Attention) was enabled, execution time did not significantly improve. The combination of both options (vae-tiling and diffusion-fa) resulted in the  but also the , confirming that while these settings help on low-VRAM devices, they introduce performance overhead. The fastest run occurred with both options disabled, though it had the highest memory footprint.Image resolution directly impacts both memory consumption and inference speed. Generating images at a resolution of 1280×640 requires approximately 2.5 times the memory of generating images at 512×512. To accommodate low-resource environments, users can opt for intermediate resolutions such as 768×384 or 1024×512, balancing image quality and computational efficiency.LoRA Delay and Conditioning Graph RecalculationWhen LoRA modules are applied, the conditioning graph must be recalculated due to LoRA’s modification of internal activations, which can reintroduce dependencies on structured tagging if the LoRA was trained on datasets requiring them. The conditioner parses and reweights prompt segments dynamically, adjusting token influence based on LoRA-induced shifts in the embedding space, ensuring compatibility between the altered diffusion model state and the input text. This recalibration process accounts for the delay seen post-LoRA application and before sampling, as the system must harmonize LoRA-induced parameter shifts with Flux.1’s inherently context-aware text encoding.Classifier-Free Guidance ScaleCFG scale (Classifier-Free Guidance Scale) can be set with the --cfg-scale argument. It defaults to 7.0 and controls how strictly the model follows the text prompt, balancing adherence and creativity. Higher values (e.g., 10-15) force the model to generate images that closely match the prompt but can introduce artifacts, while lower values (e.g., 1-5) allow more randomness and artistic interpretation. It impacts performance slightly because higher CFG values increase the weight difference between conditional (prompted) and unconditional (random) predictions, requiring additional computation to balance them. However, the effect on inference speed is minor compared to factors like resolution, model architecture, and sampling steps.Flux.1 seem to have been trained with  as target setting, meaning its internal weighting and contrast are optimized for that value. Lowering CFG to  reduces guidance too much, allowing excessive noise influence, resulting in darker, underexposed images. Raising it to 2.0 strengthens prompt adherence but overemphasizes highlights and contrast, making images appear overly bright. Unlike standard Stable Diffusion, where 7.0 is the default for balanced outputs, Flux.1’s tuning makes , with deviations causing unintended brightness shifts.To analyze the impact of CFG scale on image generation, we generated eight images using CFG 1.0 and 2.0 across the Euler, LCM, iPNDM, and Euler_a samplers with 4 steps and a fixed seed of 42, using the prompt “a beautiful girl in futuristic Tokyo, neon lights,” comparing how different guidance levels influence brightness, contrast, and prompt adherence.Memory-Efficient InferenceRunning Flux.1 (schnell) on an Apple M2 (16GB RAM), we achieve inference without a GPU, completing a 256×256 image in 216.3s with 9.28GB RAM usage. The UNet required just 96.59MB, and VAE decoding used 416MB, demonstrating the efficiency of quantized GGUF models.Our analysis of Stable-Diffusion.cpp and Flux.1 demonstrates that efficient local diffusion inference is feasible even on low-resource hardware, provided that model selection, quantization, and sampling strategies are optimized. Flux.1’s extremely low-step denoising makes it uniquely suited for fast inference, especially when paired with Euler and Euler_a samplers, which balance structure formation and controlled randomness. In contrast, models like LCM introduce excessive smoothing, making them unsuitable for Flux.1’s fast convergence.We also observed that 4 steps yield optimal results with Flux.1, as additional iterations introduce unnecessary alterations rather than improving image fidelity. While our results indicate that full quantization introduces dequantization overhead and computational stalls, external factors such as background system load may have influenced the extent of the observed slowdown. Memory-efficient techniques such as VAE tiling and Flash Attention can reduce hardware requirements but may introduce performance trade-offs. Users should select Q4_K or similar quantization for lower-end hardware, as grouped quantization schemes appear to mitigate some of the performance penalties associated with fully quantized inference.These findings highlight that with the right model architecture, quantization, and sampler choice, high-quality image generation is possible on constrained devices without relying on high-end GPUs. Future improvements may focus on further refining model architectures like Flux.1 and optimizing quantization methods for even faster, memory-efficient inference.]]></content:encoded></item><item><title>The ups and downs of stratified cross-validation</title><link>https://www.youtube.com/watch?v=EpK9pWZGjbE</link><author>probabl</author><category>dev</category><category>ai</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/EpK9pWZGjbE?version=3" length="" type=""/><pubDate>Thu, 30 Jan 2025 11:48:46 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Probabl</source><content:encoded><![CDATA[When you use GridSearchCV in scikit-learn will do a cross-validated hyperparameter search. This is neat, but there are some worthwhile details involved when it comes to selection the cross-validation segments.

Link to discussion on Github:
https://github.com/scikit-learn/scikit-learn/issues/4757#issuecomment-2419933195

Website: https://probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Twitter: https://x.com/probabl_ai
Bluesky: https://bsky.app/profile/probabl.bsky.social
Discord: https://discord.probabl.ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

#probabl]]></content:encoded></item><item><title>NVIDIA’s Agentic AI for Container Security with Amanda Saunders and Allan Enemark</title><link>https://softwareengineeringdaily.com/2025/01/30/nvidias-agentic-ai-for-container-security-with-amanda-saunders-and-allan-enemark/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=nvidias-agentic-ai-for-container-security-with-amanda-saunders-and-allan-enemark</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED6652749715.mp3" length="" type=""/><pubDate>Thu, 30 Jan 2025 10:00:34 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[Docker container vulnerability analysis involves identifying and mitigating security risks within container images. This is done to ensure that containerized applications can be securely deployed. Vulnerability analysis can often be time intensive, which has motivated the use of AI and ML to accelerate the process.Amanda Saunders is the Director of Enterprise Generative AI Software at NVIDIA, and Allan Enemark works on NVIDIA’s Morpheus cybersecurity SDK team.Amanda and Allan join the podcast with Gregor Vand to talk about Blueprints and their application to vulnerability and container security.]]></content:encoded></item><item><title>Windows Exploitation Tricks: Trapping Virtual Memory Access (2025 Update)</title><link>https://googleprojectzero.blogspot.com/2025/01/windows-exploitation-tricks-trapping.html</link><author>Google Project Zero</author><category>Google Project Zero</category><category>infosec</category><pubDate>Thu, 30 Jan 2025 09:57:00 +0000</pubDate><source url="https://googleprojectzero.blogspot.com/">Project Zero</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Windows Bug Class: Accessing Trapped COM Objects with IDispatch</title><link>https://googleprojectzero.blogspot.com/2025/01/windows-bug-class-accessing-trapped-com.html</link><author>Google Project Zero</author><category>Google Project Zero</category><category>infosec</category><pubDate>Thu, 30 Jan 2025 09:57:00 +0000</pubDate><source url="https://googleprojectzero.blogspot.com/">Project Zero</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>#492: Great Tables</title><link>https://talkpython.fm/episodes/show/492/great-tables</link><author></author><category>Talk Python</category><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://talkpython.fm/episodes/download/492/great-tables.mp3" length="" type=""/><pubDate>Thu, 30 Jan 2025 08:00:00 +0000</pubDate><source url="https://talkpython.fm/">Talk Python To Me</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Design YouTube - System Design Interview</title><link>https://blog.algomaster.io/p/design-youtube-system-design-interview</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23342c70-b11d-48c3-b30b-63ea245e5d2a_2026x1322.png" length="" type=""/><pubDate>Thu, 30 Jan 2025 05:45:50 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[With over  monthly active users,  is the second most visited website in the world—trailing only Google.As a , it enables users to upload, watch, and interact with video content, while handling hundreds of millions of daily visitors, managing petabytes of data, and ensuring real-time video delivery across the globe.In this article, we’ll explore the system design of a large-scale video streaming service like YouTube that can accommodate hundreds of millions of daily users and billions of views, all while maintaining low latency and high availability.We’ll walk through every step of the design—from requirements and high-level architecture to database and API design—before diving deep into core use cases. The concepts covered here are equally applicable to other large-scale video platforms such as  and .Before diving into the design, lets outline the functional and non-functional requirements.Users should be able to  video files.Uploaded videos must be  into multiple resolutions (e.g., 240p, 360p, 720p, 1080p) to support different network conditions and devices.Users should be able to  videos in real-time with adaptive bitrate streaming to adjust quality based on network conditions.Users can  for videos by title, tags, or description.Users can  and  on videos.Users should be able to create and subscribe to .Non-Functional Requirements: The system should support millions of concurrent users and thousands of video uploads per minute. Core features like video upload, playback, and search should have minimal downtime. Fast video streaming with minimal buffering and near-instantaneous search results. Video files must be stored reliably, with redundancy mechanisms to prevent data loss due to hardware failures. Optimize storage and bandwidth costs.Daily Active Users (DAU): 10 million ~100,000 videos/dayAverage Videos Watched per User per Day: 5 videos 500 MB. 1 KB.: 100,000 videos / day * 500 MB / video = 50 TB / dayDaily Video Metadata Storage: 100,000 * 1KB = 100MB / dayNetwork Bandwidth Estimation:10 million users × 5 videos/user = 50 million views/day Daily Bandwidth Requirements (without compression & caching) : 50 million views * 500 MB / day = 25 PB / dayGiven the high storage and bandwidth requirements, leveraging  is the most practical approach:Content Delivery Network (CDN): To cache frequently accessed videos closer to users and reduce latency.Blob Storage (e.g., AWS S3): To store video files reliably with redundancy.We can break the architecture of YouTube into two primary components:  – Handles video playback, and delivery.Video Upload & Processing – Manages user uploads, transcoding, and metadata storage.3.1 Video Streaming Architecture]]></content:encoded></item><item><title>Announcing Rust 1.84.1</title><link>https://blog.rust-lang.org/2025/01/30/Rust-1.84.1.html</link><author>The Rust Release Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[The Rust team has published a new point release of Rust, 1.84.1. Rust is a
programming language that is empowering everyone to build reliable and
efficient software.If you have a previous version of Rust installed via rustup, getting Rust
1.84.1 is as easy as:If you don't have it already, you can get  from the
appropriate page on our website.1.84.1 resolves a few regressions introduced in 1.84.0:It also includes several fixes for those building Rust from source:Many people came together to create Rust 1.84.1. We couldn't have done it
without all of you. Thanks!]]></content:encoded></item><item><title>Mini-R1: Reproduce Deepseek R1 „aha moment“ a RL tutorial</title><link>https://www.philschmid.de/mini-deepseek-r1</link><author></author><category>dev</category><category>ai</category><category>blog</category><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><source url="https://www.philschmid.de/">Phil Shmid</source><content:encoded><![CDATA[Reproduce Deepseek R1 „aha moment“ and train an open model using reinforcement learning trying to teach it self-verification and search abilities all on its own to solve the Countdown Game.]]></content:encoded></item><item><title>How I Use AI: Meet My Promptly Hired Model Intern</title><link>http://lucumr.pocoo.org/2025/1/30/how-i-ai</link><author>Armin Ronacher</author><category>dev</category><category>blog</category><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><source url="https://lucumr.pocoo.org/feed.atom">Armin Ronacher Blog</source><content:encoded><![CDATA[After Musk's acquisition of Twitter, many people I respect and follow
moved to Bluesky.  I created an account there and made an honest attempt of
making it my primary platform.  Sadly, I found Bluesky to be surprisingly
hostile towards AI content.  There is an almost religious resistance to AI
on there, at least in whatever corner of the platform I ended up in.Despite these challenges, some individuals on both Twitter and Bluesky
have shown genuine curiosity about my AI usage.  In this post, I want to
share how I use Large Language Models and why I find them so helpful.Before moving on, I want to include an an important disclaimer: I am by no
means an expert in AI; I'm mostly an enthusiastic user.  Absolutely
nothing here is novel!  What I do here is pretty boring which to some
degree is the point.  I won't be covering underlying technology or my
programmatic experience.  This is strictly about how I use AI as a “techy
consumer”.In addition, as you read through this article you will probably figure out
rather quickly that the way I use AI — despite being used in part for
content creation — does not really impact intellectual property much.
That said, I'm curious and open to discussions about how we should be
dealing with this problem.  Particularly on Bluesky a lot of the
negativity towards AI is related to watering down of copyrights and human
creation.  I don't know the answers to these things, but I think we need
to have a productive dialog here rather than wishing for the technology to
go away.In short: AI makes me significantly more productive.  I recently read
Aaron Francis's Tweet about how he gets
a week's worth of productivity out of a day now thanks to AI.  I'm not
anywhere close to that, but I use AI a lot now.  It has become
indispensable for me for both content writing as well as programming.Moreover, a common misconception is that AI is still at the level it was
when ChatGPT first launched.  Many people tried it early, saw its
limitations, and never returned.  However, AI is evolving rapidly. If you
haven't kept up, you risk drawing inaccurate conclusions based on outdated
impressions.  In some sense I believe people who never tried to use AI
yet, are in a better position to judge than the folks who used it two
years ago and never returned.I work with a variety of AI tools, mostly because of professional
curiosity and to a smaller degree because each tool excels at something
slightly different.  Here are the ones I use most often:Open WebUI.  In short this is a Python web
app that offers a chat interface similar to ChatGPT.
Unlike ChatGPT, however, it lets you talk to different models.  First and
foremost, I use this to talk to local models hosted by Ollama, but
secondarily I also use it to interface with other remote services like
OpenAI, Anthropic and DeepSeek.Simon's llm.  This is a command line
tool with plenty of plugins that lets you prompt different models.  Think
of it as a command-line version of Open WebUI.  It's particularly useful
for quick scripting and basic automation.Ollama.  This allows me to run models locally
on my MacBook Pro M1 Max.  With the 64GB of RAM it has, it's a pretty
potent machine for basic inference despite it being three years old.
I'll explain later why I use local models.Cursor.  It is a fork of Visual Studio
Code with AI-based auto completion and code generation built-in.  It's
my go-to for programming with AI assistance at the moment.ChatGPT.  Like probably most AI users, I use
ChatGPT, particularly on my phone.  I pay for the Plus subscription
because I use it enough to get a lot of value out of it.  One
significant use of this for me is in fact the voice mode (more on that
later).It's probably worth mentioning that you can get most of the benefits of this from just paying for a single AI
tool.  I think as one expands their use, as especially as one gets better at writing prompts,
the desire naturally grows to use more tooling.  As for which models
(and services) to use day to day I don't have a particular strong strategy
and preferences change quickly.  For instance after DeepSeek's R1 release,
I started exploring it quite a bit for programming — and it's doing a
phenomenal job at it —  and as of writing that's just a few days old.If you want to run models locally, Apple Silicon machines currently offer
some of the best “bang for your buck” in terms of performance, power
usage, and money.  With tailscale, I can even
access my MacBook's Open WebUI interface from my phone, as long as it is
powered on.One frequent concern I hear is “you cannot trust LLMs” as they tend to
hallucinate.  I get this in particular when I explain that I frequently
use this as a replacement for Google!  However, I approach the risk of
hallucination the same way I would when seeking advice from another human:
people can and are routinely wrong, and you learn to cross-check
selectively.I treat AI as I would a collaborator or a pretty good intern but I remain
responsible for the final outcome.  In this case the intern also happens
to get better month by month as models improve.  And a bit like a human,
that digital intern has morals and wants to be argued with.  Except, of
course, that some of those AI interns don't want to talk about China,
while others get a stroke if you talk about certain people.
But regardless of how good they get, in the end, it's my fault and my
fault alone if I do the wrong thing.  I won't blame the AI and I need to
spot check.However, the logical conclusion of this is not that it's wrong all the time
and you need to check everything, or that you cannot trust it at all.
It's similar to how you engage in a technical discussion with others about
a problem.  I have seen more than one situation where the conventional
wisdom in the room is just wrong for a few minutes, until someone points
out that we had it wrong.Another major advantage is that AI tools are relatively open.  You can run
models locally and integrate them with scripts.  Even the famous OpenAI
which is not at all open is much more open than a Google search is.  For
instance, you can create a simple script for grammar-checking right from
your command line.In other words, you  integrate it locally and nobody stops you.  By
contrast, many, many years ago I had a tool on my computer that allowed me
to issue web searches and extract text from results.  That has stopped
working such a long time ago that I almost forgot about it.  It has
stopped working because there is basically no competition in search, and
Google does not want me to use it like that.For instance, you can create a simple script for grammar checking right
from your command line:phi4:latest
 ping -q -c1 google.com >/dev/nullclaude-3-5-sonnet-latest
cat
llm -m  -s This script can automatically switch between a local model ( via
Ollama) and a remote one () based on internet
connectivity.  With a command like  in Vim, I can fix up
sentences with a single step.Or you can manipulate the contents of the clipboard like this:pbpaste | llm-spell | pbcopy && say "AI is done"
I don't let AI write my articles.  As a non-native Speaker, mistakes and
linguistic quirks are part of me.  That said, I do rely on AI tools for
helping me write.Often, after I have made my first pass through a page, I ask an LLM to
read through it and give me comments.  I have a Apple Note with various
prompts I can use or I just come up with what I need in the moment.  The
process is not particularly scripted out.  I basically talk to the LLM to
figure out what it thinks of the text, etc.Here are some of the things I use AI for when writing: I compare the AI’s suggested revisions side by
side with my original text and pick the changes I prefer. AI often helps me see when my writing is too wordy.
In the days before AI, I often ended up with super long articles that
did not read well and that I did not publish.  Models like o1 are very
helpful in identifying things that don't need to be said.Writing Notes and finding key points: Here, I ask the AI to read
through a draft “like a Computer Science 101 student” and take notes.
This helps me see if what it absorbed matches what I intended to
convey.  I have a few prompts that asks the AI to
“roast” or criticize my article, as if commenting on Reddit, Twitter,
or Hacker News.  Even though these critiques seem shallow, they can
sting, and they often highlight weaknesses in my argument or lack of
clarity.  Even if they don't necessarily impact the writing, they
prime me for some of the feedback I inevitably receive. If I worry there's too much jargon, I use AI to
resolve acronyms and point out technical terms I've used without
explanation, helping me make the text more accessible.This is probably the bulk of what I do and it falls into three major
categories.Debugging and summarizing: Throwing stack traces and errors at an
AI is usually a good first step in finding out where I should look,
what might be wrong.  Sometimes it fixes a problem entirely by itself.
I also sometimes just throw a long bug report or issue at the thing to
try to reason through it. Sometimes you have an output, but you don't
have the input.  As an example I have a little side project to write a
command line tool help generator, and AI is really good at taking the
output, “parsing“ it and creating a structure as input.  That is a
good first step to then generate out the code that should match again
that output. lastly it just programs really well.  It can do
single code snippets, it can generate complex stuff.  Particularly for
prototyping it's amazing.Now it's not great at all languages.  I find it performs exceptionally
well at Python, somewhat okay at JavaScript and horrible at Rust.
However, particularly if I wanna solve an algorithmic problem, I can often
iterate in Python until I have what I want, then ask it to translate it to
Rust.  That's from my experience at least a decent way to get going
quickly on some harder problems.Pair programming is fantastic, but not always possible if you're
programming alone at home.  An LLM can serve as a stand-in discussion
partner — a poor man's interlocutor.  Thankfully it's actually not all
that poor, because it's really good at telling you what stuff is called.
This is most fun if you do it with voice mode, but it also works quite
well in text.Here a motivating example from voice mode: So, do you remember if you have, like, , it always copies from
the start, but if you keep concatenating to the same buffer, it's really
inefficient. So, remember what the Big-O of that is if you do it in a loop,
and wasn't there, like, a humorous name for this kind of algorithm? You’re thinking of the “Schlemiel the Painter’s algorithm.” It’s a
humorous way of describing the inefficiency of repeatedly appending to a
string using operations like , which copies from the start each
time. The Big O complexity for this operation is O(n^2) if done repeatedly
in a loop, since each  operation takes time proportional to the
current length of the string.It's also fun and educational for my kids.  They ask a lot of questions,
and ChatGPT can tailor its explanations to their age level.  If the
explanation is still too advanced, I can immediately ask it to simplify
further.  If it was too dumbed down, I can make it go the other way.  Now
again, there is always a risk that it gets it wrong, but that is okay.
Part of all of this is to convey to the children the idea that we need to
validate it and that being critical is important.What makes ChatGPT so enjoyable here is that it's able to keep the
conversation going — it has state.  Answered in large metric numbers?  Can
always ask it to scale to elephants or soccer fields.ChatGPT is also incredibly helpful when having to work with multiple
languages.  For a recent example, my kids have Greek friends and we tried
to understand the difference between some Greek words that came up.  I
have no idea how to write it, Google translate does not understand my
attempts of pronouncing them either.  However, ChatGPT does.  If I ask it
in voice mode what “pa-me-spee-tee” in Greek means it knows what I tried
to mumble and replies in a helpful manner.Lastly the use on the go.  Sometimes I'm just not in a position where I
can actually write it down on a keyboard, but if I basically talk to
ChatGPT, it can transcribe it down and it will also remove some of my
“um's” and it will just be there and I can copy-paste it later and edit
it.  (To prove a point, I transcribed the previous sentence with the
mobile app).  This is a very different experience than transcribing on iOS
natively.  It understands enough context of what I'm babbling that it will
start and stop transcribing.  Now this is not perfect, but pretty damn
good and incredibly helpful.The multilingual aspect is particularly helpful because our family is
multilingual.  Being able to fluidly switch between German, Russian and
English is such a refreshing experience.I mentioned earlier that I don't let LLMs write my texts.  I also don't
use AI to make illustrations, though I have in the past.  The reason is
that there is a certain style that goes along with these illustrations,
which is just incredibly off-putting.  They are noticeably AI-generated,
and typically quite low-quality.  People have taken to calling these
images “AI slop” and I personally respond really badly to it.  When
someone throws me a pull request, an email or a text message that is
obviously AI-generated without disclosing this, I immediately have a very
low opinion of them.Slop like hallucinations are a problem, but they are only a problem if you
don't use your brain.  Even the worst slop can be the foundation of
really good content.  I'm a horrible artist, but I can use Illustrator.
Even an AI slop image can help me trace the person in the pose I wanted.
Likewise you can throw your notes into a document and let the AI imagine a
story around it.  You probably can't use that story right away, but you
can use it as potential inspiration.AI tools, at their best, feel less like disruptive and dark magic and more
like a natural extension of the creative process as long as you see them
as curious collaborators.  My approach isn't about outsourcing thinking,
but augmenting it: using LLMs to accelerate grunt work, untangle mental
knots, and prototype ideas faster.  Skepticism is healthy, but dismissing
AI outright risks missing its potential as a multiplier for those willing
to engage critically.]]></content:encoded></item><item><title>China’s DeepSeek - A Balanced Overview</title><link>https://www.youtube.com/watch?v=-KK8SuvwoRQ</link><author>ColdFusion</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/-KK8SuvwoRQ?version=3" length="" type=""/><pubDate>Wed, 29 Jan 2025 22:17:22 +0000</pubDate><source url="https://www.youtube.com/channel/UC4QZ_LsYcvcq7qOsOhpAX4A">Cold Fusion</source><content:encoded><![CDATA[Go to https://brilliant.org/coldfusion for 30-day free trial + 20% off!

Accusations of theft by OpenAI, a stock market flash crash and a small AI model from China. This is the story of DeepSeek, the AI model that's changed the game. In this episode we take a deep look into all the news and ramification surrounding its release.

ColdFusion Podcast: 

https://www.youtube.com/@ThroughTheWeb 

Watch or listen to ColdFusion on Spotify: https://open.spotify.com/show/1YEwCKoRz8fEDqheXB6UJ1


ColdFusion Music: 

https://www.youtube.com/@ColdFusionmusic
http://burnwater.bandcamp.com   

Get my book: 

http://bit.ly/NewThinkingbook 

ColdFusion Socials: 

https://discord.gg/coldfusion
https://facebook.com/ColdFusionTV 
https://twitter.com/ColdFusion_TV 
https://instagram.com/coldfusiontv

Created by: Dagogo Altraide
Producers: Tawsif Akkas, Dagogo Altraide 
Writers: 
Editors: 
Animator:]]></content:encoded></item><item><title>DeepSeek stole our tech... says OpenAI</title><link>https://www.youtube.com/watch?v=hpwoGjpYygI</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/hpwoGjpYygI?version=3" length="" type=""/><pubDate>Wed, 29 Jan 2025 20:47:22 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Build better apps with PostHog https://posthog.com/fireship

OpenAI and AI Czar David Sacks accuse DeepSeek of stealing their IP to train their new R1 model, citing hard evidence gained from Microsoft. In addition, Alibaba just released Qwen 2.5 with impressive benchmark results. 

#tech #ai #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Source https://www.ft.com/content/a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6
Qwen 2.5 on GitHub https://github.com/QwenLM/Qwen2.5
DeepSeek Ollama Tutorial https://youtu.be/clJCDHml2cA
DeepSeek Crashes Stocks https://youtu.be/Nl7aCUsWykg

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Qwen 2.5 vs DeepSeek
- Deepseek accused of IP theft by OpenAI
- Latest trends in artificial intelligence
- Chinese AI development boom explained 
- DeepSeek Janus image diffusion model]]></content:encoded></item><item><title>Carving Out Your Competitive Advantage With AI</title><link>https://www.topbots.com/competitive-advantage-with-ai/</link><author>Janna Lipenkova</author><category>dev</category><category>ai</category><enclosure url="https://www.topbots.com/wp-content/uploads/2025/01/figure5.png" length="" type="text/html"/><pubDate>Wed, 29 Jan 2025 18:42:50 +0000</pubDate><source url="https://www.topbots.com/">TopBots</source><content:encoded><![CDATA[When I talk to corporate customers, there is often this idea that AI, while powerful, won’t give any company a lasting competitive edge. After all, over the past two years, large-scale LLMs have become a commodity for everyone. I’ve been thinking a lot about how companies can shape a competitive advantage using AI, and a recent article in the Harvard Business Review (AI Won’t Give You a New Sustainable Advantage) inspired me to organize my thoughts around the topic.Indeed, maybe one day, when businesses and markets are driven by the invisible hand of AI, the equal-opportunity hypothesis might ring true. But until then, there are so many ways — big and small — for companies to differentiate themselves using AI. I like to think of it as a complex ingredient in your business recipe — the success of the final dish depends on the cook who is making it. The magic lies in how you combine AI craft with strategy, design, and execution.In this article, I’ll focus on real-life business applications of AI and explore their key sources of competitive advantage. As we’ll see, successful AI integration goes far beyond technology, and certainly beyond having the trendiest LLM at work. It’s about finding AI’s unique sweet spot in your organization, making critical design decisions, and aligning a variety of stakeholders around the optimal design, deployment, and usage of your AI systems. In the following, I will illustrate this using the mental model we developed to structure our thinking about AI projects (cf. this article for an in-depth introduction).AI is often used to automate existing tasks, but the more space you allow for creativity and innovation when selecting your AI use cases, the more likely they will result in a competitive advantage. You should also prioritize the unique needs and strengths of your company when evaluating opportunities.Identifying use cases with differentiation potentialWhen we brainstorm AI use cases with customers, 90% of them typically fall into one of four buckets — productivity, improvement, personalization, and innovation. Let’s take the example of an airline business to illustrate some opportunities across these categories:Of course, the first branch — productivity and automation — looks like the low-hanging fruit. It is the easiest one to implement, and automating boring routine tasks has an undeniable efficiency benefit. However, if you’re limiting your use of AI to basic automation, don’t be surprised when your competitors do the same. In our experience, strategic advantage is built up in the other branches. Companies that take the time to figure out how AI can help them offer something different, not just faster or cheaper, are the ones that see long-term results.As an example, let’s look at a project we recently implemented with the Lufthansa Group. The company wanted to systematize and speed up its innovation processes. We developed an AI tool that acts as a giant sensor into the airline market, monitoring competitors, trends, and the overall market context. Based on this broad information, the tool now provides tailored innovation recommendations for Lufthansa. There are several aspects that cannot be easily imitated by potential competitors, and certainly not by just using a bigger AI model:Understanding which information exactly is needed to make decisions about new innovation initiativesBlending public data with unique company-specific knowledgeEducating users at company scale on the right usage of the data in their assessment of new innovation initiativesAll of this is novel know-how that was developed in tight cooperation between industry experts, practitioners, and a specialized AI team, involving lots of discovery, design decisions, and stakeholder alignment. If you get all of these aspects right, I believe you are on a good path toward creating a sustainable and defensible advantage with AI.Finding your unique sweet spot for value creationValue creation with AI is a highly individual affair. I recently experienced this firsthand when I challenged myself to build and launch an end-to-end AI app on my own. I’m comfortable with Python and don’t massively benefit from AI help there, but other stuff like frontend? Not really my home turf. In this situation, AI-powered code generation worked like a charm. It felt like flowing through an effortless no-code tool, while having all the versatility of the underlying — and unfamiliar — programming languages under my fingertips. This was my very own, personal sweet spot — using AI where it unlocks value I wouldn’t otherwise tap into, and sparing a frontend developer on the way. Most other people would not get so much value out of this case:A professional front-end developer would not see such a drastic increase in speed.A person without programming experience would hardly ever get to the finish line. You must understand how programming works to correctly prompt an AI model and integrate its outputs.While this is a personal example, the same principle applies at the corporate level. For good or for bad, most companies have some notion of strategy and core competence driving their business. The secret is about finding the right place for AI in that equation — a place where it will complement and amplify the existing skills.Data is the fuel for any AI system. Here, success comes from curating high-quality, focused datasets and continuously adapting them to evolving needs. By blending AI with your unique expertise and treating data as a dynamic resource, you can transform information into long-term strategic value.Managing knowledge and domain expertiseTo illustrate the importance of proper knowledge management, let’s do a thought experiment and travel to the 16th century. Antonio and Bartolomeo are the best shoemakers in Florence (which means they’re probably the best in the world). Antonio’s family has meticulously recorded their craft for generations, with shelves of notes on leather treatments, perfect fits, and small adjustments learned from years of experience. On the other hand, Bartolomeo’s family has kept their secrets more closely guarded. They don’t write anything down; their shoemaking expertise has been passed down verbally, from father to son.Now, a visionary named Leonardo comes along, offering both families a groundbreaking technology that can automate their whole shoemaking business — if it can learn from their data. Antonio comes with his wagon of detailed documentation, and the technology can directly learn from those centuries of know-how. Bartolomeo is in trouble — without written records, there’s nothing explicit for the AI to chew on. His family’s expertise is trapped in oral tradition, intuition, and muscle memory. Should he try to write all of it down now — is it even possible, given that most of his work is governed intuitively? Or should he just let it be and go on with his manual business-as-usual? Succumbing to inertia and uncertainty, he goes for the latter option, while Antonio’s business strives and grows with the help of the new technology. Freed from daily routine tasks, he can get creative and invent new ways to make and improve shoes.Beyond explicit documentation, valuable domain expertise is also hidden across other data assets such as transactional data, customer interactions, and market insights. AI thrives on this kind of information, extracting meaning and patterns that would otherwise go unnoticed by humans.Data doesn’t need to be big — on the contrary, today, big often means noisy. What’s critical is the quality of the data you’re feeding into your AI system. As models become more sample-efficient — i.e., able to learn from smaller, more focused datasets — the kind of data you use is far more important than how much of it you have.In my experience, the companies that succeed with AI treat their data — be it for training, fine-tuning, or evaluation — like a craft. They don’t just gather information passively; they  and edit it, refining and selecting data that reflects a deep understanding of their specific industry. This careful approach gives their AI sharper insights and a more nuanced understanding than any competitor using a generic dataset. I’ve seen firsthand how even small improvements in data quality can lead to significant leaps in AI performance.Capturing the dynamics with the data flywheelData needs to evolve along with the real world. That’s where DataOps comes in, ensuring data is continuously adapted and doesn’t drift apart from reality. The most successful companies understand this and regularly update their datasets to reflect changing environments and market dynamics. A power mechanism to achieve this is the . The more your AI generates insights, the better your data becomes, creating a self-reinforcing feedback loop because users will come back to your system more often. With every cycle, your data sharpens and your AI improves, building an advantage that competitors will struggle to match. To kick off the data flywheel, your system needs to demonstrate some initial value to start with — and then, you can bake in some additional incentives to nudge your users into using your system on a regular basis.Now, let’s dive into the “intelligence” component. This component isn’t just about AI models in isolation — it’s about how you integrate them into larger intelligent systems. Big Tech is working hard to make us believe that AI success hinges on the use of massive LLMs such as the GPT models. Good for them — bad for those of us who want to use AI in real-life applications. Overrelying on these heavyweights can bloat your system and quickly become a costly liability, while smart system design and tailored models are important sources for differentiation and competitive advantage.Toward customization and efficiencyMainstream LLMs are generalists. Like high-school graduates, they have a mediocre-to-decent performance across a wide range of tasks. However, in business, decent isn’t enough. You need to send your AI model to university so it can specialize, respond to your specific business needs, and excel in your domain. This is where fine-tuning comes into play. However, it’s important to recognize that mainstream LLMs, while powerful, can quickly become slow and expensive if not managed efficiently. As Big Tech boasts about larger model sizes and longer context windows — i.e., how much information you can feed into one prompt — smart tech is quietly moving towards efficiency. Techniques like prompt compression reduce prompt size, making interactions faster and more cost-effective. Small language models (SLMs) are another trend (Figure 4). With up to a couple of billions of parameters, they allow companies to safely deploy task- and domain-specific intelligence on their internal infrastructure (Anacode).But before fine-tuning an LLM, ask yourself whether generative AI is even the right solution for your specific challenge. In many cases, predictive AI models — those that focus on forecasting outcomes rather than generating content — are more effective, cheaper, and easier to defend from a competitive standpoint. And while this might sound like old news, most of AI value creation in businesses actually happens with predictive AI.Crafting compound AI systemsAI models don’t operate in isolation. Just as the human brain consists of multiple regions, each responsible for specific capabilities like reasoning, vision, and language, a truly intelligent AI system often involves multiple components. This is also called a “compound AI system” (BAIR). Compound systems can accommodate different models, databases, and software tools and allow you to optimize for cost and transparency. They also enable faster iteration and extension — modular components are easier to test and rearrange than a huge monolithic LLM.Take, for example, a customer service automation system for an SME. In its basic form — calling a commercial LLM — such a setup might cost you a significant amount — let’s say $21k/month for a “vanilla” system. This cost can easily scare away an SME, and they will not touch the opportunity at all. However, with careful engineering, optimization, and the integration of multiple models, the costs can be reduced by as much as 98% (FrugalGPT). Yes, you read it right, that’s  — a staggering difference, putting a company with stronger AI and engineering skills at a clear advantage. At the moment, most businesses are not leveraging these advanced techniques, and we can only imagine how much there is yet to optimize in their AI usage.Generative AI isn’t the finish lineWhile generative AI has captured everyone’s imagination with its ability to produce content, the real future of AI lies in reasoning and problem-solving. Unlike content generation, reasoning is nonlinear — it involves skills like abstraction and generalization which generative AI models aren’t trained for.AI systems of the future will need to handle complex, multi-step activities that go far beyond what current generative models can do. We’re already seeing early demonstrations of AI’s reasoning capabilities, whether through language-based emulations or engineered add-ons. However, the limitations are apparent — past a certain threshold of complexity, these models start to hallucinate. Companies that invest in crafting AI systems designed to handle these complex, iterative processes will have a major head start. These companies will thrive as AI moves beyond its current generative phase and into a new era of smart, modular, and reasoning-driven systems.User experience is the channel through which you can deliver the value of AI to users. It should smoothly transport the benefits users need to speed up and perfect their workflows, while inherent AI risks and issues such as erroneous outputs need to be filtered or mitigated.Optimizing on the strengths of humans and AIIn most real-world scenarios, AI alone can’t achieve full automation. For example, at my company Equintel, we use AI to assist in the ESG reporting process, which involves multiple layers of analysis and decision-making. While AI excels at large-scale data processing, there are many subtasks that demand human judgment, creativity, and expertise. An ergonomic system design reflects this labor distribution, relieving humans from tedious data routines and giving them the space to focus on their strengths.This strength-based approach also alleviates common fears of job replacement. When employees are empowered to focus on tasks where their skills shine, they’re more likely to view AI as a supporting tool, not a competitor. This fosters a win-win situation where both humans and AI thrive by working together.Every AI model has an inherent failure rate. Whether generative AI hallucinations or incorrect outputs from predictive models, mistakes happen and accumulate into the dreaded “last-mile problem.” Even if your AI system performs well 90% of the time, a small error rate can quickly become a showstopper if users overtrust the system and don’t address its errors.Consider a bank using AI for fraud detection. If the AI fails to flag a fraudulent transaction and the user doesn’t catch it, the resulting loss could be significant — let’s say $500,000 siphoned from a compromised account. Without proper trust calibration, users might lack the tools or alerts to question the AI’s decision, allowing fraud to go unnoticed.Now, imagine another bank using the same system but with proper trust calibration in place. When the AI is uncertain about a transaction, it flags it for review, even if it doesn’t outright classify it as fraud. This additional layer of trust calibration encourages the user to investigate further, potentially catching fraud that would have slipped through. In this scenario, the bank could avoid the $500,000 loss. Multiply that across multiple transactions, and the savings — along with improved security and customer trust — are substantial.Success with AI requires more than just adopting the latest technologies — it’s about identifying and nurturing the individual sweet spots where AI can drive the most value for your business. This involves:Pinpointing the areas where AI can create a significant impact.Aligning a top-tier team of engineers, domain experts, and business stakeholders to design AI systems that meet these needs.Ensuring effective AI adoption by educating users on how to maximize its benefits.Finally, I believe we are moving into a time when the notion of competitive advantage itself is shaken up. While in the past, competing was all about maximizing profitability, today, businesses are expected to balance financial gains with sustainability, which adds a new layer of complexity. AI has the potential to help companies not only optimize their operations but also move toward more sustainable practices. Imagine AI helping to reduce plastic waste, streamline shared economy models, or support other initiatives that make the world a better place. The real power of AI lies not just in efficiency but in the potential it offers us to reshape whole industries and drive both profit and positive social impact.Note: Unless noted otherwise, all images are the author’s.This article was originally published on Towards Data Science and re-published to TOPBOTS with permission from the author.Enjoy this article? Sign up for more AI updates.We’ll let you know when we release more articles like this one.]]></content:encoded></item><item><title>Cybersecurity researchers: Digital detectives in a connected world</title><link>https://github.blog/security/vulnerability-research/cybersecurity-researchers-digital-detectives-in-a-connected-world/</link><author>Nancy Gariché</author><category>official</category><pubDate>Wed, 29 Jan 2025 17:00:31 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[Have you ever considered yourself a detective at heart? Cybersecurity researchers are digital detectives, uncovering vulnerabilities before malicious actors exploit them. To succeed, they adopt the mindset of an attacker, thinking creatively to predict and outmaneuver threats. Their expertise ensures the internet remains a safer place for everyone.If you love technology, solving puzzles, and making a difference, this might be the perfect career—or pivot—for you. This blog will guide you through the fascinating world of security research, how to get started, and how to thrive in this rapidly changing field.What is a security researcher?Security researchers investigate systems with the mindset of an attacker to uncover vulnerabilities before they can be exploited. They test for weaknesses and design robust security measures to protect against cyber threats.But their work doesn’t stop at identifying problems. Security researchers work with developers, system administrators, and open source maintainers to report and fix problems. They protect essential data and ensure digital infrastructure is robust against new threats.Types of security researchSecurity researchers often specialize in areas such as: Finding and fixing software vulnerabilities. Working closely with developers to build secure applications.   Analyzing and improving encryption methods to protect data. Testing protocols for flaws.   Designing protections to secure networks and identifying potential threats.  Operating system security: Strengthening operating systems to resist attacks. Developing new security measures or refining existing ones.   Taking apart software or hardware to understand how it works and find weaknesses.Why security researchers matter: Real-life impactsUnderstanding the significance of cybersecurity researchers requires looking at their impact through real-world examples.A notable example is the  identified in 2021 in the Log4j logging framework. Security researchers played a key role in uncovering this issue, which had the potential to allow attackers to remotely execute code and compromise systems globally. Thanks to their swift action and collaboration with the community, patches were developed and shared before attackers could widely exploit the vulnerability. This effort highlights the researchers’ vital role in safeguarding systems.Similarly, in 2023, security researchers discovered a zero-day vulnerability in the l, identifying the issue before it could be exploited on a large scale. The flaw had the potential to allow unauthorized access to file transfer systems, which could have resulted in data breaches. By proactively identifying the vulnerability and working with vendors to develop timely patches, these researchers helped secure critical systems and prevent potential breaches.These examples show that security researchers don’t just protect systems—they protect people and organizations. This makes their work not just important but crucial in the digital age. Their efforts save businesses, governments, and individuals from devastating cyberattacks, giving their work a deep sense of purpose.What makes a great security researcher?The essence of a great security researcher lies in a blend of traits and skills. An inherent  and  for security are what drives them. This isn’t just about loving technology; it’s about being captivated by the intricacies of how systems can be manipulated or secured. This curiosity leads to continuous learning and exploration, pushing the boundaries of what’s known to uncover what’s hidden. is another important part of security research. Security research involves solving complex puzzles where understanding how to break something can often lead to knowing how to fix it.  is equally crucial. The best researchers think outside the box, finding innovative ways to secure systems or expose weaknesses that conventional methods might miss. is paramount in this field, where a single oversight can lead to significant vulnerabilities.  rules guide their work. They make sure they use their skills to help security, not for personal gain or harm. is necessary due to the ever-changing landscape of cyber threats. Researchers must stay updated with new technologies and attack methods, always learning to keep ahead of malicious actors. Finally,  is what lets them look deep into systems, finding weaknesses that might be hidden or deeply buried.The journey can be long and arduous, but their determination leads to breakthroughs.Forget the traditional path—focus on skillsOne of the most inspiring aspects of security research is that it’s a field that welcomes diverse backgrounds. While degrees and certifications offer structured learning, they’re not required to succeed. Many top researchers come from eclectic paths and thrive because of their creativity and practical experience.This diversity shows that formal qualifications aren’t always needed. What matters most is your ability to find real vulnerabilities and solve complex problems.Many breakthroughs in security research come from someone noticing something unusual and investigating it deeply. Take the XZ Utils backdoor, discovered by a Microsoft employee who uncovered a hidden vulnerability while troubleshooting slow SSH connections. Similarly, the Sony BMG rootkit scandal came to light because someone dug deeper into unexpected behavior. These examples highlight how curiosity, observation, and persistence often lead to significant discoveries.This investigative mindset is central to security research, but it needs to be paired with practical skills to uncover and mitigate vulnerabilities effectively. So, how can you get started? By building the essential skills that form the foundation of a successful security researcher.How to build these skills Use security tools like OWASP ZAP, Burp Suite Community Edition, and Ghidra to develop practical skills. Experiment in safe test environments, such as intentionally vulnerable applications or local test setups, where you can break systems and learn how to fix them. Try fuzzing with tools like AFL++ to uncover hidden vulnerabilities and strengthen software. Understand how malicious actors exploit systems. This mindset sharpens your ability to spot vulnerabilities, predict potential exploits, and design effective defenses.Develop programming skills: Practice writing secure, efficient code in the language of your choice. Contribute to open source projects or join hackathons to enhance your skills and gain experience.Understand vulnerabilities: Study common issues like SQL injection, cross-site scripting (XSS), and other frequent weaknesses, such as those on the Top 25 CWE Weaknesses List. Use tools like CodeQL to analyze, exploit, and mitigate vulnerabilities effectively.Gain practical experience:Join bug bounty platforms like HackerOne or Bugcrowd to test your skills on systems in the wild.  Intern in IT security or vulnerability assessment roles to gain professional experience.  Hunt for and fix bugs in your favorite open source project. Attend conferences, forums, and local meetups to connect with like-minded professionals. Exchange knowledge, find mentors, and stay updated on the latest trends and tools in cybersecurity.For those transitioning into security researchWhile building experience and networking are essential for all researchers, they’re especially valuable for those transitioning into cybersecurity research. If you’re considering a shift, here’s how to leverage your existing skills and make the leap without starting over.If you’re currently employed, you can begin your journey by leveraging opportunities in your current role:Identify security-related tasks: Developers can use secure coding practices or conduct code reviews. IT admins might audit network configurations or manage firewalls. Analysts can assess data for anomalies that could indicate breaches.  Support security projects: Help with projects like making scripts to check for weaknesses or holding Red Team/Blue Team exercises.  Collaborate with your company’s security team: Assist with vulnerability scans, penetration testing, or incident response exercises.   Access training platforms, pursue certifications, or attend workshops your organization provides.Your existing skills can provide a strong foundation, even if you’re coming from an unrelated field. Explore any opportunities available, including the tools and platforms mentioned earlier, to sharpen your skills and gain real-world experience.Participate in forums and meetups, for example, on meetup.com, and join online groups to exchange knowledge and gain mentorship. Chances are, you’ll meet someone working in a role you’re interested in, presenting a good opportunity to ask for feedback and insight into the next steps you can take to work toward a career in cybersecurity.Security research is more than a career—it’s a journey fueled by curiosity, creativity, and persistence. No matter your background, your willingness to dig deeper, think critically, and take action can make a meaningful difference in the digital world. The vulnerabilities you uncover could protect millions. The only question is—what action can you take today?How to stay updated on cybersecurity threatsCybersecurity evolves rapidly, and staying informed is critical. Use these strategies: Track vulnerabilities and exploits through platforms like Common Vulnerabilities and Exposures (CVE) Details and Threatpost.   Participate in forums like Reddit’s r/netsec or cybersecurity-focused Discord channels.   Use platforms like PicoCTF and Hack The Box to refine your skills in realistic scenarios.The journey to becoming a cybersecurity researcher is as much about curiosity and exploration as it is about structured learning. There’s no single path—your next move is yours to choose.Here are some ideas to spark your journey:Every action you take is a step forward in building your expertise and making the digital world safer. What will you explore next?Did you know GitHub has a Security Lab dedicated to improving open source security? Check out the GitHub Security Lab resources to learn more, explore tools, and join the effort to make open source safer.OWASP: A global community providing resources, tools, and documentation to improve software security.  CodeQL: A powerful tool for writing custom queries to identify vulnerabilities in source code.  Hack The Box: Provides challenges to practice penetration testing and reverse engineering.  TryHackMe: Interactive cybersecurity training with real-world scenarios and labs.  Secure Code Game: A fun, interactive way to learn and practice secure coding by identifying and fixing vulnerabilities.  Threatpost: Industry news and threat analysis to stay informed on vulnerabilities and exploits.  CVE Details: A resource for tracking and analyzing publicly known cybersecurity vulnerabilities.]]></content:encoded></item><item><title>NVIDIA&apos;s New AI: 150x Faster Virtual Worlds!</title><link>https://www.youtube.com/watch?v=jyhu1VipWpo</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/jyhu1VipWpo?version=3" length="" type=""/><pubDate>Wed, 29 Jan 2025 16:48:04 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

📝 The paper "InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds" is available here:
https://instantsplat.github.io/
Try it out (hopefully works): https://huggingface.co/spaces/kairunwen/InstantSplat

Clouds paper: https://arcanous98.github.io/projectPages/gaussianVolumes.html

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli Gallizzi
If you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu

#nvidia]]></content:encoded></item><item><title>Embeddings in GenAI Products</title><link>https://martinfowler.com/articles/gen-ai-patterns/#embedding</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 29 Jan 2025 15:55:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[GenAI systems, like many modern AI approaches, have to handle vast
      quantities of data, and find similarities between elements in an image or
      chunk of words.  and I describe a key tool
      to do this - Embeddings - transforming large data blocks into
      numeric vectors so that embeddings near each other represent related
      concepts]]></content:encoded></item><item><title>Generative AI and human-robot interaction: Implications and future agenda for business, society and ethics</title><link>https://www.datasciencecentral.com/generative-ai-and-human-robot-interaction-implications-and-future-agenda-for-business-society-and-ethics/</link><author>John Lee</author><category>dev</category><category>ai</category><pubDate>Wed, 29 Jan 2025 15:44:29 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dynamic Forms with Flask</title><link>https://blog.miguelgrinberg.com/post/dynamic-forms-with-flask</link><author>Miguel Grinberg</author><category>Miguel Grinberg</category><category>dev</category><category>python</category><pubDate>Wed, 29 Jan 2025 15:04:01 +0000</pubDate><source url="https://blog.miguelgrinberg.com/feed">https://blog.miguelgrinberg.com/feed</source><content:encoded><![CDATA[A common need in web applications is to create a form that allows the user to enter a list of items, with the number of items not known in advance. This is a pattern often used when entering user information, specifically for phone numbers or addresses, but has a lot of other uses as well. In the example below you can see how the user dynamically adds more phone numbers to the form by clicking the "Add another" link.Implementing this with Flask is surprisingly tricky, as it requires a combination of back and front end techniques working together. In this article I will show you two possible solutions, a basic one that uses only Flask and a more complete one for the Flask-WTF form handling extension.]]></content:encoded></item><item><title>What on Earth is the `types` Descriptor in View Transitions?</title><link>https://css-tricks.com/what-on-earth-is-the-types-descriptor-in-view-transitions/</link><author>Juan Diego Rodríguez</author><category>frontend</category><enclosure url="https://css-tricks.com/wp-content/uploads/2025/01/custom-view-transition.mp4" length="" type=""/><pubDate>Wed, 29 Jan 2025 14:13:53 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[Have you ever stumbled upon something new and went to research it just to find that there is little-to-no information about it? It’s a mixed feeling: confusing and discouraging because there is no apparent direction, but also exciting because it’s probably new to lots of people, not just you. Something like that happened to me while writing an Almanac’s entry for the  at-rule and its  descriptor.You may already know about Cross-Document View Transitions: With a few lines of CSS, they allow for transitions between two pages, something that in the past required a single-app framework with a side of animation library. In other words, lots of JavaScript.@view-transition {
  navigation: auto;
}That’s it! And  is the only descriptor we need. In fact, it’s the only descriptor available for the  at-rule, right? Well, turns out there is  descriptor, a lesser-known brother, and one that probably envies how much attention  gets: the .What do people say about ?Cross-Documents View Transitions are still fresh from the oven, so it’s normal that people haven’t fully dissected every aspect of them, especially since they introduce a  of new stuff: a new at-rule, a couple of new properties and tons of pseudo-elements and pseudo-classes. However, it still surprises me the little mention of . Some documentation fails to even name it among the valid   descriptors. Luckily, though, the CSS specification does offer a little clarification about it:The  descriptor sets the active types for the transition when capturing or performing the transition.To be more precise,  can take a space-separated list with the names of the active types (as ), or  if there aren’t valid active types for that page.Value: So the following values would work inside :@view-transition {
  navigation: auto;
  types: bounce;
}

/* or a list */

@view-transition {
  navigation: auto;
  types: bounce fade rotate;
}Yes, but what exactly are “active” types? That word “active” seems to be doing a lot of heavy lifting in the CSS specification’s definition and I want to unpack that to better understand what it means.Active types in view transitions A cross-fade animation for every page is good, but a common thing we need to do is change the transition depending on the pages we are navigating between. For example, on paginated content, we could slide the content to the right when navigating forward and to the left when navigating backward. In a social media app, clicking a user’s profile picture could persist the picture throughout the transition. All this would mean defining several transitions in our CSS, but doing so would make them conflict with each other in one big slop. What we need is a way to define several transitions, but only pick one depending on how the user navigates the page. Active types define which transition gets used and which elements should be included in it. In CSS, they are used through :active-view-transition-type(), a pseudo-class that matches an element if it has a specific active type. Going back to our last example, we defined the document’s active type as . We could enclose that  animation behind an :active-view-transition-type(bounce), such that it only triggers on that page./* This one will be used! */
html:active-view-transition-type(bounce) {
  &::view-transition-old(page) {
    /* Custom Animation */
  }

  &::view-transition-new(page) {
    /* Custom Animation */
  }
}This prevents other view transitions from running if they don’t match any active type:/* This one won't be used! */
html:active-view-transition-type(slide) {
  &::view-transition-old(page) {
    /* Custom Animation */
  }

  &::view-transition-new(page) {
    /* Custom Animation */
  }
}
I asked myself whether this triggers the transition when going to the page, when out of the page, or in both instances. Turns out it only limits the transition when , so the last bounce animation is only triggered when navigating toward a page with a  value on its  descriptor, but not when leaving that page. This allows for custom transitions depending on which page we are going to.The following demo has two pages that share a stylesheet with the  and  view transitions, both respectively enclosed behind an :active-view-transition-type(bounce) and :active-view-transition-type(slide) like the last example. We can control which page uses which view transition through the  descriptor.The first page uses the  animation:@view-transition {
  navigation: auto;
  types: bounce;
}The second page uses the  animation:@view-transition {
  navigation: auto;
  types: slide;
}The  descriptor is used more in JavaScriptThe main problem is that we can only control the transition depending on the page we’re navigating to, which puts a major cap on how much we can customize our transitions. For instance, the pagination and social media examples we looked at aren’t possible just using CSS, since we need to know where the user is coming from. Luckily, using the  descriptor is just one of three ways that active types can be populated. Per spec, they can be:Passed as part of the arguments to startViewTransition(callbackOptions)Mutated at any time, using the transition’s typesDeclared for a cross-document view transition, using the  descriptor.The first option is when starting a view transition from JavaScript, but we want to trigger them when the user navigates to the page by themselves (like when clicking a link). The third option is using the  descriptor which we already covered. The second option is the right one for this case! Why? It lets us set the active transition type on demand, and we can perform that change just before the transition happens using the  event. That means we can get the user’s start and end page from JavaScript and then set the correct active type for that case.I must admit, I am not the most experienced guy to talk about this option, so once I demo the heck out of different transitions with active types I’ll come back with my findings! In the meantime, I encourage you to read about active types here if you are like me and want  on view transitions:]]></content:encoded></item><item><title>Bliki: Forest And Desert</title><link>https://martinfowler.com/bliki/ForestAndDesert.html</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 29 Jan 2025 13:00:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[The Forest and the Desert is a metaphor for thinking about software
  development processes, developed by Beth Andres-Beck and hir father Kent Beck.
  It posits that two communities of software developers have great difficulty
  communicating to each other because they live in very different contexts, so
  advice that applies to one sounds like nonsense to the other.The desert is the common world of software development, where bugs are
  plentiful, skill isn't cultivated, and communications with users is difficult.
  The forest is the world of a well-run team that uses something like Extreme Programming, where developers swiftly put changes into
  production, protected by their tests, code is invested in to keep it healthy,
  and there is regular contact with The Customer.Clearly Beth and Kent prefer The Forest (as do I). But the metaphor is more
  about how description of The Forest and the advice for how to work there often
  sounds nonsensical to those whose only experience is The Desert. It reminds us
  that any lessons we draw about software development practice, or architectural
  patterns, are governed by the context that we experienced them. It is possible
  to change Desert into Forest, but it's difficult - often requiring people to do
  things that are both hard and counter-intuitive. (It seems sadly easier for
  The Forest to submit to desertification.) In this framing I'm definitely a Forest Dweller, and seek with Thoughtworks
  to cultivate a healthy forest for us and our clients. I work to explain The Forest to Desert
  Dwellers, and help my fellow Forest Dwellers to make their forest even more
  plentiful.Kent Beck supplied the image, which he may have painstakingly drew pixel by
    pixel. Or not.]]></content:encoded></item><item><title>The Raylib C Library for Game Development with Ramon Santamaria</title><link>https://softwareengineeringdaily.com/2025/01/29/the-raylib-c-library-for-game-development-with-ramon-santamaria/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-raylib-c-library-for-game-development-with-ramon-santamaria</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED9835831252.mp3" length="" type=""/><pubDate>Wed, 29 Jan 2025 10:00:30 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[Raylib is a lightweight, beginner-friendly, and open-source C library for game development, known for its simplicity and lack of external dependencies. It’s designed to streamline the creation of 2D and 3D games, and has an intuitive API for managing graphics, audio, and input.Ramon Santamaria is the Founder and Lead Developer of Raylib. He joins the show with Joe Nash to talk about the Raylib project.Joe Nash is a developer, educator, and award-winning community builder, who has worked at companies including GitHub, Twilio, Unity, and PayPal. Joe got his start in software development by creating mods and running servers for Garry’s Mod, and game development remains his favorite way to experience and explore new technologies and concepts.]]></content:encoded></item><item><title>This Week in Rust 584</title><link>https://this-week-in-rust.org/blog/2025/01/29/this-week-in-rust-584/</link><author>TWiR Contributors</author><category>This week in Rust</category><category>dev</category><category>rust</category><pubDate>Wed, 29 Jan 2025 05:00:00 +0000</pubDate><source url="https://this-week-in-rust.org/">This Week in Rust</source><content:encoded><![CDATA[This week's crate is embed_it, a crate that helps you to embed assets into your binary and generates structs / trait implementations for each file or directory.Thanks to Riberk for the self-suggestion!An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.  The following
RFCs would benefit from user testing before moving forward:No calls for testing were issued this week.No calls for testing were issued this week.No calls for testing were issued this week.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.Relatively quiet week, with one large-ish regression that will be reverted.
#132666 produced a nice perf. win, by skipping
unnecessary work. This PR actually reversed a regression caused by a previous PR.Improvements ✅  (secondary)2 Regressions, 3 Improvements, 2 Mixed; 4 of them in rollups
45 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.No RFCs entered Final Comment Period this week.No Cargo Tracking Issues or PRs entered Final Comment Period this week.No Language Team Proposals entered Final Comment Period this week.No Language Reference RFCs entered Final Comment Period this week.No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.No New or Updated RFCs were created this week.Rusty Events between 2025-01-29 - 2025-02-26 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.I have experience in multiple styles of MMA gained from fighting the borrow checker, if that counts.]]></content:encoded></item><item><title>Risky Business #777 -- It&apos;s SonicWall&apos;s turn</title><link>https://risky.biz/RB777/</link><author></author><category>Risky Business blog</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/media3.risky.biz/RB777.mp3" length="" type=""/><pubDate>Wed, 29 Jan 2025 03:29:48 +0000</pubDate><source url="https://risky.biz/">Risky Business</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Linkerd, with William Morgan</title><link>http://sites.libsyn.com/419861/linkerd-with-william-morgan</link><author>gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)</author><category>podcast</category><category>k8s</category><category>devops</category><enclosure url="https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPod246.mp3?dest-id=3486674" length="" type=""/><pubDate>Wed, 29 Jan 2025 00:25:00 +0000</pubDate><source url="https://kubernetespodcast.com/">Kubernetes Podcast</source><content:encoded><![CDATA[William Morgan is the CEO of Buoyant, the company behind Linkerd. You worked at Twitter before as a software engineer and engineering manager and you have a long experience in the field.Do you have something cool to share? Some questions? Let us know: News of the week  Links from the interview ]]></content:encoded></item><item><title>Computing with tuple types in TypeScript</title><link>https://2ality.com/2025/01/typescript-tuples.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[JavaScript’s Arrays are so flexible that TypeScript provides two different kinds of types for handling them:Array types for arbitrary-length sequences of values that all have the same type – e.g.: Tuple types for fixed-length sequences of values where each one may have a different type – e.g.: [number, string, boolean]In this blog post, we look at the latter – especially how to compute with tuples at the type level.]]></content:encoded></item><item><title>This Week In React #219: CRA, Next.js, Remix, Tremor, SaaS-Starter, Local-First, JSDom, 2n8, RN-Windows, OTA, Polygen, Audio API, Debugging, Shopify, Bottom Tabs, Reanimated, Tailwind, Standard Schema, TypeScript...</title><link>https://thisweekinreact.com/newsletter/219</link><author></author><category>frontend</category><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><source url="https://thisweekinreact.com/newsletter">This week in react</source><content:encoded><![CDATA[It's been a fairly quiet week compared with the previous one, and I couldn't pick any headliner!The most important news for our ecosystem is probably the official deprecation of Create React App. There's also Tailwind v4 and Standard Schema v1.I'm quite excited by the upcoming TypeScript 5.8  flag that forbids usage of features such as enums. This is the path toward making your app compatible Node.js built-in support, and maybe browsers too in the future.Check our partner conf dotJS 2025 - 🇫🇷 Paris - 03 April - Speakers announced! Ryan Dahl, creator of Node.js, Sarah Drasner, Director of Engineering at Google and Wes Bos, co-host of Syntax.fm podcast. Get 15% off tickets with code "TWIR”.As always, thanks for supporting us on your favorite platform:Convex is the all-in-one platform that gives you everything you need to build, launch, and scale a production-ready backend.And that’s just the beginning. With Convex, you can:Easily schedule functions and write cron jobsWrite efficient server functionsGenerate your backend with your favorite AI coding toolReactile is an UI/UX turnkey framework which features an out-of-the-box, intuitive tiling display with workspaces. It encourages parallel workloads for large web solutions within a single browser tab. Implement your own view containers, widgets and logic using React and let Reactile handle the display for you.On top of that, Reactile also offers:A promise-based persistence mechanism which permits a uniform workflow across time and on multiple devices.The possibility to use nested tabs for your tiles.A simplified global search mechanism.CSS @container based responsiveness.Version 2 coming out soon. Get started with your unique 30-day free trial. Learn more about it on reactile.net.]]></content:encoded></item><item><title>A continued focus on the benefits of Go 1.24</title><link>https://golangweekly.com/issues/540</link><author></author><category>dev</category><category>go</category><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><source url="https://golangweekly.com/">Golang Weekly</source><content:encoded><![CDATA[SRCL is a neat new suite of React components for creating webapps with a monospaced, terminal-style aesthetic.If you ever need to work with JavaScript server-side, Bun 1.2 is a huge release for an increasingly popular, high performance -based Node-a-like. It 'just works' for a lot of stuff where Node gives me headaches.Take a look at every byte that goes over the network when making a real Postgres query over TLS . Everything is annotated to help you understand the complexity.]]></content:encoded></item><item><title>EFF to State AGs: Time to Investigate Crisis Pregnancy Centers</title><link>https://www.eff.org/deeplinks/2025/01/eff-state-ags-time-investigate-crisis-pregnancy-centers</link><author>Corynne McSherry</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/medical-privacy_1.png" length="" type=""/><pubDate>Tue, 28 Jan 2025 22:21:51 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Discovering that you’re pregnant can trigger a mix of emotions—excitement, uncertainty, or even distress—depending on your circumstances. Whatever your feelings are, your next steps will likely involve disclosing that news, along with other deeply personal information, to a medical provider or counselor as you explore your options.Many people will choose to disclose that information to their trusted obstetricians, or visit their local Planned Parenthood clinic. Others, however, may instead turn to a crisis pregnancy center (CPC). Trouble is, some of these centers may not be doing a great job of prioritizing or protecting their clients’ privacy.Last year, researchers at the Campaign for Accountability filed multiple complaints urging attorneys general in five states—Idaho, Minnesota, Washington, Pennsylvania, and New Jersey—to investigate crisis pregnancy centers that allegedly had misrepresented, through their client intake process and/or websites, that information provided to them was protected by the Health Insurance Portability and Accountability Act (“HIPAA”).Additionally, an incident in Louisiana raised concerns that CPCs may be sharing client information with other centers in their affiliated networks, without appropriate privacy or anonymity protections. In that case, a software training video inadvertently disclosed the names and personal information of roughly a dozen clients.Unfortunately, these privacy practices aren’t confined to those states. For example, the Pregnancy Help Center, located in Missouri, states on its website that:Pursuant to the Health Insurance Portability and Accountability Act (HIPAA), Pregnancy Help Center has developed a notice for patients, which provides a clear explanation of privacy rights and practices as it relates to private health informationAnd its Notice of Privacy Practices suggests oversight by the U.S. Department of Health and Human, instructing clients who feel their rights were violated to:file a complaint with the U.S. Department of Health and Human Services Office for Civil Rights by sending a letter to 200 Independence Avenue, S.W., Washington, D.C. 20201, calling 1-877-696-6775, or visiting www.hhs.gov/ocr/privacy/hipaa/complaints/ Websites for centers in other states, such as Florida, Texas, and Arkansas, contain similar language.As we’ve noted before, there are far too few protections for user privacy–including medical privacy—and individuals have little control over how their personal data is collected, stored, and used. Until Congress passes a comprehensive privacy law that includes a private right of action, state attorneys general must take proactive steps to protect their constituents from unfair or deceptive privacy practices. Accordingly, EFF has called on attorneys general in Florida, Texas, Arkansas, and Missouri to investigate potential privacy violations and hold accountable CPCs that engage in deceptive practices.Regardless of your views on reproductive healthcare, we should all agree that privacy is a basic human right, and that consumers deserve transparency. Our elected officials have a responsibility to ensure that personal information, especially our sensitive medical data, is protected.]]></content:encoded></item><item><title>OpenAI&apos;s nightmare: Deepseek R1 on a Raspberry Pi</title><link>https://www.youtube.com/watch?v=o1sN1lB76EA</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/o1sN1lB76EA?version=3" length="" type=""/><pubDate>Tue, 28 Jan 2025 22:18:31 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[DeepSeek R1 runs on a Pi 5, but don't believe every headline you read.

Resources referenced in this video:

  - DeepSeek R1: https://github.com/deepseek-ai/DeepSeek-R1
  - Ollama and llama.cpp benchmarks on Pi 5: https://github.com/geerlingguy/ollama-benchmark/issues/9
  - Reuters article on DeepSeek: https://www.reuters.com/technology/artificial-intelligence/what-is-deepseek-why-is-it-disrupting-ai-sector-2025-01-27/
  - DeepSeek FAQ from Stratchery: https://stratechery.com/2025/deepseek-faq/

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff

Contents:

00:00 - OpenAI's Nightmare
01:00 - What can a Pi 5 do, really?
01:39 - 671b on AmpereOne
02:00 - Pi 5 14b - CPU inference
02:20 - Pi 5 14b - GPU inference
03:05 - GPUs on Pi (and year of the Arm PC)
03:37 - Still in an AI bubble]]></content:encoded></item><item><title>Apple chips can be hacked to leak secrets from Gmail, iCloud, and more</title><link>https://arstechnica.com/security/2025/01/newly-discovered-flaws-in-apple-chips-leak-secrets-in-safari-and-chrome/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2023/10/Apple-M3-chip-series_screen-1152x648.jpeg" length="" type=""/><pubDate>Tue, 28 Jan 2025 20:56:32 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Apple-designed chips powering Macs, iPhones, and iPads contain two newly discovered vulnerabilities that leak credit card information, locations, and other sensitive data from the Chrome and Safari browsers as they visit sites such as iCloud Calendar, Google Maps, and Proton Mail.The vulnerabilities, affecting the CPUs in later generations of Apple A- and M-series chip sets, open them to side channel attacks, a class of exploit that infers secrets by measuring manifestations such as timing, sound, and power consumption. Both side channels are the result of the chips’ use of speculative execution, a performance optimization that improves speed by predicting the control flow the CPUs should take and following that path, rather than the instruction order in the program.The Apple silicon affected takes speculative execution in new directions. Besides predicting control flow CPUs should take, it also predicts the data flow, such as which memory address to load from and what value will be returned from memory.]]></content:encoded></item><item><title>AI alignment, safety more pivotal than fewer computing or low-cost training</title><link>https://www.datasciencecentral.com/ai-alignment-safety-more-pivotal-than-fewer-computing-or-low-cost-training/</link><author>David Stephen</author><category>dev</category><category>ai</category><pubDate>Tue, 28 Jan 2025 20:09:37 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What Proponents of Digital Replica Laws Can Learn from the Digital Millennium Copyright Act</title><link>https://www.eff.org/deeplinks/2025/01/what-proponents-digital-replica-laws-can-learn-digital-millennium-copyright-act</link><author>Corynne McSherry</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/dmca-copyright-1_0.png" length="" type=""/><pubDate>Tue, 28 Jan 2025 18:37:16 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Considerations for making a tree view component accessible</title><link>https://github.blog/engineering/user-experience/considerations-for-making-a-tree-view-component-accessible/</link><author>Eric Bailey</author><category>official</category><pubDate>Tue, 28 Jan 2025 17:00:39 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[Tree views are a core part of the GitHub experience. You’ve encountered one if you’ve ever navigated through a repository’s file structure or reviewed a pull request.On GitHub, a tree view is the list of folders and the files they contain. It is analogous to the directory structure your operating system uses as a way of organizing things.Tree views are notoriously difficult to implement in an accessible way. This post is a deep dive into some of the major considerations that went into how we made GitHub’s tree view component accessible. We hope that it can be used as a reference and help others.It’s important to have components with complex interaction requirements map to something people are already familiar with using. This allows for responsiveness to the keypresses they will try to navigate and take action on our tree view instances.Navigating and taking actions on items in Windows’ tree view using NVDA and JAWS helped us get a better understanding of how things worked, including factors such as focus management, keyboard shortcuts, and expected assistive technology announcements.Then maybe reference the APGThis is to say that the APG can serve as a helpful high-level resource for things to consider for your overall approach, but its suggestions for code necessitate deeper scrutiny.Build from a solid, semantic foundationAt its core, a tree view is a list of lists. Because of this, we used  and  elements for parent and child nodes:<ul>
  <li>
    <ul>
      <li>.github/</li>
      <li>source/</li>
      <li>test/</li>
    </ul>
  </li>
  <li>.gitignore</li>
  <li>README.md</li>
</ul>
There are a few reasons for doing this, but the main considerations are:Lessening the work we need for future maintenance, and consequential re-verification that our updates continue to work properly, and  Better guaranteed interoperability between different browsers, apps, and other technologies.: GitHub currently does not virtualize its file trees. We would need to revisit this architectural decision if this ever changes.Better broad assistive technology supportWe found that utilizing semantic HTML elements also performed better for some less-common assistive technologies. This was especially relevant with some lower-power devices, like an entry-level Android smartphone from 2021.Better Forced Color Mode supportSemantic HTML elements also map to native operating system UI patterns, meaning that Forced Color Mode’s heuristics will recognize them without any additional effort. This is helpful for people who rely on the mode to see screen content.The heuristic mapping behavior does not occur if we used semantically neutral  or  elements, and would have to be manually recreated and maintained.A composite widget allows a component that contains multiple interactive elements to only require one tab stop unless someone chooses to interact with it further.Consider a file tree for a repository that contains 500+ files in 20+ directories. Without a composite widget treatment, someone may have to press  far too many times to bypass the file tree component and get what they need.Think about wrapping it in a landmarkLike using a composite widget, landmark regions help some people quickly and efficiently navigate through larger overall sections of the page. Because of this, we wrapped the entire file tree in a  landmark element.Go with a roving  approachA roving tabindex is a technique that uses  applied to each element in a series, and then updates the  value to use  instead in response to user keyboard input. This allows someone to traverse the series of elements, as focus “roves” to follow their keypresses.<li tabindex="-1">File 1</li>
<li tabindex="-1">File 2</li>
<li tabindex="0">File 3</li>
<li tabindex="-1">File 4</li>
We use a considered set of ARIA declarations to build off our semantic foundation.Note that while we intentionally started with semantic HTML, there are certain ARIA declarations that are needed. The use of ARIA here is necessary and intentional, as it expands the capabilities of HTML to describe something that HTML alone cannot describe—a tree view construct. is placed on the parent  element, to communicate that it is a tree view construct.   is placed on the child  elements, to communicate that they are tree view nodes.   is declared on child  elements, to communicate that they contain branch and leaf nodes.   is declared on directories, with a value of  to communicate that the branch node is in an opened state and a value of  to communicate that it is in a collapsed state instead.   is used to indicate if branch or leaf nodes have been chosen by user navigation, and can therefore have user actions applied to them.We also made the following additions: is applied to SVG icons (folders, files, etc.) to ensure its content is not announced.   is placed on the selected node to better support when a node is deep linked to via URL.: We use “branch node” and “leaf node” as broad terms that can apply to all tree view components we use on GitHub. For the file tree, branch nodes would correspond to directories and subdirectories, and leaf nodes would correspond to files.Support expected navigation techniquesThe following behaviors are what people will try when operating a tree view construct, so we support them:: Places focus on the entire tree view component, then moves focus to the next focusable item on the view.  :
If a branch node is selected: Displays the directory’s contents.  If a leaf node is selected: Displays the leaf node’s contents.  : Moves selection to the next node that can be selected without opening or closing a node.  : Moves selection to the previous node that can be selected without opening or closing a node.  :
If a branch node is selected and in a collapsed state: Expands the selected collapsed branch node and does not move selection.  If a branch node is selected and in an expanded state: Moves selection to the directory’s first child node.  :
If a branch node is selected and in an expanded state: Collapses the selected collapsed directory node and does not move selection.  If a branch node is selected and in a collapsed state: Moves selection to the node’s parent directory.  If a leaf node is selected: Moves selection to the leaf node’s parent directory.  : Moves selection to the last node that can be selected.  : Moves selection to the first node that can be selected.We also support , as we are modeling Windows File Explorer’s tree view behaviors. Here, we move selection to the node closest to the currently selected node whose name matches what the user types.Nodes on tree view constructs are tree items, not links. Because of this, tree view nodes do not support the behaviors you get with using an anchor element, such as opening its URL in a new tab or window.We use JavaScript to listen for middle clicks and + keypresses to replicate this behavior.Tree views on GitHub can take time to retrieve their content, and we may not always know how much content a branch node contains.If there is a known amount of nodes that load, we enumerate the incoming content with an announcement that reads, “Loading {x} items.”  If there is an unknown number of nodes that load, we instead use a more generic announcement of, “Loading…”  If there are no nodes that load we use an announcement message that reads, “{branch node name} is empty.”If focus is placed on a placeholder loading node when the content loads in: Move focus from the placeholder node to the first child node in the branch node.   If focus is on a placeholder loading node but the branch node does not contain content: Move focus back to the branch node. Additionally, we remove the branch node’s  declaration.Circumstances can conspire to interfere with a tree view component’s intended behavior. Examples of this could be a branch node failing to retrieve content or a partial system outage.In these scenarios, the tree view component will use a straightforward dialog component to communicate the error.Fix interoperability issuesAs previously touched on, complicated interaction patterns run the risk of compatibility issues. Because of this, it’s essential to test your efforts with actual assistive technology to .We made the following adjustments to provide better assistive technology support:Screen readers can report on the depth of a nested list item. For example, a  element placed inside of a  element nested three levels deep can announce itself as such.We found that we needed to explicitly declare the level on each  element to recreate this behavior for a tree view. For our example, we’d also need to set  on the  element.This fix addressed multiple forms of assistive technology we tested with.Explicitly set the node’s accessible name on the  elementA node’s accessible name is typically set by the text string placed inside the  element:However, we found that VoiceOver on macOS and iOS did not support this. This may be because of the relative complexity of each node’s inner DOM structure.We used  to get around this problem, with a value that pointed to the  set on the text portion of each node:<li aria-labelledby="readme-md">
  <div>
   <!-- Icon -->
  </div>
  <div id="readme-md">
    README.md
  </div>
</li>
the node’s accessible name is announced when focus is placed on the  element,   and that the announcement matches what is shown visually.Where we’d like to go from hereThere’s a couple areas we’re prototyping and iterating on to better serve our users:Supporting links inside a nodeBrowsers apply a lot of behaviors to anchor elements, such as the ability to copy the URL.We’d like to replace the JavaScript that listens for middle clicks with a more robust native solution, only without sacrificing interoperability and assistive technology support.Supporting multiple actions per nodeTree views constructs were designed assuming a user will only ever navigate to a node and activate it.GitHub has use cases that require actions other than activating the node, and we’re exploring how to accomplish that. This is exciting, as it represents an opportunity to evolve the tree view construct on the web.An accessible tree view is a complicated component to make, and it requires a lot of effort and testing to get right. However, this work helps to ensure that everyone can use a core part of GitHub, regardless of device, circumstance, or ability.We hope that highlighting the considerations that went into our work can help you on your accessibility journey.: We’d love to hear from you if you’ve run into issues using our tree view component with assistive technology. This feedback is invaluable to helping us continue to make GitHub more accessible.]]></content:encoded></item><item><title>California Law Enforcement Misused State Databases More Than 7,000 Times in 2023</title><link>https://www.eff.org/deeplinks/2025/01/california-police-misused-state-databases-more-7000-times-2023</link><author>Dave Maass</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/police-2.png" length="" type=""/><pubDate>Tue, 28 Jan 2025 16:30:16 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[A slide from a Long Beach Police Department training for new recruits. ]]></content:encoded></item><item><title>Creating a Scalable Flask Web Application From Scratch</title><link>https://realpython.com/courses/create-scalable-flask-web-app/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Tue, 28 Jan 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[Flask is a powerful and flexible micro web framework for Python, ideal for both small and large web projects. It provides a straightforward way to get a web application up and running, with all the features that you need to get started.Over the course of this video course, you’ll explore the process of creating a boilerplate for a Flask web project. This boilerplate will serve as a great starting point for any scalable Flask web app that you wish to develop in the future, from basic web pages to complex web applications.In this video course, you’ll learn how to:Create a Add  with blueprints]]></content:encoded></item><item><title>Bypassing character blocklists with unicode overflows</title><link>https://portswigger.net/research/bypassing-character-blocklists-with-unicode-overflows</link><author></author><category>infosec</category><pubDate>Tue, 28 Jan 2025 13:58:28 +0000</pubDate><source url="https://portswigger.net/research">Dev - PortSwitgger Research</source><content:encoded><![CDATA[Unicode codepoint truncation - also called a Unicode overflow attack - happens when a server tries to store a Unicode character in a single byte. Because the maximum value of a byte is 255, an overflo]]></content:encoded></item><item><title>Emerging Patterns in Building GenAI Products</title><link>https://martinfowler.com/articles/gen-ai-patterns/</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 28 Jan 2025 12:43:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[Everyone is fascinated about using generative AI these days, and my
      colleagues are no exception. Some of them have had the opportunity to put
      these system into practice, both as proof-of-concept, and more importantly
      as production system. I've known  for
      many years as a technology leader in India, he's been assembling the
      lessons we've learned and I've worked with him to describe them as
      patterns.In this first installment, we look the limits of the base case of
      Direct Prompting, and how we might assess the capability of a system using
      Evals.]]></content:encoded></item><item><title>Anduril with Gokul Subramanian</title><link>https://softwareengineeringdaily.com/2025/01/28/anduril-with-gokul-subramanian/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=anduril-with-gokul-subramanian</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED5987055178.mp3" length="" type=""/><pubDate>Tue, 28 Jan 2025 10:00:38 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[Anduril is a technology defense company with a focus on drones, computer vision, and other problems related to national security. It is a full-stack company that builds its own hardware and software, which leads to a great many interesting questions about cloud services, engineering workflows, and management.Gokul Subramanian is Senior Vice President of Engineering for Software Programs at Anduril Industries. He joins the show to share his knowledge of the national security problem set, how Anduril operates and what the company has built.]]></content:encoded></item><item><title>Don&apos;t Make Copyright Law in Smoke-Filled Rooms</title><link>https://www.eff.org/deeplinks/2025/01/dont-make-copyright-law-smoke-filled-rooms</link><author>Mitch Stoltz</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/dmca-copyright-1_0.png" length="" type=""/><pubDate>Tue, 28 Jan 2025 06:32:19 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[.Copyright law affects everything we do on the internet. So why do some lawmakers and powerful companies still think they can write new copyright law behind closed doors, with ordinary internet users on the outside?Major movie and TV studios  pushing Congress to create a vast new censorship regime on the U.S. internet, one that could even reach abroad and conscript infrastructure companies to help make whole websites disappear. The justification is, as always, creating ever more draconian means of going after copyright infringement, and never mind all of the powerful tools that already exist.The movie studios and other major media companies last tried this in 2012, seeking to push a pair of internet censorship bills called SOPA and PIPA through Congress, without hearings. Lawmakers were preparing to ignore the concerns of internet users not named Disney, Warner, Paramount, or Fox. At first, they ignored the long, sad history of copyright enforcement tools being used for censorship. They ignored the technologists, including some of the creators of the internet, who explained how website-blocking creates security threats and inevitably blocks lawful speech. And they ignored the pleas of ordinary users who were concerned about the websites they relied on going dark because of hasty site-blocking orders.Writing new copyright laws in the proverbial smoke-filled backroom was somewhat less surprising in 2012. Before the internet, copyright mainly governed the relationships between authors and publishers, movie producers and distributors, photographers and clients, and so on. The easiest way to make changes was to get representatives of these industries together to hash out the details, then have Congress pass those changes into law. It worked well enough for most people.In the internet age, that approach  unworkable. Every use of the internet, whether sending a photo, reading a social media post, or working on a shared document, causes a copy of some creative work. And nearly every creative work that’s recorded on a computing device is automatically governed by copyright law, with no registration or copyright notices required. That makes copyright a fundamental governing law of the internet. It shapes the design and functions of the devices we use, what software we can run, and when and how we can participate in culture. Its massive penalties and confusing exceptions can ensnare everyone from landlords to librarians, from students to salespeople.Users fought back. In a historic protest, thousands of websites went dark for a day, with messages encouraging users to oppose the SOPA/PIPA bills. EFF alone helped users send more than 1,000,000 emails to Congress, and countless more came from other organizations. Web traffic briefly brought down some Senate websites. 162 million people visited Wikipedia and 8 million looked up their representatives’ phone numbers. Google received more than 7 million signatures on its petition. Everyone who wrote, called, and visited their lawmakers sent a message that laws affecting the internet can't be made in a backroom by insiders bearing campaign cash. Congress quickly scrapped the bills.After that, although Congress avoided changing copyright law for years, the denizens of the smoke-filled room never gave up. The then-leaders of the Motion Picture Association and the Recording Industry Association of America both vented angrily about ordinary people getting a say over copyright. Big Media went on a world tour, pushing for site-blocking laws that led to the same problems [Italy etc] of censorship and over-blocking in many countries that U.S. users had mostly avoided.Now, they’re trying again. Major media companies are pushing Congress to pass new site-blocking laws that would conscript internet service providers, domain name services, and potentially others to build a new censorship machine. The problems of overblocking and misuse haven’t gone away—if anything they've gotten as ever more of our life is lived online. The biggest tech companies, who in 2012 were prodded into action by a mass movement of internet users, are now preoccupied by antitrust lawsuits and seeking favor from the new administration in Washington. And as with other extraordinary tools that Congress has given to the largest copyright holders, site-blocking won’t stay confined to copyright—other powerful industries and governments will clamor to use the system for censorship, and it will get ever harder to resist those calls.It seems like lawmakers have learned nothing, because copyright law is again being written in secret by a handful of industry representatives. That was unacceptable in 2012, and it’s even more unacceptable in 2025. Before considering site blocking, or any major changes to copyright, Congress needs to consult with every kind of internet user, including small content creators, small businesses, educators, librarians, and technologists not beholden to the largest tech and media companies.We can’t go backwards. Copyright law affects everyone, and everyone needs a say in its evolution. Before taking up site-blocking or any other major changes to copyright law, Congress needs to air those proposals publicly, seek input from far and wide—and listen to it.]]></content:encoded></item><item><title>DtSR Episode 638 - Matt Shufeldt Cyber Security&apos;s Specialist Problem</title><link></link><author></author><category>Down the Security Rabbit Hole Podcast</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/www.buzzsprout.com/2153215/episodes/16511638-dtsr-episode-638-matt-shufeldt-cyber-security-s-specialist-problem.mp3" length="" type=""/><pubDate>Tue, 28 Jan 2025 05:00:00 +0000</pubDate><source url="https://blogwh1t3rabbit.medium.com/">Down the Security Rabbithole Podcast (DtSR)</source></item><item><title>Long Polling vs WebSockets</title><link>https://blog.algomaster.io/p/long-polling-vs-websockets</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/90181ae6-4dcb-456f-a43a-7824d97c740b_2002x1472.png" length="" type=""/><pubDate>Tue, 28 Jan 2025 04:30:40 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[Whether you are playing an online game or chatting with a friend—updates appear in real-time without hitting .Behind these seamless experiences lies a critical engineering decision: how to push real-time updates from servers to clients.The traditional HTTP model was designed for request-response: "Client asks, server answers.". But in many real-time systems, the server needs to talk first and more often.This is where  and  come into play—two popular methods for achieving real-time updates.In this article, we’ll explore these two techniques, how they work, their pros and cons, and use cases.If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .HTTP, the backbone of the web, follows a :The client (e.g., a browser or mobile app) sends a request to the server.The server processes the request and sends back a response.This model is simple and works for many use-cases, but it has limitations: With plain HTTP, the server cannot proactively push data to the client. The client has to request the data periodically. HTTP is stateless, meaning each request stands alone with no persistent connection to the server. This can be problematic if you need continuous exchange of data.To build truly real-time features—live chat, financial tickers, or gaming updates—you need a mechanism where the server can instantly notify the client when something changes. is a technique that mimics real-time behavior by keeping HTTP requests open until the server has data.Long Polling is an enhancement over traditional polling. In regular polling, the client repeatedly sends requests at fixed intervals (e.g., every second) to check for updates. This can be wasteful if no new data exists. Long Polling tweaks this approach: the client asks the server for data and then “waits” until the server has something new to return or until a timeout occurs.How Does Long Polling Work? to the server, expecting new data.Server holds the request open until it has an update or a timeout is reached.If there's new data, the server immediately responds.If there’s no new data and the timeout is reached, the server responds with an empty or minimal message.Once the client receives a response—new data or a timeout—it immediately sends a new request to the server to keep the connection loop going.Simple to implement (uses standard HTTP).Supported universally since it uses standard HTTP, and it works reliably through firewalls and proxies.Higher latency after each update (client must re-establish connection).Resource-heavy on servers (many open hanging requests).Simple chat or comment systems where real-time but slightly delayed updates (near real-time) are acceptable.Notification systems for less frequent updates (e.g., Gmail’s "new email" alert).Legacy systems where WebSockets aren’t feasible.Code Example (JavaScript) provide a full-duplex, persistent connection between the client and the server.Once established, both parties can send data to each other at any time, without the overhead of repeated HTTP requests. Client sends an HTTP request with .: If supported, the server upgrades the connection to WebSocket (switching from  to ). After the handshake, client and server keep a TCP socket open for communication.Full-Duplex Communication: Once upgraded, data can be exchanged bidirectionally in real time until either side closes the connection.Ultra-low latency (no repeated handshakes).Lower overhead since there’s only one persistent connection rather than repeated HTTP requests.Scalable for real-time applications that need to support large number of concurrent users.More complex setup (requires the client and server to support WebSocket).Some proxies and firewalls may not allow WebSocket traffic.Complexity in implementation and handling reconnections/errors.Server resource usage might grow if you have a large number of concurrent connections.Live chat and collaboration tools (Slack, Google Docs, etc.).Multiplayer online games with real-time state synchronization.Live sports/financial dashboards that need to push frequent updates.Code Example (JavaScript)Both methods achieve real-time updates, but your choice depends on your project’s requirements: is easier to implement using standard libraries. Any environment that supports HTTP can handle it, often without extra packages. require a bit more setup and a capable proxy environment (e.g., support in Nginx or HAProxy). However, many frameworks (e.g., Socket.io) simplify the process significantly.Scalability and Performance can become resource-intensive with a large number of simultaneous clients, due to multiple open connections waiting on the server side. offer a more efficient, persistent connection and scale better for heavy, frequent data streams. fits scenarios where data updates aren’t super frequent. If new data arrives every few seconds or minutes, long polling might be enough. are better for high-frequency updates or two-way communication (e.g., multiple participants editing a document or interacting in a game). typically works even in older networks or those with strict firewalls. might face issues in certain corporate or older mobile environments, though this is less of a problem as the standard becomes more widespread.While both achieve real-time communication, WebSockets are generally more efficient for truly real-time applications, while Long Polling can be simpler to implement for less demanding scenarios.1. Server-Sent Events (SSE)Allows the server to push messages to the client over HTTP.It's simpler than WebSockets for one-way communication, but not full-duplex.Best suited for use cases like news feeds, real-time notifications, and status updates.Commonly used in IoT for lightweight publish-subscribe messaging.Specialized for device-to-device or device-to-server communication with minimal overhead.3. Libraries like Socket.ioProvides an abstraction over WebSockets for easier real-time communication.Automatically falls back to long polling if WebSockets are unsupported.Ensures cross-browser compatibility with robust and reliable performance.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re enjoying this newsletter and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>A Tumultuous Week for Federal Cybersecurity Efforts</title><link>https://krebsonsecurity.com/2025/01/a-tumultuous-week-for-federal-cybersecurity-efforts/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Tue, 28 Jan 2025 02:50:10 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[Image: Shutterstock. Greg Meland. last week issued a flurry of executive orders that upended a number of government initiatives focused on improving the nation’s cybersecurity posture. The president fired all advisors from the Department of Homeland Security’s Cyber Safety Review Board, called for the creation of a strategic cryptocurrency reserve, and voided a Biden administration action that sought to reduce the risks that artificial intelligence poses to consumers, workers and national security.The CSRB was in the midst of an inquiry into cyber intrusions uncovered recently across a broad spectrum of U.S. telecommunications providers at the hands of Chinese state-sponsored hackers. One of the CSRB’s most recognizable names is (no relation), the former director of the Cybersecurity and Infrastructure Security Agency (CISA). Krebs was fired by President Trump in November 2020 for declaring the presidential contest was the most secure in American history, and for refuting Trump’s false claims of election fraud.South Dakota Governor , confirmed by the U.S. Senate last week as the new director of the DHS, criticized CISA at her confirmation hearing, reports.Noem told lawmakers CISA needs to be “much more effective, smaller, more nimble, to really fulfill their mission,” which she said should be focused on hardening federal IT systems and hunting for digital intruders. Noem said the agency’s work on fighting misinformation shows it has “gotten far off mission” and involved “using their resources in ways that was never intended.”“The misinformation and disinformation that they have stuck their toe into and meddled with, should be refocused back onto what their job is,” she said., a cybersecurity instructor with the SANS Institute, compared the sacking of the CSRB members to firing all of the experts at the National Transportation Safety Board (NTSB) while they’re in the middle of an investigation into a string of airline disasters.“I don’t recall seeing an ‘NTSB Board’ being fired during the middle of a plane crash investigation,” Frost said in a recent SANS newsletter. “I can say that the attackers in the phone companies will not stop because the review board has gone away. We do need to figure out how these attacks occurred, and CISA did appear to be doing some good for the vast majority of the federal systems.”Speaking of transportation,  notes that Transportation Security Administration chief was fired despite overseeing critical cybersecurity improvements across pipeline, rail and aviation sectors. Pekoske was appointed by Trump in 2017 and had his 5-year tenure renewed in 2022 by former President Joe Biden.Shortly after being sworn in for a second time, Trump voided a Biden executive order that focused on supporting research and development in artificial intelligence. The previous administration’s order on AI was crafted with an eye toward managing the safety and security risks introduced by the technology. But a statement released by the White House said Biden’s approach to AI had hindered development, and that the United States would support AI systems that are “free from ideological bias or engineered social agendas,” to maintain leadership.The Trump administration issued its own executive order on AI, which calls for an “AI Action Plan” to be led by the assistant to the president for science and technology, the White House “AI & crypto czar,” and the national security advisor. It also directs the White House to revise and reissue policies to federal agencies on the government’s acquisition and governance of AI “to ensure that harmful barriers to America’s AI leadership are eliminated.”Trump’s AI & crypto czar is , an entrepreneur and Silicon Valley venture capitalist who argues that the Biden administration’s approach to AI and cryptocurrency has driven innovation overseas. Sacks recently asserted that non-fungible cryptocurrency tokens and memecoins are neither securities nor commodities, but rather should be treated as “collectibles” like baseball cards and stamps. no market regulations that apply to collectibles under U.S. securities law. Hall said Sacks’ comments “suggest a viewpoint that it would not be appropriate to regulate these things the way we regulate securities.”The new administration’s position makes sense considering that the Trump family is deeply and personally invested in a number of recent memecoin ventures that have attracted billions from investors. President Trump and First Lady Melania Trump each launched their own vanity memecoins this month, dubbed  and .reported Thursday the market capitalization of $TRUMP stood at about $7 billion, down from a peak of near $15 billion, while $MELANIA is hovering somewhere in the $460 million mark. Just two months before the 2024 election, Trump’s three sons debuted a cryptocurrency token called .Despite maintaining a considerable personal stake in how cryptocurrency is regulated, Trump issued an executive order on January 23 calling for a working group to be chaired by Sacks that would develop “a federal regulatory framework governing digital assets, including stablecoins,” and evaluate the creation of a “strategic national digital assets stockpile.”Translation: Using taxpayer dollars to prop up the speculative, volatile, and highly risky cryptocurrency industry, which has been marked by endless scams, rug-pulls, 8-figure cyber heists, rampant fraud, and unrestrained innovations in money laundering.WEAPONIZATION & DISINFORMATIONPrior to the election, President Trump frequently vowed to use a second term to exact retribution against his perceived enemies. Part of that promise materialized in an executive order Trump issued last week titled “Ending the Weaponization of the Federal Government,” which decried “an unprecedented, third-world weaponization of prosecutorial power to upend the democratic process,” in the prosecution of more than 1,500 people who invaded the U.S. Capitol on Jan. 6, 2021.On Jan. 21, Trump commuted the sentences of several leaders of the Proud Boys and Oath Keepers who were convicted of seditious conspiracy. He also issued “a full, complete and unconditional pardon to all other individuals convicted of offenses related to events that occurred at or near the United States Capitol on January 6, 2021,” which include those who assaulted law enforcement officers.The New York Times reports “the language of the document suggests — but does not explicitly state — that the Trump administration review will examine the actions of local district attorneys or state officials, such as the district attorneys in Manhattan or Fulton County, Ga., or the New York attorney general, all of whom filed cases against President Trump.”“Over the last 4 years, the previous administration trampled free speech rights by censoring Americans’ speech on online platforms, often by exerting substantial coercive pressure on third parties, such as social media companies, to moderate, deplatform, or otherwise suppress speech that the Federal Government did not approve,” the Trump administration alleged. “Under the guise of combatting ‘misinformation,’ ‘disinformation,’ and ‘malinformation,’ the Federal Government infringed on the constitutionally protected speech rights of American citizens across the United States in a manner that advanced the Government’s preferred narrative about significant matters of public debate.”Both of these executive orders have potential implications for security, privacy and civil liberties activists who have sought to track conspiracy theories and raise awareness about disinformation efforts on social media coming from U.S. adversaries.In the wake of the 2020 election, Republicans created the House Judiciary Committee’s Select Subcommittee on the Weaponization of the Federal Government. Led by GOP  of Ohio, the committee’s stated purpose was to investigate alleged collusion between the Biden administration and tech companies to unconstitutionally shut down political speech.The GOP committee focused much of its ire at members of the short-lived Disinformation Governance Board, an advisory board to DHS created in 2022 (the “combating misinformation, disinformation, and malinformation” quote from Trump’s executive order is a reference to the board’s stated mission). Conservative groups seized on social media posts made by the director of the board, who resigned after facing death threats. The board was dissolved by DHS soon after.In his first administration, President Trump created a special prosecutor to probe the origins of the FBI’s investigation into possible collusion between the Trump campaign and Russian operatives seeking to influence the 2016 election. Part of that inquiry examined evidence gathered by some of the world’s most renowned cybersecurity experts who identified frequent and unexplained communications between an email server used by the  and , one of Russia’s largest financial institutions.Trump’s Special Prosecutor  later subpoenaed and/or deposed dozens of security experts who’d collected, viewed or merely commented on the data. Similar harassment and deposition demands would come from lawyers for Alfa Bank. Durham ultimately indicted , the former federal cybercrime prosecutor who reported the oddity to the FBI. Sussman was acquitted in May 2022. Last week, Trump appointed Durham to lead the U.S. attorney’s office in Brooklyn, NY. at notes that while the executive actions are ominous, they are also vague, and could conceivably generate either a campaign of retaliation, or nothing at all.“The two orders establish that there will be investigations but leave open the questions of what kind of investigations, what will be investigated, how long this will take, and what the consequences might be,” Jurecic wrote. “It is difficult to draw firm conclusions as to what to expect. Whether this ambiguity is intentional or the result of sloppiness or disagreement within Trump’s team, it has at least one immediate advantage as far as the president is concerned: generating fear among the broad universe of potential subjects of those investigations.”On Friday, Trump moved to fire at least 17 inspectors general, the government watchdogs who conduct audits and investigations of executive branch actions, and who often uncover instances of government waste, fraud and abuse. Lawfare’s  argues that the removals are probably legal even though Trump defied a 2022 law that required congressional notice of the terminations, which Trump did not give.“Trump probably acted lawfully, I think, because the notice requirement is probably unconstitutional,” Goldsmith wrote. “The real bite in the 2022 law, however, comes in the limitations it places on Trump’s power  the terminated IGs—limitations that I believe are constitutional. This aspect of the law will make it hard, but not impossible, for Trump to put loyalists atop the dozens of vacant IG offices around the executive branch. The ultimate fate of IG independence during Trump 2.0, however, depends less on legal protections than on whether Congress, which traditionally protects IGs, stands up for them now. Don’t hold your breath.”Among the many Biden administration executive orders revoked by President Trump last week was an action from December 2021 establishing the United States Council on Transnational Organized Crime, which is charged with advising the White House on a range of criminal activities, including drug and weapons trafficking, migrant smuggling, human trafficking, cybercrime, intellectual property theft, money laundering, wildlife and timber trafficking, illegal fishing, and illegal mining.So far, the White House doesn’t appear to have revoked an executive order that former President Biden issued less than a week before President Trump took office. On Jan. 16, 2025, Biden released a directive that focused on improving the security of federal agencies and contractors, and giving the government more power to sanction the hackers who target critical infrastructure.]]></content:encoded></item><item><title>UK Council Sells Assets To Fund Ballooning $50 Million Oracle Project</title><link>https://news.slashdot.org/story/25/01/28/004213/uk-council-sells-assets-to-fund-ballooning-50-million-oracle-project?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Tue, 28 Jan 2025 00:20:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[West Sussex County Council is using up to $31 million from the sale of capital assets to fund an Oracle-based transformation project, originally budgeted at $3.2 million but now expected to cost nearly $50 million due to delays and cost overruns. The project, intended to replace a 20-year-old SAP system with a SaaS-based HR and finance system, has faced multiple setbacks, renegotiated contracts, and a new systems integrator, with completion now pushed to December 2025. The Register reports: West Sussex County Council is taking advantage of the so-called "flexible use of capital receipts scheme" introduced in 2016 by the UK government to allow councils to use money from the sale of assets such as land, offices, and housing to fund projects that result in ongoing revenue savings. An example of the asset disposals that might contribute to the project -- set to see the council move off a 20-year-old SAP system -- comes from the sale of a former fire station in Horley, advertised for $3.1 million.
 
Meanwhile, the delays to the project, which began in November 2019, forced the council to renegotiate its terms with Oracle, at a cost of $3 million. The council had expected the new SaaS-based HR and finance system to go live in 2021, and signed a five-year license agreement until June 2025. The plans to go live were put back to 2023, and in the spring of 2024 delayed again until December 2025. According to council documents published this week [PDF], it has "approved the variation of the contract with Oracle Corporation UK Limited" to cover the period from June 2025 to June 2028 and an option to extend again to the period June 2028 to 2030. "The total value of the proposed variation is $2.96 million if the full term of the extension periods are taken," the council said.]]></content:encoded></item><item><title>Facebook Flags Linux Topics As &apos;Cybersecurity Threats&apos;</title><link>https://linux.slashdot.org/story/25/01/27/2120230/facebook-flags-linux-topics-as-cybersecurity-threats?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>slashdot</category><category>linux</category><pubDate>Mon, 27 Jan 2025 23:00:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Facebook has banned posts mentioning Linux-related topics, with the popular Linux news and discussion site, DistroWatch, at the center of the controversy. Tom's Hardware reports: A post on the site claims, "Facebook's internal policy makers decided that Linux is malware and labeled groups associated with Linux as being 'cybersecurity threats.' We tried to post some blurb about distrowatch.com on Facebook and can confirm that it was barred with a message citing Community Standards. DistroWatch says that the Facebook ban took effect on January 19. Readers have reported difficulty posting links to the site on this social media platform. Moreover, some have told DistroWatch that their Facebook accounts have been locked or limited after sharing posts mentioning Linux topics.
 
If you're wondering if there might be something specific to DistroWatch.com, something on the site that the owners/operators perhaps don't even know about, for example, then it seems pretty safe to rule out such a possibility. Reports show that "multiple groups associated with Linux and Linux discussions have either been shut down or had many of their posts removed." However, we tested a few other Facebook posts with mentions of Linux, and they didn't get blocked immediately. Copenhagen-hosted DistroWatch says it has tried to appeal against the Community Standards-triggered ban. However, they say that a Facebook representative said that Linux topics would remain on the cybersecurity filter. The DistroWatch writer subsequently got their Facebook account locked... DistroWatch points out the irony at play here: "Facebook runs much of its infrastructure on Linux and often posts job ads looking for Linux developers." 

UPDATE: Facebook has admited they made a mistake and stopped blocking the posts.]]></content:encoded></item><item><title>DeepSeek-R1&apos;s epic pull request (News)</title><link>https://changelog.com/news/129</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/news/129/changelog-news-129.mp3" length="" type=""/><pubDate>Mon, 27 Jan 2025 20:30:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Xuan-Son Nguyen opened a low-level code PR written 99% by DeepSeek-R1, Adam Wathan announces the release of Tailwind CSS 4.0, Matheus Lima opens up the Computer Science history books to create list of influential papers, Namanyay Goel thinks AI is creating a generation of illiterate programmers & Russell Baylis shares what he’s learned about optimizing WFH lighting to reduce eye strain.Changelog++ members support our work, get closer to the metal, and make the ads disappear. Join today!]]></content:encoded></item><item><title>Microsoft Takes on MongoDB with PostgreSQL-Based Document Database</title><link>https://developers.slashdot.org/story/25/01/27/2023232/microsoft-takes-on-mongodb-with-postgresql-based-document-database?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Mon, 27 Jan 2025 20:22:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[Microsoft has launched an open-source document database platform built on PostgreSQL, partnering with FerretDB as a front-end interface. The solution includes two PostgreSQL extensions: pg_documentdb_core for BSON optimization and pg_documentdb_api for data operations. 

FerretDB CEO Peter Farkas said the integration with Microsoft's DocumentDB extension has improved performance twentyfold for certain workloads in FerretDB 2.0. The platform carries no commercial licensing fees or usage restrictions under its MIT license, according to Microsoft.]]></content:encoded></item><item><title>The Illustrated DeepSeek-R1</title><link>https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1</link><author>Jay Alammar</author><category>dev</category><category>ai</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F623a9dbf-c76e-438c-ba69-43ae9613ebbe_2930x1496.png" length="" type=""/><pubDate>Mon, 27 Jan 2025 20:15:46 +0000</pubDate><source url="https://newsletter.languagemodels.co/">LLM.newsletter</source><content:encoded><![CDATA[DeepSeek-R1 is the latest resounding beat in the steady drumroll of AI progress. For the ML R&D community, it is a major release for reasons including: It is an open weights model with smaller, distilled versions and It shares and reflects upon a training method to reproduce a reasoning model like OpenAI O1. In this post, we’ll see how it was built.: Chinese, Korean, Turkish (Feel free to translate the post to your language and send me the link to add here)Recap: How LLMs are trainedDeepSeek-R1 Training Recipe1- Long chains of reasoning SFT Data2- An interim high-quality reasoning LLM (but worse at non-reasoning tasks).3- Creating reasoning models with large-scale reinforcement learning (RL) 3.1- Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero)3.2- Creating SFT reasoning data with the interim reasoning model3.3- General RL training phase Recap: How LLMs are trainedJust like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.The following figure, from Chapter 12 of our book shows the general recipe of creating a high-quality LLM over three steps:1) The language modeling step where we train the model to predict the next word using a massive amount of web data. This step results in a base model.2) a supervised fine-tuning step that makes the model more useful in following instructions and answering questions. This step results in an instruction tuned model or a supervised fine -tuning / SFT model.3) and finally a preference tuning step which further polishes its behaviors and aligns to human preferences, resulting in the final preference-tuned LLM which you interact with on playgrounds and apps.DeepSeek-R1 Training RecipeDeepSeek-R1 follows this general recipe. The details of that first step come from a previous paper for the DeepSeek-V3 model. R1 uses the  model (not the final DeepSeek-v3 model) from that previous paper, and still goes through an SFT and preference tuning steps, but the details of how it does them are what's different.There are three special things to highlight in the R1 creation process.1- Long chains of reasoning SFT DataThis is a large number of long chain-of-thought reasoning examples (600,000 of them). These are very hard to come by and very expensive to label with humans at this scale. Which is why the process to create them is the second special thing to highlight2- An interim high-quality reasoning LLM (but worse at non-reasoning tasks).This data is created by a precursor to R1, an unnamed sibling which specializes in reasoning. This sibling is inspired by a third model called (that we’ll discuss shortly). It is significant not because it’s a great LLM to use, but because creating it required so little labeled data alongside large-scale reinforcement learning resulting in a model that excels at solving reasoning problems. The outputs of this unnamed specialist reasoning model can then be used to train a more general model that can also do other, non-reasoning tasks, to the level users expect from an LLM.3- Creating reasoning models with large-scale reinforcement learning (RL) This happens in two steps:3.1 Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero)Here, RL is used to create the interim reasoning model. The model is then used to  generate the SFT reasoning examples. But what makes creating this model possible is an earlier experiment creating an earlier model called .R1-Zero is special because it is able to excel at reasoning tasks without having a labeled SFT training set. Its training goes directly from a pre-trained base model through a RL training process (no SFT step). It does this so well that it’s competitive with o1.This is significant because data has always been the fuel for ML model capability. How can this model depart from that history? This points to two things:1- Modern base models have crossed a certain threshold of quality and capability (this base model was trained on 14.8 trillion high-quality tokens).2- Reasoning problems, in contrast to general chat or writing requests, can be automatically verified or labeled. Let’s show this with an example. Example: Automatic Verification of a Reasoning ProblemThis can be a prompt/question that is a part of this RL training step:Write python code that takes a list of numbers, returns them in a sorted order, but also adds 42 at the start.A question like this lends itself to many ways of automatic verification. Say we present this this to the model being trained, and it generates a completion:A software linter can check if the completion is proper python code or notWe can execute the python code to see if it even runsOther modern coding LLMs can create unit tests to verify the desired behavior (without being reasoning experts themselves). We can go even one step further and measure execution time and make the training process prefer more performant solutions over other solutions — even if they’re correct python programs that solve the issue.We can present a question like this to the model in a training step, and generate multiple possible solutions.We can automatically check (with no human intervention) and see that the first completion is not even code. The second one is code, but is not python code. The third is a possible solution, but fails the unit tests, and the forth is a correct solution.These are all signals that can be directly used to improve the model. This is of course done over many examples (in mini-batches) and over successive training steps.These reward signals and model updates are how the model continues improving on tasks over the RL training process as seen in Figure 2 from the paper.Corresponding with the improvement of this capability is the length of the generated response, where the model generates more thinking tokens to process the problem.This process is useful, but the R1-Zero model, despite scoring high on these reasoning problems, confronts other issues that make it less usable than desired. Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing.R1 is meant to be a more usable model. So instead of relying completely on the RL process, it is used in two places as we’ve mentioned earlier in this section:1- creating an interim reasoning model to generate SFT data points2- Training the R1 model to improve on reasoning and non-reasoning problems (using other types of verifiers)3.2 Creating SFT reasoning data with the interim reasoning modelTo make the interim reasoning model more useful, it goes through an supervised fine-tuning (SFT) training step on a few thousand examples of reasoning problems (some of which are generated and filtered from R1-Zero). The paper refers to this as cold start data”Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators.But wait, if we have this data, then why are we relying on the RL process? It’s because of the scale of the data. This dataset might be 5,000 examples (which is possible to source), but to train R1, 600,000 examples were needed. This interim model bridges that gap and allows to synthetically generate that extremely valuable data.If you’re new to the concept of Supervised Fine-Tuning (SFT), that is the process that presents the model with training examples in the form of prompt and correct completion. This figure from chapter 12 shows a couple of SFT training examples:3.3 General RL training phase This enables R1 to excel at reasoning as well as other non-reasoning tasks. The process is similar to the the RL process we’ve seen before. But since it extends to non-reasoning applications, it utilizes a helpfulnes and a safety reward model (not unlike the Llama models) for prompts that belong to these applications.In terms of model dimension size and other hyperparameters, they look like this:More details about the model architecture are presented in their two earlier papers:With this, you should now have the main intuitions to wrap your head around the DeepSeek-R1 model. Other suggested resources are:]]></content:encoded></item><item><title>Big Tech in panic mode... Did DeepSeek R1 just pop the AI bubble?</title><link>https://www.youtube.com/watch?v=Nl7aCUsWykg</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/Nl7aCUsWykg?version=3" length="" type=""/><pubDate>Mon, 27 Jan 2025 19:27:11 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Chip stocks like Nvidia are in trouble after the DeepSeek R1 AI model has proven that it is possible to train and run state-of-the-art reasoning models with minimal hardware. Let's find out why China's latest AI model has big tech and wall street in panic mode. 

#tech #ai #thecodereport

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

DeepSeek First Look https://youtu.be/-2k1rcRzsLA
Nvidia CUDA in 100 Seconds https://youtu.be/pPStdjuYzSI

📚 Chapters

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Why is DeepSeek causing stocks to drop? 
- Latest trends in artificial intelligence
- How do AI companies make money?]]></content:encoded></item><item><title>A long, costly road ahead for customers abandoning Broadcom’s VMware</title><link>https://arstechnica.com/information-technology/2025/01/a-long-costly-road-ahead-for-customers-abandoning-broadcoms-vmware/</link><author>Scharon Harding</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/01/GettyImages-2177158675-1024x648.jpg" length="" type=""/><pubDate>Mon, 27 Jan 2025 18:36:31 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Now, over a year after the acquisition, the pressure is on for customers to start committing to a VMware subscription, forego VMware support, or move on from VMware technologies. The decision is complex, with long-term implications no matter which way a customer goes.Ars Technica spoke with an IT vendor manager who has been using VMware's vSphere since the early 2000s. The employee, who works for a global food manufacturing firm with about 5,500 employees, asked to keep their name and company anonymous due to privacy concerns for the business.]]></content:encoded></item><item><title>It&apos;s Copyright Week 2025: Join Us in the Fight for Better Copyright Law and Policy</title><link>https://www.eff.org/deeplinks/2025/01/its-copyright-week-2025-join-us-fight-better-copyright-law-and-policy</link><author>Katharine Trendacosta</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/fixcopyright-graphic-banner.jpg" length="" type=""/><pubDate>Mon, 27 Jan 2025 17:45:09 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Monday: Copyright Policy Should Be Made in the Open With Input From Everyone:Tuesday: Copyright Enforcement as a Tool of Censorship:Wednesday: Device and Digital Ownership:Thursday: The Preservation and Sharing of Information and Culture:Friday: Free Expression and Fair Use:]]></content:encoded></item><item><title>ASL Container</title><link>https://www.youtube.com/watch?v=meUtsFU7ndo</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/meUtsFU7ndo?version=3" length="" type=""/><pubDate>Mon, 27 Jan 2025 17:27:18 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ASL Autoscaling 2</title><link>https://www.youtube.com/watch?v=cGONmC1smaM</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/cGONmC1smaM?version=3" length="" type=""/><pubDate>Mon, 27 Jan 2025 17:11:27 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ASL Reliability</title><link>https://www.youtube.com/watch?v=pQluo2FG2eA</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/pQluo2FG2eA?version=3" length="" type=""/><pubDate>Mon, 27 Jan 2025 17:10:55 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ASL Serverless 2</title><link>https://www.youtube.com/watch?v=rbyBgXqCN2k</link><author>CNCF [Cloud Native Computing Foundation]</author><category>k8s</category><category>devops</category><category>video</category><enclosure url="https://www.youtube.com/v/rbyBgXqCN2k?version=3" length="" type=""/><pubDate>Mon, 27 Jan 2025 17:10:26 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">CNCF</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Apple&apos;s storage pricing is INSANE! Do this instead.</title><link>https://www.youtube.com/watch?v=eLtE2kMTVOQ</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/eLtE2kMTVOQ?version=3" length="" type=""/><pubDate>Mon, 27 Jan 2025 17:00:51 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[You CAN upgrade your M4 Mac mini's storage to 2TB youself. It's not hard.

Resources mentioned in this video (for Amazon links, I earn for qualifying purchases):

  - Expand Mac mini 2TB Upgrade Kit: https://expandmacmini.com/product/ssd-m4-2tb/
  - M4-SSD 2TB Upgrade Kit: https://store.m4-ssd.com/products/2tb-drive-for-mac-mini-m4
  - iFixit Mac mini SSD replacement guide: https://www.ifixit.com/Guide/Mac+mini+(2024)+SSD+Replacement/180199
  - dosdude1's M4 Mac mini SSD upgrade video: https://www.youtube.com/watch?v=cJPXLE9uPr8
  - UGREEN 40 Gbps Thunderbolt 4 NVMe Enclosure: https://amzn.to/40Omjyf
  - UGREEN 10 Gbps USB-C NVMe Enclosure: https://amzn.to/3CC1Kf4
  - Sabrent 2M Thunderbolt 4 Cable: https://amzn.to/3PT7YKS

Fyde Innovations (makers of the Expand Mac mini kit featured in this video) provided the kit for review and testing. They did not send any money, nor have any say in the content of this video (or any ability to review it before publication). See my sponsorship policies: https://github.com/geerlingguy/youtube?tab=readme-ov-file#sponsorships

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff

Contents:

00:00 - Apple charges HOW much?!
01:01 - I was bamboozled. Don't be me
03:30 - Cracking the case
05:17 - Going deeper
07:29 - Echoes of the past
09:10 - Apple's DFU
12:54 - Cheaper AND better performance?]]></content:encoded></item><item><title>Navigating Machine Learning Careers: Insights from Meta to Consulting // Ilya Reznik // #286</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/Navigating-Machine-Learning-Careers-Insights-from-Meta-to-Consulting--Ilya-Reznik--286-e2tui16</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/97519078/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-24%2F393657905-44100-2-fda7c9be7256c.mp3" length="" type=""/><pubDate>Mon, 27 Jan 2025 16:09:49 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[In his 13 years of software engineering,  has specialized in commercializing machine learning solutions and building robust ML platforms. He's held technical lead and staff engineering roles at premier firms like Adobe, Twitter, and Meta. Currently, Ilya channels his expertise into his travel startup, Jaunt, while consulting and advising emerging startups.

Navigating Machine Learning Careers: Insights from Meta to Consulting // MLOps Podcast #286 with Ilya Reznik, ML Engineering Thought Leader at Instructed Machines, LLC.

// Abstract
Ilya Reznik's insights into machine learning and career development within the field. With over 13 years of experience at leading tech companies such as Meta, Adobe, and Twitter, Ilya emphasizes the limitations of traditional model fine-tuning methods. He advocates for alternatives like prompt engineering and knowledge retrieval, highlighting their potential to enhance AI performance without the drawbacks associated with fine-tuning.

Ilya's recent discussions at the NeurIPS conference reflect a shift towards practical applications of Transformer models and innovative strategies like curriculum learning. Additionally, he shares valuable perspectives on navigating career progression in tech, offering guidance for aspiring ML engineers aiming for senior roles. His narrative serves as a blend of technical expertise and practical career advice, making it a significant resource for professionals in the AI domain.

// Bio
Ilya has navigated a diverse career path since 2011, transitioning from physicist to software engineer, data scientist, ML engineer, and now content creator. He is passionate about helping ML engineers advance their careers and making AI more impactful and beneficial for society.

Previously, Ilya was a technical lead at Meta, where he contributed to 12% of the company’s revenue and managed approximately 30 production ML models. He also worked at Twitter, overseeing offline model evaluation, and at Adobe, where his team was responsible for all intelligent services within Adobe Analytics.

Based in Salt Lake City, Ilya enjoys the outdoors, tinkering with Arduino electronics, and, most importantly, spending time with his family.

// MLOps Swag/Merch
https://shop.mlops.community/

// Related Links
Website: mlepath.com
 
--------------- ✌️Connect With Us ✌️ -------------
Join our slack community: https://go.mlops.community/slack
Follow us on Twitter: @mlopscommunity
Sign up for the next meetup: https://go.mlops.community/register
Catch all episodes, blogs, newsletters, and more: https://mlops.community/

Connect with Demetrios on LinkedIn: https://www.linkedin.com/in/dpbrinkm/
Connect with Ilya on LinkedIn: https://www.linkedin.com/in/ibreznik/]]></content:encoded></item><item><title>Revisiting CSS Multi-Column Layout</title><link>https://css-tricks.com/revisiting-css-multi-column-layout/</link><author>Andy Clarke</author><category>frontend</category><pubDate>Mon, 27 Jan 2025 15:35:44 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[Honestly, it’s difficult for me to come to terms with, but almost 20 years have passed since I wrote my first book, . In it, I explained how and why to use what was the then-emerging Multi-Column Layoutmodule.Perhaps because, before the web, I’d worked in print, I was over-excited at the prospect of dividing content into columns without needing extra markup purely there for presentation. I’ve used Multi-Column Layout regularly ever since. Yet, CSS Columns remains one of the most underused CSS layout tools. I wonder why that is?Holes in the specificationFor a long time, there were, and still are, plenty of holes in Multi-Column Layout. As Rachel Andrew — now a specification editor — noted in her article five years ago:“The column boxes created when you use one of the column properties can’t be targeted. You can’t address them with JavaScript, nor can you style an individual box to give it a background colour or adjust the padding and margins. All of the column boxes will be the same size. The only thing you can do is add a rule between columns.”She’s right. And that’s  true. You can’t style columns, for example, by alternating background colours using some sort of  pseudo-class selector. You can add a  between columns using  values like , , and , and who can forget those evergreen  and  styles? But you can’t apply  values to a , which seems odd as they were introduced at roughly the same time. The Multi-Column Layout is imperfect, and there’s plenty I wish it could do in the future, but that doesn’t explain why most people ignore what it can do today.Patchy browser implementation for a long timeLegacy browsers simply ignored the column properties they couldn’t process. But, when Multi-Column Layout was first launched, most designers and developers had yet to accept that websites needn’t look the same in every browser.Early on, support for Multi-Column Layout was patchy. However, browsers caught up over time, and although there are still discrepancies — especially in controlling content breaks — Multi-Column Layout has now been implemented widely. Yet, for some reason, many designers and developers I speak to feel that CSS Columns remain broken. Yes, there’s plenty that browser makers should do to improve their implementations, but that shouldn’t prevent people from using the solid parts today.Maybe the main reason designers and developers haven’t embraced Multi-Column Layout as they have CSS Grid and Flexbox isn’t in the specification or its implementation but in its usability. Rachel pointed this out in her article:“One reason we don’t see multicol used much on the web is that it would be very easy to end up with a reading experience which made the reader scroll in the block dimension. That would mean scrolling up and down vertically for those of us using English or another vertical writing mode. This is not a good reading experience!”That’s true. No one would enjoy repeatedly scrolling up and down to read a long passage of content set in columns. She went on:“Neither of these things is ideal, and using multicol on the web is something we need to think about very carefully in terms of the amount of content we might be aiming to flow into our columns.”But, let’s face it, thinking very carefully is what designers and developers should always be doing.Sure, if you’re dumb enough to dump a large amount of content into columns without thinking about its design, you’ll end up serving readers a poor experience. But why would you do that when headlines, images, and quotes can span columns and reset the column flow, instantly improving readability? Add to that container queries and newer unit values for text sizing, and there really isn’t a reason to avoid using Multi-Column Layout any longer.A brief refresher on properties and valuesLet’s run through a refresher. There are two ways to flow content into multiple columns; first, by defining the number of columns you need using the  property:Second, and often best, is specifying the column width, leaving a browser to decide how many columns will fit along the inline axis. For example, I’m using  to specify that my columns are over . A browser creates as many  columns as possible to fit and then shares any remaining space between them.Then, there is the gutter (or ) between columns, which you can specify using any length unit. I prefer using rem units to maintain the gutters’ relationship to the text size, but if your gutters need to be , you can leave this out, as that’s a browser’s default gap.The final column property is that divider (or ) to the gutters, which adds visual separation between columns. Again, you can set a thickness and use  values like , , and .These examples will be seen whenever you encounter a Multi-Column Layout tutorial, including CSS-Tricks’ own Almanac. The Multi-Column Layout syntax is one of the simplest in the suite of CSS layout tools, which is another reason why there are few reasons not to use it.Multi-Column Layout is even more relevant todayWhen I wrote  and first explained the emerging Multi-Column Layout, there were no rem or viewport units, no  or other advanced selectors, no container queries, and no routine use of media queries because responsive design hadn’t been invented.We didn’t have  or  for adjusting text sizes, and there was no CSS Grid or Flexible Box Layout for precise control over a layout. Now we do, and all these properties help to make Multi-Column Layout even more relevant today.Now, you can use rem or viewport units combined with  and  to adapt the text size inside CSS Columns. You can use  to specify when columns are created, depending on the type of content they contain. Or you might use container queries to implement several columns only when a container is large enough to display them. Of course, you can also combine a Multi-Column Layout with CSS Grid or Flexible Box Layout for even more imaginative layout designs.Using Multi-Column Layout todayMy challenge was to implement a flexible article layout without media queries which adapts not only to screen size but also whether or not a  is present. To improve the readability of running text in what would potentially be too-long lines, it should be set in columns to narrow the measure. And, as a final touch, the text size should adapt to the width of the container, not the viewport.The HTML for this layout is rudimentary. One , one , and one  (or not:)<section>
  <main>
    <h1>About Patty</h1>
    <p>…</p>
  </main>

  <figure>
    <img>
  </figure>
</section>I started by adding Multi-Column Layout styles to the  element using the  property to set the width of each column to  (characters). The  and automatic inline margins reduce the content width and center it in the viewport:main {
  margin-inline: auto;
  max-width: 100ch;
  column-width: 40ch;
  column-gap: 3rem;
  column-rule: .5px solid #98838F;
}Next, I applied a flexible box layout to the  only if it  a direct descendant which is a :section:has(> figure) {
  display: flex;
  flex-wrap: wrap;
  gap: 0 3rem;
}This next min-width: min(100%, 30rem) — applied to both the  and  — is a combination of the  property and the  CSS function. The  function allows you to specify two or more values, and a browser will choose the smallest value from them. This is incredibly useful for responsive layouts where you want to control the size of an element based on different conditions:section:has(> figure) main {
  flex: 1;
  margin-inline: 0;
  min-width: min(100%, 30rem);
}

section:has(> figure) figure {
  flex: 4;
  min-width: min(100%, 30rem);
}What’s efficient about this implementation is that Multi-Column Layout styles are applied throughout, with no need for media queries to switch them on or off.Adjusting text size in relation to column width helps improve readability. This has only recently become easy to implement with the introduction of container queries, their associated values including , , , and . And the  function. Fortunately, you don’t have to work out these text sizes manually as ClearLeft’s Utopia will do the job for you.My headlines and paragraph sizes are clamped to their minimum and maximum rem sizes and between them text is fluid depending on their container’s inline size:h1 { font-size: clamp(5.6526rem, 5.4068rem + 1.2288cqi, 6.3592rem); }

h2 { font-size: clamp(1.9994rem, 1.9125rem + 0.4347cqi, 2.2493rem); }

p { font-size: clamp(1rem, 0.9565rem + 0.2174cqi, 1.125rem); }So, to specify the  as the container on which those text sizes are based, I applied a container query for its inline size:main {
  container-type: inline-size;
}Open the final result in a desktop browser, when you’re in front of one. It’s a flexible article layout without media queries which adapts to screen size and the presence of a . Multi-Column Layout sets text in columns to narrow the measure and the text size adapts to the width of its container, not the viewport.Modern CSS is solving many prior problemsAlmost every article I’ve ever read about Multi-Column Layout focuses on its flaws, especially usability. CSS-Tricks’ own Geoff Graham even mentioned the scrolling up and down issue when he asked, “When Do You Use CSS Columns?”“But an entire long-form article split into columns? I love it in newspapers but am hesitant to scroll down a webpage to read one column, only to scroll back up to do it again.”Fortunately, the  property — which enables headlines, images, and quotes to span columns, resets the column flow, and instantly improves readability — now has solid support in browsers:h1, h2, blockquote {
  column-span: all; 
}But the solution to the scrolling up and down issue isn’t purely technical. It also requires content design. This means that content creators and designers must think carefully about the frequency and type of spanning elements, dividing a Multi-Column Layout into shallower sections, reducing the need to scroll and improving someone’s reading experience.Another prior problem was preventing headlines from becoming detached from their content and figures, dividing their images and captions between columns. Thankfully, the  property now also has widespread support, so orphaned images and captions are now a thing of the past:figure {
  break-after: column;
}Open this final example in a desktop browser:You should take a fresh look at Multi-Column LayoutMulti-Column Layout isn’t a shiny new tool. In fact, it remains one of the most underused layout tools in CSS. It’s had, and still has, plenty of problems, but they haven’t reduced its usefulness or its ability to add an extra level of refinement to a product or website’s design. Whether you haven’t used Multi-Column Layout in a while or maybe have never tried it, now’s the time to take a fresh look at Multi-Column Layout.]]></content:encoded></item><item><title>The role of AI in closing the skills gap in the business world</title><link>https://www.datasciencecentral.com/the-role-of-ai-in-closing-the-skills-gap-in-the-business-world/</link><author>Dan Wilson</author><category>dev</category><category>ai</category><pubDate>Mon, 27 Jan 2025 15:35:39 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Introduction The accelerating pace of technological advancements has created an unprecedented paradox in the business world. While automation, artificial intelligence (AI), and machine learning (ML) have introduced significant efficiencies, they have also contributed to a widening skills gap, the disparity between the skills that businesses require and the skills that the current workforce possesses. As… Read More »]]></content:encoded></item><item><title>#418 I&apos;m a tea pot</title><link>https://pythonbytes.fm/episodes/show/418/im-a-tea-pot</link><author></author><category>Python bytes</category><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://pythonbytes.fm/episodes/download/418/im-a-tea-pot.mp3" length="" type=""/><pubDate>Mon, 27 Jan 2025 08:00:00 +0000</pubDate><source url="https://pythonbytes.fm/">Python Bytes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Zig Cross Compiling</title><link>https://zig.news/david_vanderson/zig-cross-compiling-37en</link><author>David Vanderson</author><category>dev</category><category>zig</category><pubDate>Mon, 27 Jan 2025 02:24:08 +0000</pubDate><source url="https://zig.news/">Zig News</source><content:encoded><![CDATA[My first cross-compiling attempt with zig was great.Both run Linux, and I had already successfully compiled on the phone itself, but the edit/debug cycle was too long.So first up, find out what the phone target is, by running  which produces tons of output, but at the bottom: "native": {
  "triple": "aarch64-linux.4.19.325...4.19.325-gnu.2.39",
  "cpu": {
   "arch": "aarch64",
   "name": "cortex_a78",
$zig build -Dtarget=aarch64-linux.4.19.325...4.19.325-gnu.2.39
/home/dvanderson/.cache/zig/p/122004e82d4a0c61a9c414539c1f87bb125cb2b293573af77b153ea3903cb209b65b/library/aesce.c:182:18: 
error: always_inline function 'vaesimcq_u8' requires target feature 'aes', 
but would be inlined into function 'mbedtls_aesce_inverse_key' 
that is compiled without support for 'aes'
It looks like I need to also pass some , so again using the output of  from above:$zig build -Dtarget=aarch64-linux.4.19.325...4.19.325-gnu.2.39 -Dcpu=cortex_a78
Note that this is all with zig 0.13, and there are a few issues that might change how to pass the target/cpu/libc/abi information in the future.]]></content:encoded></item><item><title>New Michigan Law Requires High Schools to Offer CS Classes</title><link>https://news.slashdot.org/story/25/01/26/1547204/new-michigan-law-requires-high-schools-to-offer-cs-classes?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 26 Jan 2025 17:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[The state of Michigan will now require each public high school in the state to offer at least one computer science course to its students. "This bill aligns Michigan with a majority of the country," according to the state's announcement, which says the bill "advances technological literacy" and ensures their students "are well-equipped with the critical thinking skills necessary for success in the workforce." 
Slashdot reader theodp writes:



From the Michigan House Fiscal Agency Analysis: "Supporters of the bill say that increasing access to computer science courses for students in schools should be a priority of the state in order to ensure that students can compete for the types of jobs that have good pay and will be needed in the coming decades." 
That analysis goes on to report that testifying in favor of the bill were tech-giant backed nonprofit Code.org (Microsoft is a $30 million Code.org donor), Amazon and AWS (Amazon is a $30+ million Code.org donor), the tech-supported Computer Science Teachers Association (CSTA), and the lobbying organization TechNet, whose members include Amazon, Apple, Google, Meta, and OpenAI). 
It's not clear how many high schools in Michigan are already teaching CS courses, but this still raises a popular question for discussion. Should high schools be required to teach at least one CS course?]]></content:encoded></item><item><title>Linux 6.14 Brings Some Systems Faster Suspend and Resume</title><link>https://linux.slashdot.org/story/25/01/25/2343225/linux-614-brings-some-systems-faster-suspend-and-resume?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>slashdot</category><category>linux</category><pubDate>Sun, 26 Jan 2025 16:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Amid the ongoing Linux 6.14 kernel development cycle, Phoronix spotted a pull request for ACPI updates which "will allow for faster suspend and resume cycles on some systems." 

Wikipedia defines ACPI as "an open standard that operating systems can use to discover and configure computer hardware components" for things like power management and putting unused hardware components to sleep. Phoronix reports:

The ACPI change worth highlighting for Linux 6.14 is switching from msleep() to usleep_range() within the acpi_os_sleep() call in the kernel. This reduces spurious sleep time due to timer inaccuracy. Linux ACPI/PM maintainer Rafael Wysocki of Intel who authored this change noted that it could "spectacularly" reduce the duration of system suspend and resume transitions on some systems... 

Rafael explained in the patch making the sleep change: 

 "The extra delay added by msleep() to the sleep time value passed to it can be significant, roughly between 1.5 ns on systems with HZ = 1000 and as much as 15 ms on systems with HZ = 100, which is hardly acceptable, at least for small sleep time values." 
One 2022 bug report complained a Dell XPS 13 using Thunderbolt took "a full 8 seconds to suspend and a full 8 seconds to resume even though no physical devices are connected." In November an Intel engineer posted on the kernel mailing list that the fix gave a Dell XPS 13 a 42% improvement in kernel resume time (from 1943ms to 1127ms).]]></content:encoded></item><item><title>Leverage a value-creation framework to unleash GenAI innovation</title><link>https://www.datasciencecentral.com/leverage-a-value-creation-framework-to-unleash-genai-innovation/</link><author>Bill Schmarzo</author><category>dev</category><category>ai</category><pubDate>Sun, 26 Jan 2025 12:57:09 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Note: there is a gift for those who read to the end of this blog. “If you don’t know where you are going, any road will get you there.” – Lewis Carroll. Generative AI tools like ChatGPT and CoPilot are revolutionizing how businesses and individuals approach decision-making. However, even the most advanced tools require a… Read More »]]></content:encoded></item><item><title>Oracle and US Investors (Including Microsoft) Discuss Taking Control of TikTok in the US</title><link>https://tech.slashdot.org/story/25/01/26/0037255/oracle-and-us-investors-including-microsoft-discuss-taking-control-of-tiktok-in-the-us?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 26 Jan 2025 02:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Slashdot - Dev</source><content:encoded><![CDATA[A plan to keep TikTok available in the U.S. "involves tapping software company Oracle and a group of outside investors," reports NPR, "to effectively take control of the app's global operations, according to two people with direct knowledge of the talks..." 

"[P]otential investors who are engaged in the talks include Microsoft."

Under the deal now being negotiated by the White House, TikTok's China-based owner ByteDance would retain a minority stake in the company, but the app's algorithm, data collection and software updates will be overseen by Oracle, which already provides the foundation of TikTok's web infrastructure... "The goal is for Oracle to effectively monitor and provide oversight with what is going on with TikTok," said the person directly involved in the talks, who was not authorized to speak publicly about the deliberations. "ByteDance wouldn't completely go away, but it would minimize Chinese ownership...." Officials from Oracle and the White House held a meeting on Friday about a potential deal, and another meeting has been scheduled for next week, according to the source involved in the discussions, who said Oracle is interested in a TikTok stake "in the tens of billions," but the rest of the deal is in flux... 


Under a law passed by Congress and upheld by the Supreme Court, TikTok must execute what is known as "qualified divestiture" from ByteDance in order to stay in business in the U.S... A congressional staffer involved in talks about TikTok's future, who was not authorized to speak publicly, said binding legal agreements from the White House ensuring ByteDance cannot covertly manipulate the app will prove critical in winning lawmakers' approval. "A key part is showing there is no operational relationship with ByteDance, that they do not have control," the Congressional staffer said. "There needs to be no backdoors where China can potentially gain access...." 


Chinese regulators, who have for years opposed the selling of TikTok, recently signaled that they would not stand in the way of a TikTok ownership change, saying acquisitions "should be independently decided by the enterprises and based on market principles." The statement, at first, does not seem to say much, but negotiators in the White House believe it indicates that Beijing is not planning to block a deal that gives American investors a majority-stake position in the company.
 

"Meanwhile, Apple and Google still have not returned TikTok to app stores..."]]></content:encoded></item><item><title>Could New Linux Code Cut Data Center Energy Use By 30%?</title><link>https://hardware.slashdot.org/story/25/01/25/2111225/could-new-linux-code-cut-data-center-energy-use-by-30?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>slashdot</category><category>linux</category><pubDate>Sat, 25 Jan 2025 23:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Two computer scientists at the University of Waterloo in Canada believe changing 30 lines of code in Linux "could cut energy use at some data centers by up to 30 percent," according to the site Data Centre Dynamics. 

It's the code that processes packets of network traffic, and Linux "is the most widely used OS for data center servers," according to the article:

The team tested their solution's effectiveness and submitted it to Linux for consideration, and the code was published this month as part of Linux's newest kernel, release version 6.13. "All these big companies — Amazon, Google, Meta — use Linux in some capacity, but they're very picky about how they decide to use it," said Martin Karsten [professor of Computer Science in the Waterloo's Math Faculty]. "If they choose to 'switch on' our method in their data centers, it could save gigawatt hours of energy worldwide. Almost every single service request that happens on the Internet could be positively affected by this." 

The University of Waterloo is building a green computer server room as part of its new mathematics building, and Karsten believes sustainability research must be a priority for computer scientists. "We all have a part to play in building a greener future," he said. The Linux Foundation, which oversees the development of the Linux OS, is a founder member of the Green Software Foundation, an organization set up to look at ways of developing "green software" — code that reduces energy consumption. 

Karsten "teamed up with Joe Damato, distinguished engineer at Fastly" to develop the 30 lines of code, according to an announcement from the university. "The Linux kernel code addition developed by Karsten and Damato was based on research published in ACM SIGMETRICS Performance Evaluation Review" (by Karsten and grad student Peter Cai). 

Their paper "reviews the performance characteristics of network stack processing for communication-heavy server applications," devising an "indirect methodology" to "identify and quantify the direct and indirect costs of asynchronous hardware interrupt requests (IRQ) as a major source of overhead... 

"Based on these findings, a small modification of a vanilla Linux system is devised that improves the efficiency and performance of traditional kernel-based networking significantly, resulting in up to 45% increased throughput..."]]></content:encoded></item><item><title>Exploring prefect for scheduling data workloads</title><link>https://www.youtube.com/watch?v=hkm5OfiiorE</link><author>probabl</author><category>dev</category><category>ai</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/hkm5OfiiorE?version=3" length="" type=""/><pubDate>Sat, 25 Jan 2025 00:30:55 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Probabl</source><content:encoded><![CDATA[It is easy to train a model in a notebook, but if we want to do anything with it in production we probably want to be able to train it regularly. There's good old cron, but it's 2025 now so maybe it's time to explore something a bit more modern. That's why in this livestream we will be exploring Prefect.

Website: https://probabl.ai/
Discord: https://discord.probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Twitter: https://x.com/probabl_ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

If you're keen to see more videos like this, you can follow us over at  @probabl_ai.

#probabl]]></content:encoded></item><item><title>EFF to Michigan Supreme Court: Cell Phone Search Warrants Must Strictly Follow The Fourth Amendment’s Particularity and Probable Cause Requirements</title><link>https://www.eff.org/deeplinks/2025/01/eff-michigan-supreme-court-cell-phone-search-warrants-must-strictly-follow-fourth</link><author>Hannah Zhao</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/mobile-privacy.png" length="" type=""/><pubDate>Sat, 25 Jan 2025 00:03:59 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Last week, EFF, along with the Criminal Defense Attorneys of Michigan, ACLU, and ACLU of Michigan, filed an amicus brief in  in the Supreme Court of Michigan, challenging the constitutionality of the search warrant of Mr. Carson's smart phone.In this case, Mr. Carson was arrested for stealing money from his neighbor's safe with a co-conspirator. A few months later, law enforcement applied for a search warrant for Mr. Carson's cell phone. The search warrant enumerated the claims that formed the basis for Mr. Carson's arrest, but the only mention of a cell phone was a law enforcement officer's general assertion that phones are communication devices often used in the commission of crimes. A warrant was issued which allowed the search of the entirety of Mr. Carson's smart phone, with no temporal or category limits on the data to be searched. Evidence found on the phone was then used to convict Mr. Carson.On appeal, the Court of Appeals made a number of rulings in favor of Mr. Carson, including that evidence from the phone should not have been admitted because the search warrant lacked particularity and was unconstitutional. The government's appeal to the Michigan Supreme Court was accepted and we filed an amicus brief.In our brief, we argued that the warrant was constitutionally deficient and overbroad, because there was no probable cause for searching the cell phone and that the warrant was insufficiently particular because it failed to limit the search to within a time frame or certain categories of information.As the U.S. Supreme Court recognized in , electronic devices such as smart phones “differ in both a quantitative and a qualitative sense” from other objects. The devices contain immense storage capacities and are filled with sensitive and revealing data, including apps for everything from banking to therapy to religious practices to personal health. As the refrain goes, whatever the need, there's an app for that. This special nature of digital devices requires courts to review warrants to search digital devices with heightened attention to the Fourth Amendment’s probable cause and particularity requirements.In this case, the warrant fell far short. In order for there to be probable cause to search an item, the warrant application must establish a “nexus” between the incident being investigated and the place to be searched. But the application in this case gave no reason why evidence of the theft would be found on Mr. Carson's phone. Instead, it only stated the allegations leading to Mr. Carson's arrest and boilerplate language about cell phone use among criminals. While those facts may establish  Mr. Carson, they did not establish  Mr. Carson's phone. If it were otherwise, the government would always be able to search the cell phone of someone they had probable cause to arrest, thereby eradicating the independent determination of whether probable cause exists to search something. Without a nexus between the crime and Mr. Carson’s phone, there was no probable cause.Moreover, the warrant allowed for the search of “any and all data” contained on the cell phone, with no limits whatsoever. This type of "all content" warrants are the exact type of general warrants against which the Fourth Amendment and its state corollaries were meant to protect. Cell phone search warrants that have been upheld have contained temporal constraints and a limit to the categories of data to be searched. Neither limitationsor any other limitationswere in the issued search warrant. The police should have used date limitations in applying for the search warrant, as they do in their warrant applications for other searches in the same investigation. Additionally, the warrant allowed the search of all the information on the phone, the vast majority of which did not—and could not—contain evidence related to the investigation.As smart phones become more capacious and entail more functions, it is imperative that courts adhere to the narrow construction of warrants for the search of electronic devices to support the basic purpose of the Fourth Amendment to safeguard the privacy and security of individuals against arbitrary invasions by governmental officials.]]></content:encoded></item><item><title>Podcast with Luca Rossi</title><link>https://refactoring.fm/p/growing-the-development-forest-with</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Fri, 24 Jan 2025 18:01:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[Luca Rossi hosts a podcast (and newsletter) called Refactoring, so it's
      obvious that we have some interests in common. The tile comes from me
      leaning heavily on Beth Anders-Beck and Kent Beck's metaphor of The Forest and The Desert. We talk
      about the impact of AI on software development, the metaphor of technical
      debt, and the current state of agile software development.]]></content:encoded></item><item><title>Bringing Jetpack Compose to Instagram for Android</title><link>https://engineering.fb.com/2025/01/24/android/bringing-jetpack-compose-to-instagram-for-android/</link><author></author><category>dev</category><category>official</category><pubDate>Fri, 24 Jan 2025 17:30:53 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[Introducing a new Android UI framework like Jetpack Compose into an existing app is more complicated than importing some AARS and coding away. What if your app has specific performance goals to meet? What about existing design components, integrations with navigation, and logging frameworks?On this episode of the Meta Tech Podcast Pascal Hartig is joined by Summer, a software engineer whose team handles large-scale migrations for Instagram. Summer walks through the various thoughtful and intricate phases that Instagram goes through to ensure that developers have the best possible experience when working on our codebases. She also discusses balancing all of this with Meta’s infrastructure teams, who have to maintain multiple implementations at once.Learn how Meta approaches the rollout of a new framework and more!Download or listen to the podcast episode below:
You can also find the episode wherever you get your podcasts, including:The Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.And if you’re interested in learning more about career opportunities at Meta visit the Meta Careers page.]]></content:encoded></item><item><title>How long until quantum computing is a real data security risk?</title><link>https://www.datasciencecentral.com/how-long-until-quantum-computing-is-a-real-data-security-risk/</link><author>Zachary Amos</author><category>dev</category><category>ai</category><pubDate>Fri, 24 Jan 2025 16:46:03 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Quantum computing could do a lot of good for the world, but it also raises serious security concerns. Any technology that powerful could cause significant damage if cybercriminals get ahold of it, so cybersecurity professionals must prepare for a post-quantum future. But how much time do they have? The United Nations has labeled 2025 the… Read More »]]></content:encoded></item><item><title>Collective Memory for AI on Decentralized Knowledge Graph // Tomaž Levak // #285</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/Collective-Memory-for-AI-on-Decentralized-Knowledge-Graph--Toma-Levak--285-e2tr84a</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/97410634/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-22%2F393522714-44100-2-c347ea83225bc.mp3" length="" type=""/><pubDate>Fri, 24 Jan 2025 16:12:54 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[Tomaž Levak is the Co-founder and CEO of Trace Labs – OriginTrail core developers. OriginTrail is a web3 infrastructure project combining a decentralized knowledge graph (DKG) and blockchain technologies to create a neutral, inclusive ecosystem.

Collective Memory for AI on Decentralized Knowledge Graph // MLOps Podcast #285 with Tomaz Levak, Founder of Trace Labs, Core Developers of OriginTrail.

// Abstract
The talk focuses on how OriginTrail Decentralized Knowledge Graph serves as a collective memory for AI and enables neuro-symbolic AI. We cover the basics of OriginTrail’s symbolic AI fundamentals (i.e. knowledge graphs) and go over details how decentralization improves data integrity, provenance, and user control. We’ll cover the DKG role in AI agentic frameworks and how it helps with verifying and accessing diverse data sources, while maintaining compatibility with existing standards. 

We’ll explore practical use cases from the enterprise sector as well as latest integrations into frameworks like ElizaOS. We conclude by outlining the future potential of decentralized AI, AI becoming the interface to “eat” SaaS and the general convergence of AI, Internet and Crypto.

// Bio
Tomaz Levak, founder of OriginTrail, is active at the intersection of Cryptocurrency, the Internet, and Artificial Intelligence (AI). At the core of OriginTrail is a pursuit of Verifiable Internet for AI, an inclusive framework addressing critical challenges of the world in an AI era. To achieve the goal of Verifiable Internet for AI, OriginTrail's trusted knowledge foundation ensures the provenance and verifiability of information while incentivizing the creation of high-quality knowledge. These advancements are pivotal to unlock the full potential of AI as they minimize the technology’s shortfalls such as hallucinations, bias, issues of data ownership, and model collapse.

Tomaz's contributions to OriginTrail span over a decade and across multiple fields. He is involved in strategic technical innovations for OriginTrail Decentralized Knowledge Graph (DKG) and NeuroWeb blockchain and was among the authors of all three foundational White Paper documents that defined how OriginTrail technology addresses global challenges. Tomaz contributed to the design of OriginTrail token economies and is driving adoption with global brands such as British Standards Institution, Swiss Federal Railways and World Federation of Haemophilia, among others.

Committed to the ongoing expansion of the OriginTrail ecosystem, Tomaz is a regular speaker at key industry events. In his appearances, he highlights the significant value that the OriginTrail DKG brings to diverse sectors, including supply chains, life sciences, healthcare, and scientific research. In a rapidly evolving digital landscape, Tomaz and the OriginTrail ecosystem as a whole are playing an important role in ensuring a more inclusive, transparent and decentralized AI.

// MLOps Swag/Merch
https://shop.mlops.community/

// Related Links
Website: https://origintrail.io]]></content:encoded></item><item><title>Llama 3.1 Community License is not a free software license</title><link>http://www.fsf.org/blogs/licensing/llama-3-1-community-license-is-not-a-free-software-license</link><author></author><category>linux</category><pubDate>Fri, 24 Jan 2025 15:24:27 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Radio Hacking 101: Raspberry Pi SDR with a CaribouLite</title><link>https://www.youtube.com/watch?v=Hz2WqhWmjZE</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/Hz2WqhWmjZE?version=3" length="" type=""/><pubDate>Fri, 24 Jan 2025 15:00:18 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[SDR lets you peek into the invisible world of RF, or radiofrequency!

The Raspberry Pi is a great tool for SDR, and in this video I finally tested it with the CaribouLite I bought a couple years ago.

See my blog post for full setup instructions: https://www.jeffgeerling.com/blog/2025/cariboulite-sdr-hat-sdr-on-raspberry-pi

More resources (as an Amazon Associate, I earn for qualifying purchases):

  - CaribouLite: https://www.crowdsupply.com/cariboulabs/cariboulite-rpi-hat
  - RTL-SDR v4 kit: https://amzn.to/3WzId5R
  - Project MINI RACK: https://mini-rack.jeffgeerling.com
  - LabStack mini modular printable mounts: https://github.com/JaredC01/LabStack
  - Raspberry Pi SMI: https://iosoft.blog/2020/07/16/raspberry-pi-smi/

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff

Contents:

00:00 - A peek behind the curtains
01:14 - Setting up CaribouLite with a Pi
02:13 - Driver and software installation
03:32 - GQRX SDR intro
09:29 - Testing CaribouLite SDR, decoding signals
10:53 - Is a Pi 4 enough?
12:18 - Mini Rack and PoE SDR setup
15:06 - Remote SDR and a WiFi issue
17:40 - It's like magic]]></content:encoded></item><item><title>Positioning Text Around Elements With CSS Offset</title><link>https://css-tricks.com/positioning-text-around-elements-with-css-offset/</link><author>Preethi</author><category>frontend</category><pubDate>Fri, 24 Jan 2025 14:59:25 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[When it comes to positioning elements on a page, including text, there are many ways to go about it in CSS — the literal  property with corresponding  properties, , ,  (limited browser support at the moment), and so forth. The  property is another one that belongs in that list.The  property is typically used for animating an element along a predetermined path. For instance, the square in the following example traverses a circular path:<div class="circle">
  <div class="square"></div>
</div>@property --p {
  syntax: '<percentage>';
  inherits: false;
  initial-value: 0%;
}
.square {
  offset: top 50% right 50% circle(50%) var(--p);
  transition: --p 1s linear;

  /* Equivalent to:
    offset-position: top 50% right 50%;
    offset-path: circle(50%);
    offset-distance: var(--p); */

  /* etc. */
}

.circle:hover .square{ --p: 100%; }A registered CSS custom property () is used to set and animate the offset distance of the square element. The animation is possible because an element can be positioned at any point in a given path using . and maybe you didn’t know this, but  is a shorthand property comprised of the following constituent properties: The path’s starting point The shape along which the element can be moved A distance along the path on which the element is moved The rotation angle of an element relative to its anchor point and offset path A position within the element that’s aligned to the pathThe  property is the one that’s important to what we’re trying to achieve. It accepts a shape value — including SVG shapes or CSS shape functions — as well as reference boxes of the containing element to create the path.Reference boxes? Those are an element’s dimensions according to the CSS Box Model, including , , , as well as SVG contexts, such as the , , and . These simplify how we position elements along the edges of their containing elements. Here’s an example: all the small squares below are placed in the default top-left corner of their containing elements’ . In contrast, the small circles are positioned along the top-right corner ( into their containing elements’ square perimeter) of the , , and , respectively.<div class="big">
  <div class="small circle"></div>
  <div class="small square"></div>
  <p>She sells sea shells by the seashore</p>
</div>

<div class="big">
  <div class="small circle"></div>
  <div class="small square"></div>
  <p>She sells sea shells by the seashore</p>
</div>

<div class="big">
  <div class="small circle"></div>
  <div class="small square"></div>
  <p>She sells sea shells by the seashore</p>
</div>.small {
  /* etc. */
  position: absolute;

  &.square {
    offset: content-box;
    border-radius: 4px;
  }

  &.circle { border-radius: 50%; }
}

.big {
  /* etc. */
  contain: layout; /* (or position: relative) */

  &:nth-of-type(1) {
    .circle { offset: content-box 25%; }
  }

  &:nth-of-type(2) {
    border: 20px solid rgb(170 232 251);
    .circle { offset: border-box 25%; }
  }

  &:nth-of-type(3) {
    padding: 20px;
    .circle { offset: padding-box 25%; }
  }
} You can separate the element’s offset-positioned layout context if you don’t want to allocated space for it inside its containing parent element. That’s how I’ve approached it in the example above so that the paragraph text inside can sit flush against the edges. As a result, the offset positioned elements (small squares and circles) are given their own contexts using , which removes them from the normal document flow.This method, positioning relative to reference boxes, makes it easy to place elements like notification dots and ornamental ribbon tips along the periphery of some UI module. It further simplifies the placement of texts along a containing block’s edges, as  can also rotate elements along the path, thanks to . A simple example shows the date of an article placed at a block’s right edge:<article>
  <h1>The Irreplaceable Value of Human Decision-Making in the Age of AI</h1>
  <!-- paragraphs -->
  <div class="date">Published on 11<sup>th</sup> Dec</div>
  <cite>An excerpt from the HBR article</cite>
</article>article {
  container-type: inline-size;
  /* etc. */
}

.date {
  offset: padding-box 100cqw 90deg / left 0 bottom -10px;
  
  /*
    Equivalent to:
    offset-path: padding-box;
    offset-distance: 100cqw; (100% of the container element's width)
    offset-rotate: 90deg;
    offset-anchor: left 0 bottom -10px;
  */
}As we just saw, using the  property with a reference box path  container units is even more efficient — you can easily set the offset distance based on the containing element’s width or height. I’ll include a reference for learning more about container queries and container query units in the “Further Reading” section at the end of this article.There’s also the  property that’s used in that last example. It provides the anchor for the element’s displacement and rotation — for instance, the 90 degree rotation in the example happens from the element’s bottom-left corner. The  property can also be used to move the element either inward or outward from the reference box by adjusting  values — for instance, the  arguments pull the element’s bottom edge outwards from its containing element’s . This enhances the precision of placements, also demonstrated below.<figure>
  <div class="big">4</div>
  <div class="small">number four</div>
</figure>.small {
  width: max-content;
  offset: content-box 90% -54deg / center -3rem;

  /*
    Equivalent to:
    offset-path: content-box;
    offset-distance: 90%;
    offset-rotate: -54deg;
    offset-anchor: center -3rem;
  */

  font-size: 1.5rem;
  color: navy;
}As shown at the beginning of the article, offset positioning is animateable, which allows for dynamic design effects, like this:<article>
  <figure>
    <div class="small one">17<sup>th</sup> Jan. 2025</div>
    <span class="big">Seminar<br>on<br>Literature</span>
    <div class="small two">Tickets Available</div>
  </figure>
</article>@property --d {
  syntax: "<percentage>";
  inherits: false;
  initial-value: 0%;
}

.small {
  /* other style rules */
  offset: content-box var(--d) 0deg / left center;

  /*
    Equivalent to:
    offset-path: content-box;
    offset-distance: var(--d);
    offset-rotate: 0deg;
    offset-anchor: left center;
  */

  transition: --d .2s linear;

  &.one { --d: 2%; }
  &.two { --d: 70%; }
}

article:hover figure {
  .one { --d: 15%;  }
  .two { --d: 80%;  }
}Whether for graphic designs like text along borders, textual annotations, or even dynamic texts like error messaging, CSS offset is an easy-to-use option to achieve all of that. We can position the elements along the reference boxes of their containing parent elements, rotate them, and even add animation if needed.]]></content:encoded></item><item><title>Fallthrough &amp; Friends (Friends)</title><link>https://changelog.com/friends/77</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/friends/77/changelog--friends-77.mp3" length="" type=""/><pubDate>Fri, 24 Jan 2025 14:00:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Kris Brandow & Matthew Sanabria from Fallthrough.fm join Jerod to discuss tools we’re switching to, whether or not Go is still a great systems programming language choice, user-centric documentation, the need for archivists & more.Changelog++ members save 7 minutes on this episode because they made the ads disappear. Join today!Augment Code – Developer AI that uses deep understanding of your large codebase and how you build software to deliver personalized code suggestions and insights. Augment provides relevant, contextualized code right in your IDE or Slack. It transforms scattered knowledge into code or answers, eliminating time spent searching docs or interrupting teammates.
Fly.io – The home of Changelog.com — Deploy your apps close to your users — global Anycast load-balancing, zero-configuration private networking, hardware isolation, and instant WireGuard VPN connections. Push-button deployments that scale to thousands of instances. Check out the speedrun to get started in minutes.
]]></content:encoded></item><item><title>Digest #157: GitHub Actions Risks, FinOps Shift, Kubernetes Security, and Terraform Best Practices</title><link>https://www.devopsbulletin.com/p/digest-157-github-actions-risks-finops</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F388278dc-7596-4322-b679-ddd0aee5d8e0_2029x1021.png" length="" type=""/><pubDate>Fri, 24 Jan 2025 12:31:06 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!Ever stumbled upon a bizarre root cause that left you scratching your head? Or questioned whether GitHub Actions is still the best CI/CD option? This week’s edition explores unexpected failures, lessons from building observability with GCP, and the secret weapon every DevOps engineer needs. There’s also a breakdown of the Flexera-NetApp FinOps acquisition, along with tutorials on securing Grafana, detecting honeypots in AWS, and mastering IAM policy checks.On the open-source front, we’re highlighting some fascinating projects: a Bash-to-Go transpiler for faster and more secure scripts, Klarna’s internal threat modeling tool for system security, and a distributed key-value NoSQL database built on RocksDB with full Redis compatibility. Plus, an open-source Terraform pre-processor that helps you write less code without losing coverage and an Apache project redefining data lakehouse efficiency.All this and more in this week’s DevOps Bulletin—don’t miss out!Highlighting cool DevOps projects to keep an eye on: transpiles Bash scripts into Go, compiling them into fast, portable binaries for better performance and security. is Klarna's internal threat modeling tool that helps engineers collaboratively create system dataflow diagrams with attached threats and controls. is a native implementation of a document-oriented NoSQL database, enabling seamless CRUD operations on BSON data types within a PostgreSQL framework. is a distributed key-value NoSQL database that uses RocksDB as a storage engine and is compatible with Redis protocol. is a Terraform code pre-processor. Its primary goal is to minimize your total Terraform codebase without giving up on coverage. To do more with less. Hudi is an open data lakehouse platform for efficient data ingestion, storage, indexing, and management across clouds.If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>The Real Python Podcast – Episode #236: Simon Willison: Using LLMs for Python Development</title><link>https://realpython.com/podcasts/rpp/236/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Fri, 24 Jan 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[What are the current large language model (LLM) tools you can use to develop Python? What prompting techniques and strategies produce better results? This week on the show, we speak with Simon Willison about his LLM research and his exploration of writing Python code with these rapidly evolving tools.]]></content:encoded></item><item><title>Linux 6.14 Adds Support For The Microsoft Copilot Key Found On New Laptops</title><link>https://linux.slashdot.org/story/25/01/23/2333254/linux-614-adds-support-for-the-microsoft-copilot-key-found-on-new-laptops?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>slashdot</category><category>linux</category><pubDate>Fri, 24 Jan 2025 00:20:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[The Linux 6.14 kernel now maps out support for Microsoft's "Copilot" key "so that user-space software can determine the behavior for handling that key's action on the Linux desktop," writes Phoronix's Michael Larabel. From the report: A change made to the atkbd keyboard driver on Linux now maps the F23 key to support the default copilot shortcut action. The patch authored by Lenovo engineer Mark Pearson explains [...]. Now it's up to the Linux desktop environments for determining what to do if the new Copilot key is pressed. The patch was part of the input updates now merged for the Linux 6.14 kernel.]]></content:encoded></item><item><title>Bun&apos;s on a roll with v1.2</title><link>https://javascriptweekly.com/issues/720</link><author></author><category>dev</category><category>frontend</category><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><source url="https://javascriptweekly.com/">Javascript Weekly</source><content:encoded><![CDATA[ — A fast and flexible DataGrid for building modern web apps. Key features and virtualized rendering are included in the core grid module. Pick & choose special features to keep your app small. Built for JavaScript, extended to Angular, React, and Vue.Wijmo From MESCIUS inc. ]]></content:encoded></item><item><title>JavaScript Temporal is coming</title><link>https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/</link><author>brian-smith</author><category>dev</category><category>official</category><category>frontend</category><enclosure url="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/featured.png" length="" type=""/><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><source url="https://developer.mozilla.org/en-US/blog/">MDN Blog</source><content:encoded><![CDATA[A new way to handle dates and times is being added to JavaScript. Let's take a look at Temporal, what problems it solves, the current state, and what you'll find in the new documentation about it on MDN.
]]></content:encoded></item><item><title>Template literal types in TypeScript: parsing during type checking and more</title><link>https://2ality.com/2025/01/template-literal-types.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In this blog post, we take a closer look at template literal types in TypeScript: While their syntax is similar to JavaScript’s template literals, they operate at the type level. Their use cases include:Static syntax checking for string literalsTransforming the casing of property names (e.g. from hyphen case to camel case)Concisely specifying large string literal union types]]></content:encoded></item><item><title>How we accidentally built a better build system for OCaml</title><link>https://blog.janestreet.com/how-we-accidentally-built-a-better-build-system-for-ocaml-index/</link><author></author><category>dev</category><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><source url="https://blog.janestreet.com/">Dev - Jane Street</source><content:encoded><![CDATA[A “build system” is one of the most important tools in a developer’s
toolbox. Roughly, it figures out how to create runnable programs from
a bunch of different source files by calling out to the compiler,
setting up and executing test suites, and so on. Because you interact
with it daily, above all it has to be fast –
but it also has to be flexible.]]></content:encoded></item><item><title>Build It Yourself</title><link>http://lucumr.pocoo.org/2025/1/24/build-it-yourself</link><author>Armin Ronacher</author><category>dev</category><category>blog</category><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><source url="https://lucumr.pocoo.org/feed.atom">Armin Ronacher Blog</source><content:encoded><![CDATA[Another day, another rantaboutdependencies. from me.  This time I will ask you that we
start and support a vibe shift when it comes to dependencies.You're probably familiar with the concept of “dependency churn.”  It's that
never-ending treadmill of updates, patches, audits, and transitive
dependencies that we as developers love to casually install in the name of
productivity.  Who doesn't enjoy waiting for yet another 
just so you can get that fix for a bug you don't even have?It's a plague in most ecosystems with good packaging solutions.
JavaScript and Rust are particularly badly affected by that.  A brand new
Tokio project drags in 28 crates, a new Rocket project balloons that to
172, and a little template engine like MiniJinja can exist with just a
single dependency — while its CLI variant slurps up 142.If that doesn't sound like a big deal, let's consider terminal_size.  It is a crate that does
exactly what its name suggests: it figures out your terminal dimensions.
The underlying APIs it uses have effectively been stable since the earliest days of computing
terminals—what, 50 years or so? And yet, for one function, terminal-size
manages to introduce three or four additional crates, depending on your
operating system.  That triggers a whole chain reaction, so you end up
compiling thousands of other functions just to figure out if your terminal
is 80x25 or 120x40.  That crate had 26 releases.  My own version of that
that I have stuck away in a project from 10 years ago still works without
a single update.  Because shocker: nothing about figuring out terminal
sizes has changed.  So why does  have so many updates if it's so stable?
Because it's build on top of platform abstraction libraries that
constantly churn, so it needs to update to avoid code duplication and
blowing up compile times even more.But “big supply chain” will tell you that you must do it this way.  Don't
you dare to copy paste that function into your library.  Or don't you date
to use “unsafe” yourself.  You're not qualified enough to write unsafe
code, let the platform abstraction architects do that.  Otherwise someone
will slap you.  There are
entire companies who are making a living of supplying you with the tools
needed to deal with your dependency mess.  In the name of security, we're
pushed to having dependencies and keeping them up to date, despite most of
those dependencies being the primary source of security problems.The goal of code in many ways should be to be written in a way that it
does not need updates.  It should eventually achieve some level of
stability.  In the Rust ecosystem stable code is punished.  If you have a
perfectly working dependency but you have a somewhat inactive bug tracker,
RUSTSEC will come by and give you a chunk rating.But there  a simpler path.  You write code yourself.  Sure, it's more
work up front, but once it's written, it's done. No new crates, no waiting
for upsteam authors to fix that edge case.  If it's broken for you, you
fix it yourself.  Code that works doesn't necessarily need the
maintenance treadmill.  Your code has a corner case?  Who cares.  This is
that vibe shift we need in the Rust world: celebrating fewer dependencies
rather than more.We're at a point in the most ecosystems where pulling in libraries is not
just the default action, it's seen positively: “Look how modular and
composable my code is!”  Actually, it might just be a symptom of never
wanting to type out more than a few lines.Now one will make the argument that it takes so much time to write all of
this.  It's 2025 and it's faster for me to have ChatGPT or Cursor whip up
a dependency free implementation of these common functions, than it is for
me to start figuring out a dependency.  And it makes sense as for many
such small functions the maintenance overhead is tiny and much lower than
actually dealing with constant upgrading of dependencies.  The code is just
a few lines and you also get the benefit of no longer need to compile
thousands of lines of other people's code for a single function.But let's face it: corporate code review culture has also has infected
Open Source software.  Companies are more likely to reward engineers than
scold them for pulling in that new “shiny library” that solves the problem
they never actually had.  That creates problems, so dependabot and friends
were born.  Today I just dread getting dependabot pull requests but on
projects but I have to accept it.  I'm part of an ecosystem with my stuff
and that ecosystem is all about churn, churn, churn.  In companies you can
also keep entire internal engineering teams busy with vendoring
dependencies, internal audits and upgrading things throughout the company.Fighting this fight is incredibly hard!  Every new hire has been trained
on the idea that dependencies are great, that code reuse is great.  That
having old code sitting around is a sign of bad engineering culture.It's also hard to fight this in Open Source.  Years ago I wrote sha1-smol which originally was just called
.  It became the standard crate to calculate SHA1 hashes.
Eventually I was pressured to donate that package name to rust-crypto and
to depend on the rest of the crypto ecosystem as it was so established.
If you want to use the new sha1 crate, you get to enjoy 10 dependencies.
But there was just no way around it, because that name in the registry is
precious and people also wanted to have trait compatibility.  It feels
tiring to be the only person in a conversation pushing to keep the churn
down and dependencies low.It's time to have a new perspective: we should give kudos to engineers who
write a small function themselves instead of hooking in a transitive web
of crates.  We should be suspicious of big crate graphs.  Celebrated are
the minimal dependencies, the humble function that just quietly does the
job, the code that doesn't need to be touched for years because it was
done right once.And sure, it's not black and white.  There are the important libraries
that solve hard problems.  Graphics libraries that abstract over complex
drivers, implementations of protocols like HTTP and QUIC.  I won't be able
to get rid of tokio and I have no desire to.  But when you end up using
one function, but you compile hundreds, some alarm bell should go off.We need that vibe shift.  To celebrate building it yourself when it's
appropriate to do so.  To give credit to library authors who build low to
no-dependency Open Source libraries.For instance minijinja celebrates it in the readme:$ cargo tree
minimal v0.1.0 (examples/minimal)
└── minijinja v2.6.0 (minijinja)
    └── serde v1.0.144
And it has a PR to eventually get rid of the last dependency.  And sometime this
year I will make it my goal to go ahead proudly and trim down all that fat
in my projects.]]></content:encoded></item><item><title>Face Scans to Estimate Our Age: Harmful and Creepy AF</title><link>https://www.eff.org/deeplinks/2025/01/face-scans-estimate-our-age-creepy-af-and-harmful</link><author>Adam Schwartz</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/whyfacebanner.png" length="" type=""/><pubDate>Thu, 23 Jan 2025 23:56:53 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Some advocates of these censorship schemes argue we can nerd our way out of the many harms they cause to speech, equity, privacy, and infosec. Their silver bullet? “Age estimation” technology that scans our faces, applies an algorithm, and guesses how old we are – before letting us access online content and opportunities to communicate with others. But when confronted with age estimation face scans, many people will refrain from accessing restricted websites, even when they have a legal right to use them. Why?Because quite simply, age estimation face scans are creepy AF – and harmful. First, age estimation is inaccurate and discriminatory. Second, its underlying technology can be used to try to estimate our other demographics, like ethnicity and gender, as well as our names. Third, law enforcement wants to use its underlying technology to guess our emotions and honesty, which in the hands of jumpy officers is likely to endanger innocent people. Fourth, age estimation face scans create privacy and infosec threats for the people scanned. In short, government should be restraining this hazardous technology, not normalizing it through age verification mandates.Age estimation is ofteninaccurate. It’s in the name: age . That means these face scans will regularly mistake adults for adolescents, and wrongfully deny them access to restricted websites. By the way, it will also sometimes mistake adolescents for adults.Age estimation also is discriminatory. Studies show face scans are more likely to err in estimating the age of people of color and women. Which means that as a tool of age verification, these face scans will have an unfair disparate impact.Estimating our identity and demographicsAge estimation is a tech sibling of face identification and the estimation of other demographics. To users, all face scans look the same and we shouldn’t allow them to become a normal part of the internet. When we submit to a face scan to estimate our age, a less scrupulous company could flip a switch and use the same face scan, plus a slightly different algorithm, to guess our name or other demographics.Some companies are in both the age estimation business and the face identification business.Other developers claim they can use age estimation’s underlying technology – application of an algorithm to a face scan – to estimate our gender (likethesevenders) and our ethnicity (likethesevenders). But these scans are likely to misidentify the many people whose faces do not conform to gender and ethnic averages (such as transgenderpeople). Worse, powerful institutions can harm people with this technology. China uses face scans to identify ethnicUyghurs. Transphobic legislators may try to use them to enforce bathroom bans. For this reason, advocates have sought to prohibit gender estimation face scans.Estimating our emotions and honestyDevelopers claim they can use face estimation’s underlying technology to estimate our emotions (likethesevenders). But this will always have a high error rate, because people express emotions differently, based on culture, temperament, and neurodivergence. Worse, researchers are trying to use face scans to estimate deception, and even criminality. Mind-reading technologies have a long and dubious history, from phrenology to polygraphs.Unfortunately, powerful institutions may believe the hype. In 2008, the U.S. Department of Homeland Security disclosed its efforts to use “image analysis” of “facial features” (among other biometrics) to identify “malintent” of people being screened. Other policing agencies are using algorithms to analyze emotions and deception.When police technology erroneously identifies a civilian as a threat, many officers overreact. For example, ALPR errors recurringly prompt police officers to draw guns on innocent drivers. Some government agencies now advise drivers to keep their hands on the steering wheel during a traffic stop, to reduce the risk that the driver’s movements will frighten the officer. Soon such agencies may be advising drivers not to roll their eyes, because the officer’s smart glasses could misinterpret that facial expression as anger or deception.The government should not be forcing tech companies to collect even more personal data from users. Companies already collect too much data and have proved they cannot be trusted to protect it.Age verification face scans create new threats to our privacy and information security. These systems collect a scan of our face and guess our age. A poorly designed system might store this personal data, and even correlate it to the online content that we look at. In the hands of an adversary, and cross-referenced to other readily available information, this information can expose intimate details about us. Our faces are unique, immutable, and constantly on display – creating risk of biometric tracking across innumerable virtual and IRL contexts. Last year, hackers breached an age verification company (among many other companies).Of course, there are better and worse ways to design a technology. Some privacy and infosec risks might be reduced, for example, by conducting face scans on-device instead of in-cloud, or by deleting everything immediately after a visitor passes the age test. But lower-risk does not mean zero-risk. Clever hackers might find ways to breach even well-designed systems, companies might suddenly change their systems to make them less privacy-protective (perhaps at the urging of government), and employees and contractors might abuse their special access. Numerous states are mandating age verification with varying rules for how to do so; numerous websites are subject to these mandates; and numerous vendors are selling face scanning services. Inevitably, many of these websites and services will fail to maintain the most privacy-preserving systems, because of carelessness or greed.Also, face scanning algorithms are often trained on data that was collected using questionable privacy methods—whether it be from users with murky-consent or non-users. The government data sets used to test biometric algorithms sometimes come from prisoners and immigrants.Most significant here, when most people arrive at most age verification checkpoints, they will have no idea whether the face scan system has minimized the privacy and infosec risks. So many visitors will turn away, and forego the content and conversations available on restricted website.Algorithmic face scans are dangerous, whether used to estimate our age, our other demographics, our name, our emotions, or our honesty. Thus, EFF supports a ban on governmentuse of this technology, and strict regulation (including consent and minimization) for corporate use.At a minimum, government must stop coercing websites into using face scans, as a means of complying with censorious age verification mandates. Age estimation does not eliminate the privacy and security issues that plague all age verification systems. And these face scans cause many people to refrain from accessing websites they have a legal right to access. Because face scans are creepy AF.]]></content:encoded></item><item><title>Backdoor infecting VPNs used “magic packets” for stealth and security</title><link>https://arstechnica.com/security/2025/01/backdoor-infecting-vpns-used-magic-packets-for-stealth-and-security/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/02/backdoor2.jpeg" length="" type=""/><pubDate>Thu, 23 Jan 2025 23:42:29 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[When threat actors use backdoor malware to gain access to a network, they want to make sure all their hard work can’t be leveraged by competing groups or detected by defenders. One countermeasure is to equip the backdoor with a passive agent that remains dormant until it receives what’s known in the business as a “magic packet.” On Thursday, researchers revealed that a never-before-seen backdoor that quietly took hold of dozens of enterprise VPNs running Juniper Network’s Junos OS has been doing just that.J-Magic, the tracking name for the backdoor, goes one step further to prevent unauthorized access. After receiving a magic packet hidden in the normal flow of TCP traffic, it relays a challenge to the device that sent it. The challenge comes in the form of a string of text that’s encrypted using the public portion of an RSA key. The initiating party must then respond with the corresponding plaintext, proving it has access to the secret key.The lightweight backdoor is also notable because it resided only in memory, a trait that makes detection harder for defenders. The combination prompted researchers at Lumen Technology’s Black Lotus Lab to sit up and take notice.]]></content:encoded></item><item><title>Second Circuit Rejects Record Labels’ Attempt to Rewrite the DMCA</title><link>https://www.eff.org/deeplinks/2025/01/second-circuit-rejects-record-labels-attempt-rewrite-dmca</link><author>Tori Noble</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/dmca-copyright-1_0.png" length="" type=""/><pubDate>Thu, 23 Jan 2025 23:22:21 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[In a major win for creator communities, the U.S. Court of Appeals for the Second Circuit has once again handed video streaming site Vimeo a solid win in its long-running legal battle with Capitol Records and a host of other record labels.The labels claimed that Vimeo was liable for copyright infringement on its site, and specifically that it can’t rely on the Digital Millennium Copyright Act’s safe harbor because Vimeo employees “interacted” with user-uploaded videos that included infringing recordings of musical performances owned by the labels. Those interactions included commenting on, liking, promoting, demoting , or posting them elsewhere on the site. The record labels contended that these videos contained popular songs, and it would’ve been obvious to Vimeo employees that this music was unlicensed.But as EFF explained in an amicus brief filed in support of Vimeo, even rightsholders themselves mistakenly demand takedowns. Labels often request takedowns of music they don’t own or control, and even request takedowns of their own content. They also regularlytargetfairuses. When rightsholders themselves cannot accurately identify infringement, courts cannot presume that a service provider can do so, much less a blanket presumption as to hundreds of videos.In an earlier ruling, the court  held that the labels had to show that it would be apparent to a person without specialized knowledge of copyright law that the particular use of the music was unlawful, or prove that the Vimeo workers had expertise in copyright law. The labels argued that Vimeo’s own efforts to educate its employees and user about copyright, among other circumstantial evidence, were enough to meet that burden. The Second Circuit disagreed, finding that:Vimeo’s exercise of prudence in instructing employees not to use copyrighted music and advising users that use of copyrighted music “generally (but not always) constitutes copyright infringement” did not educate its employees about how to distinguish between infringing uses and fair use.The Second Circuit also rejected another equally dangerous argument: that Vimeo lost safe harbor protection by receiving a “financial benefit” from infringing activity, such as user-uploaded videos, that the platform had a “right and ability to control.” The labels contended that any website that exercises editorial judgment—for example, by removing, curating, or organizing content—would necessarily have the “right and ability to control” that content. If they were correct, ordinary content moderation would put a platform at risk of crushing copyright liability.As the Second Circuit put it, the labels’ argument:would substantially undermine what has generally been understood to be one of Congress’s major objectives in passing the DMCA: encouraging entrepreneurs to establish websites that can offer the public rapid, efficient, and inexpensive means of communication by shielding service providers from liability for infringements placed on the sites by users.Fortunately, the Second Circuit’s decisions in this case help preserve the safe harbors and the expression and innovation that they make possible. But it should not have taken well over a decade of litigation—and likely several millions of dollars in legal fees—to get there.]]></content:encoded></item><item><title>Speaking Freely: Lina Attalah</title><link>https://www.eff.org/deeplinks/2025/01/speaking-freely-lina-attalah</link><author>Jillian C. York</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/speaking-freely-lina_0.jpg" length="" type=""/><pubDate>Thu, 23 Jan 2025 19:51:35 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[This interview has been edited for length and clarity.*Jillian York: Welcome, let’s start here. What does free speech or free expression mean to you personally?York: What are the qualities that make you passionate about the work that you do, and also about telling stories and utilizing your free expression in that way? York: I’d like to ask you about Syria, since you just traveled there. I know that you're familiar with the context there in terms of censorship and the Internet in particular. What do you see in terms of people's hopes for more expression in Syria in the future?York: I love that. Thank you. Okay, let’s go to Egypt a little bit. What can you tell us about the situation for free speech in the context of Egypt? We're coming up on fourteen years since the uprising in 2011 and eleven years since  came to power. And I mean, I guess, contextualize that for our readers who don't know what's happened in Egypt in the past decade or so.York: Earlier you touched on the privatization of online spaces. Having watched the way tech companies have behaved over the past decade, what do you think that these companies fail to understand about the Egyptian and the regional context?York: I don't disagree at this point. I think that there was a point in time where I would have disagreed. When you work closely with companies, it’s easy to fall into the trap of believing that change is possible because you know good people who work there, people who really are trying their best. But those people are rarely capable of shifting the direction of the company, and are often the ones to leave first.Let’s shift to talking about our friend, Egyptian political prisoner . You mentioned the impact that the past 11 years, really the past 14 years, have had on people in Egypt. And, of course, there are many political prisoners, but one of the prisoners that that EFF readers will be familiar with is Alaa. You recently accepted the  on his behalf. Can you tell us more about what he has meant to you?York: I guess I also have to ask, what would your message be to the UK Government at this point?York: Who is your free speech hero? ]]></content:encoded></item><item><title>The Stargate situation is crazy... Elon vs Altman beef intensifies</title><link>https://www.youtube.com/watch?v=YrHsw4Oja7w</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/YrHsw4Oja7w?version=3" length="" type=""/><pubDate>Thu, 23 Jan 2025 18:55:27 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[What is the new Startgate project just announced by OpenAI and why is Elon Musk calling it fake? Learn how the new Stargate datacenters could affect the future of artificial intelligence development. 

#tech #ai #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Elon vs Sam Altman https://youtu.be/Sf4WqHBCYSY
DeepSeek R1 first look https://youtu.be/-2k1rcRzsLA

📚 Chapters

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- What is Project Stargate for AI infrastructure?
- Elon Musk vs Sam Altman controversy
- Grok 3 vs OpenAI o1]]></content:encoded></item><item><title>The Impact of Age Verification Measures Goes Beyond Porn Sites</title><link>https://www.eff.org/deeplinks/2025/01/impact-age-verification-measures-goes-beyond-porn-sites</link><author>Rindala Alajaji</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/rainbow-1_0_0.png" length="" type=""/><pubDate>Thu, 23 Jan 2025 17:34:42 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Some Things You Might Not Know About Custom Counter Styles</title><link>https://css-tricks.com/some-things-you-might-not-know-about-custom-counter-styles/</link><author>Geoff Graham</author><category>frontend</category><pubDate>Thu, 23 Jan 2025 17:21:15 +0000</pubDate><source url="https://css-tricks.com/">CSS Tricks blog</source><content:encoded><![CDATA[I was reading through Juan’s recent Almanac entry for the  at-rule and I’ll be darned if he didn’t uncover and unpack some extremely interesting things that we can do to style lists, notably the list marker. You’re probably already aware of the  pseudo-element. You’ve more than likely dabbled with custom counters using  and . Or maybe your way of doing things is to wipe out the  (careful when doing that!) and hand-roll a marker on the list item’s  pseudo.But have you toyed around with ? Turns out it does a lot of heavy lifting and opens up new ways of working with lists and list markers.You can style the marker of just one list itemThis is called a “fixed”  set to a specific item.@counter-style style-fourth-item {
  system: fixed 4;
  symbols: "💠";
  suffix: " ";
}

li {
  list-style: style-fourth-item;
}You can assign characters to specific markersIf you go with an “additive” , then you can define which symbols belong to which list items.@counter-style dice {
  system: additive;
  additive-symbols: 6 "⚅", 5 "⚄", 4 "⚃", 3 "⚂", 2 "⚁", 1 "⚀";
  suffix: " ";
}

li {
  list-style: dice;
}Notice how the  repeats once it reaches the end of the cycle and begins a new series based on the first item in the pattern. So, for example, there are six sides to typical dice and we start rolling two dice on the seventh list item, totaling seven.You can add a prefix and suffix to list markersA long while back, Chris showed off a way to insert punctuation at the end of a list marker using the list item’s  pseudo:ol {
  list-style: none;
  counter-reset: my-awesome-counter;

  li {
    counter-increment: my-awesome-counter;

    &::before {
      content: counter(my-awesome-counter) ") ";
    }
  }
}That’s much easier these days with :@counter-style parentheses {
  system: extends decimal;
  prefix: "(";
  suffix: ") ";
}You can style multiple ranges of list itemsLet’s say you have a list of 10 items but you only want to style items 1-3. We can set a  for that:@counter-style single-range {
  system: extends upper-roman;
  suffix: ".";
  range: 1 3;
}

li {
  list-style: single-range;
}We can even  our own dice example from earlier:@counter-style dice {
  system: additive;
  additive-symbols: 6 "⚅", 5 "⚄", 4 "⚃", 3 "⚂", 2 "⚁", 1 "⚀";
  suffix: " ";
}

@counter-style single-range {
  system: extends dice;
  suffix: ".";
  range: 1 3;
}

li {
  list-style: single-range;
}Another way to do that is to use the  keyword as the first value:@counter-style dice {
  system: additive;
  additive-symbols: 6 "⚅", 5 "⚄", 4 "⚃", 3 "⚂", 2 "⚁", 1 "⚀";
  suffix: " ";
}

@counter-style single-range {
  system: extends dice;
  suffix: ".";
  range: infinite 3;
}

li {
  list-style: single-range;
}Speaking of , you can set it as the second value and it will count up infinitely for as many list items as you have.Maybe you want to style two ranges at a time and include items 6-9. I’m not sure why the heck you’d want to do that but I’m sure you (or your HIPPO) have got good reasons.@counter-style dice {
  system: additive;
  additive-symbols: 6 "⚅", 5 "⚄", 4 "⚃", 3 "⚂", 2 "⚁", 1 "⚀";
  suffix: " ";
}

@counter-style multiple-ranges {
  system: extends dice;
  suffix: ".";
  range: 1 3, 6 9;
}

li {
  list-style: multiple-ranges;
}You can add padding around the list markersYou ever run into a situation where your list markers are unevenly aligned? That usually happens when going from, say, a single digit to a double-digit. You can  the marker with extra characters to line things up./* adds leading zeroes to list item markers */
@counter-style zero-padded-example {
  system: extends decimal;
  pad: 3 "0";
}Now the markers will always be aligned… well, up to 999 items.I just thought those were some pretty interesting ways to work with list markers in CSS that run  (get it?!) to how I’ve traditionally approached this sort of thing. And with  becoming Baseline “newly available” in September 2023, it’s well-supported in browsers.]]></content:encoded></item><item><title>Combating bias in AI systems: A guide for data scientists</title><link>https://www.datasciencecentral.com/combating-bias-in-ai-systems-a-guide-for-data-scientists/</link><author>Pritesh Patel</author><category>dev</category><category>ai</category><pubDate>Thu, 23 Jan 2025 15:20:41 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Data is at the heart of any system. Today, AI is largely used to maintain large volumes of data and make decisions from it. AI is even being used in sensitive areas such as healthcare, hiring, and criminal justice which has sparked a huge debate in recent years.  The debate is about whether AI can be… Read More »]]></content:encoded></item><item><title>From open source to acquired (Interview)</title><link>https://changelog.com/podcast/625</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/podcast/625/the-changelog-625.mp3" length="" type=""/><pubDate>Thu, 23 Jan 2025 14:00:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Ashley Jeffs shares his journey with Benthos, an open source stream processor that was acquired by Redpanda. We talk about the evolution of data streaming technologies, the challenges he faced while growing the project, the decision to bootstrap versus seek venture capital, and what ultimately led to the acquisition. We discuss reactions to licensing changes, what it’s like to have your thing acquired, the challenging yet fulfilling nature of open source work, what’s next for Benthos, and what it takes to enjoy the journey.Changelog++ members get a bonus 2 minutes at the end of this episode and zero ads. Join today!Augment Code – Developer AI that uses deep understanding of your large codebase and how you build software to deliver personalized code suggestions and insights. Augment provides relevant, contextualized code right in your IDE or Slack. It transforms scattered knowledge into code or answers, eliminating time spent searching docs or interrupting teammates.
Fly.io – The home of Changelog.com — Deploy your apps close to your users — global Anycast load-balancing, zero-configuration private networking, hardware isolation, and instant WireGuard VPN connections. Push-button deployments that scale to thousands of instances. Check out the speedrun to get started in minutes.
]]></content:encoded></item><item><title>Data breach hitting PowerSchool looks very, very bad</title><link>https://arstechnica.com/security/2025/01/students-parents-and-teachers-still-smarting-from-breach-exposing-their-info/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2021/04/data-leak-1000x648.jpeg" length="" type=""/><pubDate>Thu, 23 Jan 2025 12:30:57 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Parents, students, teachers, and administrators throughout North America are smarting from what could be the biggest data breach of 2025: an intrusion into the network of a cloud-based service storing detailed data of millions of pupils and school personnel.The hack, which came to light earlier this month, hit PowerSchool, a Folsom, California, firm that provides cloud-based software to some 16,000 K–12 schools worldwide. The schools serve 60 million students and employ an unknown number of teachers. Besides providing software for administration, grades, and other functions, PowerSchool stores personal data for students and teachers, with much of that data including Social Security numbers, medical information, and home addresses.On January 7, PowerSchool revealed that it had experienced a network intrusion two weeks earlier that resulted in the “unauthorized exportation of personal information” customers stored in PowerSchool’s Student Information System (SIS) through PowerSource, a customer support portal. Information stolen included individuals’ names, contact information, dates of birth, medical alert information, Social Security Numbers, and unspecified “other related information.”]]></content:encoded></item><item><title>Researchers say new attack could take down the European power grid</title><link>https://arstechnica.com/security/2025/01/could-hackers-use-new-attack-to-take-down-european-power-grid/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/01/euripean-ripple-attack-flipper-zero-1152x648.jpg" length="" type=""/><pubDate>Thu, 23 Jan 2025 12:00:42 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Late last month, researchers revealed a finding that’s likely to shock some people and confirm the low expectations of others: Renewable energy facilities throughout Central Europe use unencrypted radio signals to receive commands to feed or ditch power into or from the grid that serves some 450 million people throughout the continent.Fabian Bräunlein and Luca Melette stumbled on their discovery largely by accident while working on what they thought would be a much different sort of hacking project. After observing a radio receiver on the streetlight poles throughout Berlin, they got to wondering: Would it be possible for someone with a central transmitter to control them en masse, and if so, could they create a city-wide light installation along the lines of Project Blinkenlights?
      Images showing Project Blinkenlights throughout the years.
        Credit:
          Positive Security
      
The first Project Blinkenlights iteration occurred in 2001 in Berlin, when the lights inside a large building were synchronized to turn on and off to give the appearance of a giant, low-resolution monochrome computer screen.]]></content:encoded></item><item><title>Mamba and Software Package Security with Sylvain Corlay</title><link>https://softwareengineeringdaily.com/2025/01/23/mamba-and-software-package-security-with-sylvain-corlay/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=mamba-and-software-package-security-with-sylvain-corlay</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED7456657227.mp3" length="" type=""/><pubDate>Thu, 23 Jan 2025 10:00:36 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[QuantStack is an open-source technology software company specializing in tools for data science, scientific computing, and visualization. They are known for maintaining vital projects such as Jupyter, the conda-forge package channel, and the Mamba package manager.Sylvain Corlay is the CEO of QuantStack. He joins the podcast to talk about his company, Conda, Mamba, the new Mamba 2.0 release, software package security, and more.]]></content:encoded></item><item><title>DeepSeek: New Free AI That Changes Everything!</title><link>https://www.youtube.com/watch?v=btvB56PkvwE</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/btvB56PkvwE?version=3" length="" type=""/><pubDate>Thu, 23 Jan 2025 09:46:27 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

Try it out (choose DeepSeek as your model): https://huggingface.co/chat/
Official (read the privacy policy below before you use this one): https://www.deepseek.com/

Run it at home:
https://www.reddit.com/r/selfhosted/comments/1i6ggyh/got_deepseek_r1_running_locally_full_setup_guide/
https://lmstudio.ai/

Links:
https://x.com/ivanfioravanti/status/1881565759123702140
https://eu.jotform.com/tables/241831324457354
https://x.com/deepseek_ai/status/1881318130334814301
https://x.com/nobody_qwert/status/1881620406710452535
https://github.com/bytedance/UI-TARS
https://github.com/bytedance/UI-TARS-desktop
https://x.com/arcprize/status/1881761991116927066
https://x.com/awnihannun/status/1881412271236346233
https://x.com/thegenioo/status/1881720731458797793
https://x.com/awnihannun/status/1881386796266946743
https://chat.deepseek.com/downloads/DeepSeek%20Privacy%20Policy.html
https://github.com/MoonshotAI/Kimi-k1.5
https://github.com/bytedance/UI-TARS-desktop
https://x.com/_akhaliq/status/1881886333431628001
https://x.com/awnihannun/status/1882105560201617903

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli Gallizzi
If you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

#deepseek

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu]]></content:encoded></item><item><title>Playing with the classification report</title><link>https://www.youtube.com/watch?v=765qaIk30Rs</link><author>probabl</author><category>dev</category><category>ai</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/765qaIk30Rs?version=3" length="" type=""/><pubDate>Thu, 23 Jan 2025 09:08:13 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Probabl</source><content:encoded><![CDATA[In this video we will play around with a confusion matrix widget that will help us understand how the numbers in the classification report in scikit-learn are created. The classification report is a great utility, but it can help to remind oneself of what the numbers really mean. 

Scikit-learn documentation:
https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html

Appendix with notebooks:
https://github.com/probabl-ai/youtube-appendix/tree/main/16-metrics

Website: https://probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Twitter: https://x.com/probabl_ai
Bluesky: https://bsky.app/profile/probabl.bsky.social
Discord: https://discord.probabl.ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

#probabl]]></content:encoded></item><item><title>Master the Art of REST API Design</title><link>https://blog.algomaster.io/p/master-the-art-of-rest-api-design</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64780231-6a91-4d72-8ea2-159d4cd71f4e_1504x1056.png" length="" type=""/><pubDate>Thu, 23 Jan 2025 05:34:38 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[ is one of the most crucial steps in  and a key topic of discussion in .A well-designed API allows developers to easily integrate with a system while ensuring scalability and security.Over the years, various  have emerged, including REST, GraphQL, gRPC, Webhooks and SOAP, each designed to address different needs.However,  continue to dominate web development due to their simplicity, scalability, flexibility, widespread adoption and alignment with HTTP standards.In this article, we will dive into covering: for building a well-structured, scalable, and secure RESTful API.Performance optimization techniques to enhance API efficiency and response times.REST (Representational State Transfer) is an architectural style for designing web services that enable communication between clients (e.g., web browsers, mobile apps) and servers over the .REST uses HTTP methods (GET, POST, PUT, DELETE, etc.) to retrieve, create, update, and delete resources.To build a well-designed REST APIyou must first understand the fundamentals of theHTTP protocol.1. HTTP Methods (Verbs) in REST APIsHTTP provides a set of  that define the type of operation to be performed on a resource.In RESTful architectures, these methods typically map to CRUD operations:It’s essential to use the correct HTTP method to make your API clear and intuitive. For example,  signals a read-only request to developers and should never modify server data, while  indicates data creation or an action that results in a change.2. REST is Resource-OrientedIn RESTful API design, data is represented as , and each resource is identified by a Uniform Resource Identifier (URI).→ A collection (or list) of books → A specific book with ID 123An  is a combination of:An HTTP method (GET, POST, PUT etc.)A resource URI (, )Each endpoint represents a specific operation on a resource. → Fetch all books → Create a new book → Delete the book with ID 123Using clear and consistent endpoints helps developers quickly understand how to interact with your API.4. HTTP Status Codes: Understanding API ResponsesEach API response includes an , which indicates the result of the request.Using meaningful status codes is important for helping consumers of your API understand why a request might have failed and how they can fix or retry it.Common status codes include:: The request was successfully received and processed.: The request succeeded.: A new resource was successfully created.: The request succeeded, but there is no content to return.: Further action is needed to complete the request (e.g., a different endpoint or resource location).: There was an error in the request sent by the client.: The request was malformed or invalid.: Authentication is required or has failed.: The client does not have permission to access the resource.: The requested resource does not exist.: Rate limit exceeded.: The server encountered an error while processing the request.500 Internal Server Error: A general error occurred on the server.: The server is currently unable to handle the request, often due to maintenance or overload.1. Define Clear Resource Naming ConventionsUsing a consistent, intuitive, and hierarchical structure for API endpoints improves both readability and usability. The goal is to help developers quickly understand how to interact with your API without extensive documentation.Since REST is resource-oriented, focus on  (nouns) rather than  (verbs) for your endpoints. The HTTP methods (, , etc.) already describe the action, so using verbs in the URL are redundant.GET /getAllUsers
POST /createNewOrder
DELETE /removeProduct/123GET /users
POST /orders
DELETE /products/123]]></content:encoded></item><item><title>How Meta discovers data flows via lineage at scale</title><link>https://engineering.fb.com/2025/01/22/security/how-meta-discovers-data-flows-via-lineage-at-scale/</link><author></author><category>dev</category><category>official</category><pubDate>Thu, 23 Jan 2025 05:00:45 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[ involves collecting various code and data assets (e.g., web endpoints, data tables, AI models) used across Meta. expresses data assets in structural detail (e.g., indicating that a data asset has a field called “religion”). labels data to describe its content (e.g., specifying that the identity column contains religion data).Scalable data flow discoveryEfficient rollout of privacy controlsContinuous compliance verificationCollecting data flow signalsIdentifying relevant data flowsExclude and include candidates: Focus on lineage early and reap the rewardsBuild lineage consumption tools to gain engineering efficiencyIntegrate lineage with systems to scale the coverageMeasurement improves our outcomesImproving consumption experience]]></content:encoded></item><item><title>December Project Goals Update</title><link>https://blog.rust-lang.org/2025/01/23/Project-Goals-Dec-Update.html</link><author>David Wood and Niko Matsakis</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[Our big goal for this period was async closures, and we are excited to announce that work there is done! Stable support for async closures landed on nightly on Dec 12 and it will be included in Rust 1.85, which ships on Feb 20. Big kudos to compiler-errors for driving that.For our other goals, we made progress, but there remains work to be done:Return Type Notation (RTN) is implemented and we had a call for experimentation but it has not yet reached stable. This will be done as part of our 2025H1 goal.Async Functions in Traits (and Return Position Impl Trait in Trait) are currently not consided  compatible. We would eventually like to have first-class  support, but as an intermediate step we created a procedural macro crate  that can create wrappers that enable . We are planning a comprehensive blog post in 2025H1 that shows how to use this crate and lays out the overall plan for async functions in traits.Work was done to prototype an implementation for async drop but we didn't account for reviewing bandwidth. nikomatsakis has done initial reads and is working with PR author to get this done in 2025H1. To be clear though the scope of this is an experiment with the goal of uncovering implementation hurdles. There remains significant language design work before this feature would be considered for stabilization (we don't even have an RFC, and there are lots of unknowns remaining).We have had fruitful discussions about the trait for  but do not have widespread consensus, that's on the docket for 2025H1.We largely completed our goal to stabilize the language features used by the Rust for Linux project. In some cases a small amount of work remains. Over the last six months, we...stabilized the  macro to get the offset of fields; stabilization PR and reference updates are up, excluding the "output" feature.completed the majority of the work for arbitrary self types, which is being used by RfL and just needs documentation before stabilisationWe also began work on compiler flag stabilization with RFC 3716, which outlines a scheme for stabilizing flags that modify the target ABI.The final release of Rust 2024 is confirmed for February 20, 2025 as part of Rust 1.85. Rust 1.85 is currently in beta. Feedback from the nightly beta and crater runs has been actively addressed, with adjustments to migrations and documentation to enhance user experience.Big shout-outs to TC and  Eric Huss for their hard work driving this program forward.Over the last six months a number of internal refactorings have taken place that are necessary to support a  prototype.One refactoring is that we have changed how we represent const arguments in the compiler to allow for adding a separate representation for the kinds of const arguments that  will add.Another big refactoring is that we have changed the API surface for our representation of const arguments in the type system layer, there is no longer a way to evaluate a const argument without going through our general purpose type system logic. This was necessary to ensure that we correctly handle equality of the kinds of const arguments that  will support.With all of these pre-requisite refactorings completed, a feature gate has been added to the compiler (feature(min_generic_const_args)) that uses the new internal representation of const arguments. We are now beginning to implement the actual language changes under this feature gate.Over the course of the last six months...cargo semver-checks began to include generic parameters and bounds in its schema, allowing for more precise lints;cargo manifest linting was implemented and merged, allowing for lints that look at the cargo manifest;building on cargo manifest linting, the  lint was added, which identifies breakage caused by the removal of a package feature.In addition, we fleshed out a design sketch for the changes in rustdoc's JSON support that are needed to support cross-crate item linting. This in turn requires compiler extensions to supply that information to rustdoc.Progress was made on adding const traits and implementation in the compiler, with improvements being carefully considered.  was constified in rust#133237 and / in rust#133260.Over the last six months, we created a lang-team experiment devoted to this issue and spastorino began work on an experimental implementation. joshtriplett authored RFC 3680, which has received substantial feedback. The current work is focused on identifying "cheaply cloneable" types and making it easy to create closures that clone them instead of moving them.Alternatives to sandboxed build scripts are going to be investigated instead of continuing this project goal into 2025h1 - namely, declaratively configuring system dependencies with , using an approach similar to code-checker Cackle and its sandbox environment Bubblewrap, or fully-sandboxed build environments like Docker or Nix.Significant speedups have been achieved, reducing the slowest crate resolution time from over 120 seconds to 11 seconds, and decreasing the time to check all crates from 178 minutes to 71.42 minutes.Performance improvements have been made to both the existing resolver and the new implementation, with the lock file verification time for all crates reduced from 44.90 minutes to 32.77 minutes (excluding some of the hardest cases).Our pull request adding example searches and adding a search button has been added to the agenda for the rustdoc team next meeting.The  stabilization is now stable in version 1.84, with a new update blogpost published.Significant progress was made on bootstrap with . We're now able to compile rustc and cargo, enabling try-builds and perf runs.An optimisation for the  lint is open, benchmarked, and currently under review.Help is needed on any issue marked with , especially on issue #13714.Over the course of this goal, Nadrieril wrote and posted the never patterns RFC as an attempt to make progress without figuring out the whole picture, and the general feedback was "we want to see the whole picture". Next step will be to write up an RFC that includes a clear proposal for which empty patterns can and cannot be omitted. This is 100% bottlenecked on my own writing bandwidth (reach out if you want to help!). Work will continue but the goal won't be resubmitted for 2025h1.Amanda has made progress on removing placeholders, focusing on lazy constraints and early error reporting, as well as investigating issues with rewriting type tests; a few tests are still failing, and it seems error reporting and diagnostics will be hard to keep exactly as today.@lqd has opened PRs to land the prototype of the location-sensitive analysis. It's working well enough that it's worthwhile to land; there is still a lot of work left to do, but it's a major milestone, which we hoped to achieve with this project goal.A fix stopping cargo-script from overriding the release profile was posted and merged.Help is wanted for writing frontmatter support in rustc, as rustfmt folks are requesting it to be represented in the AST.RFC is done, waiting for all rustdoc team members to take a look before implementation can start.SparrowLii proposed a 2025H1 project goal to continue stabilizing the parallel front end, focusing on solving reproducible deadlock issues and improving parallel compilation performance.The team discussed solutions to avoid potential deadlocks, finding that disabling work-stealing in rayon's subloops is effective, and will incorporate related modifications in a PR.Progress on  continued despite a busy schedule, with a focus on improving suggestions and addressing architectural challenges.A new API was designed in collaboration with epage, aiming to align  more closely with  for easier contribution and integration.The project goal slate for 2025h1 has been posted as an RFC and is waiting on approval from project team leads.Another pull request was merged with only one remaining until a working MVP is available on nightly.Some features were removed to simplify upstreaming and will be added back as single PRs.Will start work on  feature of LLVM/Enzyme which allows Array of Struct and Struct of Array vectorisation.There's been a push to add a AMD GPU target to the compiler which would have been needed for the LLVM offload project.We have written and verified around 220 safety contracts in the verify-rust-std fork.3 out of 14 challenges have been solved.We have successfully integrated Kani in the repository CI, and we are working on the integration of 2 other verification tools: VeriFast and Goto-transcoder (ESBMC)There wasn't any progress on this goal, but building a community around a-mir-formality is still a goal and future plans are coming.The following goals have not received updates in the last month:]]></content:encoded></item><item><title>How to align open LLMs in 2025 with DPO and and synthetic data</title><link>https://www.philschmid.de/rl-with-llms-in-2025-dpo</link><author></author><category>dev</category><category>ai</category><category>blog</category><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><source url="https://www.philschmid.de/">Phil Shmid</source><content:encoded><![CDATA[Learn how to align LLMs using Hugging Face TRL and RLHF through Direct Preference Optimization (DPO) and on-policy synthetic data.]]></content:encoded></item><item><title>Texas Is Enforcing Its State Data Privacy Law. So Should Other States.</title><link>https://www.eff.org/deeplinks/2025/01/texas-enforcing-its-state-data-privacy-law-so-should-other-states</link><author>Hayley Tsukayama</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/location-city.png" length="" type=""/><pubDate>Wed, 22 Jan 2025 22:31:12 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[States need to have and use data privacy laws to bring privacy violations to light and hold companies accountable for them. So, we were glad to see that the Texas Attorney General’s Office has filed its first lawsuit under Texas Data Privacy and Security Act (TDPSA) to take the Allstate Corporation to task for sharing driver location and other driving data without telling customers.In its complaint, the attorney general’s office alleges that Allstate and a number of its subsidiaries (some of which go by the name “Arity”) “conspired to secretly collect and sell ‘trillions of miles’ of consumers’ ‘driving behavior’ data from mobile devices, in-car devices, and vehicles.” (The defendant companies are also accused of violating Texas’ data broker law and its insurance law prohibiting unfair and deceptive practices.)On the privacy front, the complaint says the defendant companies created a software development kit (SDK), which is basically a set of tools that developers can create to integrate functions into an app. In this case, the Texas Attorney General says that Allstate and Arity specifically designed this toolkit to scrape location data. They then allegedly paid third parties, such as the app Life360, to embed it in their apps. The complaint also alleges that Allstate and Arity chose to promote their SDK to third-party apps that  required the use of location date, specifically so that people wouldn’t be alerted to the additional collection.That’s a dirty trick. Data that you can pull from cars is often highly sensitive, as wehaveraisedrepeatedly. Everyone should know when that information's being collected and where it's going.More state regulators should follow suit and use the privacy laws on their books.The Texas Attorney General’s office estimates that 45 million Americans, including those in Texas, unwittingly downloaded this software that collected their information, including location information, without notice or consent. This violates Texas’ privacy law, which went into effect in July 2024 and requires companies to provide a reasonably accessible notice to a privacy policy, conspicuous notice that they’re selling or processing sensitive data for targeting advertising, and to obtain consumer consent to process sensitive data.This is a low bar, and the companies named in this complaint still allegedly failed to clear it. As law firm Husch Blackwell pointed out in its write-up of the case, all Arity had to do, for example, to fulfill one of the notice obligations under the TDPSA was to put up a line on their website saying, “NOTICE: We may sell your sensitive personal data.”Texas’ privacy law also has a “right to cure”—essentially a 30-day period in which a company can “fix” a privacy violation and duck a Texas enforcement action. EFF opposes rights to cure, because they essentially give companies a “get-out-jail-free” card when caught violating privacy law. In this case, Arity was notified and given the chance to show it had cured the violation. It just didn’t.According the complaint, Arity apparently failed to take even basic steps that would have spared it from this enforcement action. Other companies violating our privacy may be more adept at getting out of trouble, but they should be found and taken to task too. That’s why we advocate for strong privacy laws that do even more to protect consumers.Nineteen states now have some version of a data privacy law. Enforcement has been a bit slower. California has brought a few enforcement actions since its privacy law went into effect in 2020; Texas and New Hampshire are two states that have created dedicated data privacy units in their Attorney General offices, signaling they’re staffing up to enforce their laws. More state regulators should follow suit and use the privacy laws on their books. And more state legislators should enact and strengthen their laws to make sure companies are truly respecting our privacy.]]></content:encoded></item><item><title>The FTC’s Ban on GM and OnStar Selling Driver Data Is a Good First Step</title><link>https://www.eff.org/deeplinks/2025/01/ftcs-ban-gm-and-onstar-selling-driver-behavior-good-first-step</link><author>Thorin Klosowski</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/icon-2019-privacy.png" length="" type=""/><pubDate>Wed, 22 Jan 2025 21:30:34 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dark web PHP dev Ross Ulbricht released from prison…</title><link>https://www.youtube.com/watch?v=gi-wuoIDdjw</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/gi-wuoIDdjw?version=3" length="" type=""/><pubDate>Wed, 22 Jan 2025 20:04:42 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Ross Ulbricht received a presidential pardon for his crimes related to the Silk Road. Let's take a look at the tech stack behind the Silk Road website, which included LAMP stack, PHP, Linux, Tor, and Cryptocurrency. 

#tech #webdev #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Tech Bros who went to jail https://youtu.be/CWeSzhJpJ9U
TailsOS in 100 Seconds https://youtu.be/mVKAyw0xqxw

📚 Chapters

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Why was Ross Ulbricht pardoned? 
- What was the Silk Road website?
- Tor explained 
- Technology used for anonymous browsing
- How does the dark web work?]]></content:encoded></item><item><title>VICTORY! Federal Court (Finally) Rules Backdoor Searches of 702 Data Unconstitutional</title><link>https://www.eff.org/deeplinks/2025/01/victory-federal-court-finally-rules-backdoor-searches-702-data-unconstitutional</link><author>Matthew Guariglia</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/nationalsecurity_banner-2_0.png" length="" type=""/><pubDate>Wed, 22 Jan 2025 19:08:08 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Better late than never: last night a federal district court held that backdoor searches of databases full of Americans’ private communications collected under Section 702 ordinarily require a warrant. The landmark ruling comes in a criminal case, United States v. Hasbajrami, after more than a decade of litigation, and over four years since the Second Circuit Court of Appeals found that backdoor searches constitute “separate Fourth Amendment events” and directed the district court to determine a warrant was required. Now, that has been officially decreed.In the intervening years, Congress has reauthorized Section 702 multiple times, each time ignoring overwhelming evidence that the FBI and the intelligence community abuse their access to databases of warrantlessly collected messages and other data. The Foreign Intelligence Surveillance Court (FISC), which Congress assigned with the primary role of judicial oversight of Section 702, has also repeatedly dismissed arguments that the backdoor searches violate the Fourth Amendment, giving the intelligence community endless do-overs despite its repeated transgressions of even lax safeguards on these searches.This decision sheds light on the government’s liberal use of what is essential a “finders keepers” rule regarding your communication data. As a legal authority, FISA Section 702 allows the intelligence community to collect a massive amount of communications data from overseas in the name of “national security.” But, in cases where one side of that conversation is a person on US soil, that data is still collected and retained in large databases searchable by federal law enforcement. Because the US-side of these communications is already collected and just sitting there, the government has claimed that law enforcement agencies do not need a warrant to sift through them. EFF argued for over a decade that this is unconstitutional, and now a federal court agrees with us.EFF argued for over a decade that this is unconstitutional, and now a federal court agrees with us.involves a U.S. resident who was arrested at New York JFK airport in 2011 on his way to Pakistan and charged with providing material support to terrorists. Only after his original conviction did the government explain that its case was premised in part on emails between Mr. Hasbajrami and an unnamed foreigner associated with terrorist groups, emails collected warrantless using Section 702 programs, placed in a database, then searched, again without a warrant, using terms related to Mr. Hasbajrami himself.The district court found that regardless of whether the government can lawfully warrantlessly collect communications between foreigners and Americans using Section 702, it cannot ordinarily rely on a “foreign intelligence exception” to the Fourth Amendment’s warrant clause when searching these communications, as is the FBI’s routine practice. And, even if such an exception did apply, the court found that the intrusion on privacy caused by reading our most sensitive communications rendered these searches “unreasonable” under the meaning of the Fourth Amendment. In 2021 alone, the FBI conducted 3.4 million warrantless searches of US person’s 702 data.In light of this ruling, we ask Congress to uphold its responsibility to protect civil rights and civil liberties by refusing to renew Section 702 absent a number of necessary reforms, including an official warrant requirement for querying US persons data and increased transparency. On April 15, 2026, Section 702 is set to expire. We expect any lawmaker worthy of that title to listen to what this federal court is saying and create a legislative warrant requirement so that the intelligence community does not continue to trample on the constitutionally protected rights to private communications. More immediately, the FISC should amend its rules for backdoor searches and require the FBI to seek a warrant before conducting them.]]></content:encoded></item><item><title>A stuff-a-thon is happening at the FSF, Jan. 24, 28</title><link>http://www.fsf.org/blogs/community/fsf-stuff-a-thon-jan-24-and-28</link><author></author><category>linux</category><pubDate>Wed, 22 Jan 2025 18:15:40 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[Volunteer with the FSF January 24, 28 by helping us send thank-you letters and welcome packets to supporters worldwide! ]]></content:encoded></item><item><title>Announcing the winner of the FSF40 Anniversary Logo Contest</title><link>http://www.fsf.org/blogs/community/fsf-40-logo-winner</link><author></author><category>linux</category><pubDate>Wed, 22 Jan 2025 16:50:00 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Codemods in other languages</title><link>https://martinfowler.com/articles/codemods-api-refactoring.html#CodemodsInOtherLanguages</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 22 Jan 2025 15:54:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Martin Fowler</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Protecting “Free Speech” Can’t Just Be About Targeting Political Opponents</title><link>https://www.eff.org/deeplinks/2025/01/protecting-free-speech-cant-just-be-about-targeting-political-opponents</link><author>Joe Mullin</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/intermediary-4.jpg" length="" type=""/><pubDate>Wed, 22 Jan 2025 15:40:34 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[In the Santa Clara Principles, a guideline for online content moderation authored by EFF and other civil society groups, we state that “governments and other state actors should themselves report their involvement in content moderation decisions, including data on demands or requests for content to be actioned or an account suspended, broken down by the legal basis for the request." This Executive Order doesn’t come close to embracing such a principle. As we have written before, the First Amendment forbids the government from coercing private entities to censor speech. This principle has countered efforts to pressure intermediaries like bookstores and credit card processors to limit others’ speech. But not every communication about user speech is unconstitutional; some are beneficial, like when platforms reach out to government agencies as authoritative sources of information. ]]></content:encoded></item><item><title>MasterCard DNS Error Went Unnoticed for Years</title><link>https://krebsonsecurity.com/2025/01/mastercard-dns-error-went-unnoticed-for-years/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Wed, 22 Jan 2025 15:24:41 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[The payment card giant  just fixed a glaring error in its domain name server settings that could have allowed anyone to intercept or divert Internet traffic for the company by registering an unused domain name. The misconfiguration persisted for nearly five years until a security researcher spent $300 to register the domain and prevent it from being grabbed by cybercriminals.A DNS lookup on the domain az.mastercard.com on Jan. 14, 2025 shows the mistyped domain name a22-65.akam.ne.From June 30, 2020 until January 14, 2025, one of the core Internet servers that MasterCard uses to direct traffic for portions of the mastercard.com network was misnamed. MasterCard.com relies on five shared Domain Name System (DNS) servers at the Internet infrastructure provider [DNS acts as a kind of Internet phone book, by translating website names to numeric Internet addresses that are easier for computers to manage].All of the Akamai DNS server names that MasterCard uses are supposed to end in “akam.net” but one of them was misconfigured to rely on the domain “.”This tiny but potentially critical typo was discovered recently by , founder of the security consultancy Seralys. Caturegli said he guessed that nobody had yet registered the domain akam.ne, which is under the purview of the top-level domain authority for the West Africa nation of Niger.Caturegli said it took $300 and nearly three months of waiting to secure the domain with the registry in Niger. After enabling a DNS server on akam.ne, he noticed hundreds of thousands of DNS requests hitting his server each day from locations around the globe. Apparently, MasterCard wasn’t the only organization that had fat-fingered a DNS entry to include “akam.ne,” but they were by far the largest.But the researcher said he didn’t attempt to do any of that. Instead, he alerted MasterCard that the domain was theirs if they wanted it, copying this author on his notifications. A few hours later, MasterCard acknowledged the mistake, but said there was never any real threat to the security of its operations.“We have looked into the matter and there was not a risk to our systems,” a MasterCard spokesperson wrote. “This typo has now been corrected.”Meanwhile, Caturegli received a request submitted through , a program that offers financial rewards and recognition to security researchers who find flaws and work privately with the affected vendor to fix them. The message suggested his public disclosure of the MasterCard DNS error via a post on LinkedIn (after he’d secured the akam.ne domain) was not aligned with ethical security practices, and passed on a request from MasterCard to have the post removed.MasterCard’s request to Caturegli, a.k.a. “Titon” on infosec.exchange.Caturegli said while he does have an account on Bugcrowd, he has never submitted anything through the Bugcrowd program, and that he reported this issue directly to MasterCard.“I did not disclose this issue through Bugcrowd,” Caturegli wrote in reply. “Before making any public disclosure, I ensured that the affected domain was registered to prevent exploitation, mitigating any risk to MasterCard or its customers. This action, which we took at our own expense, demonstrates our commitment to ethical security practices and responsible disclosure.” Most organizations have at least two authoritative domain name servers, but some handle so many DNS requests that they need to spread the load over additional DNS server domains. In MasterCard’s case, that number is five, so it stands to reason that if an attacker managed to seize control over just one of those domains they would only be able to see about one-fifth of the overall DNS requests coming in.But Caturegli said the reality is that many Internet users are relying at least to some degree on public traffic forwarders or DNS resolvers like  and .“So all we need is for one of these resolvers to query our name server and cache the result,” Caturegli said. By setting their DNS server records with a long TTL or “Time To Live” — a setting that can adjust the lifespan of data packets on a network — an attacker’s poisoned instructions for the target domain can be propagated by large cloud providers.“With a long TTL, we may reroute a LOT more than just 1/5 of the traffic,” he said.The researcher said he’d hoped that the credit card giant might thank him, or at least offer to cover the cost of buying the domain.“We obviously disagree with this assessment,” Caturegli wrote in a follow-up post on LinkedIn regarding MasterCard’s public statement. “But we’ll let you judge— here are some of the DNS lookups we recorded before reporting the issue.”Caturegli posted this screenshot of MasterCard domains that were potentially at risk from the misconfigured domain.As the screenshot above shows, the misconfigured DNS server Caturegli found involved the MasterCard subdomain . It is not clear exactly how this subdomain is used by MasterCard, however their naming conventions suggest the domains correspond to production servers at Microsoft’s  cloud service. Caturegli said the domains all resolve to Internet addresses at Microsoft.“Don’t be like Mastercard,” Caturegli concluded in his LinkedIn post. “Don’t dismiss risk, and don’t let your marketing team handle security disclosures.”One final note: The domain akam.ne has been registered previously — in December 2016 by someone using the email address um-i-delo@yandex.ru. The Russian search giant Yandex reports this user account belongs to an “Ivan I.” from Moscow. Passive DNS records from DomainTools.com show that between 2016 and 2018 the domain was connected to an Internet server in Germany, and that the domain was left to expire in 2018.This is interesting given a comment on Caturegli’s LinkedIn post from an ex-Cloudflare employee who linked to a report he co-authored on a similar typo domain apparently registered in 2017 for organizations that may have mistyped their AWS DNS server as “” instead of “.” DomainTools reports that this typo domain also was registered to a Yandex user (playlotto@yandex.ru), and was hosted at the same German ISP — Team Internet (AS61969).]]></content:encoded></item><item><title>The Internet is (once again) awash with IoT botnets delivering record DDoSes</title><link>https://arstechnica.com/security/2025/01/the-internet-is-once-again-awash-with-iot-botnets-delivering-record-ddoses/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/12/ddos-for-hire.jpg" length="" type=""/><pubDate>Wed, 22 Jan 2025 15:10:58 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[We’re only three weeks into 2025, and it’s already shaping up to be the year of Internet of Things-driven DDoSes. Reports are rolling in of threat actors infecting thousands of home and office routers, web cameras, and other Internet-connected devices.Here is a sampling of research released since the first of the year.Lax security, ample bandwidthA post on Tuesday from content-delivery network Cloudflare reported on a recent distributed denial-of-service attack that delivered 5.6 terabits per second of junk traffic—a new record for the largest DDoS ever reported. The deluge, directed at an unnamed Cloudflare customer, came from 13,000 IoT devices infected by a variant of Mirai, a potent piece of malware with a long history of delivering massive DDoSes of once-unimaginable sizes.]]></content:encoded></item><item><title>Introducing the EstimatorReport</title><link>https://www.youtube.com/watch?v=2rjkrR7CjHc</link><author>probabl</author><category>dev</category><category>ai</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/2rjkrR7CjHc?version=3" length="" type=""/><pubDate>Wed, 22 Jan 2025 14:49:03 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Probabl</source><content:encoded><![CDATA[Skore version 0.6 introduces a new EstimatorReport, which can be seen as a wrapper around an estimator that automatically detects all the relevant metrics and charts. The goal of the project is to be a useful sidekick for scikit-learn and you can expect more utilities to be released soon. 

Links:
skore v0.6 documentation:
https://skore.probabl.ai/0.6/index.html

skore GitHub repository:
https://github.com/probabl-ai/skore]]></content:encoded></item><item><title>Stealing HttpOnly cookies with the cookie sandwich technique</title><link>https://portswigger.net/research/stealing-httponly-cookies-with-the-cookie-sandwich-technique</link><author></author><category>infosec</category><pubDate>Wed, 22 Jan 2025 14:45:11 +0000</pubDate><source url="https://portswigger.net/research">Dev - PortSwitgger Research</source><content:encoded><![CDATA[In this post, I will introduce the "cookie sandwich" technique which lets you bypass the HttpOnly flag on certain servers. This research follows on from Bypassing WAFs with the phantom $Version cookie]]></content:encoded></item><item><title>How Docker Streamlines the Onboarding Process and Sets Up Developers for Success</title><link>https://www.docker.com/blog/how-docker-streamlines-the-onboarding-process-and-sets-up-developers-for-success/</link><author>Yiwen Xu</author><category>docker</category><category>devops</category><pubDate>Wed, 22 Jan 2025 14:00:00 +0000</pubDate><source url="https://www.docker.com/">Docker blog</source><content:encoded><![CDATA[Nearly half (45%) of developers say they don’t have enough time for learning and development, according to a developer experience research study by Harness and Wakefield Research. Additionally, developer onboarding is a slow and painful process, with 71% of executive buyers saying that onboarding new developers takes at least two months. To accelerate innovation and bring products to market faster, organizations must empower developers with robust support and intuitive guardrails, enabling them to succeed within a structured yet flexible environment. That’s where Docker fits in: We help developers onboard quickly and help organizations set up the right guardrails to give developers the flexibility to innovate within the boundaries of company policies. Setting up developer teams for success Docker is recognized as one of the most used, desired, and admired developer tools, making it an essential component of any development team’s toolkit. For developers who are new to Docker, you can quickly get them up and running with Docker’s integrated development workflows, verified secure content, and accessible learning resources and community support.Streamlined developer onboardingWhen new developers join a team, Docker Desktop can significantly reduce the time and effort required to set up their development environments. Docker Desktop integrates seamlessly with popular IDEs, such as Visual Studio Code, allowing developers to containerize directly within familiar tools, accelerating learning within their usual workflows. Docker Extensions expand Docker Desktop’s capabilities and establish new functionalities, integrating developers’ favorite development tools into their application development and deployment workflows. Developers can also use Docker for GitHub Copilot for seamless onboarding with assistance for containerizing applications, generating Docker assets, and analyzing project vulnerabilities. In fact, the Docker extension is a top choice among developers in GitHub Copilot’s extension leaderboard, as highlighted by .Docker Build Cloud integrates with Docker Compose and CI workflows, making it a seamless transition for dev teams. Verified content on Docker Hub gives developers preconfigured, trusted images, reducing setup time and ensuring a secure foundation as they onboard onto projects. Docker Scout provides actionable insights and recommendations, allowing developers to enhance their container security awareness, scan for vulnerabilities, and improve security posture with real-time feedback. And, Testcontainers Cloud lets developers run reliable integration tests, with real dependencies defined in code. With these tools, developers can be confident about delivering high-quality and reliable apps and experiences in production.  Continuous learning with accessible knowledge resourcesContinuous learning is a priority for Docker, with a wide range of accessible resources and tools designed to help developers deepen their knowledge and stay current in their containerization journey.Docker Docs offers beginner-friendly guides, tutorials, and AI tools to guide developers through foundational concepts, empowering them to quickly build their container skills. Our collection of guides takes developers step by step to learn how Docker can optimize development workflows and how to use it with specific languages, frameworks, or technologies.Docker Hub’s AI Catalog empowers developers to discover, pull, and integrate AI models into their workflows, bridging the gap between innovation and implementation. Docker also offers regular webinars and tech talks that help developers stay updated on new features and best practices and provide a platform to discuss real-world challenges. If you’re a Docker Business customer, you can even request additional, customized training from our Docker experts. Docker’s partnerships with educational platforms and organizations, such as Udemy Training and LinkedIn Learning, ensure developers have access to comprehensive training — from beginner tutorials to advanced containerization topics.Docker’s global developer communityOne of Docker’s greatest strengths is its thriving global developer community, offering organizations a unique advantage by connecting them with a wealth of shared expertise, resources, and real-world solutions.With more than 20 million monthly active users, Docker’s community forums and events foster vibrant collaboration, giving developers access to a collective knowledge base that spans industries and expertise levels. Developers can ask questions, solve challenges, and gain insights from a diverse range of peers — from beginners to seasoned experts. Whether you’re troubleshooting an issue or exploring best practices, the Docker community ensures you’re never working in isolation.A key pillar of this ecosystem is the Docker Captains program — a network of experienced and passionate Docker advocates who are leaders in their fields. Captains share technical knowledge through blog posts, videos, webinars, and workshops, giving businesses and teams access to curated expertise that accelerates onboarding and productivity.Beyond forums and the Docker Captains program, Docker’s community-driven events, such as meetups and virtual workshops (Figure 1), provide developers with direct access to real-world use cases, innovative workflows, and emerging trends. These interactions foster continuous learning and help developers and their organizations keep pace with the ever-evolving software development landscape.For businesses, tapping into Docker’s extensive community means access to a vast pool of knowledge, support, and inspiration, which is a critical asset in driving developer productivity and innovation.Empowering developers with enhanced user management and securityTo scale and standardize app development processes across the entire company, you also need to have the right guardrails in place for governance, compliance, and security, which is often handled through enterprise control and admin management tools. Ideally, organizations provide guardrails without being overly prescriptive and slowing developer productivity and innovation. Modern enterprises require a layered security approach, beginning with trusted content as the foundation for building robust and compliant applications. This approach gives your dev teams a good foundation for building securely from the start. Throughout the software development process, you need a secure platform. For regulated industries like finance and public sectors, this means fortified dev environments. Security vulnerability analysis and policy evaluation tools also help inform improvements and remediation. Additionally, you need enterprise controls and dashboards that ensure enterprise IT and security teams can confidently monitor and manage risk. Setting up the right guardrails Docker provides a number of admin tools to safeguard your software with integrated container security in the Docker Business plan. Our goal is to improve security and compliance of developer environments with minimal impact on developer experience or productivity. Centralized settings for improved dev environments security With a solid foundation to build securely from the start, your organization can further enhance security throughout the software development process. Docker ensures software supply chain integrity through vulnerability scanning and image analysis with Docker Scout. Rapid remediation capabilities paired with detailed CVE reporting help developers quickly find and fix vulnerabilities, resulting in speedy time to resolution.Although containers are generally secure, container development tools still must be properly secured to reduce the risk of security breaches in the developer’s environment. Hardened Docker Desktop is an example of Docker’s fortified development environments with enhanced container isolation. It lets you enforce strict security settings and prevent developers and their containers from bypassing these controls. With air-gapped containers, you can further restrict containers from accessing network resources, limiting where data can be uploaded to or downloaded from.Continuous monitoring and managing risksWith the Admin Console and Docker Desktop Insights, IT administrators and security teams can visualize and understand how Docker is used within their organizations and manage the implementation of organizational configurations and policies (Figure 2). These insights help teams streamline processes and improve efficiency. For example, you can enforce sign-in for developers who don’t sign in to an account associated with your organization. This step ensures that developers receive the benefits of your Docker subscription and work within the boundaries of the company policies. For business and engineering leaders, full visibility and governance over the development process help ensure compliance and mitigate risk while driving developer productivity. Unlock innovation with Docker’s development suiteDocker is the leading suite of tools purpose-built for cloud-native development, combining a best-in-class developer experience with enterprise-grade security and governance. With Docker, your organization can streamline onboarding, foster innovation, and maintain robust compliance — all while empowering your teams to deliver impactful solutions to market faster and more securely. Explore the Docker Business plan today and unlock the full potential of your development processes.]]></content:encoded></item><item><title>How Bluesky Works 🦋</title><link>https://newsletter.systemdesign.one/p/how-does-bluesky-work</link><author>Neo Kim</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/449c6406-d812-4547-bc2a-8e55c325ce3c_1280x720.gif" length="" type=""/><pubDate>Wed, 22 Jan 2025 13:13:07 +0000</pubDate><source url="https://newsletter.systemdesign.one/">Dev - System Design Newsletter</source><content:encoded><![CDATA[Get the powerful template to approach system design for FREE on newsletter sign-up:This post outlines Bluesky architecture; you will find references at the bottom of this page if you want to go deeper.Note: I wrote this post after reading their engineering blog and documentation.Once upon a time, Twitter had only a few million users.And each user interaction went through their centralized servers.So content moderation was easy.Yet their growth rate was explosive and became one of the most visited sites in the world.So they added automation and human reviewers to scale moderation.Although it temporarily solved their content moderation issues, there were newer problems. Here are some of them:The risk of error increases if a single authority decides the moderation policies.There’s a risk of bias when a single authority makes moderation decisions.Managing moderation at scale needs a lot of effort; it becomes a bottleneck.So they set up Bluesky: a research initiative to build a decentralized social network.A decentralized architecture distributes control across many servers.Here are 3 popular decentralized architectures in distributed systems:Federated architecture: client-server model; but different people run parts of the system and parts communicate with each other.Peer-to-Peer architecture: there’s no difference between client and server; each device acts as both.Blockchain architecture: distributed ledger for consensus and trustless interactions between servers.Bluesky uses a federated architecturefor its familiar client-server model, reliability, and convenience. So each server could be run by different people and servers communicate with each other over HTTP.Think of the federated network as email; a person with Gmail can communicate with someone using Protonmail.Yet building a decentralized social network at scale is difficult.So smart engineers at Bluesky used simple ideas to solve this hard problem.I wrote a summary of this post (save it for later):They created a decentralized open-source framework to build social networking apps, called Authenticated Transfer Protocol (), and built Bluesky on top of it.Put simply, Bluesky doesn’t run separate servers; instead, ATProto servers distribute messages to each other.A user’s data is shared across apps built on ATProto; Bluesky is one of the apps. So if a user switches between apps on ATProto, such as a photo-sharing app or blogging app, they don’t lose their followers (social graph).Imagine ATProto as Disneyland Park and Bluesky as one of its attractions. A single ticket is enough to visit all the attractions in the park. And if you don’t like one of the attractions, try another one in the park.Here’s how Bluesky works:A post is a short status update by a user; it includes text and images. The text content and timestamp of the post are stored in a . Think of the repository as a collection of data published by a single user.SQLite is used as its data layer for simplicity; each repository gets a separate SQLite database.The data records are encoded in CBOR, a compact binary format, before storing it in SQLite for low costs.Repositories of different users are stored on a ; they set up many data servers for scale.A data server exposes HTTP to handle client requests. Put simply, the data server acts as a proxy for all client interactions. Also it manages user authentication and authorization. The data server includes tooling to automatically apply updates in the federated architecture.They run 6 million user repositories on a single data server at 150 USD per month.Think of the user repository as a Git repo and the data server as GitHub. It’s easy to move a Git repo from GitHub to GitLab. Similarly, a user repository is movable from one data server to another.Besides it’s possible to use alternative clients for Bluesky. Yet it’s necessary to maintain a standard data schema for interactions. So separate  and API endpoints are defined for each app on ATProto, including Bluesky.A user's repository doesn’t store information about actions performed by their followers such as comments or likes on their post. Instead, it’s stored only in the repository of the follower who took the action.A post is shown to the user’s followers.Yet it’s expensive to push updates to each follower’s repository. So information is collected from every data server using the .The crawler doesn’t index data but forwards it.The crawler subscribes for updates on the data server: new posts, likes, or comments.The data server notifies the crawler about updates in real time over websockets.The crawler collects information from data servers and generates a .Consider the generated stream as a log over websockets; put simply, the crawler combines each user’s actions into a single TCP connection.A user's post is shown to followers only after counting the likes, comments, and reposts on it.Yet the stream doesn’t contain this information. So the stream’s data is aggregated using the ; it transforms raw data into a consumable form by processing it. Imagine the index server as a data presentation layer.The index server is built using the Go language for concurrency. A NoSQL database, ScyllaDB, is used as its data layer for horizontal scalability.A reference to the user's post ID is added to the follower’s repository when they like or repost a post. So the total number of likes and reposts is calculated by crawling every user repository and adding the numbers.Here’s the workflow for displaying a post:A user’s request is routed via their data server to the index server.The data server finds the people a user follows by looking at their repository.The index server creates a list of post IDs in reverse chronological order.The index server expands the list of post IDs to full posts with content.The index server then responds to the client.In short, a user repository stores primary data, while the index server stores derived data from repositories.The index server is the most read-heavy service; so, its results are cached using Redis, an in-memory storage, for performance.JSON Web Token () is used for authentication between Bluesky services.Media files, such as images and videos, are stored on the data server’s disk for simplicity. A cryptographic ID () is used to reference the media files in the repository. The index server fetches the media files from the data server on user request and caches them on the content delivery network () for efficiency.A user updates only their repository when they follow someone. Their repository adds a reference to the user’s unique decentralized identifier () to indicate follow.The number of followers for a user is found by indexing every repository. This is similar to how Google finds inbound links to a web page; all documents on the web are crawled.A user account includes a handle based on the domain name (); it keeps things simple.And each user automatically gets a handle from the ‘bsky.social’ subdomain upon account creation. Yet posts are stored using DID, and the user handle is displayed along with the posts. So changes to a user's handle don’t affect their previous posts.A user’s DID is immutable, but the handle is mutable; put simply, a user’s handle is reassignable to a custom domain name.Here’s how a user handle with a custom domain name is verified on Bluesky:The user enters their custom domain name in the Bluesky account settings.Bluesky generates a unique text value for the user: public key.The user stores this value in the DNS TXT record of the custom domain name.The index server then periodically checks the DNS TXT record and validates the user handle. Imagine  as a text data field for domain name settings.3. User Timeline and FeedA user’s timeline is created by arranging their posts in reverse chronological order.Yet a user’s repository doesn’t contain information about likes and comments received on a post. So the request is sent to the index server; it returns the user’s timeline with aggregated data.A feed is created from posts by people a user follows.Bluesky supports feeds with custom logic, and there are 50K+ custom feeds available. Consider the  as a filter for a specific list of keywords or users.The crawler generates a stream from data servers.The feed generator filters, sorts, and ranks the content based on custom logic.The feed generator creates a list of post IDs.The index server then populates the feed’s content on a user request. is used to fetch the feed; it offers better performance. It includes an extra parameter in API requests and responses. The cursor parameter points to a specific item in the dataset; for example, the post’s timestamp to fetch feed until a specific post.A sequential unique column, such as the post's timestamp, is chosen for pagination.The user requests include the cursor parameter to indicate the result’s offset.The index server uses the cursor parameter to paginate the result dataset.The index server responds with the cursor parameter for future requests.The client decides the cursor parameter’s window size based on its viewport. The cursor-based pagination scales well as it prevents a full table scan in the index server.Imagine a  as metadata to categorize content. Besides it’s possible to apply a label manually. A user chooses to hide or show a warning for posts with a specific label.This preference is stored on their data server.Here’s how Bluesky moderation works:The moderation service consumes the stream from the crawler.The moderation service analyzes the content and assigns a label to it.The index server stores the content along with its label.The user requests are routed via the data server to the index server.The data server includes label IDs in HTTP request headers before forwarding them.The index server applies the label setting in the response.Also a data server operator does basic moderation. The data server filters out the muted users before responding to the client.Put simply, a user can post about anything, but the content is moderated before being shown to the public.Bluesky allows a user to see how many people they follow also follow a specific user: social proof.For example, when I visit Jay’s Bluesky profile, it shows how many people I follow also follow her.Let’s dive into the different approaches they used to build this feature.Find the people I follow by querying the database.Do separate parallel queries for each user to find the people they follow.Check if any of them also follow Jay.But this approach won’t scale as the number of parallel queries increases with the number of people a user follows.A scalable approach is to convert it into a set intersection problem:Set 1 tracks the people I follow.Set 2 tracks the people who follow Jay.The intersection of these sets gives the expected result.An in-memory graph service prevents expensive database queries and performs intersections quickly. Yet Redis Sets don’t use different CPU cores at once. So here’s how they implemented a minimum viable product:Each user has a 32-character DID.The DID values are converted into uint64 to reduce memory usage.Each user’s DID maintains 2 sets: people they follow and people who follow them.But it still consumes a lot of memory and takes extra time to start.So they optimized the graph service by implementing it using Roaring Bitmaps.Yet let’s take a step back and learn Bitmaps to better understand Roaring Bitmaps.A  represents binary states using bit arrays.Imagine Bluesky has only 100 users.The index value of the people I follow is set to 1 on the Bitmap; the people I follow are then found by walking the Bitmap and recording the index of non-zero bits. A constant time lookup tells whether I follow a specific user. Although Bitmaps do faster bitwise operations, it’s inefficient for sparse data. A 100-bit long Bitmap is needed even if I follow only a single user; so, it won’t scale for Bluesky’s needs.It turns consecutive 0s and 1s into a number for reduced storage. For example, if I follow the last 10 users, only the last 10 indices are marked by 1. With Run-Length Encoding, it’s stored as 90 0s and 10 1s, thus low storage costs.But this approach won’t scale for randomly populated data, and a lookup walks the entire Bitset to find an index.Ready for the best technique? .Think of it as compressed Bitmaps, but 100 times faster. A Roaring Bitmap splits data into containers and each container uses a different storage mechanism.The dense data is stored in a container using Bitmap; it uses a fixed-size bit array.The data with a large contiguous range of integers are stored in a container using run-length encoding; which reduces storage costs.The sparse data is stored in a container as integers in a sorted array; it reduces storage needs.Put simply, Roaring Bitmaps use different containers based on data sparsity.The set intersection is done in parallel and a container is converted into a different format for the set intersection. Also the graph data is stored and transferred over the network in Roaring Bitmap’s serialization format.Many instances of graph service are run for high availability and rolling updates.Bluesky supports short videos up to 90 seconds long.The video is streamed through HTTP Live Streaming (). Think of HLS as a group of text files; it’s a standard for adaptive bitrate video streaming.A client dynamically changes video quality based on network conditions.A video is encoded into different quality levels: 480p, 720p, 1080p, and so on.Each encoded video is split into small segments of 5 seconds long.The client checks the network conditions and requests video segments of the right quality.HLS uses Playlist files to manage encoded video segments. Imagine the  as a text file, and there are 2 types of Playlists:Master playlist: list of all video quality levels available.Media playlist: list of all video segments for a specific video quality.First, the client downloads the Master playlist to find available quality levels. Second, it selects a Media playlist based on network conditions. Third, it downloads the video segments in the sequence defined on the Media playlist.The videos and playlists are cached in CDN to reduce costs and handle bandwidth needs. The stream requests are routed to CDN via 302 HTTP redirects.The video views are tracked by counting requests to its Master playlist file.And the response for the Master playlist includes a Session ID; it’s included in future requests for Media playlists to track the user session.Besides the video seconds watched by a user are found by checking the last fetched video segment.A video subtitle is stored in text format and has a separate Media playlist file.The Master playlist includes a reference to subtitles; here’s how it works:The client finds the available subtitles by querying the Master playlist.The client downloads a specific subtitle's Media playlist based on the language selected by the user.The client then downloads the subtitle segments defined in the Media Playlist.Subscribe to get simplified system design case studies delivered to your inbox:Bluesky Quality AttributesA user’s data is cryptographically signed before saving it in the data server; it shows data authorship.The Merkle search tree is used to check the integrity of data, such as posts, transferred over the network.Think of the  as an efficient data structure to check data integrity. It doesn’t store original data; instead, it has only cryptographic hashes representing data.Besides the crawler keeps a copy of each user’s repository; it verifies the Merkle search tree signatures on data updates.The crawler generates a stream to update the index server.Yet the stream increases bandwidth and infrastructure costs. So the events are validated on the crawler and services, such as the feed generator, are connected to a trusted crawler. Besides Merkle search tree blocks are converted to JSON objects, which lack verifiability and signatures, to reduce stream size.Put simply, Merkle search tree verification overhead is avoided by consuming the stream from a trustable crawler.The crawler transfers the stream to the index server over websockets; zstd compresses websocket traffic for performance.A user, with a massive following, needs a data server with only tiny computing resources as the index server displays aggregated data.Every change to a user's profile is propagated via the index server; which, means eventual consistency.Yet a user viewing their profile right after an update should see the latest changes. So Bluesky offers read-after-write consistency for profile updates; here’s how:All client requests are routed via the data server to the index server.The response headers from the index server are compared against the data server. The response is updated to include missing records before returning it to the client.This read-after-write behavior applies only to a user viewing their profile; others see the changes eventually.A popular user's post notifies many users, which results in many requests to view the post at once.The results from the index server are cached to reduce the database load. Yet a popular user's post creates a thundering herd problem.A view request, the first one, from a follower reaches the cache server.The cache server queries the database for post content.Requests from others reach the database while the cache waits for the first response.It groups requests for the same data within a short period. Then forwards only a single request to the database; thus, preventing the thundering herd problem.The data server notifies the crawler about data changes in repositories over websockets.Yet network interruptions occur, so the crawler periodically checks repositories. It then compares the data against its local replica to find data changes.Also requests are rate-limited to keep the system operational.HTTP 429 (Too Many Requests) is returned in response headers when the rate limit threshold is exceeded. Interactions between servers are also rate-limited to avoid abuse in the decentralized architecture. A new data server’s request limit capacity is increased over time, based on trust, to keep the crawler running smoothly.Besides a user is only allowed up to 1,666 requests an hour to reduce bot activity.Bluesky has more than 28 million users; it became an independent company in 2021.Although anybody can host a server on ATProto, most Bluesky services are now run by a single company.ATProto and Bluesky look like an interesting new approach to social networks.👋  -Do you want to level up at work and better understand technical discussions?My mission is to help you go from 0 to 1 in system design by spending less than 37 minutes a month.This is the first deep dive article in this newsletter; it's a free preview.Yet future deep dives, starting this February, are available only to paid subscribers.And subscription fees will be way higher than the current fees.So pledge now to get access at a very low price."This newsletter gave me ideas on how to build scalable systems." Aditya, Google.Subscribe to get simplified case studies delivered straight to your inbox:Thank you for supporting this newsletter. Consider sharing this post with your friends and get rewards. Y’all are the best.]]></content:encoded></item><item><title>Ableton Live with Tobias Hahn</title><link>https://softwareengineeringdaily.com/2025/01/22/ableton-live-with-tobias-hahn/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=ableton-live-with-tobias-hahn</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED4933024707.mp3" length="" type=""/><pubDate>Wed, 22 Jan 2025 10:00:07 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[Ableton is a music software and hardware company based in Germany. The company develops Ableton Live which is a digital audio workstation for both improvisation and traditional arrangements.The software is remarkable for successfully blending good UI design with a powerful feature set. This has made it popular with new musicians as well as professionals such as Tame Impalla, Knxwledge, Mac DeMarco, and Daft Punk, among many others.Tobi Hahn is Ableton’s Engineering Manager. He joins the podcast to talk about software engineering for Ableton Live.]]></content:encoded></item><item><title>Using AI to Solve the Deepest Math Conjectures</title><link>https://www.datasciencecentral.com/using-ai-to-solve-the-deepest-math-conjectures/</link><author>Vincent Granville</author><category>dev</category><category>ai</category><pubDate>Wed, 22 Jan 2025 09:18:50 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Tools such as OpenAI can on occasion give the impression that they are able to prove theorems and even generalize them. Whether this is a sign of real (artificial) intelligence or simply combining facts retrieved from technical papers and put things together without advanced logic, is irrelevant. A correct proof is a correct proof, no… Read More »]]></content:encoded></item><item><title>This Week in Rust 583</title><link>https://this-week-in-rust.org/blog/2025/01/22/this-week-in-rust-583/</link><author>TWiR Contributors</author><category>This week in Rust</category><category>dev</category><category>rust</category><pubDate>Wed, 22 Jan 2025 05:00:00 +0000</pubDate><source url="https://this-week-in-rust.org/">This Week in Rust</source><content:encoded><![CDATA[This week's crate is resvg, an SVG rendering library.An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.  The following
RFCs would benefit from user testing before moving forward:No calls for testing were issued this week.No calls for testing were issued this week.No calls for testing were issued this week.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.A very quiet week for performance, with small improvements essentially on all benchmarks.0 Regression, 1 Improvement, 2 Mixed; 0 of them in rollups
40 artifact comparisons made in totalEvery week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.No Cargo Tracking Issues or PRs entered Final Comment Period this week.No Language Team Proposals entered Final Comment Period this week.No Language Reference RFCs entered Final Comment Period this week.No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.Rusty Events between 2025-01-22 - 2025-02-19 🦀If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.Memory safety issues mean you can’t trust what you’re seeing in your source code anymore.]]></content:encoded></item><item><title>Risky Business #776 -- Trump will flex American cyber muscles</title><link>https://risky.biz/RB776/</link><author></author><category>Risky Business blog</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/media3.risky.biz/RB776.mp3" length="" type=""/><pubDate>Wed, 22 Jan 2025 03:18:15 +0000</pubDate><source url="https://risky.biz/">Risky Business</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>This Week In React #218: React-Scan, Server Functions, Portals, Forms, Storybook, React Aria, Unpic, React Native 0.77, Reanimated, Expo, EAS, Legend-List, Shadowlist, Document Picker, BottomTabs, Swift, Bun, Rspack, Vitest, Biome, ESLint...</title><link>https://thisweekinreact.com/newsletter/218</link><author></author><category>frontend</category><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><source url="https://thisweekinreact.com/newsletter">This week in react</source><content:encoded><![CDATA[This is a crazy week, we have a lot of major releases coming from everywhere, in all the sections of this newsletter, that it's impossible to cover them all as they deserve.I'm particularly excited about the React Native ecosystem improving CSS compatibility with v0.77 and Reanimated v4. This is awesome news for cross-platform development compatibility.The frontend ecosystem is on fire too 🔥 with awesome releases such as Bun 1.2, Rspack 1.2, Vitest 3.0.And, Tailwind v4 is expected to drop anytime soon too ! (just after I hit send of course 😅)As always, thanks for supporting us on your favorite platform:If your authentication isn't , it's .With PropelAuth, you get everything you need to launch and scale your B2B product:UIs that cover all your use cases - signup, login, MFA, organization management and more. so you can provide top-notch support to your users. and advanced security features so you can be upmarket-ready. to help you strategize and expand.This is the first official release of React Scan, a new React performance tool that has been trending on social platforms lately with many great demos from its creator Aiden Bai. It’s easy to install and helps you find unwanted React re-renders with a great DX, a bit like “why-did-you-update” on steroids. Related podcast: 🎙️ JavaScript Jabber - Optimizing React with React Scan.Meticulous creates and maintains an exhaustive suite of e2e ui tests with  developer effort.This quote from the CTO of Traba sums the product up best: "Meticulous has fundamentally changed the way we approach frontend testing in our web applications, fully eliminating the need to write any frontend tests. The software gives us confidence that every change will be completely regression tested, allowing us to ship more quickly with significantly fewer bugs in our code. The platform is easy to use and reduces the barrier to entry for backend-focused devs to contribute to our frontend codebase."Meticulous is used by companies like Lattice, Bilt Rewards, Power and many others.This post from our CTO sets out the context of why exhaustive testing can double engineering velocity.As planned, the next React Native version just dropped. It notably ships with:CSS support improvement: , , , and Support for Android 15 edge-to-edge and 16KB memory page size (perf improvement)iOS init template with Swift AppDelegateGood news for Expo users: SDK 52 already supports v0.77, alongside many infrastructure packages (Screens, RNGH, Reanimated). For React 19 support, we’ll have to wait v0.78, but I could come sooner than you expect!Reanimated 4 is in beta, and it’s a huge one! 🔥 The animation core has been revamped. You can now just animate things, with regular CSS, on the native thread, thanks to newly added support for:This is particularly exciting for React Native micro-interactions with greatly reduced boilerplate, and the ability to ship cross-platform animations. The existing worklet APIs remains retrocompatible and relevant for more advanced needs. However, the New Architecture is now a requirement.]]></content:encoded></item><item><title>Rust 2024 in beta channel</title><link>https://blog.rust-lang.org/2025/01/22/rust-2024-beta.html</link><author>TC &amp; Eric Huss</author><category>dev</category><category>official</category><category>rust</category><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[The next edition, Rust 2024, has entered the beta channel.  It will live there until 2025-02-20, when Rust 1.85 and Rust 2024 will be released as stable.We're really happy with how Rust 2024 has turned out, and we're looking forward to putting it in your hands.You can get a head start in preparing your code for the new edition, and simultaneously help us with final testing of Rust 2024, by following these steps within a project:Run cargo +beta fix --edition.Set  and, if needed, , in .Run , address any remaining warnings, and then run other tests.More details on how to migrate can be found here and within each of the chapters describing the changes in Rust 2024.  For more on the changes themselves, see the Edition Guide.If you encounter any problems or see areas where we could make the experience better, tell us about it by filing an issue.]]></content:encoded></item><item><title>Pre-RFC - Rename annotations</title><link>https://poignardazur.github.io//2025/01/22/rename-annotations/</link><author>Olivier Faure</author><category>Olivier Faure blog</category><category>dev</category><category>rust</category><category>blog</category><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><source url="https://poignardazur.github.io//">PoignardAzur</source><content:encoded><![CDATA[Let’s imagine you’re writing a crate.Your crate has a single  file, with two modules and an arbitrary number of items:After a while, you decide that Foobar should really be exported from the  module. It’s a breaking change, but you’re fine with releasing a new major version:Any user previously importing from your crate will get this error when they bump the version number:error[E0432]: unresolved import `best_crate::bar::Foobar`
  --> src/lib.rs:12:5
   |
12 | use best_crate::bar::Foobar;
   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`
   |
help: consider importing this struct instead
   |
12 | use best_crate::foo::Foobar;
   |     ~~~~~~~~~~~~~~~~~~~~~~~~
This isn’t ideal, but at least there’s the “consider importing this instead” message giving these users an easy way to fix this.But now let’s say you decide that “Foobar” is a terrible name, and your struct should really be named “Foofoo” instead for consistency:Now your users will get this message:error[E0432]: unresolved import `best_crate::bar::Foobar`
  --> src/lib.rs:12:5
   |
12 | use best_crate::bar::Foobar;
   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`
No message to help them figure out what to use instead.Rust should have an attribute to inform the compiler that an item previously existed, but has been moved and/or renamed:You might want to put the attribute on the “new” item instead, in which case the syntax would be:You’d probably want both, for cases where either the module of origin is removed, or the destination is no longer in the same crate (e.g. because you’ve split your crate into sub-crates).This diagnostic could help the compiler give more helpful error messages:error[E0432]: unresolved import `best_crate::bar::Foobar`
  --> src/lib.rs:12:5
   |
12 | use best_crate::bar::Foobar;
   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`
   |
help: this item has been renamed to `best_crate::foo::Foofoo`
   |
12 | use best_crate::foo::Foofoo;
   |     ~~~~~~~~~~~~~~~~~~~~~~~~
Because the compiler is  that this is the correct move and not just guessing based on name similarity,  and similar tools would be able to automatically apply the rename.Rename annotations would be helpful as a “grace period” after a crate’s major version change, but they would also be useful for purely internal refactors, using  to change  directives throughout your codebase.All in all, this feels like a pretty useful feature which, thanks to the  namespace’s relaxed constraints, could be implemented relatively swiftly in the Rust toolchain.]]></content:encoded></item><item><title>Go 1.24 Release Candidate 2</title><link>https://golangweekly.com/issues/539</link><author></author><category>dev</category><category>go</category><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><source url="https://golangweekly.com/">Golang Weekly</source><content:encoded><![CDATA[VHS 0.9 – CLI 'home video recorder'. "Write terminal GIFs as code for integration testing and demoing your CLI tools." v0.9 adds a  keyword for waiting for certain conditions before continuing .🤖 Mods 1.7 – Charm's pipeline-able AI CLI client. Now with improved GitHub Copilot support and OpenAI o1 support.☎︎ phonenumbers 1.5 – Go port of Google's libphonenumber library for parsing and validating phone numbers.GoReleaser 2.6 – Build/release binaries for multiple platforms. v2.6 gains Bun and Deno support.Notify 1.3 – Library to send notifications to various services.sqlc 1.28 – Generate type-safe Go code from SQL.]]></content:encoded></item><item><title>We surpassed our year-end goal of $400,000 USD thanks to you!</title><link>http://www.fsf.org/blogs/community/we-surpassed-our-year-end-goal-of-400-000-usd-thanks-to-you</link><author></author><category>linux</category><pubDate>Tue, 21 Jan 2025 22:01:05 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DSC Weekly 21 January 2025</title><link>https://www.datasciencecentral.com/dsc-weekly-21-january-2025/</link><author>Scott Thompson</author><category>dev</category><category>ai</category><pubDate>Tue, 21 Jan 2025 19:38:09 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Announcements Top Stories In-Depth]]></content:encoded></item><item><title>This free Chinese AI just crushed OpenAI&apos;s $200 o1 model...</title><link>https://www.youtube.com/watch?v=-2k1rcRzsLA</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/-2k1rcRzsLA?version=3" length="" type=""/><pubDate>Tue, 21 Jan 2025 19:18:57 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Try Brilliant free for 30 days https://brilliant.org/fireship You’ll also get 20% off an annual premium subscription.

Let's take a look at the new DeepSeek R1 chain-of-thought AI model. Not only does it beat OpenAI o1 on key benchmarks, but it's also free and open source. 

#programming #ai #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

DeepSeek R1 GitHub https://github.com/deepseek-ai/DeepSeek-R1
OpenAI o1 $200 plan https://youtu.be/Sf4WqHBCYSY

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- What is Deepseek R1
- How does chain-of-thought AI work?
- Deepseek R1 vs OpenAI O1]]></content:encoded></item><item><title>Cutting-edge Chinese “reasoning” model rivals OpenAI o1—and it’s free to download</title><link>https://arstechnica.com/ai/2025/01/china-is-catching-up-with-americas-best-reasoning-ai-models/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/01/chinese_ai_header-1152x648.jpg" length="" type=""/><pubDate>Tue, 21 Jan 2025 18:28:54 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[On Monday, Chinese AI lab DeepSeek released its new R1 model family under an open MIT license, with its largest version containing 671 billion parameters. The company claims the model performs at levels comparable to OpenAI's o1 simulated reasoning (SR) model on several math and coding benchmarks.Alongside the release of the main DeepSeek-R1-Zero and DeepSeek-R1 models, DeepSeek published six smaller "DeepSeek-R1-Distill" versions ranging from 1.5 billion to 70 billion parameters. These distilled models are based on existing open source architectures like Qwen and Llama, trained using data generated from the full R1 model. The smallest version can run on a laptop, while the full model requires far more substantial computing resources.The releases immediately caught the attention of the AI community because most existing open-weights models—which can often be run and fine-tuned on local hardware—have lagged behind proprietary models like OpenAI's o1 in so-called reasoning benchmarks. Having these capabilities available in an MIT-licensed model that anyone can study, modify, or use commercially potentially marks a shift in what's possible with publicly available AI models.]]></content:encoded></item><item><title>Strobelight: A profiling service built on open source technology</title><link>https://engineering.fb.com/2025/01/21/production-engineering/strobelight-a-profiling-service-built-on-open-source-technology/</link><author></author><category>dev</category><category>official</category><pubDate>Tue, 21 Jan 2025 17:00:54 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[add_continuous_override_for_offcpu_data(
    "my_awesome_team", // the team that owns this service
    Type.SERVICE_ID,
    "my_awesome_service",
    30_000, // desired samples per hour
)
]]></content:encoded></item><item><title>Big AI: Cloud infrastructure dependence and the industrialization of AI</title><link>https://www.datasciencecentral.com/big-ai-cloud-infrastructure-dependence-and-the-industrialization-of-ai/</link><author>John Lee</author><category>dev</category><category>ai</category><pubDate>Tue, 21 Jan 2025 16:21:00 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[The ‘cloud AI wars’ at major technology companies are intensifying (Goldman, 2022). Industry leaders say AI is transforming society. AI’s foundational and transformative nature is emphasised by Bill Gates (2023) and Alphabet’s Sundar Pichai (2023). Amazon, Microsoft, and Google are allegedly driving this transformation. Cloud computing has created the socio-technical infrastructure for expansive growth and… Read More »]]></content:encoded></item><item><title>OpenAI’s New ChatGPT Learned from 100,000 Conversations!</title><link>https://www.youtube.com/watch?v=9Q0qPTKVTJE</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/9Q0qPTKVTJE?version=3" length="" type=""/><pubDate>Tue, 21 Jan 2025 16:15:50 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

📝 The paper is available here:
https://openai.com/index/openai-o1-system-card/

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Alex Balfanz, Alex Haro, B Shang, Benji Rabhan, Gaston Ingaramo, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Martin, Michael Albrecht, Michael Tedder, Owen Skarpness, Richard Sundvall, Taras Bobrovytsky,, Thomas Krcmar, Tybie Fitzhugh, Ueli Gallizzi.
If you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu]]></content:encoded></item><item><title>EFF Sends Transition Memo on Digital Policy Priorities to New Administration and Congress</title><link>https://www.eff.org/press/releases/eff-sends-transition-memo-digital-policy-priorities-new-administration-and-congress</link><author>Josh Richman</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/white-house-1.jpg" length="" type=""/><pubDate>Tue, 21 Jan 2025 15:30:49 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Rise of Vietnam</title><link>https://www.youtube.com/watch?v=PzsCQ152FRg</link><author>ColdFusion</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/PzsCQ152FRg?version=3" length="" type=""/><pubDate>Tue, 21 Jan 2025 15:19:09 +0000</pubDate><source url="https://www.youtube.com/channel/UC4QZ_LsYcvcq7qOsOhpAX4A">Cold Fusion</source><content:encoded><![CDATA[Compare news coverage. Spot media bias. Avoid algorithms. Try Ground News today and get 40% off your subscription by going to https://ground.news/coldfusion

The economy of Vietnam has been an absolute rocket ship over the past few years. By 2035, CNBC believes that globally they'll have the most rapid rise in wealth. Long gone are the days of turmoil and war. In this episode, we'll take a look at Vietnam's fascinating economic rise.

Show notes: https://docs.google.com/document/d/1mIGWaStrz0dZ1mUcchD_jfAwXXvC3TEpXlTxtQcB7R0/edit?usp=sharing

ColdFusion Podcast: 

https://www.youtube.com/@ThroughTheWeb 

ColdFusion Music: 

https://www.youtube.com/@ColdFusionmusic
http://burnwater.bandcamp.com   

Get my book: 

http://bit.ly/NewThinkingbook 

ColdFusion Socials: 

https://discord.gg/coldfusion
https://facebook.com/ColdFusionTV 
https://twitter.com/ColdFusion_TV 
https://instagram.com/coldfusiontv

Created by: Dagogo Altraide
Producers: Tawsif Akkas, Dagogo Altraide]]></content:encoded></item><item><title>The role of data science in verifying pay stub accuracy</title><link>https://www.datasciencecentral.com/the-role-of-data-science-in-verifying-pay-stub-accuracy/</link><author>Rob Turner</author><category>dev</category><category>ai</category><pubDate>Tue, 21 Jan 2025 15:18:13 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Image source: https://www.pexels.com/photo/man-in-black-jacket-using-black-laptop-computer-6693637/  Ensuring pay stubs are accurate is crucial – for both companies and employees. After all, errors in payroll can lead to financial losses, compliance issues, and even strained employee/employer relationships.  This is where data science steps in. It is transforming the way businesses verify pay stub details. From spotting accidental errors to… Read More »]]></content:encoded></item><item><title>FSD meeting recap 2025 01 17</title><link>http://www.fsf.org/blogs/licensing/fsd-meeting-recap-2025-01-17</link><author></author><category>linux</category><pubDate>Tue, 21 Jan 2025 14:37:47 +0000</pubDate><source url="http://www.fsf.org/blogs/recent-blog-posts">Dev - Free Software Foundation</source><content:encoded><![CDATA[Check out the important work our volunteers accomplished at last Friday's Free Software Directory (FSD) IRC meeting.]]></content:encoded></item><item><title>How to ensure data consistency in machine learning</title><link>https://www.datasciencecentral.com/how-to-ensure-data-consistency-in-machine-learning/</link><author>Zachary Amos</author><category>dev</category><category>ai</category><pubDate>Tue, 21 Jan 2025 14:01:00 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Machine learning enables systems to analyze data and make decisions without manual intervention. However, reliability hinges on the quality of the information they use. Data consistency ensures it remains accurate, uniform and reliable throughout the pipeline. Without coherence, models can produce flawed predictions and ultimately fail to deliver actionable insights. For businesses and researchers, prioritizing… Read More »]]></content:encoded></item><item><title>Exploring Python&apos;s tuple Data Type With Examples</title><link>https://realpython.com/courses/exploring-tuple-data-type-examples/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Tue, 21 Jan 2025 14:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[In Python, a  is a built-in data type that allows you to create  of values. The values or items in a tuple can be of any type. This makes tuples pretty useful in those situations where you need to store heterogeneous data, like that in a database record, for example.Through this tutorial, you’ll dive deep into Python tuples and get a solid understanding of their key features and use cases. This knowledge will allow you to write more efficient and reliable code by taking advantage of tuples.In this video course, you’ll learn how to: the items in an existing tuple, , , and  tuples, , and  existing tuplesExplore other  and common  of tuples]]></content:encoded></item><item><title>Quiz: How to Deal With Missing Data in Polars</title><link>https://realpython.com/quizzes/how-to-deal-with-missing-data-in-polars/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Tue, 21 Jan 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[By working through the questions, you’ll review your understanding of dealing with  values in Polars and also expand on what you learned in the tutorial.You’ll need to do some research outside of the tutorial to answer all the questions. Embrace this challenge and let it take you on a learning journey.]]></content:encoded></item><item><title>beeps and on-call for Next.js developers with Joey Parsons</title><link>https://softwareengineeringdaily.com/2025/01/21/beeps-and-on-call-next-js-developers-with-joey-parsons/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=beeps-and-on-call-next-js-developers-with-joey-parsons</link><author>SEDaily</author><category>podcast</category><enclosure url="https://traffic.megaphone.fm/SED8940308376.mp3" length="" type=""/><pubDate>Tue, 21 Jan 2025 10:00:07 +0000</pubDate><source url="http://softwareengineeringdaily.com/category/all-episodes/exclusive-content/podcast/">Podcast - Software Engineering Daily</source><content:encoded><![CDATA[beeps is a startup focused on building an on-call platform for Next.js. The company is grounded in the key insight that Next.js has become a dominant framework for modern development. A key motivation in leveraging Next.js is to create a developer-first experience for on-call.Joey Parsons is the founder and CEO of beeps, and he previously founded effx which was acquired by Figma in 2021. Joey joins the show to talk about the platform, starting a company without an explicit AI focus, the limitations of current on-call systems, building on Next.js, and more.Full Disclosure: This episode is sponsored by .]]></content:encoded></item><item><title>#417 Bugs hide from the light</title><link>https://pythonbytes.fm/episodes/show/417/bugs-hide-from-the-light</link><author></author><category>Python bytes</category><category>dev</category><category>python</category><category>podcast</category><enclosure url="https://pythonbytes.fm/episodes/download/417/bugs-hide-from-the-light.mp3" length="" type=""/><pubDate>Tue, 21 Jan 2025 08:00:00 +0000</pubDate><source url="https://pythonbytes.fm/">Python Bytes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What&apos;s an API?</title><link>https://blog.algomaster.io/p/whats-an-api</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><category>learning</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/ae14a376-92a3-4ff6-a329-8e4f2a7ac9b5_1546x1074.png" length="" type=""/><pubDate>Tue, 21 Jan 2025 05:31:04 +0000</pubDate><source url="https://blog.algomaster.io/">Algomaster</source><content:encoded><![CDATA[API stands for Application Programming Interface.At its core, an API is a  that takes an  and gives you predictable Think of an API as a  that enables applications to interact without needing direct access to each other's code or database.Almost every digital service you use today—social media, e-commerce, online banking, ride-hailing apps—all of them are a bunch of APIs working together. – If you provide a city name as input (), the API returns the current temperature, humidity, and weather conditions. – If you provide a pickup and destination address, the API finds the  and calculates the estimated fare. – If you provide a list of numbers (), the API returns the  ().When engineers build APIs, they clearly define what inputs the API accepts and , ensuring consistent behavior across different applications.APIs follow a simple  model:A client (such as a web app or mobile app) makes a request to an API.The API (hosted on an API server) processes the request, interacts with the necessary databases or services, and prepares a response.The API sends the response back to the client in a structured format (usually JSON or XML).Every API requires , and passing incorrect data can result in errors.For example: If you tried putting your name into the Google Maps API as an input, that wouldn’t work very well.Some APIs also require inputs in a specific format.Example: The  might need the input as  instead of .APIs often  to ensure they are correct before processing them, which helps maintain .Just as APIs require , they also return .For example, the  always returns coordinates in the same format.{   "latitude": 40.6892,   "longitude": -74.0445 }If the API can’t find the location, it provides an error response explaining why.{   "error": "Invalid address format",   "code": 400 }If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .The apps you use every day—whether it's Gmail, Instagram, Uber, or Spotify—are essentially a collection of APIs with a polished user interface (UI) on top.Most applications follow the frontend/backend architecture, where:The  consists of APIs that handle data processing, business logic, and communication with databases.The  is a graphical user interface (GUI) that interacts with these APIs, making applications user-friendly and accessible without requiring users to write code.Let’s break this down with a real-world example: .Before the Uber app existed as a sleek, user-friendly experience, the company first built the core APIs that power ride-hailing services:Calculating Fares & RoutesMatching Riders & DriversThese APIs run on Uber’s servers, forming the . Every time you request a ride, track your driver, or make a payment, these backend APIs handle the request. are responsible for optimizing these APIs, improving ride-matching algorithms, securing transactions, and ensuring a smooth experience for millions of users.The backend APIs handle , but they —which isn't practical for everyday users. That’s why companies build a frontend (user interface) on top of these APIs, allowing users to interact with the system .When you enter your pickup & destination address, the frontend sends an API request to  and displays available cars.Once the trip is complete, the frontend may call the process payment API to display the receipt.APIs come in different forms depending on , , and .1. Open APIs (Public APIs)Open APIs, also known as , are accessible to external developers with minimal restrictions.Companies provide these APIs to encourage  to integrate their services and build new applications on top of them.Example: YouTube Data APINormally, when you use the , it makes  to fetch your video feed, search for content, or post comments. However, YouTube also provides a  that allows developers to access some of this functionality .For example, the  allows developers to fetch video results based on a keyword. If you send a request to the API with "machine learning tutorial" as the search term, it will return a structured response (JSON format) containing a list of relevant videos, including titles, descriptions, thumbnails, and video links.This is incredibly useful because it enables developers to build custom applications on top of YouTube.2. Internal APIs (Private APIs), also known as , are designed xclusively for internal use within an organization. Unlike Open APIs, these are not accessible to external developers and are used to facilitate seamless communication between different internal systems within a company.Let’s take  as an example. When you place an order, you might assume that a single system processes your request. In reality,  (order processing, inventory, payment, logistics etc..) work together behind the scenes to fulfill your order efficiently.Each of these APIs , but they communicate through well-defined protocols to ensure a smooth and efficient process.Internal APIs allow companies to break down their applications into smaller, manageable services, making it easier to scale. Developers can  across different projects, reducing  and speeding up development.The first two types of APIs we discussed—Open APIs and Internal APIs—are functional and serve  like fetching weather data or booking a ride.But there’s another category of APIs that developers use daily:  (also called  or ).These APIs don’t connect different applications; instead, they provide predefined functions within a programming language or framework to make development easier.Python’s built-in list APIWhen working with lists, Python provides a set of built-in functions (methods) to manipulate data.numbers = [5, 3, 8, 1, 4] numbers.sort()  # API call to sort the list  fruits = ["apple", "banana"] fruits.append("orange")  # API call to add an element  fruits.pop()  # API call to remove the last elementInstead of writing sorting algorithms from scratch, developers can use  or  in Python.Code APIs are not just limited to built-in programming language functions. Take , an AI/ML library. It provides a  for training machine learning models without needing to implement complex mathematical operations from scratch.For example, creating a  using TensorFlow's API is as simple as:import tensorflow as tf model = tf.keras.Sequential([tf.keras.layers.Dense(64, activation="relu")])Programming APIs abstract away complexity so that developers can focus on building solutions rather than reinventing the wheel.APIs communicate using different protocols and architectures that define how requests are sent, how responses are formatted, and how data is exchanged between systems.1. REST (Representational State Transfer)REST is the most widely used API communication method today. It is lightweight, stateless, and scalable, making it perfect for web services and mobile applications. REST APIs follow a set of design principles and use  (GET, POST, PUT, DELETE) to perform operations.REST APIs are based on , and each resource is accessed through a . The API follows the , meaning the client sends a request, and the server processes it and sends a response.Example: REST API for a BookstoreRetrieve a list of books (GET Request):GET https://api.bookstore.com/books[   { "id": 1, "title": "Clean Code", "author": "Robert C. Martin" },   { "id": 2, "title": "The Pragmatic Programmer", "author": "Andrew Hunt" } ]2. SOAP (Simple Object Access Protocol)SOAP is an older API communication method that relies on XML-based messaging. Unlike REST, which is lightweight, SOAP is more structured and secure, making it ideal for banking, healthcare, and enterprise applications.SOAP messages are sent using  and require a WSDL (Web Services Description Language) file, which defines the API's available functions and request structure.Example: SOAP API for a Banking Service Fetching account balance<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:bank="http://bank.example.com/">    <soapenv:Header/>    <soapenv:Body>       <bank:GetAccountBalance>          <bank:accountNumber>123456</bank:accountNumber>       </bank:GetAccountBalance>    </soapenv:Body> </soapenv:Envelope> <soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/">    <soapenv:Body>       <bank:GetAccountBalanceResponse>          <bank:balance>5000.00</bank:balance>       </bank:GetAccountBalanceResponse>    </soapenv:Body> </soapenv:Envelope>GraphQL is an alternative to REST that allows clients to request exactly the data they need, making it more efficient for modern applications. Unlike REST, which requires multiple API calls to fetch related data, GraphQL can fetch all necessary data in a single request.Instead of predefined endpoints, GraphQL exposes a , and the client sends queries to request specific fields.Example: Fetching a user's profile and their recent posts in a single request.{   user(id: 123) {     name     email     posts {       title       likes     }   } }{   "data": {     "user": {       "name": "Alice",       "email": "alice@example.com",       "posts": [         { "title": "Hello World", "likes": 100 },         { "title": "GraphQL is Amazing!", "likes": 200 }       ]     }   } }gRPC (Google Remote Procedure Call) is a high-performance API communication method that uses Protocol Buffers (Protobuf) instead of JSON or XML, making it faster and more efficient.gRPC uses  instead of text-based formats, reducing payload size and it supports , meaning the client and server can send data at the same time.Using an API might seem complex at first, but it follows a simple  pattern.Here’s a guide on how to find, access, and interact with an API step by step:Step 1: Find an API to UseBefore using an API, you need to  for your needs. APIs are available for different services like weather data, finance, social media, etc.Official API Documentation:Step 2: Read the API DocumentationAPI documentation explains how to use the API, available endpoints, authentication, and response formats.The OpenWeatherMap API allows users to fetch real-time weather data. Here's a breakdown of its key components:https://api.openweathermap.org/data/3.0/weather?q=city_name&appid=YOUR_API_KEY: City name (e.g., ): API Key (required for access)Step 3: Get API Access (API Key / Authentication)Most APIs  to prevent unauthorized access and manage usage limits.Common Authentication Methods:A unique key provided by the API serviceSecure login via Google, Github, etc.Token-based authenticationUsername + password (Base64 encoded)Example: Getting an API Key (OpenWeather API)Sign up at https://home.openweathermap.org/users/sign_up.Go to the  section and generate a key.Use the API key in requests:GET https://api.openweathermap.org/data/2.5/weather?q=London&appid=YOUR_API_KEYStep 4: Test the API Using Postman or cURLBefore writing code,  to see how it responds.Option 1: Using Postman (Recommended for Beginners)Click , enter the API endpoint URL (https://api.openweathermap.org/data/3.0/weather?q=London&appid=YOUR_API_KEY).Select  as the HTTP method.Click  and view the response in .Option 2: Using cURL (For Command Line Users)You can also test APIs directly from the  using .curl -X GET "https://api.openweathermap.org/data/3.0/weather?q=New+York&appid=YOUR_API_KEY"Step 5: Write Code to Call the APINow that you’ve tested the API, it’s time to integrate it into your application.Example: Calling an API in Pythonimport requests  url = "https://api.openweathermap.org/data/3.0/weather?q=New York&appid=YOUR_API_KEY" response = requests.get(url)  if response.status_code == 200:     data = response.json()     print(f"Temperature: {data['main']['temp']}°C") else:     print("Error:", response.status_code) – Sends an API request. – Converts response to JSON.if response.status_code == 200 – Checks if the request was successful.Step 6: Handle Errors & Rate LimitsAPIs don’t always return perfect responses. You should handle: (e.g., wrong city name). (e.g., expired API keys). (e.g., exceeding request limits).Example: Handling API Errors in Pythonif response.status_code == 200:     data = response.json()     print(f"Weather: {data['weather'][0]['description']}") elif response.status_code == 401:     print("Error: Invalid API key") elif response.status_code == 404:     print("Error: City not found") else:     print(f"Unexpected error: {response.status_code}")Step 7: Use API Responses in Your ApplicationOnce you fetch data from an API, you can display it dynamically in a web or mobile app. You can build a weather dashboard using the OpenWeatherMap API.Fetch live weather data from the API.Parse and extract relevant details (temperature, humidity, condition).Display the weather report in a user-friendly format.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re finding this newsletter helpful and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>DtSR Episode 637 - Amanda Berlin Build SMB Tools That Don’t Suck</title><link></link><author></author><category>Down the Security Rabbit Hole Podcast</category><category>infosec</category><category>podcast</category><enclosure url="https://dts.podtrac.com/redirect.mp3/www.buzzsprout.com/2153215/episodes/16471164-dtsr-episode-637-amanda-berlin-build-smb-tools-that-don-t-suck.mp3" length="" type=""/><pubDate>Tue, 21 Jan 2025 05:00:00 +0000</pubDate><source url="https://blogwh1t3rabbit.medium.com/">Down the Security Rabbithole Podcast (DtSR)</source></item><item><title>ECMAScript proposal: RegExp escaping</title><link>https://2ality.com/2025/01/regexp-escape.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[The ECMAScript proposal “RegExp escaping” (by Jordan Harband and Kevin Gibbons) specifies a function  that, given a string , creates an escaped version that matches  – if interpreted as a regular expression.This proposal is currently at stage 3.]]></content:encoded></item><item><title>Spotlight on SIG Architecture: Enhancements</title><link>https://kubernetes.io/blog/2025/01/21/sig-architecture-enhancements/</link><author></author><category>official</category><category>k8s</category><category>devops</category><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[This is the fourth interview of a SIG Architecture Spotlight series that will cover the different
subprojects, and we will be covering SIG Architecture:
Enhancements.In this SIG Architecture spotlight we talked with Kirsten
Garrison, lead of the Enhancements subproject.The Enhancements subprojectFrederico (FSM): Hi Kirsten, very happy to have the opportunity to talk about the Enhancements
subproject. Let's start with some quick information about yourself and your role.: I’m a lead of the Enhancements subproject of SIG-Architecture and
currently work at Google. I first got involved by contributing to the service-catalog project with
the help of Carolyn Van Slyck. With time, I joined the Release
team,
eventually becoming the Enhancements Lead and a Release Lead shadow. While on the release team, I
worked on some ideas to make the process better for the SIGs and Enhancements team (the opt-in
process) based on my team’s experiences. Eventually, I started attending Subproject meetings and
contributing to the Subproject’s work.FSM: You mentioned the Enhancements subproject: how would you describe its main goals and areas of
intervention?FSM: The improvement of the KEP process was (and is) one in which SIG Architecture was heavily
involved. Could you explain the process to those that aren’t aware of it?: Every release, the SIGs let the
Release Team know which features they intend to work on to be put into the release. As mentioned
above, the prerequisite for these changes is a KEP - a standardized design document that all authors
must fill out and approve in the first weeks of the release cycle. Most features will move
through 3
phases:
alpha, beta and finally GA so approving a feature represents a significant commitment for the SIG.The KEP serves as the full source of truth of a feature. The KEP
template
has different requirements based on what stage a feature is in, but it generally requires a detailed
discussion of the design and the impact as well as providing artifacts of stability and
performance. The KEP takes quite a bit of iterative work between authors, SIG reviewers, api review
team and the Production Readiness Review team before it is approved. Each set of reviewers is
looking to make sure that the proposal meets their standards in order to have a stable and
performant Kubernetes release. Only after all approvals are secured, can an author go forth and
merge their feature in the Kubernetes code base.FSM: I see, quite a bit of additional structure was added. Looking back, what were the most
significant improvements of that approach?: In general, I think that the improvements with the most impact had to do with focusing on
the core intent of the KEP. KEPs exist not just to memorialize designs, but provide a structured way
to discuss and come to an agreement about different facets of the change. At the core of the KEP
process is communication and consideration.To that end, some of the significant changes revolve around a more detailed and accessible KEP
template. A significant amount of work was put in over time to get the
k/enhancements repo into its current form -- a
directory structure organized by SIG with the contours of the modern KEP template (with
Proposal/Motivation/Design Details subsections). We might take that basic structure for granted
today, but it really represents the work of many people trying to get the foundation of this process
in place over time.As Kubernetes matures, we’ve needed to think about more than just the end goal of getting a single
feature merged. We need to think about things like: stability, performance, setting and meeting user
expectations. And as we’ve thought about those things the template has grown more detailed. The
addition of the Production Readiness Review was major as well as the enhanced testing requirements
(varying at different stages of a KEP’s lifecycle).: We’re currently working on two things:Creating a Process KEP template. Sometimes people want to harness the KEP process for
significant changes that are more process oriented rather than feature oriented. We want to
support this because memorializing changes is important and giving people a better tool to do so
will only encourage more discussion and transparency. While our template changes aim to be as non-disruptive as possible, we
believe that it will be easier to track and communicate those changes to the community better with
a versioned KEP template and the policies that go alongside such versioning.Both features will take some time to get right and fully roll out (just like a KEP feature) but we
believe that they will both provide improvements that will benefit the community at large.FSM: You mentioned improvements: I remember when project boards for Enhancement tracking were
introduced in recent releases, to great effect and unanimous applause from release team members. Was
this a particular area of focus for the subproject?: The Subproject provided support to the Release Team’s Enhancement team in the migration away
from using the spreadsheet to a project board. The collection and tracking of enhancements has
always been a logistical challenge. During my time on the Release Team, I helped with the transition
to an opt-in system of enhancements, whereby the SIG leads "opt-in" KEPs for release tracking. This
helped to enhance communication between authors and SIGs before any significant work was undertaken
on a KEP and removed toil from the Enhancements team. This change used the existing tools to avoid
introducing too many changes at once to the community. Later, the Release Team approached the
Subproject with an idea of leveraging GitHub Project Boards to further improve the collection
process. This was to be a move away from the use of complicated spreadsheets to using repo-native
labels on k/enhancement issues and project boards.FSM: That surely adds an impact on simplifying the workflow...: Removing sources of friction and promoting clear communication is very important to the
Enhancements Subproject. At the same time, it’s important to give careful consideration to
decisions that impact the community as a whole. We want to make sure that changes are balanced to
give an upside and while not causing any regressions and pain in the rollout. We supported the
Release Team in ideation as well as through the actual migration to the project boards. It was a
great success and exciting to see the team make high impact changes that helped everyone involved in
the KEP process!FSM: For those reading that might be curious and interested in helping, how would you describe the
required skills for participating in the sub-project?: Familiarity with KEPs either via experience or taking time to look through the
kubernetes/enhancements repo is helpful. All are welcome to participate if interested - we can take
it from there.FSM: Excellent! Many thanks for your time and insight -- any final comments you would like to
share with our readers?: The Enhancements process is one of the most important parts of Kubernetes and requires
enormous amounts of coordination and collaboration of people and teams across the project to make it
successful. I’m thankful and inspired by everyone’s continued hard work and dedication to making the
project great. This is truly a wonderful community.]]></content:encoded></item><item><title>Make computing personal again (News)</title><link>https://changelog.com/news/128</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/news/128/changelog-news-128.mp3" length="" type=""/><pubDate>Mon, 20 Jan 2025 20:00:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Benj Edwards wants to put the “personal” back in “personal computer”, the answer.ai folks took Devin for a month-long spin, Asaf Zamir explains why senior engineers can remain ICs and still have a fulfilling career, Fabrizio Ferri Benedetti rethinks documentation by putting user actions first & Tero Piirainen lays out his case for Nue, the standards first web framework.Changelog++ members support our work, get closer to the metal, and make the ads disappear. Join today!Augment Code – Developer AI that uses deep understanding of your large codebase and how you build software to deliver personalized code suggestions and insights. Augment provides relevant, contextualized code right in your IDE or Slack. It transforms scattered knowledge into code or answers, eliminating time spent searching docs or interrupting teammates.
]]></content:encoded></item><item><title>VPNs Are Not a Solution to Age Verification Laws</title><link>https://www.eff.org/deeplinks/2025/01/vpns-are-not-solution-age-verification-laws</link><author>Paige Collings</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/free-speech-cat3.jpg" length="" type=""/><pubDate>Mon, 20 Jan 2025 14:28:38 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Linux 6.13 Released</title><link>https://linux.slashdot.org/story/25/01/20/0555227/linux-613-released?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>slashdot</category><category>linux</category><pubDate>Mon, 20 Jan 2025 08:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA["Nothing horrible or unexpected happened last week," Linux Torvalds posted tonight on the Linux kernel mailing list, "so I've tagged and pushed out the final 6.13 release." 

Phoronix says the release has "plenty of fine features":
Linux 6.13 comes with the introduction of the AMD 3D V-Cache Optimizer driver for benefiting multi-CCD Ryzen X3D processors. The new AMD EPYC 9005 "Turin" server processors will now default to AMD P-State rather than ACPI CPUFreq for better power efficiency.... 
Linux 6.13 also brings more Rust programming language infrastructure and more.
 
Phoronix notes that Linux 6.13 also brings "the start of Intel Xe3 graphics bring-up, support for many older (pre-M1) Apple devices like numerous iPads and iPhones, NVMe 2.1 specification support, and AutoFDO and Propeller optimization support when compiling the Linux kernel with the LLVM Clang compiler." 

And some lucky Linux kernel developers will also be getting a guitar pedal soldered by Linus Torvalds himself, thanks to a generous offer he announced a week ago:

For _me_ a traditional holiday activity tends to be a LEGO build or two, since that's often part of the presents... But in addition to the LEGO builds, this year I also ended up doing a number of guitar pedal kit builds ("LEGO for grown-ups with a soldering iron"). Not because I play guitar, but because I enjoy the tinkering, and the guitar pedals actually do something and are the right kind of "not very complex, but not some 5-minute 555 LED blinking thing"... 
[S]ince I don't actually have any _use_ for the resulting pedals (I've already foisted off a few only unsuspecting victims^Hfriends), I decided that I'm going to see if some hapless kernel developer would want one.... as an admittedly pretty weak excuse to keep buying and building kits... 

"It may be worth noting that while I've had good success so far, I'm a software person with a soldering iron. You have been warned... [Y]ou should set your expectations along the lines of 'quality kit built by a SW person who doesn't know one end of a guitar from the other.'"]]></content:encoded></item><item><title>FastAPI vs Go (Golang) vs Node.js: Performance &amp; Price</title><link>https://www.youtube.com/watch?v=shAELuHaTio</link><author>Anton Putra</author><category>dev</category><category>video</category><enclosure url="https://www.youtube.com/v/shAELuHaTio?version=3" length="" type=""/><pubDate>Sun, 19 Jan 2025 19:03:01 +0000</pubDate><source url="https://www.youtube.com/channel/UCeLvlbC754U6FyFQbKc0UnQ">Anton Putra</source><content:encoded><![CDATA[Python vs Go (Golang) vs NodeJS Performance Comparison

🔴 To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)

🍿 Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&si=p-UOaVM_6_SFx52H

👋 AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra

▬▬▬▬▬ Experience & Location 💼 ▬▬▬▬▬
►  I’m a Senior Software Engineer at Juniper Networks (13+ years of experience)
►  Located in San Francisco Bay Area, CA (US citizen)

▬▬▬▬▬▬ Connect with me 👋 ▬▬▬▬▬▬
►  LinkedIn: https://www.linkedin.com/in/anton-putra
►  Twitter/X: https://twitter.com/antonvputra
►  Instagram: https://www.instagram.com/aputrabay
►  GitHub: https://github.com/antonputra
►  Email: me@antonputra.com

▬▬▬▬▬▬▬ Timestamps ⏰  ▬▬▬▬▬▬▬
0:00 Intro
1:42 Why Measure Latency from the Client Side?
3:38 1st Test
8:04 2nd Test

▬▬▬▬▬▬▬ Source Code 📚 ▬▬▬▬▬▬▬
► Original Source Code: https://github.com/antonputra/tutorials/tree/239/lessons/239

#python #golang #nodejs]]></content:encoded></item><item><title>NVIDIA’s New AI: Huge Game Changer!</title><link>https://www.youtube.com/watch?v=4TpwfbUjCYs</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/4TpwfbUjCYs?version=3" length="" type=""/><pubDate>Sun, 19 Jan 2025 17:36:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

📝 The paper "CLoSD - Closing the Loop between Simulation and
Diffusion for multi-task character control" is available here:
https://guytevet.github.io/CLoSD-page/

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Alex Balfanz, Alex Haro, B Shang, Benji Rabhan, Gaston Ingaramo, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Martin, Michael Albrecht, Michael Tedder, Owen Skarpness, Richard Sundvall, Taras Bobrovytsky,, Thomas Krcmar, Tybie Fitzhugh, Ueli Gallizzi.
If you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu

#nvidia]]></content:encoded></item><item><title>Encryption at Rest with SQLAlchemy</title><link>https://blog.miguelgrinberg.com/post/encryption-at-rest-with-sqlalchemy</link><author>Miguel Grinberg</author><category>Miguel Grinberg</category><category>dev</category><category>python</category><pubDate>Sun, 19 Jan 2025 15:48:45 +0000</pubDate><source url="https://blog.miguelgrinberg.com/feed">https://blog.miguelgrinberg.com/feed</source><content:encoded><![CDATA[In this tutorial I'm going to show you how to extend SQLAlchemy so that you can define database columns that are stored encrypted. In the solution I'm going to share, the columns that are designated as encrypted will transparently encrypt and decrypt themselves as data moves to and from the database.For this tutorial I'm going to use SQLAlchemy version 2, the latest and greatest. All the techniques I will present can be adapted to SQLAlchemy 1.x if you need to work with legacy versions. If you are interested in updating your knowledge of SQLAlchemy, I have a SQLAlchemy 2 book that can help.]]></content:encoded></item><item><title>TypeScript enums: use cases and alternatives</title><link>https://2ality.com/2025/01/typescript-enum-patterns.html</link><author>Dr. Axel Rauschmayer</author><category>dev</category><category>frontend</category><category>blog</category><pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate><source url="https://feeds.feedburner.com/2ality">Axel Raushmayer</source><content:encoded><![CDATA[In this blog post, we take a closer look at TypeScript enums:What are their use cases?What are the alternatives if we don’t want to use them?The blog post concludes with recommendations for what to use when.]]></content:encoded></item><item><title>Automatic Server Reloading in Rust on Change: What is listenfd/systemfd?</title><link>http://lucumr.pocoo.org/2025/1/19/what-is-systemfd</link><author>Armin Ronacher</author><category>dev</category><category>blog</category><pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate><source url="https://lucumr.pocoo.org/feed.atom">Armin Ronacher Blog</source><content:encoded><![CDATA[When I developed Werkzeug (and
later Flask), the most
important part of the developer experience for me was enabling fast, automatic
reloading.  Werkzeug (and with it Flask), this is achieved by using two
procsses at all times.  The parent process holds on to the file descriptor
of the socket on which the server listens, and a subprocess picks up that
file descriptor.  That subprocess restarts when it detects changes.  This
ensures that no matter what happens, there is no window where the browser
reports a connection error.  At worst, the browser will hang until the
process finishes reloading, after which the page loads successfully.  In
case the inner process fails to come up during restarts, you get an error
message.A few years ago, I wanted to accomplish the same experience for working
with Rust code which is why I wrote systemfd and listenfd.  I however realized that I
never really wrote here about how they work and disappointingly I think
those crates, and a good auto-reloading experience in Rust are largely
unknown.Firstly one needs to monitor the file system for changes.  While in theory
I could have done this myself, there was already a tool that could do
that.At the time there was cargo watch.  Today one might instead use it
together with the more generic watchexec.  Either one monitor your
workspace for changes and then executes a command.  So you can for
instance tell it to restart your program.  One of these will work:watchexec -r -- cargo run
cargo watch -x run
You will need a tool like that to do the watching part.  At this point I
recommend the more generic  which you can find on homebrew and
elsewhere.But what about the socket?  The solution to this problem I picked comes
from systemd.  Systemd has a
“protocol” that standardizes passing file descriptors from one process to
another through environment variables.  In systemd parlance this is called
“socket activation,” as it allows systemd to only launch a program if
someone started making a request to the socket.  This concept was
originally introduced by Apple as part of launchd.To make this work with Rust, I created two crates:systemfd is the command
line tool that opens sockets and passes them on to other programs.listenfd is a Rust crate that
accepts file descriptors from systemd or .It's worth noting that systemfd is not exclusivly useful to Rust.  The
systemd protocol can be implemented in other languages as well, meaning
that if you have a socket server written in Go or Python, you can also use
systemfd.So here is how you use it.First you need to add  to your project:Then, modify your server code to accept sockets via listenfd before
falling back to listening itself on ports provided through command-line
arguments or configuration files.  Here is an example using  in
axum:::::::::-> -> ::::::::::::::::The key point here is to accept socket 0 from the environment as a TCP
listener and use it if available.  If the socket is not provided (e.g.
when launched without systemd/), the code falls back to opening a
fixed port.Finally you can use  /  together with :systemfd --no-pid -s http::8888 -- watchexec -r -- cargo run
systemfd --no-pid -s http::8888 -- cargo watch -x run
This is what the parameters mean: needs to be first it's the program that opens the sockets. is a flag prevents the PID from being passed.  This is necessary
for  to accept the socket.  This is a departure of the socket
passing protocol from systemd which otherwise does not allow ports to be
passed through another program (like ).  In short: when the
PID information is not passed, then listenfd will accept the socket
regardless.  Otherwise it would only accept it from the direct parent
process. tells  to open one TCP socket on port 8888.
Using  instead of  is a small improvement that will cause
systemfd to print out a URL on startup. makes  restart the process when something
changes in the current working directory. is the program that watchexec will start and re-start onm
changes.  In Rust this will first compile the changes and then run the
application.  Because we put  in, it will try to first accept
the socket from .The end result is that you can edit your code, and it will recompile
automatically and restart the server without dropping any requests.  When
you run it, and perform changes, it will look a bit like this:$ systemfd --no-pid -s http::5555 -- watchexec -r -- cargo run
~> socket http://127.0.0.1:5555/ -> fd #3
[Running: cargo run]
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.02s
     Running `target/debug/axum-test`
[Running: cargo run]
   Compiling axum-test v0.1.0 (/private/tmp/axum-test)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.52s
     Running `target/debug/axum-test`
For easier access, I recommend putting this into a  or similar
so you can just run  and it runs the server in watch mode.To install  you can use curl to bash:curl -sSfL https://github.com/mitsuhiko/systemfd/releases/latest/download/systemfd-installer.sh | sh
Now how does this work on Windows?  The answer is that  and
 have a custom, proprietary protocol that also makes socket
passing work on Windows.  That's a more complex system which involves a
local RPC server.  However the system does also support Windows and the
details about how it works are largely irrelevant for you as a user
— unless you want to implement that protocol for another programming
language.I really enjoy using this combination, but it can be quite frustrating to
require so many commands, and the command line workflow isn't optimal.
Ideally, this functionality would be better integrated into specific Rust
frameworks like axum and provided through a dedicated cargo plugin.  In a
perfect world, one could simply run , and everything
would work seamlessly.However, maintaining such an integrated experience is a much more involved
effort than what I have.  Hopefully, someone will be inspired to further
enhance the developer experience and achieve deeper integration with Rust
frameworks, making it more accessible and convenient for everyone.]]></content:encoded></item><item><title>Apple Intelligence rolled back after doing dumb stuff...</title><link>https://www.youtube.com/watch?v=7rXgVsIGvGQ</link><author>Fireship</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/7rXgVsIGvGQ?version=3" length="" type=""/><pubDate>Sat, 18 Jan 2025 15:22:50 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</source><content:encoded><![CDATA[Apple recently rolled back its news summary feature on iOS powered by Apple Intelligence because it was hallucinating misinformation. Let's examine recent failures and controversies in the tech world. 

#tech #ai #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Tech Trends in 2025 https://youtu.be/v4H2fTgHGuc
Apple Intelligence Announcement https://youtu.be/ek2yOqAIYuU

📚 Chapters

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Apple Intelligence failures
- Progress of Artificial Super Intelligence
- Elon feud with Asmongold 
- Zuckerberg lies on Joe Rogan
- Software engineering trends]]></content:encoded></item><item><title>More Python parallel stuff with joblib-modal</title><link>https://www.youtube.com/watch?v=0SXyIcsT4tg</link><author>probabl</author><category>dev</category><category>ai</category><category>video</category><category>learning</category><enclosure url="https://www.youtube.com/v/0SXyIcsT4tg?version=3" length="" type=""/><pubDate>Sat, 18 Jan 2025 00:33:09 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Probabl</source><content:encoded><![CDATA[One of the probabl colleagues has been working on an integration between joblib (which scikit-learn uses for parallel processing) and Modal (a cloud-provider known for it's likeable configuration for compute resources in Python). In this livestream we will explore a very early version and show you what's possible now and what is different. 

The notebook from this episode can be found here:
https://github.com/probabl-ai/youtube-appendix/blob/main/00-livestream/modal-jobs.ipynb

Website: https://probabl.ai/
Discord: https://discord.probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Twitter: https://x.com/probabl_ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

If you're keen to see more videos like this, you can follow us over at  @probabl_ai.]]></content:encoded></item><item><title>Patterns of use of Vello crate</title><link>https://poignardazur.github.io//2025/01/18/vello-analysis/</link><author>Olivier Faure</author><category>Olivier Faure blog</category><category>dev</category><category>rust</category><category>blog</category><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><source url="https://poignardazur.github.io//">PoignardAzur</source><content:encoded><![CDATA[This document tries to establish patterns among a list of crates and Github projects using the Vello renderer.The crate list isn’t meant to be exhaustive, but it’s pretty large: I’ve sifted through maybe 40 or so reverse dependencies of the Vello repository, up to the point where most of the READMEs I read were along the lines of “WIP: quick experiment paint shapes with Vello”.Don’t expect anything groundbreaking. My main focus is on common patterns among people using Vello’s  API; most of this is going to be pretty dry.I’ve mostly noticed two types of projects with Vello as a direct dependency: bridging Vello with some other format or framework, or providing wrapper functions for Vello’s API. Ex: , , etc. that renders specific things with Vello.(Though in practice, a few blurred the lines, and a lot of projects that I put in one category or the other mostly ended up using Vello for example code or for very basic painting.)The numbers (18 engine projects, 13 app-like projects) aren’t too surprising if you’re familiar with the “Rust has 50 game engines and 3 games” stereotype. They also match my “gut feeling” reading the code, where it feels like a lot more projects use Vello in a very systematized way, as some kind of middleware or optional backend than as a plug-and-play dependency to paint a bunch of shapes.For instance, I saw almost no project calling the  function more than five times.This is confounded by the fact that I only looked for direct dependents of Vello; I might have found more app projects looking for dependents of Masonry or bevy_vello. Maybe the painting-privitive-heavy code is in the dependents of one of those engine code projects I cited.Graphite, for example, is a primitive-heavy 2D editor, but its use of Vello is bottlenecked through a middleware layer with its own internal representation.In general, though, my impression is that the stereotype is mostly true.Another interesting pattern is that code actually using Vello to paint things was often in amateur stub projects, which is good for our purposes: it tells us how people who have little experience with Vello end up using it.Fill and stroke arguments and  are the most used methods by a very wide margin.As a reminder, their prototype is:A lot code calling them looks like this:Note the heavy usage of default values: is the default fill setting for most paint APIs. creates a stroke with rount joins and caps, no dashes, and the given width. is set to  for both methods. is set to  for both methods.These patterns can be found throughout the projects I’ve linked. In total, I’ve counted:16 projects using  with ,  and .10 projects using  with ,  and .7 projects using  with , a transform and .3 projects using  with , a transform and .Projects that used all the arguments of  or  were rare, and were generally written as middleware code passing these arguments from another source. For example:These projects tended to have one instance code calling each Scene method in the entire repository.Most of the projects I’ve looked at used the  and  API exclusively.
Few of them used , , , etc.In total, I’ve counted about a dozen projects using any of these APIs.Those that did tended to be the “render any arbitrary SVG” kinds of projects.Based on the above, I’d recommend having Vello export the following API: and  use the minimum number of arguments. and  use an additional  argument. and  use the full API.With this API, the  code I quoted would look like this:This would also let us remove most of the helpers in  in Masonry.]]></content:encoded></item><item><title>Other people&apos;s robots (Friends)</title><link>https://changelog.com/friends/76</link><author></author><category>podcast</category><enclosure url="https://op3.dev/e/https://cdn.changelog.com/uploads/friends/76/changelog--friends-76.mp3" length="" type=""/><pubDate>Fri, 17 Jan 2025 20:30:00 +0000</pubDate><source url="https://changelog.com/podcast">Podcast - Changelog</source><content:encoded><![CDATA[Jerod & Adam discuss Nvidia’s recently announced personal AI supercomputer, Waymo’s latest infinite loop, what’s involved in getting a “modern” terminal setup, and whether or not AI has gone mainstream… warts & all!Changelog++ members get a bonus 8 minutes at the end of this episode and zero ads. Join today!Augment Code – Developer AI that uses deep understanding of your large codebase and how you build software to deliver personalized code suggestions and insights. Augment provides relevant, contextualized code right in your IDE or Slack. It transforms scattered knowledge into code or answers, eliminating time spent searching docs or interrupting teammates.
Fly.io – The home of Changelog.com — Deploy your apps close to your users — global Anycast load-balancing, zero-configuration private networking, hardware isolation, and instant WireGuard VPN connections. Push-button deployments that scale to thousands of instances. Check out the speedrun to get started in minutes.
DeleteMe – Text  to 64000 to join DeleteMe. They make it quick, easy, and safe to remove your personal data online.
]]></content:encoded></item><item><title>Efficient Deployment of Models at the Edge // Krishna Sridhar // #284</title><link>https://podcasters.spotify.com/pod/show/mlops/episodes/Efficient-Deployment-of-Models-at-the-Edge--Krishna-Sridhar--284-e2tkifl</link><author>Demetrios</author><category>podcast</category><category>ai</category><enclosure url="https://anchor.fm/s/174cb1b8/podcast/play/97191861/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-17%2F393252053-44100-2-f52a7de7a6939.mp3" length="" type=""/><pubDate>Fri, 17 Jan 2025 18:36:19 +0000</pubDate><source url="https://mlops.community/">MLOps podcast</source><content:encoded><![CDATA[Krishna Sridhar is an experienced engineering leader passionate about building wonderful products powered by machine learning.

Efficient Deployment of Models at the Edge // MLOps Podcast #284 with Krishna Sridhar, Vice President of Qualcomm.Big shout out to Qualcomm for sponsoring this episode!

// Abstract
Qualcomm® AI Hub helps to optimize, validate, and deploy machine learning models on-device for vision, audio, and speech use cases.

With Qualcomm® AI Hub, you can:

Convert trained models from frameworks like PyTorch and ONNX for optimized on-device performance on Qualcomm® devices.
Profile models on-device to obtain detailed metrics including runtime, load time, and compute unit utilization.
Verify numerical correctness by performing on-device inference.
Easily deploy models using Qualcomm® AI Engine Direct, TensorFlow Lite, or ONNX Runtime.

The Qualcomm® AI Hub Models repository contains a collection of example models that use Qualcomm® AI Hub to optimize, validate, and deploy models on Qualcomm® devices.

Qualcomm® AI Hub automatically handles model translation from source framework to device runtime, applying hardware-aware optimizations, and performs physical performance/numerical validation. The system automatically provisions devices in the cloud for on-device profiling and inference. The following image shows the steps taken to analyze a model using Qualcomm® AI Hub.

// Bio
Krishna Sridhar leads engineering for Qualcomm™ AI Hub, a system used by more than 10,000 AI developers spanning 1,000 companies to run more than 100,000 models on Qualcomm platforms. 

Prior to joining Qualcomm, he was Co-founder and CEO of Tetra AI which made its easy to efficiently deploy ML models on mobile/edge hardware. 

Prior to Tetra AI, Krishna helped design Apple's CoreML which was a software system mission critical to running several experiences at Apple including Camera, Photos, Siri, FaceTime, Watch, and many more across all major Apple device operating systems and all hardware and IP blocks. 

He has a Ph.D. in computer science from the University of Wisconsin-Madison, and a bachelor’s degree in computer science from Birla Institute of Technology and Science, Pilani, India.

// MLOps Swag/Merch
https://shop.mlops.community/

// Related Links
Website: https://www.linkedin.com/in/srikris/
 
--------------- ✌️Connect With Us ✌️ -------------
Join our slack community: https://go.mlops.community/slack
Follow us on Twitter: @mlopscommunity
Sign up for the next meetup: https://go.mlops.community/register
Catch all episodes, blogs, newsletters, and more: https://mlops.community/

Connect with Demetrios on LinkedIn: https://www.linkedin.com/in/dpbrinkm/
Connect with Krishna on LinkedIn: https://www.linkedin.com/in/srikris/]]></content:encoded></item><item><title>Mad at Meta? Don&apos;t Let Them Collect and Monetize Your Personal Data</title><link>https://www.eff.org/deeplinks/2025/01/mad-meta-dont-let-them-collect-and-monetize-your-personal-data</link><author>Lena Cohen</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/facebook-zuck-2b_0.png" length="" type=""/><pubDate>Fri, 17 Jan 2025 15:59:30 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Once you’re on the “Accounts Center” page, make the following changes:1) Stop Meta from targeting ads based on data it collects about you on other apps and websites: Click the Activity information from ad partnersNo, don’t make my ads more relevant by using this information2) Stop Meta from using your data (from Facebook and Instagram) to help advertisers target you on other apps.Meta’s ad network connects advertisers with other apps through privacy-invasive ad auctions—generating more money and data for Meta in the process.Ads shown outside of Meta Depending on your location, this setting will be called on the tab.3) Disconnect the data that other companies share with Meta about you from your account:Your information and permissionsYour activity off Meta technologiesDisconnect future activityDisconnect future activityDisable your phone’s advertising IDTurn off location access for Meta’s apps]]></content:encoded></item><item><title>Digital Twins in manufacturing: Revolutionizing production and maintenance</title><link>https://www.datasciencecentral.com/digital-twins-in-manufacturing-revolutionizing-production-and-maintenance/</link><author>Shanthababu Pandian</author><category>dev</category><category>ai</category><pubDate>Fri, 17 Jan 2025 15:53:04 +0000</pubDate><source url="https://www.datasciencecentral.com/">Data Science Central</source><content:encoded><![CDATA[Digital Twins technologies are virtual replicas of physical assets that help streamline processes or systems that mirror real-time data and operational conditions. They provide solutions using Artificial Intelligence technology. In this article, we will discuss the influence of Digital Twins and AI In manufacturing in terms of how they are revolutionizing production and maintenance by… Read More »]]></content:encoded></item><item><title>EFF Statement on U.S. Supreme Court&apos;s Decision to Uphold TikTok Ban</title><link>https://www.eff.org/deeplinks/2025/01/eff-statement-us-supreme-courts-decision-uphold-tiktok-ban</link><author>David Greene</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/tik-tok-ban-1.png" length="" type=""/><pubDate>Fri, 17 Jan 2025 15:49:05 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Project MINI RACK - a Homelab Revolution!</title><link>https://www.youtube.com/watch?v=y1GCIwLm3is</link><author>Jeff Geerling</author><category>tech</category><category>video</category><enclosure url="https://www.youtube.com/v/y1GCIwLm3is?version=3" length="" type=""/><pubDate>Fri, 17 Jan 2025 15:01:27 +0000</pubDate><source url="https://www.youtube.com/channel/UCR-DXc1voovS8nhAvccRZhg">Jeff Goerling yt channel</source><content:encoded><![CDATA[The time has come for the MINI homelab!

Special thanks to DeskPi for sending over their mini racks! Note: they did not pay me to make this video nor have any input into the contents.

The DeskPi RackMate T0 Pi 5 cluster giveaway is over, and a winner has been contacted. I have not yet heard back, but will update this post with further information.

The entry form (now closed) was located here: https://forms.gle/PP5rPXLF6Gm8X2FfA — **Congratulations to Sam J on winning the mini rack**; happy homelabbing! NO PURCHASE NECESSARY. See official contest rules here: https://www.jeffgeerling.com/2025-sweepstakes-rules

Resources mentioned in this video:

  - Project MINI RACK: https://mini-rack.jeffgeerling.com
  - LabStack Mini: https://github.com/JaredC01/LabStack
  - Blog post with more info: https://www.jeffgeerling.com/blog/2025/project-mini-rack
  - Mini Rack 001: https://github.com/geerlingguy/mini-rack/issues/3
  - Mini Rack 002: https://github.com/geerlingguy/mini-rack/issues/4
  - Mini Rack 003: https://github.com/geerlingguy/mini-rack/issues/6

Support me on Patreon: https://www.patreon.com/geerlingguy
Sponsor me on GitHub: https://github.com/sponsors/geerlingguy
Merch: https://www.redshirtjeff.com
2nd Channel: https://www.youtube.com/@GeerlingEngineering
3rd Channel: https://www.youtube.com/@Level2Jeff

Contents:

00:00 - You CAN take it with you!
01:17 - Little modular racks
02:49 - Project MINI RACK
03:25 - LabStack Mini - modular, open, and free!
08:27 - Some things work, some things need improvement
09:35 - Mini Rack 001 - go-anywhere multi-gig cluster
11:41 - A solar-powered portable rack
12:44 - Mini Rack 002 - a more economical Pi cluster
14:34 - Mini Rack 003 - ITX to the gills
15:44 - Build Showcase
15:59 - JaredC01's party tricks
19:56 - Raid Owl's hashtag
20:41 - TechnoTim's RGB-powered rack
21:41 - Hardware Haven 3D printed all the things
22:18 - Just the beginning - and 002 giveaway]]></content:encoded></item><item><title>The Real Python Podcast – Episode #235: Principles for Considering Your Python Tooling</title><link>https://realpython.com/podcasts/rpp/235/</link><author>Real Python</author><category>Real Python Blog</category><category>dev</category><category>python</category><pubDate>Fri, 17 Jan 2025 12:00:00 +0000</pubDate><source url="https://realpython.com/atom.xml">Real Python</source><content:encoded><![CDATA[What are the principles you should consider when making decisions about which Python tools to use? What anti-patterns get in the way of making the right choices for your team? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.]]></content:encoded></item><item><title>Digest #156: AWS Billing Views, Cloudflare’s Free Tier, API Strategy, DevOps Handbook, Terraform CI/CD</title><link>https://www.devopsbulletin.com/p/digest-156-aws-billing-views-cloudflares</link><author>Mohamed Labouardy</author><category>devops</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F182d4424-9b1c-421d-95bd-622f386249f0_692x336.png" length="" type=""/><pubDate>Fri, 17 Jan 2025 10:15:06 +0000</pubDate><source url="https://www.devopsbulletin.com/">DevOps bulletin</source><content:encoded><![CDATA[Welcome to this week’s edition of the DevOps Bulletin!AWS just introduced Custom Billing Views—but will it actually simplify cloud cost management? Cloudflare Pages continues to offer a surprisingly generous free tier, but what’s the real reason behind it? If you think your API strategy is solid, you might want to reconsider—find out why industry experts are urging a rethink. Meanwhile, PostgreSQL continues to thrive with the power of open-source contributions, and Vercel is making headlines by slashing build times with MicroVMs.In tutorials, we’re covering everything from handling Terraform conflicts in Azure to anonymizing Postgres data and cost-effective log archiving in S3. Plus, if you’ve ever struggled with git, we’ve got a guide that might just save your day.For open-source enthusiasts, this week’s projects are must-sees: an open-source GitOps CI/CD platform, a policy-as-code framework, a Kubernetes cluster management tool, and a cross-platform network traffic monitor. And if you’re looking for a free DevOps book, we’ve included one.All this and more in this week’s DevOps Bulletin—don’t miss out!Free Book for DevOps Engineers 📖Fannie Mae’s process for building policy as code:Highlighting cool DevOps projects to keep an eye on: is an open-source GitOps CI/CD platform for automating infrastructure workflows. It integrates with GitHub to orchestrate Terraform, OpenTofu, CDKTF, and Terragrunt operations via pull requests. is a one-stop-shop for GRC, covering Risk, AppSec, and Audit Management and supporting +70 frameworks worldwide with auto-mapping: NIST CSF, ISO 27001, SOC2, and so much more. provides automated management and operation of Kubernetes clusters as a service. It enables the creation and management of homogeneous Kubernetes clusters across various cloud and infrastructure providers. is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, It also works with Jinja templating and dbt. is an application to comfortably monitor your Internet traffic cross-platform, intuitive, and reliable. takes in an etcd data directory or db file used in Kubernetes, extracts the Kubernetes resources, and then writes the resources to disk in YAML format.If you have feedback to share or are interested in sponsoring this newsletter, feel free to reach out via , or simply reply to this email.]]></content:encoded></item><item><title>How to use Anthropic MCP Server with open LLMs, OpenAI or Google Gemini</title><link>https://www.philschmid.de/mcp-example-llama</link><author></author><category>dev</category><category>ai</category><category>blog</category><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><source url="https://www.philschmid.de/">Phil Shmid</source><content:encoded><![CDATA[How to use Anthropic MCP Server with open LLMs, OpenAI or Google Gemini]]></content:encoded></item><item><title>Bite: How Deepseek R1 was trained</title><link>https://www.philschmid.de/deepseek-r1</link><author></author><category>dev</category><category>ai</category><category>blog</category><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><source url="https://www.philschmid.de/">Phil Shmid</source><content:encoded><![CDATA[5 Minute Read on how Deepseek R1 was trained using Group Relative Policy Optimization (GRPO) and RL-focused multi-stage training approach.]]></content:encoded></item><item><title>Service Reliability Mathematics</title><link>https://addyosmani.com/blog/service-reliability/</link><author></author><category>dev</category><category>blog</category><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><source url="https://addyosmani.com/">Addy Osmani Blog</source><content:encoded><![CDATA[Service reliability is often reduced to a simple percentage but the reality is far more nuanced than those decimal points suggest. Lets explore what these numbers actually mean.]]></content:encoded></item><item><title>Chinese Innovations Spawn Wave of Toll Phishing Via SMS</title><link>https://krebsonsecurity.com/2025/01/chinese-innovations-spawn-wave-of-toll-phishing-via-sms/</link><author>BrianKrebs</author><category>Krebs on Security</category><category>infosec</category><pubDate>Thu, 16 Jan 2025 21:18:48 +0000</pubDate><source url="https://krebsonsecurity.com/">Krebs on Security</source><content:encoded><![CDATA[Residents across the United States are being inundated with text messages purporting to come from toll road operators like , warning that recipients face fines if a delinquent toll fee remains unpaid. Researchers say the surge in SMS spam coincides with new features added to a popular commercial phishing kit sold in China that makes it simple to set up convincing lures spoofing toll road operators in multiple U.S. states.Last week, the Massachusetts Department of Transportation (MassDOT) warned residents to be on the lookout for a new SMS phishing or “smishing” scam targeting users of , MassDOT’s all electronic tolling program. Those who fall for the scam are asked to provide payment card data, and eventually will be asked to supply a one-time password sent via SMS or a mobile authentication app.Reports of similar SMS phishing attacks against customers of other U.S. state-run toll facilities surfaced around the same time as the MassDOT alert. People in Florida reported receiving SMS phishing that spoofed , Florida’s prepaid toll program.This phishing module for spoofing MassDOT’s EZDrive toll system was offered on Jan. 10, 2025 by a China-based SMS phishing service called “Lighthouse.”In Texas, residents said they received text messages about unpaid tolls with the North Texas Toll Authority. Similar reports came from readers in California, Colorado, Connecticut, Minnesota, and Washington. This is by no means a comprehensive list.A new module from the Lighthouse SMS phishing kit released Jan. 14 targets customers of the North Texas Toll Authority (NTTA).In each case, the emergence of these SMS phishing attacks coincided with the release of new phishing kit capabilities that closely mimic these toll operator websites as they appear on mobile devices. Notably, none of the phishing pages will even load unless the website detects that the visitor is coming from a mobile device. works in security research at SecAlliance, a CSIS Security Group company. Merrill said the volume of SMS phishing attacks spoofing toll road operators skyrocketed after the New Year, when at least one Chinese cybercriminal group known for selling sophisticated SMS phishing kits began offering new phishing pages designed to spoof toll operators in various U.S. states.According to Merrill, multiple China-based cybercriminals are selling distinct SMS-based phishing kits that each have hundreds or thousands of customers. The ultimate goal of these kits, he said, is to phish enough information from victims that their payment cards can be added to mobile wallets and used to buy goods at physical stores, online, or to launder money through shell companies.A component of the Chinese SMS phishing kit Lighthouse made to target customers of The Toll Roads, which refers to several state routes through Orange County, Calif.Merrill said the different purveyors of these SMS phishing tools traditionally have impersonated shipping companies, customs authorities, and even governments with tax refund lures and visa or immigration renewal scams targeting people who may be living abroad or new to a country.“What we’re seeing with these tolls scams is just a continuation of the Chinese smishing groups rotating from package redelivery schemes to toll road scams,” Merrill said. “Every one of us by now is sick and tired of receiving these package smishing attacks, so now it’s a new twist on an existing scam.”In October 2023, KrebsOnSecurity wrote about a massive uptick in SMS phishing scams targeting  customers. That story revealed the surge was tied to innovations introduced by “,” a mainland China-based proprietor of a popular phishing kit and service. At the time, Chenlun had just introduced new phishing pages made to impersonate postal services in the United States and at least a dozen other countries.SMS phishing kits are hardly new, but Merrill said Chinese smishing groups recently have introduced innovations in deliverability, by more seamlessly integrating their spam messages with Apple’s  technology, and with RCS, the equivalent “rich text” messaging capability built into  devices.“While traditional smishing kits relied heavily on SMS for delivery, nowadays the actors make heavy use of iMessage and RCS because telecom operators can’t filter them and they likely have a higher success rate with these delivery channels,” he said.It remains unclear how the phishers have selected their targets, or from where their data may be sourced. A notice from MassDOT cautions that “the targeted phone numbers seem to be chosen at random and are not uniquely associated with an account or usage of toll roads.”Indeed, one reader shared on Mastodon yesterday that they’d received one of these SMS phishing attacks spoofing a local toll operator, when they didn’t even own a vehicle.Targeted or not, these phishing websites are dangerous because they are operated dynamically in real-time by criminals. If you receive one of these messages, just ignore it or delete it, but please do not visit the phishing site. The FBI asks that before you bin the missives, consider filing a complaint with the agency’s Internet Crime Complaint Center (IC3), including the phone number where the text originated, and the website listed within the text.]]></content:encoded></item><item><title>Home Microsoft 365 plans use Copilot AI features as pretext for a price hike</title><link>https://arstechnica.com/gadgets/2025/01/home-microsoft-365-plans-use-copilot-ai-features-as-pretext-for-a-price-hike/</link><author>Andrew Cunningham</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/01/copilot-office-1152x648.jpeg" length="" type=""/><pubDate>Thu, 16 Jan 2025 19:49:59 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Microsoft has two announcements for subscribers to its Microsoft 365 Personal and Family plans today. First, you're getting the Copilot-powered AI features that Microsoft has been rolling out to businesses and Copilot Pro subscribers, like summarizing or generating text in Word, drafting slideshows in PowerPoint based on a handful of criteria, or analyzing data in Excel. Second, you'll be paying more for the privilege of using those features, to the tune of an extra $3 a month or $30 a year.This raises the price of a Microsoft 365 Personal subscription from $7 a month or $70 a year to $10 and $100; a family subscription goes from $10 a month or $100 a year to $13 a month or $130 a year. For current subscribers, these prices go into effect the next time your plan renews.Current subscribers are also being given an escape hatch "for a limited time." "Classic" Personal and Family plans at the old prices with no Copilot features included will still be offered, but you'll need to go to the "services & subscriptions" page of your Microsoft account and attempt to cancel your existing subscription to be offered the discounted pricing.]]></content:encoded></item><item><title>Systemic Risk Reporting: A System in Crisis?</title><link>https://www.eff.org/deeplinks/2025/01/systemic-risk-reporting-system-crisis</link><author>Svea Windwehr</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/eu-flag-11.png" length="" type=""/><pubDate>Thu, 16 Jan 2025 17:45:24 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Measuring productivity impact with Diff Authoring Time</title><link>https://engineering.fb.com/2025/01/16/developer-tools/measuring-productivity-impact-with-diff-authoring-time/</link><author></author><category>dev</category><category>official</category><pubDate>Thu, 16 Jan 2025 17:00:00 +0000</pubDate><source url="https://engineering.fb.com/">Facebook engineering</source><content:encoded><![CDATA[Do types actually make developers more productive? Or is it just more typing on the keyboard? To answer that question we’re revisiting Diff Authoring Time (DAT) – how Meta measures how long it takes to submit changes to a codebase.DAT is just one of the ways we measure developer productivity and this latest episode of the Meta Tech Podcast takes a look at two concrete use cases for DAT, including a type-safe mocking framework in Hack.Tune in to learn how we leverage metrics to run experiments on productivity in our internal codebase at Meta.Download or listen to the podcast episode below:
You can also find the episode wherever you get your podcasts, including:The Meta Tech Podcast is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.And if you’re interested in learning more about career opportunities at Meta visit the Meta Careers page.]]></content:encoded></item></channel></rss>