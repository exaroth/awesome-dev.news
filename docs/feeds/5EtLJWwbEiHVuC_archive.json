{"id":"5EtLJWwbEiHVuC","title":"Kubernetes","displayTitle":"Kubernetes","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":17,"items":[{"title":"Help with k3s setup on wsl","url":"https://www.reddit.com/r/kubernetes/comments/1iqqly6/help_with_k3s_setup_on_wsl/","date":1739706589,"author":"/u/watterbottle800","guid":526,"unread":true,"content":"<p>I'm trying to install a mern stack application consisting of 11 microservices some which have init containers that depend response from some of the other containers, I have a k3s cluster installed on wsl2, with single node and the external IP of the node is the eth0 ip of the wsl which is in 192.168 range. My pods are in 10.42.0.0/24 and svc in 10.43.0.0/24. All the pods are in default subnet, one of the pods is exposed on port 15672, behind a nodeport svc (say my-svc) with nodeport 30760. One of the init container completed only after a 200 response to curl http:my-svc:15762, but the connectivity is failing with \"failed to connect to &lt;svc cluster ip&gt; port 15672 : couldn't connect to server\" after sometime. </p><p>This specific initcontainer doesn't have nslookup utility doesn't have nslookup or curl utility hence I tried both curl and nslookup from a test pod in the same namespace. Curl failed while nslookup resolved to correct service name and ip), I'm assuming the traffic is going till the svc but not beyond that. I tried with other pods for example call nginx test pod at port 80 from another test pod it failed as well. </p><p>The same setup works fine in k3s cluster in my ec2 and my personal pc, this is my work pc. It would be really helpful if someone could advice on how to troubleshoot this. Thanks</p>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes In-Place Pod Vertical Scaling","url":"https://scaleops.com/blog/kubernetes-in-place-pod-vertical-scaling/","date":1739703019,"author":"/u/Wownever","guid":527,"unread":true,"content":"<p>Kubernetes continues to evolve, offering features that enhance efficiency and adaptability for developers and operators. Among these are <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/\" target=\"_blank\" rel=\"noreferrer noopener\">Resize CPU and Memory Resources assigned to Containers</a>, introduced in Kubernetes version 1.27. This feature allows for adjusting the CPU and memory resources of running pods without  them, helping to minimize downtime and optimize resource usage. This blog post explores how this feature works, its practical applications, limitations, and cloud provider support. Understanding this functionality is vital for effectively managing containerized workloads and maintaining system reliability.</p><h2>What Is In-Place Pod Vertical Scaling?</h2><p>Traditionally, modifying the resource allocation for a Kubernetes pod required a restart, potentially disrupting applications and causing downtime. In-place scaling changes this by enabling real-time CPU and memory adjustments while the pod continues running. This is particularly useful for workloads with a very low tolerance for pod evictions.</p><p>What’s behind the feature gate?</p><p>The new  spec element allows you to specify how a pod reacts to a patch command that changes its resource requests, enabling changing resource requests without rescheduling the pod.</p><p>The result of the change attempt is communicated as part of the pods’ status in a field called&nbsp;  (for more information on the new fields, check out the Kubernetes API <a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#container-v1-core\" target=\"_blank\" rel=\"noreferrer noopener\">documentation</a>.)<p>Additionally, this feature introduces the </p> in the spec element for containers, allowing fine-grained control over resizing behavior and allowing the developer to choose if CPU change or Memory change should lead to rescheduling the pod.</p><div><ul><li>Dynamic Scaling: Modify CPU and memory allocations while pods run.</li><li>No Restarts: Avoid downtime caused by pod restarts.</li><li>Granular Control: Enable precise resource tuning for better efficiency.</li></ul></div><p>The <code>InPlacePodVerticalScaling</code> feature integrates seamlessly into Kubernetes to provide a more dynamic approach to resource allocation. Here’s a detailed breakdown of how it operates:</p><div><ol><li> Activating the <code>InPlacePodVerticalScaling</code> feature gate in your cluster configuration is required to enable this functionality. This allows the kubelet on each node to detect and process resource updates dynamically.</li><li><strong>Dynamic Resource Updates via Kube API:</strong> With the feature enabled, the kubelet directly applies resource changes to running pods without requiring restarts. Supported container runtimes (e.g., <a href=\"https://github.com/containerd/containerd/releases/tag/v1.6.9\" target=\"_blank\" rel=\"noreferrer noopener\">containerd v1.6.9</a> or later) ensure these updates are applied efficiently. If constraints like insufficient free memory or CPU prevent the changes, the pod follows the regular flow: it is recreated and rescheduled.</li><li> The  field dictates how CPU and memory adjustments are handled. For instance, you can set  for live updates without restarts or  to force a restart when a specific resource is modified.</li></ol></div><h2>Limitations and Considerations</h2><p>While In-Place Pod Vertical Scaling offers significant benefits, it has limitations:</p><p>1. Cloud Provider Support</p><div><ul><li>AWS: Not supported by Amazon Elastic Kubernetes Service (EKS) as there is no way to activate the needed feature gate.</li><li>GCP: Google Kubernetes Engine (GKE) supports this feature as an alpha capability, starting with Kubernetes version 1.27. It must be enabled during cluster creation and requires disabling auto-repair and auto-upgrade. See the <a href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/alpha-clusters\" target=\"_blank\" rel=\"noreferrer noopener\">GKE alpha clusters documentation</a>.</li></ul></div><p>Several Kubernetes policies and mechanisms govern resource scaling. These include:</p><div><ul><li>Resource quotas limit the total CPU and memory usage for a namespace. If an <code>InPlacePodVerticalScaling</code> operation exceeds these limits, the scaling request will fail. For example:</li></ul></div><pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: example-namespace\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: \"32Gi\"\n</code></pre><div><ul><li>Limit ranges enforce minimum and maximum resource constraints for individual pods or containers within a namespace. The pod will be denied the resource adjustment if a scaling operation exceeds these bounds. Example configuration:</li></ul></div><pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limits\n  namespace: example-namespace\nspec:\n  limits:\n  - type: Container\n    max:\n      cpu: \"2\"\n      memory: \"4Gi\"\n    min:\n      cpu: \"100m\"\n      memory: \"128Mi\"\n</code></pre><div><ul><li>Admission controllers, such as <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\" target=\"_blank\" rel=\"noreferrer noopener\">Pod Security Admission</a> or custom webhook controllers, can deny scaling operations if they conflict with security or operational policies. For example, a controller may restrict pods from exceeding certain CPU limits.</li></ul></div><div><ul><li>Not all applications can dynamically consume additional resources or adjust to reduced allocations. Examples include:<div><ul><li>Thread Pool Bound applications, like Gunicorn or Unicorn, rely on predefined worker counts.</li><li>Memory-Bound Applications: Applications like Java with fixed Xmx parameters.</li></ul></div></li></ul></div><div><ul><li>In cases where the <a href=\"https://scaleops.com/blog/kubernetes-hpa/\">HPA</a> is based on the resource being patched, this can cause an erratic horizontal scaling behavior. For example:<div><ul><li>HPA scaling behavior is based on CPU average utilization</li></ul></div></li></ul></div><div><ol><li>A pod is changing from 1 core to 2 cores; this can cause a scale-down in pods and affect the bottom-line performance of the application.</li><li>A pod changes from 2 cores to 1; this can cause a scale-up in pods, creating a waste of resources or potential downstream pressure due to the additional and unexpected pods created.</li></ol></div><div><ul><li> Dynamically allocate resources during training and inference phases.</li><li> Combine Horizontal Pod Autoscaler (HPA) with In-Place Pod Vertical Scaling for efficient surge handling.</li><li> Reduce waste by allocating the right amount of resources to each pod in real-time.</li><li> Some applications require significantly higher CPU and memory resources during startup compared to their runtime needs. Google’s example, <a href=\"https://cloud.google.com/blog/products/containers-kubernetes/understanding-kubernetes-dynamic-resource-scaling-and-cpu-boost\" target=\"_blank\" rel=\"noreferrer noopener\">Startup CPU Boost</a>, demonstrates how dynamic resource scaling can address such scenarios effectively.</li></ul></div><h3>1. Enable the Feature Gate</h3><p>Add the following configuration to enable the <code>InPlacePodVerticalScaling</code> feature:</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\napiServer:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\ncontrollerManager:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\nscheduler:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\n</code></pre><p>For GKE, create a cluster with alpha features enabled:</p><pre><code>gcloud container clusters create poc \\\n    --enable-kubernetes-alpha \\\n    --no-enable-autorepair \\\n    --no-enable-autoupgrade\n</code></pre><p>Define a deployment with initial CPU and memory requests and limits:</p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n        resizePolicy:\n        - resourceName: cpu\n          restartPolicy: NotRequired\n        - resourceName: memory\n          restartPolicy: NotRequired\n</code></pre><p>Once deployed, you can check the <code>cpu.weight, cpu.max, memory.max, memory.min</code> from within the container to see the initial values that the container starts with.</p><pre><code>kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max \n</code></pre><p>Adjust resource allocations for a running pod dynamically:</p><pre><code>kubectl patch pod $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -p '{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"resources\":{\"requests\":{\"cpu\":\"750m\"}}}]}}'\n</code></pre><p>Confirm updated resource settings:</p><pre><code>kubectl describe pod -l app=app\n</code></pre><p>Additionally, you can connect to the container and see the change in <code>cpu.weight, cpu.max, memory.max, memory.min</code> from within the container.</p><pre><code>kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max\n</code></pre><p>In-Place Pod Vertical Scaling is a powerful tool for managing dynamic workloads in Kubernetes, reducing downtime, and <a href=\"https://scaleops.com/blog/optimizing-kubernetes-resources/\">optimizing resource usage</a>. While its adoption depends on cloud provider support and application compatibility, this feature offers significant efficiency and cost-saving benefits. As Kubernetes evolves, such features will become essential for effective container orchestration.</p><p>While Google’s <a href=\"https://github.com/google/kube-startup-cpu-boost\" target=\"_blank\" rel=\"noreferrer noopener\">Kube Startup CPU Boost</a> example is just a specific use case scenario, <a href=\"https://try.scaleops.com\">ScaleOps</a> provides an all in one resource management solution to address all needed scenarios related to Kubernetes resource management.</p>","contentLength":9121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iqps4m/kubernetes_inplace_pod_vertical_scaling/"},{"title":"rke2 and DNS","url":"https://www.reddit.com/r/kubernetes/comments/1iqdela/rke2_and_dns/","date":1739658902,"author":"/u/Affectionate_Horse86","guid":515,"unread":true,"content":"<p>I'm going crazy trying to get coredns to talk to my DNS server for names in my domain (I'm using a pihole server that is updated by terraform for VM addresses and by external-dns for k8s services)</p><p>I'm using lablabs ansible role, but a pure rke2 answer is fine, I can figure out the rest. I have</p><pre><code> dest: /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml content: | apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: rke2-coredns namespace: kube-system spec: valuesContent: |- nodelocal: enabled: true ipvs: true zoneFiles: - filename: my-domain.com.conf domain: my-domain.com contents: | my-domain.com:53 { errors cache 30 forward . 10.0.200.1 # my Pihole DNS server } extraConfig: import: parameters: /etc/coredns/my-domain.com.conf when: rke2_type == \"server\" </code></pre><p>and this should have the effect of instructing coredns to use my DNS server for everyting in 'my-domain.com', but although this part lands in the appropriate config map, it doesn't seem to do any good.</p><p>I can replace coredns completely with kubelet flags, but then I lose the resolution of cluster addresses and I don;t get too far in bringing the cluster up.</p>","contentLength":1146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Networking in K8s","url":"https://www.reddit.com/r/kubernetes/comments/1iq9mqp/networking_in_k8s/","date":1739648990,"author":"/u/I-Ad-7","guid":270,"unread":true,"content":"<p>Background: Never used k8s before 4 months ago. I would say I’m pretty good at picking up new stuff and already have lots of knowledge and hands on experience (mostly from doing stuff on my own and reading lots of Oreilly books) for someone like me (age 23). Have a CS background. Doing an internship. </p><p>I was put into a position where I had to use K8s for everyday work and don’t get me wrong I’m ecstatic about being an intern but already having the opportunity to work with deployments etc. </p><p>What I did was read The kubernetes book by Nigel Poulton and got myself 3 cheap PCs and bootstrapped myself a K3s cluster and installed Longorn as the storage and Nginx as the ingress controller.</p><p>Right now I can pretty much do most stuff and have some cool projects running on my cluster.</p><p>I’m also learning new stuff every day. </p><p>But where I find myself lacking is Networking. Not just in Kubernetes but also generally. </p><p>There are two examples of me getting frustrated because of my lacking networking knowledge:</p><ul><li><p>I wanted to let a GitHub actions step access my cluster through the tailscale K8s operator which runs on my cluster but failed</p></li><li><p>Was wondering why I can’t see the real IPs of people that are accessing my api which is on a pod on my cluster and got intimidated by stuff like Layer 2 Networking and why you need a load balancer for that etc.</p></li></ul><p>Do I really have to be as competent as a network engineer to be a good dev ops engineer / data engineer / cloud engineer or anything in ops?</p><p>I don’t mind it but I’m struggling to learn Networking and it’s not that I don’t have the basics but I don’t have the advanced knowledge needed yet, so how do I actually get there?</p>","contentLength":1675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How are you monitoring your cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1iq94yg/how_are_you_monitoring_your_cluster/","date":1739647696,"author":"/u/psavva","guid":269,"unread":true,"content":"<p>I have a 3 node bare metal cluster and installed Kube Prometheus Stack helm chart.</p><p>I'm having a very hard time getting the service monitors working correctly. I have any 30% of the 150 or so service monitors failing.</p><p>CPU and networking are always displaying 'No Data'</p><p>I fixed the bind addresses for etdc, scheduler, Kube proxy, controller manager from 127.0.0.1 to bind to 0.0.0.0</p><p>That fixes the alerts on a fresh install of the stack. </p><p>1) CPU Metrics 2) Network Metrics 3) Resource Dashboards are all not working properly (Namespace and pods are always empty,) 4) Service Monitors failing.</p><p>I'm using the latest version of the stack on bare metal cluster 1.31, running calico as a CNI.</p><p>Any advice would be appreciated.</p><p>If anyone has a fully working example of the helm chart values that fully work, that would be awesome.</p>","contentLength":813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Questions around LoadBalancer","url":"https://www.reddit.com/r/kubernetes/comments/1iq7y2v/questions_around_loadbalancer/","date":1739644561,"author":"/u/HahaHarmonica","guid":525,"unread":true,"content":"<p>New to k8s. I’ve deployed rke2 and i’ve got several questions. </p><p>Main Question) So i’m trying to install rancher UI on it. When you go to install with helm it asks for a “hostname” and the hostname should be the name of your load balancer…i enabled the load balancer of rke2 but I have no clue how to operate with it…how do I change the configuration to point to rancher? The instructions aren’t very clear on the rke2 site on how to use it other than setting the enable-loadbalancer flag. </p><p>2) During my debugging, i ran the command “kubectl get pods -A -o wide. I have a server node and an agent node. In the column of IP it showed the two IPs of the sever and agent. What was odd was that it showed pods running that were running on the agent node that shouldn’t have been running since I stopped the agent service on the agent node and I ran the kill all script. So how in the world can the containers supposedly running on the agent node…actually be running.</p><p>3) I had some problems with ports not opened initially. Forgot to apply the reload command to make sure the ports were open. I then ran systemctl restart rke2-server on the sever and then systemctl restart rke2-agent on the agent and it was still broken. I finally after 30 min of thinking that wasn’t the problem completely resetting the services by running the killall scripts on both of them before it works…so why in the world won’t k8s actually respect systemctl and restart properly without literally shutting everything down. </p>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Career transition in to Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/","date":1739626865,"author":"/u/Similar-Secretary-86","guid":272,"unread":true,"content":"<p>\"I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated </p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My new blog post comparing networking in EKS vs. GKE","url":"https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/","date":1739617569,"author":"/u/jumiker","guid":271,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Dive into VPA Recommender","url":"https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/","date":1739615164,"author":"/u/erik_zilinsky","guid":273,"unread":true,"content":"<p>I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.</p><p>Based on my findings, I wrote a <a href=\"https://erikzilinsky.com/posts/vpa1.html\">blog post</a> about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.</p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Container Networking - Kubernetes with Calico","url":"https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/","date":1739604355,"author":"/u/tkr_2020","guid":268,"unread":true,"content":"<ul><li>: VLAN 10</li><li>: VLAN 20</li></ul><p>When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:</p><p>The inner IP header reflects:</p><p>The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.</p><p>Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?</p>","contentLength":502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11","url":"https://www.youtube.com/watch?v=eHa6GhK7L0I","date":1739541570,"author":"CNCF [Cloud Native Computing Foundation]","guid":334,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eHa6GhK7L0I?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 46 (Dragonfly)","url":"https://www.youtube.com/watch?v=gd6HRgr8KcA","date":1739512616,"author":"CNCF [Cloud Native Computing Foundation]","guid":333,"unread":true,"content":"<article>Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. \n\nThis CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.</article>","contentLength":576,"flags":null,"enclosureUrl":"https://www.youtube.com/v/gd6HRgr8KcA?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1739491200,"author":"","guid":404,"unread":true,"content":"<p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><figure><img src=\"https://kubernetes.io/images/docs/components-of-kubernetes.svg\" alt=\"Components of Kubernetes\"></figure><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the Node\nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial Node is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><figure><img src=\"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/ccm-chicken-egg-problem-sequence-diagram.svg\" alt=\"Chicken and egg problem sequence diagram\"><figcaption><p>Chicken and egg problem sequence diagram</p></figcaption></figure><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a Node resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a Node resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one “correct way” to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting “hostNetwork” to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider’s instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure’s node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.\n<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider’s documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nname: cloud-controller-manager\nnamespace: kube-system\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nannotations:\nkubernetes.io/description: Cloud controller manager for my infrastructure\nspec:\ncontainers: # the container details will depend on your specific cloud controller manager\n- name: cloud-controller-manager\ncommand:\n- /bin/my-infrastructure-cloud-controller-manager\n- --leader-elect=true\n- -v=1\nimage: registry/my-infrastructure-cloud-controller-manager@latest\nresources:\nrequests:\ncpu: 200m\nmemory: 50Mi\nhostNetwork: true # these Pods are part of the control plane\nnodeSelector:\nnode-role.kubernetes.io/control-plane: \"\"\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- topologyKey: \"kubernetes.io/hostname\"\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\ntolerations:\n- effect: NoSchedule\nkey: node-role.kubernetes.io/master\noperator: Exists\n- effect: NoExecute\nkey: node.kubernetes.io/unreachable\noperator: Exists\ntolerationSeconds: 120\n- effect: NoExecute\nkey: node.kubernetes.io/not-ready\noperator: Exists\ntolerationSeconds: 120\n- effect: NoSchedule\nkey: node.cloudprovider.kubernetes.io/uninitialized\noperator: Exists\n- effect: NoSchedule\nkey: node.kubernetes.io/not-ready\noperator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":10702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes History Inspector, with Kakeru Ishii","url":"http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii","date":1739445780,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":312,"unread":true,"content":"<p dir=\"ltr\">Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week ","contentLength":341,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"Sandbox environments: Creating efficient and isolated testing realms","url":"https://www.youtube.com/watch?v=fh7-lQVmX-o","date":1739426433,"author":"CNCF [Cloud Native Computing Foundation]","guid":332,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fh7-lQVmX-o?version=3","enclosureMime":"","commentsUrl":null},{"title":"KitOps: AI Model Packaging Standards","url":"https://www.youtube.com/watch?v=1TD-e_wVe4Q","date":1739426400,"author":"CNCF [Cloud Native Computing Foundation]","guid":331,"unread":true,"content":"<article>Chat with us on Discord:  https://discord.gg/Tapeh8agYy\n\nCheck out our repos:\nKitOps      https://github.com/jozu-ai/kitops\nPyKitOps Python Library  https://github.com/jozu-ai/pykitops\nKitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin</article>","contentLength":253,"flags":null,"enclosureUrl":"https://www.youtube.com/v/1TD-e_wVe4Q?version=3","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing Kyverno policy enforcement performance for large clusters","url":"https://www.youtube.com/watch?v=DWmCAUCs3bc","date":1739211188,"author":"CNCF [Cloud Native Computing Foundation]","guid":330,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DWmCAUCs3bc?version=3","enclosureMime":"","commentsUrl":null}],"tags":["k8s"]}