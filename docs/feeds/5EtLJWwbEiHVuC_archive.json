{"id":"5EtLJWwbEiHVuC","title":"Kubernetes","displayTitle":"Kubernetes","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":8,"items":[{"title":"ChatLoopBackOff: Episode 68 (KANISTER)","url":"https://www.youtube.com/watch?v=-dy_J3VmmOg","date":1755903066,"author":"CNCF [Cloud Native Computing Foundation]","guid":237127,"unread":true,"content":"<article>Join us LIVE as CNCF Ambassador Carlos Santana dives into Kanister, the open source framework for application-level data management on Kubernetes.\n\nCarlos will be exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that highlight how Kanister helps extend Kubernetes for backup, recovery, and data operations.\n\nIf you’re curious about how cloud native projects approach complex data management challenges, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.\n\nBring your questions, share your experiences, and learn in real time as we explore Kanister together!</article>","contentLength":731,"flags":null,"enclosureUrl":"https://www.youtube.com/v/-dy_J3VmmOg?version=3","enclosureMime":"","commentsUrl":null},{"title":"Blind & Visually Impaired Initiative (BVI) Meeting - 2025-08-19","url":"https://www.youtube.com/watch?v=bJej44Ug8tU","date":1755893803,"author":"CNCF [Cloud Native Computing Foundation]","guid":237082,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":319,"flags":null,"enclosureUrl":"https://www.youtube.com/v/bJej44Ug8tU?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff: Episode 67 (Kserve)","url":"https://www.youtube.com/watch?v=BjXZxUR8NMo","date":1755808425,"author":"CNCF [Cloud Native Computing Foundation]","guid":235914,"unread":true,"content":"<article>Join us LIVE as CNCF Ambassador Shivay Lamaba dives into KServe, the open source project designed for scalable and reliable ML models serving on Kubernetes.\n\nShivay will be  exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that make KServe a key piece in the cloud native AI/ML ecosystem.\nIf you’re curious about production-grade model inference, want to see how cloud native communities approach machine learning workloads, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.\n\nBring your questions, share your experiences, and learn in real time as we explore KServe together!</article>","contentLength":751,"flags":null,"enclosureUrl":"https://www.youtube.com/v/BjXZxUR8NMo?version=3","enclosureMime":"","commentsUrl":null},{"title":"LLM-D, with Clayton Coleman and Rob Shaw","url":"http://sites.libsyn.com/419861/llm-d-with-clayton-coleman-and-rob-shaw","date":1755696240,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":234551,"unread":true,"content":"<p dir=\"ltr\">Guests are Clayton Coleman and Rob Shaw. Clayton is a Core contributor to Kubernetes, the containerized cluster manager, and founding architect for OpenShift, the open source platform as a service. Clayton helped launch the shift to cloud native applications and the platforms that enable them. At Google my mission is to make Kubernetes and GKE the best place to run workloads, especially accelerated AI/ML workloads, and especially especially very large model inference at scale with the inference gateway and llm-d. Rob Shaw is an Engineering Director at Redhat and is a contributor to the vLLM project.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week  Links from the interview ","contentLength":715,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD258.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"Tuning Linux Swap for Kubernetes: A Deep Dive","url":"https://kubernetes.io/blog/2025/08/19/tuning-linux-swap-for-kubernetes-a-deep-dive/","date":1755628200,"author":"","guid":233464,"unread":true,"content":"<p>The Kubernetes <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/swap-memory-management/\">NodeSwap feature</a>, likely to graduate to  in the upcoming Kubernetes v1.34 release,\nallows swap usage:\na significant shift from the conventional practice of disabling swap for performance predictability.\nThis article focuses exclusively on tuning swap on Linux nodes, where this feature is available. By allowing Linux nodes to use secondary storage for additional virtual memory when physical RAM is exhausted, node swap support aims to improve resource utilization and reduce out-of-memory (OOM) kills.</p><p>However, enabling swap is not a \"turn-key\" solution. The performance and stability of your nodes under memory pressure are critically dependent on a set of Linux kernel parameters. Misconfiguration can lead to performance degradation and interfere with Kubelet's eviction logic.</p><p>In this blogpost, I'll dive into critical Linux kernel parameters that govern swap behavior. I will explore how these parameters influence Kubernetes workload performance, swap utilization, and crucial eviction mechanisms.\nI will present various test results showcasing the impact of different configurations, and share my findings on achieving optimal settings for stable and high-performing Kubernetes clusters.</p><h2>Introduction to Linux swap</h2><p>At a high level, the Linux kernel manages memory through pages, typically 4KiB in size. When physical memory becomes constrained, the kernel's page replacement algorithm decides which pages to move to swap space. While the exact logic is a sophisticated optimization, this decision-making process is influenced by certain key factors:</p><ol><li>Page access patterns (how recently pages are accessed)</li><li>Page dirtyness (whether pages have been modified)</li><li>Memory pressure (how urgently the system needs free memory)</li></ol><h3>Anonymous vs File-backed memory</h3><p>It is important to understand that not all memory pages are the same. The kernel distinguishes between anonymous and file-backed memory.</p><p>: This is memory that is not backed by a specific file on the disk, such as a program's heap and stack. From the application's perspective this is private memory, and when the kernel needs to reclaim these pages, it must write them to a dedicated swap device.</p><p>: This memory is backed by a file on a filesystem. This includes a program's executable code, shared libraries, and filesystem caches. When the kernel needs to reclaim these pages, it can simply discard them if they have not been modified (\"clean\"). If a page has been modified (\"dirty\"), the kernel must first write the changes back to the file before it can be discarded.</p><p>While a system without swap can still reclaim clean file-backed pages memory under pressure by dropping them, it has no way to offload anonymous memory. Enabling swap provides this capability, allowing the kernel to move less-frequently accessed memory pages to disk to conserve memory to avoid system OOM kills.</p><h3>Key kernel parameters for swap tuning</h3><p>To effectively tune swap behavior, Linux provides several kernel parameters that can be managed via .</p><ul><li>: This is the most well-known parameter. It is a value from 0 to 200 (100 in older kernels) that controls the kernel's preference for swapping anonymous memory pages versus reclaiming file-backed memory pages (page cache).\n<ul><li>: The kernel will be aggressive in swapping out less-used anonymous memory to make room for file-cache.</li><li>: The kernel will strongly prefer dropping file cache pages over swapping anonymous memory.</li></ul></li><li>: This parameter tells the kernel to keep a minimum amount of memory free as a buffer. When the amount of free memory drops below the this safety buffer, the kernel starts more aggressively reclaiming pages (swapping, and eventually handling OOM kills).\n<ul><li> It acts as a safety lever to ensure the kernel has enough memory for critical allocation requests that cannot be deferred.</li><li>: Setting a higher  effectively raises the floor for for free memory, causing the kernel to initiate swap earlier under memory pressure.</li></ul></li><li><code>vm.watermark_scale_factor</code>: This setting controls the gap between different watermarks: ,  and , which are calculated based on .\n<ul><li>:\n<ul><li>: When free memory is below this mark, the  kernel process wakes up to reclaim pages in the background. This is when a swapping cycle begins.</li><li>: When free memory hits this minimum level, then aggressive page reclamation will block process allocation. Failing to reclaim pages will cause OOM kills.</li><li>: Memory reclamation stops once the free memory reaches this level.</li></ul></li><li>: A higher  careates a larger buffer between the  and  watermarks. This gives  more time to reclaim memory gradually before the system hits a critical state.</li></ul></li></ul><p>In a typical server workload, you might have a long-running process with some memory that becomes 'cold'. A higher  value can free up RAM by swapping out the cold memory, for other active processes that can benefit from keeping their file-cache.</p><p>Tuning the  and  parameters to move the swapping window early will give more room for  to offload memory to disk and prevent OOM kills during sudden memory spikes.</p><p>To understand the real-impact of these parameters, I designed a series of stress tests.</p><ul><li>: GKE on Google Cloud</li><li>: 1.33.2</li><li>:  (8GiB RAM, 50GB swap on a  disk, without encryption), Ubuntu 22.04</li><li>: A custom Go application designed to allocate memory at a configurable rate, generate file-cache pressure, and simulate different memory access patterns (random vs sequential).</li><li>: A sidecar container capturing system metrics every second.</li><li>: Critical system components (kubelet, container runtime, sshd) were prevented from swapping by setting  in their respective cgroups.</li></ul><p>I ran a stress-test pod on nodes with different swappiness settings (0, 60, and 90) and varied the  and  parameters to observe the outcomes under heavy memory allocation and I/O pressure.</p><h4>Visualizing swap in action</h4><p>The graph below, from a 100MBps stress test, shows swap in action. As free memory (in the \"Memory Usage\" plot) decreases, swap usage () and swap-out activity () increase. Critically, as the system relies more on swap, the I/O activity and corresponding wait time ( in the \"CPU Usage\" plot) also rises, indicating CPU stress.</p><p>My initial tests with default kernel parameters (, , <code>watermark_scale_factor=10</code>) quickly led to OOM kills and even unexpected node restarts under high memory pressure. With selecting appropriate kernel parameters a good balance in node stability and performance can be achieved.</p><p>The swappiness parameter directly influences the kernel's choice between reclaiming anonymous memory (swapping) and dropping page cache. To observe this, I ran a test where one pod generated and held file-cache pressure, followed by a second pod allocating anonymous memory at 100MB/s, to observe the kernel preference on reclaim:</p><p>My findings reveal a clear trade-off:</p><ul><li>: The kernel proactively swapped out the inactive anonymous memory to keep the file cache. This resulted in high and sustained swap usage and significant I/O activity (\"Blocks Out\"), which in turn caused spikes in I/O wait on the CPU.</li><li>: The kernel favored dropping file-cache pages delaying swap consumption. However, it's critical to understand that this <strong>does not disable swapping</strong>. When memory pressure was high, the kernel still swapped anonymous memory to disk.</li></ul><p>The choice is workload-dependent. For workloads sensitive to I/O latency, a lower swappiness is preferable. For workloads that rely on a large and frequently accessed file cache, a higher swappiness may be beneficial, provided the underlying disk is fast enough to handle the load.</p><h4>Tuning watermarks to prevent eviction and OOM kills</h4><p>The most critical challenge I encountered was the interaction between rapid memory allocation and Kubelet's eviction mechanism. When my test pod, which was deliberately configured to overcommit memory, allocated it at a high rate (e.g., 300-500 MBps), the system quickly ran out of free memory.</p><p>With default watermarks, the buffer for reclamation was too small. Before  could free up enough memory by swapping, the node would hit a critical state, leading to two potential outcomes:</p><ol><li> If kubelet's eviction manager detected  was below its threshold, it would evict the pod.</li><li> In some high-rate scenarios, the OOM Killer would activate before eviction could complete, sometimes killing higher priority pods that were not the source of the pressure.</li></ol><p>To mitigate this I tuned the watermarks:</p><ol><li>Increased  to 512MiB: This forces the kernel to start reclaiming memory much earlier, providing a larger safety buffer.</li><li>Increased  to 2000: This widened the gap between the  and  watermarks (from ≈337MB to ≈591MB in my test node's ), effectively increasing the swapping window.</li></ol><p>This combination gave  a larger operational zone and more time to swap pages to disk during memory spikes, successfully preventing both premature evictions and OOM kills in my test runs.</p><p>Table compares watermark levels from  (Non-NUMA node):</p><table><thead><tr><th> and <code>watermark_scale_factor=10</code></th><th><code>min_free_kbytes=524288KiB</code> and <code>watermark_scale_factor=2000</code></th></tr></thead><tbody><tr><td>Node 0, zone Normal  &nbsp; pages free 583273  &nbsp; min 10504  &nbsp; high 15756  &nbsp; present 1310720 </td><td>Node 0, zone Normal  &nbsp; pages free 470539  &nbsp; low 337017  &nbsp; spanned 1310720 &nbsp; managed 1274542</td></tr></tbody></table><p>The graph below reveals that the kernel buffer size and scaling factor play a crucial role in determining how the system responds to memory load. With the right combination of these parameters, the system can effectively use swap space to avoid eviction and maintain stability.</p><p>Enabling swap in Kubernetes is a powerful tool, but it comes with risks that must be managed through careful tuning.</p><ul><li><p><strong>Risk of performance degradation</strong> Swapping is orders of magnitude slower than accessing RAM. If an application's active working set is swapped out, its performance will suffer dramatically due to high I/O wait times (thrashing). Swap could preferably be provisioned with a SSD backed storage to improve performance.</p></li><li><p><strong>Risk of masking memory leaks</strong> Swap can hide memory leaks in applications, which might otherwise lead to a quick OOM kill. With swap, a leaky application might slowly degrade node performance over time, making the root cause harder to diagnose.</p></li><li><p><strong>Risk of disabling evictions</strong> Kubelet proactively monitors the node for memory-pressure and terminates pods to reclaim the resources. Improper tuning can lead to OOM kills before kubelet has a chance to evict pods gracefully. A properly configured  is essential to ensure kubelet's eviction mechanism remains effective.</p></li></ul><p>Together, the kernel watermarks and kubelet eviction threshold create a series of memory pressure zones on a node. The eviction-threshold parameters need to be adjusted to configure Kubernetes managed evictions occur before the OOM kills.</p><p>As the diagram shows, an ideal configuration will be to create a large enough 'swapping zone' (between  and  watermarks) so that the kernel can handle memory pressure by swapping before available memory drops into the Eviction/Direct Reclaim zone.</p><p>Based on these findings, I recommend the following as a starting point for Linux nodes with swap enabled. You should benchmark this with your own workloads.</p><ul><li>: Linux default is a good starting point for general-purpose workloads. However, the ideal value is workload-dependent, and swap-sensitive applications may need more careful tuning.</li><li><code>vm.min_free_kbytes=500000</code> (500MB): Set this to a reasonably high value (e.g., 2-3% of total node memory) to give the node a reasonable safety buffer.</li><li><code>vm.watermark_scale_factor=2000</code>: Create a larger window for  to work with, preventing OOM kills during sudden memory allocation spikes.</li></ul><p>I encourage running benchmark tests with your own workloads in test-environments, when setting up swap for the first time in your Kubernetes cluster. Swap performance can be sensitive to different environment differences such as CPU load, disk type (SSD vs HDD) and I/O patterns.</p>","contentLength":11722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Bringing Agentic AI to Cloud Native with kagent & Kyverno","url":"https://www.youtube.com/watch?v=C0y8bDvL47M","date":1755623036,"author":"CNCF [Cloud Native Computing Foundation]","guid":233416,"unread":true,"content":"<article>Let’s stop babysitting clusters and build AI agents to operate and govern for us! Kagent, a new CNCF project, enables simple declarative development of Kubernetes-native AI agents for any cloud native projects. This session will show you how to rapidly construct custom AI agents for essential operational and management tasks. Learn how Kyverno, the Kubernetes-native policy engine for managing validation, mutation, generation, and cleanup, can be extended with AI capabilities through kagent using the emerging Model Context Protocol (MCP) to enable intelligent policy recommendations and automation.</article>","contentLength":605,"flags":null,"enclosureUrl":"https://www.youtube.com/v/C0y8bDvL47M?version=3","enclosureMime":"","commentsUrl":null},{"title":"CNCF Deaf and Hard of Hearing Working Group at KubeCon + CloudNativeCon EU 2025","url":"https://www.youtube.com/watch?v=CLhA-du1Yy8","date":1755614998,"author":"CNCF [Cloud Native Computing Foundation]","guid":233363,"unread":true,"content":"<article>Huge shout-out to the CNCF Deaf and Hard of Hearing Working Group for an incredible week at KubeCon + CloudNativeCon in London earlier this year! Check out this latest video to get a glimpse of what they were up to and what to expect in Atlanta!\n\nThey brought so much energy and had lots of activities, including a keynote, several talks, and a fun sign language crash course. Their kiosk in the Project Pavilion was a hub of activity, creating meaningful connections and fostering a more inclusive community.\n\nThis group is full of passion and brilliant people. We're so proud of what they've accomplished, and look forward to seeing the team in Atlanta! And for a fun challenge, watch the video and see if you can spot the new sign they created for \"CNCF.\" ✨</article>","contentLength":762,"flags":null,"enclosureUrl":"https://www.youtube.com/v/CLhA-du1Yy8?version=3","enclosureMime":"","commentsUrl":null},{"title":"CNL: Network-level and Identity-based Observability with Calico Open Source","url":"https://www.youtube.com/watch?v=P7RUzvXr7Vg","date":1755525083,"author":"CNCF [Cloud Native Computing Foundation]","guid":231623,"unread":true,"content":"<article>Don't let your Kubernetes environment be a mystery! Gain the visibility you need to keep things running smoothly. This session dives into why network observability is key in Kubernetes, and includes a live demo showing how to identify misconfigurations, simplify troubleshooting, streamline overall network management, and safely implement and test network security policies across any Kubernetes environment.\n\nHere's what we'll explore:\n\n* Why network observability is important (hint: Kubernetes can be a bit chaotic! )\n* How observability is the foundation for zero trust security\n* Common customer pain points: Troubleshooting, incorrect policies, compliance, and audits\n* Calico Whisker: What it is and how it provides that crucial bird's-eye view\n* LIVE DEMO: Cluster-wide network observability using Calico Whisker</article>","contentLength":821,"flags":null,"enclosureUrl":"https://www.youtube.com/v/P7RUzvXr7Vg?version=3","enclosureMime":"","commentsUrl":null}],"tags":["k8s"]}