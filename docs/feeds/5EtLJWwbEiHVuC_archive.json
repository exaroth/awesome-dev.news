{"id":"5EtLJWwbEiHVuC","title":"Kubernetes","displayTitle":"Kubernetes","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":85,"items":[{"title":"Amazon EKS Downgrade: A Practical Solution","url":"https://www.reddit.com/r/kubernetes/comments/1iw7puq/amazon_eks_downgrade_a_practical_solution/","date":1740308426,"author":"/u/Complete-Emu-6287","guid":9546,"unread":true,"content":"<p>Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup &amp; restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.</p><p>üîó Read the full article here: </p><p>If you've faced EKS downgrade challenges, let's discuss your experiences! ‚¨áÔ∏è #AWS #Kubernetes #EKS #DevOps</p>","contentLength":490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon EKS Downgrade: A Practical Solution","url":"https://www.reddit.com/r/kubernetes/comments/1iw7pqt/amazon_eks_downgrade_a_practical_solution/","date":1740308415,"author":"/u/Complete-Emu-6287","guid":9545,"unread":true,"content":"<p>Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup &amp; restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.</p><p>üîó Read the full article here: </p><p>If you've faced EKS downgrade challenges, let's discuss your experiences! ‚¨áÔ∏è #AWS #Kubernetes #EKS #DevOps</p>","contentLength":490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1740303005,"author":"/u/Aciddit","guid":9526,"unread":true,"content":"<div>By <b>Antonio Ojea, Michael McCune</b> |\n<time datetime=\"2025-02-14\">Friday, February 14, 2025</time></div><p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><figure><img src=\"https://kubernetes.io/images/docs/components-of-kubernetes.svg\" alt=\"Components of Kubernetes\"></figure><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the Node\nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial Node is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><figure><img src=\"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/ccm-chicken-egg-problem-sequence-diagram.svg\" alt=\"Chicken and egg problem sequence diagram\"><figcaption><p>Chicken and egg problem sequence diagram</p></figcaption></figure><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a Node resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a Node resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one ‚Äúcorrect way‚Äù to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting ‚ÄúhostNetwork‚Äù to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider‚Äôs instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure‚Äôs node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider‚Äôs documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: cloud-controller-manager\n  name: cloud-controller-manager\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cloud-controller-manager\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cloud-controller-manager\n      annotations:\n        kubernetes.io/description: Cloud controller manager for my infrastructure\n    spec:\n      containers: # the container details will depend on your specific cloud controller manager\n      - name: cloud-controller-manager\n        command:\n        - /bin/my-infrastructure-cloud-controller-manager\n        - --leader-elect=true\n        - -v=1\n        image: registry/my-infrastructure-cloud-controller-manager@latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 50Mi\n      hostNetwork: true # these Pods are part of the control plane\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - topologyKey: \"kubernetes.io/hostname\"\n            labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: cloud-controller-manager\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoExecute\n        key: node.kubernetes.io/unreachable\n        operator: Exists\n        tolerationSeconds: 120\n      - effect: NoExecute\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n        tolerationSeconds: 120\n      - effect: NoSchedule\n        key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - effect: NoSchedule\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iw6fpj/the_cloud_controller_manager_chicken_and_egg/"},{"title":"Talos on IPv6 only network?","url":"https://www.reddit.com/r/kubernetes/comments/1ivumee/talos_on_ipv6_only_network/","date":1740262538,"author":"/u/Moleventions","guid":9371,"unread":true,"content":"<div><p>Does anyone know if you can deploy Talos on an IPv6 only network in AWS?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Moleventions\"> /u/Moleventions </a>","contentLength":107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why K8s when there‚Äôs k3s with less resource requirements?","url":"https://www.reddit.com/r/kubernetes/comments/1ivu87n/why_k8s_when_theres_k3s_with_less_resource/","date":1740261469,"author":"/u/Crafty0x","guid":9321,"unread":true,"content":"<div><p>I don‚Äôt get why a business will run the more demanding k8s instead of k3s. What could possibly be the limitations of running k3s on full fledged servers.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Crafty0x\"> /u/Crafty0x </a>","contentLength":186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to implement dynamic storage provisioning for onPrem cluster","url":"https://www.reddit.com/r/kubernetes/comments/1ivlitx/how_to_implement_dynamic_storage_provisioning_for/","date":1740239019,"author":"/u/Impossible_Nose_2956","guid":9237,"unread":true,"content":"<p>Hi I have setup Onprem cluster for dev qa and preprod environments onPrem.</p><p>And I use redis, rabbitmq, mqtt, sqlite(for celery) in the cluster. And all these need persistent volumes.</p><p>Without dynamic provisioning, i have to create a folder, then create pv with node affinity and then create pvc and assign it to the statefulset.</p><p>I dont want to handle PVs for my onPrem clusters.</p><p>What options are available?</p><p>Do let me know if my understanding of things is wrong anywhere. </p>","contentLength":464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"anyone tried kro for kubernetes resource management yet?","url":"https://www.reddit.com/r/kubernetes/comments/1ivhubw/anyone_tried_kro_for_kubernetes_resource/","date":1740227970,"author":"/u/AnnualRich5252","guid":9118,"unread":true,"content":"<p>i just came across this article on the new resource orchestrator for kubernetes called kro, and i think it's worth discussing here. for anyone who's been dealing with the ever-growing complexity of kubernetes deployments, kro could be a game changer. it simplifies how we manage and define complex kubernetes resources by grouping them into reusable units, making everything more efficient and predictable.</p><p>what i find cool about kro is that it focuses on making kubernetes resource management easier to handle, without needing the in-depth, advanced skills that most operators and devs have to rely on today. it's got this thing called a ResourceGraphDefinition (RGD) which essentially lets you define and manage resources as a unit, and it‚Äôs smart enough to figure out deployment sequences automatically based on dependencies. really takes the guesswork out of it.</p><p>it‚Äôs worth noting kro isn‚Äôt trying to replace helm or kustomize directly, but it definitely offers a more structured and predictable approach, with better handling of CRD upgrades and dependencies. while helm has been a go-to for packaging, kro's approach might be more useful for teams looking for a more secure, governed way to manage kubernetes resources at scale.</p><p>looking forward to hearing your thoughts!</p>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's a good combination of tools to get a proper application observation solution together?","url":"https://www.reddit.com/r/kubernetes/comments/1ivh9co/whats_a_good_combination_of_tools_to_get_a_proper/","date":1740225819,"author":"/u/tofagerl","guid":9117,"unread":true,"content":"<p>I work for a company with tons of k8s clusters, but they haven't really got the whole \"let's provide all the benefits of this to the product teams\" together yet, so we're stuck with a basic Grafana + Kibana package for now. That's fine, it works. </p><p>But since I used to work with Anthos, I got used to getting the full tracing benefits from Anthos Service Mesh, and I really miss having that. </p><p>So now I'd like to pressure the infra teams to provide something better for us, but I can't just say \"use Anthos Service Mesh\", because they are already running on GCS, so there'd be no point in using Anthos. Obviously they could use a normal Istio service mesh, but I'd like to know if there are easier solutions -- Service Meshes are complicated and come with serious drawbacks, and I'm really just looking for the observation layer, not the network security layer. </p><p>Keep in mind we prefer OSS solutions as a rule, and prefer non-managed solutions as a core philosophy because we believe in understanding each tool because we know it might break. </p>","contentLength":1038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I have KCA 50% coupun that i dont need, i will give to anyone who can give me aws aor redhat coupon in exchange?","url":"https://www.reddit.com/r/kubernetes/comments/1ivgiu6/i_have_kca_50_coupun_that_i_dont_need_i_will_give/","date":1740222832,"author":"/u/sabir8992","guid":9184,"unread":true,"content":"<div><p>If you have other exam i will give to anyone who can give me aws or Redhat coupon in exchange? DM ME Please</p></div>   submitted by   <a href=\"https://www.reddit.com/user/sabir8992\"> /u/sabir8992 </a>","contentLength":139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thought We Had Our EKS Upgrade Figured Out‚Ä¶ We Did Not","url":"https://www.reddit.com/r/kubernetes/comments/1ivf8u3/thought_we_had_our_eks_upgrade_figured_out_we_did/","date":1740217389,"author":"/u/rohit_raveendran","guid":9303,"unread":true,"content":"<p>You ever think you‚Äôve got everything under control, only for prod to absolutely humble you? Yeah, that was us.</p><ul><li>Lower environments? ‚úÖ Tested a bunch.</li><li>Version mismatches? ‚úÖ All within limits.</li><li>EKS addons? ‚úÖ Using the standard upgrade flow.</li></ul><p>So we run Terraform on upgrade day. Everything‚Äôs looking fine‚Äîuntil <strong>kube-proxy upgrade just straight-up fails.</strong> Some pods get stuck in  Great.</p><p>Cool, thanks, very helpful. We hadn‚Äôt changed anything on kube-proxy beyond the upgrade, so what the hell?</p><p>At this point, one of us starts frantically digging through the EKS docs while another engineer manually downgrades kube-proxy just to get things back up. That works, but obviously, we can‚Äôt leave it like that.</p><p>And then we find it: <strong>a tiny note in the AWS docs added just a few days ago.</strong> Turns out, <strong>kube-proxy 1.31 needs an ARMv8.2 processor with Cryptographic Extensions</strong> (<a href=\"https://github.com/awsdocs/amazon-eks-user-guide/blob/mainline/latest/ug/nodes/hybrid-nodes-os.adoc#arm\">link</a>).</p><p>And guess what Karpenter had spun up?  AWS confirmed that A1s are a no-go in EKS 1.31+. We updated our Karpenter configs to block them, ran the upgrade again, and boom‚Äîeverything worked.</p><ol><li><strong>You‚Äôre never actually prepared.</strong> We tested everything, but something always slips through. The real test is how fast you fix it.</li><li><strong>Karpenter is great, but don‚Äôt let it go rogue.</strong> We‚Äôre now explicitly blocking unsupported instance families.</li></ol><p>Anyway, if you guys have ever had one of those ‚Äúwe did everything right, and it still blew up‚Äù moments, drop your stories. Misery loves company.</p>","contentLength":1449,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best way to develop talos locally?","url":"https://www.reddit.com/r/kubernetes/comments/1iv4ttd/best_way_to_develop_talos_locally/","date":1740179989,"author":"/u/obviouslyGAR","guid":8877,"unread":true,"content":"<div><p>I am currently learning and building a cluster using talos.</p><p>One thing I want to know is how are you all developing locally? </p><p>Is using docker and using the command  the best way or is there another way that can be done like utilizing terraform?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/obviouslyGAR\"> /u/obviouslyGAR </a>","contentLength":276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reading the Source Code","url":"https://www.reddit.com/r/kubernetes/comments/1iv1def/reading_the_source_code/","date":1740171145,"author":"/u/TopNo6605","guid":8856,"unread":true,"content":"<p>Curious does anyone have any advice or vids/blogs/books that go through the source code of k8s? I'm the type of person who likes to see what's happening under the hood. But k8s is a beast of an application. I was reading the apiserver source and got up the point where it's creating handlers and doing something with an openapi controller...which I didn't know existed.</p><p>Fascinating stuff but the amount of abstraction here is what gets me. Everything is an interface and abstracted to some other file, you end up following a long chain only to end up at an interface function without a definition. I get it, for development purposes. But man it's a beast to learn.</p><p>With the apiserver I literally just started logging when functions were called but I had to take a break after 4 hours of that. How do knew contributors get brought up to speed?</p>","contentLength":840,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Streamline Kubernetes Management with Rancher","url":"https://youtube.com/shorts/fOVTDobiwIE?feature=share","date":1740167919,"author":"/u/abhimanyu_saharan","guid":8833,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iv0357/streamline_kubernetes_management_with_rancher/"},{"title":"Meetup: All in Kubernetes (Munich)","url":"https://www.reddit.com/r/kubernetes/comments/1iuvh2k/meetup_all_in_kubernetes_munich/","date":1740156652,"author":"/u/simplyblock-r","guid":8722,"unread":true,"content":"<p>Hey folks, if you're in or around Munich or Bavaria: this is for you! (if it's not a right place to post it, pls delete)</p><p>We're running our second meetup of the \"All in Kubernetes\" roadshow in Munich on Thursday, 13th of March. The first meetup, last month in Berlin, one was a big success with over 80 participants in Berlin.</p><p>Community is focused around stateful workloads in Kubernetes. The sessions lined up are:</p><ol><li>Architecting and Building a K8s-based AI Platform</li><li>Databases on Kubernetes: A Storage Story</li></ol>","contentLength":501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bugs with k8s snap and IPv6 only","url":"https://www.reddit.com/r/kubernetes/comments/1iurtp1/bugs_with_k8s_snap_and_ipv6_only/","date":1740147192,"author":"/u/hblok","guid":8633,"unread":true,"content":"<p>I'm setting up an IPv6  cluster, using Ubuntu 24.04 and the k8s and kubelet snaps. I've disabled IPv4 on the eth0 interface, but not on loopback. The CP comes up fine, and can be used locally and remotely. However, when trying to connect a worker node, there are some configuration options relating to IPv6 which I believe are bugs. I'd be interested to hear if these are misunderstandings on my part, or actual bugs.</p><p>The first is in the k8s-apiserver-proxy config file <code>/var/snap/k8s/common/args/conf.d/k8s-apiserver-proxy.json</code>. It looks like this, where the the last part is the port number 6443. The service does not start with a <em>\"failed to parse endpoint\"</em> error:</p><pre><code>{\"endpoints\":[\"dead:beef:1234::1:6443\"]} </code></pre><p>When correcting the address to use brackets, it will start up correctly.</p><pre><code>{\"endpoints\":[\"[dead:beef:1234::1]:6443\"]} </code></pre><p>Secondly, the snap.k8s.kubelet.service will not start, trying to bind to 0.0.0.0:10250 , but fails with <em>\"Failed to listen and serve\" err=\"listen tcp 0.0.0.0:10250: bind: address already in use\"</em>. Here I'm not sure where the address and port is coming from, but I'm guessing it's a default somewhere. Possibly <a href=\"https://github.com/kubernetes/kubernetes/issues/108248\">related to this report</a>.</p>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is this architecture possible without using haproxy but nginx(in rocky linux 9)?","url":"https://www.reddit.com/r/kubernetes/comments/1iupwhs/is_this_architecture_possible_without_using/","date":1740141445,"author":"/u/Keeper-Name_2271","guid":8575,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alerting from Prometheus and Grafana with kube-prometheus-stack","url":"https://www.reddit.com/r/kubernetes/comments/1iupvn8/alerting_from_prometheus_and_grafana_with/","date":1740141365,"author":"/u/HumanResult3379","guid":8917,"unread":true,"content":"<p>In Grafana page's , I find the built-in alert rules named .</p><p>I set Slack Contact points. But when the Alert Firing, it didn't send to Slack.</p><p>If I create a customized alert in Grafana, it can be sent to Slack. So does the alert-rules above only for seeing?</p><p>By the way, I find almost the same alert in Prometheus' AlertManager. I set a slack notification endpoint and the messages been sent there!</p><ol><li>Are the prometheus' alert-rules the same as  in Grafana Alert rules page like the picture above?</li><li>If want send alert from Grafana, does it only possible use new created alert rule manually in Grafana?</li></ol>","contentLength":589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Share your victories thread","url":"https://www.reddit.com/r/kubernetes/comments/1iuob4d/weekly_share_your_victories_thread/","date":1740135633,"author":"/u/gctaylor","guid":8500,"unread":true,"content":"<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 47 (external-secrets)","url":"https://www.youtube.com/watch?v=F1VRkXR1UG0","date":1740117732,"author":"CNCF [Cloud Native Computing Foundation]","guid":8399,"unread":true,"content":"<article>External-Secrets, a CNCF Sandbox project, is an open source project that simplifies the management and retrieval of secrets from external secret management systems (like AWS Secrets Manager, HashiCorp Vault, or Google Secret Manager). It bridges the gap between cloud native applications running in Kubernetes and secure external secret stores, ensuring secure and efficient secret management. \n\nJoin CNCF Ambassador, Edson Ferreira as he explores how external-secrets can be valuable for teams operating in hybrid or multi-cloud environments with stringent security needs.</article>","contentLength":573,"flags":null,"enclosureUrl":"https://www.youtube.com/v/F1VRkXR1UG0?version=3","enclosureMime":"","commentsUrl":null},{"title":"Docker Hub will only allow an unauthenticated 10/pulls per hour starting March 1st","url":"https://docs.docker.com/docker-hub/usage/","date":1740096401,"author":"/u/onedr0p","guid":7514,"unread":true,"content":"<blockquote><p>Starting April 1, 2025, all users with a Pro, Team, or Business\nsubscription will have unlimited Docker Hub pulls with fair use.\nUnauthenticated users and users with a free Personal account have the\nfollowing pull limits:</p><ul><li>Unauthenticated users: 10 pulls/hour</li><li>Authenticated users with a free account: 100 pulls/hour</li></ul></blockquote><p>The following table provides an overview of the included usage and limits for each\nuser type, subject to fair use:</p><div><table><thead><tr><th>Number of public repositories</th><th>Number of private repositories</th></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>10 per IPv4 address or IPv6 /64 subnet</td></tr></tbody></table></div><p>For more details, see the following:</p><p>When utilizing the Docker Platform, users should be aware that excessive data\ntransfer, pull rates, or data storage can lead to throttling, or additional\ncharges. To ensure fair resource usage and maintain service quality, we reserve\nthe right to impose restrictions or apply additional charges to accounts\nexhibiting excessive data and storage consumption.</p><p>Docker Hub has an abuse rate limit to protect the application and\ninfrastructure. This limit applies to all requests to Hub properties including\nweb pages, APIs, and image pulls. The limit is applied per IPv4 address or per\nIPv6 /64 subnet, and while the limit changes over time depending on load and\nother factors, it's in the order of thousands of requests per minute. The abuse\nlimit applies to all users equally regardless of account level.</p><p>You can differentiate between the pull rate limit and abuse rate limit by\nlooking at the error code. The abuse limit returns a simple  response. The pull limit returns a longer error message that includes\na link to documentation.</p>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iudj0d/docker_hub_will_only_allow_an_unauthenticated/"},{"title":"Using one ingress controller to proxy to another cluster","url":"https://www.reddit.com/r/kubernetes/comments/1iub1dp/using_one_ingress_controller_to_proxy_to_another/","date":1740089739,"author":"/u/djjudas21","guid":7472,"unread":true,"content":"<p>I'm planning a migration between two on-premise clusters. Both clusters are on the same network, with an ingress IP provided by MetalLB. The network is behind a NAT gateway with a single public IP, and port forwarding.</p><p>I need to start moving applications from cluster A to cluster B, but I can only set my port forwarding to point to cluster A  cluster B.</p><p>I'm trying to figure out if there's a way to use one cluster's ingress controller to proxy some sites to the other cluster's ingress controller. Something like SSL passthrough.</p><p>I've tried to configure the following on cluster B to proxy some specific site back to cluster A, with SSL passthrough as cluster A is running all its sites with TLS enabled. Unfortunately it isn't working properly and attempting to connect to <a href=\"http://app.example.com\">app.example.com</a> on cluster B only presents the default ingress controller self-signed cert, not the real app cert from cluster A.</p><pre><code>apiVersion: v1 kind: Service metadata: name: microk8s-proxy namespace: default spec: type: ExternalName externalName: ingress-a.example.com --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" name: microk8s-proxy namespace: default spec: ingressClassName: public rules: - host: app.example.com http: paths: - backend: service: name: microk8s-proxy port: number: 443 path: / pathType: Prefix </code></pre><p>I've been working on this for hours and can't get it working. Seems like it might be easier to just schedule a day of downtime for all sites! Thanks</p>","contentLength":1570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CustomResourceDefinitions to provision Azure resources such as storage blob","url":"https://www.reddit.com/r/kubernetes/comments/1iuay1o/customresourcedefinitions_to_provision_azure/","date":1740089501,"author":"/u/Valuable-Ad3229","guid":7471,"unread":true,"content":"<p>I am developer working with Azure Kubernetes Service, and I wonder if it is possible to define a CustomResourceDefinitions to provision other Azure resources such as Azure storage blobs, or Azure identities?</p><p>I am mindful that this may be anti-pattern but I am curious. Thank you!</p>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing cost, performance, and security in K8s with policy-as-code","url":"https://www.youtube.com/watch?v=O5YBwJO6FCw","date":1740089431,"author":"CNCF [Cloud Native Computing Foundation]","guid":7474,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/O5YBwJO6FCw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Cloud Native Live: Insights from the Radar","url":"https://www.youtube.com/watch?v=Sxnqk6EoB-s","date":1740088802,"author":"CNCF [Cloud Native Computing Foundation]","guid":7473,"unread":true,"content":"<article>Insights from the Radar: Exploring trends &amp; ecosystem gaps from the CNCF AI &amp; Multicluster radar report</article>","contentLength":103,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Sxnqk6EoB-s?version=3","enclosureMime":"","commentsUrl":null},{"title":"Learning Project - Deploy Flask App With MySQL on Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iu92sy/learning_project_deploy_flask_app_with_mysql_on/","date":1740084836,"author":"/u/kchandank","guid":8379,"unread":true,"content":"<p>If anyone has just started playing with Kubernetes, below project would help them to understand many key concepts around Kubernetes. I just deployed it yesterday and open for feedback on this.</p><p>In this Project , you are required to build a containerized application that consists of a Flask web application and a MySQL database. The two components will be deployed on a public cloud Kubernetes cluster in separate namespaces with proper configuration management using ConfigMaps and Secrets.</p><ul><li>Kubernetes Cluster (can be a local cluster like Minikube or a cloud-based one).</li><li>kubectl installed and configured to interact with your Kubernetes cluster.</li><li>Docker installed on your machine to build and push the Docker image of the Flask app.</li><li>Docker Hub account to push the Docker image.</li></ul><p>You will practically use the following key Kubernetes objects. It will help you understand how these objects can be used in real-world project implementations:</p><ul></ul><p>Create a <a href=\"http://app.py\">app.py</a> file with following content</p><pre><code>from flask import Flask, jsonify import os import mysql.connector from mysql.connector import Error app = Flask(__name__) def get_db_connection(): \"\"\" Establishes a connection to the MySQL database using environment variables. Expected environment variables: - MYSQL_HOST - MYSQL_DB - MYSQL_USER - MYSQL_PASSWORD \"\"\" host = os.environ.get(\"MYSQL_HOST\", \"localhost\") database = os.environ.get(\"MYSQL_DB\", \"flaskdb\") user = os.environ.get(\"MYSQL_USER\", \"flaskuser\") password = os.environ.get(\"MYSQL_PASSWORD\", \"flaskpass\") try: connection = mysql.connector.connect( host=host, database=database, user=user, password=password ) if connection.is_connected(): return connection except Error as e: app.logger.error(f\"Error connecting to MySQL: {e}\") return None u/app.route(\"/\") def index(): return f\"Welcome to the Flask App running in {os.environ.get('APP_ENV', 'development')} mode!\" u/app.route(\"/dbtest\") def db_test(): \"\"\" A simple endpoint to test the MySQL connection. Executes a query to get the current time from the database. \"\"\" connection = get_db_connection() if connection is None: return jsonify({\"error\": \"Failed to connect to MySQL database\"}), 500 try: cursor = connection.cursor() cursor.execute(\"SELECT NOW();\") current_time = cursor.fetchone() return jsonify({ \"message\": \"Successfully connected to MySQL!\", \"current_time\": current_time[0] }) except Error as e: return jsonify({\"error\": str(e)}), 500 finally: if connection and connection.is_connected(): cursor.close() connection.close() if __name__ == \"__main__\": debug_mode = os.environ.get(\"DEBUG\", \"false\").lower() == \"true\" app.run(host=\"0.0.0.0\", port=5000, debug=debug_mode) </code></pre><pre><code>FROM python:3.9-slim # Install ping (iputils-ping) for troubleshooting RUN apt-get update &amp;&amp; apt-get install -y iputils-ping &amp;&amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements.txt . RUN pip install --upgrade pip &amp;&amp; pip install --no-cache-dir -r requirements.txt COPY app.py . EXPOSE 5000 ENV FLASK_APP=app.py CMD [\"python\", \"app.py\"] </code></pre><pre><code>docker build -t becloudready/my-flask-app </code></pre><p>It will show a 6 digit Code, which you need to enter to following URL</p><p>Push the Image to DockerHub</p><pre><code>docker push becloudready/my-flask-app </code></pre><p>You should be able to see the Pushed Image</p><pre><code>apiVersion: apps/v1 kind: Deployment metadata: name: flask-deployment namespace: flask-app labels: app: flask spec: replicas: 2 selector: matchLabels: app: flask template: metadata: labels: app: flask spec: containers: - name: flask image: becloudready/my-flask-app:latest # Replace with your Docker Hub image name. ports: - containerPort: 5000 env: - name: APP_ENV valueFrom: configMapKeyRef: name: flask-config key: APP_ENV - name: DEBUG valueFrom: configMapKeyRef: name: flask-config key: DEBUG - name: MYSQL_DB valueFrom: configMapKeyRef: name: flask-config key: MYSQL_DB - name: MYSQL_HOST valueFrom: configMapKeyRef: name: flask-config key: MYSQL_HOST - name: MYSQL_USER valueFrom: secretKeyRef: name: db-credentials key: username - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: db-credentials key: password </code></pre><pre><code>apiVersion: v1 kind: Service metadata: name: flask-svc namespace: flask-app spec: selector: app: flask type: LoadBalancer ports: - port: 80 targetPort: 5000 </code></pre><pre><code>apiVersion: v1 kind: ConfigMap metadata: name: flask-config namespace: flask-app data: APP_ENV: production DEBUG: \"false\" MYSQL_DB: flaskdb MYSQL_HOST: mysql-svc.mysql.svc.cluster.local </code></pre><pre><code>apiVersion: v1 kind: Namespace metadata: name: flask-app --- apiVersion: v1 kind: Namespace metadata: name: mysql </code></pre><pre><code>kubectl create secret generic db-credentials \\ --namespace=flask-app \\ --from-literal=username=flaskuser \\ --from-literal=password=flaskpass \\ --from-literal=database=flaskdb </code></pre><pre><code>apiVersion: v1 kind: ConfigMap metadata: name: mysql-initdb namespace: mysql data: initdb.sql: | CREATE DATABASE IF NOT EXISTS flaskdb; CREATE USER 'flaskuser'@'%' IDENTIFIED BY 'flaskpass'; GRANT ALL PRIVILEGES ON flaskdb.* TO 'flaskuser'@'%'; FLUSH PRIVILEGES; </code></pre><pre><code>apiVersion: v1 kind: Service metadata: name: mysql-svc namespace: mysql spec: selector: app: mysql ports: - port: 3306 targetPort: 3306 </code></pre><pre><code>apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-statefulset namespace: mysql labels: app: mysql spec: serviceName: \"mysql-svc\" replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: initContainers: - name: init-clear-mysql-data image: busybox command: [\"sh\", \"-c\", \"rm -rf /var/lib/mysql/*\"] volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql containers: - name: mysql image: mysql:5.7 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: rootpassword # For production, use a Secret instead. - name: MYSQL_DATABASE value: flaskdb - name: MYSQL_USER value: flaskuser - name: MYSQL_PASSWORD value: flaskpass volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: initdb mountPath: /docker-entrypoint-initdb.d volumes: - name: initdb configMap: name: mysql-initdb volumeClaimTemplates: - metadata: name: mysql-persistent-storage spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi storageClassName: do-block-storage </code></pre><ul><li><p>kubectl apply -f namespaces.yaml</p></li><li><p>Deploy ConfigMaps and Secrets:</p><p>kubectl apply -f flask-config.yaml kubectl apply -f mysql-initdb.yaml kubectl apply -f db-credentials.yaml</p></li><li><p>kubectl apply -f mysql-svc.yaml kubectl apply -f mysql-statefulset.yaml</p></li><li><p>kubectl apply -f flask-deployment.yaml kubectl apply -f flask-svc.yaml</p></li></ul><p><code>kubectl get svc -n flask-app</code><code>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</code><code>flask-svc LoadBalancer 10.109.112.171 146.190.190.51 80:32618/TCP 2m53s</code></p><p>Unable to connect to MySQL from Flask App</p><p>Login to the Flask app pod to ensure all values are loaded properly</p><pre><code>kubectl exec -it flask-deployment-64c8955d64-hwz7m -n flask-app -- bash root@flask-deployment-64c8955d64-hwz7m:/app# env | grep -i mysql MYSQL_DB=flaskdb MYSQL_PASSWORD=flaskpass MYSQL_USER=flaskuser MYSQL_HOST=mysql-svc.mysql.svc.cluster.local </code></pre><ul><li>Flask App:Access the external IP provided by the LoadBalancer service to verify the app is running.</li><li>Database Connection:Use the /dbtest endpoint of the Flask app to confirm it connects to MySQL.</li><li>Troubleshooting:Use kubectl logs and kubectl exec to inspect pod logs and verify environment variables.</li></ul>","contentLength":7195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 51 (cert-manager)","url":"https://www.youtube.com/watch?v=UR64KulZDCM","date":1740081068,"author":"CNCF [Cloud Native Computing Foundation]","guid":7404,"unread":true,"content":"<article>Cert-manager is a recently graduated CNCF project designed to automate the management and issuance of TLS certificates. It simplifies the process of obtaining, renewing, and revoking certificates for applications running on Kubernetes.\n\nJoin CNCF Ambassador, Ronit Banerjee as he explores how cert-manager automates many aspects of managing TLS certificates, ensuring that they are always up to date, valid, and properly configured.</article>","contentLength":432,"flags":null,"enclosureUrl":"https://www.youtube.com/v/UR64KulZDCM?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 51 (WasmEdge)","url":"https://www.youtube.com/watch?v=Cxz7pC9Lq2k","date":1740080252,"author":"CNCF [Cloud Native Computing Foundation]","guid":7403,"unread":true,"content":"<article>WasmEdge is a CNCF Sandbox project that is a high-performance WebAssembly runtime optimized for cloud-native and edge computing use cases. It‚Äôs designed to run WebAssembly (Wasm) applications at the edge and in the cloud, offering a fast, efficient, and secure way to execute containerized applications.\n\nUnlike traditional container runtimes, WasmEdge is designed to work efficiently with cloud-native ecosystems. Join CNCF Ambassador, Faeka Ansari as she explores how WasmEdge allows developers to run Wasm modules with low overhead.</article>","contentLength":537,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Cxz7pC9Lq2k?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 50 (Tekton Pipelines)","url":"https://www.youtube.com/watch?v=vHnI_hty9zc","date":1740079094,"author":"CNCF [Cloud Native Computing Foundation]","guid":7402,"unread":true,"content":"<article>Tekton Pipelines is a project that focuses on providing CI/CD (Continuous Integration/Continuous Delivery) systems. Tekton is designed to facilitate the automation of the software development lifecycle, from code commit to deployment, and is deeply integrated into Kubernetes.\n\nJoin CNCF Ambassador, Chamod Perera as he explores this robust and flexible CI/CD solution suitable for teams using Kubernetes who want a Kubernetes-native approach to CI/CD.</article>","contentLength":452,"flags":null,"enclosureUrl":"https://www.youtube.com/v/vHnI_hty9zc?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 49 (Linkerd)","url":"https://www.youtube.com/watch?v=WltDqvMzZIw","date":1740078440,"author":"CNCF [Cloud Native Computing Foundation]","guid":7401,"unread":true,"content":"<article>Linkerd is a CNCF graduated project as of July 2021. This ultra light, simple, and powerful cloud native project adds security, observability, and reliability to Kubernetes, without the complexity.\n\nLinkerd is an advanced service mesh written in Rust, and has been in production for over 8 years with over 500 contributors. Join CNCF Ambassador ChengHao Yang as he explores Linkerd for the first time with the hopes of learning about its benefits, and why companies like Adidas and Xbox chose it for their tech stack.</article>","contentLength":517,"flags":null,"enclosureUrl":"https://www.youtube.com/v/WltDqvMzZIw?version=3","enclosureMime":"","commentsUrl":null},{"title":"EKS vs. GKE differences in Services and Ingresses for their respective NLBs and ALBs","url":"https://www.reddit.com/r/kubernetes/comments/1itx2uh/eks_vs_gke_differences_in_services_and_ingresses/","date":1740053554,"author":"/u/jumiker","guid":7289,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1itvy0m/weekly_this_week_i_learned_twil_thread/","date":1740049229,"author":"/u/gctaylor","guid":7143,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EKS Auto Mode a.k.a managed Karpenter.","url":"https://www.reddit.com/r/kubernetes/comments/1itumdr/eks_auto_mode_aka_managed_karpenter/","date":1740043614,"author":"/u/lynxerious","guid":7360,"unread":true,"content":"<p>It's relatively new, has anyone tried it before? Someone just told me about it recently.</p><p><a href=\"https://aws.amazon.com/eks/pricing/\">https://aws.amazon.com/eks/pricing/</a> The pricing is a bit strange, it adds up cost to EC2 pricing instead of Karpenter pods. And there are many type of instance I can't search for in that list.</p>","contentLength":280,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Tools for Kubernetes: What Have I Missed?","url":"https://www.reddit.com/r/kubernetes/comments/1ittpj1/ai_tools_for_kubernetes_what_have_i_missed/","date":1740039627,"author":"/u/Electronic_Role_5981","guid":7051,"unread":true,"content":"<p><strong>karpor (kusionstack subproject)</strong></p><p>Intelligence for Kubernetes. World's most promising Kubernetes Visualization Tool for Developer and Platform Engineering teams</p><p><strong>kube-copilot (personal project from Azure)</strong></p><ul><li>Automate Kubernetes cluster operations using ChatGPT (GPT-4 or GPT-3.5).</li><li>Diagnose and analyze potential issues for Kubernetes workloads.</li><li>Generate Kubernetes manifests based on provided prompt instructions.</li><li>Utilize native  and  commands for Kubernetes cluster access and security vulnerability scanning.</li><li>Access the web and perform Google searches without leaving the terminal.</li></ul><p><strong>some cost related `observibility and analysis`</strong></p><p>I did not check if all below projects focus on k8s.</p><p>Are there any ai-for-k8s projects that I miss?</p>","contentLength":713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Perform Cleanup Tasks When a Pod Crashes (Including OOM Errors)?","url":"https://www.reddit.com/r/kubernetes/comments/1itt3ja/how_to_perform_cleanup_tasks_when_a_pod_crashes/","date":1740037029,"author":"/u/SamaDinesh","guid":7099,"unread":true,"content":"<p>I have a requirement where I need to delete a specific file in a shared volume whenever a pod goes down.</p><p>I initially tried using the  lifecycle hook, and it works fine when the pod is deleted normally (e.g., via ). However, the problem is that  does not trigger when the pod crashes unexpectedly, such as due to an OOM error or a node failure. </p><p>I am looking for a reliable way to ensure that the file is deleted even when the pod crashes unexpectedly. Has anyone faced a similar issue or found a workaround?</p><pre><code>lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"rm -f /data/your-file.txt\"] </code></pre>","contentLength":587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing Kyverno policy enforcement performance for large clusters","url":"https://www.youtube.com/watch?v=DWmCAUCs3bc","date":1740031543,"author":"CNCF [Cloud Native Computing Foundation]","guid":6980,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DWmCAUCs3bc?version=3","enclosureMime":"","commentsUrl":null},{"title":"Cluster restoration","url":"https://www.reddit.com/r/kubernetes/comments/1itq9c8/cluster_restoration/","date":1740026321,"author":"/u/Upper-Aardvark-6684","guid":6959,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Upper-Aardvark-6684\"> /u/Upper-Aardvark-6684 </a>","contentLength":42,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to run VM using kubevirt in kind cluster in MacOS (M2)?","url":"https://www.reddit.com/r/kubernetes/comments/1itpzch/how_to_run_vm_using_kubevirt_in_kind_cluster_in/","date":1740025387,"author":"/u/Wooden_Departure1285","guid":6958,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Wooden_Departure1285\"> /u/Wooden_Departure1285 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"how advancements like Dynamic Resource Allocation (DRA) and the Container Device Interface (CDI) are shaping Kubernetes for AI workloads","url":"https://furiosa.ai/blog/the-next-chapter-of-kubernetes-enabling-ml-inference-at-scale","date":1740023767,"author":"/u/woowookim","guid":6206,"unread":true,"content":"<p dir=\"ltr\">CDI is designed to solve two problems. First, all device vendors expose their devices to containers in different ways. Second, device support in container runtimes is fragmented. </p><p dir=\"ltr\">This imposes high costs on both AI chip vendors and the open-source community. Yes, these circumstances clearly illustrate why a standard interface is necessary. </p><p dir=\"ltr\">Now AI chip vendors can support various container runtimes through standardized interfaces by implementing CDI. Docker also supports it as an experimental feature, but it is expected to become a default feature in the near future since containerd now supports it as a default feature. Originally, CDI was part of the DRA. However, it now has a greater influence on the entire container ecosystem.</p><p dir=\"ltr\">In addition to standardized device exposure, CDI delivers several benefits for enterprise deployments using different kinds of AI hardware (such as RNGD):</p><ul><li dir=\"ltr\"><p dir=\"ltr\">Simplified runtime support</p></li><li dir=\"ltr\"><p dir=\"ltr\">Reduced implementation costs</p></li></ul><p dir=\"ltr\">Furiosa has built RNGD to be the best accelerator for real world deployments using large language models, multimodal models and agentic AI systems. To achieve this, we will leverage new Kubernetes functionality like CDI and DRA.  RNGD uses the CDI to define how devices are assigned to containers. </p><p dir=\"ltr\">The CDI specification lays out the format, guidelines, and interfaces needed to properly describe these devices. For example, RNGD‚Äôs vendor-specific interfaces like device nodes and sysfs files can be expressed using the interfaces provided by the CDI. This makes it simpler for container-related systems to understand which system resources need to be present when running workloads on RNGD hardware. </p><p dir=\"ltr\">We‚Äôre currently building a DRA plugin for flexible and efficient resource scheduling. When we release the plugin in 2025, users will be able to request RNGD resources defined through the DRA interface. </p><p dir=\"ltr\">One key benefit of DRA is that it allows users to specify their desired hardware topology. For example, a user might say they need four RNGD devices under a single physical CPU socket, or two RNGD devices under a specific PCIe switch. They can even specify that multiple servers must be located beneath a particular network switch. </p><p>When the scheduler hands off the RNGD scheduling task to the Furiosa DRA plugin, the plugin calculates which server meets the user‚Äôs topology requirements. Among the servers that match these conditions, it then assigns the requested RNGD resources to the user‚Äôs container (reality is binding the user's pod on that server).</p><h2 dir=\"ltr\">Future outlook for Kubernetes and ML workloads</h2><p dir=\"ltr\">The evolution of Kubernetes, driven by advancements like DRA and CDI, is crucial for the future of AI. As the industry moves beyond traditional GPUs and embraces more efficient chip architectures like RNGD, the ability to effectively orchestrate and manage these resources will become even more critical.</p><p dir=\"ltr\">This will accelerate the adoption of specialized hardware, leading to faster, more efficient, and more cost-effective ML inference.</p><p dir=\"ltr\">The ongoing collaboration between the Kubernetes community and AI chip vendors is essential to ensure that Kubernetes continues to meet the evolving needs of the AI landscape. By working together, we can unlock the full potential of AI and drive innovation across industries. We believe that sharing our experience with developing and deploying RNGD will contribute to this important effort. We also hope to foster more open-source contributions that will speed progress toward an open ecosystem that supports a wide range of AI-specific hardware to serve different needs in the industry.</p>","contentLength":3579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1itphl0/how_advancements_like_dynamic_resource_allocation/"},{"title":"Dapr.io - enterprise enabler for boosting developer productivity","url":"https://www.youtube.com/watch?v=XoNcJYoJkRY","date":1740001867,"author":"CNCF [Cloud Native Computing Foundation]","guid":6062,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/XoNcJYoJkRY?version=3","enclosureMime":"","commentsUrl":null},{"title":"Master Node Migration","url":"https://www.reddit.com/r/kubernetes/comments/1itfclm/master_node_migration/","date":1739996081,"author":"/u/BrockWeekley","guid":6084,"unread":true,"content":"<p>Hello all, I've been running a k3s cluster for my home lab for several months now. My master node hardware has begun failing - it is always maxed out on CPU and is having all kinds of random failures. My question is, would it be easier to simply recreate a new cluster and apply all of my deployments there, or should mirroring the disk of the master to new hardware be fairly painless for the switch over?</p><p>I'd like to add HA with multiple master nodes to prevent this in the future, which is why I'm leaning towards just making a new cluster, as switching from an embedded sqlite DB to a shared database seems like a pain. </p>","contentLength":623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubemgr: Open-Source Kubernetes Config Merger","url":"https://www.reddit.com/r/kubernetes/comments/1ite641/kubemgr_opensource_kubernetes_config_merger/","date":1739993255,"author":"/u/RAPlDEMENT","guid":6139,"unread":true,"content":"<p>I'm excited to share a personal project I've been working on recently. My classmates and I found it tedious to manually change environment variables or modify Kubernetes configurations by hand. Merging configurations can be straightforward but often feels cumbersome and annoying.</p><p>To address this, I created Kubemgr, a Rust crate that abstracts a command for merging Kubernetes configurations:</p><pre><code>KUBECONFIG=config1:config2... kubectl config view --flatten </code></pre><p>Available on <a href=\"http://crates.io\">crates.io</a>, this CLI makes the process less painful and more intuitive.</p><p>But that's not all! For those who prefer not to install the crate locally, I also developed a user interface using Next.js and WebAssembly (WASM). The goal was to ensure that both the interface and the CLI use the exact same logic while keeping everything client-side for security reasons.</p><p>I understand that this project might not be useful for everyone, especially those who are already experienced with Kubernetes. However, it was primarily a learning exercise for me to explore new technologies and improve my skills. I'm eager to get feedback and hear any ideas for new features or improvements that could make Kubemgr more useful for the community.</p><p>The project is open-source, so feel free to check out the code and provide recommendations or suggestions for improvement on GitHub. Contributions are welcome!</p><p>If you like the project, please consider starring the GitHub repo!</p>","contentLength":1412,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNCF Research End User Group: Managing Kubeflow deployments and updates at CERN (February 19, 2025)","url":"https://www.youtube.com/watch?v=QvNIS0M0VJE","date":1739988531,"author":"CNCF [Cloud Native Computing Foundation]","guid":5993,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/QvNIS0M0VJE?version=3","enclosureMime":"","commentsUrl":null},{"title":"TOC Meeting 2025-02-18","url":"https://www.youtube.com/watch?v=deyssJesSII","date":1739984032,"author":"CNCF [Cloud Native Computing Foundation]","guid":5910,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/deyssJesSII?version=3","enclosureMime":"","commentsUrl":null},{"title":"C.K.A Exam Change - Did Anyone Take the Exam with the New Syllabus Effective 18th?","url":"https://www.reddit.com/r/kubernetes/comments/1it6rzx/cka_exam_change_did_anyone_take_the_exam_with_the/","date":1739975397,"author":"/u/fighting_cockroach","guid":5851,"unread":true,"content":"<p>I'm curious if anyone has taken the Certified Kubernetes Administrator exam with the new syllabus that became effective on February 18th. If so, how does it compare to the old exam? </p><p>Any insights on the changes and how they impacted your experience would be greatly appreciated!</p>","contentLength":277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Share your EXPLOSIONS thread","url":"https://www.reddit.com/r/kubernetes/comments/1it2vyy/weekly_share_your_explosions_thread/","date":1739962833,"author":"/u/gctaylor","guid":6138,"unread":true,"content":"<div><p>Did anything explode this week (or recently)? Share the details for our mutual betterment.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a>","contentLength":121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is future technology for Sr, Devops Engineer from now-2025,","url":"https://www.reddit.com/r/kubernetes/comments/1it2tme/what_is_future_technology_for_sr_devops_engineer/","date":1739962581,"author":"/u/sabir8992","guid":5734,"unread":true,"content":"<div><p>Can you list out the technology and certification that will be alteast in trend for next 5 to 8 years.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/sabir8992\"> /u/sabir8992 </a>","contentLength":134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenTelemetry resource attributes: Best practices for Kubernetes","url":"https://www.dash0.com/blog/opentelemetry-resource-attributes-best-practices-for-kubernetes","date":1739958555,"author":"/u/Aciddit","guid":5733,"unread":true,"content":"<p><a target=\"_self\" href=\"https://www.dash0.com/faq/what-are-opentelemetry-resources\"> in OpenTelemetry</a> are used to document which systems the telemetry is describing, and they are often the difference between telemetry from which you can gain insights from, and ‚Äújust data‚Äù.</p><p>When instrumenting services with OpenTelemetry, adhering to semantic conventions ensures consistent, accurate, and meaningful telemetry data across systems.</p><p>A variety of attributes allow you to describe which workload on your Kubernetes cluster is emitting which telemetry. The <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/resource/k8s/\">Kubernetes resource semantic conventions</a> specify, among others, pairs of  and  attributes for pods ( and ), as well for all workloads ‚Äúhigher level‚Äù constructs (which in Kubernetes are also called ‚Äúresources‚Äù, but we don‚Äôt want to confuse the two terms), like deployment ( and ), daemonset ( and ) and so on.</p><p>Nevertheless, despite the well-defined attributes, it is not entirely straightforward to have all the right Kubernetes-related metadata attached to your telemetry.</p><p>Among all the resource attributes that describe telemetry coming from your workloads on Kubernetes,  is by far the most important: through it, you can add most other pieces of k8s-related metadata by funneling your telemetry through the <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md\"><code></code> component</a> of an OpenTelemetry collector running inside the same cluster. The <code></code> ‚Äúfills‚Äù the blanks using data it gets from Kubernetes API, and with the right tool, you can effortlessly filter and group your telemetry across all types of aggregations.</p><p>The fact that the <code></code> exists actually fills a very important gap in telemetry metadata: the OpenTelemetry SDKs running within your containers have access to virtually no metadata about the pod surrounding them is running. They are not going to know, unless you specifically add it to the pod, e.g., using environment variables, which daemonset scheduled the pod, or which replicaset of which deployment.</p><p>As a matter of fact, without additional assistance by you, for example via environment variables (either setting values yourself, or adding pod uid, pod name, and namespace name to the environment via Kubernetes‚Äô <a target=\"_blank\" href=\"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/\">Downward API</a>), an OpenTelemetry SDK within a container can pretty much know only the pod uid (though very esoteric magic involving the parsing of <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cgroups\">cgroup</a> metadata, see e.g. this <a target=\"_blank\" href=\"https://github.com/dash0hq/opentelemetry-js-distribution/blob/main/src/detectors/node/opentelemetry-resource-detector-kubernetes-pod/index.ts\">resource detector</a> in the Dash0 distribution of OpenTelemetry for Node.js) and the pod name (via the networking hostname). And unfortunately, as of writing, OpenTelemetry SDKs do not even implement those consistently (see, e.g., <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-python-contrib/issues/1474\">this issue</a>).</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>By the way, the <code></code> is capable of identifying which pod is sending telemetry to it via the unique pod ip (see the <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#configuration\">‚Äúassociations‚Äù configurations</a>): the processor in the OpenTelemetry Collector matches the IP address of the ‚Äúother side‚Äù of the TCP connection that sends the telemetry, and since each pod in a Kubernetes cluster has a unique IP address assigned to it, it can figure out from which pod it‚Äôs coming from. </p><p>And here, there is a caveat: the detection of the pod based on the IP address is known not to work reliably if you use a service mesh or some of the less conventional network setups. (Don‚Äôt ask us how we know. It was not fun to troubleshoot that.) So, better safe than sorry: ensure your telemetry is annotated with  and, if you are OK with proxying your telemetry through an OpenTelemetry Collector inside the cluster, have that fill most of the resource attributes mentioned in the remainder of this article for you using a <a target=\"_blank\" href=\"https://www.otelbin.io/s/249772c6c88ab31e77c168af6131df0248902bbf\">configuration like this</a>.</p><p>Pod UIDs are effectively unique across all your workloads. (And all Kubernetes workloads anywhere, ever.) But long, random strings of characters are not something we humans are good at remembering things by, or even searching for in lists.</p><p>So, in a similar way same way we set the  resource attribute, we can set the :</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>Did you know that the Kubernetes pod, i.e., one or more containers that share local networking and other resources like CPU and memory allotment and volumes, is called like that because:</p><ol><li>‚Äúpod‚Äù is the collective name of a group of whales</li><li>the logo of Docker, the original container runtime of Kubernetes, is a whale</li></ol><p>If your pod has more than one container, you are really going to need to know which of them is having issues. Usually, applications on Kubernetes have just one ‚Äúmain‚Äù container, running your application and may have one or more ‚Äúsidecars‚Äù, i.e., containers that have inside ancillary processes like log collectors or service mesh proxies. The <code></code> (see previous section) cannot tell containers apart (all your containers in the same pod share the same IP address and pod uid, among other things), so you should set the  yourself. It can be a bit toilsome, because you cannot do it generically using the Downward API the way we did for , but in the end is as simple as adding another entry to <code></code>:</p><p><em>A Kubernetes pod spec template snippet showing how to use the <code></code> environment variable to set the  resource attribute. Be sure to set it to the same value of the container name!</em></p><p>Note that  could be redundant: the  attribute is supposed to contain the same data, and there are detectors in most OpenTelemetry SDKs (see e.g. for <a target=\"_blank\" href=\"http://node.js/\">Node.js</a>), that can collect  for you by parsing, for example, the  metadata (<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cgroups\">cgroups</a> are one of the foundational Linux facilities for containers). However, it may not always be possible to add detectors to your containerized applications, especially when you are using sidecars from 3rd parties. But if they support the <code></code> environment variable, you can fill the gaps in resource attributes through the process environment.</p><p>The same way you are likely to have a  service in most applications (<code></code>) service, that service will likely be powered by a  deployment (<code></code>). Names in Kubernetes are unique only for resources of the same type (e.g., deployments) within the same Kubernetes namespace. If you want to avoid confusion and simplify your life aggregating data, set not only  but also , and use the UIDs to group.</p><p>After reading the previous paragraph, you might have wondered:</p><p>‚ÄúThe deployment name is unique inside the namespace, and the namespace name is unique inside the cluster. So why do I need unique identifiers anyhow?‚Äù</p><p>That logic works if you deploy your software in only one cluster. But that is seldom the case. Between development, testing, and various production clusters (which is a rather common type of layout), most organizations run the same software, at the same time, on  of Kubernetes clusters. And, to make matters more complicated, telling Kubernetes clusters apart is harder than it should (see next section).</p><p>The fun with identifiers is not over yet. Now let‚Äôs talk about some identifiers that . Specifically, the identifier of a specific Kubernetes cluster. In most Kubernetes setups, it literally does not exist. In other words, a<em> Kubernetes cluster has no own notion of identity</em>!</p><p>You surely have names for your clusters, but the name ‚Äúprod-eu-awesomesauce‚Äù that you define in your Kubernetes cluster management tool is more a matter of how you call the profile in  to connect to that cluster rather than some metadata you can find from within the cluster itself! (Your mileage may vary with specific setups and Kubernetes flavor; but in general, this is the case.) So, you should use the OpenTelemetry collector‚Äôs <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor\"></a> to inject the value of  like this:</p><p><em>A snippet of the OpenTelemetry collector configuration to add the  resource attribute to all telemetry coming through.</em></p><p>Moreover, the same way most Kubernetes  attributes have matching  ones, so does  have a match in . The common practice in this case is to set  using as value the uid of the  namespace, which is pretty much the only namespace you are guaranteed to find in every Kubernetes cluster you will ever see (as it is the one that by default hosts the control plane components).</p><p>The  attribute, and its far lesser used  sibling, are really useful when investigating performance issues due to resource underprovisioning (pods on the same node fighting for resources like CPU or memory) or <a target=\"_blank\" href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/\">node-pressure evictions</a>. The only notable exception to this is when you run on AWS EKS on Fargate, where each pod, from the point of view of Kubernetes, runs on a dedicated node.</p><p>If you have already set up the <code></code> so that it can add resource attributes to your telemetry, it can also take care of  for you. Otherwise, setting the  attribute is pretty simple using Kubernetes‚Äô Downward API and ‚Äú<a target=\"_blank\" href=\"https://kubernetes.io/docs/tasks/inject-data-application/define-interdependent-environment-variables/\">dependent environment variables</a>‚Äù:</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>Resource attributes are a key ingredient to make your telemetry useful. In this blog post, we have looked at the OpenTelemetry resource semantic conventions for Kubernetes, and how you can ensure to always know which workload of which cluster is sending the errors.</p><p>In this post, we kept to the fundamental Kubernetes metadata your telemetry should have. There is, of course, a lot more to be said about OpenTelemetry semantic conventions, even just for resources on Kubernetes. For example, you will find more resource attributes specified in the <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/resource/k8s/\">Kubernetes resource semantic conventions</a>. Also, there are semantic conventions for <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/system/k8s-metrics/\">metrics about Kubernetes</a>, which are, at the time of writing, in an experimental state (meaning: the convention is not declared stable, so it might still change in backwards-incompatible ways) and are based on what the OpenTelemetry Collector knows how to collect with various receivers like <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8sclusterreceiver\"></a> and <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kubeletstatsreceiver\"></a>.</p><p>By the way, if you liked this article, you are probably going to love Ben's \"<a target=\"_self\" href=\"https://www.dash0.com/blog/top-10-opentelemetry-collector-components\">Top 10 OpenTelemetry Collector Components</a>\" blog post. It will likely give you a bunch more ideas about what you can do to ensure that your resource metadata is top-notch.</p><p>And if you want all the awesomeness of the resource metadata we covered in this post, but do none of the work to get them, give the <a target=\"_self\" href=\"https://www.dash0.com/documentation/dash0/dash0-kubernetes-operator\">Dash0 operator</a> a spin: it is open source, built on OpenTelemetry components with opinionation and an appliance-like philosophy (‚Äúit just works‚Ñ¢‚Äù), and under the hood it uses most of the techniques described in this post to get your telemetry annotated to the state of the art, with literally none of the toil from on your side.</p>","contentLength":10476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1it1u9k/opentelemetry_resource_attributes_best_practices/"},{"title":"Has anyone used Nginx Ingress controller with the AWS Load Balancer Controller service instead of the default service?","url":"https://www.reddit.com/r/kubernetes/comments/1it16ic/has_anyone_used_nginx_ingress_controller_with_the/","date":1739955716,"author":"/u/lynxerious","guid":7018,"unread":true,"content":"<p>So the nginx-ingress-controller creates a LoadBalancer service by default, this load balancer is created by the in-tree controller managed by EKS. And I want to manage the load balancer with the AWS Load Balancer Controller instead, using a custom service, it has more features than the default LoadBalancer service.</p><p>After I had successfully created the new load balancer, route the service to the nginx-ingress-controller pods, the target groups pods IPs are all correct, and change all domains DNS records to the new load balancer DNS name, change the publishService in the nginx pods to the new service. It was sure this has worked properly.</p><p>Then I tried to disable the default service of the nginx-ingress-controller, voila, everything went down, and I had to re-enable it quickly, after I checked the Monitoring sections of the load balancers, the old ones still got the traffic, while the old ones barely got any. This just doesn't make sense to me. I ping all domains and it goes to the correct IP of the new load balancer, yet the old one still got traffics and I don't even know why, could it be DNS records cache? But I don't think it would be cached for that long since it's been 2 days already.</p><p>Edit: I found out something really weird: dig <a href=\"http://domain.com\">domain.com</a> -&gt; new load balancer IP dig <a href=\"https://domain.com\">https://domain.com</a> -&gt; old load balancer IP I'm investigating why here.</p>","contentLength":1359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Khronoscope Pre-Alpha ‚Äì A New Way to Explore Your Kubernetes Cluster Over Time","url":"https://www.reddit.com/r/kubernetes/comments/1isz5gq/introducing_khronoscope_prealpha_a_new_way_to/","date":1739947088,"author":"/u/HoyleHoyle","guid":5616,"unread":true,"content":"<p>I'm excited to share , a  tool designed to give you a  view of your Kubernetes cluster. Inspired by k9s, it lets you pause, rewind, and fast-forward through historical states, making it easier to debug issues, analyze performance, and understand how your cluster evolves.</p><ul><li>Connects to your Kubernetes cluster and tracks resource states over time</li><li>Provides a  to navigate past events</li><li>Lets you <strong>filter, inspect, and interact</strong> with resources dynamically</li><li>Supports <strong>log collection and playback</strong> for deeper analysis</li></ul><p>üìñ <strong>Debugging the Past with Khronoscope</strong></p><p>Imagine inspecting your Kubernetes cluster when you notice something strange‚Äîa deployment with flapping pods. They start, crash, restart. Something‚Äôs off.</p><p>You pause the cluster state and check related resources. Nothing obvious. Rewinding a few minutes, you see the pods failing right after startup. Fast-forwarding, you mark one to start collecting logs. More crashes. Rewinding again, you inspect the logs just before failure‚Äîeach pod dies trying to connect to a missing service.</p><p>Jumping to another namespace, you spot the issue: a critical infrastructure pod failed to start earlier. A quick fix, a restart, and everything stabilizes.</p><p>With Khronoscope‚Äôs ability to navigate through time, track key logs, and inspect past states, you solve in minutes what could‚Äôve taken hours.</p><p>This is an , and I‚Äôm looking for  from anyone willing to try it out. I‚Äôd love to hear what works, what doesn‚Äôt, and how it could be improved.</p><pre><code>brew tap hoyle1974/homebrew-tap brew install khronoscope </code></pre><pre><code>git clone https://github.com/hoyle1974/khronoscope.git cd khronoscope go run cmd/khronoscope/main.go </code></pre>","contentLength":1630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KubeVPN: Revolutionizing Kubernetes Local Development","url":"https://www.reddit.com/r/kubernetes/comments/1isxgo2/kubevpn_revolutionizing_kubernetes_local/","date":1739940995,"author":"/u/HamsterTall8168","guid":5599,"unread":true,"content":"<p>In the Kubernetes era, developers face a critical conflict between  and <strong>local development agility</strong>. Traditional workflows force developers to:</p><ol><li>Suffer frequent / operations</li><li>Set up mini Kubernetes clusters locally (e.g., minikube)</li><li>Risk disrupting shared dev environments</li></ol><p>KubeVPN solves this through <strong>cloud-native network tunneling</strong>, seamlessly extending Kubernetes cluster networks to local machines with three breakthroughs:</p><ul><li>üöÄ : Access cluster services without code changes</li><li>üíª <strong>Real-Environment Debugging</strong>: Debug cloud services in local IDEs</li><li>üîÑ <strong>Bidirectional Traffic Control</strong>: Route specific traffic to local or cloud</li></ul><h3>1. Direct Cluster Networking</h3><ul><li>‚úÖ Service name access (e.g., )</li><li>‚úÖ Native Kubernetes DNS resolution</li></ul><p><code>shell ‚ûú curl productpage:9080 # Direct cluster access &lt;!DOCTYPE html&gt; &lt;html&gt;...&lt;/html&gt; </code></p><h3>2. Smart Traffic Interception</h3><p>Precision routing via header conditions:</p><p><code>bash kubevpn proxy deployment/productpage --headers user=dev-team </code></p><ul><li>Requests with  ‚Üí Local service</li><li>Others ‚Üí Original cluster handling</li></ul><p>Connect two clusters simultaneously:</p><p><code>bash kubevpn connect -n dev --kubeconfig ~/.kube/cluster1 # Primary kubevpn connect -n prod --kubeconfig ~/.kube/cluster2 --lite # Secondary </code></p><h3>4. Local Containerized Dev</h3><p>Clone cloud pods to local Docker:</p><p><code>bash kubevpn dev deployment/authors --entrypoint sh </code></p><p>Launched containers feature:</p><ul><li>üåê Identical network namespace</li><li>‚öôÔ∏è Matching environment variables</li></ul><p>KubeVPN's three-layer architecture:</p><table><thead><tr></tr></thead><tbody><tr><td>Cluster-side interception</td><td>MutatingWebhook + iptables</td></tr><tr><td>Secure local-cluster channel</td></tr><tr></tr></tbody></table><p><code>mermaid graph TD Local[Local Machine] --&gt;|Encrypted Tunnel| Tunnel[VPN Gateway] Tunnel --&gt;|Service Discovery| K8sAPI[Kubernetes API] Tunnel --&gt;|Traffic Proxy| Pod[Workload Pods] subgraph K8s Cluster K8sAPI --&gt; TrafficManager[Traffic Manager] TrafficManager --&gt; Pod end </code></p><p>100QPS load test results:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>KubeVPN outperforms alternatives in overhead control.</p><p>kubectl krew install kubevpn/kubevpn ```</p><p><code>bash kubevpn connect --namespace dev </code></p><p>kubevpn proxy deployment/frontend --headers x-debug=true ```</p><p><code>bash curl -H \"x-debug: true\" frontend.dev.svc/cluster-api </code></p><p>KubeVPN's growing toolkit:</p><ul><li>üîå : Visual traffic management</li><li>üß© : Automated testing/deployment</li><li>üìä : Real-time network metrics</li></ul><p>Join developer community:</p><p>With KubeVPN, developers finally enjoy cloud-native debugging while sipping coffee ‚òïÔ∏èüöÄ</p>","contentLength":2282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best approach to manifests/infra?","url":"https://www.reddit.com/r/kubernetes/comments/1isql4s/best_approach_to_manifestsinfra/","date":1739921216,"author":"/u/RespectNo9085","guid":4474,"unread":true,"content":"<p>I've been provisioning various Kube clusters throughout the years, and now I\"m about to start a new project. </p><p>To me the best practice is to have a repo for the infrastructure using Terraform/Open Tofu, in this repo I usually set conditionals to provision either a Minikube for local or an EKS for prod, </p><p>Then I would create another repo to put together all cross-cutting concerns as a helm chart. That means I will use Grafana, Tempo, Vault Helm Charts and then I will package them in to one 'shared infrastructure' helm chart which is then applied to the clusters. </p><p>Each microservice will have its own helm chart that is generated on push to master and serverd on GIthub packages, there is also a dev manifest where people update the chart version for their microservice. The dev manifest has all they need to run the cluster, all the services.</p><p>The problem here is that sometimes I want to add a new technology to the cluster, for example recently I wanted to add the API gateway, Vault, Cillium or some other time I wanted to add a Mattermost instance, and some of these don't have proper helm charts. </p><p>Most of their instructions are simple cases where you apply a manifest from a URL into the cluster and that's no way to provision a cluster, because if I want to change things in the future, then should I apply again with a new values.yaml ? not fun, I like to see, understand and control what is going into my cluster. </p><p>So the question is, is the only option to read those manifest and create my own Helm charts? should I even Helm? is there a better approach? any opinion is appreciated. </p>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Configuration Languages (config DSLs) solve configuration complexity?","url":"https://www.reddit.com/r/kubernetes/comments/1isph9t/can_configuration_languages_config_dsls_solve/","date":1739918543,"author":"/u/wineandcode","guid":4448,"unread":true,"content":"<p>Configuration languages are not the best solution to configuration complexity. Each language has its pros and cons, but none moves the needle much. In this post, Brian Grant explores what they are. Why would someone create a new one? And do they reduce configuration complexity?</p>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to stop SSL-Certs from being deleted when uninstalling a helm deployment","url":"https://www.reddit.com/r/kubernetes/comments/1ismivb/how_to_stop_sslcerts_from_being_deleted_when/","date":1739909981,"author":"/u/Eldiabolo18","guid":4432,"unread":true,"content":"<p>when trying a helm chart I often have to reinstall it a couple of times until it works the way I want it. If that Helm-Chart has an ingress and generates a SSL-Cert from Letsencrypt via Cert-Manager, the cert also gets deleted and regenerated. </p><p>I just ran into the issue, that I redployed the helm chart more than 5 times in 24 (48?) hrs for the same domain, so letsencrypt blocks the request. </p><p>Is there any way to stop the SSL-Certs from being deleted when in uninstall a helm chart, so it can be reused for the next deployment? Or is there any other way around this? </p>","contentLength":567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GPU nodes on-premise","url":"https://www.reddit.com/r/kubernetes/comments/1isk9ne/gpu_nodes_onpremise/","date":1739904616,"author":"/u/blu-base","guid":4342,"unread":true,"content":"<p>My company acquired a few GPU nodes with a couple of nvidia h100 cards each. The app team is likely wanting to use nvidias Trition interference server. For this purpose we need to operate kubernetes on those nodes. I am now wondering whether to maintain native kubernetes on these nodes. Or to use some suite, such as open shift or rancher. Running natively means a lot of work on reinventing the wheel, having an operation documentation/ process. However, using suites could mean an overhead of complexity relative to the few number of local nodes.</p><p>I am not experienced with doing the admin side of operating an on-premise kubernetes. Have you any recommendations how to run such GPU focused clusters?</p>","contentLength":701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Update API Server SANs and sync issue","url":"https://www.reddit.com/r/kubernetes/comments/1ishh0h/update_api_server_sans_and_sync_issue/","date":1739898036,"author":"/u/nfreder","guid":4275,"unread":true,"content":"<p>We have a use case to leverage a cloud provider I won't name that starts with an O. I have a requirement to setup a hostname and IP for connectivity. Is there a way to run kubeadm commands across all control plane nodes that I'm missing? I think a daemonset might work but, I have a chicken and egg issue to setup any kind of automation there. </p>","contentLength":344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"issue with ingress","url":"https://www.reddit.com/r/kubernetes/comments/1isgbi8/issue_with_ingress/","date":1739895206,"author":"/u/GeneEfficient1481","guid":4274,"unread":true,"content":"<p>hello everyone i am having trouble with this ingress exercise</p><p>Create an Ingress resource named web and configure it as follows:</p><p>Route traffic for the host web.kubernetes and all routes to the existing web service. Enable TLS termination using the existing Secret web certification.</p><p>Redirect HTTP requests to HTTPS. </p><p>I have configured /etc/hosts I will pair the node ip with the web.kubernetes host</p><p>[curl: (7) Unable to connect to web.k8s.local port 80: connection refused]</p><p>what should i do to solve the problem?</p><p>this my txt containing deploy,svc secret and ingress: # 1. Deployment</p><p>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=web.k8s.local/O=web.k8s.local\"</p><p>kubectl create secret tls web-cert --namespace=prod --cert=tls.crt --key=tls.key</p>","contentLength":776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elixir in kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1isfy34/elixir_in_kubernetes/","date":1739894278,"author":"/u/Latter-Change-9228","guid":4243,"unread":true,"content":"<p>I'm currently learning elixir in order to use it in production. I heard of the node architecture that elixir provides thanks to the OTP but I can't find resources about some return on experienec on using distributed elixir in a kubernetes context. Any thoughts about that ?</p>","contentLength":273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Good books/video/article to understand ingress controllers","url":"https://www.reddit.com/r/kubernetes/comments/1isfeoc/good_booksvideoarticle_to_understand_ingress/","date":1739892908,"author":"/u/khaloudkhaloud","guid":4242,"unread":true,"content":"<div><p>Any good ressources to \"really\" understand how ingress controllers works</p></div>   submitted by   <a href=\"https://www.reddit.com/user/khaloudkhaloud\"> /u/khaloudkhaloud </a>","contentLength":109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RKE2-Agent and Cilium HostFirewall Blocking Port 9345","url":"https://www.reddit.com/r/kubernetes/comments/1isc6cm/rke2agent_and_cilium_hostfirewall_blocking_port/","date":1739883773,"author":"/u/zdeneklapes","guid":4186,"unread":true,"content":"<p>I'm setting up a Kubernetes cluster using Rancher RKE2 with Cilium as the CNI. Everything works fine on the <strong>RKE2 server (master node)</strong> with  and <strong>kube-proxy replacement activated</strong>.</p><p>However, when I try to add a , it seems that some rules are pulled to the worker node, and after approximately , port  is . This results in the following error on the worker node:</p><pre><code>Feb 18 09:45:28 compute-07 rke2[173412]: time=\"2025-02-18T09:45:28Z\" level=error msg=\"Failed to connect to proxy. Empty dialer response\" error=\"dial tcp &lt;my-public-server-ip&gt;:9345: connect: connection timed out\" </code></pre><p>To fix this, I tried allowing the port  before adding the new worker node by applying the following <strong>CiliumClusterwideNetworkPolicy</strong>:</p><pre><code>apiVersion: cilium.io/v2 kind: CiliumClusterwideNetworkPolicy metadata: name: allow-hostfirewall-9345 spec: nodeSelector: {} # Applies to all nodes ingress: - fromEntities: - all toPorts: - ports: - port: \"9345\" protocol: TCP egress: - toEntities: - all toPorts: - ports: - port: \"9345\" protocol: TCP </code></pre><p>Unfortunately, this did not resolve the issue.</p><p>Before starting , I confirmed that the port :</p><pre><code>root@compute-07:~# nc -zv &lt;ip&gt; 9345 Ncat: Version 7.92 () Ncat: Connected to &lt;ip&gt;:9345. Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds.https://nmap.org/ncat </code></pre><p>After starting , the port :</p><pre><code>root@compute-07:~# nc -zv &lt;ip&gt; 9345 Ncat: Version 7.92 ( https://nmap.org/ncat ) Ncat: Connection timed out. </code></pre><ol><li><strong>Why is port 9345 being closed after the RKE2 agent starts?</strong></li><li><strong>Is there a better way to explicitly allow this port through Cilium's hostFirewall?</strong></li><li><strong>What additional troubleshooting steps should I take to debug this issue?</strong></li></ol>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simplifying Kubernetes deployments with a unified Helm chart","url":"https://www.reddit.com/r/kubernetes/comments/1isb1e2/simplifying_kubernetes_deployments_with_a_unified/","date":1739879866,"author":"/u/danielepolencic","guid":4122,"unread":true,"content":"<div><p>Managing  in  at scale often leads to inconsistent deployments and maintenance overhead. This episode explores a practical solution that standardizes service deployments while maintaining team autonomy.</p><p> discusses how a unified  chart approach can help platform teams support multiple development teams efficiently while maintaining consistent standards across services.</p><ul><li>Why inconsistent  chart configurations across teams create maintenance challenges and slow down deployments</li><li>How to implement a unified  chart that balances standardization with flexibility through override functions</li><li>How to maintain quality through automated documentation and testing with tools like  and </li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/danielepolencic\"> /u/danielepolencic </a>","contentLength":710,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Questions and advice","url":"https://www.reddit.com/r/kubernetes/comments/1isa58r/weekly_questions_and_advice/","date":1739876419,"author":"/u/gctaylor","guid":4081,"unread":true,"content":"<p>Have any questions about Kubernetes, related tooling, or how to adopt or use Kubernetes? Ask away!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Longhorn does not recognize dm-crypt module in ubunti 24.04 vm.","url":"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/","date":1739872887,"author":"/u/MrSliff84","guid":4054,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Do i have to set up secrets first, to get rid of this warning in longhorn?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MrSliff84\"> /u/MrSliff84 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Cgroup v2 Features Are You Using Beyond Basic CPU and Memory limit in Kubernetes? (Alpha features or customized plugins)","url":"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/","date":1739869690,"author":"/u/Electronic_Role_5981","guid":4053,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">https://kubernetes.io/docs/concepts/architecture/cgroups/</a> </p> <p><a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">cgroup v2 </a>is stable since v1.25.</p> <p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#memory-qos-with-cgroup-v2\">MemoryQoS</a> started using memory.high, but it may cause throttling issue to hang the application sometimes. It is still alpha since 1.22.</p> <p>For OOMKill behavior change, kubelet added <a href=\"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/\">singleProcessOOMKill</a> to keep the behavior of cgroups v1 when users want. </p> <p><a href=\"https://github.com/kubernetes/enhancements/issues/4205\">PSI KEP</a> was merged recently for v1.33.</p> <p><a href=\"https://github.com/kubernetes/enhancements/issues/2400\">NodeSwap</a> was beta now.</p> <p>Cgroup v2 controller includes:</p> <ul> <li><strong><em>memory (since Linux 4.5)</em></strong></li> <li><em>pids</em> (since Linux 4.5)</li> <li><strong><em>io</em></strong> <strong>(since Linux 4.5)</strong></li> <li><em>rdma (since Linux 4.11)</em></li> <li><em>perf_event</em> (since Linux 4.11)</li> <li><strong><em>cpu (since Linux 4.15)</em></strong></li> <li><em>cpuset</em> (since Linux 5.0)</li> <li><strong><em>freezer</em></strong> <strong>(since Linux 5.2)</strong></li> <li><em>hugetlb</em> (since Linux 5.6)</li> <li><em>nsdelegate</em> (since Linux 4.15)</li> <li>PSI(since Linux 4.20)</li> </ul> <p>Anyone started using the blkio limit or other cgroup controllers? Are you enable the CgroupV2 related feature gates above or flags? </p> <ul> <li>Some related projects: <ul> <li> <a href=\"https://facebookmicrosites.github.io/oomd/\">https://facebookmicrosites.github.io/oomd/</a></li> <li> <a href=\"https://github.com/facebookincubator/oomd\">https://github.com/facebookincubator/oomd</a></li> </ul></li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"unexpected side effects in pod routing","url":"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/","date":1739862394,"author":"/u/Cyclonit","guid":4004,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I am working on hosting <a href=\"https://www.home-assistant.io/\">Home Assistant</a> in my Kubernetes Homelab. For Home Assistant being able to discover devices in my home network, I added a secondary bridged macvlan0 network interface using Multus. Given that my router manages IP addresses for my home network, I decided to use DHCP for the pod&#39;s second IP address too. This part works fine.</p> <pre><code>apiVersion: &quot;k8s.cni.cncf.io/v1&quot; kind: NetworkAttachmentDefinition metadata: name: eth0-macvlan-dhcp spec: config: | { &quot;cniVersion&quot;: &quot;0.3.0&quot;, &quot;type&quot;: &quot;macvlan&quot;, &quot;master&quot;: &quot;eth0&quot;, &quot;mode&quot;: &quot;bridge&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;dhcp&quot; } } </code></pre> <p>However, using DHCP results in the pod receiving a second default route via my home network&#39;s router. This route takes precedence over the default route via the pod network and completely breaks pod-to-pod communication.</p> <p>This is how the routes look like inside of the container after deployment:</p> <pre><code>```sh $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 192.168.178.1 0.0.0.0 UG 0 0 0 net1 default 10.0.2.230 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 ``` </code></pre> <p>This is what happens after trying to delete the first route. As you can see, the default route via <a href=\"http://10.0.2.230\">10.0.2.230</a> was replaced by a default route via localhost. <a href=\"http://10.0.2.230\">10.0.2.230</a> is not an IP of the pod.</p> <pre><code>$ route del -net default gw 192.168.178.1 netmask 0.0.0.0 dev net1 $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default localhost 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 </code></pre> <p>Interestingly, this is completely reversible by adding the undesired route back:</p> <pre><code>$ route add -net default gw 192.168.178.1 netmask 0.0.0.0 dev net1 $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 192.168.178.1 0.0.0.0 UG 0 0 0 net1 default 10.0.2.230 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 </code></pre> <p>Any ideas on what is going on?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cyclonit\"> /u/Cyclonit </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Whats the most kubefriendly pubsub messaging broker?","url":"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/","date":1739831242,"author":"/u/leeliop","guid":2089,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Like rabbitmq or even amazon sns?</p> <p>Or is it easier just using sns if we are in eks/amazon managed k8s land?</p> <p>Its for enterprise messaging volume, not particularly complex but just lots of it</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/leeliop\"> /u/leeliop </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[ Removed by Reddit ]","url":"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/","date":1739826481,"author":"/u/Vw-Bee5498","guid":3977,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>[ Removed by Reddit on account of violating the <a href=\"/help/contentpolicy\">content policy</a>. ]</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vw-Bee5498\"> /u/Vw-Bee5498 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[ Removed by Reddit ]","url":"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/","date":1739823237,"author":"/u/Vw-Bee5498","guid":4025,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>[ Removed by Reddit on account of violating the <a href=\"/help/contentpolicy\">content policy</a>. ]</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vw-Bee5498\"> /u/Vw-Bee5498 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bootstrapping Argo for Entra ID OIDC","url":"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/","date":1739813751,"author":"/u/UberBoob","guid":1947,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks! I&#39;m trying to spin up an Argo-managed Cluster to use Azure AD credentials as the sole SSO provider.</p> <p>I have the secrets mounted on the Argo Server pods, provided from AWS Secrets Manager by AWS Secrets Store CSI driver and provider. client_id and client_secret are located at /mnt/secrets-store. My terrafrom modules are running a helm release install of Argo CD 7.7.7.</p> <p>Im trying to use env variables passed as helm values.yaml. Argo CD runs fine, I can login via initial Admin creds. The Entra ID button is in place for login, however response from Microsoft is that I must provide a client id in the request.</p> <p>Anyone else take this approach and have it working? We, can pass the values via Terraform, however the secret ends up in plan files and is not masked even when using the sensitive() in Terraform. This fails our scan audits and want to keep the secrets in AWS secrets manager as a permanent solution.</p> <p>The Argo Docs don&#39;t go into much detail around OIDC, other than setting the OIDC details in the ConfiMap.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/UberBoob\"> /u/UberBoob </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Canonical Extends Kubernetes Distro Support to a Dozen Years","url":"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/","date":1739813703,"author":"/u/CrankyBear","guid":1997,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/\"> <img src=\"https://external-preview.redd.it/CrTnhrf0m844iLfAuNSRmJ3R8qO-sAQM4pZX5wPcg5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74e86d0c279e67e1c32bd05a5d434e80aba9d3f0\" alt=\"Canonical Extends Kubernetes Distro Support to a Dozen Years\" title=\"Canonical Extends Kubernetes Distro Support to a Dozen Years\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CrankyBear\"> /u/CrankyBear </a> <br/> <span><a href=\"https://thenewstack.io/canonical-extends-kubernetes-distro-support-to-a-dozen-years/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self hosted kubernetes, how to make control plane easier....","url":"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/","date":1739800870,"author":"/u/TheBeardMD","guid":1865,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Very familiar with AWS EKS, where you don&#39;t really have to worry about the control plane at all. Thinking about starting a cluster from scratch, but find the control plane very complex. Are there any options to make managing the control plane easier so it&#39;s possible to create a cluster from scratch?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheBeardMD\"> /u/TheBeardMD </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The state of Kubernetes job market in 2024","url":"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/","date":1739800433,"author":"/u/danielepolencic","guid":1864,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/\"> <img src=\"https://external-preview.redd.it/BycxNH7ovolfHwotvNh-bJXbjRfA8Et5_h5nErx1JMA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca3d7a63e70d5d842882f98b3db33b15e3165655\" alt=\"The state of Kubernetes job market in 2024\" title=\"The state of Kubernetes job market in 2024\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/danielepolencic\"> /u/danielepolencic </a> <br/> <span><a href=\"https://kube.careers/state-of-kubernetes-jobs-2024-q4\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kubegui.io - user friendly kubernetes desktop application","url":"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/","date":1739795999,"author":"/u/Live_Landscape_7570","guid":1809,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\"> <img src=\"https://a.thumbs.redditmedia.com/3eh_HCnUiUTn-Kn5n_tSv4JYVCz06lQL6-ycwAITBU8.jpg\" alt=\"kubegui.io - user friendly kubernetes desktop application\" title=\"kubegui.io - user friendly kubernetes desktop application\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Who we are? SRE engineers! What do we want? </p> <p><a href=\"http://kubegui.io\">One more GUI-based Kubernetes management tool</a> for daily operations. </p> <p>I&#39;ve just created a golang/wails based client for any available Kubernetes cluster that&#39;s better then alternatives (based on exceptional research made within my family members) and much much cheaper. </p> <p><a href=\"https://preview.redd.it/veh4twld2pje1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=33b972e13d958d3ff208aef70a2c6497fc4f9aea\">kubegui.io</a></p> <p>Some advantages: </p> <p>‚ö° Lightning Fast Performance: Built with Go official kubernetes client for maximum speed/cache usage + minimal resource usage </p> <p>üíª Zero Dependencies: No kubectl required (or any other tools) </p> <p>üîÑ Seamless Multi-cluster Management: Switch between clusters with last viewed resource state saved </p> <p>üí° AI provided suggestions: Realtime AI integrations for fix suggestions (for deployments/pods/events issues) </p> <p>üìä Advanced Monitoring: Real-time metrics out of the box (for pods/nodes for the last hour) </p> <p>üîí Enhanced Security: No external calls (except for AI fix suggestions if enabled) </p> <p>üì¶ Single Binary Distribution: No runtime dependencies required </p> <p>üìÑ Smart yaml viewer: Context-aware editor with indentation linter and error detection </p> <p>üìù Interactive Shell access: One click pod exec (xterm with copy-paste available) </p> <p>üéÆ Pod ports forwarding: One click inside pod details exposed ports (via default browser session). </p> <p>üõ°Ô∏è Network Policies: Visualize policy feature inside resource details </p> <p>üîç Enhanced log viewer: Built-in logs syntax highlighter </p> <p>üîë Custom resource generator: Unique custom resource example creation based on crd schema </p> <p>‚ûï Auto updates: self-updating application (via github public project repo background calls)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Live_Landscape_7570\"> /u/Live_Landscape_7570 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask r/kubernetes: What are you working on this week?","url":"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/","date":1739790030,"author":"/u/gctaylor","guid":1775,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell <a href=\"/r/kubernetes\">/r/kubernetes</a> what you&#39;re up to this week!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Doesn‚Äôt Have to Be Complicated ‚Äì A Practical Guide to Simplifying Your Workflow","url":"https://www.reddit.com/r/kubernetes/comments/1irghmz/kubernetes_doesnt_have_to_be_complicated_a/","date":1739786358,"author":"/u/Sheishofert","guid":1743,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sheishofert\"> /u/Sheishofert </a> <br/> <span><a href=\"https://rust-on-nails.com/blog/kubernetes-complicated/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irghmz/kubernetes_doesnt_have_to_be_complicated_a/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses","url":"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/","date":1739781956,"author":"/u/Wownever","guid":1679,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/\"> <img src=\"https://external-preview.redd.it/htjVgpTbC6LEHHA_g3zSEEkmVxg_6sYEyx94ado_4FU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8858d3c38de489836abc3910c8502b1710b9834\" alt=\"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses\" title=\"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Wownever\"> /u/Wownever </a> <br/> <span><a href=\"https://www.wiz.io/blog/kubernetes-report-preview-2025\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do you scale to zero and from zero?","url":"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/","date":1739779527,"author":"/u/Electronic_Role_5981","guid":1680,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://github.com/kubernetes/enhancements/issues/2021\">https://github.com/kubernetes/enhancements/issues/2021</a> is open. `HPAScaleToZero` is alpha in v1.16 and has no much updates. </p> <p>There are several known choices like</p> <ul> <li>Scale from zero(<strong>No native support</strong>), or known as activator. (Also some Faas platforms, like OpenFaas or Serverless apps supports)</li> </ul> <ol> <li>Knative: Activator. <a href=\"https://knative.dev/docs/serving/architecture/#diagram\">https://knative.dev/docs/serving/architecture/#diagram</a> <a href=\"https://github.com/knative/serving\">knative/serving</a></li> <li>KEDA <ol> <li>Example: OpenFunction: <a href=\"https://github.com/OpenFunction/OpenFunction/pull/483\">https://github.com/OpenFunction/OpenFunction/pull/483</a> &amp; <a href=\"https://github.com/OpenFunction/OpenFunction/blob/main/docs/proposals/20230726-integrate-keda-http-add-on.md\">https://github.com/OpenFunction/OpenFunction/blob/main/docs/proposals/20230726-integrate-keda-http-add-on.md</a></li> </ol></li> <li>A initial implementation using service, like kube-proxy: <a href=\"https://github.com/wzshiming/kube-activator\">https://github.com/wzshiming/kube-activator</a> <ul> <li>Scale to zero (natively alpha feature)</li> </ul></li> <li>HPA: HPAScaleToZero feature gate</li> <li><a href=\"https://knative.dev/docs/serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period\">https://knative.dev/docs/serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period</a></li> <li><a href=\"https://github.com/deislabs/osiris\">https://github.com/deislabs/osiris</a> archived as the hpa supports it</li> </ol> <p>The last discussion in reddit is <a href=\"https://www.reddit.com/r/kubernetes/comments/1de8qiz/scaling%5C_to%5C_zero/\">https://www.reddit.com/r/kubernetes/comments/1de8qiz/scaling\\_to\\_zero/</a>. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Set Up a Persistent Volume for MinIO on GKE Free Tier? Do I Get Any Free Storage?","url":"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/","date":1739775790,"author":"/u/blvck_viking","guid":1633,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m setting up a self-hosted MinIO instance on Google Kubernetes Engine (GKE) and need to configure a persistent volume for storage. I&#39;m currently using the GKE free tier and was wondering:</p> <ol> <li>Does GKE free tier include any free persistent storage, or will I need to pay for it?</li> <li>What&#39;s the best way to set up a Persistent Volume (PV) and Persistent Volume Claim (PVC) for MinIO in a GKE cluster?</li> <li>Any recommendations on storage classes and best practices?</li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/blvck_viking\"> /u/blvck_viking </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ingress Help","url":"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/","date":1739754260,"author":"/u/MeerkatMoe","guid":860,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to setup ingress using ingress nginx, but I can&#39;t figure out how to get routing to work...either my frontend breaks or my api is unreachable.</p> <p>I have an nginx service (not ingress nginx) that serves a frontend on port 80 and an express service that serves a backend API on port 5000.</p> <p>My first attempt was two separate ingresses (not sure about terminology):</p> <pre><code>--- metadata: name: api-ingress annotations: kubernetes.io/ingress.class: &quot;nginx&quot; spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} --- metadata: name: frontend-ingress namespace: {{ k3s_namespace }} annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: / pathType: Prefix backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} </code></pre> <p>but that didn&#39;t work, and sometimes my API won&#39;t get routed correctly. I think it&#39;s because they get combined and I can&#39;t guarantee the order.</p> <p>My next try was to combine them:</p> <pre><code>kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} - path: &quot;/(?!api).*&quot; pathType: ImplementationSpecific backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} </code></pre> <p>(left some stuff out to save space)</p> <p>but that also didn&#39;t work.</p> <p>What is the best way to get this working? To summarize, I just need</p> <p>&quot;/api/*&quot; -&gt; api service port 5000 (it can route as /api/&lt;whatever&gt; or just &lt;whatever&gt;)</p> <p>&quot;/*&quot; -&gt; nginx port 80</p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MeerkatMoe\"> /u/MeerkatMoe </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Event driven workloads on K8s - how do you handle them?","url":"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/","date":1739752293,"author":"/u/sniktasy","guid":846,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks! </p> <p>I have been working with <a href=\"https://github.com/numaproj/numaflow\">Numaflow</a>, an open source project that helps build event driven applications on K8s. It basically makes it easier to process streaming data (think events on kafka, pulsar, sqs etc). </p> <p>Some cool stuff - autoscaling based on pending events/ back pressure handling (scale to 0 if need be), source and sink connectors, multi-language support, can support real time data processing use cases with the pipeline semantics etc</p> <p>Curious, how are you handling event-driven workloads today? Would love to hear what&#39;s working for others?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sniktasy\"> /u/sniktasy </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Issues with logrotate when logrotate failed to rotate the logs for container","url":"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/","date":1739739973,"author":"/u/barely_malted","guid":796,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I am using AWS EKS and using default kubelet logrotate parameters (maxsize = 10 Mi and maxfiles = 5)<br/> I am facing an issue where I believe these default values are not respected. The kubelet is failing with &#39;Failed to rotate log for container&#39; &#39;err=failed to compress log (container/pod log paths) nospace left on device&#39;<br/> At the same time one of my pods generated 200 GB logs in one single file. How is this possible ?<br/> I was not able to find out any documentation regarding this behaviour.<br/> Does this mean that since the kubelet was not able to rotate logs, it just kept on writing them to this one log file till it reached the diskspace limits of my worker nodes ?<br/> K8s/EKS version 1.27</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/barely_malted\"> /u/barely_malted </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do devs use kubernetes services locally via ingress on the likes of docker desktop","url":"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/","date":1739734137,"author":"/u/TheRandyOne","guid":746,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have recently started getting some toolkits running for my devs. I need to get them started on k8s as I am moving services over to k8s.</p> <p>I was explaining how this works to a friend and it dawned on me that to use a resource inside the cluster you need to enter via an ingress. The ingress is easy enough since we have the nginx ingress. </p> <p>The problem comes in with the dns records required to point to the defined resource to 127.0.0.1 in the /etc/hosts file. Since we have quite few services that need to hosted in k8s, it&#39;ll really suck to have the devs to add a bunch of records to the hosts file</p> <p>Basically I want something like a wild card record that always returns 127.0.0.1 outside the cluster. So they can pick whatever name they want and always have that delivered to the ingress.</p> <p>Am I doing this wrong? Is there some other way that I should be approaching this problem?<br/> Or can someone explain how they deal with this other than just editing hosts files.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheRandyOne\"> /u/TheRandyOne </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pull Request testing on Kubernetes: working with GitHub Actions and GKE","url":"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/","date":1739730123,"author":"/u/nfrankel","guid":722,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I‚Äôm continuing my series on running the test suite for each Pull Request on Kubernetes. In the <a href=\"https://blog.frankel.ch/integration-test-kubernetes/1/\">previous post</a>, I laid the groundwork for our learning journey: I developed a basic JVM-based CRUD app, tested it locally using Testcontainers, and tested it in a GitHub workflow with a GitHub service container.</p> <p>This week, I will raise the ante to run the end-to-end test in the target Kubernetes environment. For this, I‚Äôve identified gaps that I‚Äôll implement in this blog post:</p> <ul> <li>Create and configure a Google Kubernetes Engine instance</li> <li>Create a Kubernetes manifest for the app, with Kustomize for customization</li> <li>Allow the GitHub workflow to use the GKE instance</li> <li>Build the Docker image and store it in the GitHub Docker repo</li> <li>Install the PostgreSQL Helm chart</li> <li>Apply our app manifest</li> <li>Finally, run the end-to-end test</li> </ul> <p>Stages 1, 2, and 3 are upstream, while the workflow executes the latter steps for each PR.</p> <p>As I had to choose a tech stack for the app, I had to select a Cloud provider for my infrastructure. I choose GKE because I‚Äôm more familiar with Google Cloud, but you can apply the same approach to any other provider. The concept will be the same, only the implementation will differ slightly.</p> <p><a href=\"https://blog.frankel.ch/pr-testing-kubernetes/2/\">Read more</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nfrankel\"> /u/nfrankel </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Managing a Talos cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/","date":1739727418,"author":"/u/simen64","guid":698,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have been looking into moving my homelab to Kubernetes and Talos seems great for the job. I use OpenTofu for deploying infra in my homelab like VM&#39;s in proxmox, but how do people integrate Talos into OpenTofu / Terraform? I have not gotten the talos terraform provider to work and it lacks basic functionality for stuff like updating. So how do people manage their talos clusters?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/simen64\"> /u/simen64 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measure cpu utilization per deployment?","url":"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/","date":1739722689,"author":"/u/netcat23","guid":745,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi guys, does measuring cpu utilization of a deployment brings any value?</p> <p>What is you opinion about it?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/netcat23\"> /u/netcat23 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Starting a Weekly Rancher Series ‚Äì From Zero to Hero!","url":"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/","date":1739717223,"author":"/u/abhimanyu_saharan","guid":650,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I&#39;m kicking off a weekly YouTube series on Rancher, covering everything from getting started to advanced use cases. Whether you&#39;re new to Rancher or looking to level up your Kubernetes management skills, this series will walk you through step-by-step tutorials, hands-on demos, and real-world troubleshooting.</p> <p>I&#39;ve just uploaded the introductory video where I break down what Rancher is and why it matters: üì∫ <a href=\"https://youtu.be/_CRjSf8i7Vo?si=ZR6IcXaNOCCppFiG\">https://youtu.be/_CRjSf8i7Vo?si=ZR6IcXaNOCCppFiG</a></p> <p>I&#39;ll be posting new videos every week, so if you&#39;re interested in mastering Rancher, make sure to follow along. Would love to hear your feedback and any specific topics you&#39;d like to see covered!</p> <p>Let‚Äôs build and learn together! üöÄ</p> <h1>Kubernetes #Rancher #DevOps #Containers #SelfHosting #Homelab</h1> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/abhimanyu_saharan\"> /u/abhimanyu_saharan </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["k8s"]}