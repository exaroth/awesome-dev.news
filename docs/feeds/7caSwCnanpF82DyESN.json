{"id":"7caSwCnanpF82DyESN","title":"Official News","displayTitle":"Official News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":12,"items":[{"title":"Python eBook Fourth of July Sale","url":"https://www.blog.pythonlibrary.org/2025/07/03/python-ebook-fourth-of-july-sale/","date":1751549555,"author":"Mike","guid":183169,"unread":true,"content":"<p data-pm-slice=\"1 1 []\">Happy Fourth of July! I am hosting a sale for the 4th of July weekend, where you can get 25% off most of my books and courses.</p><p>Here are the books included in the sale and the direct links with the  already applied:</p><p>I hope you’ll check out the sale, but even if you don’t, I hope you have a great holiday weekend!</p>","contentLength":314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navigating Failures in Pods With Devices","url":"https://kubernetes.io/blog/2025/07/03/navigating-failures-in-pods-with-devices/","date":1751500800,"author":"","guid":181564,"unread":true,"content":"<p>Kubernetes is the de facto standard for container orchestration, but when it\ncomes to handling specialized hardware like GPUs and other accelerators, things\nget a bit complicated. This blog post dives into the challenges of managing\nfailure modes when operating pods with devices in Kubernetes, based on insights\nfrom <a href=\"https://sched.co/1i7pT\">Sergey Kanzhelev and Mrunal Patel's talk at KubeCon NA\n2024</a>. You can follow the links to\n<a href=\"https://static.sched.com/hosted_files/kccncna2024/b9/KubeCon%20NA%202024_%20Navigating%20Failures%20in%20Pods%20With%20Devices_%20Challenges%20and%20Solutions.pptx.pdf?_gl=1*191m4j5*_gcl_au*MTU1MDM0MTM1My4xNzMwOTE4ODY5LjIxNDI4Nzk1NDIuMTczMTY0ODgyMC4xNzMxNjQ4ODIy*FPAU*MTU1MDM0MTM1My4xNzMwOTE4ODY5\">slides</a>\nand\n<a href=\"https://www.youtube.com/watch?v=-YCnOYTtVO8&amp;list=PLj6h78yzYM2Pw4mRw4S-1p_xLARMqPkA7&amp;index=150\">recording</a>.</p><h2>The AI/ML boom and its impact on Kubernetes</h2><p>The rise of AI/ML workloads has brought new challenges to Kubernetes. These\nworkloads often rely heavily on specialized hardware, and any device failure can\nsignificantly impact performance and lead to frustrating interruptions. As\nhighlighted in the 2024 <a href=\"https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\">Llama\npaper</a>,\nhardware issues, particularly GPU failures, are a major cause of disruption in\nAI/ML training. You can also learn how much effort NVIDIA spends on handling\ndevices failures and maintenance in the KubeCon talk by <a href=\"https://kccncna2024.sched.com/event/1i7kJ/all-your-gpus-are-belong-to-us-an-inside-look-at-nvidias-self-healing-geforce-now-infrastructure-ryan-hallisey-piotr-prokop-pl-nvidia\">Ryan Hallisey and Piotr\nProkop All-Your-GPUs-Are-Belong-to-Us: An Inside Look at NVIDIA's Self-Healing\nGeForce NOW\nInfrastructure</a>\n(<a href=\"https://www.youtube.com/watch?v=iLnHtKwmu2I\">recording</a>) as they see 19\nremediation requests per 1000 nodes a day!\nWe also see data centers offering spot consumption models and overcommit on\npower, making device failures commonplace and a part of the business model.</p><p>However, Kubernetes’s view on resources is still very static. The resource is\neither there or not. And if it is there, the assumption is that it will stay\nthere fully functional - Kubernetes lacks good support for handling full or partial\nhardware failures. These long-existing assumptions combined with the overall complexity of a setup lead\nto a variety of failure modes, which we discuss here.</p><h3>Understanding AI/ML workloads</h3><p>Generally, all AI/ML workloads require specialized hardware, have challenging\nscheduling requirements, and are expensive when idle. AI/ML workloads typically\nfall into two categories - training and inference. Here is an oversimplified\nview of those categories’ characteristics, which are different from traditional workloads\nlike web services:</p><dl><dd>These workloads are resource-intensive, often consuming entire\nmachines and running as gangs of pods. Training jobs are usually \"run to\ncompletion\" - but that could be days, weeks or even months. Any failure in a\nsingle pod can necessitate restarting the entire step across all the pods.</dd><dd>These workloads are usually long-running or run indefinitely,\nand can be small enough to consume a subset of a Node’s devices or large enough to span\nmultiple nodes. They often require downloading huge files with the model\nweights.</dd></dl><p>These workload types specifically break many past assumptions:</p><table><caption>Workload assumptions before and now</caption><tbody><tr><td>Can get a better CPU and the app will work faster.</td><td>Require a  device (or ) to run.</td></tr><tr><td>When something doesn’t work, just recreate it.</td><td>Allocation or reallocation is expensive.</td></tr><tr><td>Any node will work. No need to coordinate between Pods.</td><td>Scheduled in a special way - devices often connected in a cross-node topology.</td></tr><tr><td>Each Pod can be plug-and-play replaced if failed.</td><td>Pods are a part of a larger task. Lifecycle of an entire task depends on each Pod.</td></tr><tr><td>Container images are slim and easily available.</td><td>Container images may be so big that they require special handling.</td></tr><tr><td>Long initialization can be offset by slow rollout.</td><td>Initialization may be long and should be optimized, sometimes across many Pods together.</td></tr><tr><td>Compute nodes are commoditized and relatively inexpensive, so some idle time is acceptable.</td><td>Nodes with specialized hardware can be an order of magnitude more expensive than those without, so idle time is very wasteful.</td></tr></tbody></table><p>The existing failure model was relying on old assumptions. It may still work for\nthe new workload types, but it has limited knowledge about devices and is very\nexpensive for them. In some cases, even prohibitively expensive. You will see\nmore examples later in this article.</p><h3>Why Kubernetes still reigns supreme</h3><p>This article is not going deeper into the question: why not start fresh for\nAI/ML workloads since they are so different from the traditional Kubernetes\nworkloads. Despite many challenges, Kubernetes remains the platform of choice\nfor AI/ML workloads. Its maturity, security, and rich ecosystem of tools make it\na compelling option. While alternatives exist, they often lack the years of\ndevelopment and refinement that Kubernetes offers. And the Kubernetes developers\nare actively addressing the gaps identified in this article and beyond.</p><h2>The current state of device failure handling</h2><p>This section outlines different failure modes and the best practices and DIY\n(Do-It-Yourself) solutions used today. The next session will describe a roadmap\nof improving things for those failure modes.</p><h3>Failure modes: K8s infrastructure</h3><p>In order to understand the failures related to the Kubernetes infrastructure,\nyou need to understand how many moving parts are involved in scheduling a Pod on\nthe node. The sequence of events when the Pod is scheduled in the Node is as\nfollows:</p><ol><li> is scheduled on the Node</li><li> is registered with the  via local gRPC</li><li> uses  to watch for devices and updates capacity of\nthe node</li><li> places a  on a Node based on the updated capacity</li><li> asks  to  devices for a </li><li> creates a  with the allocated devices attached to it</li></ol><p>This diagram shows some of those actors involved:</p><p>As there are so many actors interconnected, every one of them and every\nconnection may experience interruptions. This leads to many exceptional\nsituations that are often considered failures, and may cause serious workload\ninterruptions:</p><ul><li>Pods failing admission at various stages of its lifecycle</li><li>Pods unable to run on perfectly fine hardware</li><li>Scheduling taking unexpectedly long time</li></ul><p>The goal for Kubernetes is to make the interruption between these components as\nreliable as possible. Kubelet already implements retries, grace periods, and\nother techniques to improve it. The roadmap section goes into details on other\nedge cases that the Kubernetes project tracks. However, all these improvements\nonly work when these best practices are followed:</p><ul><li>Configure and restart kubelet and the container runtime (such as containerd or CRI-O)\nas early as possible to not interrupt the workload.</li><li>Monitor device plugin health and carefully plan for upgrades.</li><li>Do not overload the node with less-important workloads to prevent interruption\nof device plugin and other components.</li><li>Configure user pods tolerations to handle node readiness flakes.</li><li>Configure and code graceful termination logic carefully to not block devices\nfor too long.</li></ul><p>Another class of Kubernetes infra-related issues is driver-related. With\ntraditional resources like CPU and memory, no compatibility checks between the\napplication and hardware were needed. With special devices like hardware\naccelerators, there are new failure modes. Device drivers installed on the node:</p><ul><li>Be compatible with an app</li><li>Must work with other drivers (like <a href=\"https://developer.nvidia.com/nccl\">nccl</a>,\netc.)</li></ul><p>Best practices for handling driver versions:</p><ul><li>Monitor driver installer health</li><li>Plan upgrades of infrastructure and Pods to match the version</li><li>Have canary deployments whenever possible</li></ul><p>Following the best practices in this section and using device plugins and device\ndriver installers from trusted and reliable sources generally eliminate this\nclass of failures. Kubernetes is tracking work to make this space even better.</p><h3>Failure modes: device failed</h3><p>There is very little handling of device failure in Kubernetes today. Device\nplugins report the device failure only by changing the count of allocatable\ndevices. And Kubernetes relies on standard mechanisms like liveness probes or\ncontainer failures to allow Pods to communicate the failure condition to the\nkubelet. However, Kubernetes does not correlate device failures with container\ncrashes and does not offer any mitigation beyond restarting the container while\nbeing attached to the same device.</p><p>This is why many plugins and DIY solutions exist to handle device failures based\non various signals.</p><p>In many cases a failed device will result in unrecoverable and very expensive\nnodes doing nothing. A simple DIY solution is a . The\ncontroller could compare the device allocatable count with the capacity and if\nthe capacity is greater, it starts a timer. Once the timer reaches a threshold,\nthe health controller kills and recreates a node.</p><p>There are problems with the  approach:</p><ul><li>Root cause of the device failure is typically not known</li><li>The controller is not workload aware</li><li>Failed device might not be in use and you want to keep other devices running</li><li>The detection may be too slow as it is very generic</li><li>The node may be part of a bigger set of nodes and simply cannot be deleted in\nisolation without other nodes</li></ul><p>There are variations of the health controller solving some of the problems\nabove. The overall theme here though is that to best handle failed devices, you\nneed customized handling for the specific workload. Kubernetes doesn’t yet offer\nenough abstraction to express how critical the device is for a node, for the\ncluster, and for the Pod it is assigned to.</p><p>Another DIY approach for device failure handling is a per-pod reaction on a\nfailed device. This approach is applicable for  workloads that are\nimplemented as Jobs.</p><p>There are some problems with the  approach for Jobs:</p><ul><li>There is no well-known  condition, so this approach does not work for the\ngeneric Pod case</li><li>Error codes must be coded carefully and in some cases are hard to guarantee.</li><li>Only works with Jobs with , due to the limitation of a pod\nfailure policy feature.</li></ul><p>So, this solution has limited applicability.</p><p>A little more generic approach is to implement the Pod watcher as a DIY solution\nor use some third party tools offering this functionality. The pod watcher is\nmost often used to handle device failures for inference workloads.</p><p>Since Kubernetes just keeps a pod assigned to a device, even if the device is\nreportedly unhealthy, the idea is to detect this situation with the pod watcher\nand apply some remediation. It often involves obtaining device health status and\nits mapping to the Pod using Pod Resources API on the node. If a device fails,\nit can then delete the attached Pod as a remediation. The replica set will\nhandle the Pod recreation on a healthy device.</p><p>The other reasons to implement this watcher:</p><ul><li>Without it, the Pod will keep being assigned to the failed device forever.</li><li>There is no  for a pod with .</li><li>There are no built-in controllers that delete Pods in CrashLoopBackoff.</li></ul><p>Problems with the :</p><ul><li>The signal for the pod watcher is expensive to get, and involves some\nprivileged actions.</li><li>It is a custom solution and it assumes the importance of a device for a Pod.</li><li>The pod watcher relies on external controllers to reschedule a Pod.</li></ul><p>There are more variations of DIY solutions for handling device failures or\nupcoming maintenance. Overall, Kubernetes has enough extension points to\nimplement these solutions. However, some extension points require higher\nprivilege than users may be comfortable with or are too disruptive. The roadmap\nsection goes into more details on specific improvements in handling the device\nfailures.</p><h3>Failure modes: container code failed</h3><p>When the container code fails or something bad happens with it, like out of\nmemory conditions, Kubernetes knows how to handle those cases. There is either\nthe restart of a container, or a crash of a Pod if it has \nand scheduling it on another node. Kubernetes has limited expressiveness on what\nis a failure (for example, non-zero exit code or liveness probe failure) and how\nto react on such a failure (mostly either Always restart or immediately fail the\nPod).</p><p>This level of expressiveness is often not enough for the complicated AI/ML\nworkloads. AI/ML pods are better rescheduled locally or even in-place as that\nwould save on image pulling time and device allocation. AI/ML pods are often\ninterconnected and need to be restarted together. This adds another level of\ncomplexity and optimizing it often brings major savings in running AI/ML\nworkloads.</p><p>There are various DIY solutions to handle Pod failures orchestration. The most\ntypical one is to wrap a main executable in a container by some orchestrator.\nAnd this orchestrator will be able to restart the main executable whenever the\njob needs to be restarted because some other pod has failed.</p><p>Solutions like this are very fragile and elaborate. They are often worth the\nmoney saved comparing to a regular JobSet delete/recreate cycle when used in\nlarge training jobs. Making these solutions less fragile and more streamlined\nby developing new hooks and extension points in Kubernetes will make it\neasy to apply to smaller jobs, benefiting everybody.</p><h3>Failure modes: device degradation</h3><p>Not all device failures are terminal for the overall workload or batch job.\nAs the hardware stack gets more and more\ncomplex, misconfiguration on one of the hardware stack layers, or driver\nfailures, may result in devices that are functional, but lagging on performance.\nOne device that is lagging behind can slow down the whole training job.</p><p>We see reports of such cases more and more often. Kubernetes has no way to\nexpress this type of failures today and since it is the newest type of failure\nmode, there is not much of a best practice offered by hardware vendors for\ndetection and third party tooling for remediation of these situations.</p><p>Typically, these failures are detected based on observed workload\ncharacteristics. For example, the expected speed of AI/ML training steps on\nparticular hardware. Remediation for those issues is highly depend on a workload needs.</p><p>As outlined in a section above, Kubernetes offers a lot of extension points\nwhich are used to implement various DIY solutions. The space of AI/ML is\ndeveloping very fast, with changing requirements and usage patterns. SIG Node is\ntaking a measured approach of enabling more extension points to implement the\nworkload-specific scenarios over introduction of new semantics to support\nspecific scenarios. This means prioritizing making information about failures\nreadily available over implementing automatic remediations for those failures\nthat might only be suitable for a subset of workloads.</p><p>This approach ensures there are no drastic changes for workload handling which\nmay break existing, well-oiled DIY solutions or experiences with the existing\nmore traditional workloads.</p><p>Many error handling techniques used today work for AI/ML, but are very\nexpensive. SIG Node will invest in extension points to make those cheaper, with\nthe understanding that the price cutting for AI/ML is critical.</p><p>The following is the set of specific investments we envision for various failure\nmodes.</p><h3>Roadmap for failure modes: K8s infrastructure</h3><p>The area of Kubernetes infrastructure is the easiest to understand and very\nimportant to make right for the upcoming transition from Device Plugins to DRA.\nSIG Node is tracking many work items in this area, most notably the following:</p><p>Basically, every interaction of Kubernetes components must be reliable via\neither the kubelet improvements or the best practices in plugins development\nand deployment.</p><h3>Roadmap for failure modes: device failed</h3><p>For the device failures some patterns are already emerging in common scenarios\nthat Kubernetes can support. However, the very first step is to make information\nabout failed devices available easier. The very first step here is the work in\n<a href=\"https://kep.k8s.io/4680\">KEP 4680</a> (Add Resource Health Status to the Pod Status for\nDevice Plugin and DRA).</p><p>Longer term ideas include to be tested:</p><ul><li>Integrate device failures into Pod Failure Policy.</li><li>Node-local retry policies, enabling pod failure policies for Pods with\nrestartPolicy=OnFailure and possibly beyond that.</li><li>Ability to  pod, including with the , so it can\nget a new device allocated.</li><li>Add device health to the ResourceSlice used to represent devices in DRA,\nrather than simply withdrawing an unhealthy device from the ResourceSlice.</li></ul><h3>Roadmap for failure modes: container code failed</h3><p>The main improvements to handle container code failures for AI/ML workloads are\nall targeting cheaper error handling and recovery. The cheapness is mostly\ncoming from reuse of pre-allocated resources as much as possible. From reusing\nthe Pods by restarting containers in-place, to node local restart of containers\ninstead of rescheduling whenever possible, to snapshotting support, and\nre-scheduling prioritizing the same node to save on image pulls.</p><p>Consider this scenario: A big training job needs 512 Pods to run. And one of the\npods failed. It means that all Pods need to be interrupted and synced up to\nrestart the failed step. The most efficient way to achieve this generally is to\nreuse as many Pods as possible by restarting them in-place, while replacing the\nfailed pod to clear up the error from it. Like demonstrated in this picture:</p><p>It is possible to implement this scenario, but all solutions implementing it are\nfragile due to lack of certain extension points in Kubernetes. Adding these\nextension points to implement this scenario is on the Kubernetes roadmap.</p><h3>Roadmap for failure modes: device degradation</h3><p>There is very little done in this area - there is no clear detection signal,\nvery limited troubleshooting tooling, and no built-in semantics to express the\n\"degraded\" device on Kubernetes. There has been discussion of adding data on\ndevice performance or degradation in the ResourceSlice used by DRA to represent\ndevices, but it is not yet clearly defined. There are also projects like\n<a href=\"https://github.com/medik8s/node-healthcheck-operator\">node-healthcheck-operator</a>\nthat can be used for some scenarios.</p><p>We expect developments in this area from hardware vendors and cloud providers, and we expect to see mostly DIY\nsolutions in the near future. As more users get exposed to AI/ML workloads, this\nis a space needing feedback on patterns used here.</p><p>The Kubernetes community encourages feedback and participation in shaping the\nfuture of device failure handling. Join SIG Node and contribute to the ongoing\ndiscussions!</p><p>This blog post provides a high-level overview of the challenges and future\ndirections for device failure management in Kubernetes. By addressing these\nissues, Kubernetes can solidify its position as the leading platform for AI/ML\nworkloads, ensuring resilience and reliability for applications that depend on\nspecialized hardware.</p>","contentLength":18095,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Angular Custom Profiling Track is now available","url":"https://blog.angular.dev/the-angular-custom-profiling-track-is-now-available-0f9d8d36218a?source=rss----447683c3d9a3---4","date":1751485086,"author":"Angular","guid":181151,"unread":true,"content":"<p>Profiling web applications can be a complex task, often requiring developers to juggle between different tools that present data in disconnected ways. Traditionally, Chrome’s performance panel is excellent for detailed function call analysis, while Angular DevTools offers a higher-level view based on framework concepts like components, lifecycle hooks, bindings etc. Unfortunately, having two separate tools leads to a fragmented understanding of performance.</p><p>This fragmentation presented a significant opportunity to improve the developer experience when profiling Angular applications. As a result, the Angular and Chrome teams have partnered to bring Angular-specific data and insights directly into the Chrome DevTools performance panel, creating a unified profiling experience. <strong>We’re excited to introduce the new custom track for Angular in Chrome DevTools.</strong></p><p>This integration allows developers to combine the benefits of both tools, offering a more synchronized and comprehensive view of their application’s performance.</p><h3>Getting to Know the New Custom&nbsp;Track</h3><p>With this new integration, you’ll find performance data using Angular concepts, such as application bootstrap, components, UI synchronization, and lifecycle hooks:</p><p>The custom track’s flame charts group function invocations together under corresponding components and other Angular concepts. You can drill down to individual functions for a more granular view when needed and still glean meaning from the groupings in other scenarios</p><p>One of the most significant benefits is the ability to distinguish between your application’s code, other scripts, or browser activities like layout and&nbsp;paint.</p><h3>Decoding the Colors: What Your Code is&nbsp;Doing</h3><p>The flame chart entries are color coded to help you quickly identify what’s happening in your application.</p><ul><li>: These represent Dependency Injection (DI) services instantiated during the application bootstrap process. In general, green signifies the execution of code written by application developers.</li></ul><ul><li>: This color is reserved for templates compiled by Angular. Even though it’s still your code, it has been transformed by Angular before execution in the browser. This allows you to clearly see which templates are creating or updating the DOM and how long these operations take.</li></ul><ul><li>: These mark the entry points. At the very top, you’ll see the trigger — why Angular decided to run application code. Subsequent blue bars represent all the components that need to perform work, which is particularly useful for understanding how user interactions impact DOM updates. Below component names, you’ll find the familiar purple (templates) and green (your component code).</li></ul><p>Since the custom track is interactive, clicking on an entry in the flame chart reveals more detailed information about a given entry. This data empowers developers to dive deeper into specific function calls and understand their impact on the application’s performance.</p><p>Enabling this powerful new feature is straightforward. Complete the following steps:</p><ol><li>Ensure you are using the latest version of Angular (v20 at the time of this post) and an up-to-date version of the Google Chrome&nbsp;browser.</li><li>Run your Angular application in developer mode.</li><li>With your application running, open Chrome DevTools and enable the custom track by typing ng.enableProfiling() in the&nbsp;console.</li><li>Once enabled, start recording a performance profile. The dedicated “Angular” track in the flame chart will be available.</li></ol><h3>More Performant Apps are on the&nbsp;Way</h3><p>This new integration with Chrome DevTools demonstrates our ongoing commitment to improving the developer experience within the Angular ecosystem. By providing tools that offer more focused and actionable insights, the Angular and Chrome teams are empowering developers to build faster, more efficient applications. Please try out this new integration and let us know what you&nbsp;think.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0f9d8d36218a\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"5 ways to transform your workflow using GitHub Copilot and MCP","url":"https://github.blog/ai-and-ml/github-copilot/5-ways-to-transform-your-workflow-using-github-copilot-and-mcp/","date":1751478242,"author":"Klint Finley","guid":180846,"unread":true,"content":"<p>Traditional AI coding assistants typically operate in isolation, limited to the code in your current workspace. Now with the introduction of the Model Context Protocol (MCP), AI development workflows are further evolving to incorporate more tools and context.&nbsp;</p><p>MCP can enable AI assistants to interact with external systems like knowledge bases, data stores, and testing applications.</p><p>The real value of MCP integration is that you can now perform tasks that previously required multiple tools, context switching, and manual effort—all directly in your IDE. That means you can save time, maintain focus, and ship code faster.</p><p>In this article, we’ll explore five practical ways MCP integrations with GitHub Copilot can streamline your workflow. We’ll follow a realistic scenario: implementing a secure JWT (JSON Web Token) authentication system for a web application, illustrating an end-to-end workflow with MCP.</p><h2>1. Using MCP to bridge design and development with Figma&nbsp;</h2><p>The gap between design and development has long been a source of friction in product teams. MCP provides a standardized way for GitHub Copilot to securely access and interpret design specifications directly.&nbsp;</p><p>Instead of manually translating design details into code, MCP enables Copilot to automatically retrieve exact design parameters—such as colors, spacing, typography, and component states—and generate accurate, ready-to-use code. This integration reduces guesswork and streamlines the handoff between designers and developers.</p><p>We’ll start developing our new JWT authentication system by taking a look at the user-facing side. Let’s say the design team updated the authentication UI components in Figma, including login forms, error states, loading spinners, and success messages. Now, you need to implement these changes to match the new design system.</p><p>Start by asking Copilot, “What are the latest design updates for the login form and authentication components?” It will then retrieve specs for the elements that need to change. Then you can prompt it to create React components for each element:</p><ul><li> with exact spacing, colors, typography</li><li> component with proper error styling</li><li> component</li></ul><p>Copilot will then give you ready-to-use code that maintains consistency with the design specifications from Figma.</p><h2>2. Tap into your Obsidian knowledge base with MCP</h2><p>When implementing complex features like JWT authentication, you often need to reference past decisions, architectural notes, and research findings scattered across your knowledge base. The unofficial, community-maintained Obsidian MCP server bridges this gap by connecting GitHub Copilot directly to your Obsidian vault.</p><p>Let’s say you’re implementing JWT token validation and need to understand your team’s previous security decisions. You tell Copilot: “Search for all files where JWT or token validation is mentioned and explain the context.”</p><ul><li>Search across all Markdown files in your vault for relevant security patterns</li><li>Retrieve contents from specific architecture decision records (ADR)</li><li>Access meeting notes from previous security reviews</li><li>Pull implementation guidelines from your team’s coding standards</li></ul><p>You might follow up with the following prompt: “Get the contents of the last architecture call note about authentication and summarize the key decisions.” Copilot will locate the relevant file and extract the critical information you need to inform your implementation approach.</p><p>Once you’ve gathered the necessary context, you can ask Copilot to synthesize this information: “Create a new note called ‘jwt-implementation-summary.md’ that combines our authentication standards with the new JWT approach.” Copilot will create this documentation directly in your vault, helping maintain your team’s knowledge base.</p><p>: This integration requires the community “Obsidian Local REST API” plugin and an API key.</p><p>With your research complete and documented, you can proceed to test your application.</p><h2>3. Test your code with Playwright</h2><p>Integrating MCP with Playwright transforms test creation from a manual, error-prone process into a simple, guided experience.</p><p>Modern web applications often involve complex user journeys, asynchronous operations, and dynamic content. Authentication flows are particularly challenging to test comprehensively.</p><p>Continuing with our JWT authentication system, you need to test the complete authentication flow including login, token refresh, and secure route access. To do this, you’ll start by giving Copilot a prompt like this: “Test the JWT authentication flow including login, automatic token refresh, and access to protected routes.”</p><p>From there, Copilot will analyze your authentication implementation and generate comprehensive test coverage. But it doesn’t stop there. Copilot then runs the tests with Playwright and provides immediate feedback on failures, suggesting fixes for common issues, like timing problems or selector changes.</p><h2>4. File pull requests faster</h2><p>Turning back to our JWT authentication example, you can prompt Copilot: “Create a pull request for my authentication feature changes”</p><p>Copilot will then analyze:</p><ul><li>Code changes across multiple files&nbsp;&nbsp;</li><li>Related issues and project context&nbsp;&nbsp;</li><li>Team review patterns and expertise areas&nbsp;&nbsp;</li><li>Previous similar implementations</li></ul><p>Copilot returns Markdown with an overview, changes made, a testing strategy, and even related issues.</p><p>It will then suggest appropriate reviewers for each aspect of the change based on code ownership, expertise mapping, and current workload.</p><p>Once your application is deployed, you can move on to monitoring it.</p><p>With the core authentication logic handled, now it’s time to ensure that our application performs well by monitoring how it behaves in production. Using MCP to connect to Grafana through the open-source Grafana MCP server makes this easier—though setup requires a few configuration steps.</p><p>Let’s say you need to analyze the JWT authentication system’s latency metrics and error rates. You tell Copilot: “Show me auth latency and error-rate panels for the auth-service dashboard for the last 6 hours.”</p><p>After configuring the Grafana MCP server with your API key and host URL, Copilot can then query your Grafana instance to:</p><ul><li>Examine authentication latency metrics and p95 response times</li><li>Analyze error rates for login endpoints over time</li><li>Review existing alert rules for authentication services</li><li>Identify patterns in failed authentication attempts</li></ul><p>Copilot returns panel data as base64-encoded images and can extract raw time-series data when needed. If you need a longer time range, you can specify: “Show me the same metrics for the last 24 hours” and Copilot will adjust the query parameters accordingly.</p><p>For more advanced monitoring workflows, you can enable write operations by launching the server with the  flag and an Editor-role API key. This allows Copilot to create new alert rules or modify dashboard configurations based on your authentication metrics analysis.</p><p>Before diving into these powerful integrations, you’ll need to configure your development environment. Here’s how:</p><ol><li>: Enable MCP support in your IDE through official extensions</li><li>: Set up authentication for each service (GitHub, Obsidian, Figma, etc.)</li><li><strong>Define context boundaries</strong>: Establish what information should be accessible to AI</li><li>: Implement proper access controls and data privacy measures</li></ol><ul><li>: Begin with one integration and gradually expand your usage</li><li>: Keep your knowledge bases and documentation current for optimal AI assistance</li><li><strong>Regularly review Copilot’s outputs</strong>: Periodically audit AI-generated suggestions to ensure quality and security</li><li>: Ensure your team understands and adopts consistent MCP usage patterns</li></ul><p>The five integration patterns we’ve explored represent just the beginning of what’s possible. As MCP’s ecosystem grows, new tools and integrations will continue to expand what’s possible.</p>","contentLength":7862,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Driving Content Delivery Efficiency Through Classifying Cache Misses","url":"https://netflixtechblog.com/driving-content-delivery-efficiency-through-classifying-cache-misses-ffcf08026b6c?source=rss----2615bd06b42e---4","date":1751469623,"author":"Netflix Technology Blog","guid":180639,"unread":true,"content":"<p><a href=\"https://openconnect.netflix.com/en/#what-is-open-connect\"></a><em>, our dedicated content delivery network (CDN), is to deliver the best quality of experience (QoE) to our members. By localizing our Open Connect Appliances (OCAs), we bring Netflix content closer to the end user. This is achieved through close partnerships with internet service providers (ISPs) worldwide. Our ability to efficiently localize traffic, known as Content Delivery Efficiency, is a critical component of Open Connect’s service.</em></p><p><em>In this post, we discuss one of the frameworks we use to evaluate our efficiency and identify sources of inefficiencies. Specifically, we classify the causes of traffic not being served from local servers, a phenomenon that we refer to as cache&nbsp;misses.</em></p><h4>Why does Netflix have the Open Connect&nbsp;Program?</h4><p>The Open Connect Program is a cornerstone of Netflix’s commitment to delivering unparalleled QoE for our customers. By localizing traffic delivery from Open Connect servers at IX or ISP sites, we significantly enhance the speed and reliability of content delivery. The inherent latencies of data traveling across physical links, compounded by Internet infrastructure components like routers and network stacks, can disrupt a seamless viewing experience. Delays in video start times, reduced initial video quality, and the frustrating occurrence of buffering lead to an overall reduction in customer QoE. Open Connect empowers Netflix to maintain hyper-efficiency, ensuring a flawless client experience for new, latency-sensitive, on-demand content such as live streams and&nbsp;ads.</p><p>Our custom-built servers, known as Open Connect Appliances (OCAs), are designed for both efficiency and cost-effectiveness. By logging detailed historical streaming behavior and using it to model and forecast future trends, we hyper-optimize our OCAs for long-term caching efficiency. We build methods to efficiently and reliably store, stream, and move our&nbsp;content.</p><p>The mission of Open Connect hinges on our ability to effectively localize content on our OCAs globally, despite limited storage space, and also by design with specific storage sizes. This ensures that our cost and power efficiency metrics continue to improve, enhancing client QoE and reducing costs for our ISP partners. A critical question we continuously ask is: How do we evaluate and monitor which bytes should have been served from local OCAs but resulted in a cache&nbsp;miss?</p><p><strong>The Anatomy of a Playback&nbsp;Request</strong></p><p>Let us start by introducing the logic that directs or “steers” a specific Netflix client device to its dedicated OCA. The lifecycle from when a client device presses play until the video starts being streamed to that device is referred to as “playback.” Figure 1 illustrates the logical components involved in playback.</p><p> Components for&nbsp;Playback</p><p>The components involved in playback are important to understand as we elaborate on the concept of how we determine a cache miss versus hit. Independent of client requests, every OCA in our CDN periodically reports its capacity and health, learned BGP routes, and current list of stored files. All of this data is reported to the Cache Control Service (CCS). When a member hits the play button, this request is sent to our AWS services, specifically the Playback Apps service. After Playback Apps determines which files correspond to a specific movie request, it issues a request to “steer” the client’s playback request to OCAs via the Steering Service. The Steering Service in turn, using the data reported from OCAs to CCS as well as other client information such as geo location, identifies the set of OCAs that can satisfy that client’s request. This set of OCAs is then returned in the form of rank-ordered URLs to the client device, the client connects to the top-ranked OCA and requests the files it needs to begin the video&nbsp;stream.</p><p>A cache miss occurs when bytes are not served from the best available OCA for a given Netflix client, independent of OCA state. For each playback request, the Steering Service computes a ranked list of local sites for the client, ordered by network proximity alone. This ranked list of sites is known as the “proximity rank.” Network proximity is determined based on the IP ranges (BGP routes) that are advertised by our ISP partners. Any OCA from the first “most proximal” site on this list is the most preferred and closest, having advertised the longest, most specific matching prefix to the client’s IP address. A cache miss is logged when bytes are not streamed from any OCA at this first local site, and we log when and why that&nbsp;happens.</p><p>It is important to note that our concept of cache misses is viewed from the client’s perspective, focusing on the optimal delivery source for the end user and prepositioning content accordingly, rather than relying on traditional CDN proxy caching mechanisms. Our “prepositioning” differentiator allows us to prioritize client QoE by ensuring content is served from the most optimal&nbsp;OCA.</p><p>We attribute cache misses to three logical categories. The intuition behind the delineated categories is that each category informs parallel strategies to achieve content delivery efficiency.</p><ul><li> This happens when the files were not found on OCAs in the local site. In previous articles like “<a href=\"https://netflixtechblog.com/content-popularity-for-open-connect-b86d56f613b\">Content Popularity for Open Connect</a>” and “<a href=\"https://netflixtechblog.com/distributing-content-to-open-connect-3e3e391d4dc9\">Distributing Content to Open Connect</a>,” we discuss how we decide what content to prioritize populating first onto our OCAs. A sample of efforts this insights informs include: (1) how accurately we predict the popularity of content, (2) how rapidly we pre-position that content, (3) how well we design our OCA hardware, and (4) how well we provision storage capacity at our locations of presence.</li><li> This happens when the local site’s OCA hardware resources are becoming saturated, and one or more OCA can not handle more traffic. As a result, we direct clients to other OCAs with capacity to serve that content. Each OCA has a control loop that monitors its bottleneck metrics (such as CPU, disk usage, etc.) and assesses its ability to serve additional traffic. This is referred to as “OCA health.” Insight into health misses informs efforts such as: (1) how well we load balance traffic across OCAs with heterogeneous hardware resources, (2) how well we provision enough copies of highly popular content to distribute massive traffic, which is also tied to how accurately we predict the popularity of content, and (3) how well we preposition content to specific hardware components with varying traffic serve capabilities and bottlenecks.</li></ul><p>Next we will dig into the framework we built to log and compute these metrics in real-time, with some extra attention to technical detail.</p><h4>Cache Miss Computation Framework</h4><p>There are two critical data components that we log, gather, and analyze to compute cache&nbsp;misses:</p><ul><li><strong>Steering Playback Manifest Logs:</strong> Within the Steering Service, we compute and log the ranked list of sites for each client request, i.e. the “proximity rank” introduced earlier. We also enrich that list with information that reflects the logical decisions and filters our algorithms applied across all proximity ranks given that point-in-time state of our systems. This information allows us to replay/simulate any hypothetical scenario easily, such as to evaluate whether an outage across all sites in the first proximity rank would overwhelm sites in the second proximity rank, and many more such scenarios!</li><li> Once a Netflix client connects with an OCA to begin video streaming, the OCAs log any data regarding that streaming session, such as the files streamed and total bytes. All OCA logs are consolidated to identify which OCA(s) each client actually watched its video stream from, and the amount of content streamed.</li></ul><p>The above logs are joined for every Netflix client’s playback request to compute detailed cache miss metrics (in bytes and hours streamed) at different aggregation levels (such as per OCA, movie, file, encode type, country, and so&nbsp;on).</p><p>Figure 2 outlines how the logging components fit into the general engineering architecture that allows us to compute content miss metrics at low-latency and almost real-time.</p><p> Components of the cache miss computation framework.</p><p>We will now describe the system requirements of each component.</p><ol><li>: The logs for computing cache miss are emitted to Kafka clusters in each of our evaluated AWS regions, enabling us to send logs with the lowest possible latency. After a client device makes a playback request, the Steering Service generates a <em>steering playback manifest</em>, logs it, and sends the data to a Kafka cluster. Kafka is used for event streaming at Netflix because of its high-throughput event processing, low latency, and reliability. After the client device starts the video stream from an OCA, the OCA stores information about the bytes served for each file requested by each unique client playback stream. This data is what we refer to as .</li><li>: The logs emitted by the Steering Service and the OCAs can result in data for a single playback request being distributed across different AWS regions, because logs are recorded in geographically distributed Kafka clusters.  might be stored in one region’s Kafka cluster while <em>steering playback manifest logs</em> are stored in another. One approach to consolidate data for a single playback is to build complex many-to-many joins. In streaming pipelines, performing these joins requires replicating logs across all regions, which leads to data duplication and increased complexity. This setup complicates downstream data processing and inflates operational costs due to multiple redundant cross-region data transfers. To overcome these challenges, we perform a cross-region transfer only once, consolidating all logs into a single&nbsp;region.</li><li>: We enrich the logs during streaming joins with metadata using various slow-changing dimension tables and services so that we have the necessary information about the OCA and the played&nbsp;content.</li><li><strong>Streaming Window-Based Join</strong>: We perform a streaming window-based join to merge the <em>steering playback manifest logs</em> with the . Performing enrichment and log consolidation upstream allows for more seamless and un-interrupted joining of our log data&nbsp;sources.</li><li>: After joining the logs, we compute the cache miss metrics. The computation checks whether the client played content from an OCA in the first site listed in the <em>steering playback manifest</em>’s proximity rank or from another site. When a video stream occurs at a higher proximity rank, this indicates that a cache miss occurred.</li></ol><h3>Data Model to Evaluate Cache&nbsp;Misses</h3><p>One of the most exciting opportunities we have enabled through these logs (in these authors’ opinions) is the ability to replay our logic offline and in simulations with variable parameters, to reproduce impact in production under different conditions. This allows us to test new conditions, features, and hypothetical scenarios without impacting production Netflix&nbsp;traffic.</p><p>To achieve the above, our data should satisfy two main conditions. First, the data should be comprehensive in representing the state of each distinct logical step involved in steering, including the decisions and their reasons. In order to achieve this, the underlying logic, here the Steering Service, needs to be built in a modularized fashion, where each logical component overlays data from the prior component, resulting in a rich blurb representing the system’s full state, which is finally logged. This all needs to be achieved without adding perceivable latency to client playback requests! Second, the data should be in a format that allows near-real-time aggregate metrics for monitoring purposes.</p><p>Some components of our final, joined data model that enables us to collect rich insights in a scalable and timely manner are listed in Table&nbsp;1.</p><p><strong>Table 1: Unified Data Model after joining <em>steering playback manifest</em> and .</strong></p><h4>Cache Miss Computation Sample</h4><p>Let us share an example of how we compute cache miss metrics. For a given unique client play request, we know we had a cache miss when the client streams from an OCA that is not in the client’s first proximity rank. As you can see from Table 1, each file needed for a client’s video streaming session is linked to routable OCAs and their corresponding sites with a proximity rank. These are 0 based indexes with proximity rank zero indicating the most optimal OCA for the client. “Proximity Rank Zero” indicates that the client connected to an OCA in the most preferred site(s), thus no misses occurred. Higher proximity ranks indicate a miss has occurred. The aggregation of all bytes and hours streamed from non-preferred sites constitutes a missed opportunity for Netflix and are reported in our cache miss&nbsp;metrics.</p><p><strong>Decision Labels and Bytes&nbsp;Sent</strong></p><p>Sourced from the <em>steering playback manifest logs</em>, we record why we did not select an OCA for playback. These are denoted&nbsp;by:</p><ul></ul><p><strong>Metrics Calculation and Categorization</strong></p><p>For each file needed for a client’s video streaming session, we can categorize the bytes streamed by the client into different types of&nbsp;misses:</p><ul><li>No Miss: If proximity rank is zero, bytes were streamed from the optimal&nbsp;OCA.</li><li>Health Miss (“H”): Miss due to the OCA reporting high utilization.</li><li>Content Miss (“C”): Miss due to the OCA not having the content available locally.</li></ul><h4>How are miss metrics used to monitor our efficiency?</h4><p>Open Connect uses cache miss metrics to manage our Open Connect infrastructure. One of the team’s goals is to reduce the frequency of these cache misses, as they indicate that our members are being served by less proximal OCAs. By maintaining a detailed set of metrics that reveal the reasons behind cache misses, we can set up alerts to quickly identify when members are streaming from suboptimal locations. This is crucial because we operate a global CDN with millions of members worldwide and tens of thousands of&nbsp;servers.</p><p>The figure below illustrates how we track the volume of total streaming traffic alongside the proportion of traffic streamed from less preferred locations due to content shedding. By calculating the ratio of content shed traffic to total streamed traffic, we derive a content shed&nbsp;ratio:</p><p>content shed ratio = content shed traffic total streamed&nbsp;traffic</p><p>This active monitoring of content shedding allows us to maintain a tight feedback loop to ensure the efficacy of our deployment and prediction algorithms, streaming traffic, and the QoE of our members. Given that content shedding can occur for multiple reasons, it is essential to have clear signals indicating when it happens, along with known and automated remediation strategies, such as mechanisms to quickly deploy mispredicted content onto OCAs. When special intervention is necessary to minimize shedding, we use it as an opportunity to enhance our systems as well as to ensure they are comprehensive in considering all known failure&nbsp;cases.</p><p>Open Connect’s unique strategy requires us to be incredibly efficient in delivering content from our OCAs. We closely track miss metrics to ensure we are maximizing the traffic our members stream from most proximal locations. This ensures we are delivering the best quality of experience to our members globally.</p><p>Our methods for managing cache misses are evolving, especially with the introduction of new streaming types like Live and Ads, which have different streaming behaviors and access patterns compared to traditional video. We remain committed to identifying and seizing opportunities for improvement as we face new challenges.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ffcf08026b6c\" width=\"1\" height=\"1\" alt=\"\">","contentLength":15506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AV1 @ Scale: Film Grain Synthesis, The Awakening","url":"https://netflixtechblog.com/av1-scale-film-grain-synthesis-the-awakening-ee09cfdff40b?source=rss----2615bd06b42e---4","date":1751466104,"author":"Netflix Technology Blog","guid":180638,"unread":true,"content":"<h4><em>Unleashing Film Grain Synthesis on Netflix and Enhancing Visuals for&nbsp;Millions</em></h4><p><strong>Picture this: you’re watching a classic film, and the subtle dance of film grain adds a layer of authenticity and nostalgia to every scene.</strong> This grain, formed from tiny particles during the film’s development, is more than just a visual effect. It plays a key role in storytelling by enhancing the film’s depth and contributing to its realism. However, film grain is as elusive as it is beautiful. Its random nature makes it notoriously difficult to compress. Traditional compression algorithms struggle to manage it, often forcing a choice between preserving the grain and reducing file&nbsp;size.</p><p>In the digital age, noise remains a ubiquitous element in video content. Camera sensor noise introduces its own characteristics, while filmmakers often add intentional grain during post-production to evoke mood or a vintage feel. These elements create a visually rich experience that tests conventional compression methods.</p><p>We’re giving members globally a transformed streaming experience with the recent rollout of AV1 Film Grain Synthesis (FGS) streams. While FGS has been part of the AV1 standard since its inception, we only enabled it for a limited number of titles during <a href=\"https://netflixtechblog.com/bringing-av1-streaming-to-netflix-members-tvs-b7fc88e42320\">our initial launch of the AV1 codec in 2021</a>. Now, we’re enabling this innovative technology at scale, leveraging it to preserve the artistic integrity of film grain while optimizing data efficiency. In this blog post, we’ll explore how FGS revolutionizes video streaming and enhances your viewing experience.</p><h3>Understanding Film Grain Synthesis in&nbsp;AV1</h3><p>The AV1 Film Grain Synthesis tool models film grain through two key components, with model parameters estimated before the encoding of the denoised&nbsp;video:</p><p>: an <em>auto-regressive (AR) model</em> is used to replicate the pattern of film grain. The key parameters are the AR coefficients, which can be estimated from the residual between the source video and the denoised video, essentially capturing the noise. This model captures the spatial correlation between the grain samples, ensuring that the noise characteristics of the original content are accurately preserved. By adjusting the auto-regressive coefficients {ai}, the model can control the grain’s shape, making it appear coarser or finer. With these coefficients, a 64x64 noise template is generated, as illustrated in the animation below. To construct the noise layer during playback, random 32x32 patches are extracted from the 64x64 noise template and added to the decoded&nbsp;video.</p><p>: a  is employed to control the grain’s appearance under varying lighting conditions. This function, estimated during the encoding process, models the relationship between pixel value and noise intensity using a piecewise linear function. This allows for precise adjustments to the grain strength based on video brightness and color. Consequently, the film grain strength is adapted to the areas of the picture, closely recreating the look of the original video. The animation below demonstrates how the grain intensity is adjusted by the scaling function:</p><p>With these models specified by AV1 standard, the encoding process first removes the film grain from the video. The standard does not mandate a specific method for this step, allowing users to choose their preferred denoiser. Following the denoising, the video is compressed, and the grain’s pattern and intensity are estimated and transmitted alongside the compressed video data. During playback, the film grain is recreated and reintegrated into the video using a block-based method. This approach is optimized for consumer devices, ensuring smooth playback and high-quality visuals. For a more detailed explanation, please refer to the <a href=\"https://norkin.org/pdf/DCC_2018_AV1_film_grain.pdf\">original&nbsp;paper</a>.</p><p>By combining these components, the AV1 Film Grain Synthesis tool preserves the artistic integrity of film grain while making the content “easier to compress” by denoising the source video prior to encoding. This process enables high-quality video streaming, even in content with heavy grain, resulting in significant bitrate savings and improved visual&nbsp;quality.</p><h3>Visual Quality Improvement, Bitrate Reduction, and Member&nbsp;Benefits</h3><p>In our pursuit of premium streaming quality, enabling AV1 Film Grain Synthesis has led to significant bitrate reduction, allowing us to deliver high-quality video with less data while preserving the artistic integrity of film grain. Below, we showcase visual examples highlighting the improved quality and reduced bitrate, using a frame from the Netflix title <a href=\"https://www.netflix.com/title/80996324\"></a>:</p><p>The visual comparison highlights a significant bitrate reduction of approximately 66%, with regular AV1 encoding at 8274 kbps compared to AV1 with FGS at 2804 kbps. In this example, which features strong film grain, it may be observed that the regular version exhibits distorted noise with a discrete cosine transform (DCT)-like pattern. In contrast, the FGS version preserves the integrity of the film grain at a lower&nbsp;bitrate.</p><p>Additionally, synthesized noise effectively masks compression artifacts, resulting in a more visually appealing experience. In this comparison below, both the regular AV1 stream and the AV1 FGS stream without synthesized noise (equivalent to compressing the denoised video) show compression artifacts. In contrast, the AV1 FGS stream with grain synthesis (rightmost figure) improves visual quality through contrast masking in human visual systems. The added film grain, a form of mask, effectively conceals some compression artifacts.</p><p>Currently, we lack a dedicated quality model for film grain synthesis. The noise appearing at different pixel locations between the source and decoded video poses challenges for pixelwise comparison methods like <a href=\"https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\">PSNR</a> or <a href=\"https://netflixtechblog.com/toward-a-practical-perceptual-video-quality-metric-653f208b9652\">VMAF</a>, leading to penalized quality scores. Despite this, our internal assessment highlights the improvements in visual quality and the value of these advancements.</p><p>To evaluate the impact of AV1 Film Grain Synthesis, we selected approximately 300 titles from the Netflix catalog, each with varying levels of graininess. The bar chart below illustrates a 36% reduction in average bitrate for resolutions of 1080p and above when AV1 film grain synthesis is enabled, highlighting its efficacy in optimizing data usage. For resolutions below 1080p, the reduction in bitrate is relatively small, reaching only a 10% decrease, likely because noise is filtered out during the downscaling process. Furthermore, enabling the film grain synthesis coding tool consistently introduces syntax overhead to the bitstream.</p><p>Finally, we conducted A/B testing prior to rollout to understand the overall streaming impact of enabling AV1 Film Grain Synthesis. This testing showcased a smoother and more reliable Quality of Experience (QoE) for our members. The improvements include:</p><ul><li><strong>Lower Initial and Average Bitrate</strong>: Bitrate at the start of the playback reduced by 24% and average bitrate by 31.6%, lower network bandwidth requirements and reduced storage needs for downloaded streams.</li><li><strong>Decreased Playback Errors</strong>: Playback error rate reduced by approximately 3%.</li><li>: 10% fewer rebuffers and a 5% reduction in rebuffer duration resulting from the lower&nbsp;bitrate.</li><li>: Start play delay reduced by 10%, potentially due to the lower bitrate, which may help devices reach the target buffer level more&nbsp;quickly.</li><li><strong>Improved Playback Stability</strong>: Observed 10% fewer noticeable bitrate drops and a 10% reduction in the time users spend adjusting their playback position during video playback, likely influenced by reduced bitrate and rebuffering.</li><li><strong>Higher Resolution Streaming</strong>: About 0.7% of viewing hours shifted from lower resolutions (≤ 1080p) to 2160p on 4K-capable devices. This shift is attributed to reduced bitrates at switching points, which make it easier to achieve the highest resolution during a&nbsp;session.</li></ul><h3>Behind the Scenes: Our Film Grain Adventure Continues</h3><p>We’re always excited to share our progress with the community. This blog provides an overview of our journey: from the initial launch of the AV1 codec to the recent addition of Film Grain Synthesis (FGS) streams, highlighting the impact these innovations have on Netflix’s streaming quality. Since March, we’ve been rolling out FGS across scale, and many users can now enjoy the FGS-enabled streams, provided their device supports this feature. We encourage you to watch some of the author’s favorite titles <a href=\"https://www.netflix.com/title/81985186\">The Hot Spot</a>, <a href=\"https://www.netflix.com/title/70018511\">Kung Fu Cult Master</a>, <a href=\"https://www.netflix.com/title/70043379\">Initial D</a>, <a href=\"https://www.netflix.com/title/70005331\">God of Gamblers II</a>, <a href=\"https://www.netflix.com/title/80203996\">Baahubali 2: The Conclusion</a>, or <a href=\"https://www.netflix.com/title/81487660\">Dept. Q</a> (you may need to toggle off HDR from the settings menu) on Netflix to experience the new FGS streams firsthand.</p><p>This achievement is the result of a collaborative effort among several Open Connect teams at Netflix, including Video Algorithms, Media Encoding Pipeline, Media Foundations, Infrastructure Capacity Planning, and Open Connect Control Plane. We also received invaluable support from Client &amp; Partner Technologies, Streaming &amp; Discovery Experiences, Media Compute &amp; Storage Infrastructure, Data Science &amp; Engineering, and the Global Production Technology team. We would like to express our sincere gratitude to the following individuals for their contributions to the project’s success:</p><ul><li>Prudhvi Kumar Chaganti and Ken Thomas for the discussion and assistance on rollout&nbsp;strategy</li><li>Poojarani Chennai Natarajan, Lara Deek&nbsp;, Ivan Ivanov, and Ishaan Shastri for their essential support in planning and operations for Open&nbsp;Connect.</li><li>Alex Chang for his support in everything related to data analysis, and Jessica Tweneboah and Amelia Taylor for their assistance with AB&nbsp;testing.</li><li>David Zheng, Janet Xue, Scott Bolter, Brian Li, Allan Zhou, Vivian Li, Sarah Kurdoghlian, Artem Danylenko, Greg Freedman, and many other dedicated team members played a crucial role in device certification and collaboration with device partners. Their efforts significantly improved compatibility across platforms. (Spoiler alert: this was one of the biggest challenges we faced for productizing AV1&nbsp;FGS!)</li><li>Javier Fernandez-Ivern and Ritesh Makharia expertly managed the playback&nbsp;logic</li><li>Joseph McCormick and JD Vandenberg for providing valuable insights from a content production point of view, and Alex ‘Ally’ Michaelson for assisting in monitoring customer&nbsp;service.</li><li>A special thanks to Roger Quero, who played a key role in supporting various aspects of the project and contributed significantly to its overall success while he was at&nbsp;Netflix.</li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ee09cfdff40b\" width=\"1\" height=\"1\" alt=\"\">","contentLength":10412,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From idea to PR: A guide to GitHub Copilot’s agentic workflows","url":"https://github.blog/ai-and-ml/github-copilot/from-idea-to-pr-a-guide-to-github-copilots-agentic-workflows/","date":1751396242,"author":"Chris Reddington","guid":179176,"unread":true,"content":"<p>I got into software to ship ideas, not to chase down hard-coded strings after a late-breaking feature request. Unfortunately, many of our day-to-day tasks as developers involve branches working on boilerplate code, refactoring, and the “pre-work” to get to the fun stuff: shipping new features.</p><p>So I turned to Copilot’s agentic workflows to help speed along some of that grunt work. In my latest  live stream, I put that theory to the test in a project where I wanted to localize an application that used:</p><ul><li> a  web app and a matching  iOS app living in two separate GitHub repos.</li><li> spun up rapidly in  (on-demand dev environment) and  for the mobile portion.</li><li> an issue built from a couple of paragraphs to “Add English, French, and Spanish localization.”</li></ul><p>By the end of my stream, that idea became a GitHub issue, which turned into a fully tested, review-ready PR while I fielded chat questions, and learned about the preview custom chat mode features in VS Code.</p><h2>Why I use agentic workflows</h2><p>Even seasoned developers and teams still burn hours on jobs like:</p><ul><li>Turning vague requests into well-scoped issues</li><li>Hunting down every file in a cross-cutting refactor</li><li>Writing the same unit-test scaffolding again and again</li></ul><p>Copilot’s ability to create issues, along with its coding agent, custom chat modes in VS Code, and the new remote MCP backend fold those chores into one tight loop—issue to PR—while you stay firmly in the driver’s seat. You still review, tweak, and decide when to merge, but you skip the drudgery.</p><h2>Key capabilities covered in this livestream&nbsp;</h2><figure><table><thead><tr></tr></thead><tbody><tr><td>Turns any GitHub Issue you assign to Copilot into a PR, and works on that task asynchronously.</td><td>Allows you to offload the boilerplate work while you focus on reviews and edge case logic.</td></tr><tr><td>Create issues with Copilot</td><td>Converts a natural-language prompt into a well-structured Issue with title, body, acceptance criteria, and file hints.</td><td>Saves PM/eng refining and sets team members, or Copilot coding agent, up with the context they need to work effectively.</td></tr><tr><td>Custom chat modes (in preview in VS Code)</td><td>Lets you script repeatable AI workflows (e.g., , , ) that appear alongside the default  /  /  chat modes.</td><td>Allows you to package instructions and relevant tools for easier use, helping your team follow similar conventions.</td></tr><tr><td>Allows AI tools to access live GitHub context and tools, like issues, pull requests and code files. With the remote GitHub MCP server, you don’t need to install it locally, and can even authenticate with OAuth 2.0.</td><td>Provides a smooth experience to accessing the GitHub MCP server, reducing the management overhead of a local server.</td></tr><tr><td>Copilot agent mode is a real‑time collaborator that sits in your editor, works with you, and edits files based on your needs. Unlike the coding agent, Copilot agent mode works synchronously with you.</td><td>Think of agent mode as the senior dev pair programming with you. It has access to several tools (like reading/writing code, running commands in the terminal, executing tools on MCP servers), and works alongside you.</td></tr></tbody></table></figure><ol><li>A GitHub repo you can push to</li><li>A Copilot subscription with  enabled. (Did you know it’s now available for all paid tiers of GitHub Copilot including <a href=\"https://github.blog/changelog/2025-06-24-github-copilot-coding-agent-is-now-available-for-copilot-business-users/\">Copilot Business</a> and <a href=\"https://github.blog/changelog/2025-06-25-github-copilot-coding-agent-is-available-for-copilot-pro-users-in-public-preview/\">Copilot Pro</a>?)</li><li>VS Code 1.101+ with the latest Copilot extension.</li><li>Either:  (update your MCP configuration), or a local GitHub MCP server.</li></ol><h2>Walk-through: localizing a Next.js app</h2><p>Here’s the exact flow I demoed on the most recent  stream.</p><h3>1. Capture the request as a GitHub Issue</h3><p>Go to the <a href=\"https://github.com/copilot\">immersive view of Copilot Chat</a>. At the bottom of the page, in the “Ask Copilot” box, describe what you want. For example, below is the prompt that I used.&nbsp;</p><pre><code>Create a GitHub Issue that brings i11n capability to the application. We must support English, French and Spanish.\n\nThe user must be able to change their language in their profile page. When they change the language, it must apply immediately across the site.\n\nPlease include an overview/problem statement in the issue, a set of acceptance criteria, and pointers on which files need updating/creating.</code></pre><p>Copilot drafts that into an issue, which includes a title, acceptance criteria, and a loose action plan. From there, you can assign that issue to Copilot, and let it cook in the background.&nbsp;</p><h3>2. Let the coding agent turn the issue into a PR</h3><p>Shortly after assignment, the coding agent:</p><ol><li>Reviews the task at hand, explores the current state of the codebase, and forms a plan to complete the task.</li><li>If you have <a href=\"https://docs.github.com/en/copilot/how-tos/agents/copilot-coding-agent/best-practices-for-using-copilot-to-work-on-tasks#adding-custom-instructions-to-your-repository\">any custom instructions configured</a>, then the coding agent will also use those as context. For example, we specify that npm run lint and npm run test should pass before committing.</li><li>Once complete, it opens a draft PR for your review.</li></ol><p>While that runs, you can keep coding, use it as an opportunity to learn (like we learned about custom chat modes) or grab a coffee.</p><h3>3. Review the PR like you normally would</h3><p>Whether it’s a colleague, collaborator, or Copilot writing the code, you still need a reviewer. So it’s important to make sure you look the code over carefully, just like you would any other pull request.</p><ol><li>Start by reviewing the body of the pull request, which Copilot will have helpfully kept up to date.</li><li>Then, review the code changes in the files changed tab, understanding what has changed and why. I also like to take a look at the coding agent session to understand the approach Copilot took to solving the problem.</li><li>Once you are comfortable, you may want to try the code out manually in a GitHub Codespace. Or, you may want to run any existing CI checks through your GitHub Actions workflows. But again, make sure you have carefully reviewed the code before executing it.</li><li>All being well, you will have green check marks being returned from your CI.&nbsp;</li></ol><p>However, there’s always a possibility that you encounter failures, or spot some changes in your manual testing. For example, I spotted some hard-coded strings that the agent hadn’t addressed. Once again, we approach this just like we would any other pull request. We can post our feedback in a comment. For example, here’s the comment I used:</p><p>That’s a great start. However, there are a lot of pages which are hardcoded in English still. For example, the flight search/bookings page, the check reservation page. Can you implement the localization on those pages, please?</p><p>Copilot will react to the comment once again, and get to work in another session.&nbsp;</p><h2>Level up your workflows with custom chat modes</h2><ol><li>Open the command palette in Visual Studio Code</li><li>Select <strong>Create new custom chat mode file</strong>.You’ll be asked to save it either in the workspace (to allow collaborating with others), or in the local user data folder (for your use). We opted for the workspace option.</li><li>Enter the name. This is the name that will appear in the chat mode selection box, so pay attention to any capitalization.</li><li>You should see a new file has been created with the extension . This is where you can configure the instructions, and the available tools for your new custom chat mode.</li></ol><p>Below is the example that we used in the livestream, slightly modified from the VS Code team’s docs example. We’ve added the create_issue tool to the list of allowed tools, adjusted our expectations of what’s included in the issue and added an instruction about creating the issue with the `create_issue` tool once revisions are complete and approved by the user.</p><pre><code>---\n\ndescription: Generate an implementation plan for new features or refactoring existing code.\n\ntools: ['codebase', 'fetch', 'findTestFiles', 'githubRepo', 'search', 'usages', 'github', 'create_issue']\n\n---\n\n# Planning mode instructions\n\nYou are in planning mode. Your task is to generate an implementation plan for a new feature or for refactoring existing code.\n\nDon't make any code edits, just generate a plan.\n\nThe plan consists of a Markdown document that describes the implementation plan, including the following sections:\n\n* Overview: A brief description of the feature or refactoring task.\n\n* Requirements: A list of requirements for the feature or refactoring task.\n\n* Implementation Steps: A detailed list of steps to implement the feature or refactoring task.\n\n* Testing: A list of tests that need to be implemented to verify the feature or refactoring task.\n\nOnce the plan is complete, ask the user if they would like to create a GitHub issue for this implementation plan. If they respond affirmatively, proceed to create the issue using the `create_issue` tool.</code></pre><p>When the file is available in your teammate’s local repositories (so they’ve pulled the changes locally), VS Code surfaces the mode in the chat dropdown, allowing you to configure chat modes that are consistent and convenient across your team.</p><h2>Remote MCP: removing the local setup</h2><p>You may be used to running MCP locally through npm packages or as docker containers. However, remote MCP servers allow you to reduce the management overhead of running these tools locally. There may be other benefits too. For example, the remote GitHub MCP Servers allows you to authenticate using OAuth 2.0 instead of Personal Access Tokens.</p><p>To use the GitHub Remote MCP Server in VS Code, you’ll need to update the MCP configuration. You can find the instructions on how to do that <a href=\"https://github.com/github/github-mcp-server?tab=readme-ov-file#remote-github-mcp-server\">in the GitHub MCP Server repository</a>.</p><h2>Going mobile: Copilot agent mode in Xcode</h2><p>While we didn’t show it in depth, I quickly walked through one of my previous agent mode sessions in Xcode. It showed how I gave a similar prompt to Copilot, asking to add internationalization to the app, which we were able to see in the main navigation bar of the app running in the simulator.</p><pre><code>We need to implement internationalization in the app. Please make the following changes:\n\n1. The user can select from suported languages (English, Spanish, French) from a dropdown in their profile.\n\n2. The main tab view should support internationalization. No other parts of the app should be changed for now.\n\n3. When the user changes the language, it should update the rendered text instantly.</code></pre><figure><table><tbody><tr><td>Keep issues tightly scoped</td><td>Ask the agent to “re-architect the app”</td></tr><tr><td>Provide acceptance criteria</td><td>Assume the agent knows your intent</td></tr><tr><td>Carefully review the changes made</td><td>Execute code or merge a PR without a review</td></tr><tr><td>Iterate with Copilot. How often do you get something right on the first shot?</td><td>Expect perfection first time</td></tr></tbody></table></figure><p>Agentic workflows within GitHub Copilot aren’t magic; they’re tools. When a single click can help reduce technical debt (or knock out any other repetitive task you dread), why not let Copilot handle the boilerplate while you tackle the more challenging, fun, and creative problems?</p>","contentLength":10516,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An inside look at Meta’s transition from C to Rust on mobile","url":"https://engineering.fb.com/2025/07/01/developer-tools/an-inside-look-at-metas-transition-from-c-to-rust-on-mobile/","date":1751385623,"author":"","guid":179013,"unread":true,"content":"<p>Have you ever worked is legacy code? Are you curious what it takes to modernize systems at a massive scale?</p><p><a href=\"https://www.threads.net/@passy_\" target=\"_blank\" rel=\"noopener\">Pascal Hartig</a> is joined on the latest Meta Tech Podcast by Elaine and Buping, two software engineers working on a bold project to rewrite the decades-old C code in one of Meta’s core messaging libraries in Rust. It’s an ambitious effort that will transform a central messaging library that is shared across Messenger, Facebook, Instagram, and Meta’s AR/VR platforms.</p><p>They discuss taking on a project of this scope – even without a background in Rust, how they’re approaching it, and what it means to optimize for ‘developer happiness.’</p><p>Download or listen to the episode below:</p><p>You can also find the episode wherever you get your podcasts, including:</p><p>The&nbsp;<a href=\"https://insidefacebookmobile.libsyn.com/\" target=\"_blank\" rel=\"noopener\">Meta Tech Podcast</a>&nbsp;is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.</p><p>And if you’re interested in learning more about career opportunities at Meta visit the&nbsp;<a href=\"https://www.metacareers.com/?ref=engineering.fb.com\" target=\"_blank\" rel=\"noopener\">Meta Careers</a>&nbsp;page.</p>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understand your software’s supply chain with GitHub’s dependency graph","url":"https://github.blog/security/supply-chain-security/understand-your-softwares-supply-chain-with-githubs-dependency-graph/","date":1751385600,"author":"Andrea Griffiths","guid":179050,"unread":true,"content":"<p>What if you could spot the weakest link in your software supply chain before it breaks?</p><p>With GitHub’s <a href=\"https://docs.github.com/en/code-security/supply-chain-security/understanding-your-software-supply-chain/about-the-dependency-graph\">dependency graph</a>, you can. By providing a clear, complete view of the external packages your code depends on, both directly and indirectly, it allows you to understand, secure, and manage your project’s true footprint.</p><p>If you’re like me and sometimes lose track of what’s actually powering your applications (we’ve all been there!), GitHub’s dependency graph is about to become your new best friend.&nbsp;</p><h2>What is the dependency graph?</h2><p>Here’s the thing: Every modern software project is basically an iceberg. That small manifest file with your direct dependencies seems quite harmless at first glance. But underneath? There’s this massive, hidden world of transitive dependencies that most of us never think about. The GitHub dependency graph maps this entire underwater world. Think of it like a family tree, but for your code. Each package is a family member, and each dependency relationship shows who’s related to whom (and trust me, some of these family trees get  complicated).</p><p>Each package is a node. Each dependency relationship is an edge. The result? A full visual and structured representation of your software’s external codebase.</p><blockquote><p>In some cases, 95–97% of your code is actually someone else’s. The dependency graph helps you make sense of that reality.</p></blockquote><p>Let that sink in for a moment. We’re basically curators of other people’s work, and the dependency graph finally helps us make sense of that reality.</p><p>When vulnerabilities are discovered in open source packages, the consequences ripple downstream. If you don’t know a vulnerable dependency is part of your project, it’s hard to take action.</p><p>The dependency graph isn’t just a cool visualization (though it is pretty neat to look at). It’s the foundation that makes <a href=\"https://github.com/dependabot\">Dependabot</a> alerts possible. When a security issue is found in any of your dependencies (even a transitive one), GitHub notifies you. You get the full picture of what’s in your supply chain, how it got there, and what you can actually do about it.</p><h2>See it in action: From 21 to 1,000 dependencies</h2><p>Eric showed us a project that looked innocent enough:</p><ul><li> (the ones actually listed in package.json)</li><li> (including everything that got pulled in along the way)</li></ul><p>With the dependency graph, you can finally:</p><ul><li><strong>Understand which dependencies are direct vs. transitive</strong></li><li><strong>Trace how a package like Log4j ended up in your codebase.</strong> (Spoiler: it probably came along for the ride with something else.)</li><li><strong>Know what’s yours to fix and what depends on an upstream maintainer</strong></li></ul><h2>Tighten your supply chain with Dependabot</h2><p>Dependabot runs on top of the dependency graph—so enabling the graph is what makes Dependabot’s vulnerability alerts and automatic fix suggestions possible.&nbsp;</p><p>Pro tip: Filter for direct dependencies first. These are the ones you can actually control, so focus your energy there instead of pulling your hair out over transitive dependencies that are someone else’s responsibility.</p><h2>How to enable the dependency graph</h2><p>You can enable the dependency graph in your repository settings under <strong>Security &gt; Dependency Graph</strong>. If you turn on , the graph will be enabled automatically.</p><p>Using GitHub Actions? Community-maintained actions can generate a Software Bill of Materials (SBOM) and submit it to GitHub’s Dependency submission API, even if your language ecosystem doesn’t support auto-discovery.</p><p>✅  Dependency graph and Dependabot alerts are free for all repositories.</p><p>You can’t secure what you can’t see. GitHub’s dependency graph gives you visibility into the 90%+ of your codebase that comes from open source libraries and helps you take action when it counts.</p><ul><li>(seriously, do it now)</li><li><strong>Use it with Dependabot for automated alerts and fixes</strong></li><li><strong>Finally discover what’s actually in your software supply chain</strong></li></ul><p>Your future self (and your security team) will thank you.</p>","contentLength":3897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating a Website with Sphinx and Markdown","url":"https://www.blog.pythonlibrary.org/2025/07/01/creating-a-website-with-sphinx-and-markdown/","date":1751372880,"author":"Mike","guid":178904,"unread":true,"content":"<p><a href=\"https://www.sphinx-doc.org/en/master/\">Sphinx</a> is a Python-based documentation builder. The Python documentation is written using Sphinx. The Sphinx project supports using ReStructuredText and Markdown, or a mixture of the two. Each page of your documentation or website must be written using one of those two formats.</p><p>In this tutorial, you will learn how to use Sphinx to create a documentation site. Here is an overview of what you’ll learn:</p><ul><li>Making Markdown work in Sphinx</li><li>Building your Sphinx site</li><li>Adding content to your site</li></ul><p>Let’s start by installing all the packages you need to get Sphinx working!</p><p>You will need the following packages to be able to use Sphinx and Markdown:</p><p>You should install these package in a Python virtual environment. Open up your terminal and pick a location where you would like to create a new folder. Then run the following command:</p><pre data-enlighter-language=\"generic\">python -m venv NAME_OF_VENV_FOLDER</pre><p>Once you have the virtual environment, you need to activate it. Go into the&nbsp; folder and run the activate command in there.</p><p>Now you can install the dependencies that you need using pip, which will install them to your virtual environment.</p><p>Here’s how to install them using pip:</p><pre data-enlighter-language=\"generic\">python -m pip install myst-parser sphinx</pre><p>Once your packages are installed, you can learn how to set up your site!</p><p>Now that your packages are installed, you must set up your Sphinx website. To create a barebones Sphinx site, run the following command inside your virtual environment:</p><pre data-enlighter-language=\"generic\">sphinx-quickstart NAME_OF_SITE_FOLDER</pre><p>It will ask you a series of questions. The Sphinx documentation recommends keeping the source and build folders separate. Otherwise, you can set the other fields as needed or accept the defaults.</p><p>You will now have the following tree structure in your SITE_FOLDER:</p><p>You will work with the files and directories in this structure for the rest of the tutorial.</p><p>The next step on your Sphinx journey is to enable Markdown support.</p><h2>Making Markdown Work in Sphinx</h2><p>Go into the  directory and open the  file in your favorite Python IDE. Update the&nbsp; and the&nbsp; variables to the following (or add them if they do not exist):</p><pre data-enlighter-language=\"python\">extensions = ['myst_parser']\n\nsource_suffix = ['.rst', '.md']</pre><p>These changes tell Sphinx to use the Myst parser for Markdown files. You also leave ReStructuredText files in there so that your Sphinx website can handle that format.</p><p>You now have enough of your site available to build it and ensure it works.</p><h2>Building Your Sphinx Site</h2><p>You can now build a simple site with only an index page and the auto-generated boilerplate content. In your terminal, run the following command in the root of your Sphinx folder:</p><pre data-enlighter-language=\"generic\">sphinx-build -M html .\\source\\ .\\build\\</pre><p>The HTML files will be created inside the  folder. If you open the index page, it will look something like this:</p><p>Good job! You now have a Sphinx website!</p><p>Now you need to add some custom content to it.</p><h2>Adding Content to Your Site</h2><p>You can add ReStructuredText or Markdown files for each page of your site.&nbsp; using the  section:</p><pre data-enlighter-language=\"generic\">.. toctree::\n   :maxdepth: 2\n   :caption: Contents:\n\n   SUB_FOLDER/acknowledgments.md\n   doc_page1.md\n   OTHER_FOLDER/sub_doc_page1.md</pre><p>Let’s add some real content. Create a new file called&nbsp; in the root folder that contains the&nbsp; file. Then enter the following text in your new Markdown file:</p><pre data-enlighter-language=\"md\"># Python: All About Decorators\n\nDecorators can be a bit mind-bending when first encountered and can also be a bit tricky to debug. But they are a neat way to add functionality to functions and classes. Decorators are also known as a “higher-order function”. This means that they can take one or more functions as arguments and return a function as its result. In other words, decorators will take the function they are decorating and extend its behavior while not actually modifying what the function itself does.\n\nThere have been two decorators in Python since version 2.2, namely **classmethod()** and **staticmethod()**. Then PEP 318 was put together and the decorator syntax was added to make decorating functions and methods possible in Python 2.4. Class decorators were proposed in PEP 3129 to be included in Python 2.6. They appear to work in Python 2.7, but the PEP indicates they weren’t accepted until Python 3, so I’m not sure what happened there.\n\nLet’s start off by talking about functions in general to get a foundation to work from.\n\n## The Humble Function\n\nA function in Python and in many other programming languages is just a collection of reusable code. Some programmers will take an almost bash-like approach and just write all their code in a file with no functions. The code just runs from top to bottom. This can lead to a lot of copy-and-paste spaghetti code. Whenever two pieces of code do the same thing, they can almost always be put into a function. This will make updating your code easier since you’ll only have one place to update them.</pre><p>Make sure you save the file. Then, re-run the build command from the previous section. Now, when you open the  file, you should see your new Markdown file as a link that you click on and view.</p><p>Sphinx is a powerful way to create documentation for your projects. Sphinx has many plugins that you can use to make it even better. For example, you can use <a href=\"https://www.sphinx-doc.org/en/master/man/sphinx-apidoc.html\">sphinx-apidoc</a> to automatically generate documentation from your source code using the autodoc extension.</p><p>If you are an author and you want to share your books online, Sphinx is a good option for that as well. Having a built-in search functionality makes it even better. Give Sphinx a try and see what it can do for you!</p>","contentLength":5461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta joins Kotlin Foundation","url":"https://engineering.fb.com/2025/06/30/android/meta-joins-kotlin-foundation/","date":1751299230,"author":"","guid":176751,"unread":true,"content":"<p><a href=\"https://engineering.fb.com/2024/12/18/android/translating-java-to-kotlin-at-scale/\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://www.infoq.com/news/2024/12/meta-java-kotlin-port/\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://youtu.be/POmlM7OshwA?si=15r6zufGnwrkTolG\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://buck2.build/\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://www.youtube.com/watch?v=bC_grxuSO08\" target=\"_blank\" rel=\"noopener\"></a></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub Advisory Database by the numbers: Known security vulnerabilities and what you can do about them","url":"https://github.blog/security/github-advisory-database-by-the-numbers-known-security-vulnerabilities-and-what-you-can-do-about-them/","date":1751040000,"author":"Jonathan Evans","guid":183160,"unread":true,"content":"<p>The <a href=\"https://github.com/advisories\">GitHub Advisory Database</a> (Advisory DB) is a vital resource for developers, providing a comprehensive list of known security vulnerabilities and malware affecting open source packages. This post analyzes trends in the Advisory DB, highlighting the growth in reviewed advisories, ecosystem coverage, and source contributions in 2024. We’ll delve into how GitHub provides actionable data to secure software projects.</p><p>The GitHub Advisory Database contains a list of known security vulnerabilities and malware, grouped in three categories:&nbsp;</p><ul><li><strong>GitHub-reviewed advisories:</strong> Manually reviewed advisories in software packages that GitHub supports.</li><li> These are automatically pulled from the National Vulnerability Database (NVD) and are either in the process of being reviewed, do not affect a supported package, or do not discuss a valid vulnerability.</li><li> These are specific to malware threats identified by the npm security team.</li></ul><p>GitHub-reviewed advisories are security vulnerabilities that have been mapped to packages in ecosystems we support. We carefully review each advisory for validity and ensure that they have a full description, and contain both ecosystem and package information.</p><p>Every year, GitHub increases the number of advisories we publish. We have been able to do this due to the increase in advisories coming from our sources (see Sources section below), expanding our ecosystem coverage (also described below), and review campaigns of advisories published before we started the database.&nbsp;</p><p>In the past five years, the database has gone from fewer than 400 reviewed advisories to over 20,000 reviewed advisories in October of 2024.</p><p>Unreviewed advisories are security vulnerabilities that we publish automatically into the GitHub Advisory Database directly from the National Vulnerability Database feed. The name is a bit of a misnomer as many of these advisories have actually been reviewed by a GitHub analyst. The reason why they fall into this category is because they are not found in a package in one of the supported ecosystems or are not discussing a valid vulnerability, and all have been reviewed by analysts other than someone from the GitHub Security Lab. Even though most of these advisories will never turn into a reviewed advisory, we still publish them so that you do not have to look in multiple databases at once.</p><p>Malware advisories relate to vulnerabilities caused by malware, and are security advisories that GitHub publishes automatically into the GitHub Advisory Database directly from information provided by the npm security team. Malware advisories are currently exclusive to the npm ecosystem. GitHub doesn’t edit or accept community contributions on these advisories.</p><p>GitHub-reviewed advisories include security vulnerabilities that have been mapped to packages in ecosystems we support. Generally, we name our supported ecosystems after the software programming language’s associated package registry. We review advisories if they are for a vulnerability in a package that comes from a supported registry.</p><p>Vulnerabilities in Maven and Composer packages are nearly half of the advisories in the database. npm, pip, and Go make up much of the rest, while the other ecosystems have a much smaller footprint.</p><p>This has not always been the case. When the database was initially launched, NPM advisories dominated the database, but as we have expanded our coverage and added support for new ecosystems, the distribution mix has changed.</p><p>We add advisories to the GitHub Advisory Database from the following sources:</p><ul><li> This is a huge source of vulnerabilities covering all types of software. We publish all NVD advisories but only review those relevant to our supported ecosystems, which reduces noise for our users.</li><li><strong>GitHub Repository Advisories:</strong> The second largest source is made up of advisories published through GitHub’s repository security advisory feature. Similar to NVD, these aren’t restricted to our supported ecosystems. However, we provide better coverage of the repository advisories because they focus exclusively on open source software.</li><li> These are reports from the community that are almost exclusively requesting updates to existing advisories.</li><li><strong>Other Specialized Sources:</strong> Sources like PyPA Advisories (for Python) and Go Vulncheck (for Go) that focus on specific ecosystems. Because they only cover packages within our supported ecosystems, most of their advisories are relevant to us and get reviewed.</li></ul><p>If you add up the number of reviewed advisories from each source, you will find that total is more than the total reviewed advisories. This is because each source can publish an advisory for the same vulnerability. In fact, over half of our advisories have more than one source.</p><p>Of the advisories with a single source, nearly all of them come from NVD/CVE. This justifies NVD/CVE as a source, even though it is by far the noisiest.</p><p>2024 saw a significant increase (39%) in the number of advisories imported from our sources. This is for the most part caused by an increase in the number of CVE records published.</p><p>In addition to publishing advisories in the GitHub Advisory Database, we are also a <a href=\"https://docs.github.com/en/code-security/security-advisories/working-with-repository-security-advisories/about-repository-security-advisories#cve-identification-numbers\">CVE Numbering Authority (CNA)</a> for any repository on GitHub. This means that we issue CVE IDs for vulnerabilities reported to us by maintainers, and we publish the vulnerabilities to the CVE database once the corresponding repository advisory is published.</p><p>GitHub published over 2,000 CVE records in 2024, making us the fifth-largest CNA in the CVE Program.</p><p>The GitHub CNA is open to  on GitHub, not just ones in a supported ecosystem.</p><p>Given the constant deluge of reported vulnerabilities, you’ll want tools that can help you prioritize your remediation efforts. To that end, GitHub provides additional data in the advisory to allow readers to prioritize their vulnerabilities. In particular, there are:</p><ul><li> A low to critical rating for how severe the vulnerability is likely to be, along with a corresponding CVSS score and vector.</li><li> CWE identifiers provide a programmatic method for determining the type of vulnerability.</li><li> The Exploit Prediction Scoring System, or EPSS, is a system devised by the global Forum of Incident Response and Security Teams (FIRST) for quantifying the likelihood a vulnerability will be attacked in the next 30 days.</li></ul><ul></ul><p>Using these ratings, half of all vulnerabilities (15% are Critical and 35% are High) warrant immediate or near-term attention. By focusing remediation efforts on these, you can significantly reduce risk exposure while managing workload more efficiently.</p><p>The CVSS specification says the base score we provide, “reflects the severity of a vulnerability according to its intrinsic characteristics which are constant over time and assumes the reasonable worst-case impact across different deployed environments.” However, the worst-case scenario for your deployment may not be the same as CVSS’s. After all, a crash in a word processor is not as severe as a crash in a server. In order to give more context to your prioritization, GitHub allows you to filter alerts based on the type of vulnerability or weakness using CWE identifiers. So you have the capability to never see another regular expression denial of service (<a href=\"https://cwe.mitre.org/data/definitions/1333.html\">CWE-1333</a>) vulnerability again or always see SQL injection (<a href=\"https://cwe.mitre.org/data/definitions/89.html\">CWE-89</a>) vulnerabilities.</p><figure><table><thead><tr><th><strong>Number of advisories in 2024</strong></th></tr></thead><tbody><tr><td>Improper Neutralization of Input During Web Page Generation (‘Cross-site Scripting’)</td></tr><tr><td>Exposure of Sensitive Information to an Unauthorized Actor</td></tr><tr><td>Improper Limitation of a Pathname to a Restricted Directory (‘Path Traversal’)</td></tr><tr><td>Improper Input Validation</td></tr><tr><td>Improper Control of Generation of Code (‘Code Injection’)</td></tr><tr><td>Improper Neutralization of Special Elements used in an SQL Command (‘SQL Injection’)</td></tr><tr><td>Cross-Site Request Forgery (CSRF)</td></tr><tr></tr><tr><td>Uncontrolled Resource Consumption</td></tr><tr></tr></tbody></table></figure><p>Still drowning in vulnerabilities? Try using EPSS to focus on vulnerabilities likely to be attacked in the next 30 days. EPSS uses data from a <a href=\"https://www.first.org/epss/partners\">variety of sources</a> to create a probability of whether exploitation attempts will be seen in the next 30 days for a given vulnerability. As you can see from the chart below, if you focus on vulnerabilities with EPSS scores of 10% or higher (approx. 7% of all vulnerabilities in the Advisory DB), you can cover nearly all of the vulnerabilities that are likely to see exploit activity.</p><figure><table><thead><tr><th><strong>Percentage of overall vulnerabilities</strong></th><th><strong>Expected vulnerabilities in range attacked within the next 30 days</strong></th><th><strong>Percentage of total attacked vulnerabilities</strong></th></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></figure><p>Important caveats to remember when using EPSS:</p><ul><li>Low probability events occur.</li><li>EPSS does not tell you whether a vulnerability is exploited; it only claims how likely it is.</li><li>EPSS scores are updated daily and will change as new information comes in, so a low-probability vulnerability today may become high probability tomorrow.</li></ul><p>For more details on how to use CVSS and EPSS for prioritization, see our <a href=\"https://github.blog/security/application-security/cutting-through-the-noise-how-to-prioritize-dependabot-alerts/\">blog</a> on prioritizing Dependabot alerts.</p><p>The GitHub Advisory DB isn’t just a repository of vulnerabilities. It powers tools that help developers secure their projects. Services like Dependabot use the Advisory DB to:</p><ul><li><strong>Identify vulnerabilities:</strong> It checks if your projects use any software packages with known vulnerabilities.</li><li> It recommends updated versions of packages that fix those vulnerabilities when available.</li><li> You’ll only get notified about vulnerabilities that affect the version of the package you are using.</li></ul><p>The GitHub Advisory Database is a powerful resource for tracking open source software vulnerabilities, with over 22,000 reviewed advisories to date. By focusing on popular package registries, GitHub allows you to definitively connect vulnerabilities to the packages you are using. Additional data such as CVSS and EPSS scores help you properly prioritize your mitigation efforts.</p><p>GitHub’s role as a CVE Numbering Authority extends beyond the Advisory Database, ensuring that thousands of vulnerabilities each year reach the broader CVE community. Want to ensure your vulnerability fix reaches your users? <a href=\"https://docs.github.com/en/code-security/security-advisories/working-with-repository-security-advisories/editing-a-repository-security-advisory\">Create a GitHub security advisory</a> in your repository to take advantage of both the GitHub Advisory Database and GitHub’s CNA services.</p>","contentLength":10095,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["official"]}