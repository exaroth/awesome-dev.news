{"id":"7caSwCnanpF82DyESN","title":"Official News","displayTitle":"Official News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":8,"items":[{"title":"GitHub Game Off 2025 theme announcement","url":"https://github.blog/company/github-game-off-2025-theme-announcement/","date":1762029420,"author":"Lee Reilly","guid":324185,"unread":true,"content":"<p>Get ready for the annual <a href=\"https://gameoff.github.com/\">Game Off</a>, our month-long game jam that has inspired thousands of developers to make, share, and play games since 2012. Whether you‚Äôre a first-time jammer or a returning champion, this November is your chance to make something unforgettable.</p><p>The theme for this year? !</p><p>You have until <strong>December 1, 2025, at 13:37 PST</strong> to build a game  based on the theme. How you interpret it is entirely up to you. Don‚Äôt overthink it. Just ride the creative wave and see where it takes you. üèÑüèª</p><h2>Need inspiration? Here are a few concept ideas</h2><ul><li>A space shooter where you <strong>fly through gravitational waves and wormholes</strong>.</li><li>A survival game where you build a coastal base and </li><li>A tower defense game where you <strong>battle waves of increasingly powerful baddies</strong>.</li><li>A skateboard game where you ride a sine wave, shredding through peaks and troughs.&nbsp;</li><li>A rhythm game where you <strong>catch the beat and ride the wave</strong>.</li><li>A racing game where you <strong>drift through vaporwave skylines</strong> and a <strong>totally tubular synthwave soundtrack</strong>.</li><li>A physics puzzler where you <strong>bounce, reflect, and refract energy waves.</strong></li><li>A remake of a class you enjoyed when you were younger resulting in <strong>endless waves of nostalgia.</strong></li></ul><p>Whatever form your game takes, whether it crashes, ripples, or totally wipes out‚Ä¶ we can‚Äôt wait to see it.</p><p>Stuck for ideas? GitHub Copilot might be able to help. Try asking, ‚Äú<em>What are some fun games I could create with the game jam theme, WAVES</em>?‚Äù</p><p>Work alone or on a team. Use whatever programming languages, game engines, or libraries you like.</p><ol><li> Create a free GitHub account if you don‚Äôt have one.</li><li> Hop onto the itch.io <a href=\"https://gameoff.github.com/\">Game Off 2025</a> page. If you don‚Äôt already have an itch.io account, you can sign in with your GitHub account.</li><li><strong>Create a public repository.</strong> Store your source code on GitHub. Push your game before </li><li><strong>Submit your game on itch.io.</strong> Once submitted, you‚Äôll be able to play other entries and cast your votes.</li></ol><p>After the submission period ends, participants will vote on each other‚Äôs games. Entries will be evaluated in the following categories:</p><ul></ul><p>Voting will end on January 8, 2026, at 13:37 PST. Winners will be announced on the GitHub Blog and social channels on January 10, 2026, at 13:37 PST.</p><p>Game Off is intentionally relaxed, but here are a few simple guidelines to keep things fair and fun:</p><ul><li><strong>Your game must live in a GitHub repository.</strong> You should start from scratch, but you can use templates. The vast majority of the work should be done in the game jam period.</li><li><strong>License it however you like.</strong> Open source is encouraged, but not required.</li><li> Work however you‚Äôre most comfortable.</li><li><strong>Use any tools or assets you prefer.</strong> Open source, commercial, or your own creations are all welcome.</li><li><strong>AI-assisted development is allowed.</strong></li></ul><p>That‚Äôs it. Keep it creative, respectful, and fun, and remember to push your code before the deadline.</p><p>You don‚Äôt need to be an expert. Many participants build their first game during Game Off. Some use popular engines, others build their own, and a few even create games for classic hardware like the NES, Game Boy, or ZX Spectrum. However you make it, there‚Äôs no wrong way to play.</p><p>Here are a few engines you might want to explore:</p><ul><li><a href=\"https://godotengine.org/\"></a>: Great for 2D and 3D games. Open source, lightweight, and beginner-friendly.</li><li><a href=\"https://unity.com/\"></a>: Ideal for 3D or mobile games with plenty of tutorials and asset packs available.</li><li><a href=\"https://www.unrealengine.com/\"></a>: Best for cinematic visuals, complex 3D games, and high-end experiences.</li><li><a href=\"https://phaser.io/\"></a>: Good choice for browser-based 2D arcade or platformer games.</li><li><a href=\"https://www.pygame.org/\"></a>: A solid option for learning game development basics or prototyping ideas quickly.</li><li><a href=\"https://bevy.org/\"></a>: Modern, data-driven engine for developers who like performance and clean ECS design.</li><li><a href=\"https://love2d.org/\"></a>: Lightweight and fast, good for 2D games and creative coding projects.</li><li><a href=\"https://flame-engine.org/\"></a>: Works well for mobile-first 2D games if you already use Flutter.</li><li><a href=\"https://ebitengine.org/\"></a>: Simple and powerful engine for 2D games written in Go.</li><li><a href=\"https://defold.com/\"></a>: Cross-platform 2D engine with built-in tools and an active indie community.</li><li><a href=\"https://libgdx.com/\"></a>: A familiar choice for developers coming from Java or Android backgrounds.</li><li><a href=\"https://haxeflixel.com/\"></a>: Great for retro-style 2D games, platformers, and jam projects.</li></ul><p>The <a href=\"https://itch.io/jam/game-off-2025/community\">Game Off 2025 Community</a> is a great place to ask questions or look for teammates. There‚Äôs also a friendly community-run Discord server.</p><p>Game Off is the perfect opportunity to check it out (version control pun intended).</p><p>Whether your build floats or sinks, you‚Äôre part of something swell. Join thousands of developers around the world for a month of creativity, learning, and code-powered fun. Let‚Äôs <em>hang ten on your keyboard </em>&nbsp;üåä ü§ô and make some WAVES together.</p>","contentLength":4462,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measuring what matters: How offline evaluation of GitHub MCP Server works","url":"https://github.blog/ai-and-ml/generative-ai/measuring-what-matters-how-offline-evaluation-of-github-mcp-server-works/","date":1761860767,"author":"Ksenia Bobrova","guid":322350,"unread":true,"content":"<p><a href=\"https://modelcontextprotocol.io/docs/getting-started/intro\">MCP (Model Context Protocol)</a> is a simple, common way for AI models (LLMs) to talk to APIs and data. Think of it like a universal plug: if both sides support MCP, they can connect and work together. An MCP server is any service or app that ‚Äúspeaks MCP‚Äù and offers tools the model can use, publishing a list of tools, what each tool does, and what inputs (parameters) each tool needs.&nbsp;</p><p><a href=\"https://github.com/github/github-mcp-server?utm_source=blog-github-mcp-server&amp;utm_medium=blog&amp;utm_campaign=universe25post\">The GitHub MCP Server</a> is the foundation for many <a href=\"https://github.com/features/copilot?utm_source=blog-copilot-feature&amp;utm_medium=blog&amp;utm_campaign=universe25post\">GitHub Copilot</a> workflows, both inside and outside of GitHub. As an engineering team working on GitHub MCP, we‚Äôre always looking to deliver new features and functionality, while avoiding regressions and improving quality with every iteration. And how we name a tool, explain what it does, and spell out its parameters directly affects whether the model picks the right tool, in the right order, with the right arguments.&nbsp;</p><p>When it comes to our work, small edits matter: tightening a description, adding or removing a tool, or combining a few similar tools can shift results a lot. When descriptions are off, agents choose the wrong tool, skip a step, send arguments in the wrong format, or drop them entirely. The outcome is weak. We need a safe way to change MCP and know if things actually got better, not worse. That‚Äôs where offline evaluation comes in.</p><p>Offline evaluation catches regressions before users see them and keeps the feedback loop short, so we can ship changes that genuinely improve performance.</p><p>This article walks through our evaluation pipeline and explains the metrics and algorithms that help us achieve these goals.</p><h2>How automated offline evaluation works</h2><p>Our offline evaluation pipeline checks how well our tool prompts work across different models. The tool instructions are kept simple and precise so the model can choose the right tool and fill in the correct parameters. Because LLMs vary in how they use tools, we systematically test each model‚ÄìMCP pairing to measure compatibility, quality, and gaps.</p><p>We have curated datasets that we use as benchmarks. Every benchmark contains the following parameters:&nbsp;</p><ol><li>: This is a user request formulated in natural language.&nbsp;</li><li>: Tools we expect to be called.</li><li>: Arguments we expect to be passed to each tool.</li></ol><h3>Asking how many issues were created in a given time period</h3><p>&nbsp; How many issues were created in the github/github-mcp-server repository during April 2025?&nbsp; list_issues with:</p><pre><code>owner: github \nrepo: github-mcp-server \nsince: 2025-04-01T00:00:00Z</code></pre><p> Merge PR 123 in github/docs using squash merge with title ‚ÄúUpdate installation guide‚Äù merge_pull_request with :</p><pre><code>owner: github\nrepo: docs \npullNumber: 123 \nmerge_method: squash \ncommit_title: Update installation guide</code></pre><p>Request reviews from alice456 and bob123 for PR 67 in team/project-alpha update_pull_request with </p><pre><code>owner: team \nrepo: project-alpha \npullNumber: 67\nreviewers: [\"alice456\", \"bob123\"]</code></pre><p> Summarize the comments in discussion 33801, in the facebook/react repository&nbsp;: get_discussion_comments with :</p><pre><code>owner: facebook\nrepo: react\ndiscussionNumber: 33801</code></pre><p>The evaluation pipeline has three stages: , , and .</p><ul><li> We run each benchmark across multiple models, providing the list of available MCP tools with every request. For each run, we record which tools the model invoked and the arguments it supplied.</li><li> We process the raw outputs and compute metrics and scores.</li><li> We aggregate dataset-level statistics and produce the final evaluation report.</li></ul><h2>Evaluation metrics and algorithms</h2><p>Our evaluation targets two aspects: whether the model <strong>selects the correct tools</strong> and whether it <strong>supplies correct arguments</strong>.</p><p>When benchmarks involve a single tool call,  reduces to a <strong>multi-class classification</strong> problem. Each benchmark is labeled with the tool it expects, and each tool is a ‚Äúclass.‚Äù</p><p>Models tasked with this classification are evaluated using , , , and .</p><ol><li> is the simplest measure that shows the percentage of correct classifications. In our case it means the percentage of inputs that resulted in an expected tool call. This is calculated on the whole dataset.</li><li> shows the proportion of the cases for which the tool was called correctly out of all cases where the tool was called. Low precision means the model picks the tool even for the cases where the tool is not expected to be called.</li><li> shows the proportion of correctly called tools out of all cases where the given tool call was expected. Low recall may indicate that the model doesn‚Äôt understand that the tool needs to be called and fails to call the tool or calls another tool instead.</li><li> is a harmonic mean showing how well the model is doing in terms of both precision and recall.&nbsp;</li></ol><p>If the model confuses two tools, it can result in low precision or recall for these tools.</p><p>We have two similar tools that used to be confused often, which are  and . Let‚Äôs say we have 10 benchmarks for &nbsp; and 10 benchmarks for . Imagine  is called correctly in all of 10 cases and on top in 30% of cases where search_issues should be called.</p><p>This means we‚Äôre going to have lower recall for  and lower precision for :</p><p> () = 10 (cases where tool is called correctly) / (10 + 3 (cases where tool is called instead of )) = 0.77</p><p> () =&nbsp; 7 (tool was called correctly) / 10 (cases where tool is expected to be called) = 0.7</p><p>In order to have visibility into what tools are confused with each other, we build a confusion matrix. Confusion matrix for the  and  tools from the example above would look the following:</p><figure><table><thead><tr><th><strong>Expected tool / Called tool</strong></th></tr></thead><tbody></tbody></table></figure><p>The confusion matrix allows us to see the reason behind low precision and recall for certain tools and tweak their descriptions to minimize confusion.</p><p>Selecting the right tool isn‚Äôt enough. The model must also supply correct arguments. We‚Äôve defined a set of argument-correctness metrics that pinpoint specific issues, making regressions easy to diagnose and fix.</p><p>We track four argument-quality metrics:</p><ul><li> How often the model supplies argument names that aren‚Äôt defined for the tool.</li><li><strong>All expected arguments provided:</strong> Whether every expected argument is present.</li><li><strong>All required arguments provided:</strong> Whether all required arguments are included.</li><li> Whether provided argument values match the expected values exactly.</li></ul><p>These metrics are computed for tools that were correctly selected. The final report summarizes each tool‚Äôs performance across all four metrics.</p><h2>Looking forward and filling the gaps</h2><p>The current evaluation framework gives us a solid read on tool performance against curated datasets, but there‚Äôs still room to improve.</p><p>Benchmark volume is the weak point of offline evaluation. With so many classes (tools), we need more robust per-tool coverage. Evaluations based on just a couple of examples aren‚Äôt dependable alone. Adding more benchmarks is always useful to increase the reliability of classification evaluation and other metrics.</p><p>Our current pipeline handles only single tool calls. In practice, tools are often invoked sequentially, with later calls consuming the outputs of earlier ones. To evaluate these flows, we must go beyond fetching the MCP tool list and actually execute tool calls (or mock their responses) during evaluation.</p><p>We‚Äôll also update summarization. Today we treat tool selection as <strong>multi-class classification</strong>, which assumes one tool per input. For flows where a single input can trigger multiple tools, <a href=\"https://en.wikipedia.org/wiki/Multi-label_classification\"><strong>multi-label classification</strong></a> is the better fit.</p><p>Offline evaluation gives us a fast, safe way to iterate on MCP, so models pick the right GitHub tools with the right arguments. By combining curated benchmarks with clear metrics‚Äîclassification scores for tool selection and targeted checks for argument quality‚Äîwe turn vague ‚Äúit seems better‚Äù into measurable progress and actionable fixes.</p><p>We‚Äôre not stopping here. We‚Äôre expanding benchmark coverage, refining tool descriptions to reduce confusion, and extending the pipeline to handle real multi-tool flows with execution or faithful mocks. These investments mean fewer regressions, clearer insights, and more reliable agents that help developers move faster.</p><p>Most importantly, this work raises the bar for product quality without slowing delivery. As we grow the suite and deepen the evaluation, you can expect steadier improvements to GitHub MCP Server‚Äîand a better, more predictable experience for anyone building with it.</p>","contentLength":8221,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing Rust 1.91.0","url":"https://blog.rust-lang.org/2025/10/30/Rust-1.91.0/","date":1761782400,"author":"The Rust Release Team","guid":322278,"unread":true,"content":"<p>The Rust team is happy to announce a new version of Rust, 1.91.0. Rust is a programming language empowering everyone to build reliable and efficient software.</p><p>If you have a previous version of Rust installed via , you can get 1.91.0 with:</p><p>If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please <a href=\"https://github.com/rust-lang/rust/issues/new/choose\">report</a> any bugs you might come across!</p><p>The Rust compiler supports <a href=\"https://doc.rust-lang.org/rustc/platform-support.html\">a wide variety of targets</a>, but\nthe Rust Team can't provide the same level of support for all of them. To\nclearly mark how supported each target is, we use a tiering system:</p><ul><li>Tier 3 targets are technically supported by the compiler, but we don't check\nwhether their code build or passes the tests, and we don't provide any\nprebuilt binaries as part of our releases.</li><li>Tier 2 targets are guaranteed to build and we provide prebuilt binaries, but\nwe don't execute the test suite on those platforms: the produced binaries\nmight not work or might have bugs.</li><li>Tier 1 targets provide the highest support guarantee, and we run the full\nsuite on those platforms for every change merged in the compiler. Prebuilt\nbinaries are also available.</li></ul><p>Rust 1.91.0 promotes the  target to Tier 1 support,\nbringing our highest guarantees to users of 64-bit ARM systems running Windows.</p><h3><a href=\"https://blog.rust-lang.org/2025/10/30/Rust-1.91.0/#add-lint-against-dangling-raw-pointers-from-local-variables\" aria-hidden=\"true\"></a>\nAdd lint against dangling raw pointers from local variables</h3><p>While Rust's borrow checking prevents dangling references from being returned, it doesn't\ntrack raw pointers. With this release, we are adding a warn-by-default lint on raw\npointers to local variables being returned from functions. For example, code like this:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre><code></code></pre><p>Note that the code above is not unsafe, as it itself doesn't perform any dangerous\noperations. Only dereferencing the raw pointer after the function returns would be\nunsafe. We expect future releases of Rust to add more functionality helping authors\nto safely interact with raw pointers, and with unsafe code more generally.</p><p>These previously stable APIs are now stable in const contexts:</p><p>Many people came together to create Rust 1.91.0. We couldn't have done it without all of you. <a href=\"https://thanks.rust-lang.org/rust/1.91.0/\">Thanks!</a></p>","contentLength":2118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Green Tea Garbage Collector","url":"https://go.dev/blog/greenteagc","date":1761696000,"author":"Michael Knyszek and Austin Clements","guid":320808,"unread":true,"content":"<p>Go 1.25 includes a new experimental garbage collector called Green Tea,\navailable by setting  at build time.\nMany workloads spend around 10% less time in the garbage collector, but some\nworkloads see a reduction of up to 40%!</p><p>It‚Äôs production-ready and already in use at Google, so we encourage you to\ntry it out.\nWe know some workloads don‚Äôt benefit as much, or even at all, so your feedback\nis crucial to helping us move forward.\nBased on the data we have now, we plan to make it the default in Go 1.26.</p><p>What follows is a blog post based on Michael Knyszek‚Äôs GopherCon 2025 talk.\nWe‚Äôll update this blog post with a link to the talk once it‚Äôs available online.</p><h2>Tracing garbage collection</h2><p>Before we discuss Green Tea let‚Äôs get us all on the same page about garbage\ncollection.</p><p>The purpose of garbage collection is to automatically reclaim and reuse memory\nno longer used by the program.</p><p>To this end, the Go garbage collector concerns itself with  and\n.</p><p>In the context of the Go runtime,  are Go values whose underlying\nmemory is allocated from the heap.\nHeap objects are created when the Go compiler can‚Äôt figure out how else to allocate\nmemory for a value.\nFor example, the following code snippet allocates a single heap object: the backing\nstore for a slice of pointers.</p><pre><code>var x = make([]*int, 10) // global\n</code></pre><p>The Go compiler can‚Äôt allocate the slice backing store anywhere except the heap,\nsince it‚Äôs very hard, and maybe even impossible, for it to know how long  will\nrefer to the object for.</p><p> are just numbers that indicate the location of a Go value in memory,\nand they‚Äôre how a Go program references objects.\nFor example, to get the pointer to the beginning of the object allocated in the\nlast code snippet, we can write:</p><p>Go‚Äôs garbage collector follows a strategy broadly referred to as <em>tracing garbage\ncollection</em>, which just means that the garbage collector follows, or traces, the\npointers in the program to identify which objects the program is still using.</p><p>More specifically, the Go garbage collector implements the mark-sweep algorithm.\nThis is much simpler than it sounds.\nImagine objects and pointers as a sort of graph, in the computer science sense.\nObjects are nodes, pointers are edges.</p><p>The mark-sweep algorithm operates on this graph, and as the name might suggest,\nproceeds in two phases.</p><p>In the first phase, the mark phase, it walks the object graph from well-defined\nsource edges called .\nThink global and local variables.\nThen, it  everything it finds along the way as , to avoid going in\ncircles.\nThis is analogous to your typical graph flood algorithm, like a depth-first or\nbreadth-first search.</p><p>Next is the sweep phase.\nWhatever objects were not visited in our graph walk are unused, or ,\nby the program.\nWe call this state unreachable because it is impossible with normal safe Go code\nto access that memory anymore, simply through the semantics of the language.\nTo complete the sweep phase, the algorithm simply iterates through all the\nunvisited nodes and marks their memory as free, so the memory allocator can reuse\nit.</p><p>You may think I‚Äôm oversimplifying a bit here.\nGarbage collectors are frequently referred to as , and .\nAnd you‚Äôd be partially right, there are more complexities.</p><p>For example, this algorithm is, in practice, executed concurrently with your\nregular Go code.\nWalking a graph that‚Äôs mutating underneath you brings challenges.\nWe also parallelize this algorithm, which is a detail that‚Äôll come up again\nlater.</p><p>But trust me when I tell you that these details are mostly separate from the\ncore algorithm.\nIt really is just a simple graph flood at the center.</p><p>Let‚Äôs walk through an example.\nNavigate through the slideshow below to follow along.</p><p>After all that, I think we have a handle on what the Go garbage collector is actually doing.\nThis process seems to work well enough today, so what‚Äôs the problem?</p><p>Well, it turns out we can spend  of time executing this particular algorithm in some\nprograms, and it adds substantial overhead to nearly every Go program.\nIt‚Äôs not that uncommon to see Go programs spending 20% or more of their CPU time in the\ngarbage collector.</p><p>Let‚Äôs break down where that time is being spent.</p><p>At a high level, there are two parts to the cost of the garbage collector.\nThe first is how often it runs, and the second is how much work it does each time it runs.\nMultiply those two together, and you get the total cost of the garbage collector.</p><figure><figcaption>\n    Total GC cost = Number of GC cycles √ó Average cost per GC cycle\n    </figcaption></figure><p>But for now let‚Äôs focus only on the second part, the cost per cycle.</p><p>From years of poring over CPU profiles to try to improve performance, we know two big things\nabout Go‚Äôs garbage collector.</p><p>The first is that about 90% of the cost of the garbage collector is spent marking,\nand only about 10% is sweeping.\nSweeping turns out to be much easier to optimize than marking,\nand Go has had a very efficient sweeper for many years.</p><p>The second is that, of that time spent marking, a substantial portion, usually at least 35%, is\nsimply spent  on accessing heap memory.\nThis is bad enough on its own, but it completely gums up the works on what makes modern CPUs\nactually fast.</p><h3>‚ÄúA microarchitectural disaster‚Äù</h3><p>What does ‚Äúgum up the works‚Äù mean in this context?\nThe specifics of modern CPUs can get pretty complicated, so let‚Äôs use an analogy.</p><p>Imagine the CPU driving down a road, where that road is your program.\nThe CPU wants to ramp up to a high speed, and to do that it needs to be able to see far ahead of it,\nand the way needs to be clear.\nBut the graph flood algorithm is like driving through city streets for the CPU.\nThe CPU can‚Äôt see around corners and it can‚Äôt predict what‚Äôs going to happen next.\nTo make progress, it constantly has to slow down to make turns, stop at traffic lights, and avoid\npedestrians.\nIt hardly matters how fast your engine is because you never get a chance to get going.</p><p>Let‚Äôs make that more concrete by looking at our example again.\nI‚Äôve overlaid the heap here with the path that we took.\nEach left-to-right arrow represents a piece of scanning work that we did\nand the dashed arrows show how we jumped around between bits of scanning work.</p><p>Notice that we were jumping all over memory doing tiny bits of work in each place.\nIn particular, we‚Äôre frequently jumping between pages, and between different parts of pages.</p><p>Modern CPUs do a lot of caching.\nGoing to main memory can be up to 100x slower than accessing memory that‚Äôs in our cache.\nCPU caches are populated with memory that‚Äôs been recently accessed, and memory that‚Äôs nearby to\nrecently accessed memory.\nBut there‚Äôs no guarantee that any two objects that point to each other will  be close to each\nother in memory.\nThe graph flood doesn‚Äôt take this into account.</p><p>Quick side note: if we were just stalling fetches to main memory, it might not be so bad.\nCPUs issue memory requests asynchronously, so even slow ones could overlap if the CPU could see\nfar enough ahead.\nBut in the graph flood, every bit of work is small, unpredictable, and highly dependent on the\nlast, so the CPU is forced to wait on nearly every individual memory fetch.</p><p>And unfortunately for us, this problem is only getting worse.\nThere‚Äôs an adage in the industry of ‚Äúwait two years and your code will get faster.‚Äù</p><p>But Go, as a garbage collected language that relies on the mark-sweep algorithm, risks the opposite.\n‚ÄúWait two years and your code will get slower.‚Äù\nThe trends in modern CPU hardware are creating new challenges for garbage collector performance:</p><p><strong>Non-uniform memory access.</strong>\nFor one, memory now tends to be associated with subsets of CPU cores.\nAccesses by  CPU cores to that memory are slower than before.\nIn other words, the cost of a main memory access <a href=\"https://jprahman.substack.com/p/sapphire-rapids-core-to-core-latency\" rel=\"noreferrer\" target=\"_blank\">depends on which CPU core is accessing\nit</a>.\nIt‚Äôs non-uniform, so we call this non-uniform memory access, or NUMA for short.</p><p><strong>Reduced memory bandwidth.</strong>\nAvailable memory bandwidth per CPU is trending downward over time.\nThis just means that while we have more CPU cores, each core can submit relatively fewer\nrequests to main memory, forcing non-cached requests to wait longer than before.</p><p>\nAbove, we looked at a sequential marking algorithm, but the real garbage collector performs this\nalgorithm in parallel.\nThis scales well to a limited number of CPU cores, but the shared queue of objects to scan becomes\na bottleneck, even with careful design.</p><p><strong>Modern hardware features.</strong>\nNew hardware has fancy features like vector instructions, which let us operate on a lot of data at once.\nWhile this has the potential for big speedups, it‚Äôs not immediately clear how to make that work for\nmarking because marking does so much irregular and often small pieces of work.</p><p>Finally, this brings us to Green Tea, our new approach to the mark-sweep algorithm.\nThe key idea behind Green Tea is astonishingly simple:</p><p><em>Work with pages, not objects.</em></p><p>Sounds trivial, right?\nAnd yet, it took a lot of work to figure out how to order the object graph walk and what we needed to\ntrack to make this work well in practice.</p><p>More concretely, this means:</p><ul><li>Instead of scanning objects we scan whole pages.</li><li>Instead of tracking objects on our work list, we track whole pages.</li><li>We still need to mark objects at the end of the day, but we‚Äôll track marked objects locally to each\npage, rather than across the whole heap.</li></ul><p>Let‚Äôs see what this means in practice by looking at our example heap again, but this time\nrunning Green Tea instead of the straightforward graph flood.</p><p>As above, navigate through the annotated slideshow to follow along.</p><p>Let‚Äôs come back around to our driving analogy.\nAre we finally getting on the highway?</p><p>Let‚Äôs recall our graph flood picture before.</p><p>We jumped around a whole lot, doing little bits of work in different places.\nThe path taken by Green Tea looks very different.</p><p>Green Tea, in contrast, makes fewer, longer left-to-right passes over pages A and B.\nThe longer these arrows, the better, and with bigger heaps, this effect can be much stronger.\n the magic of Green Tea.</p><p>It‚Äôs also our opportunity to ride the highway.</p><p>This all adds up to a better fit with the microarchitecture.\nWe can now scan objects closer together with much higher probability, so\nthere‚Äôs a better chance we can make use of our caches and avoid main memory.\nLikewise, per-page metadata is more likely to be in cache.\nTracking pages instead of objects means work lists are smaller,\nand less pressure on work lists means less contention and fewer CPU stalls.</p><p>And speaking of the highway, we can take our metaphorical engine into gears we‚Äôve never been able to\nbefore, since now we can use vector hardware!</p><p>If you‚Äôre only vaguely familiar with vector hardware, you might be confused as to how we can use it here.\nBut besides the usual arithmetic and trigonometric operations,\nrecent vector hardware supports two things that are valuable for Green Tea:\nvery wide registers, and sophisticated bit-wise operations.</p><p>Most modern x86 CPUs support AVX-512, which has 512-bit wide vector registers.\nThis is wide enough to hold all of the metadata for an entire page in just two registers,\nright on the CPU, enabling Green Tea to work on an entire page in just a few straight-line\ninstructions.\nVector hardware has long supported basic bit-wise operations on whole vector registers, but starting\nwith AMD Zen 4 and Intel Ice Lake, it also supports a new bit vector ‚ÄúSwiss army knife‚Äù instruction\nthat enables a key step of the Green Tea scanning process to be done in just a few CPU cycles.\nTogether, these allow us to turbo-charge the Green Tea scan loop.</p><p>This wasn‚Äôt even an option for the graph flood, where we‚Äôd be jumping between scanning objects that\nare all sorts of different sizes.\nSometimes you needed two bits of metadata and sometimes you needed ten thousand.\nThere simply wasn‚Äôt enough predictability or regularity to use vector hardware.</p><p>If you want to nerd out on some of the details, read along!\nOtherwise, feel free to skip ahead to the <a href=\"https://go.dev/blog/greenteagc#evaluation\">evaluation</a>.</p><p>To get a sense of what AVX-512 GC scanning looks like, take a look at the diagram below.</p><p>There‚Äôs a lot going on here and we could probably fill an entire blog post just on how this works.\nFor now, let‚Äôs just break it down at a high level:</p><ol><li><p>First we fetch the ‚Äúseen‚Äù and ‚Äúscanned‚Äù bits for a page.\nRecall, these are one bit per object in the page, and all objects in a page have the same size.</p></li><li><p>Next, we compare the two bit sets.\nTheir union becomes the new ‚Äúscanned‚Äù bits, while their difference is the ‚Äúactive objects‚Äù bitmap,\nwhich tells us which objects we need to scan in this pass over the page (versus previous passes).</p></li><li><p>We take the difference of the bitmaps and ‚Äúexpand‚Äù it, so that instead of one bit per object,\nwe have one bit per word (8 bytes) of the page.\nWe call this the ‚Äúactive words‚Äù bitmap.\nFor example, if the page stores 6-word (48-byte) objects, each bit in the active objects bitmap\nwill be copied to 6 bits in the active words bitmap.\nLike so:</p></li></ol><figure><div> ‚Üí <pre>000000 000000 111111 111111 ...</pre></div></figure><ol start=\"4\"><li><p>Next we fetch the pointer/scalar bitmap for the page.\nHere, too, each bit corresponds to a word (8 bytes) of the page, and it tells us whether that word\nstores a pointer.\nThis data is managed by the memory allocator.</p></li><li><p>Now, we take the intersection of the pointer/scalar bitmap and the active words bitmap.\nThe result is the ‚Äúactive pointer bitmap‚Äù: a bitmap that tells us the location of every\npointer in the entire page contained in any live object we haven‚Äôt scanned yet.</p></li><li><p>Finally, we can iterate over the memory of the page and collect all the pointers.\nLogically, we iterate over each set bit in the active pointer bitmap,\nload the pointer value at that word, and write it back to a buffer that\nwill later be used to mark objects seen and add pages to the work list.\nUsing vector instructions, we‚Äôre able to do this 64 bytes at a time,\nin just a couple instructions.</p></li></ol><p>Part of what makes this fast is the  instruction,\npart of the ‚ÄúGalois Field New Instructions‚Äù x86 extension,\nand the bit manipulation Swiss army knife we referred to above.\nIt‚Äôs the real star of the show, since it lets us do step (3) in the scanning kernel very, very\nefficiently.\nIt performs a bit-wise <a href=\"https://en.wikipedia.org/wiki/Affine_transformation\" rel=\"noreferrer\" target=\"_blank\">affine\ntransformations</a>,\ntreating each byte in a vector as itself a mathematical vector of 8 bits\nand multiplying it by an 8x8 bit matrix.\nThis is all done over the <a href=\"https://en.wikipedia.org/wiki/Finite_field\" rel=\"noreferrer\" target=\"_blank\">Galois field</a>,\nwhich just means multiplication is AND and addition is XOR.\nThe upshot of this is that we can define a few 8x8 bit matrices for each\nobject size that perform exactly the 1:n bit expansion we need.</p><p>For the full assembly code, see <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/scan_amd64.s;l=23;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">this\nfile</a>.\nThe ‚Äúexpanders‚Äù use different matrices and different permutations for each size class,\nso they‚Äôre in a <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/expand_amd64.s;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">separate file</a>\nthat‚Äôs written by a <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/mkasm.go;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">code generator</a>.\nAside from the expansion functions, it‚Äôs really not a lot of code.\nMost of it is dramatically simplified by the fact that we can perform most of the above\noperations on data that sits purely in registers.\nAnd, hopefully soon this assembly code <a href=\"https://go.dev/issue/73787\">will be replaced with Go code</a>!</p><p>Credit to Austin Clements for devising this process.\nIt‚Äôs incredibly cool, and incredibly fast!</p><p>So that‚Äôs it for how it works.\nHow much does it actually help?</p><p>It can be quite a lot.\nEven without the vector enhancements, we see reductions in garbage collection CPU costs\nbetween 10% and 40% in our benchmark suite.\nFor example, if an application spends 10% of its time in the garbage collector, then that\nwould translate to between a 1% and 4% overall CPU reduction, depending on the specifics of\nthe workload.\nA 10% reduction in garbage collection CPU time is roughly the modal improvement.\n(See the <a href=\"https://go.dev/issue/73581\">GitHub issue</a> for some of these details.)</p><p>We‚Äôve rolled Green Tea out inside Google, and we see similar results at scale.</p><p>We‚Äôre still rolling out the vector enhancements,\nbut benchmarks and early results suggest this will net an additional 10% GC CPU reduction.</p><p>While most workloads benefit to some degree, there are some that don‚Äôt.</p><p>Green Tea is based on the hypothesis that we can accumulate enough objects to scan on a\nsingle page in one pass to counteract the costs of the accumulation process.\nThis is clearly the case if the heap has a very regular structure: objects of the same size at a\nsimilar depth in the object graph.\nBut there are some workloads that often require us to scan only a single object per page at a time.\nThis is potentially worse than the graph flood because we might be doing more work than before while\ntrying to accumulate objects on pages and failing.</p><p>The implementation of Green Tea has a special case for pages that have only a single object to scan.\nThis helps reduce regressions, but doesn‚Äôt completely eliminate them.</p><p>However, it takes a lot less per-page accumulation to outperform the graph flood\nthan you might expect.\nOne surprise result of this work was that scanning a mere 2% of a page at a time\ncan yield improvements over the graph flood.</p><p>Green Tea is already available as an experiment in the recent Go 1.25 release and can be enabled\nby setting the environment variable  to  at build time.\nThis doesn‚Äôt include the aforementioned vector acceleration.</p><p>We expect to make it the default garbage collector in Go 1.26, but you‚Äôll still be able to opt-out\nwith <code>GOEXPERIMENT=nogreenteagc</code> at build time.\nGo 1.26 will also add vector acceleration on newer x86 hardware, and include a whole bunch of\ntweaks and improvements based on feedback we‚Äôve collected so far.</p><p>If you can, we encourage you to try at Go tip-of-tree!\nIf you prefer to use Go 1.25, we‚Äôd still love your feedback.\nSee <a href=\"https://go.dev/issue/73581#issuecomment-2847696497\">this GitHub\ncomment</a> with some details on\nwhat diagnostics we‚Äôd be interested in seeing, if you can share, and the preferred channels for\nreporting feedback.</p><p>Before we wrap up this blog post, let‚Äôs take a moment to talk about the journey that got us here.\nThe human element of the technology.</p><p>The core of Green Tea may seem like a single, simple idea.\nLike the spark of inspiration that just one single person had.</p><p>But that‚Äôs not true at all.\nGreen Tea is the result of work and ideas from many people over several years.\nSeveral people on the Go team contributed to the ideas, including Michael Pratt, Cherry Mui, David\nChase, and Keith Randall.\nMicroarchitectural insights from Yves Vandriessche, who was at Intel at the time, also really helped\ndirect the design exploration.\nThere were a lot of ideas that didn‚Äôt work, and there were a lot of details that needed figuring out.\nJust to make this single, simple idea viable.</p><p>The seeds of this idea go all the way back to 2018.\nWhat‚Äôs funny is that everyone on the team thinks someone else thought of this initial idea.</p><p>Green Tea got its name in 2024 when Austin worked out a prototype of an earlier version while cafe\ncrawling in Japan and drinking LOTS of matcha!\nThis prototype showed that the core idea of Green Tea was viable.\nAnd from there we were off to the races.</p><p>Throughout 2025, as Michael implemented and productionized Green Tea, the ideas evolved and changed even\nfurther.</p><p>This took so much collaborative exploration because Green Tea is not just an algorithm, but an entire\ndesign space.\nOne that we don‚Äôt think any of us could‚Äôve navigated alone.\nIt‚Äôs not enough to just have the idea, but you need to figure out the details and prove it.\nAnd now that we‚Äôve done it, we can finally iterate.</p><p>The future of Green Tea is bright.</p><p>Once again, please try it out by setting  and let us know how it goes!\nWe‚Äôre really excited about this work and want to hear from you!</p>","contentLength":19499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Agent HQ: Any agent, any way you work","url":"https://github.blog/news-insights/company-news/welcome-home-agents/","date":1761667695,"author":"Kyle Daigle","guid":319228,"unread":true,"content":"<p>The current AI landscape presents a challenge we‚Äôre all too familiar with: incredible power fragmented across different tools and interfaces. At GitHub, we‚Äôve always worked to solve these kinds of systemic challenges‚Äîby making Git accessible, code review systematic with pull requests, and automating deployment with Actions.&nbsp;</p><p>With 180 million developers, <a href=\"https://github.blog/news-insights/octoverse/octoverse-a-new-developer-joins-github-every-second-as-ai-leads-typescript-to-1/\"><strong>GitHub is growing at its fastest rate ever</strong></a>‚Äîa new developer joining every second. What‚Äôs more, 80% of new developers are using Copilot in their first week. AI isn‚Äôt just a tool anymore; it‚Äôs an integral part of the development experience. Our responsibility is to ensure this new era of collaboration is powerful, secure, and seamlessly integrated into the workflow you already trust.</p><p>At GitHub Universe, we‚Äôre announcing , GitHub‚Äôs vision for the next evolution of our platform. Agents shouldn‚Äôt be bolted on. They should work the way you already work. <strong>That‚Äôs why we‚Äôre making agents native to the GitHub flow.</strong></p><p>Agent HQ transforms GitHub into an open ecosystem that unites every agent on a single platform. .</p><p>To bring this vision to life, we‚Äôre shipping a suite of new capabilities built on the primitives you trust. This starts with a ,a single command center to assign, steer, and track the work of multiple agents from anywhere. It extends to  with new ways to plan and customize agent behavior. And it is backed by enterprise-grade functionality: a new generation of ,a dedicated to govern AI access and agent behavior, and a  to understand the impact of AI on your work.&nbsp;</p><p><strong>We are also deeply committed to investing in our platform and strengthening the primitives you rely on every day</strong>. This new world of development is powered by that foundational work, and we look forward to sharing more updates.&nbsp;</p><h2>GitHub is your Agent HQ: An open ecosystem for all agents&nbsp;&nbsp;</h2><p>The future is about giving you the power to orchestrate a fleet of specialized agents to perform complex tasks in parallel, not juggling a patchwork of disconnected tools or relying on a single agent. As the pioneer of asynchronous collaboration, we believe it‚Äôs our responsibility to make sure these next-generation async tools.&nbsp;</p><p>Withwhat‚Äôs  changing is just as important as what You‚Äôre still working with the primitives you know‚ÄîGit, pull requests, issues‚Äîand using your preferred compute, whether that‚Äôs GitHub Actions or self-hosted runners. You‚Äôre accessing agents through your existing paid Copilot subscription.&nbsp;</p><p>On top of that foundation, we‚Äôre opening the doors to a new world of capability.Over the coming months, <strong>coding agents from Anthropic, OpenAI, Google, Cognition, and xAI will be available on GitHub</strong><strong>as part of your paid GitHub Copilot subscription</strong>.</p><p>Don‚Äôt want to wait? Starting this week, Copilot Pro+ users can begin working with <a href=\"https://code.visualstudio.com/insiders/\"></a>, the first of our partner agents to extend beyond its native surfaces and directly into the editor.</p><h2>Mission control: Your command center, wherever you build</h2><p>The power of Agent HQ comes from, a unified command center that follows you wherever you work. It‚Äôs not a single destination; it‚Äôs a consistent interface across GitHub, VS Code, mobile, and the CLI that lets you direct, monitor, and manage every AI-driven task. With mission control, you can choose from a fleet of agents, assign them work in parallel, and track their progress from any device.&nbsp;</p><ul><li>New  that give you granular oversight over when to run CI and other checks for agent-created code.</li><li> to control which agent is building the task, managing access, and policies just like you would with any other developer on your team.</li><li><strong>One-click merge conflict resolution</strong>, improved file navigation, and better code commenting capabilities.</li><li><strong>New integrations for Slack and Linear,</strong> on top of our recently announced connections for Atlassian Jira, Microsoft Teams and Azure Boards, and Raycast.&nbsp;</li></ul><h2>New in VS Code: Plan, customize, and connect</h2><p>Mission control is in VS Code, too, so you‚Äôve got a single view of all your agents running in VS Code, in the Copilot CLI, or on GitHub.</p><p>Today‚Äôs <strong>brand new release in VS Code</strong> is all about working alongside agents on projects, and it‚Äôs not surprising that great results start with a great plan. Getting the context right before a project is critical, but that same context needs to carry through into the work. Copilot already adapts to the way your team works by learning from your files and your project‚Äôs culture, but sometimes you need more pointed context.</p><p>So today, we‚Äôre introducing , which works with Copilot, and asks you clarifying questions along the way, to help you to build a step-by-step approach for your task. Providing the context upfront improves what Copilot can do and helps you find gaps, missing decisions, or project deficiencies early in the process‚Äîbefore any code is written. Once you approve, your plan goes to Copilot to start implementing, whether that‚Äôs locally in VS Code or using an agent in the cloud.</p><p>For even finer control, you can now create custom agents in VS Code with  files, source-controlled documents that let you set clear rules and guardrails such as ‚Äúprefer this logger‚Äù or ‚Äúuse table-driven tests for all handlers.‚Äù This shapes Copilot‚Äôs behavior without you re-prompting it every time.</p><p>Now you can rely on the new . VS Code is the  editor that supports the full MCP specification. Discover, install, and enable MCP servers like Stripe, Figma, Sentry, and others, with a single click. When your task calls for a specialist, create custom agents in GitHub Copilot with their own system prompt and tools to help you define the ways you want Copilot to work.&nbsp;</p><h2>Increased confidence and control for your team</h2><p>Agent HQ doesn‚Äôt just give you more power‚Äîit gives you confidence. Ensuring code quality, understanding AI‚Äôs influence on your workflow, and maintaining control over how AI interacts with your codebase and organization are essential for your team‚Äôs success, and we‚Äôre tackling these challenges head-on.</p><p>When it comes to code quality, the core problem is that ‚ÄúLGTM‚Äù doesn‚Äôt always mean ‚Äúthe code is healthy.‚Äù A review can pass, but can still degrade the codebase and quickly become long-term technical debt. With , in public preview today, you‚Äôve got org-wide visibility, governance, and reporting to systematically improve code maintainability, reliability, and test coverage across every repository. Enabling it extends Copilot‚Äôs security checks to look at the maintainability and reliability impact of the code that‚Äôs been changed.</p><p>And we‚Äôve into the Copilot coding agent‚Äôs workflow, too, so Copilot gets an initial first-line review and addresses problems (before you even see the code).&nbsp;</p><p>As an organization, you need to know how Copilot is being used. So today, we‚Äôre announcing the public preview of the , showing Copilot‚Äôs impact and critical usage metrics across your entire organization.</p><p>For enterprise administrators who are managing AI access, including AI agents and MCP, we‚Äôre focused on providing consistent ‚Äî<strong>your agent governance layer</strong>. Set security policies, audit logging, and manage access all in one place. Enterprise admins can also control which agents are allowed, define access to models, and obtain metrics about the Copilot usage in your organization.</p><h2>For developers, by developers&nbsp;</h2><p>We built Agent HQ because we‚Äôre developers, too. We know what it‚Äôs like when it feels like your tools are you instead of helping you. When ‚ÄúAI-powered‚Äù ends up meaning more context-switching, more babysitting, more subscriptions, and more time explaining what you need to get the value you were promised.&nbsp;</p><p>Agent HQ isn‚Äôt about the hype of AI. It‚Äôs about the reality of shipping code.&nbsp; It‚Äôs about bringing order and governance to this new era without compromising choice. It‚Äôs about giving the power to build faster, with more confidence, and on your terms.</p><p>Welcome home. Let‚Äôs build.&nbsp;</p>","contentLength":7911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Octoverse: A new developer joins GitHub every second as AI leads TypeScript to #1","url":"https://github.blog/news-insights/octoverse/octoverse-a-new-developer-joins-github-every-second-as-ai-leads-typescript-to-1/","date":1761667626,"author":"GitHub Staff","guid":319227,"unread":true,"content":"<p>If 2025 had a theme, it would be growth. Every second, more than one new developer on average joined GitHub‚Äîover 36 million in the past year. It‚Äôs our fastest absolute growth rate yet and <strong>180 million-plus developers now work and build on GitHub</strong>.&nbsp;</p><p>The release of <a href=\"https://github.com/features/copilot?utm_source=octoverse-copilot-cta&amp;utm_medium=octoverse&amp;utm_campaign=universe25\">GitHub Copilot Free</a> in late 2024 coincided with a step-change in developer sign-ups, exceeding prior projections. Beyond bringing millions of new developers into the ecosystem, we saw record-level activity across repositories, pull requests, and code pushes. Developers created more than 230 new repositories every minute, merged  pull requests on average each month (+23% YoY), and pushed nearly 1 billion commits in 2025 (+25.1% YoY)‚Äîincluding a record of nearly 100 million in August alone.&nbsp;</p><p>This surge in activity coincides with a structural milestone: for the first time, TypeScript overtook both Python and JavaScript in August 2025 to become the most used language on GitHub, reflecting how developers are reshaping their toolkits. This marks the most significant language shift in more than a decade.</p><p>And the growth we see is global: India alone added more than 5 million developers this year (over 14% of all new accounts) and is on track to account for one in every three new developers on GitHub by 2030.&nbsp;</p><p><strong>This year‚Äôs data highlights three key shifts:&nbsp;</strong></p><ol><li><strong>Generative‚ÄØAI is now standard in development.</strong> More than 1.1 million public repositories now use an LLM SDK with 693,867 of these projects created in just the past 12 months alone (+178% YoY, Aug ‚Äò25 vs. Aug ‚Äò24). Developers also merged a record 518.7M pull requests (+29% YoY). Moreover, AI adoption starts quickly: 80% of new developers on GitHub use Copilot in their first week.</li><li><strong>TypeScript is now the most used language on GitHub</strong>. In August 2025, TypeScript overtook both Python and JavaScript. Its rise illustrates how developers are shifting toward typed languages that make agent-assisted coding more reliable in production. It doesn‚Äôt hurt that nearly every major frontend framework now scaffolds with TypeScript by default. Even still, Python remains dominant for AI and data science workloads, while the JavaScript/TypeScript ecosystem still accounts for more overall activity than Python alone.</li><li><strong>AI is reshaping choices, not just code. </strong>In the past, developer choice meant picking an IDE, language, or framework. In 2025, that‚Äôs changing. We see correlations between the rapid adoption of AI tools and evolving language preferences. This and other shifts suggest AI is influencing not only how fast code is written, but which languages and tools developers use.</li></ol><p>And one of the biggest things in 2025?  Early signals in our data are starting to show their impact, but ultimately point to one key thing: we‚Äôre just getting started and we expect far greater activity in the months and years ahead.&nbsp;</p><p><strong>üí° Oh, and if you‚Äôre a visual learner, we have you covered.üëá</strong></p><h2>The state‚ÄØof‚ÄØGitHub in‚ÄØ2025: A year of record growth</h2><p>In 2023, GitHub crossed 100 million developers after nearly three years of growth from 50 million to 100 million. But the past year alone has rewritten that curve with our fastest absolute growth yet. Today, more than 180 million developers build on GitHub.</p><p><strong>So, what does ‚Äúmore than one new developer joining GitHub every second on average‚Äù actually mean?&nbsp;</strong></p><ul><li><strong>Developers are converging on GitHub. </strong>More than 36 million developers joined GitHub in a single year (23% YoY), confirming GitHub as the primary hub for collaboration.&nbsp;</li><li><strong>AI adoption starts immediately. </strong>We see nearly 80% of new developers on GitHub use <a href=\"https://github.com/features/copilot?utm_source=octoverse-copilot-cta&amp;utm_medium=octoverse&amp;utm_campaign=universe25\">GitHub Copilot</a> within their first week, offering evidence that AI is now an expectation among new coders.&nbsp;</li><li><strong>The talent boom is geographically diverse. </strong>Every minute, ~25 developers joined from APAC, ~12 from Europe, ~6.5 from Africa and the Middle East, and ~6 from LATAM. India alone added over 5 million developers this year.&nbsp;</li></ul><h3>GitHub Copilot steepened growth curves</h3><p>Historically, developer sign ups and repository creation followed predictable year-over-year patterns. The launch of <a href=\"https://github.com/features/copilot?utm_source=octoverse-copilot-cta&amp;utm_medium=octoverse&amp;utm_campaign=universe25\">Copilot Free</a> in December 2024 accelerated those curves globally, giving millions access to AI-powered workflows for the first time. The end result? Our typical models for growth overturned dramatically.&nbsp;&nbsp;</p><h3>Private and public repositories play different but interdependent roles</h3><p>In 2025, 81.5% of contributions happened in private repositories, while 63% of all repositories were public. The split highlights GitHub‚Äôs dual role: most day-to-day work takes place in private projects, but depends on libraries, models, and frameworks in public open source.</p><p>Private repositories also grew faster (+33% YoY) than public repositories (+19% YoY), reflecting the growth in organizational development happening on GitHub. We also sometimes see open source software (OSS) work start in private projects.</p><figure><table><thead><tr></tr></thead><tbody><tr><td>Enterprise and team‚Äëlevel collaboration is happening on GitHub.&nbsp;</td></tr><tr><td>The  of work is smaller, yet these projects supply the libraries, models, and workflows that power the broader ecosystem.</td></tr></tbody></table></figure><ul><li>are now on GitHub</li><li> total repositories with  new repositories in 2025 marking our biggest year yet</li><li> private repositories (up ‚ÄØ33%) underscore the activity happening outside of the public eye.&nbsp;</li><li><strong>Open source and public projects represent the majority of repositories on GitHub. </strong>63% of all repositories are open source or public.</li></ul><h2>Developer productivity: shipping more, waiting less</h2><p>2025 marked the most active 12-month period in GitHub history with more than 1.12B contributions to public and open source projects. Following <a href=\"https://github.blog/enterprise-software/devops/measuring-enterprise-developer-productivity/\">the SPACE framework</a> (a model that looks at developer Satisfaction, Performance, Activity, Communication, and Efficiency), this increase reflects record levels of developer activity. As developers are increasingly working with LLMs and agents, there are some new, notable correlations in this year‚Äôs data.</p><h3>Developer activity reached record levels in 2025</h3><p>Across every productivity signal on GitHub, developers set new records in 2025.</p><figure><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table></figure><p>Code pushes are driving the surge with more than 986M commits in 2025 (+25% YoY) and monthly pushes topping 90M by May. Other activity followed:</p><ul><li> +20.4% (47.5M vs 39.5M)</li><li> +11.3% (17.5M vs 15.7M)</li><li> essentially flat (+0.35%)</li><li> down -27% (sharp decline)</li></ul><p>These are <strong>observational signals rather than causal claims </strong>and more work is needed to understand the full impact AI is having in software development.&nbsp;</p><h3>Jupyter Notebooks and Dockerfiles highlight two stages of modern development</h3><p>Notebooks are now a mature tool for experimentation, while Dockerfiles are considered  bridge to reproducibility and production. In 2025, <strong>2.4 million repositories used Notebooks (+75% YoY)</strong> and <strong>1.9 million used Dockerfiles (+120% YoY).</strong> This growth is likely fueled by the need to sandbox agents and LLMs, and containerization is a practical method to run and scale them securely.</p><figure><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table></figure><h3>AI agents enter the mainstream</h3><ul><li><strong>Developers are using AI and agentic tools to build and share their work faster. </strong>Between the total number of developers on GitHub growing in tandem with the release of popular agentic tools and the overall activity increases in languages such as TypeScript and Python, 2025 data shows an increase in rapid prototyping and experimentation.&nbsp;</li></ul><h2>Where the world codes‚ÄØin‚ÄØ2025</h2><p>The last five years have redrawn not just GitHub‚Äôs developer map, but also the distribution of global activity, faster than any period on record.</p><p>India added more than 5.2 million developers in 2025, which accounts for a little over 14% of GitHub‚Äôs total +36 million new developers in 2025. That makes India the single largest source of new developers on GitHub this year, continuing its rapid rise since 2020.</p><ul><li><strong>Significant growth came from India, Brazil, and Indonesia. </strong>These regions combine large, young developer populations with expanding internet access and booming startup ecosystems. Many are also seeing some of the fastest growth in AI-related projects, as local companies adopt new tools to compete globally.</li><li>Over the past five years (from 2020 to 2025), India, Brazil, and Indonesia have more than quadrupled the number of developers on the platform; Japan and Germany have more than tripled their developer numbers; and the USA, UK, and Canada have more than doubled the number of developers on the platform.</li><li>Brazil, in particular, is benefiting from activity investment in fintech and open banking.</li></ul><h4>Regional growth snapshots</h4><figure><table><thead><tr><th><strong>2024‚ÄØto ‚ÄØ2025 net new devs</strong></th></tr></thead><tbody><tr><td>Government skilling, AI‚Äëassisted local‚Äëlanguage tooling. Japan, in particular, has embraced digital transformation in recent years, leading to a boom in developers.&nbsp;</td></tr><tr><td>Remote hiring by US/EU firms, fintech startup density</td></tr><tr><td>Germany, United Kingdom, France&nbsp;</td><td>Cloud infrastructure spend, AI investment, startup‚Äëvisa pipelines</td></tr><tr><td>Increased mobile adoption, community bootcamps, LLMs that work locally</td></tr></tbody></table></figure><h3>Modeling the global developer landscape through 2030</h3><p>Looking ahead, our data team modeled the next five years of developer growth using regression analysis, which can help to capture more of the real-world dynamics impacting the data. (You can get more information about this in our methodology section.)&nbsp;</p><p>The results of our analysis suggest India will continue to expand its lead, reaching <strong>57.5 million developers by 2030,</strong> and accounting for more than one in three of all projected sign ups worldwide. The United States will be the second-largest community with more than 40 million developers expected, while Brazil (19.6M), Japan (11.7M), and the United Kingdom (11M) round out the top five.</p><p><strong>Notably, emerging regions across Africa and the Middle East show momentum with Egypt, Nigeria, Kenya, and Morocco all projected to add millions of developers in the coming years</strong>. This points to a developer population that is not only growing but diversifying geographically at unprecedented speed.</p><ul><li> new developers who joined GitHub this year comes from a country that wasn‚Äôt in the global top 10 in 2020.</li><li>added the most developers this year of any country, outpacing the US in growth</li><li>:‚ÄØThe number of contributors to generative AI projects on GitHub continues to grow fast, and those contributors are working around the world.&nbsp;&nbsp;</li></ul><h2>Open source in 2025: activity and influence in the AI era</h2><p>Open source development reached record levels this year with <strong>1.12 billion contributions</strong> across public repositories (+13% YoY). March 2025 marked the largest single month of new open source contributors in GitHub history: 255,000 first-timers.</p><p>In total, 395 million public repositories hosted 1.12 billion contributions and 518.7 million merged pull requests‚Äîeach a record.&nbsp;</p><h3>This year‚Äôs fastest‚Äëgrowing projects by contributors</h3><p>Six of the 10 fastest-growing repositories were AI infrastructure projects, underscoring demand for runtimes, orchestration, and efficiency tools.</p><h3>The top open source projects by contributors</h3><p>2025‚Äôs top projects split between AI infrastructure (vllm, ollama, huggingface/transformers) and enduring ecosystems (vscode, godot, home-assistant).</p><ul><li>On one side, projects like , , , , and  dominate, confirming that contributors are investing in the foundation layers of AI‚Äîmodel runtimes, inference engines, and orchestration frameworks.</li><li>On the other side, mainstay ecosystems such as , , , and  continue to attract steady contributor bases, showing that open source momentum extends well beyond AI.</li></ul><p>? AI infrastructure is emerging as a major magnet, but developer ecosystems remain strong.</p><h3>The fastest-growing projects by contributors show AI‚Äôs impact along with evergreen and utility projects</h3><p><strong>We see a mix of projects driving the fastest growth</strong>. zen-browser/desktop leads the pack, with fast-rising, AI-focused projects like vllm-project/vllm, continue-dev/continue, ollama/ollama, and Aider-AI/aider showing the pull of local inference, coding agents, and model runners.</p><p><strong>Growth in open source is broad. </strong>AI infrastructure projects are prominent among top-growth repositories. When we zoom out to the top 20 projects (not all of these are captured in our above graphic), we see a few things at play:&nbsp;</p><ol><li><strong>Reproducibility and dependency hygiene are hot. </strong>The rise of astral-sh/uv and NixOS/nixpkgs points to a hunger for deterministic builds, faster installs, and less ‚Äúworks on my machine.‚Äù</li><li><strong>Performance-centric developer tools win attention.</strong> Ghostty, Tailwind CSS, and uv are all about speed, tight feedback loops, and minimal friction.</li><li><strong>Developers are contributing to projects that emphasize privacy and control.</strong> Zen Browser and Clash-Verge reflect interest in privacy, content control, and routing around networks.</li><li><strong>Open source social media continues to grow.</strong> As one of the biggest social projects, Bluesky‚Äôs momentum suggests developers are still investing in open protocols and portable identity.</li></ol><h3>AI, tinkering, and frontend projects attract first-time contributors</h3><p>Nearly 20% of the most popular open source projects among first-time contributors in 2025 were AI-focused. But we‚Äôre also seeing other project types capture mindshare among developers who are new to open source.&nbsp;</p><ul><li><strong>Most of the most popular projects sit firmly in AI infrastructure. </strong>Repositories like ollama/ollama, comfyanonymous/ComfyUI, and ultralytics/ultralytics appear prominently, confirming that newcomers want to experiment with models, create local runners, or build pipelines.</li><li><strong>Major platforms bring visibility. </strong>microsoft/vscode shows up as one of the top destinations for first-time contributors, underscoring the pull of widely used developer tools and the scale of contribution opportunities within them.</li><li><strong>firstcontributions/first-contributions exist almost entirely to help people submit their first pull request</strong>. Their year-over-year presence says first-timers still need and seek low-friction practice sandboxes.</li><li><strong>Smart-home, mobile, and game-dev ecosystems attract newcomers.</strong> Smart-home (home-assistant/core), mobile (flutter/flutter, expo/expo), game-dev (godotengine/godot), and 3D printing (bambulab/BambuStudio) rank closely behind the learning repositories. These ecosystems offer visible results on day one, which is perfect for those who want to ‚Äúlearn by doing.‚Äù</li></ul><p><strong>Frontend and dev tool projects also light up.</strong> shadcn/ui and uBlockOrigin/uAssets show that CSS, UI, and browser tooling remain magnets for fresh contributors.</p><h3>The global landscape in open source activity</h3><ul><li><strong>India now has the largest public and open source contributor base in the world.</strong> This reflects both the country‚Äôs booming developer population and its increasing role in OSS adoption.</li><li><strong>The U.S. continues to lead in contributions.</strong> Despite having fewer contributors, U.S.-based developers contributed more to public and open source projects on GitHub. This suggests higher per-developer activity.</li><li><strong>Brazil, Indonesia, and Germany round out the next tier.</strong> Brazil contributes both strong headcount and volume, while Indonesia enters the top 5 for contributors, showing how emerging regions are shaping OSS.</li></ul><p>‚Äã‚ÄãGovernance is not keeping pace with velocity. This gap presents an opportunity for developers, organizations, and companies to contribute documentation as well as code.&nbsp;</p><ul><li> ~63% of public repositories include a README, holding steady year over year.&nbsp;</li><li> At , contributor guides remain an ecosystem-wide opportunity as the number of first-time contributors increases.</li><li> 2% penetration; governance still lags growth.</li></ul><p>Key repository files like README or a LICENSE file are more than formalities. They‚Äôre foundational to scaling inclusive, legal, secure, and sustainable collaboration. This <a href=\"https://github.blog/enterprise-software/collaboration/a-checklist-and-guide-to-get-your-repository-collaboration-ready/\">guide to getting your repository collaboration-ready</a> shares what documentation is most important for fostering a sense of shared ownership.</p><ul><li> public and open source repositories (+19% YoY)</li><li> public and open source contributions (+13% YoY)</li><li> pull requests merged (+29% YoY)</li><li> of top 10 open source projects by contributors are AI‚Äëfocused</li><li>Only  repositories ships with a Code of Conduct</li></ul><h2>Security: from ‚Äúshift left‚Äù to secure‚ÄØby‚ÄØdefault</h2><p>Average fix times for critical severity vulnerabilities have improved by 30% over the past year, as remediation is beginning to keep pace with faster software development.&nbsp;</p><p><strong>Automation is driving this acceleration</strong>. Dependabot usage more than doubled (846k projects, +137% YoY), and AI tools like Copilot Autofix are resolving <a href=\"https://owasp.org/www-project-top-ten/\">common OWASP Top 10</a> issues across thousands of repositories every month. This is underscored by the fact that in 2025, 26% fewer repositories received critical alerts through a combination of increased automation and AI usage.&nbsp;</p><p>At the same time, new risks are emerging. Broken Access Control overtook Injection as the most common CodeQL alert, flagged in <strong>151k+ repositories (+172% YoY).</strong> Much of this stems from misconfigured permissions in CI/CD pipelines and AI-generated scaffolds that skip critical auth checks (GitHub‚Äôs engineers <a href=\"https://github.blog/security/web-application-security/inside-github-how-we-hardened-our-saml-implementation/\">published a walkthrough of how they improved their SAML authentication flow</a>, which offers some valuable lessons).&nbsp;</p><h3>Automation is working (until the merge queue stalls)</h3><p><strong>Developers are automating more build, test, and security activity.</strong> In 2025, we saw developers use 11.5 billion total GitHub Actions minutes (measured in CPU minutes) in public projects for free. That‚Äôs up 35% year over year from 8.5 billion GitHub Actions in 2024. Note: in last year‚Äôs report, we included GitHub Actions minutes across public projects and self-hosted usage. If we use the same rubric this year, 13.5 billion minutes were used, which is up 30% from last year.</p><p><strong>Automation raises fixes quickly, but merges still stall when approval depends on humans or policy.</strong> Projects that configure Dependabot with auto-merge rules remediate vulnerabilities more consistently than those relying solely on manual review.</p><ul><li><strong>We saw a peak of more than 12M Dependabot alerts in December 2022</strong>, which came a year after the Log4Shell vulnerability and immediately after key vulnerabilities in OpenSSL.</li><li>Monthly openings have since settled near the 3-4M range, but merges hover around 1M.  within the same month it‚Äôs proposed.</li><li><strong>Security alerts follow the same pattern</strong>: brief spikes when new CVEs land, then a long tail of unresolved notifications (a number of which are likely attributable to zombie projects that are no longer maintained).</li></ul><p>In 2025, we saw 30% faster fixes of critical severity vulnerabilities with 26% fewer repositories receiving critical alerts. And this acceleration is happening at scale with the average fix time shrinking from 37 to 26 days in total.&nbsp;&nbsp;</p><h3>Configuring and codifying security</h3><p>Repositories that define Dependabot behavior in  more than doubled this year (), marking a shift from ‚Äúnotify me‚Äù to <strong>‚Äúpatch me automatically, within guardrails.‚Äù</strong></p><figure><table><thead><tr></tr></thead><tbody><tr><td>Repositories with </td></tr></tbody></table></figure><h3>CodeQL in 2025: Broken access control vulnerabilities spike</h3><p>Broken Access Control overcame Injection to become the top CodeQL alert, flagged in 151k+ repositories. New CodeQL coverage for GitHub Actions revealed widespread misconfigured permissions and token scopes.</p><p>This points to a broader issue: <strong>authentication and authorization remain difficult for both developers and LLMs.</strong> Injection still dominates JavaScript, but Broken Access Control now leads in Python, Go, Java, and C++ (languages where AI-assisted ‚Äúvibe coding‚Äù sometimes scaffolds endpoints that look correct but lack critical auth checks).</p><p><strong>That same category became the fastest-growing target for Copilot Autofix.</strong> By mid-2025, developers were accepting AI-generated fixes for Broken Access Control in 6,000+ repositories per month. Autofix also gained traction for Injection (3,100 projects), Insecure Design (2,300 projects), and Logging/Monitoring failures (3,500 projects).</p><p>: 47 of the top 50 open source projects (94%) defined by their Mona ranking (combined ranking of stars, forks, and issue authors) now use the <a href=\"https://github.com/ossf/scorecard\">OpenSSF Scorecard</a> via GitHub Actions or are independently scanned, bringing real-time checks for security best practices.&nbsp;</p><h2>The top programming languages of 2025: TypeScript jumps to #1 while Python takes #2</h2><p><strong>By GitHub contributor counts, August 2025 marks the first time TypeScript emerged as the most used language on GitHub, surpassing Python by ~42k contributors </strong>(other industry indices use different methodologies and may still rank JavaScript and Python higher). This caps a decade-long trend of developers shifting toward typed JavaScript and signals a new default for modern development.</p><ul><li> grew by over 1 million contributors in 2025 (+66% YoY), driven by frameworks that scaffold projects in TypeScript by default and by AI-assisted development that benefits from stricter type systems.</li><li> remains dominant in AI and data science with 2.6 million contributors (+48% YoY). Jupyter Notebook remains the go-to exploratory environment for AI (‚âà403k repositories;  inside AI-tagged projects).</li><li> is still massive (2.15M contributors), but its growth slowed as developers shifted toward TypeScript.</li></ul><p>Together, TypeScript and Python now account for more than 5.2 million contributors (roughly 3% of all active GitHub developers in August 2025). The rise of typed languages suggests AI isn‚Äôt just changing the speed of coding, but also influencing which languages teams trust to take AI-generated code into production.</p><figure><table><thead><tr><th><strong>YoY % growth (Aug 2024 vs. Aug 2025)</strong></th></tr></thead><tbody><tr><td>TypeScript overtook Python and JavaScript for #1 growth, showing its dominance in new green-field development.</td></tr><tr><td>Considered the lingua franca of AI and ML, Python‚Äôs usage has increased significantly amidst generative AI work.&nbsp;</td></tr><tr><td>Still massive in scale, but more incremental growth as usage shifts toward TypeScript.</td></tr><tr><td>Java continues its steady enterprise-driven growth.</td></tr><tr><td>Cloud, desktop, and game dev keep momentum for C#.</td></tr></tbody></table></figure><p><strong>Python still trails the combined JavaScript and TypeScript ecosystem</strong>, a continuation of last year‚Äôs trend that highlights just how large the typed and untyped JavaScript community remains.&nbsp;</p><p>But starting in 2025, <strong>Python‚Äôs growth curve began to track almost identically in parallel with JavaScript and TypeScript</strong>, suggesting that AI adoption is influencing language choice across these ecosystems.</p><ul><li><strong>Python dominates AI projects.</strong> It remains the clear leader inside AI-tagged repositories, where Jupyter Notebook usage nearly doubled in 2025 offering evidence of its role as the go-to language for prototyping, training, and orchestrating AI workloads.</li><li> TypeScript‚Äôs growth confirms our 2024 observation: much of what was previously counted as ‚ÄúJavaScript‚Äù activity already came through TypeScript transpilation pipelines. The data shows typed languages are increasingly becoming the default.</li><li><strong>Enterprise stacks endure.</strong> Java and C# each added over 100k contributors this year, showing steady growth across large enterprise and game-dev environments even as AI reshapes the landscape.</li><li><strong>Legacy experiments emerge.</strong> COBOL appeared in our dataset with nearly 3,000 active developers‚Äîlikely driven by organizations and hobbyists creating AI-assisted tutorial repositories aimed at modernizing legacy codebases.</li></ul><h3>The fastest-growing languages by percentage growth</h3><p>The following languages may not have the biggest developer communities behind them, but each has at least 1,000 monthly contributors and they‚Äôre posting the fastest year-over-year growth rates on GitHub.&nbsp;</p><figure><table><thead><tr></tr></thead><tbody><tr><td>Luau is Roblox‚Äôs scripting language and a  language, reflecting a broader industry trend toward typed flexibility.&nbsp;</td></tr><tr><td>As a , Typst aims to make academic and technical publishing faster, less cryptic, and more collaborative.</td></tr><tr><td>Astro‚Äôs ‚Äúislands architecture‚Äù and focus on shipping zero-JavaScript by default resonate with developers building fast, content-heavy sites (we <a href=\"https://github.com/github-linguist/linguist/issues/5459\">added Astro to Linguist in 2021</a>, which is our source for languages).</td></tr><tr><td>As <strong>Laravel‚Äôs templating engine</strong>, Blade rides on Laravel‚Äôs continued dominance in PHP web development.</td></tr><tr><td><strong>Offering type safety for the JavaScript world</strong>, TypeScript‚Äôs combination of JavaScript ubiquity and type safety is compelling for both greenfield and legacy projects (plus, its types work well with AI coding tools).</td></tr></tbody></table></figure><h3>Core stacks for new projects built in the last 12 months</h3><p>Nearly 80% of new repositories used just six languages: <strong>Python, JavaScript, TypeScript, Java, C++, and C#.</strong> These core languages anchor most modern development.</p><figure><table><thead><tr><th><strong>Total repositories (Sep 2024-Aug 2025)&nbsp;</strong></th><th><strong>Growth (Jan-Aug 2025 vs. Jan-Aug 2024)&nbsp;</strong></th><th><strong>What this growth tells us</strong></th></tr></thead><tbody><tr><td>AI‚Äôs default glue with growth driven by ML, agents, notebooks, and orchestration.</td></tr><tr><td>Still ubiquitous for scripts and web apps, though growth is slower as TypeScript gains share.</td></tr><tr><td>Typed standard for modern web dev. Ideal for safe API/SDK integration, especially with AI.</td></tr><tr><td>Reliable enterprise and backend workhorse. Gradual AI integration without language churn.</td></tr><tr><td>Performance-critical workloads used in game engines, inference, and embedded systems supporting AI.</td></tr><tr><td>Steady enterprise and game dev usage, with AI capabilities folded into established ecosystems.</td></tr></tbody></table></figure><ul><li><strong>Experimentation is becoming more common. </strong>Though not a language, Jupyter Notebooks grew 24.5% YoY, with exploratory LLM experiments and data analysis now generating new standalone repos rather than remaining siloed in monorepos.</li><li><strong>Performance and systems languages are rising with AI (but not evenly). </strong>C grew ~20.9% YoY and C++ grew ~11.8% YoY, reflecting demand for faster runtimes, inference engines, and hardware-optimized loops.</li><li>C# grew ~10.6% YoY, consistent with enterprise and game/tooling ecosystems. This suggests AI features are being integrated into existing .NET workflows rather than driving a wholesale language shift.</li></ul><h3>The languages powering AI development</h3><p><strong>Python and Jupyter Notebook continue to anchor new AI projects, but the story this year is Python‚Äôs growth.</strong> Python now powers nearly half of all new AI repositories (), underscoring its role as the backbone of applied AI work, from training and inference to orchestration and deployment. Jupyter Notebook remains the go-to exploratory environment for experimentation (), but the shift toward Python codebases signals more projects moving out of prototypes and into production stacks.</p><p>Front-end and app-layer languages grew sharply from smaller bases‚Äî<strong>TypeScript +77.9% (85,746)</strong> and <strong>JavaScript +24.8% (88,023)</strong>‚Äîmirroring the rise of demos, dashboards, and lightweight apps built around model endpoints.  emerged as the fastest riser, reflecting how teams codify eval harnesses, data prep, and deployment pipelines. And <strong>C++ crossed 7,800 repos (+11%)</strong>, a steady reminder of its role in performance-critical inference engines, runtimes, and hardware-close systems.</p><p>Last year we saw AI move from experiment to mainstream. In 2025, it became part of the everyday workflow. And no matter what tool developers used over the last 12 months, their work converged on GitHub.&nbsp;</p><ul><li><strong>AI-related repositories on GitHub now exceed 4.3 million, </strong>nearly doubling in less than two years.</li><li><strong>Roughly 80% of new GitHub users tried Copilot within their first week, </strong>showing that AI is no longer an advanced tool to grow into, but part of the default developer experience.</li></ul><p>Monthly contributors to generative-AI projects climbed sharply across our measurement year. From September 2024 through August 2025, months averaged ~151k contributors (median ~160k). Activity rose from ~86k in January 2025 to a peak of 206,830 in May (+132% YoY vs. May 2024). It then held near ~200k through the summer. On a like-for-like basis, Jan‚ÄìAug 2025 averaged ~175k contributors, up +108% YoY vs. Jan‚ÄìAug 2024 (~84k), indicating a durable step-change rather than a one-off spike.</p><ul><li><strong>Generative AI is becoming infrastructure.</strong> More than 1.1M public repositories now import an LLM SDK (+178% YoY, August ‚Äò25 vs. August ‚Äò24), supported by 1.05M+ contributors and 1.75M monthly commits (+4.8X since 2023).&nbsp;</li><li><strong>Growth surged early in the calendar year, then normalized as projects shifted from experimentation to shipping.</strong> Contributors ran +100-118% YoY from Feb-May 2025, then cooled to +31% (Jun), +11% (Jul), and -3% (Aug) as teams focused on shipping vs. experimenting.&nbsp;</li><li> Half (50%) of open source projects have at least one maintainer using <a href=\"https://github.com/features/copilot?utm_source=octoverse-copilot-cta&amp;utm_medium=octoverse&amp;utm_campaign=universe25\">GitHub Copilot</a>.</li><li><strong>Early evidence of a prototype-to-production pivot.</strong> Python-based code accelerated mid-2025 while Notebook growth flattened‚Äîsignaling packaging into production. (By year‚Äôs end, Notebooks rebounded, keeping pace with Python.)</li></ul><h3>Strong signals of mainstream appeal&nbsp;</h3><figure><table><tbody><tr><td><strong>178% YoY increase in projects that import an LLM SDK</strong></td><td> public repositories now import an LLM SDK;  were created in the last 12 months alone.</td><td>Growth rates indicate a shift from early experimentation to sustained building.</td></tr><tr><td><strong>Contributors up &gt;3X since 2023</strong></td><td>Monthly distinct contributors to AI repos rose from 68k (Jan 2024) to ~200k (Aug 2025). August 2025 is up 111% YoY vs. August 2024.</td><td>AI work is no longer the domain of specialists.</td></tr><tr><td><strong>Monthly contributions near 6M</strong></td><td>Monthly  to AI projects reached  hitting a peak of . August 2025 is up  vs. August 2024.</td><td>More code, more often, offering evidence of production-grade adoption and active iteration.</td></tr></tbody></table></figure><p><strong>1.13M+ public repositories now depend on generative-AI SDKs (up 178% YoY). </strong>More than  were created in the last 12 months, sharply outpacing 2024‚Äôs total (~400,000). The compounding curve that began in early 2023 shows no sign of tapering; every week, on average, we are still seeing new all-time highs.</p><p>The U.S. remains the largest source of contributions (~12.8M, 31.8%). India ranks second (~5M, 12.5%) and leads by distinct repositories (405k vs. 342k).</p><p>A second tier (Germany, Japan, U.K., Korea, Canada, Brazil, Spain, France) contributes another ~40%, globalizing the map.</p><p>A first glimpse of coding agent shows1+ million pull requests that were created between May 2025 and September 2025.</p><p>A repository-level comparison of public repositories with ‚â•1 coding agent-authored pull request vs. a random sample without Copilot coding agent shows strong selection effects: coding agent activity is skewed toward repositories with more stars, larger size, and greater age. In other words, teams aren‚Äôt only assigning coding agent to throwaway projects; they‚Äôre trying it in better-known, more established projects as well.&nbsp;</p><p>We invite the community to run <strong>within-repository experiments</strong> (A/B or stepped-wedge) and  conditioned on size, stars, age, and complexity proxies to establish robust baselines. We‚Äôll continue looking into this as we evolve coding agent across GitHub, the Copilot CLI, and more.&nbsp;</p><h3>AI is driving notable breakouts in open source</h3><p><strong>Generative AI projects continue to be among GitHub‚Äôs most popular.</strong> Projects like vllm, ragflow, and ollama outpaced the historical contributor growth of staples such as vscode, home-assistant, flutter.&nbsp;</p><figure><table><thead><tr><th><strong>Repository (age ‚â§3 yrs unless noted)</strong></th></tr></thead><tbody><tr><td>Open source  model + training/inference stack</td></tr><tr><td>Local Llama inference on CPU/GPU</td></tr><tr><td>End-to-end retrieval-augmented-generation (RAG) template</td></tr><tr><td>‚ÄúLLM-native‚Äù command-line shell that reasons over local context</td></tr><tr><td> (6.6 yrs)</td><td>Defacto Python library for model loading/fine tuning</td></tr></tbody></table></figure><ol><li><strong>Software infrastructure outpaces everything else in velocity.</strong> Brand-new generative AI repositories (‚â§ 1 yr old) are racking up star counts that took other projects a decade to accumulate.</li><li><strong>Standards are emerging in real time.</strong> The rapid rise of (MCP) shows the community coalescing around interoperability standards.</li><li><strong>AI is reshaping classic tooling.</strong> Projects like and show how local inference and AI-augmented pipelines are moving from proof-of-concept into mainstream developer workflows.</li></ol><h3>AI is helping developers fix code, too</h3><p>GitHub Copilot Autofix contributed to measurable improvements in 2025:</p><ul><li> surged the fastest, with fixes accepted in <strong>6,000+ repositories per month</strong> by mid-2025.</li><li><strong>Security logging and monitoring failures, injection, insecure design, and misconfiguration</strong> fixes also climbed sharply, each crossing into the thousands of repositories monthly.</li><li>Autofix is addressing the most common OWASP Top 10 issues‚Äînot just exotic vulnerabilities‚Äîbringing AI into the daily fabric of software security.</li></ul><p>Early adopters using agents, open standards, and self-hosted inference are already setting the norms for the next decade. Continuous AI‚Äîsystems and workflows that are updated, retrained, and deployed on an ongoing basis‚Äîis emerging.</p><ul><li><strong>Expect AI libraries to become ‚Äúplumbing.‚Äù </strong>If your stack can‚Äôt load a model or pipe context into one, you‚Äôll feel legacy-bound quickly.&nbsp;</li><li> Package your experiments early to share them with others.</li><li><strong>Watch the toolchain, not just the models. </strong>The next productivity leap may come from LLM-native editors, shells, and test runners growing out of today‚Äôs fast-rising repositories.</li><li><strong>Build for interoperability.</strong> Standards like MCP and Llama-derived protocols are gaining momentum across ecosystems.</li></ul><p>Three years ago, we said AI wouldn‚Äôt replace developers‚Äîit would bring more people into the ecosystem. The data now proves it: activity on GitHub has reached record levels, with more contributors, more repositories, and more experimentation than ever.</p><p><strong>The past year marked historic milestones</strong>:&nbsp;</p><ul><li><strong>India overtook the United States as the largest contributor base to public and open source projects on GitHub. </strong>It‚Äôs also poised to overtake the US developer population within the next few years, underscoring how global the developer community has become.</li><li><strong>TypeScript became the most used language for the first time</strong>, overtaking both Python and JavaScript and signaling a generational shift in how modern software is built.</li><li><strong>Open source remains the foundation</strong>. Public projects supply the libraries, models, and workflows that power most private development. The strength of this ecosystem and the maintainers who sustain it will determine how far and how fast the next wave of software innovation goes.</li></ul><p>The story of 2025 isn‚Äôt AI versus developers. It‚Äôs about the evolution of developers in the AI era where they orchestrate agents, shape languages, and drive ecosystems. No matter which agent, IDE, or framework they choose, GitHub is where it all converges.</p><ul><li>: Refers to September 1, 2024 through August 31, 2025.&nbsp;</li><li><strong>Year-over-year language comparisons:</strong> Unless otherwise noted, YoY values reflect same-month comparisons (e.g., Aug 2025 vs Aug 2024) to normalize for month-length and seasonality in developer activity.</li><li>Commenting on a commit, issue, pull request, pull request diff, or team discussion; creating a gist, issue, pull request, or team discussion; pushing commits to a project; and reviewing a pull request.&nbsp;</li><li>: GitHub users who have performed any of the contribution activities defined above.&nbsp;</li><li>Anyone with a GitHub account. Also sometimes referred to as a GitHub user. The open source and developer communities are an increasingly diverse and global group of people who tinker with code, make non-code contributions, conduct scientific research, and more. GitHub users drive open source innovation, and they work across industries‚Äîfrom software development to data analysis and design.</li><li>The combined number of public and private repositories on GitHub.</li><li><strong>Programming language usage: </strong>Unless otherwise noted, ‚Äúmost used‚Äù languages are ranked by the number of distinct monthly contributors who committed code in that language. This is the standard measure for ‚ÄúTypeScript became the most used language on GitHub.‚Äù</li><li>Any repository tagged with AI-related topics (e.g., ‚ÄúAI,‚Äù ‚ÄúML,‚Äù ‚ÄúLLM‚Äù) or falling under our AI classification methodology. This broad category captures general experimentation and projects adjacent to AI.</li><li>Software development tasks completed with the aid of autonomous or semi-autonomous AI tools (e.g., GitHub Copilot coding agent creating pull requests, triaging issues, or running tests).</li><li>: A GitHub Copilot feature that can independently draft code, run tests, and open draft pull requests in a secure environment‚Äîsubject to developer review and approval.</li><li>: A GitHub Copilot feature that reviews pull requests, suggests changes, and surfaces potential issues before merging.</li><li>: CPU minutes used to execute GitHub Actions (CI/CD workflows). Reported cumulatively and as YoY growth.</li><li>: GitHub‚Äôs semantic code analysis engine used to detect security vulnerabilities. Alerts are categorized by vulnerability type (e.g., Broken Access Control, Injection, Insecure Design).</li><li>: Ranks repositories based on their stars, forks, and unique issue authors in the repository. The steps to compute Mona rank: 1) Calculate individual ranks based on stars, forks, and issue authors. 2) Sum these individual ranks. 3) Assign the final ‚ÄúMona Rank‚Äù based on the summed rank.</li><li>A set of official libraries and tools published by model providers (such as OpenAI, Anthropic, Meta, Mistral, Cohere, or AI21) that make it easier for developers to connect to and use their large language models. These SDKs wrap the underlying model APIs with client libraries, helper functions, and runtime integrations, letting developers handle prompts, responses, tokens, and extensions without needing to build low-level infrastructure. To determine usage of LLM SDKs, we referenced models offered via GitHub Models. These included SDKs from DeepSeek, Grok, Mistral, Phi, OpenAI, Cohere, Llama, and AI21.&nbsp;</li></ul><ul><li><strong>Sep 1, 2024 through Aug 31, 2025</strong>.</li><li>Unless noted, metrics reflect  only. Public indicators are also available on the <a href=\"https://innovationgraph.github.com/\"></a>. For country-level reporting there, we publish metrics only when  unique developers performed the activity in the period.</li></ul><ul><li> Calendar-month metrics to show peaks/turning points (e.g., language rankings).</li><li> vs  for year-over-year trends/averages.</li><li> Historical context from  where relevant.</li></ul><ul><li> users per metric (e.g., per language). One user can appear in multiple categories in the same month, so these do not sum across categories.</li><li> Counted in a month if created or active, per metric definition.</li><li> ‚Äúpushes,‚Äù ‚Äúpull requests created/merged,‚Äù ‚Äúcomments,‚Äù etc., follow standard GitHub event definitions.</li></ul><ul><li> Month vs. the  prior year (e.g., ) are used for milestones/seasonality control as in language comparisons.</li><li> Trailing-12-month  vs the prior trailing-12-month metric are used for sustained trends (e.g., average monthly contributors, pull requests per month).</li><li><strong>Stock vs. flow (cumulative metrics):</strong> = level at a point in time (e.g., SDK repos ).  =  over a window (e.g., ). Labeled distinctly.</li></ul><ul><li>Developers are mapped to countries via , standardized to ; country aggregates observe the  publication threshold.</li></ul><p><strong>Repository &amp; language classification</strong></p><ul><li> GitHub language detection (e.g., Linguist); mixed-language repositories are attributed to  language.</li><li> ‚ÄúJupyter Notebook‚Äù is a development-environment classification and is labeled transparently.</li><li> Identified via signals such as  (imports/dependencies) and related metadata.</li><li><strong>Open source quality signals:</strong> Presence of files/policies (e.g., codes of conduct) from repository metadata.</li></ul><ul><li> tracking;  for stock growth;  (contributors, repo counts, growth rates, activity volumes).</li><li> to reduce noise (e.g., lists may require  contributors or  repos; metric-specific).</li></ul><ul><li>Count <strong>unique users per time period and per metric</strong> to avoid within-slice double counting; cross-category duplication is expected by design.</li></ul><ul><li><strong>Exclude incomplete months</strong> (e.g., the current month) from YoY/T12.</li><li> where identifiable (account flags + behavioral heuristics).</li><li>Enforce  for inclusion in growth and rankings.</li><li> against multiple internal/public sources (e.g., Innovation Graph).</li></ul><p><strong>Interpretation &amp; reproducibility</strong></p><ul><li> motivates same-month YoY for snapshots and  for structure.</li><li> primary language is repository-level; TypeScript/JavaScript mixes may appear under one language.</li><li> public-only views undercount private activity but preserve directionality.</li><li>Public counterparts for several metrics can be verified via the .</li></ul><p><strong>Developer growth projections</strong></p><ul><li>: a collection of time-series and regression models that use historical data and statistics to predict future outcomes. Forecasting models relied on historical GitHub data, sign-up rates and product usage, as well as market-sizing information.&nbsp;</li><li>No forecast is ever accurate. Backtesting shows models used for forecasting growth projections were within reasonable levels of accuracy (less than 30% Mean Absolute Percentage Error).</li><li>Forecasts do not take into account competitive landscape changes, geopolitical/economic conditions, or future covariates (product/feature releases that may shift responses differently from historical data).</li></ul>","contentLength":40063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing the 2025 GitHub Partner Award winners üéâ","url":"https://github.blog/news-insights/company-news/announcing-the-2025-github-partner-award-winners/","date":1761667200,"author":"Jamie Cooper","guid":319295,"unread":true,"content":"<p>In today‚Äôs fast-moving tech industry, partnerships are a powerful engine for scale. More than just sales channels, they unlock new markets, fuel innovation, and enhance customer impact. At GitHub, collaboration with our partners is central to how we grow and deliver value. Their expertise and shared commitment help us push boundaries, solve complex challenges, and create better outcomes for developers and businesses alike.</p><blockquote><p>Partners are a force multiplier for GitHub. Partners amplify our capabilities, expand our reach, and accelerate innovation for our joint customers. It‚Äôs exciting to see how our partner ecosystem is growing and flourishing.</p><cite>Elizabeth Pemmerl, Microsoft CVP and GitHub Chief Revenue Officer</cite></blockquote><p>That said, we are thrilled to announce the winners of our<strong> 2025 GitHub Partner Awards</strong>‚Äîa celebration of the outstanding contributions, innovation, and collaboration from our valued partners around the globe. Each year, we recognize partners who have gone above and beyond in delivering exceptional value, driving transformative impact, and strengthening our shared mission.&nbsp;</p><p>These partners, recognized in specific categories, are an integral part of the GitHub partner landscape, enabling our joint customers to unlock innovation, fortify security, and build unique solutions and services that integrate GitHub‚Äôs secret sauce to meet our joint customers where they are.</p><p>This year‚Äôs honorees exemplify excellence, leadership, and the spirit of true partnership. Now, let‚Äôs roll out the red carpet for our 2025 GitHub Partner Award winners!</p><h2>üèÜ 2025 Partner Award winners üèÜ</h2><ul><li>Strategic Services and Channel Partner of the Year: <a href=\"https://xebia.com/\"></a></li><li>Growth Services and Channel Partner of the Year: <a href=\"https://ecanarys.com/\"></a></li></ul><ul><li>AMER Services and Channel Partner of the Year:<a href=\"https://www.slalom.com/us/en\"></a></li><li>APAC Services and Channel Partner of the Year: <a href=\"https://www.palo-it.com/en/\"></a></li><li>EMEA Services and Channel Partner of the Year: <a href=\"https://www.capgemini.com/us-en/\"></a></li><li>Emerging Market Services and Channel Partner of the Year:<a href=\"https://www.ilegra.com/en\"></a></li></ul><ul><li>Platform Services and Channel Partner of the Year:<a href=\"https://www.infosys.com/\"></a></li><li>Security Services and Channel Partner of the Year:<a href=\"https://www.eficode.com/?utm_term=eficode&amp;utm_campaign=Brand_All_US_EN_Search_Google&amp;utm_source=adwords&amp;utm_medium=ppc&amp;hsa_acc=7859814498&amp;hsa_cam=21107856399&amp;hsa_grp=159312572639&amp;hsa_ad=759445672298&amp;hsa_src=g&amp;hsa_tgt=kwd-626362631482&amp;hsa_kw=eficode&amp;hsa_mt=e&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gad_source=1&amp;gad_campaignid=21107856399&amp;gbraid=0AAAAAD-3ocQTFOFKq6GAGW52xgifxeEQx&amp;gclid=CjwKCAjwiY_GBhBEEiwAFaghvrayVGVYh8R-B7buWtBJ0YHSQ4RBqBLY4VQzw-peYrWRCJHw89yKrRoCOe0QAvD_BwE\"></a></li><li>AI Services and Channel Partner of the Year: <a href=\"https://www.cognizant.com/us/en\"></a></li></ul><ul><li>Technology Partner of the Year Award:<a href=\"https://jfrog.com/\"></a></li></ul><blockquote><p>We‚Äôre incredibly grateful for the dedication our partners bring to the table every day. Together, we‚Äôre driving a joint mission that‚Äôs fundamentally transforming how software is built‚Äîraising the bar for the entire industry and reshaping the way people live and work. Your commitment to collaboration and customer success continues to power this momentum and inspire what‚Äôs possible.</p><cite>Matt Finkelstein, VP, Global Microsoft, Partner &amp; Services Solution Sales, GitHub</cite></blockquote><p>We extend our heartfelt congratulations to all our winners. Your dedication and partnership continue to inspire us and move the industry forward. As we look to the future, we remain committed to growing together, innovating fearlessly, and creating shared success.&nbsp;</p><p>Thank you for being part of our journey.</p>","contentLength":2856,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Project goals for 2025H2","url":"https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/","date":1761609600,"author":"Niko Matsakis","guid":319342,"unread":true,"content":"<p>On Sep 9, we merged <a href=\"https://github.com/rust-lang/rfcs/pull/3849\">RFC 3849</a>, declaring our goals for the \"second half\" of 2025H2 -- well, the last 3 months, at least, since \"yours truly\" ran a bit behind getting the goals program organized.</p><p>In prior goals programs, we had a few major flagship goals, but since many of these goals were multi-year programs, it was hard to see what progress had been made. This time we decided to organize things a bit differently. We established four flagship , each of which covers a number of more specific goals. These themes cover the goals we expect to be the most impactful and constitute our major focus as a Project for the remainder of the year. The four themes identified in the RFC are as follows:</p><ul><li>, making it possible to create user-defined smart pointers that are as ergonomic as Rust's built-in references .</li><li><strong>Unblocking dormant traits</strong>, extending the core capabilities of Rust's trait system to unblock long-desired features for language interop, lending iteration, and more.</li><li><strong>Flexible, fast(er) compilation</strong>, making it faster to build Rust programs and improving support for specialized build scenarios like embedded usage and sanitizers.</li><li>, making higher-level usage patterns in Rust easier.</li></ul><p>One of Rust's core value propositions is that it's a \"library-based language\"‚Äîlibraries can build abstractions that feel built-in to the language even when they're not. Smart pointer types like  and  are prime examples, implemented purely in the standard library yet feeling like native language features. However, Rust's built-in reference types ( and ) have special capabilities that user-defined smart pointers cannot replicate. This creates a \"second-class citizen\" problem where custom pointer types can't provide the same ergonomic experience as built-in references.</p><p>The \"Beyond the \" initiative aims to share the special capabilities of , allowing library authors to create smart pointers that are truly indistinguishable from built-in references in terms of syntax and ergonomics. This will enable more ergonomic smart pointers for use in cross-language interop (e.g., references to objects in other languages like C++ or Python) and for low-level projects like Rust for Linux that use smart pointers to express particular data structures.</p><h3><a href=\"https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/#unblocking-dormant-traits\" aria-hidden=\"true\"></a>\n\"Unblocking dormant traits\"</h3><p>Rust's trait system is one of its most powerful features, but it has a number of longstanding limitations that are preventing us from adopting new patterns. The goals in this category unblock a number of new capabilities:</p><ul><li><a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./polonius.html\">Polonius</a> will enable new borrowing patterns, and in particular <a href=\"https://github.com/rust-lang/rust/issues/92985\">unblock \"lending iterators\"</a>. Over the last few goal periods, we have identified an \"alpha\" version of Polonius that addresses the most important cases while being relatively simple and optimizable. Our goal for 2025H2 is to implement this algorithm in a form that is ready for stabilization in 2026.</li><li>The <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./next-solver.html\">next-generation trait solver</a> is a refactored trait solver that unblocks better support for numerous language features (implied bounds, negative impls, the list goes on) in addition to closing a number of existing bugs and sources of unsoundness. Over the last few goal periods, the trait solver went from being an early prototype to being in production use for coherence checking. The goal for 2025H2 is to prepare it for stabilization.</li><li>The work on <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./evolving-traits.html\">evolving trait hierarchies</a> will make it possible to refactor some parts of an existing trait into a new supertrait so they can be used on their own. This unblocks a number of features where the existing trait is insufficiently general, in particular stabilizing support for custom receiver types, a prior Project goal that wound up blocked on this refactoring. This will also make it safer to provide stable traits in the standard library while preserving the ability to evolve them in the future.</li><li>The work to <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./scalable-vectors.html\">expand Rust's  hierarchy</a> will permit us to express types that are neither  nor , such as extern types (which have no size) or Arm's Scalable Vector Extension (which have a size that is known at runtime but not at compilation time). This goal builds on <a href=\"https://github.com/rust-lang/rfcs/pull/3729\">RFC #3729</a> and <a href=\"https://github.com/rust-lang/rfcs/pull/3838\">RFC #3838</a>, authored in previous Project goal periods.</li><li><a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./in-place-initialization.html\">In-place initialization</a> allows creating structs and values that are tied to a particular place in memory. While useful directly for projects doing advanced C interop, it also unblocks expanding  to support  and  methods, as compiling such methods requires the ability for the callee to return a future whose size is not known to the caller.</li></ul><p>The \"Flexible, fast(er) compilation\" initiative focuses on improving Rust's build system to better serve both specialized use cases and everyday development workflows:</p><p>People generally start using Rust for foundational use cases, where the requirements for performance or reliability make it an obvious choice. But once they get used to it, they often find themselves turning to Rust even for higher-level use cases, like scripting, web services, or even GUI applications. Rust is often \"surprisingly tolerable\" for these high-level use cases -- except for some specific pain points that, while they impact everyone using Rust, hit these use cases particularly hard. We plan two flagship goals this period in this area:</p><ul><li>We aim to stabilize <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./cargo-script.html\">cargo script</a>, a feature that allows single-file Rust programs that embed their dependencies, making it much easier to write small utilities, share code examples, and create reproducible bug reports without the overhead of full Cargo projects.</li><li>We aim to finalize the design of <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./ergonomic-rc.html\">ergonomic ref-counting</a> and to finalize the experimental impl feature so it is ready for beta testing. Ergonomic ref-counting makes it less cumbersome to work with ref-counted types like  and , particularly in closures.</li></ul><p>For the remainder of 2025 you can expect monthly blog posts covering the major progress on the Project goals.</p><p>Looking at the broader picture, we have now done three iterations of the goals program, and we want to judge how it should be run going forward. To start, Nandini Sharma from CMU has been conducting interviews with various Project members to help us see what's working with the goals program and what could be improved. We expect to spend some time discussing what we should do and to be launching the next iteration of the goals program next year. Whatever form that winds up taking, Tomas Sedovic, the <a href=\"https://blog.rust-lang.org/inside-rust/2025/06/30/program-management-update-2025-06/\">Rust program manager</a> hired by the Leadership Council, will join me in running the program.</p>","contentLength":6396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["official"]}