{"id":"7caSwCnanpF82DyESN","title":"Official News","displayTitle":"Official News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":9,"items":[{"title":"Building your first MCP server: How to extend AI tools with custom capabilities","url":"https://github.blog/ai-and-ml/github-copilot/building-your-first-mcp-server-how-to-extend-ai-tools-with-custom-capabilities/","date":1755881547,"author":"Chris Reddington","guid":236963,"unread":true,"content":"<p>Have you ever worked with AI tools and wished they had access to some additional piece of context? Or wanted them to perform actions on your behalf? Think of those scenarios where you‚Äôre working with <a href=\"https://github.com/features/copilot?utm_campaign=rdt-blog-devrel&amp;utm_source=blog&amp;utm_medium=copilot-landing-page\">GitHub Copilot</a> and you need it to check a GitHub Issue, run a Playwright test, or interact with an API. By default, these AI tools lack access to those external systems. But that‚Äôs where the <a href=\"https://github.blog/ai-and-ml/generative-ai/a-practical-guide-on-how-to-use-the-github-mcp-server/\">Model Context Protocol (MCP)</a> comes in. It‚Äôs a standardized way to extend AI tools with custom capabilities.&nbsp;</p><p>I wanted to learn more about it by building something visual and interactive. So I created a turn-based game server that lets you play Tic-Tac-Toe and Rock Paper Scissors against Copilot using MCP.</p><p>In my latest  live stream, I walked through the project, which has a few components, all written in TypeScript:</p><ul><li>A Next.JS Web App and API, intended to run locally for demo/learning purposes</li><li>A shared library for common type definitions and components, which are reused across the web app, API, and MCP server</li></ul><p><em>You can watch the full stream below </em>üëá</p><h2>Why MCP matters for developers</h2><p>Even with powerful AI agents, we continue to run into limitations:</p><ul><li>They don‚Äôt have access to the latest documentation or real-time data.</li><li>Agents can‚Äôt perform actions like creating pull requests, exploring the UI on your locally running application, or interact with your APIs.</li></ul><p>To access external context and take action, we need to extend the capabilities of these AI tools. But before MCP, there was no standard approach to integrating with third-party tools and services. You‚Äôd potentially need different plugins and different integration patterns for whatever AI tool you were using. MCP changes that by providing  to plug tools and capabilities into any tool that supports the Model Context Protocol.</p><p>MCP follows a client-server pattern that‚Äôs familiar to most developers:</p><ul><li> The AI tool you‚Äôre using, like GitHub Copilot in VS Code (you‚Äôll notice that Copilot in VS Code has good support in the <a href=\"https://modelcontextprotocol.io/clients#feature-support-matrix\">MCP feature support matrix</a>). The host initiates the connection to your MCP server via a client.</li><li>Clientslive inside the host application (for example, GitHub Copilot in VS Code), and have a 1:1 relationship with a server. When VS Code connects to a new MCP server (like GitHub, Playwright or the turn-based-game MCP server we‚Äôre talking about in this post), a new client is created to maintain the connection.</li><li>: Your custom MCP server that provides tools, resources, and prompts. In our case, we‚Äôre making an MCP server that provides capabilities for playing turn-based games!&nbsp;</li></ul><h2>Building the turn-based game MCP server</h2><p>For my learning project, I wanted something visual that would show the overall MCP interaction and could be reused when people are trying to explain it as part of a talk. So I built a web app with Tic-Tac-Toe and Rock Paper Scissors. But instead of the game having two people play locally (or online), or even a CPU in the backend, the opponent‚Äôs moves are orchestrated by an MCP server.</p><p>The architecture includes:</p><ul><li>: The game interface where I make moves</li><li><strong>API routes (part of the Next.js implementation)</strong>: Backend logic for game state management, called by the frontend and the MCP server.</li><li>: TypeScript server that handles AI game moves</li><li>: Common game logic used across components</li></ul><p>Here‚Äôs how it works in practice:</p><ol><li>We register an MCP server in VS Code so that Copilot is aware of the new capabilities and tools.</li><li>I interact with GitHub Copilot in VS Code. I can call tools explicitly, or Copilot can autodiscover them.</li><li>Copilot calls the large language model. Based on the prompt context and the available tools, it may call the MCP server.</li><li>The MCP server executes the requested tool (like making a move in the game) and returns results.</li><li>Copilot uses those results to continue the conversation.</li></ol><p>The magic step is when you register the MCP server with an MCP application host (in our example, GitHub Copilot in Visual Studio Code). Suddenly, your AI agent gains access to the capabilities that have been built into those servers.</p><h3>Setting up the MCP server in VS Code</h3><p>You can configure your MCP servers by creating a  file. You can find more details about that on the <a href=\"https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server-to-your-workspace?utm_campaign=rdt-blog-devrel&amp;utm_source=blog&amp;utm_medium=vscode-docs-mcp-setup\">Visual Studio Code docs</a>.</p><pre><code>{\n  \"servers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    },\n    \"turn-based-games\": {\n      \"command\": \"node\",\n      \"args\": [\"dist/index.js\"],\n      \"cwd\": \"./mcp-server\"\n    }\n  }\n}</code></pre><p>The above configuration tells GitHub Copilot in VS Code that there are two MCP servers that we‚Äôd like to use:</p><ul><li>A Playwright MCP server that is executed locally as an NPM package.</li><li>A turn-based-games MCP server that runs a server locally based on the compiled TypeScript code from our working directory.</li></ul><p>For this implementation, I kept my turn-based-game MCP server architecture and logic relatively simple, with all components in a single repository. This monorepo approach bundles the web app, API, and MCP server together, making it straightforward to clone and run the entire system locally without complex dependency management or cross-repository setup. But for a more robust setup, you would likely distribute that MCP server either as a package (such as npm or a docker image), and have clear publishing and versioning processes around that.&nbsp;</p><h2>The three core building blocks of MCP</h2><p>Through building this project, I familiarized myself with three fundamental MCP server concepts:</p><p><a href=\"https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions\">Tools</a> define what actions the MCP server can perform. In my game server, I specified tools like:</p><ul><li>: Get the current state of any game</li><li><code>create_rock_paper_scissors_game</code>: Start a new game of Rock Paper Scissors</li><li>: Start a new Tic-Tac-Toe game</li><li>: Make an AI choice in Rock Paper Scissors</li><li>: Make an AI move in Tic-Tac-Toe</li><li>: Polls the endpoint until the player has made their move</li></ul><p>Each tool has a clear description and input schema that tells the AI what parameters to provide:</p><pre><code>{\n  name: 'play_tic_tac_toe',\n  description: 'Make an AI move in Tic-Tac-Toe game. IMPORTANT: After calling this tool when the game is still playing, you MUST call wait_for_player_move to continue the game flow.',\n  inputSchema: {\n    type: 'object',\n    properties: {\n      gameId: {\n        type: 'string',\n        description: 'The ID of the Tic-Tac-Toe game to play',\n      },\n    },\n    required: ['gameId'],\n  },\n},</code></pre><p>GitHub Copilot and the Large Language Model (LLM) aren‚Äôt calculating the actual game moves. When Copilot calls the play_tic_tac_toe tool, the MCP server executes a handler for that specific tool that runs the CPU game logic, like random moves for Tic Tac Toe in ‚Äúeasy‚Äù difficulty or a more optimal&nbsp; algorithm for moves when using the ‚Äúhard‚Äù difficulty.</p><p>In other words, tools are reusable pieces of software that can be called by the AI, often to take some form of action (like making a move in a turn-based game!).</p><h3>2. Resources: Context your AI can access</h3><p><a href=\"https://modelcontextprotocol.io/docs/learn/server-concepts#resources-context-data\">Resources</a> provide a way for the AI to gather context, and often have a URI-based identifier. For example, I implemented custom URI schemes like:</p><ul><li>: List all Tic-Tac-Toe games</li><li><code>game://tic-tac-toe/{Game-ID}</code>: Get state for a specific game of Tic Tac Toe</li><li><code>game://rock-paper-scissors</code>: List all Rock Paper Scissors games</li><li><code>game://rock-paper-scissors/{Game-ID}</code>: Get state for a specific game of Rock Paper Scissors</li></ul><p>As the <a href=\"https://modelcontextprotocol.io/docs/learn/server-concepts#resources-context-data\">MCP resources docs</a> explain, you can choose how these resources are passed. In our turn-based-game MCP server, there is a method that translates the resource URIs into an API call to the local API server and passes on the raw response, so that it can be used as context within a tool call (like playing a game).</p><pre><code>async function readGameResource(uri) {\n  const gameSession = await callBackendAPI(gameType, gameId);\n  if (!gameSession) {\n    throw new Error(\"Game not found\");\n  }\n  return gameSession;\n}</code></pre><h3>3. Prompts: Reusable guidance for users</h3><p>The third concept is <a href=\"https://modelcontextprotocol.io/docs/learn/server-concepts#prompts-interaction-templates\">prompts</a>. You‚Äôll be very familiar with prompts and prompt crafting, as that‚Äôs the way that you interact with AI tools like GitHub Copilot. Your users could write their own prompts to use your tools, like ‚ÄúPlay a game of Tic Tac Toe‚Äù or ‚ÄúCreate a GitHub Issue for the work that we‚Äôve just scoped out.‚Äù</p><p>But you may want to ship your MCP server with predefined prompts that help users get the most out of your tools. For example, the turn-based-game MCP comes with several prompts like:</p><ul><li>Strategy guides for different difficulty levels</li><li>Game rules and optimal play instructions</li><li>Troubleshooting help for common scenarios</li></ul><p>Your users can access these prompts via slash commands in VS Code. For example, when I typed , I could access the prompt asking for advice on optimal play for a given game or difficulty level.</p><h2>Real-world applications and considerations</h2><p>While my game demo is intentionally simple to help you learn some of these initial concepts, the patterns apply to other MCP servers:</p><ul><li><a href=\"https://github.com/github/github-mcp-server?utm_campaign=rdt-blog-devrel&amp;utm_source=blog&amp;utm_medium=github-mcp-server-repo\"></a>: Get information from existing GitHub Issues or pull requests, list Dependabot alerts, or create and manage issues and pull requests, all based on the access you provide via OAuth (in the remote MCP server) or a Personal Access Token.</li><li><a href=\"https://github.com/microsoft/playwright-mcp\"></a>: Automatically navigate to specific pages in a browser, click and interact with the pages, capture screenshots, and check rendered content.</li><li>: Connect to your internal services, databases, or business logic.</li></ul><h3>Additional capabilities from the MCP specification</h3><p>Tools, resources, and prompts are some of the most commonly used capabilities of the MCP specification. Recently, a number of additional capabilities including sampling and elicitation were added to the spec. I haven‚Äôt had a chance to add those yet, but perhaps they‚Äôll feature as part of another stream!</p><h3>Authentication and security</h3><p>You may need to handle authentication and authorization for production MCP servers depending on the scenario. As an example, the GitHub MCP server supports OAuth flows for the remote MCP server and Personal Access Tokens in local and remote. This turn-based game MCP server is intended to be simple, and doesn‚Äôt include any auth requirements, but security should be a key consideration if you‚Äôre building your own MCP servers.</p><h3>Trusting third-party MCP servers</h3><p>You may not always need to create your own MCP server. For example, GitHub ships its own MCP server. Instead of creating your own version, why not make an open source contribution upstream to improve the experience for all?</p><p>Make sure to do your due diligence on MCP servers before installing them, just like you would with any other dependency as part of your project‚Äôs supply chain. Do you recognise the publisher? Are you able to review (and contribute to) the code in an open source repository?</p><p>MCP provides SDKs <a href=\"https://modelcontextprotocol.io/docs/sdk\">for multiple languages</a>, so you can build servers in whatever technology fits your stack, ranging from TypeScript to Python, Go to Rust and more. I chose TypeScript because I wanted my entire demo (frontend, backend, and MCP server) in one repository with shared code and a common language.</p><p>Here‚Äôs what you can learn from this exploration:</p><ul><li><strong>MCP standardizes AI tool extensibility</strong> across different platforms and applications (like Copilot in Visual Studio Code)</li><li> by investigating what existing MCP servers are available. Review the MCP servers: Do you recognize the publisher and can you access the code?</li><li><strong>Building your own? Start simple</strong> with focused servers that solve specific problems rather than trying to build everything at once</li><li><strong>The three building blocks</strong> (tools, resources, prompts) provide a clear framework for designing the capabilities of your MCP server</li></ul><p>MCP isn‚Äôt just about playing games with AI (though that was fun). It‚Äôs about breaking down the walls between your AI assistants and the systems they need to help you work effectively.&nbsp;</p><p>Whether you‚Äôre building internal developer tools, integrating with external APIs, or creating custom workflows, MCP provides the foundation you need to extend your AI tools in consistent, powerful ways.</p><p>Want to explore MCP further? Here are some practical starting points:</p><ul><li>Check out the <a href=\"https://github.com/github/github-mcp-server?utm_campaign=rdt-blog-devrel&amp;utm_source=blog&amp;utm_medium=github-mcp-server-repo\">GitHub MCP server</a> to use in your own workflows or learn more about a real-world MCP server implementation.</li><li>Build a simple server for your internal APIs or development tools. You can check out the <a href=\"https://github.com/github-samples/turn-based-game-mcp?utm_campaign=rdt-blog-devrel&amp;utm_source=blog&amp;utm_medium=turn-based-game-mcp\">turn-based-game-mcp</a> as an example.</li><li>Experiment with custom prompts that encode your team‚Äôs best practices.</li></ul><p>The goal of MCP is to give your AI assistants the tools they need to be truly helpful in your specific development environment. So, which tool will you be using? What will you build?</p>","contentLength":12422,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explore the best of GitHub Universe: 9 spaces built to spark creativity, connection, and joy","url":"https://github.blog/news-insights/company-news/explore-the-best-of-github-universe-9-spaces-built-to-spark-creativity-connection-and-joy/","date":1755799354,"author":"Jeimy Ruiz","guid":235824,"unread":true,"content":"<p>What does your ideal developer event look like?At <a href=\"https://github.blog/news-insights/company-news/github-universe-2025-heres-whats-in-store-at-this-years-developer-wonderland/#h-what-you-ll-experience\">GitHub Universe 2025</a>, we‚Äôre building it. Returning to Fort Mason Center in San Francisco this October, Universe will be bigger, bolder, and more interactive than ever before. In addition to 100+ expert-led sessions (our full session catalogue drops in early September), you‚Äôll find nine unique spaces designed to spark creativity, connection, and a lot of joy ‚Äî whether you‚Äôre building LEGOs, networking with peers, or exploring new career resources.</p><p>In this blog, we‚Äôre breaking down everything you can do on-site so you can start thinking about how you‚Äôll spend your time. Let‚Äôs dive in.</p><p>Go beyond the keynotes and sessions to explore GitHub‚Äôs latest tools with help from the people who know them best. If you‚Äôre curious about what‚Äôs new or want to go deep on the tools you already use, Universe is packed with opportunities to level up. Here‚Äôs where to start:</p><p>After the morning keynotes, head over to GitHub Central, where you‚Äôll find live demos, customer stories, and product journeys aligned to our three <a href=\"https://github.blog/news-insights/company-news/github-universe-2025-heres-whats-in-store-at-this-years-developer-wonderland/#h-what-you-ll-experience\">content tracks</a> ‚Äî all designed to help you build more efficiently, securely, and creatively with GitHub.</p><p>You‚Äôll explore GitHub Copilot, GitHub Actions, GitHub Advanced Security, and more. With a mix of self-guided stations, live sessions with GitHub experts, and fun surprises, GitHub Central is your hub for discovery and inspiration.</p><p>Whether you‚Äôre an enterprise architect or just getting started with automation, you‚Äôll leave with practical ideas and trusted strategies to bring back to your team.</p><p>The GitHub Expert Center is your go-to for technical deep dives, quick advice, and 1:1 conversations. From specialists in AI and GitHub Actions, to security to scaled adoption, our GitHub experts are on-site and ready to answer all your GitHub product questions.&nbsp;</p><p>Getting help is easy; you can book an appointment ahead of time or stop by in between your sessions.&nbsp;</p><h2>Fuel your growth (and your network!)</h2><p>The future of software is being shared, shaped, and supported by the people behind the code. If you‚Äôre interested in joining us at Universe to sharpen your skills, expand your network, or prep to find your next big opportunity, this is the place to start:</p><p>In the Open Source Zone, you can connect with contributors, maintainers, and community leaders from around the globe. Discover rising stars from the <a href=\"https://github.com/accelerator\">GitHub Accelerator program</a>, learn from <a href=\"https://maintainers.github.com/auth/signin\">Maintainer Community</a> champions, and explore projects that are changing the way the world builds software.&nbsp;</p><p>Looking to take the next step in your dev career? Book a 1:1 session with a career coach to get personalized advice on everything, including polishing your resume, optimizing your GitHub and LinkedIn profiles, and helping you prep for your next interview. The Career Corner is here to help you level up with confidence.</p><p>Learning isn‚Äôt one-size-fits-all. That‚Äôs why GitHub Learn brings together tutorials, certifications, and role-based learning paths to help you grow on your terms. Explore content from GitHub, Microsoft Learn, and more to help you build real, job-ready skills designed for developers at every level.</p><p>At Universe, some of the most memorable moments happen when you‚Äôre solving puzzles, swapping stories with someone you just met, or building something weird and wonderful with AI. The spaces below are yours to unwind, experiment, and rediscover the joy of making.</p><p>Take a quick break from sessions and connect over shared interests. Recess is your chance to meet fellow attendees or hang out with coworkers who share your non-dev passions, from Lego building to chatting with executives over gelato.&nbsp;</p><p>Get your hands working in the most creative way possible! The Makerspace is where code meets art, music, robotics, and beyond. This is your space to play, explore, and reimagine what code can do. No formal training required ‚Äî just bring your curiosity.</p><p>Every in-person ticket includes a hackable badge ‚Äî designed to be customized and coded. Follow tutorials, get inspired by other hackers, and create a unique piece of hardware art to commemorate your Universe experience.</p><p>Looking for GitHub Copilot gear? Want exclusive merch that speaks dev? The Shop is your destination for exclusive GitHub swag, fan-favorite collectibles, and a few surprises you‚Äôll only find in person at our global developer event.</p><h2>Don‚Äôt miss your moment at GitHub Universe</h2><p>Regardless of what you want to accomplish during your time at Fort Mason, you can fully customize your event experience at GitHub Universe.&nbsp;</p><p>Now‚Äôs the best time to grab your in-person pass and hang out with the people who love building and scaling as much as you do:</p><ul><li>Save  on your pass with our  discount, only until September 8.</li><li>Get  when you buy </li><li>Or get when you buy</li></ul><p>&nbsp;(And yes, either <strong>group discount can be combined</strong> with Early Bird pricing!)</p>","contentLength":4854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Who will maintain the future? Rethinking open source leadership for a new generation","url":"https://github.blog/open-source/maintainers/who-will-maintain-the-future-rethinking-open-source-leadership-for-a-new-generation/","date":1755705600,"author":"Abigail Cabunoc Mayes","guid":234677,"unread":true,"content":"<p>When I was a first-year student, I joined a campus club after seeing a flyer in the hallway. I didn‚Äôt know much about it ‚Äî only that my mom had been part of the same group when she was in university, and it had shaped her life. At the first meeting, I found instant community. Some of those people became lifelong friends. But they were also mostly in their third or fourth year.</p><p>The next year, most of them graduated and there was a huge leadership gap. I wasn‚Äôt ready, but I stepped up.</p><p>That experience taught me something I‚Äôll never forget: If you don‚Äôt bring in new people early, your community won‚Äôt last.</p><p>Years later, I‚Äôm seeing the same pattern in open source. We talk a lot about burnout, bus factors, and maintainers leaving ‚Äî but we don‚Äôt talk enough about how to bring in new contributors, or what it takes to help them grow into leaders.</p><h2>The graying of Open Source</h2><p>According to <a href=\"https://4008838.fs1.hubspotusercontent-na1.net/hubfs/4008838/2024-tidelift-state-of-the-open-source-maintainer-report.pdf\">Tidelift‚Äôs 2024 maintainer survey</a>, the percentage of maintainers aged 46‚Äì65 has doubled since 2021. Meanwhile, the share of contributors under 26 has dropped from 25% to just 10%.</p><p>This ‚Äúgraying‚Äù isn‚Äôt inherently a problem. But the lack of succession is. If we don‚Äôt create pathways for younger contributors, we‚Äôre setting ourselves up for burnout, knowledge loss, and long-term fragility.</p><h2>Enter Sam: A Gen Z persona</h2><p>To explore what support might look like, I introduced a persona named ‚ÄúSam‚Äù in a recent talk at Open Source Summit North America:</p><ul><li>: Urban Canada, lives mostly online</li><li>: <em>‚ÄúI want to contribute to a climate tech project that actually matters ‚Äî but I don‚Äôt know where to start. I taught myself to code on YouTube. I design and moderate online communities. But public repos feel intimidating. I want purpose, flexibility, and a place where people like me belong.‚Äù</em></li></ul><p>Sam is our Gen Z persona. They want to contribute to something that matters. They‚Äôre self-taught, community-oriented, and motivated by purpose. But they‚Äôre also navigating financial pressures, unclear pathways, and they aren‚Äôt sure how leadership in open source actually works.</p><p>How do we help Sam thrive in open source?</p><h2>The mountain of engagement</h2><p>To support contributors like Sam, here‚Äôs a framework I‚Äôve used for years in programs like Mozilla Open Leaders and GitHub‚Äôs Maintainer Programs: the Mountain of Engagement.</p><p>This model outlines a contributor‚Äôs journey in six steps:</p><ol><li>: How they first hear about the project.</li><li>: How they first engage with the project or their initial interaction.</li><li>: How they first participate or contribute.</li><li>: How their contribution or involvement can continue.</li><li>: How they may invite and onboard others or networking within the community.</li><li>: How they may take on some additional responsibility on the project, or begin to lead.</li></ol><p>At each stage, you can compare traditional best practices with what Gen Z contributors like Sam might actually need.</p><p><em>How they first hear about the project.</em></p><p> Make your project discoverable. That means publishing to a public repository, applying an open source license, and doing basic marketing: a project website, documentation, and maybe a few social posts.</p><p> Sam isn‚Äôt browsing GitHub trending pages. They‚Äôre discovering projects through TikTok, Discord, and YouTube. They want to see purpose up front, not buried in a README. And their learning starts on mobile.</p><ul><li>84% of Gen Z are on YouTube (Sprout Social Index, 2025)</li><li>86% say purpose is very or somewhat important for their job satisfaction. (Deloitte, 2025)</li></ul><p>To reach Sam, projects need to show up where they already are, with formats and values that resonate.</p><p><em>How they first engage with the project, their initial interaction.</em></p><p> A good README, clear contributing docs, and a communication channel where newcomers can ask questions.</p><p> A mobile-friendly, visual-first landing experience. A project that leads with its mission. A casual, open chat like Discord where they can lurk before jumping in.</p><ul><li>72.8% of Gen Z prefer visual learning. (TJHSS, 2025)</li></ul><p>Gen Z favors community-driven platforms like Discord over public forums. (Impero, 2022)</p><p><em>How they first participate or contribute.</em></p><p> Personal invitations, fast responses to questions, <a href=\"https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/encouraging-helpful-contributions-to-your-project-with-labels\">‚Äúgood first issues,‚Äù</a> and clear contribution docs. These reduce friction and help people get started.</p><p> Real-time feedback. Sandboxed environments to try things out. Clear spaces where it‚Äôs okay to learn, not just perform.</p><ul><li>Generation Z students tend to be keen observers; they prefer to watch others complete tasks before attempting them themselves. (TJHSS, 2025)</li></ul><h3>4. Sustained participation</h3><p><em>How their contribution or involvement can continue.</em></p><p> Recognize contributors, match tasks to interests, and show how their work connects to the project‚Äôs mission.</p><p> Recognition they can share: badges, mentions, portfolios. They care about making a difference more than climbing a hierarchy. Show impact.</p><ul><li>86% of Gen Z workers prioritize mentorship and skill development. (Deloitte, 2025)</li></ul><h3>5. Networked participation</h3><p><em>How they may invite and onboard others, networking within the community.</em></p><p> Mentorship, social events, and formal roles that build commitment and connection.</p><p> Named, shareable roles like Discord mod or community guide. Off-topic channels and casual connection. Peer-led leadership that spreads influence.</p><ul><li>70% of Gen Z join communities for belonging and voice. (Impero, 2022)</li></ul><p><em>How they may take on some additional responsibility on the project, or begin to lead.</em></p><p> Invite someone to become a maintainer. Share governance. Provide documentation on roles and responsibilities.</p><p> Shared stewardship, not top-down control. Compensation or professional growth. A clear value exchange.</p><ul><li>52% of Gen Z live paycheck to paycheck. (Deloitte, 2024)</li></ul><p>They‚Äôre more likely to contribute when there‚Äôs tangible support: mentorship, visibility, or paid time.</p><p>Open source won‚Äôt thrive without the next generation. Let‚Äôs build projects where contributors like Sam feel welcome, supported, and seen.</p><p>Here are a few concrete actions you can take this week:</p><ul><li>Turn your  into a 60-second explainer video.</li><li>Create a sandbox space for first-time contributors.</li><li>Start a Discord or off-topic channel to foster belonging.</li><li>Make your project‚Äôs mission loud and visible.</li></ul><p>Ask yourself: <em>‚ÄúWhat does a thriving project look like to Sam? What would it take for them to stay for five years, not five weeks?‚Äù</em></p><h2>Let‚Äôs build a future together</h2><p>Maintaining open source isn‚Äôt just about keeping the lights on. It‚Äôs about creating space for the next generation.</p><p>Want to go deeper? Take a look at my <a href=\"https://docs.google.com/presentation/d/1wbvrBxO4j6ssQ-rSH7su8I8XwLWAf8wpGUcIbFhMqTY/edit?slide=id.g348118b4f21_0_0#slide=id.g348118b4f21_0_0\">slides</a> (with speaker notes + references).</p><p>Let‚Äôs build an ecosystem where maintainers are supported, projects thrive, and people like Sam stay for years ‚Äî not weeks.</p>","contentLength":6639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Container-aware GOMAXPROCS","url":"https://go.dev/blog/container-aware-gomaxprocs","date":1755648000,"author":"Michael Pratt and Carlos Amedee","guid":234621,"unread":true,"content":"<p>Go 1.25 includes new container-aware  defaults, providing more sensible default behavior for many container workloads, avoiding throttling that can impact tail latency, and improving Go‚Äôs out-of-the-box production-readiness.\nIn this post, we will dive into how Go schedules goroutines, how that scheduling interacts with container-level CPU controls, and how Go can perform better with awareness of container CPU controls.</p><p>One of Go‚Äôs strengths is its built-in and easy-to-use concurrency via goroutines.\nFrom a semantic perspective, goroutines appear very similar to operating system threads, enabling us to write simple, blocking code.\nOn the other hand, goroutines are more lightweight than operating system threads, making it much cheaper to create and destroy them on the fly.</p><p>While a Go implementation could map each goroutine to a dedicated operating system thread, Go keeps goroutines lightweight with a runtime scheduler that makes threads fungible.\nAny Go-managed thread can run any goroutine, so creating a new goroutine doesn‚Äôt require creating a new thread, and waking a goroutine doesn‚Äôt necessarily require waking another thread.</p><p>That said, along with a scheduler comes scheduling questions.\nFor example, exactly how many threads should we use to run goroutines?\nIf 1,000 goroutines are runnable, should we schedule them on 1,000 different threads?</p><p>This is where <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> comes in.\nSemantically,  tells the Go runtime the ‚Äúavailable parallelism‚Äù that Go should use.\nIn more concrete terms,  is the maximum number of threads to use for running goroutines at once.</p><p>So, if  and there are 1,000 runnable goroutines, Go will use 8 threads to run 8 goroutines at a time.\nOften, goroutines run for a very short time and then block, at which point Go will switch to running another goroutine on that same thread.\nGo will also preempt goroutines that don‚Äôt block on their own, ensuring all goroutines get a chance to run.</p><p>From Go 1.5 through Go 1.24,  defaulted to the total number of CPU cores on the machine.\nNote that in this post, ‚Äúcore‚Äù more precisely means ‚Äúlogical CPU.‚Äù\nFor example, a machine with 4 physical CPUs with hyperthreading has 8 logical CPUs.</p><p>This typically makes a good default for ‚Äúavailable parallelism‚Äù because it naturally matches the available parallelism of the hardware.\nThat is, if there are 8 cores and Go runs more than 8 threads at a time, the operating system will have to multiplex these threads onto the 8 cores, much like how Go multiplexes goroutines onto threads.\nThis extra layer of scheduling is not always a problem, but it is unnecessary overhead.</p><p>Another of Go‚Äôs core strengths is the convenience of deploying applications via a container, and managing the number of cores Go uses is especially important when deploying an application within a container orchestration platform.\nContainer orchestration platforms like <a href=\"https://kubernetes.io/\" rel=\"noreferrer\" target=\"_blank\">Kubernetes</a> take a set of machine resources and schedule containers within the available resources based on requested resources.\nPacking as many containers as possible within a cluster‚Äôs resources requires the platform to be able to predict the resource usage of each scheduled container.\nWe want Go to adhere to the resource utilization constraints that the container orchestration platform sets.</p><p>Let‚Äôs explore the effects of the  setting in the context of Kubernetes, as an example.\nPlatforms like Kubernetes provide a mechanism to limit the resources consumed by a container.\nKubernetes has the concept of CPU resource limits, which signal to the underlying operating system how many core resources a specific container or set of containers will be allocated.\nSetting a CPU limit translates to the creation of a Linux <a href=\"https://docs.kernel.org/admin-guide/cgroup-v2.html#cpu\" rel=\"noreferrer\" target=\"_blank\">control group</a> CPU bandwidth limit.</p><p>Before Go 1.25, Go was unaware of CPU limits set by orchestration platforms.\nInstead, it would set  to the number of cores on the machine it was deployed to.\nIf there was a CPU limit in place, the application may try to use far more CPU than allowed by the limit.\nTo prevent an application from exceeding its limit, the Linux kernel will <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">throttle</a> the application.</p><p>Throttling is a blunt mechanism for restricting containers that would otherwise exceed their CPU limit: it completely pauses application execution for the remainder of the throttling period.\nThe throttling period is typically 100ms, so throttling can cause substantial tail latency impact compared to the softer scheduling multiplexing effects of a lower  setting.\nEven if the application never has much parallelism, tasks performed by the Go runtime‚Äîsuch as garbage collection‚Äîcan still cause CPU spikes that trigger throttling.</p><p>We want Go to provide efficient and reliable defaults when possible, so in Go 1.25, we have made  take into account its container environment by default.\nIf a Go process is running inside a container with a CPU limit,  will default to the CPU limit if it is less than the core count.</p><p>Container orchestration systems may adjust container CPU limits on the fly, so Go 1.25 will also periodically check the CPU limit and adjust  automatically if it changes.</p><p>Both of these defaults only apply if  is otherwise unspecified.\nSetting the  environment variable or calling  continues to behave as before.\nThe <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> documentation covers the details of the new behavior.</p><h2>Slightly different models</h2><p>Both  and a container CPU limit place a limit on the maximum amount of CPU the process can use, but their models are subtly different.</p><p> is a parallelism limit.\nIf  Go will never run more than 8 goroutines at a time.</p><p>By contrast, CPU limits are a throughput limit.\nThat is, they limit the total CPU time used in some period of wall time.\nThe default period is 100ms.\nSo an ‚Äú8 CPU limit‚Äù is actually a limit of 800ms of CPU time every 100ms of wall time.</p><p>This limit could be filled by running 8 threads continuously for the entire 100ms, which is equivalent to .\nOn the other hand, the limit could also be filled by running 16 threads for 50ms each, with each thread being idle or blocked for the other 50ms.</p><p>In other words, a CPU limit doesn‚Äôt limit the total number of CPUs the container can run on.\nIt only limits total CPU time.</p><p>Most applications have fairly consistent CPU usage across 100ms periods, so the new  default is a pretty good match to the CPU limit, and certainly better than the total core count!\nHowever, it is worth noting that particularly spiky workloads may see a latency increase from this change due to  preventing short-lived spikes of additional threads beyond the CPU limit average.</p><p>In addition, since CPU limits are a throughput limit, they can have a fractional component (e.g., 2.5 CPU).\nOn the other hand,  must be a positive integer.\nThus, Go must round the limit to a valid  value.\nGo always rounds up to enable use of the full CPU limit.</p><p>Go‚Äôs new  default is based on the container‚Äôs CPU limit, but container orchestration systems also provide a ‚ÄúCPU request‚Äù control.\nWhile the CPU limit specifies the maximum CPU a container may use, the CPU request specifies the minimum CPU guaranteed to be available to the container at all times.</p><p>It is common to create containers with a CPU request but no CPU limit, as this allows containers to utilize machine CPU resources beyond the CPU request that would otherwise be idle due to lack of load from other containers.\nUnfortunately, this means that Go cannot set  based on the CPU request, which would prevent utilization of additional idle resources.</p><p>Containers with a CPU request are still <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">constrained</a> when exceeding their request if the machine is busy.\nThe weight-based constraint of exceeding requests is ‚Äúsofter‚Äù than the hard period-based throttling of CPU limits, but CPU spikes from high  can still have an adverse impact on application behavior.</p><h2>Should I set a CPU limit?</h2><p>We have learned about the problems caused by having  too high, and that setting a container CPU limit allows Go to automatically set an appropriate , so an obvious next step is to wonder whether all containers should set a CPU limit.</p><p>While that may be good advice to automatically get a reasonable  defaults, there are many other factors to consider when deciding whether to set a CPU limit, such as prioritizing utilization of idle resources by avoiding limits vs prioritizing predictable latency by setting limits.</p><p>The worst behaviors from a mismatch between  and effective CPU limits occur when  is significantly higher than the effective CPU limit.\nFor example, a small container receiving 2 CPUs running on a 128 core machine.\nThese are the cases where it is most valuable to consider setting an explicit CPU limit, or, alternatively, explicitly setting .</p><p>Go 1.25 provides more sensible default behavior for many container workloads by setting  based on container CPU limits.\nDoing so avoids throttling that can impact tail latency, improves efficiency, and generally tries to ensure Go is production-ready out-of-the-box.\nYou can get the new defaults simply by setting the Go version to 1.25.0 or higher in your .</p><p>Thanks to everyone in the community that contributed to the <a href=\"https://go.dev/issue/33803\">long</a><a href=\"https://go.dev/issue/73193\">discussions</a> that made this a reality, and in particular to feedback from the maintainers of <a href=\"https://pkg.go.dev/go.uber.org/automaxprocs\" rel=\"noreferrer\" target=\"_blank\"></a> from Uber, which has long provided similar behavior to its users.</p>","contentLength":9210,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agents panel: Launch Copilot coding agent tasks anywhere on GitHub","url":"https://github.blog/news-insights/product-news/agents-panel-launch-copilot-coding-agent-tasks-anywhere-on-github/","date":1755633194,"author":"Tim Rogers","guid":233473,"unread":true,"content":"<p>If the past year has underscored anything, it‚Äôs that AI agents are becoming a bigger part of developers‚Äô day-to-day workflows.</p><p>We recently launched Copilot coding agent, an asynchronous, autonomous developer agent: it allows you to assign an issue to Copilot, and Copilot will work in the background and create a draft pull request for your review. Copilot coding agent works like a member of your team, and it‚Äôs received a great response from developers so far.&nbsp;</p><p>But we know that not all of your work lives in GitHub Issues.&nbsp;</p><p>Today, we‚Äôre launching a new agents experience on GitHub ‚Äî the agents panel ‚Äî allowing you to quickly delegate tasks to Copilot from any page on <a href=\"http://github.com\">github.com</a> with a simple prompt and track Copilot‚Äôs progress without breaking your flow.</p><p>And of course, Copilot coding agent is also integrated into VS Code, GitHub Mobile, and the GitHub MCP Server, so you can collaborate with Copilot wherever you‚Äôre working.</p><h2>Delegate any coding task to Copilot coding agent, wherever you are, with the agents panel</h2><figure><table><tbody><tr><td><em>Need a quick refresher on GitHub Copilot coding agent? <a href=\"https://github.blog/news-insights/product-news/agents-panel-launch-copilot-coding-agent-tasks-anywhere-on-github/#jump1\">Skip ahead &gt;</a></em></td></tr></tbody></table></figure><p>The agents panel, available today on every page on github.com, is your mission control center for agentic workflows on GitHub.&nbsp;</p><p>It‚Äôs a  that allows you to hand new tasks to Copilot and track existing tasks without navigating away from your current work. Just click the new Agents button in the navigation bar to get started.</p><p>From the agents panel, you can:</p><ul><li>üõ†Ô∏è  without switching pages.</li><li>üëÄ  with real-time status.</li><li>üîó  when you‚Äôre ready to review.</li></ul><h3>Launch new tasks without breaking your flow</h3><p>You can start a new Copilot task from the new agents panel with a simple prompt. Just open the panel from any page on GitHub, describe your goal in natural language, and select the relevant repository. Copilot will then take it from there and start creating a plan, drafting changes, running tests, and then preparing a pull request.</p><p>Not sure where to start? Try the following sample prompts:</p><ul><li>:\n<ul><li>‚ÄúAdd integration tests for LoginController‚Äù</li><li>‚ÄúRefactor WidgetGenerator for better code reuse‚Äù</li><li>‚ÄúAdd a dark mode/light mode switcher‚Äù</li></ul></li><li><strong>Refer to a GitHub issue or pull request as context, optionally providing extra context</strong>:\n<ul><li>‚ÄúFix #877 using pull request #855 as an example‚Äù</li><li>‚ÄúFix #1050, and make sure you update the screenshots in the README‚Äù</li></ul></li><li><strong>Run multiple tasks in parallel:</strong><ul><li>‚ÄúAdd unit test coverage for utils.go‚Äù + ‚ÄúAdd unit test coverage for helpers.go‚Äù</li></ul></li></ul><h2>Copilot coding agent: a quick reintroduction</h2><p>Copilot coding agent lets you hand off coding tasks ‚Äî via GitHub.com, GitHub Mobile, VS Code, or any MCP-enabled tool ‚Äî and get back a draft pull request when it‚Äôs done. It runs in the cloud, can work in parallel on multiple tasks, and continues even if your computer is off.</p><p>Its secure, GitHub Actions-powered environment can run builds, tests, and linters without asking for every step. You stay in control with detailed logs and pull request-based approvals, and can give feedback by mentioning  in a review.</p><h2>More ways to hand off work to Copilot</h2><p>You can also start Copilot coding agent tasks from:</p><p>Copilot coding agent and the new agents panel on GitHub is available today in public preview for all paid Copilot subscribers. Your administrator may need to <a href=\"https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/administer-copilot/manage-for-enterprise/manage-enterprise-policies\">enable the Copilot coding agent policy</a>.</p>","contentLength":3306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tuning Linux Swap for Kubernetes: A Deep Dive","url":"https://kubernetes.io/blog/2025/08/19/tuning-linux-swap-for-kubernetes-a-deep-dive/","date":1755628200,"author":"","guid":233464,"unread":true,"content":"<p>The Kubernetes <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/swap-memory-management/\">NodeSwap feature</a>, likely to graduate to  in the upcoming Kubernetes v1.34 release,\nallows swap usage:\na significant shift from the conventional practice of disabling swap for performance predictability.\nThis article focuses exclusively on tuning swap on Linux nodes, where this feature is available. By allowing Linux nodes to use secondary storage for additional virtual memory when physical RAM is exhausted, node swap support aims to improve resource utilization and reduce out-of-memory (OOM) kills.</p><p>However, enabling swap is not a \"turn-key\" solution. The performance and stability of your nodes under memory pressure are critically dependent on a set of Linux kernel parameters. Misconfiguration can lead to performance degradation and interfere with Kubelet's eviction logic.</p><p>In this blogpost, I'll dive into critical Linux kernel parameters that govern swap behavior. I will explore how these parameters influence Kubernetes workload performance, swap utilization, and crucial eviction mechanisms.\nI will present various test results showcasing the impact of different configurations, and share my findings on achieving optimal settings for stable and high-performing Kubernetes clusters.</p><h2>Introduction to Linux swap</h2><p>At a high level, the Linux kernel manages memory through pages, typically 4KiB in size. When physical memory becomes constrained, the kernel's page replacement algorithm decides which pages to move to swap space. While the exact logic is a sophisticated optimization, this decision-making process is influenced by certain key factors:</p><ol><li>Page access patterns (how recently pages are accessed)</li><li>Page dirtyness (whether pages have been modified)</li><li>Memory pressure (how urgently the system needs free memory)</li></ol><h3>Anonymous vs File-backed memory</h3><p>It is important to understand that not all memory pages are the same. The kernel distinguishes between anonymous and file-backed memory.</p><p>: This is memory that is not backed by a specific file on the disk, such as a program's heap and stack. From the application's perspective this is private memory, and when the kernel needs to reclaim these pages, it must write them to a dedicated swap device.</p><p>: This memory is backed by a file on a filesystem. This includes a program's executable code, shared libraries, and filesystem caches. When the kernel needs to reclaim these pages, it can simply discard them if they have not been modified (\"clean\"). If a page has been modified (\"dirty\"), the kernel must first write the changes back to the file before it can be discarded.</p><p>While a system without swap can still reclaim clean file-backed pages memory under pressure by dropping them, it has no way to offload anonymous memory. Enabling swap provides this capability, allowing the kernel to move less-frequently accessed memory pages to disk to conserve memory to avoid system OOM kills.</p><h3>Key kernel parameters for swap tuning</h3><p>To effectively tune swap behavior, Linux provides several kernel parameters that can be managed via .</p><ul><li>: This is the most well-known parameter. It is a value from 0 to 200 (100 in older kernels) that controls the kernel's preference for swapping anonymous memory pages versus reclaiming file-backed memory pages (page cache).\n<ul><li>: The kernel will be aggressive in swapping out less-used anonymous memory to make room for file-cache.</li><li>: The kernel will strongly prefer dropping file cache pages over swapping anonymous memory.</li></ul></li><li>: This parameter tells the kernel to keep a minimum amount of memory free as a buffer. When the amount of free memory drops below the this safety buffer, the kernel starts more aggressively reclaiming pages (swapping, and eventually handling OOM kills).\n<ul><li> It acts as a safety lever to ensure the kernel has enough memory for critical allocation requests that cannot be deferred.</li><li>: Setting a higher  effectively raises the floor for for free memory, causing the kernel to initiate swap earlier under memory pressure.</li></ul></li><li><code>vm.watermark_scale_factor</code>: This setting controls the gap between different watermarks: ,  and , which are calculated based on .\n<ul><li>:\n<ul><li>: When free memory is below this mark, the  kernel process wakes up to reclaim pages in the background. This is when a swapping cycle begins.</li><li>: When free memory hits this minimum level, then aggressive page reclamation will block process allocation. Failing to reclaim pages will cause OOM kills.</li><li>: Memory reclamation stops once the free memory reaches this level.</li></ul></li><li>: A higher  careates a larger buffer between the  and  watermarks. This gives  more time to reclaim memory gradually before the system hits a critical state.</li></ul></li></ul><p>In a typical server workload, you might have a long-running process with some memory that becomes 'cold'. A higher  value can free up RAM by swapping out the cold memory, for other active processes that can benefit from keeping their file-cache.</p><p>Tuning the  and  parameters to move the swapping window early will give more room for  to offload memory to disk and prevent OOM kills during sudden memory spikes.</p><p>To understand the real-impact of these parameters, I designed a series of stress tests.</p><ul><li>: GKE on Google Cloud</li><li>: 1.33.2</li><li>:  (8GiB RAM, 50GB swap on a  disk, without encryption), Ubuntu 22.04</li><li>: A custom Go application designed to allocate memory at a configurable rate, generate file-cache pressure, and simulate different memory access patterns (random vs sequential).</li><li>: A sidecar container capturing system metrics every second.</li><li>: Critical system components (kubelet, container runtime, sshd) were prevented from swapping by setting  in their respective cgroups.</li></ul><p>I ran a stress-test pod on nodes with different swappiness settings (0, 60, and 90) and varied the  and  parameters to observe the outcomes under heavy memory allocation and I/O pressure.</p><h4>Visualizing swap in action</h4><p>The graph below, from a 100MBps stress test, shows swap in action. As free memory (in the \"Memory Usage\" plot) decreases, swap usage () and swap-out activity () increase. Critically, as the system relies more on swap, the I/O activity and corresponding wait time ( in the \"CPU Usage\" plot) also rises, indicating CPU stress.</p><p>My initial tests with default kernel parameters (, , <code>watermark_scale_factor=10</code>) quickly led to OOM kills and even unexpected node restarts under high memory pressure. With selecting appropriate kernel parameters a good balance in node stability and performance can be achieved.</p><p>The swappiness parameter directly influences the kernel's choice between reclaiming anonymous memory (swapping) and dropping page cache. To observe this, I ran a test where one pod generated and held file-cache pressure, followed by a second pod allocating anonymous memory at 100MB/s, to observe the kernel preference on reclaim:</p><p>My findings reveal a clear trade-off:</p><ul><li>: The kernel proactively swapped out the inactive anonymous memory to keep the file cache. This resulted in high and sustained swap usage and significant I/O activity (\"Blocks Out\"), which in turn caused spikes in I/O wait on the CPU.</li><li>: The kernel favored dropping file-cache pages delaying swap consumption. However, it's critical to understand that this <strong>does not disable swapping</strong>. When memory pressure was high, the kernel still swapped anonymous memory to disk.</li></ul><p>The choice is workload-dependent. For workloads sensitive to I/O latency, a lower swappiness is preferable. For workloads that rely on a large and frequently accessed file cache, a higher swappiness may be beneficial, provided the underlying disk is fast enough to handle the load.</p><h4>Tuning watermarks to prevent eviction and OOM kills</h4><p>The most critical challenge I encountered was the interaction between rapid memory allocation and Kubelet's eviction mechanism. When my test pod, which was deliberately configured to overcommit memory, allocated it at a high rate (e.g., 300-500 MBps), the system quickly ran out of free memory.</p><p>With default watermarks, the buffer for reclamation was too small. Before  could free up enough memory by swapping, the node would hit a critical state, leading to two potential outcomes:</p><ol><li> If kubelet's eviction manager detected  was below its threshold, it would evict the pod.</li><li> In some high-rate scenarios, the OOM Killer would activate before eviction could complete, sometimes killing higher priority pods that were not the source of the pressure.</li></ol><p>To mitigate this I tuned the watermarks:</p><ol><li>Increased  to 512MiB: This forces the kernel to start reclaiming memory much earlier, providing a larger safety buffer.</li><li>Increased  to 2000: This widened the gap between the  and  watermarks (from ‚âà337MB to ‚âà591MB in my test node's ), effectively increasing the swapping window.</li></ol><p>This combination gave  a larger operational zone and more time to swap pages to disk during memory spikes, successfully preventing both premature evictions and OOM kills in my test runs.</p><p>Table compares watermark levels from  (Non-NUMA node):</p><table><thead><tr><th> and <code>watermark_scale_factor=10</code></th><th><code>min_free_kbytes=524288KiB</code> and <code>watermark_scale_factor=2000</code></th></tr></thead><tbody><tr><td>Node 0, zone Normal  &nbsp; pages free 583273  &nbsp; min 10504  &nbsp; high 15756  &nbsp; present 1310720 </td><td>Node 0, zone Normal  &nbsp; pages free 470539  &nbsp; low 337017  &nbsp; spanned 1310720 &nbsp; managed 1274542</td></tr></tbody></table><p>The graph below reveals that the kernel buffer size and scaling factor play a crucial role in determining how the system responds to memory load. With the right combination of these parameters, the system can effectively use swap space to avoid eviction and maintain stability.</p><p>Enabling swap in Kubernetes is a powerful tool, but it comes with risks that must be managed through careful tuning.</p><ul><li><p><strong>Risk of performance degradation</strong> Swapping is orders of magnitude slower than accessing RAM. If an application's active working set is swapped out, its performance will suffer dramatically due to high I/O wait times (thrashing). Swap could preferably be provisioned with a SSD backed storage to improve performance.</p></li><li><p><strong>Risk of masking memory leaks</strong> Swap can hide memory leaks in applications, which might otherwise lead to a quick OOM kill. With swap, a leaky application might slowly degrade node performance over time, making the root cause harder to diagnose.</p></li><li><p><strong>Risk of disabling evictions</strong> Kubelet proactively monitors the node for memory-pressure and terminates pods to reclaim the resources. Improper tuning can lead to OOM kills before kubelet has a chance to evict pods gracefully. A properly configured  is essential to ensure kubelet's eviction mechanism remains effective.</p></li></ul><p>Together, the kernel watermarks and kubelet eviction threshold create a series of memory pressure zones on a node. The eviction-threshold parameters need to be adjusted to configure Kubernetes managed evictions occur before the OOM kills.</p><p>As the diagram shows, an ideal configuration will be to create a large enough 'swapping zone' (between  and  watermarks) so that the kernel can handle memory pressure by swapping before available memory drops into the Eviction/Direct Reclaim zone.</p><p>Based on these findings, I recommend the following as a starting point for Linux nodes with swap enabled. You should benchmark this with your own workloads.</p><ul><li>: Linux default is a good starting point for general-purpose workloads. However, the ideal value is workload-dependent, and swap-sensitive applications may need more careful tuning.</li><li><code>vm.min_free_kbytes=500000</code> (500MB): Set this to a reasonably high value (e.g., 2-3% of total node memory) to give the node a reasonable safety buffer.</li><li><code>vm.watermark_scale_factor=2000</code>: Create a larger window for  to work with, preventing OOM kills during sudden memory allocation spikes.</li></ul><p>I encourage running benchmark tests with your own workloads in test-environments, when setting up swap for the first time in your Kubernetes cluster. Swap performance can be sensitive to different environment differences such as CPU load, disk type (SSD vs HDD) and I/O patterns.</p>","contentLength":11722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Demoting x86_64-apple-darwin to Tier 2 with host tools","url":"https://blog.rust-lang.org/2025/08/19/demoting-x86-64-apple-darwin-to-tier-2-with-host-tools/","date":1755561600,"author":"Jake Goulding","guid":233358,"unread":true,"content":"<p>In Rust 1.90.0, the target  will be demoted to Tier 2 with host tools.\nThe standard library and the compiler will continue to be built and distributed,\nbut automated tests of these components are no longer guaranteed to be run.</p><p>Rust has supported macOS for a long time,\nwith some amount of support dating back to Rust 0.1 and likely before that.\nDuring that time period,\nApple has changed CPU architectures from x86 to x86_64 and now to Apple silicon,\nultimately announcing the <a href=\"https://en.wikipedia.org/wiki/Mac_transition_to_Apple_silicon#Timeline\">end of support</a> for the x86_64 architecture.</p><p>Similarly,\n<a href=\"https://github.blog/changelog/2025-07-11-upcoming-changes-to-macos-hosted-runners-macos-latest-migration-and-xcode-support-policy-updates/#macos-13-is-closing-down\">GitHub has announced</a> that they will no longer provide free macOS x86_64 runners for public repositories.\nThe Rust Project uses these runners to execute automated tests for the  target.\nSince the <a href=\"https://doc.rust-lang.org/stable/rustc/target-tier-policy.html\">target tier policy</a> requires that Tier 1 platforms must run tests in CI,\nthe  target must be demoted to Tier 2.</p><p>Starting with Rust 1.90.0,  will be Tier 2 with host tools.\nFor users,\nnothing will change immediately;\nbuilds of both the standard library and the compiler will still be distributed by the Rust Project for use via  or alternative installation methods.</p><p>Over time,\nthis target will likely accumulate bugs faster due to reduced testing.</p><p>If the  target causes concrete problems,\nit may be demoted further.\nNo plans for further demotion have been made yet.</p><p>For more details on the motivation of the demotion, see <a href=\"https://rust-lang.github.io/rfcs/3841-demote-x86_64-apple-darwin.html\">RFC 3841</a>.</p>","contentLength":1351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Launching MDN's new front end","url":"https://developer.mozilla.org/en-US/blog/launching-new-front-end/","date":1755561600,"author":"mdn-team","guid":233431,"unread":true,"content":"<article>MDN is getting a facelift üéâ. Discover what's changed, what's improved, and how navigating the site just got smoother.\n</article>","contentLength":121,"flags":null,"enclosureUrl":"https://developer.mozilla.org/en-US/blog/launching-new-front-end/featured.png","enclosureMime":"","commentsUrl":null},{"title":"Highlights from Git 2.51","url":"https://github.blog/open-source/git/highlights-from-git-2-51/","date":1755536676,"author":"Taylor Blau","guid":231680,"unread":true,"content":"<p>To celebrate this most recent release, here is GitHub‚Äôs look at some of the most interesting features and changes introduced since last time.</p><h2>Cruft-free multi-pack indexes</h2><p>Git stores repository contents as ‚Äú<a href=\"https://git-scm.com/book/en/v2/Git-Internals-Git-Objects\">objects</a>‚Äù (blobs, trees, commits), either individually (‚Äúloose‚Äù objects, e.g. <code>$GIT_DIR/objects/08/10d6a05...</code>) or grouped into ‚Äú<a href=\"https://git-scm.com/book/en/v2/Git-Internals-Packfiles\">packfiles</a>‚Äù (). Each pack has an <a href=\"https://git-scm.com/docs/gitformat-pack/2.44.0#_version_2_pack_idx_files_support_packs_larger_than_4_gib_and\">index</a> () that maps object hashes to offsets. With many packs, lookups slow down to , (where  is the number of packs in your repository, and  is the number of objects within a given pack).</p><p>A MIDX works like a pack index but covers the objects across multiple individual packfiles, reducing the lookup cost to , where  is the total number of objects in your repository. We <a href=\"https://github.blog/engineering/scaling-monorepo-maintenance/\">use MIDXs at GitHub</a> to store the contents of your repository after splitting it into multiple packs. We also use MIDXs to store a collection of reachability bitmaps for some selection of commits to quickly determine which object(s) are reachable from a given commit.</p><p>However, we store unreachable objects separately in what is known as a&nbsp;‚Äúcruft pack‚Äù. Cruft packs were meant to exclude unreachable objects from the MIDX, but we realized pretty quickly that doing so was impossible. The exact reasons are spelled out in <a href=\"https://github.com/git/git/commit/ddee3703b36e96056f11ddc4621707b6054bab48\">this commit</a>, but the gist is as follows: if a once-unreachable object (stored in a cruft pack) later becomes reachable from some bitmapped commit, but the only copy of that object is stored in a cruft pack outside of the MIDX, then that object has no bit position, making it impossible to write a reachability bitmap.</p><p>Git 2.51 introduces a change to how the non-cruft portion of your repository is packed. When generating a new pack, Git <a href=\"https://git-scm.com/docs/git-pack-objects/2.49.0#Documentation/git-pack-objects.txt---stdin-packs\">used to exclude any object</a> which appeared in at least one pack that would not be deleted during a repack operation, including cruft packs. In 2.51, Git now <a href=\"https://git-scm.com/docs/git-pack-objects/2.51.0#Documentation/git-pack-objects.txt---stdin-packsmode\">will store additional copies</a> of objects (and their ancestors) whose only other copy is within a cruft pack. Carrying this process out repeatedly guarantees that the set of non-cruft packs does not have any object which reaches some other object not stored within that set of packs. (In other words, the set of non-cruft packs is closed under reachability.)</p><p>As a result, Git 2.51 has <a href=\"https://git-scm.com/docs/git-pack-objects/2.51.0#Documentation/git-pack-objects.txt---stdin-packsltmodegt\">a new <code>repack.MIDXMustContainCruft</code> configuration</a> which uses the new repacking behavior described above to store cruft packs outside of the MIDX. Using this at GitHub has allowed us to write significantly smaller MIDXs, in a fraction of the time, and resulting in faster repository read performance overall. (In our primary monorepo, MIDXs shrunk by about 38%, we wrote them 35% faster, and improved read performance by around 5%.)</p><p>Give cruft-less MIDXs a try today using the new <code>repack.MIDXMustContainCruft</code> configuration option.</p><h2>Smaller packs with path walk</h2><p>In Git 2.49, we talked about <a href=\"https://github.blog/open-source/git/highlights-from-git-2-49/#faster-packing-with-name-hash-v2\">Git‚Äôs new ‚Äúname-hash v2‚Äù feature</a>, which changed the way that Git selects pairs of objects to delta-compress against one another. The full details are covered in that post, but here‚Äôs a quick gist. When preparing a packfile, Git computes a <a href=\"https://en.wikipedia.org/wiki/Hash_function\">hash</a> of all objects based on their filepath. Those hashes are then used to sort the list of objects to be packed, and Git uses a sliding window to search between pairs of objects to identify good delta/base candidates.</p><p>Prior to 2.49, Git used <a href=\"https://github.com/git/git/blob/v2.48.0/pack-objects.h#L191-L209\">a single hash function</a> based on the object‚Äôs filepath, with a heavy bias towards the last 16 characters of the path. That hash function, dating back <a href=\"https://github.com/git/git/commit/ce0bd64299ae148ef61a63edcac635de41254cb5\">all the way to 2006</a>, works well in many circumstances, but can fall short when, say, unrelated blobs appear in paths whose final 16 characters are similar. Git 2.49 introduced a <a href=\"https://github.com/git/git/blob/v2.49.0/pack-objects.h#L211-L237\">new hash function</a> which takes more of the directory structure into account, resulting in <a href=\"https://github.com/git/git/commit/30696be71f64ca3764b1d334927da927d6d8df78\">significantly smaller packs</a> in some circumstances.</p><p>Git 2.51 takes the spirit of that change and goes a step further by introducing a new way to collect objects when repacking, called ‚Äúpath walk‚Äù. Instead of walking objects in <a href=\"https://git-scm.com/docs/MyFirstObjectWalk#_changing_the_order\">revision order</a> with Git emitting objects with their corresponding path names along the way, the path walk approach emits all objects from a given path at the same time. This approach avoids the name-hash heuristic altogether and can look for deltas within groups of objects that are known to be at the same path.</p><p>As a result, Git can generate packs using the path walk approach that are often significantly smaller than even those generated with the new name hash function described above. Its timings are competitive even with generating packs using the existing revision order traversal.</p><p>Try it out today by repacking with the new  command-line option.</p><p>If you‚Äôve ever needed to switch to another branch, but wanted to save any uncommitted changes, you have likely used . The <a href=\"https://git-scm.com/docs/git-stash\">stash</a> command stores the state of your working copy and index, and then restores your local copy to match whatever was in  at the time you stashed.</p><p>If you‚Äôve ever wondered how Git actually stores a stash entry, then this section is for you. Whenever you push something onto your stash, Git creates three commits behind the scenes. There are two commits generated which capture the staged and unstaged changes. The staged changes represent whatever was in your index at the time of stashing, and the working directory changes represent everything you changed in your local copy but didn‚Äôt add to the index. Finally, Git creates a third commit listing the other two as its parents, capturing the entire snapshot.</p><p>Those internally generated commits are stored in the special  ref, and multiple stash entries are managed with the <a href=\"https://git-scm.com/docs/git-reflog\">reflog</a>. They can be accessed with , and so on. Since there is only one stash entry in  at a time, it‚Äôs extremely cumbersome to migrate stash entries from one machine to another.</p><p>Git 2.51 introduces a variant of the internal stash representation that allows multiple stash entries to be represented as a sequence of commits. Instead of using the first two parents to store changes from the index and working copy, this new representation adds one more parent to refer to the previous stash entry. That results in stash entries that contain four parents, and can be treated like an ordinary log of commits.</p><p>As a consequence of that, you can now export your stashes to a single reference, and then push or pull it like you would a normal branch or tag. Git 2.51 makes this easy by introducing two new sub-commands to git stash to import and export, respectively. You can now do something like:</p><pre><code>$ git stash export --to-ref refs/stashes/my-stash\n$ git push origin refs/stashes/my-stash</code></pre><p>on one machine to push the contents of your stash to origin, and then:</p><pre><code>$ git fetch origin '+refs/stashes/*:refs/stashes/*'\n$ git stash import refs/stashes/my-stash</code></pre><p>on another, preserving the contents of your stash between the two.</p><p>Now that we‚Äôve covered some of the larger changes in more detail, let‚Äôs take a quicker look at a selection of some other new features and updates in this release.</p><ul><li><p>\nIf you‚Äôve ever scripted around the object contents of your repository, you have no doubt encountered , Git‚Äôs dedicated tool to print the raw contents of a given object.</p><p> also has specialized  and  modes, which take a sequence of objects over stdin and print each object‚Äôs information (and contents, in the case of ). For example, here‚Äôs some basic information about the  file in Git‚Äôs own repository.</p><pre><code>$ echo HEAD:README.md | git.compile cat-file --batch-check\nd87bca1b8c3ebf3f32deb557ae9796ddc5b792ca blob 3662</code></pre><p>Here, Git is telling us the object ID, type, and size for the object we specified, just as we expect.  produces the same information for tree and commit objects. But what happens if we give it the path to a submodule? Prior to Git 2.51,  would just print . But Git 2.51 improves this output, making  more useful in a new variety of scripting scenarios:</p><pre><code>[ pre-2.51 git ]\n$ echo HEAD:sha1collisiondetection | git cat-file --batch-check\nHEAD:sha1collisiondetection missing\n\n[ git 2.51 ]\n$ echo HEAD:sha1collisiondetection | git cat-file --batch-check 855827c583bc30645ba427885caa40c5b81764d2 submodule</code></pre></li><li><p>Back in our coverage of 2.28, we talked about Git‚Äôs new <a href=\"https://github.blog/open-source/git/highlights-from-git-2-28/#changed-path-bloom-filters\">changed-path Bloom feature</a>. If you aren‚Äôt familiar with Bloom filters, or could use a refresher about how they‚Äôre used in Git, then read on.</p><p>A <a href=\"https://en.wikipedia.org/wiki/Bloom_filter\">Bloom filter</a> is a probabilistic data structure that behaves like a <a href=\"https://en.wikipedia.org/wiki/Set_(abstract_data_type)\">set</a>, with one difference. It can only tell you with 100% certainty whether an element is  in the set, but may have some false positives when indicating that an item is in the set.</p><p>Git uses Bloom filters in its <a href=\"https://git-scm.com/docs/commit-graph/2.43.0\">commit-graph</a> data structure to store a probabilistic set of which paths were modified by that commit relative to its first parent. That allows history traversals like <code>git log origin -- path/to/my/file</code> to quickly skip over commits which are known not to modify that path (or any of its parents). However, because Git‚Äôs full <a href=\"https://git-scm.com/docs/gitglossary/2.51.0#def_pathspec\">pathspec syntax</a> is far more expressive than that, Bloom filters can‚Äôt always optimize pathspec-scoped history traversals.</p><p>Git 2.51 addresses part of that limitation by adding support for using multiple pathspec items, like <code>git log -- path/to/a path/to/b</code>, which previously could not make use of changed-path Bloom filters. At the time of writing, there is <a href=\"https://lore.kernel.org/git/20250807051243.96884-1-yldhome2d2@gmail.com/\">ongoing discussion</a> about adding support for even more special cases.</p></li><li><p>The modern equivalents of , known as  and  have been considered experimental since <a href=\"https://github.blog/open-source/git/highlights-from-git-2-23/\">their introduction back in Git 2.23</a>. These commands delineate the many jobs that  can perform into separate, more purpose-built commands. Six years later, these commands are no longer considered experimental, making their command-line interface stable and backwards compatible across future releases.</p></li><li><p>Even if you‚Äôre a veteran Git user, it‚Äôs not unlikely to encounter a new Git command (among the 144!)&nbsp; every once in a while. One such command you might not have heard of is , which behaves like its modern alternative .</p><p>That command is now marked as deprecated with eventual plans to remove it in Git 3.0. As with other similar deprecations, you can still use this command behind the aptly-named  flag.</p></li><li><p>Speaking of Git 3.0, this release saw a few more entries added to <a href=\"https://github.com/git/git/blob/v2.51.0/Documentation/BreakingChanges.adoc\">the  list</a>. First, Git‚Äôs reftable backend (which we talked about extensively in <a href=\"https://github.blog/open-source/git/highlights-from-git-2-45/#preliminary-reftable-support\">our coverage of Git 2.45</a>) will become the new default format in repositories created with Git 3.0, when it is eventually released. Git 3.0 will also use the SHA-256 hash function as its default hash when initializing new repositories.</p><p>Though there is no official release date yet planned for Git 3.0, you can get a feel for some of the new defaults by building Git yourself with the  flag.</p></li><li><p>Last but not least, a couple of updates on Git‚Äôs internal development process. Git has historically prioritized wide platform compatibility, and, as a result, has taken a conservative approach to adopting features from newer C standards. Though Git has required a C99-compatible compiler since <a href=\"https://github.com/git/git/commit/7bc341e21b566c6685b7d993ca80459f9994be38\">near the end of 2021</a>, it has adopted features from that standard gradually, since some of the compilers Git targets only have partial support for the standard.</p><p>One example is <a href=\"https://port70.net/~nsz/c/c99/n1256.html#7.16\">the  keyword</a>, which became part of the C standard in C99. Here, the project began experimenting with the  keyword back <a href=\"https://github.com/git/git/commit/8277dbe9872205be1588ddfbf01d5439847db1d9\">in late 2023</a>. This release declares that experiment a success and now permits the use of  throughout its codebase. This release also began documenting C99 features that the project is <a href=\"https://github.com/git/git/blob/v2.51.0/Documentation/CodingGuidelines#L304-L310\">using experimentally</a> along with C99 features that the project <a href=\"https://github.com/git/git/blob/v2.51.0/Documentation/CodingGuidelines#L312-L322\">doesn‚Äôt use</a>.</p><p>Finally, this release saw an update to Git‚Äôs guidelines on submitting patches, which have historically required contributions to be non-anonymous, and submitted under a contributor‚Äôs legal name. Git now aligns more closely with <a href=\"https://github.com/torvalds/linux/commit/d4563201f33a022fc0353033d9dfeb1606a88330\">the Linux kernel‚Äôs approach</a>, to permit submitting patches with an identity other than the contributor‚Äôs legal name.</p></li></ul><p> For some bit position (corresponding to a single object in your repository,) a  means that object can be reached from that bitmap‚Äôs associated commit, and a  means it is not reachable from that commit. There are also four type-level bitmaps (for blobs, trees, commits, and annotated tags); the  of those bitmaps is the all s bitmap. For more details on multi-pack reachability bitmaps, check out our previous post on <a href=\"https://github.blog/2021-04-29-scaling-monorepo-maintenance/\"><em>Scaling monorepo maintenance</em></a>. <a href=\"https://github.blog/open-source/git/highlights-from-git-2-51/#return1\">‚§¥Ô∏è</a></p><p>&nbsp;For the curious, each layer of the directory is hashed individually, then downshifted and  ed into the overall result. This results in a hash function which is more sensitive to the whole path structure, rather than just the final 16 characters. <a href=\"https://github.blog/open-source/git/highlights-from-git-2-51/#return2\">‚§¥Ô∏è</a></p><p> Usually. Git will sometimes generate a fourth commit if you stashed untracked (new files that haven‚Äôt yet been committed) or ignored files (that match one or more patterns in a ). <a href=\"https://github.blog/open-source/git/highlights-from-git-2-51/#return3\">‚§¥Ô∏è</a></p><p> Almost to the day; Git 2.23 was released on August 16, 2019, and Git 2.51 was released on August 18, 2025. <a href=\"https://github.blog/open-source/git/highlights-from-git-2-51/#return5\">‚§¥Ô∏è</a></p><p> It‚Äôs true; <code>git --list-cmds=builtins | wc -l</code> outputs ‚Äú144‚Äù with Git 2.51. <a href=\"https://github.blog/open-source/git/highlights-from-git-2-51/#return6\">‚§¥Ô∏è</a></p><p> If you are somehow a diehard  user, please let us know by sending a message to <a href=\"https://lore.kernel.org/git\">the Git mailing list</a>. <a href=\"https://github.blog/open-source/git/highlights-from-git-2-51/#return7\">‚§¥Ô∏è</a></p>","contentLength":13016,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["official"]}