{"id":"Lx2uDWR7f1yr334","title":"Programming","displayTitle":"Programming","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":167,"items":[{"title":"How AI & Data Literacy Addresses the GenAI Critical Thinking Challenge","url":"https://www.datasciencecentral.com/how-ai-data-literacy-addresses-the-genai-critical-thinking-challenge/","date":1740315675,"author":"Bill Schmarzo","guid":9611,"unread":true,"content":"<p>Generative AI (GenAI) has revolutionized content generation, information processing, and decision-making. However, as AI tools like ChatGPT and Bard integrate into daily workflows, a crucial question emerges: How is GenAI affecting critical thinking? A recent research paper published by Microsoft, The Impact of Generative AI on Critical Thinking, explored this issue by surveying 319 knowledgeâ€¦&nbsp;<a href=\"https://www.datasciencecentral.com/how-ai-data-literacy-addresses-the-genai-critical-thinking-challenge/\" rel=\"bookmark\">Read More Â»</a></p>","contentLength":412,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quiz: How to Use sorted() and .sort() in Python","url":"https://realpython.com/quizzes/python-sort/","date":1740312000,"author":"Real Python","guid":9579,"unread":true,"content":"<p>By working through this quiz, youâ€™ll revisit how to sort various types of data in different data structures, customize the order, and work with two different ways of sorting in Python.</p>","contentLength":186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This is don't my first lerning.","url":"https://dev.to/consolvex/this-is-dont-my-first-lerning-279c","date":1740311946,"author":"Consolex","guid":9595,"unread":true,"content":"<p>I'am learning GOlang. \nMy English is bad, but i'am also learning Eng.</p>","contentLength":69,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agricultural Product Classification","url":"https://dev.to/agam_singh_2f66d3d454144d/agricultural-product-classification-1nco","date":1740310208,"author":"Agam Singh","guid":9565,"unread":true,"content":"<p><strong>Agricultural Product Classification Using Machine Learning</strong></p><p>ğŸŒ¾ Introduction\nAgriculture plays a vital role in feeding the global population, making it essential to improve productivity and efficiency in this sector. One of the key challenges in modern agriculture is the accurate classification of agricultural products, which directly impacts quality control, pricing, and supply chain management. Traditional methods of classification often rely on manual inspection, which can be time-consuming and prone to errors.</p><p>With the rise of data-driven technologies, Machine Learning (ML) offers a powerful alternative to automate and enhance agricultural product classification. By analyzing dimensional and shape factors, ML algorithms can accurately classify various agricultural products, reducing manual labor and increasing efficiency. This project explores how machine learning techniques can be applied to classify agricultural products using real-world data, leading to smarter agricultural practices and better decision-making.</p><p>ğŸ“Š Project Overview\nThis project focuses on classifying agricultural products based on dimensional and shape factors using machine learning. The goal is to develop an accurate and efficient classification system by exploring data preprocessing techniques, visualizations, and multiple machine learning algorithms.</p><p>ğŸ“ Dataset\nNote: The Data was taken from</p><p>Entries: 13,611\nColumns: 17<p>\nData Types: 14 float64, 2 int64, 1 object</p>\nThe dataset contains detailed information about agricultural plants, including area, perimeter, major axis length, and shape factors. Preprocessing steps included:</p><p>Removing 68 duplicate entries.\nEncoding the categorical â€˜Classâ€™ column.<p>\nEnsuring no missing values.</p></p><p>ğŸ“Š Data Preprocessing\nData Cleaning: Removed duplicates and encoded categorical data.<p>\nFeature Scaling: Applied standard scaling for improved model performance.</p>\nData Splitting: Divided the dataset into training and testing sets.</p><p>ğŸ“ˆ Data Visualization\nUtilized Matplotlib and Seaborn for:</p><p>Pair Plots: To visualize feature relationships.\nHistograms &amp; Distribution Plots: To understand feature distributions.<p>\nScatter Plots: To observe trends and correlations.</p>\nHeatmap: To display feature correlation.<p>\nBox Plots: To examine feature distribution across classes.</p></p><p>ğŸ¤– Machine Learning Models Used\nRandom Forest Classifier\nPrecision: 92.65%\nF1-Score: 92.12%</p><ol><li>K-Nearest Neighbors (KNN) (Best Performer)</li></ol><p>Accuracy: 91.95%\nPrecision: 92.62%\nF1-Score: 92.41%</p><ol><li>Support Vector Classifier (SVC)</li></ol><p>Accuracy: 87.93%\nPrecision: 92.62%\nF1-Score: 92.41%</p><p>âš–ï¸ Model Comparison\nKNN outperformed the other models, making it the most suitable for classifying agricultural products in this project.<p>\nRandom Forest followed closely, while SVC showed comparatively lower accuracy.</p></p><p>ğŸ¤– Model Training &amp; Evaluation</p><div><pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# Feature and target split\nX = df.drop(['Class_BO', 'Class_CA', 'Class_DE', 'Class_HO', 'Class_SE', 'Class_SI'], axis=1)\ny = df[['Class_BO', 'Class_CA', 'Class_DE', 'Class_HO', 'Class_SE', 'Class_SI']]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Random Forest\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n\n# KNN\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\nprint(\"KNN Accuracy:\", accuracy_score(y_test, y_pred_knn))\n\n# SVC\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)\ny_pred_svc = svc.predict(X_test)\nprint(\"SVC Accuracy:\", accuracy_score(y_test, y_pred_svc))\n\n</code></pre></div><p>âœ… Conclusion\nThis project demonstrates how machine learning can be applied to classify agricultural products based on shape and dimensional factors. The KNN model proved to be the most effective, showcasing strong performance across all evaluation metrics.</p><p>ğŸ› ï¸ Tools &amp; Technologies Used\nPython\nPandas &amp; NumPy: Data preprocessing<p>\nMatplotlib &amp; Seaborn: Data visualization</p>\nScikit-learn: Machine learning algorithms and evaluation</p>","contentLength":4484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PyPy","url":"https://dev.to/sanika_sreeak_e95609b33/pypy-1o67","date":1740309227,"author":"Sanika Sree A.K","guid":9564,"unread":true,"content":"<p>While Python 2.7 and older versions are officially unsupported, a different unofficial Python putting into use, PyPy, continues to support Python 2.</p><p>PyPy is faster than CPython because it uses JIT compiler.</p>","contentLength":205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Port Forwarding with Ngrok ğŸš€: Quick Guide","url":"https://dev.to/victorchiaka/port-forwarding-with-ngrok-quick-guide-j52","date":1740306197,"author":"Victor Chiaka","guid":9549,"unread":true,"content":"<p>You're building a mobile app solo. The backend is ready, and now you're working on the UI. You run your server locally and try making requests from the appâ€”nothing works.</p><p>Why? Your base URL ( or ) points to your local machine, but your mobile device has its own localhost.</p><p>Realizing this, you deploy your backend to a free online service, but then:</p><ul><li>The server shuts down after minutes of inactivity.</li><li>Network requests are painfully slow.</li></ul><p> Ngrok, is a tool that allows you expose your local server to the internet securely.</p><ul><li>Get your auth_token from the dashboard</li></ul><div><pre><code>brew ngrok/ngrok/ngrok\n</code></pre></div><div><pre><code>ngrok config add-authtoken &lt;your_auth_token&gt;\n</code></pre></div><div><pre><code>ngrok config add-authtoken &lt;your_auth_token&gt;\n</code></pre></div><div><pre><code>wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-stable-linux-amd64.deb\n</code></pre></div><div><pre><code>dpkg  ngrok-stable-linux-amd64.deb\n</code></pre></div><div><pre><code>ngrok config add-authtoken &lt;your_auth_token&gt;\n</code></pre></div><div><pre><code>ngrok http http://127.0.0.1:&lt;your-port&gt;\n</code></pre></div><p><strong>Now ngrok will do it's thing ğŸ˜</strong> and give you the following results.</p><div><pre><code>ngrok                                                        Ctrl+C to quit\n\nSession Status                online\nAccount                       your-email@example.com Plan: Free\nVersion                       3.x.x\nRegion                        United States us\nLatency                       15ms\nWeb Interface                 http://127.0.0.1:4040\nForwarding                    https://random-subdomain.ngrok-free.app -&gt; http://127.0.0.1:8000\nForwarding                    http://random-subdomain.ngrok-free.app  -&gt; http://127.0.0.1:8000\n\nConnections                   ttl     opn     rt1     rt5     p50     p90\n                              0       0       0.00    0.00    0.00    0.00\n</code></pre></div><p>Add the following to your apps build configuration.</p><div><pre><code>https://random-subdomain.ngrok-free.app\n</code></pre></div><p>Ngrok is a game-changer when it comes to local development and testing. It eliminates the hassle of exposing your local server to the internet, making it easy to test APIs, webhooks, and mobile applications without deployment delays. With just a few commands, you get a secure, public URL that seamlessly tunnels traffic to your local machine.</p><p>Now, instead of struggling with localhost limitations or slow, unreliable free hosting, you can integrate Ngrok into your workflow and focus on building your app. ğŸš€</p><p><strong>Give it a try, and happy coding! ğŸ’»ğŸ”¥</strong></p>","contentLength":2259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to be Test Driven with Spark: 2 - CI","url":"https://dev.to/nda_27/how-to-be-test-driven-with-spark-2-ci-4a28","date":1740306114,"author":"Nicoda-27","guid":9548,"unread":true,"content":"<p>This goal of this tutorial is to provide a way to easily be test driven with spark on your local setup without using cloud resources.</p><p>This is a series of tutorials and the initial chapters can be found in:</p><h2>\n  \n  \n  Chapter 2: Continuous Integration (ci)\n</h2><p>Having a ci is mandatory for any project that aims at having multiple contributors. In the following chapter, a proposal ci will be implemented.</p><p>As ci implementation is specific to a collaborative platform being , , ,  etc. The following chapter will try to provide a technology agnostic ci as much as possible.</p><p>Similar concepts are available in all ci, you will have to transpose the concepts that will be used here.</p><p>The ci here will be very minimal but showcases concepts that you implemented in <a href=\"https://github.com/Nicoda-27/spark_tdd/blob/doc/chapter_2/tutorials/chapter_1_setup.md\" rel=\"noopener noreferrer\">Chapter 1</a>, namely:</p><ul></ul><p>There are many more addition to the continuous integration that will not be tackled here. A minimal ci is required to guarantee non regressions in terms of:</p><ul><li>code styling rules to guarantee no indivual contributors diverge from the coding style</li><li>tests, namely all tests must be passing</li></ul><p> is expecting ci files to be provided at a specific location, you can therefore create a file in <a href=\"https://github.com/Nicoda-27/spark_tdd/blob/doc/chapter_2/.github/workflows/ci.yaml\" rel=\"noopener noreferrer\"><code>.github/workflows/ci.yaml</code></a>.</p><p>In this file, you can add</p><div><pre><code></code></pre></div><ul><li>The  and  define the names of the pipeline that will run.</li><li>The  defines the event that will trigger the pipeline to run,  means that for every commit the pipeline will run.</li><li>The  defines a list of jobs, the ci is made of one job with multiple steps for the sake of simplicity.</li><li>The  defined the docker image used to run (the runner) the environment against, it's a list of <a href=\"https://github.com/actions/runner-images\" rel=\"noopener noreferrer\">docker images</a> maintained by .</li></ul><p>Now into the steps section we can add:</p><div><pre><code></code></pre></div><ul><li>The  is the  action that checkout the current branch of the repository.</li><li>The  is the  action that will read the  and install everything for us.</li><li>The  step will install the dependencies and run the formatting. It there is an error, the command will fail and the pipeline too.</li><li>The  step will run the tests. It there is an error, the command will fail and the pipeline too.</li></ul><p>As it was stated, the ci is the only source of truth. If it passes on ci, it should pass on your local setup. If not, it means there are discrepancies between the ci setup and yours.</p><p>Going through the ci implementation will help you on reproducibility. Maybe you're not using the same way to install python version, or the same dependency management tool. You need to align your tools and the ones presented in <a href=\"https://github.com/Nicoda-27/spark_tdd/blob/doc/chapter_2/tutorials/chapter_1_setup.md\" rel=\"noopener noreferrer\">chapter 1</a> help not to conflict with your local setup. You might have installed python package globally or you might have manually changed  or your  and this can easily be a mess.</p><p>To help on reproducibility, a <a href=\"https://code.visualstudio.com/docs/devcontainers/containers\" rel=\"noopener noreferrer\">dev container</a> approach can be used. It means, the ci will run inside a container and this container can be reused as a developer environment. This will not be implemented for the moment.</p><p>To improve readability and segregates between code formatting and testing,  actions can be implemented as job with interdependencies. Then, the workflow becomes:</p><div><pre><code></code></pre></div><p>In here we added the  to create dependencies between ci job. It means, we will not run the tests until the code style is compliant; this will save some time and resources. Indeed, if the code is not formatted, don't even bother running the tests. The execution graph will be like:</p><p>We can see here some duplication, which is not ideal as for future code improvements, you will have to do it at two places at the same time. This is technical debt that one would have to tackle using <a href=\"https://docs.github.com/en/actions/sharing-automations/creating-actions/creating-a-composite-action\" rel=\"noopener noreferrer\">composite action</a>. We will consider it's ok for now.</p><h3>\n  \n  \n  Caching dependency resolution\n</h3><p>You will see additional steps in the <a href=\"https://github.com/Nicoda-27/spark_tdd/blob/doc/chapter_2/.github/workflows/ci.yaml\" rel=\"noopener noreferrer\"></a>, namely related to cache</p><div><pre><code></code></pre></div><p>These steps aim at caching the  when there are no changes on the  and reusing it. The intent is to speed up the ci execution as dependency resolution and installation can be time consuming.</p><p>An extra step to minimize caching size is added as  proposes such feature, namely an extra step and an environment variable is added to configure the location of the cache.</p><div><pre><code></code></pre></div><p>On the next chapter, you will implement your first spark code and implement a way to guarantee test automation of it. This is long overdue as we spent 3 chapters on setup...</p><p>You can find the original materials in <a href=\"https://github.com/Nicoda-27/spark_tdd/tree/doc/chapter_2\" rel=\"noopener noreferrer\">spark_tdd</a>. This repository exposes what's the expected repository layout at the end of each chapter in each branch:</p>","contentLength":4278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DataWars.io: 7 free Machine Learning projects to practice using Python | DataWars","url":"https://www.datawars.io/articles/7-free-machine-learning-projects-to-practice-using-python","date":1740304096,"author":"","guid":9540,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing Rust Compilation: Smaller, Faster, or Both?","url":"https://dev.to/leapcell/optimizing-rust-compilation-smaller-faster-or-both-16pe","date":1740298736,"author":"Leapcell","guid":9510,"unread":true,"content":"<p>You have completed writing a Rust project and are now working on compilation. How can you make the compiled file as small as possible? How can you make it run as fast as possible? Or how can you achieve both small size and high speed?</p><p>You may have these considerations:</p><ul><li>: Suitable for embedded development, where the project is small and not complex, and execution speed is already fast. The main goal is to reduce the file size as much as possible.</li><li><strong>Maximizing execution speed</strong>: Suitable for network services where file size is not a concern, but maximizing concurrency is the top priority.</li><li><strong>Balancing both size and speed</strong>: A middle ground that is suitable for various types of projects.</li></ul><p>You only need to add the following configuration to your  file and run:</p><h3>\n  \n  \n  Generate a Smaller Executable\n</h3><div><pre><code></code></pre></div><h3>\n  \n  \n  Generate a Faster Executable\n</h3><div><pre><code></code></pre></div><h3>\n  \n  \n  Balance Between Size and Speed\n</h3><div><pre><code></code></pre></div><h2>\n  \n  \n  Explanation of Configurations\n</h2><p>: Specifies the level of compiler optimizations.</p><ul><li>: No optimization, fastest compilation time.</li><li>: Optimize for faster compilation.</li><li>: Balance between compilation speed and runtime performance (default).</li><li>: Optimize for maximum runtime performance.</li><li>: Optimize for smaller code size.</li><li>: Further optimize for code size, more aggressively than .</li></ul><p>: Use  to generate the smallest executable; use  to generate the fastest executable.</p><p>: Enables Link Time Optimization (LTO).</p><ul><li>: Disable LTO (default).</li><li>: Enable the most aggressive LTO.</li></ul><p>: Enabling LTO reduces binary size and improves runtime performance.  is a moderate choice, while  provides the best optimization but increases compilation time.</p><p>: Controls the number of code generation units.</p><p>: Usually . Setting it to  enables the highest level of optimization.</p><p>: Reducing the number of code generation units gives the compiler more information for global optimizations, resulting in a smaller and faster executable. Setting it to  maximizes optimization but increases compilation time.</p><p>: Controls panic behavior.</p><ul><li>: Unwind the stack (default).</li><li>: Directly abort the process.</li></ul><p>: Using  reduces the executable size and improves performance in some cases since it eliminates the need for stack unwinding information.</p><p>: Controls which debug and symbol information is removed.</p><ul><li>: Keep all information (default).</li><li>: Remove debug information.</li><li>: Remove symbol tables but retain necessary debug information.</li><li>: Remove all optional information, including debug and symbol data.</li></ul><p>: Removing unnecessary debug and symbol information significantly reduces executable size.</p><p>These are the optimization techniques for compiling a Rust project. Have you mastered them?</p><p><a href=\"https://leapcell.io/?lc_t=d_rustbuildoptim\" rel=\"noopener noreferrer\">Leapcell</a> is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:</p><ul><li>Develop with Node.js, Python, Go, or Rust.</li></ul><p><strong>Deploy unlimited projects for free</strong></p><ul><li>pay only for usage â€” no requests, no charges.</li></ul><p><strong>Unbeatable Cost Efficiency</strong></p><ul><li>Pay-as-you-go with no idle charges.</li><li>Example: $25 supports 6.94M requests at a 60ms average response time.</li></ul><p><strong>Streamlined Developer Experience</strong></p><ul><li>Intuitive UI for effortless setup.</li><li>Fully automated CI/CD pipelines and GitOps integration.</li><li>Real-time metrics and logging for actionable insights.</li></ul><p><strong>Effortless Scalability and High Performance</strong></p><ul><li>Auto-scaling to handle high concurrency with ease.</li><li>Zero operational overhead â€” just focus on building.</li></ul>","contentLength":3225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Getting micrograd to Perfectly Predict Answers To A Sample Problem","url":"https://dev.to/shrsv/getting-micrograd-to-perfectly-predict-answers-to-a-sample-problem-3cib","date":1740298696,"author":"Shrijith Venkatramana","guid":9511,"unread":true,"content":"<p><em>Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, Iâ€™m building <a href=\"https://hexmos.com/liveapi\" rel=\"noopener noreferrer\">LiveAPI</a>, a tool that makes generating API docs from your code ridiculously easy.</em></p><h2>\n  \n  \n  Training: Changing  based on  slightly (accorindg to Learning Rate)\n</h2><p>From the previous post - we created   a list of all the nodes in the neural network.</p><p>In total we have  neurons:</p><p>One of the neuron's data value is shown below:</p><p>Now the goal is to change the data value of this neuron, in accordance with the gradient feedback.</p><div><pre><code>for p in n.parameters():\n    p.data += 0.01 * p.grad # something like that (wip)\n</code></pre></div><p>We also note that the gradient is negative for that neuron:</p><h3>\n  \n  \n  Determining the sign of the step factor </h3><p>So there's a bit of reasoning to do here, to determine the sign of step factor.</p><p>The goal is to minimize the loss (bring loss = 0).</p><p> is negative - .</p><p>In , the result is  is decreased a bit, increasing the loss.</p><p>But in , the result is  is increased a bit, reducing the loss.</p><p>So therefore, the correct option is to have a negative step factor.</p><div><pre><code></code></pre></div><p>Now we can see that the loss before/after weight adjustment and conclude that - through the backward pass plus gradient descent, we got a more accurate result:</p><h2>\n  \n  \n  Automating Gradient Descent To Get a Highly Accurate Network\n</h2><p>Setting the right learning rate value is a subtle art. If it is too low, it takes too long to converge. If it is too large a step size, the process gets unstable and may explode the loss. So finding the perfect rate is a subtle art.</p><h3>\n  \n  \n  Implementing a Training Loop\n</h3><p>We put a loop repeating the forward pass, backward pass and weight updates process:</p><div><pre><code></code></pre></div><p>The training gives an output like this:</p><div><pre><code>0 4.149044341397712\n1 2.8224176124705482\n2 1.0767374634555338\n3 0.4436221441110331\n4 0.048639680823661345\n5 0.0007984305003777319\n6 5.758159329954795e-06\n7 1.1072290005342024e-07\n8 1.1331571852917713e-08\n9 1.8004031247688252e-09\n10 3.886667439780539e-10\n11 1.190170455797565e-10\n12 5.491701244447392e-11\n13 4.086071696354591e-11\n14 5.2487460541263784e-11\n15 1.235857710202349e-10\n16 5.557297068527374e-10\n17 4.829530833029305e-09\n18 7.912558681799505e-08\n19 2.2910484425631455e-06\n</code></pre></div><p>You can see that the loss is getting to really small numbers near the final passes.</p><p>Now we compare actual y to predicted y:</p><div><pre><code></code></pre></div><p>And we get perfect results:</p><div><pre><code>actual [1.0, -1.0, -1.0, 1.0]\npredicted [\n    Value(data=1.0, grad=0.0, label=''), \n    Value(data=-0.9986538494836703, grad=0.0026923010326593833, label=''), \n    Value(data=-0.9993079543151291, grad=0.0013840913697418245, label=''), \n    Value(data=1.0, grad=0.0, label='')\n]\n</code></pre></div><h3>\n  \n  \n  Fixing a subtle bug in the training loop\n</h3><p>Each of the neurons in the net has weight and grad attributes.</p><p>In our training loop, the first iteration is fine - when we do  we fill in the grad values for each neuron.</p><p>But on the second iteration and next, we keep accumulating the grad values (and are never reset to 0).</p><p>So the feedback given to each neuron could be slightly wrong. We have to reset grad to 0.</p><div><pre><code></code></pre></div><p>We get a similar result as above in this case, since the problem was quite a simple one. It so happens in neural network that sometimes, we seem to get a successful result even when the logic is a bit buggy. For complex problems, these sorts of issues/bugs can derail the solution process - and one has to watch out for common mistakes.</p>","contentLength":3289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust, ROS, and dynamic typing https://open.substack.com/pub/intrepidai/p/rust-ros-and-dynamic-typing?r=7n2a9&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false","url":"https://dev.to/fgadaleta/rust-ros-and-dynamic-typing-42n9","date":1740296161,"author":"frag","guid":9489,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build Your Own AI Agents - From Scratch & with Frameworks! (YouTube Tutorial)","url":"https://dev.to/bytesinstitute/build-your-own-ai-agents-from-scratch-with-frameworks-youtube-tutorial-d94","date":1740292673,"author":"Bytes Institute","guid":9470,"unread":true,"content":"<p>Want to dive into AI Agents but don't know where to start?  I just released a new YouTube video guiding you through building AI Agents from the ground up!</p><ul><li>  Fundamentals of AI Agents</li><li>  Building from scratch (no frameworks!)</li><li>  Building with the </li><li>  Leveraging  for powerful agents</li></ul><p>Ready to build intelligent agents?  Check out the full tutorial on YouTube.</p>","contentLength":349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Practical Guide to ORM in GoFrame: From Basics to Advanced Relationships","url":"https://dev.to/jones_charles_ad50858dbc0/a-practical-guide-to-orm-in-goframe-from-basics-to-advanced-relationships-17fk","date":1740289507,"author":"Jones Charles","guid":9457,"unread":true,"content":"<p>If you're working with Go and looking for a powerful yet lightweight web framework, GoFrame might be just what you need. One of its standout features is the  package, which provides a robust ORM (Object-Relational Mapping) system. In this guide, I'll walk you through everything you need to know to effectively use ORM in your GoFrame projects.</p><ul><li>Setting up database connections</li><li>Working with transactions</li><li>Handling relationships (one-to-one, one-to-many, many-to-many)</li></ul><ul><li>Basic knowledge of Go programming</li><li>Go installed on your machine</li><li>MySQL or compatible database</li><li>Basic understanding of ORM concepts</li></ul><h2>\n  \n  \n  Getting Started: Database Configuration\n</h2><p>First things first, let's set up our database connection. GoFrame makes this super straightforward with a YAML configuration:</p><div><pre><code></code></pre></div><p>ğŸ’¡ : The  setting is fantastic during development as it shows you the actual SQL queries being executed. Remember to disable this in production!</p><h2>\n  \n  \n  Defining Your First Model\n</h2><p>Let's start with a simple user model. In GoFrame, models are just Go structs with special tags:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  CRUD Operations Made Easy\n</h2><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>More complex query with conditions:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2>\n  \n  \n  Working with Transactions\n</h2><p>Transactions are crucial for maintaining data integrity. Here's how to use them in GoFrame:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Advanced Feature: Relationships\n</h2><p>Perfect for user profiles or detailed information:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  One-to-Many Relationships\n</h3><p>Great for handling user comments or posts:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Many-to-Many Relationships\n</h3><p>Perfect for scenarios like course enrollment systems:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Best Practices and Tips ğŸ’¡\n</h2><ol><li>: Include context in your database operations for better control and cancellation capabilities.</li><li>: Don't ignore error returns from database operations.</li><li> for operations that modify multiple tables.</li><li> appropriately based on your query patterns.</li><li>: Avoid putting business logic in your model structs.</li></ol><h2>\n  \n  \n  Common Gotchas to Watch Out For âš ï¸\n</h2><ul><li>Remember to properly close database connections</li><li>Be careful with large result sets - use pagination</li><li>Watch out for N+1 query problems</li><li>Don't forget to handle null values appropriately</li></ul><p>GoFrame's ORM system provides a powerful yet intuitive way to work with databases in Go. It strikes a great balance between functionality and simplicity, making it a solid choice for both small and large projects.</p><ul><li>Explore GoFrame's caching capabilities</li><li>Look into query optimization techniques</li><li>Learn about GoFrame's migration tools</li></ul><p>Let me know in the comments if you have any questions or if you'd like to see more GoFrame content! ğŸš€</p>","contentLength":2473,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Write AI agent from scratch without LangChain and CrewAI","url":"https://dev.to/franzwong/write-ai-agent-from-scratch-without-langchain-and-crewai-2bop","date":1740287547,"author":"Franz Wong","guid":9456,"unread":true,"content":"<p>We are going to create an AI agent which can perform timezone conversion with tools / function calling. Only  library is used.</p><p>Full source code will be given at the end.</p><p>We only need to install  library.</p><p>We will define 2 tools. One is to get today's date and the other is to convert the timezone.</p><p>We also define  which stores the details about the tools. The key of the map is the function name.  is the details we will send to LLM and  is the function we will call.</p><p>If your tools perform sensitive operations, you should verify or sanitize the parameters first.</p><div><pre><code></code></pre></div><p>We apply \"ReAct Prompting\" to enable act and reasoning abilities. This also allows us to use tools.</p><p>In order to make LLM stop generating after proposing an action, we ask it to output \"PAUSE\" text after an action.</p><div><pre><code></code></pre></div><p>Before we see the actual code of AI agent, we check the pseudocode first.</p><p>We have a loop to keep sending messages to LLM and performing actions.</p><div><pre><code>messages = [system_prompt, question_prompt]\n\ncompletion = chat(llm, messages)\nadd completion to messages with role 'assistant'\n\nwhile 'Final Answer' is not in completion and maximum round is not reached:\n\n  If 'Action' is not found in completion:\n    continue\n  action = parse_action(completion)\n\n  If 'Action Input' is not in completion:\n    continue\n  action_input = parse_action_input(completion)\n\n  result = action(acion_input)\n\n  add result to messages with role 'assistant'\n\n  completion = chat(llm, messages)\n  add completion to messages with role 'assistant'\n</code></pre></div><p> function is straight forward. It only creates chat completion with the messages we have.</p><p>Stop word \"PAUSE\" is used to make LLM stop generation after proposing an action.</p><div><pre><code></code></pre></div><p>We use regular expression to extract tool function name. Tool function will be returned. We also make sure tool function is defined in  for security reason.</p><p>You can also change the system prompt to ask LLM returning JSON to ease parsing.</p><div><pre><code></code></pre></div><h2> function\n</h2><p>As same as , we use regular expression to extract parameters of tool.</p><div><pre><code></code></pre></div><p>Here is the implementaiton of the above pseudocode.</p><div><pre><code></code></pre></div><div><pre><code>Question: Convert 13:49 today in London time to Japan time. Please consider daylight saving.\nChat completion:\nThought: To accurately convert 13:49 London time to Japan time, it's important to consider the current date and whether daylight saving time is in effect. I'll first fetch the current date in the \"Europe/London\" timezone to understand if daylight saving time needs to be taken into account.\nAction: get_today_date\nAction Input: \"Europe/London\"\n\nAction result: 2025-02-23\nChat completion:\nThought: Today is February 23, 2025. Daylight saving time in London typically starts on the last Sunday in March and ends on the last Sunday in October. Therefore, currently, London is on Greenwich Mean Time (GMT, UTC+0). Japan does not observe daylight saving time and is always on Japan Standard Time (JST, UTC+9). I will now convert 13:49 London time to Japan time for today.\nAction: convert_timezone\nAction Input: \"Europe/London\", \"Asia/Tokyo\", 2025, 2, 23, 13, 49\n\nAction result: 2025-02-23 22:49:00+09:00\nChat completion:\nThought: I have successfully converted 13:49 London time to Japan time. The converted time is 22:49 on February 23, 2025.\nFinal Answer: 13:49 in London on February 23, 2025, is 22:49 in Japan time.\n</code></pre></div><div><pre><code></code></pre></div>","contentLength":3238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ElasticTransform in PyTorch (3)","url":"https://dev.to/hyperkai/elastictransform-in-pytorch-3-56pl","date":1740282945,"author":"Super Kai (Kazuya Ito)","guid":9430,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.ElasticTransform.html\" rel=\"noopener noreferrer\">ElasticTransform()</a> can do random morphological transformation for an image as shown below. *It's about  and  argument:</p><div><pre><code></code></pre></div>","contentLength":118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ElasticTransform in PyTorch (2)","url":"https://dev.to/hyperkai/elastictransform-in-pytorch-2-2p5j","date":1740282779,"author":"Super Kai (Kazuya Ito)","guid":9429,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.ElasticTransform.html\" rel=\"noopener noreferrer\">ElasticTransform()</a> can do random morphological transformation for an image as shown below. *It's about  and  argument:</p><div><pre><code></code></pre></div>","contentLength":118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ElasticTransform in PyTorch (1)","url":"https://dev.to/hyperkai/elastictransform-in-pytorch-1-m0b","date":1740282709,"author":"Super Kai (Kazuya Ito)","guid":9428,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.ElasticTransform.html\" rel=\"noopener noreferrer\">ElasticTransform()</a> can do random morphological transformation for an image as shown below. *It's about  and  argument:</p><ul><li>The 1st argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can do morphological transformation.</li><li>It's the magnitude of displacements .</li><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value(,  or /( or )) means .</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It's the smoothness of displacements .</li><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value(,  or /( or )) means .</li></ul></li><li>The 3rd argument for initialization is (Optional-Default:<code>InterpolationMode.BILINEAR</code>-Type:<a href=\"https://pytorch.org/vision/main/_modules/torchvision/transforms/functional.html\" rel=\"noopener noreferrer\">InterpolationMode</a>).</li><li>The 4th argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can change the background of an image. *The background can be seen when doing morphological transformation for an image.</li><li>A tuple/list must be the 1D with 1 or 3 elements.</li><li>If all values are , it's black.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul></ul></li></ul><div><pre><code></code></pre></div>","contentLength":1032,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Anonymous Functions in Go: A Practical Guide","url":"https://dev.to/abstractmusa/understanding-anonymous-functions-in-go-a-practical-guide-57hd","date":1740273116,"author":"Md Abu Musa","guid":9398,"unread":true,"content":"<h2><strong>What is an Anonymous Function?</strong></h2><p>An  is a function . Instead of being declared like a traditional named function, it is defined inline and assigned to a variable or executed immediately.</p><div><pre><code></code></pre></div><p>ğŸ“Œ Here,  holds an anonymous function that takes two integers and returns their sum.</p><h2><strong>Use Cases of Anonymous Functions</strong></h2><h3><strong>1ï¸âƒ£ Passing Anonymous Functions as Arguments</strong></h3><p>Since functions can be passed as arguments, anonymous functions are useful for higher-order functions.</p><div><pre><code></code></pre></div><p>âœ…  Useful in callback functions or custom processing logic.</p><h3><strong>2ï¸âƒ£ Using Anonymous Functions in Goroutines</strong></h3><p>Goroutines allow concurrent execution, and anonymous functions are a great way to define short-lived concurrent tasks.</p><div><pre><code></code></pre></div><p>âœ…  Running background tasks without defining a named function.</p><h3><strong>3ï¸âƒ£ Returning Anonymous Functions (Closures)</strong></h3><p>Closures allow an anonymous function to capture variables from its surrounding scope.</p><div><pre><code></code></pre></div><p>âœ…  Creating  that retain state.</p><h3><strong>4ï¸âƒ£ Storing Anonymous Functions in a Map</strong></h3><p>You can store anonymous functions in a map for dynamic execution.</p><div><pre><code></code></pre></div><p>âœ…  Implementing  or .</p><h2><strong>What is an IIFE (Immediately Invoked Function Expression)?</strong></h2><p>An <strong>IIFE (Immediately Invoked Function Expression)</strong> is an anonymous function that runs <strong>immediately after being defined</strong>.</p><div><pre><code></code></pre></div><p>âœ…  One-time setup logic, reducing unnecessary variable scope.</p><p>Anonymous functions in Go offer flexibility and concise coding, making them a powerful tool for:</p><ul><li><strong>Concurrent execution (Goroutines)</strong></li><li><strong>Closures (Retaining state)</strong></li><li><strong>One-time execution (IIFE)</strong></li></ul><p>By understanding and implementing anonymous functions effectively, you can write , , and  Go code.</p>","contentLength":1555,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fixing Django FieldError at /admin/accounts/customuser/add/","url":"https://dev.to/wsvincent/fixing-django-fielderror-at-adminaccountscustomuseradd-k9n","date":1740271680,"author":"Will Vincent","guid":9397,"unread":true,"content":"<p>If you are a Django developer who wants to add a custom user model to your project, you've likely come across this error on Django versions 5.0 and above.</p><p><code>FieldError at /admin/accounts/customuser/add/\nUnknown field(s) (usable_password) specified for CustomUser. Check fields/fieldsets/exclude attributes of class CustomUserAdmin.</code></p><p>The issue is around  In Django versions up to 4.2, you could set your  file to add updated user creation and change forms.</p><div><pre><code></code></pre></div><p>However, as of Django 5.0, that leads to the above-mentioned . The fix is straightforward to do, thankfully, which is to swap out  for the newer <a href=\"https://docs.djangoproject.com/en/5.1/topics/auth/default/#django.contrib.auth.forms.AdminUserCreationForm\" rel=\"noopener noreferrer\">AdminUserCreationForm</a> instead, which includes the additional  field causing the initial issue.</p><div><pre><code></code></pre></div>","contentLength":689,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Por que vocÃª deve repensar o uso de Regex em validaÃ§Ãµes de strings em Go","url":"https://dev.to/renanbastos93/por-que-voce-deve-repensar-o-uso-de-regex-em-validacoes-de-strings-no-go-1cdi","date":1740269115,"author":"renanbastos93","guid":9401,"unread":true,"content":"<p>Quando falamos de validaÃ§Ã£o de strings no Go, uma das soluÃ§Ãµes mais comuns Ã© o uso de expressÃµes regulares (regex). No entanto, dependendo do contexto, o uso de regex pode ser menos eficiente do que alternativas mais simples. Em sistemas de alta performance, como os que lidam com grandes volumes de dados ou que precisam ser executados em ambientes com recursos limitados, Ã© importante considerar o impacto de cada escolha de implementaÃ§Ã£o.</p><p>Embora regex seja uma ferramenta poderosa, sua utilizaÃ§Ã£o indiscriminada pode resultar em impactos de performance, principalmente em Go, onde cada operaÃ§Ã£o Ã© otimizada ao mÃ¡ximo para garantir alta eficiÃªncia. Neste post, vamos abordar um exemplo simples de validaÃ§Ã£o de strings alfanumÃ©ricas, comparando o uso de regex com uma abordagem baseada em Unicode e explicando como otimizar o uso de regex no Go para obter melhores resultados.</p><p>O que acontece por trÃ¡s das cortinas: Regex vs Unicode\nA biblioteca  Ã© bastante Ãºtil, mas pode ser mais lenta do que validaÃ§Ãµes feitas manualmente usando funÃ§Ãµes da biblioteca unicode. Isso ocorre porque o Go precisa compilar o regex e avaliar sua expressÃ£o toda vez que Ã© usado dentro de um mÃ©todo. Isso consome mais tempo de CPU e memÃ³ria, especialmente em validaÃ§Ãµes simples, como a checagem, se uma string contÃ©m apenas caracteres alfanumÃ©ricos.</p><p>A alternativa, mais eficiente, Ã© iterar sobre a string e validar cada caractere individualmente, utilizando funÃ§Ãµes como unicode.IsLetter e unicode.IsDigit. Isso evita a sobrecarga de compilar o regex toda vez que a funÃ§Ã£o Ã© chamada e pode ser muito mais rÃ¡pido para cenÃ¡rios simples.</p><h2>\n  \n  \n  Exemplo prÃ¡tico: Comparando as abordagens\n</h2><p>Aqui estÃ¡ um exemplo em Go para comparar as duas abordagens: uma usando regex e outra utilizando Unicode diretamente.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  ImplementaÃ§Ã£o usando Unicode:\n</h3><div><pre><code></code></pre></div><h2>\n  \n  \n  ComparaÃ§Ã£o de Performance\n</h2><p>Vamos rodar alguns benchmarks para comparar a performance das duas abordagens.</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Como vocÃª pode observar nos benchmarks acima, a versÃ£o que usa Unicode Ã© significativamente mais rÃ¡pida do que a versÃ£o com regex. A versÃ£o com Unicode realiza cerca de 18 milhÃµes de operaÃ§Ãµes por segundo, enquanto a versÃ£o com regex realiza apenas cerca de 3 milhÃµes.</p><p>Quando usamos regex, estamos criando e compilando uma expressÃ£o regular toda vez que chamamos a funÃ§Ã£o. O processo de compilaÃ§Ã£o e execuÃ§Ã£o da expressÃ£o regular envolve mais passos do que simplesmente iterar sobre os caracteres da string com funÃ§Ãµes do unicode. Isso faz com que a execuÃ§Ã£o com regex consuma mais tempo de CPU e memÃ³ria.</p><p>Isso nÃ£o significa que vocÃª deve parar de usar regex. Em casos mais complexos, onde vocÃª precisa de validaÃ§Ãµes mais sofisticadas, regex pode ser a melhor escolha. No entanto, em validaÃ§Ãµes simples, como a checagem de caracteres alfanumÃ©ricos, o uso de Unicode diretamente Ã© muito mais eficiente.</p><p>Dica de Performance: Compile Regex fora do mÃ©todo\nSe vocÃª optar por usar regex, uma boa prÃ¡tica Ã© compilar a expressÃ£o regular fora do mÃ©todo, para que ela nÃ£o precise ser recompilada a cada chamada. Isso pode ajudar a reduzir o custo de performance de usar regex.</p><div><pre><code></code></pre></div><p>Ao compilar a expressÃ£o regular uma vez e reutilizÃ¡-la, vocÃª reduz significativamente o impacto de performance.</p><p>Embora regex seja uma ferramenta poderosa e Ãºtil, seu uso em validaÃ§Ãµes simples pode ser ineficiente, especialmente quando a performance Ã© uma prioridade. Em Go, alternativas como a utilizaÃ§Ã£o da biblioteca unicode podem oferecer ganhos significativos de performance. Se for necessÃ¡rio usar regex, lembre-se de compilar a expressÃ£o regular fora do mÃ©todo para otimizar o desempenho.</p><p>Agora, repense como vocÃª estÃ¡ validando suas strings no seu projeto e escolha a abordagem mais eficiente para o seu caso!</p>","contentLength":3808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"íŒŒì´ì¬ìœ¼ë¡œ Epitope binning í•˜ê¸°","url":"https://dev.to/partrita/paisseoneuro-epitope-binning-hagi-1ch1","date":1740268800,"author":"Joconan","guid":9547,"unread":true,"content":"<p>ì¹˜ë£Œìš© ë‹¨í´ë¡  í•­ì²´(mAbs)ëŠ” ë°”ì´ì˜¤ì˜ì•½í’ˆ ì‹œì¥ì˜ 70% ì´ìƒì„ ì°¨ì§€í•˜ë©° ì§€ì†ì ìœ¼ë¡œ ì„±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. í•­ì²´ ê°œë°œ ì´ˆê¸° ë‹¨ê³„ì—ì„œ ì¹˜ë£Œì œ ë° ì§„ë‹¨ ë„êµ¬ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì ì ˆí•œ íŠ¹ì„±ì„ ê°€ì§„ í›„ë³´ë¥¼ ì„ ë³„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì—í”¼í† í”„ ë¹ˆë‹ì€ mAbsê°€ í‘œì  ë‹¨ë°±ì§ˆ(í•­ì›)ì— ê²°í•©í•˜ëŠ” íŠ¹ì„±ì„ íŒŒì•…í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë™ì¼í•œ í‘œì  ë‹¨ë°±ì§ˆì— íŠ¹ì´ì ì¸ mAbsë¥¼ ìŒìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ í•­ì›ì˜ íŠ¹ì • ë¶€ìœ„ì— ëŒ€í•œ ê²°í•©ì„ ì„œë¡œ ì°¨ë‹¨í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ê°™ì€ ì—í”¼í† í”„ì— ëŒ€í•œ ê²°í•©ì„ ì°¨ë‹¨í•˜ëŠ” mAbsëŠ” í•¨ê»˜ â€œë¹ˆâ€ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. ê°™ì€ ë¹ˆì— ì†í•œ mAbsëŠ” ì¢…ì¢… ìœ ì‚¬í•œ ê¸°ëŠ¥ì„ í•˜ë¯€ë¡œ ì—í”¼í† í”„ ë¹ˆì„ í†µí•´ í›„ë³´ í•­ì²´ì˜ ë‹¤ì–‘ì„±ì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—í”¼í† í”„ ë‹¤ì–‘ì„±ì€ ì§€ì  ì¬ì‚°ê¶Œ ë³´í˜¸ë¥¼ í™•ëŒ€í•˜ëŠ” ë°ë„ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ í•­ì²´ë“¤ì´ ê°™ì€ í•­ì›ì— ê²°í•©í•˜ë”ë¼ë„ ì‘ìš© ë©”ì»¤ë‹ˆì¦˜ì´ ë‹¤ë¥¼ ìˆ˜ ìˆëŠ”ë° ì´ëŠ” ì¼ë¶€ ì•”ê³¼ ê°ì—¼ì„± ì§ˆí™˜ ì¹˜ë£Œì— ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p><blockquote><p>ì—í”¼í† í”„ ë¹ˆë‹ì€ ì—í”¼í† í”„ ë§¤í•‘ê³¼ í˜¼ë™í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ì—í”¼í† í”„ ë§¤í•‘ì—ì„œëŠ” í•­ì›ì˜ ê°œë³„ ë‹¨í¸ì— ëŒ€í•œ í•­ì²´ ê²°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ í•­ì²´ê°€ ê²°í•©í•˜ëŠ” í•­ì›ì˜ íŠ¹ì • ì—í”¼í† í”„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.</p></blockquote><p>SPRì„ ì´ìš©í•œ ì—í”¼í† í”„ ë¹ˆë‹ì˜ ì£¼ìš” ì¥ì ì€ í•­ì›ê³¼ ì†ŒëŸ‰ì˜ ì •ì œëœ í•­ì²´ë§Œ ìˆìœ¼ë©´ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. SPRì„ í†µí•œ ì—í”¼í† í”„ ë¹„ë‹ì˜ ì›ë¦¬ë¥¼ ê°„ëµí•˜ê²Œ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì²«ë²ˆì§¸ í•­ì²´ë¥¼ ê³ ì •ì‹œì¼œ ë†“ê³  í•­ì›ê³¼ ë‘ë²ˆì§¸ í•­ì²´ë¥¼ ë„£ì–´ì„œ RUê°’ì„ ì¸¡ì •í•˜ëŠ”ë°, ì—í”¼í† í”„ê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²½ìš°ì— RU ê°’ì´ ë†’ê²Œ ì¸¡ì •ë©ë‹ˆë‹¤. ì¦‰, ì—í”¼í† í”„ê°€ ë¹„ìŠ·í•œ ê²½ìš°ëŠ” RU ê°’ì´ ë‚®ê²Œ ì¸¡ì •ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ì œ SPRì„ í†µí•´ ì–»ì€ epitope binning ë°ì´í„°ë¥¼ íŒŒì´ì¬ìœ¼ë¡œ ë¶„ì„í•´ì„œ ì–´ë–¤ í•­ì²´ ì»¤ë®¤ë‹ˆí‹°ê°€ ìˆëŠ”ì§€ ì‹ë³„í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.</p><pre><code></code></pre><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><div><table><tbody></tbody></table></div><p>ìœ„ì˜ ê²°ê³¼ë¥¼ í†µí•´ ë°ì´í„°ì˜ í˜•íƒœë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ 2563ê°œê°€ ìˆê³  ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì„ ë³´ì•„í•˜ë‹ˆ ì‹¤ì œ ì‹¤í—˜ ê°’ì´ ì•„ë‹Œ ì¼ì¢…ì˜ í›„ì²˜ë¦¬ê°€ëœ ê°’ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p><p>íˆíŠ¸ë§µì€ ì°¨ë‹¨, ë¹„ì°¨ë‹¨ ë° ë¶ˆí™•ì‹¤í•œ í•­ì²´ ìŒì— ëŒ€í•œ ë¹ ë¥¸ ê°œìš”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íˆíŠ¸ë§µ ë‚´ ë°ì´í„°ë¥¼ ê°„í¸í•˜ê²Œ ê²€ì‚¬í•  ìˆ˜ ìˆìœ¼ë©° ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤:</p><ol><li>ì§ê´€ì  ì´í•´: ë³µì¡í•œ ë°ì´í„°ë¥¼ ìƒ‰ìƒ ì½”ë“œë¡œ í‘œí˜„í•˜ì—¬ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li><li>íŒ¨í„´ ì‹ë³„: ëŒ€ëŸ‰ì˜ ë°ì´í„°ì—ì„œ íŒ¨í„´ì´ë‚˜ íŠ¸ë Œë“œë¥¼ ì‰½ê²Œ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li><li>ìœ ì—°í•œ ë¶„ì„: ì»·ì˜¤í”„ ê°’ì„ ì¡°ì •í•¨ìœ¼ë¡œì¨ ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li><li>íš¨ìœ¨ì ì¸ ë°ì´í„° í•´ì„: ë§ì€ ì–‘ì˜ ì •ë³´ë¥¼ ì••ì¶•ëœ í˜•íƒœë¡œ í‘œí˜„í•˜ì—¬ ë¹ ë¥¸ ì˜ì‚¬ê²°ì •ì„ ë•ìŠµë‹ˆë‹¤.</li></ol><pre><code></code></pre><p>ìœ„ íˆíŠ¸ë§µ ê²°ê³¼ë¥¼ í†µí•´ í¬ê²Œ 4ê°œì˜ í´ëŸ¬ìŠ¤í„°ê°€ ì¡´ì¬í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì‰½ê²Œ ìœ ì¶”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p><p>K-Nearest Neighbors (KNN) í´ëŸ¬ìŠ¤í„°ë§ì€ ì—í”¼í† í”„ ë¹ˆë‹ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ë° ìœ ìš©í•œ ë°©ë²•ìœ¼ë¡œ íˆíŠ¸ë§µë³´ë‹¤ ë” ëª…ë£Œí•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.</p><pre><code></code></pre><p>ìœ„ì˜ ê²°ê³¼ë¥¼ í†µí•´ ì´ 4ê°œì˜ í•­ì²´ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ í™•ì¸ í•  ìˆ˜ ìˆì—ˆê³  íˆíŠ¸ë§µ ê²°ê³¼ì™€ ìœ ì‚¬í•¨ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.</p><p>ì—í”¼í† í”„ ë¹ˆë‹ì€ í•­ì²´ì˜ ê²°í•© íŠ¹ì„±ì„ íŒŒì•…í•˜ê³  ë‹¤ì–‘í•œ ì—í”¼í† í”„ë¥¼ í‘œì ìœ¼ë¡œ í•˜ëŠ” í•­ì²´ë¥¼ ì„ ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íˆíŠ¸ë§µê³¼ KNN í´ëŸ¬ìŠ¤í„°ë§ ë“±ì˜ ì‹œê°í™” ë°©ë²•ìœ¼ë¡œ í•­ì²´ íŒ¨ë„ì˜ ë‹¤ì–‘ì„±ì„ í™•ì¸í•˜ê³  ê°€ì¥ ìœ ë§í•œ í›„ë³´ë¥¼ ì„ ë³„í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.</p>","contentLength":3733,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The unexpected way in which conditional types constrain type variables in TypeScript","url":"https://2ality.com/2025/02/conditional-type-constraints.html","date":1740268800,"author":"Dr. Axel Rauschmayer","guid":9582,"unread":true,"content":"<p>The TypeScript handbook makes an interesting <a href=\"https://www.typescriptlang.org/docs/handbook/2/conditional-types.html#conditional-type-constraints\">statement</a>: â€œOften, the checks in a conditional type will provide us with some new information. Just like narrowing with type guards can give us a more specific type, the true branch of a conditional type will further constrain generics by the type we check against.â€</p><p>In this blog post, weâ€™ll see that this goes further than you may think.</p>","contentLength":388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Streaming SQL in Stateful DataFlows","url":"https://dev.to/debadyuti/streaming-sql-in-stateful-dataflows-3jng","date":1740263855,"author":"Deb","guid":9372,"unread":true,"content":"<h2>\n  \n  \n  Streaming SQL Functionality\n</h2><p>SQL Streaming Queries and Stream Processing Operations is released in Stateful DataFlow Beta 7 running on Fluvio 0.15.2</p><p><strong>With SQL Streaming on Stateful DataFlow you can:</strong></p><ul><li>Run ad-hoc queries on saved state objects and materialized views based live event streams.</li><li>Use SQL queries to run stream processing operations in data flows.</li></ul><p>For those who are not aware of Fluvio or Stateful DataFlow yet:</p><p>Stateful DataFlow - Stream processing layer built on Fluvio built using the wasm component model.</p><h2>\n  \n  \n  SQL: From Static Tables to Streaming Data\n</h2><p>Remember when SQL was the only way to talk to your data? It wasn't just a query language - it was  query language. But its story goes deeper than syntax.</p><h3>\n  \n  \n  The Universal Language of Data\n</h3><p>Just as merchants in medieval Mediterranean ports needed a shared language to trade (that's where \"lingua franca\" came from), the tech world needed SQL to make data accessible across different systems and teams.</p><p>If you're in a room with a DBA, a data analyst, and a business analyst. What's the one language they all speak? Likely SQL.</p><div><pre><code></code></pre></div><p>Look familiar? Whether you're running Oracle, Postgres, or MySQL, this just works. Well sort of!</p><p>Three key factors made SQL a long-term utility that stood the test of time:</p><ol><li><p>\nInstead of telling machines HOW to get data, you just say WHAT you want. <code>SELECT * FROM users WHERE status = 'active'</code> reads almost like English.</p></li><li><p>\nFrom startups to Fortune 500s, SQL skills travel. Write once, run anywhere - from healthcare to fintech.</p></li><li><p>\nNeed to analyze sales data? Track user behavior? SQL's got you covered, backed by decades of tooling and optimization.</p></li></ol><p>In a world of Artificial Intelligence, Web3, and global markets, event streaming is no longer a luxury - it's a basic need. Ask yourself:</p><ul><li>Is your application combining data from multiple sources in real-time?</li><li>Are your customers happy with stale insights?</li><li>Do you need fresh data on demand?</li></ul><h3>\n  \n  \n  Bridging Static and Streaming\n</h3><p>What if you could use familiar SQL syntax for real-time data processing? What if your team could leverage their existing SQL skills for stream processing?</p><p>We've been exploring these questions and implementing solutions that bring SQL's simplicity to streaming data. Want to see how? Check out the full article where we dive into:</p><ul><li>Practical examples using NY Transit data</li><li>Real-world streaming SQL queries in action</li><li>How to implement stream processing without learning a new language</li></ul>","contentLength":2437,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust Developer Survey Finds Increasing Usage, Especially on Linux","url":"https://developers.slashdot.org/story/25/02/22/042227/rust-developer-survey-finds-increasing-usage-especially-on-linux?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1740260040,"author":"EditorDavid","guid":9319,"unread":true,"content":"This year's \"State of Rust\" survey was completed by 7,310 Rust developers. DevClass note some key findings:\n\nWhen asked about their biggest worries for Rust's future, 45.5 percent cited \"not enough usage in the tech industry,\" up from 42.5 percent last year, just ahead of the 45.2 percent who cited complexity as a concern... Only 18.6 percent declared themselves \"not worried,\" though this is a slight improvement on 17.8 percent in 2023... \nAnother question asks whether respondents are using Rust at work. 38.2 percent claimed to use it for most of their coding [up from 34% in 2023], and 13.4 percent a few times a week, accounting for just over half of responses. At the organization level there is a similar pattern. 45.5 percent of organizations represented by respondents make \"non-trivial use of Rust,\" up from 38.7 percent last year. \n\nMore details from I Programmer:\n\nOn the up are \"Using Rust helps us achieve or goals\", now 82% compared to 72% in 2022; \"We're likely to use Rust again in the future\", up 3% to 78%; and \"Using Rust has been worth the cost of Adoption\". Going down are \"Adopting Rust has been challenging\", now 34.5% compared to 38.5% in 2022; and \"Overall adopting Rust has slowed down our team\" down by over 2% to 7%. \n\n\n\n\"According to the survey, organizations primarily choose Rust for building correct and bug-free software (87.1%), performance characteristics (84.5%), security and safety properties (74.8%), and development enjoyment (71.2%),\" writes The New Stack:\n\n Rust seems to be especially popular for creating server backends (53.4%), web and networking services, cloud technologies and WebAssembly, the report said. It also seems to be gaining more traction for embedded use cases... Regarding the preferred development environment, Linux remains the dominant development platform (73.7%). \n\nHowever, although VS Code remains the leading editor, its usage dropped five percentage points, from 61.7% to 56.7%, but the Zed editor gained notable traction, from 0.7% to 8.9%. Also, \"nine out of 10 Rust developers use the current stable version, suggesting strong confidence in the language's stability,\" the report said... \n\nOverall, 82% of respondents report that Rust helped their company achieve its goals, and daily Rust usage increased to 53% (up four percentage points from 2023). When asked why they use Rust at work, 47% of respondents cited a need for precise control over their software, which is up from 37% when the question was asked two years ago.","contentLength":2502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Observability Made Easy: Adding Logs, Traces & Metrics to FastAPI with Logfire","url":"https://dev.to/devgeetech/observability-made-easy-adding-logs-traces-metrics-to-fastapi-with-logfire-529l","date":1740257060,"author":"Joel Gee Roy","guid":9304,"unread":true,"content":"<p>Picture this: You deploy your shiny new application. Everything looks great in dev, the logs are clean, requests are snappy, and life is good. Thenâ€¦ disaster strikes. A user reports a bug. Another complains about slow response times. You check your logsâ€”wait, where are they? You SSH into the server, tail some logs, guess what went wrong, and hope for the best. Sound familiar?</p><p>Observabilityâ€”knowing whatâ€™s happening inside your app in real timeâ€”shouldnâ€™t be this hard. But setting up an observability stack often feels like assembling IKEA furniture with missing instructions. Thatâ€™s where  comes in.</p><p><a href=\"https://pydantic.dev/logfire\" rel=\"noopener noreferrer\">Pydantic Logfire</a> is a platform that makes it ridiculously easy to add observability to your application, no matter the size. In this post, Iâ€™ll show you how to integrate Logfire into a  app to get instant insights into logs, traces, and metricsâ€”without the usual setup headaches. By the end, youâ€™ll have real-time visibility into whatâ€™s happening under the hood, so you can debug, optimize, and sleep better at night.</p><p>We will build two services that handles orders and shipping for BigHuge Corp Inc. To keep things straightforward, we'll put both the services in the same repo and run a FastAPI for each of them to emulate two services talking to each other.</p><div><pre><code>fastapi-logfire\nfastapi-logfire\n</code></pre></div><p>We'll create a virtual environment and activate it:</p><p>Let's install the necessary packages:</p><div><pre><code>pip </code></pre></div><h3> and  services\n</h3><p>The ordering service will contain just two endpoints - one to place an order and one to get the details of a specific order. Similarly the shipping service will have two endpoints - one to initiate the shipping process and one to get the status of a shipment. </p><p>The ordering service will invoke the shipping service in both its endpoints. \nHere's the code for both the services:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Now we'll create  and  which will be used to start both the services</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>You can run both services on separate terminals using the  command.</p><div><pre><code>fastapi dev 8001 app/main2.py\n</code></pre></div><p>You can go to  to try the ordering service out and see if everything is working as expected.</p><p>Before we integrate logfire, you need to create a token from Logfire so that you can write data to the logfire dashboard.  You can read on how to generate the token <a href=\"https://logfire.pydantic.dev/docs/#overview\" rel=\"noopener noreferrer\">here</a>. Once you have the token, you can save it as an environment variable in a  file (You might need to install  to use it).</p><p>We'll start small. We'll integrate logfire into our app such that all logs generated by the app will be sent to the logfire dashboard. We'll integrate logfire into the  standard library. (Tip: you can also emit logs directly using logfire methods like )</p><div><pre><code></code></pre></div><p>We've defined a  function that we can call anywhere in our project to send logs.</p><div><pre><code></code></pre></div><p>You can add similar statements in the  handler as well.</p><p>In  and , we'll add the logic to configure logfire</p><div><pre><code></code></pre></div><p>Do the same for , but with  as shipping.</p><p>If you run both the apps again, and try out some requests, you will see some logs displaying on the Live tab of the logfire dashboad:</p><p>Great! Now we see our logging statements in the dashboard. But Logfire lets us instrument fastapi directly to get even more data for each request. Let's implement that. </p><p>Under the  in both  and , add a new line of code:</p><div><pre><code></code></pre></div><p>Now all your requests to both servers are instrumented automatically.</p><p>Now all the service level logs are neatly nested under the respective requests. But get this - it can get even better. </p><p>So we set up our application in such a way the  endpoint will call  endpoint inside it. It would be really nice if our dashboard could display this sequential flow instead of showing both the calls as separate (like we saw before). We can do this using tracing. </p><p>For tracing to work, context has to be propagated across services. This \"context\" helps keep track of the parent trace/span of a new span/log so that they can be viewed in tandem.  Thankfully logfire gives us an easy way to do this. Since we're using  to use the  service, we'll install the associated library from logfire.</p><div><pre><code>pip </code></pre></div><p>We'll update the code in  to add <code>logfire.instrument_requests()</code></p><div><pre><code></code></pre></div><p>We don't need to update  (the shipping one) because we aren't sending any requests in that server for now.</p><p> will make sure that the traceparent header is automatically set when making requests.  makes sure that the  header is extracted correctly from incoming requests. Thats it. !</p><p>Run the servers again, and try sending some requests. You'll see something like this in your logfire dashboard:</p><p>And heavens forbid, if something were to go wrong in one of the services, you now know exactly where it went wrong. Let's test this out:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>If we try  now, we'll see something like this:</p><p>Metrics signify how much of something exists. When paired with a time series, we can know the metric value at a point in time. This is useful for observing data like number of requests over a period of time, or things like CPU utilisation over time etc.</p><p>Setting up system metric tracking is pretty straightforward with logfire, so we'll do that first.</p><div><pre><code>pip </code></pre></div><div><pre><code></code></pre></div><p>Now go to the logfire platform on your browser, select the \"Dashboards\" tab. Click on the \"New Dashboard\" button. Select \"Basic System Metrics (Logfire)\" from the drop down.</p><p>Run the server again and you should be seeing the graph populate with your system data.</p><p>if you choose \"Web Server Metrics\" from the create dashboard drop down, you'll get another readymade dashboard containing useful metrics from our services.</p><p>Now let's add a custom metric to see the orders placed over time. We'll use a counter for this one.</p><div><pre><code></code></pre></div><p>Now on the logfire platform, create a new dashboard and choose \"Start from scratch\" from the dropdown. Click on \"Add Chart\". We have to retrieve the data we need from logfire using SQL. To get an idea of how to structure your SQL queries, use the \"Explore\" tab in the logfire platform. </p><p>To get the total orders placed in a time period we'll use the following query:</p><div><pre><code></code></pre></div><p>Set the visualisation type as \"Values\" and save the chart. Now the chart will get updated based on the time period that you select on top.</p><p>To chart this in a time series, use the following query:</p><div><pre><code></code></pre></div><p>Choose the visualisation as \"Time Series\", and set the metrics to \"scalar_value\". </p><p>In this post, we took a  to setting up observability in a FastAPI app using , covering <strong>logging, distributed tracing, and real-time metrics</strong> with minimal setup. While we focused on automatic instrumentation, thereâ€™s even more you can exploreâ€”like , which give you fine-grained control over spans and logs for deeper insights. If you enjoyed this post, consider subscribing to my <a href=\"https://devgeetech.beehiiv.com/subscribe\" rel=\"noopener noreferrer\">newsletter</a> for more dev-focused deep dives.</p>","contentLength":6514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"25+ Little-Known Python Resources That Will Make You a Pro!","url":"https://dev.to/dev-resources/25-little-known-python-resources-that-will-make-you-a-pro-51ed","date":1740254031,"author":"Dev Resources","guid":9282,"unread":true,"content":"<h2><strong>1. Wk 3 Orchestration: MLOPs with DataTalks</strong></h2><p>We've been introduced to the subject of Machine Learning in Operations, familarized with Experiment... <a href=\"http://dev-resources.site/topic/python/1891940/wk-3-orchestration-mlops-with-datatalks-2057\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fikjvos94cyxfw2gr51d8.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>2. RandomAdjustSharpness in PyTorch</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   RandomAdjustSharpness() can randomly... <a href=\"http://dev-resources.site/topic/python/2292935/randomadjustsharpness-in-pytorch-dd8\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomAdjustSharpness%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292935%2Frandomadjustsharpness-in-pytorch-dd8\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>3. RandomSolarize in PyTorch</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomInvert().  My post explains... <a href=\"http://dev-resources.site/topic/python/2292950/randomsolarize-in-pytorch-mc5\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomSolarize%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292950%2Frandomsolarize-in-pytorch-mc5\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>4. RandomAutocontrast in PyTorch</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   RandomAutocontrast() can randomly... <a href=\"http://dev-resources.site/topic/python/2292957/randomautocontrast-in-pytorch-1311\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomAutocontrast%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292957%2Frandomautocontrast-in-pytorch-1311\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>5. RandomInvert in PyTorch</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomSolarize().  My post explains... <a href=\"http://dev-resources.site/topic/python/2292940/randominvert-in-pytorch-40pa\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomInvert%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292940%2Frandominvert-in-pytorch-40pa\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>6. RandomPosterize in PyTorch</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   RandomPosterize() can randomly... <a href=\"http://dev-resources.site/topic/python/2292927/randomposterize-in-pytorch-24d2\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomPosterize%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292927%2Frandomposterize-in-pytorch-24d2\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   Grayscale() can convert an image to... <a href=\"http://dev-resources.site/topic/python/2292877/grayscale-in-pytorch-19og\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DGrayscale%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292877%2Fgrayscale-in-pytorch-19og\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   CenterCrop() can crop an image,... <a href=\"http://dev-resources.site/topic/python/2292771/centercrop-in-pytorch-16he\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DCenterCrop%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292771%2Fcentercrop-in-pytorch-16he\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   Resize() can resize an image as... <a href=\"http://dev-resources.site/topic/python/2292756/resize-in-pytorch-4nld\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DResize%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292756%2Fresize-in-pytorch-4nld\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>10. RandomPerspective in PyTorch</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomRotation().  My post explains RandomAffine().  My... <a href=\"http://dev-resources.site/topic/python/2292728/randomperspective-in-pytorch-3i1g\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomPerspective%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292728%2Frandomperspective-in-pytorch-3i1g\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>11. 5 Best Programming Languages to Learn: Decoding the Future</strong></h2><p>In today's tech-driven world, staying ahead of the curve is essential. As we approach 2025, certain... <a href=\"http://dev-resources.site/topic/python/2292751/5-best-programming-languages-to-learn-decoding-the-future-499l\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2jnr6n366z1sdv4fm148.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>12. ğŸš€ Day 3 of #100DaysOfCoding</strong></h2><p>Today, I dived into Linked Lists and learned the basics: âœ… Insertion âœ… Traversal âœ… Deletion âœ…... <a href=\"http://dev-resources.site/topic/python/2292544/day-3-of-100daysofcoding-1008\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3D%25F0%259F%259A%2580%2520Day%25203%2520of%2520%23100DaysOfCoding%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292544%2Fday-3-of-100daysofcoding-1008\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>13. Building a REST API with Django REST Framework: A Beginners Guide</strong></h2><p>Introduction   Imagine you're building a book management system where users can browse... <a href=\"http://dev-resources.site/topic/python/2291235/building-a-rest-api-with-django-rest-framework-a-beginners-guide-1b1n\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F02qb8vlmexgls14m5nvw.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>14. RandomAffine in PyTorch (6)</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... <a href=\"http://dev-resources.site/topic/python/2292276/randomaffine-in-pytorch-6-4813\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomAffine%2520in%2520PyTorch%2520%286%29%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292276%2Frandomaffine-in-pytorch-6-4813\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>15. Open-Source Book Creator with Multi-Agent AI</strong></h2><p>I'm excited to share ** ğŸ“ LibriScribe**, an open-source book creation system I've developed that... <a href=\"http://dev-resources.site/topic/python/2292128/open-source-book-creator-with-multi-agent-ai-1bnl\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Foyhp0bkmotpyeyswljtx.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>16. Seamlessly Compare Maps on QGIS with the QMapCompare Plugin</strong></h2><p>Introduction   When working with QGIS, you often switch between basemaps, but  comparing... <a href=\"http://dev-resources.site/topic/python/2290821/seamlessly-compare-maps-on-qgis-with-the-qmapcompare-plugin-3186\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DSeamlessly%2520Compare%2520Maps%2520on%2520QGIS%2520with%2520the%2520QMapCompare%2520Plugin%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2290821%2Fseamlessly-compare-maps-on-qgis-with-the-qmapcompare-plugin-3186\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>17. How I built an AI-Powered Code Reviewer (and you can too).</strong></h2><p>AI is flipping the game for developers, and honestly,   I got fed up of seeing people waste hours... <a href=\"http://dev-resources.site/topic/python/2291871/how-i-built-an-ai-powered-code-reviewer-and-you-can-too-2fk4\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fow1d8kgfmwhnam9y1y22.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>18. RandomAffine in PyTorch (5)</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... <a href=\"http://dev-resources.site/topic/python/2292117/randomaffine-in-pytorch-5-1c2m\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomAffine%2520in%2520PyTorch%2520%285%29%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292117%2Frandomaffine-in-pytorch-5-1c2m\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>19. RandomAffine in PyTorch (4)</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... <a href=\"http://dev-resources.site/topic/python/2292107/randomaffine-in-pytorch-4-570l\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomAffine%2520in%2520PyTorch%2520%284%29%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292107%2Frandomaffine-in-pytorch-4-570l\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>20. RandomAffine in PyTorch (3)</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... <a href=\"http://dev-resources.site/topic/python/2292103/randomaffine-in-pytorch-3-3hkm\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomAffine%2520in%2520PyTorch%2520%283%29%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292103%2Frandomaffine-in-pytorch-3-3hkm\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>21. What is LangGraph and How to Use It for Building AI Agents</strong></h2><p>I keep finding myself going back to the LangChain documentation to figure out how to use LangGraph.... <a href=\"http://dev-resources.site/topic/python/2291412/what-is-langgraph-and-how-to-use-it-for-building-ai-agents-4bj2\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyfbm8porqcndcykb004r.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>22. How to Create Python Virtual Environments on Ubuntu</strong></h2><p>When working on different Python projects, it's often necessary to create isolated environments for... <a href=\"http://dev-resources.site/topic/python/2292052/creating-python-virtual-environments-on-ubuntu-5an7\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgnh2vbvlce03ylowdhjc.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>23. RandomAffine in PyTorch (2)</strong></h2><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains RandomAffine() about degrees, translate, fill and... <a href=\"http://dev-resources.site/topic/python/2292045/randomaffine-in-pytorch-2-3d3a\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DRandomAffine%2520in%2520PyTorch%2520%282%29%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2292045%2Frandomaffine-in-pytorch-2-3d3a\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>24. Project Translate: The Translate API (Part 4)</strong></h2><p>Hey developers! ğŸ‘‹ For the last post in the series, we'll provision the infrastructure on AWS and... <a href=\"http://dev-resources.site/topic/python/2286878/project-translate-the-translate-api-part-4-1726\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DProject%2520Translate%3A%2520The%2520Translate%2520API%2520%28Part%25204%29%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2286878%2Fproject-translate-the-translate-api-part-4-1726\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>25. Explore the Future: Trending GitHub Projects Revolutionizing Tech ğŸš€âœ¨</strong></h2><p>ğŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-21   Every week, thousands of... <a href=\"http://dev-resources.site/topic/python/2291371/explore-the-future-trending-github-projects-revolutionizing-tech-2ifh\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DExplore%2520the%2520Future%3A%2520Trending%2520GitHub%2520Projects%2520Revolutionizing%2520Tech%2520%25F0%259F%259A%2580%25E2%259C%25A8%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2291371%2Fexplore-the-future-trending-github-projects-revolutionizing-tech-2ifh\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>26. **\"ğŸš€ Dive into Innovation: Top Trending GitHub Projects Shaping the Future of AI!\"</strong>**\n</h2><p>ğŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-21   Every week, thousands of... <a href=\"http://dev-resources.site/topic/python/2291222/-dive-into-innovation-top-trending-github-projects-shaping-the-future-of-ai-29c7\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3D%2A%2A\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>27. Day-03 of Kapilâ€™s learning python programming</strong></h2><p>The things i learned from python are:   1.More i.e. depth in list:  In list i learned about many... <a href=\"http://dev-resources.site/topic/python/2291553/day-03-of-kapils-learning-python-programming-530g\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DDay-03%2520of%2520Kapil%25E2%2580%2599s%2520learning%2520python%2520programming%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2291553%2Fday-03-of-kapils-learning-python-programming-530g\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><p>Buy Me a Coffeeâ˜•  *Memos:    My post explains OxfordIIITPet().   FiveCrop() can crop an image into 5... <a href=\"http://dev-resources.site/topic/python/2291183/fivecrop-in-pytorch-19bi\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DFiveCrop%2520in%2520PyTorch%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fpython%2F2291183%2Ffivecrop-in-pytorch-19bi\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>29. Generate Tailored Cover Letters with AI: A Step-by-Step Guide Using FastAPI and OpenAI</strong></h2><p>In todayâ€™s fast-paced job market, a personalized cover letter can set you apart. ResumeBurgerâ€™s... <a href=\"http://dev-resources.site/topic/python/2291503/generate-tailored-cover-letters-with-ai-a-step-by-step-guide-using-fastapi-and-openai-2584\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvxe5fjbkvr88c2bjcg64.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>30. What is a RESTful API? A Beginnerâ€™s Guide</strong></h2><p>Introduction   In todayâ€™s digital world, applications need to communicate seamlessly with... <a href=\"http://dev-resources.site/topic/python/2289228/what-is-a-restful-api-a-beginners-guide-4hdg\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp6is1aoqdzcqoasceb91.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h3>\n  \n  \n  Earn $100 Fast: AI + Notion Templates\n</h3><p>Do you want to make extra money quickly? This guide shows you how to create and sell Notion templates step by step. Perfect for beginners or anyone looking for an easy way to start earning online.</p><ul><li> Follow a simple process to create templates people want and will buy.</li><li> Learn to use tools like ChatGPT to design and improve templates.</li><li> More people are using Notion every day, and they need templates to save time and stay organized.</li></ul><ul><li> Ready-made prompts to spark ideas and create templates faster.</li><li> Stay on track as you work.</li></ul><ul><li> Learn everything from idea to sale.</li><li><strong>How to Find Popular Ideas:</strong> Research trends and needs.</li><li> Tips for improving templates with AI tools.</li><li><strong>Making Templates User-Friendly:</strong> Simple tips for better design.</li><li> Advice on sharing and selling on platforms like Gumroad or Etsy.</li><li> Solutions for issues like low sales or tricky designs.</li></ul><ul><li>Anyone who wants to make extra money online.</li><li>People who love using Notion and want to share their ideas.</li><li>Creators looking for a simple way to start selling digital products.</li></ul>","contentLength":5290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ğŸš€ Go Interface Nil Pitfall: Why Your Nil Check is Failing (and How to Fix It!) ğŸ”","url":"https://dev.to/mx_tech/go-interface-nil-pitfall-why-your-nil-check-is-failing-and-how-to-fix-it-2ie7","date":1740253407,"author":"Moksh","guid":9284,"unread":true,"content":"<p>Have you ever encountered a situation where you check for nil, but the function still executes unexpectedly? </p><p>This is a common pitfall in Go when working with interfaces and nil values. Let's break it down. ğŸ‘‡</p><p><strong>ğŸ” The Problem: A Failing Nil Check</strong>\nImagine we have an interface Notifier with a method Notify().</p><div><pre><code>package main\n\nimport \"fmt\"\n\n// Define an interface\ntype Notifier interface {\n    Notify()\n}\n\n// Define a struct\ntype Email struct {\n    Address string\n}\n\n// Implement the Notify method for Email\nfunc (e *Email) Notify() {\n    fmt.Println(\"Sending email to:\", e.Address)\n}\n\n// Function that processes Notifier entities\nfunc ProcessNotifiers(notifiers ...Notifier) {\n    for _, notifier := range notifiers {\n        if notifier == nil {\n            fmt.Println(\"Skipping nil notifier\")\n            continue\n        }\n        fmt.Println(\"Processing:\", notifier)\n        notifier.Notify()\n    }\n}\n\nfunc main() {\n    var email *Email  // Declaring a nil pointer of type *Email\n    var notifier Notifier = email // Assigning it to an interface\n\n    fmt.Println(notifier == nil)  // âŒ False! (unexpected)\n\n    ProcessNotifiers(notifier) // Will still call Notify() and panic!\n}\n</code></pre></div><ul><li><p>1ï¸âƒ£ <strong>email is a nil pointer, but notifier is NOT nil!</strong></p><ul><li>When assigning email to notifier, Go stores its type info (*Email) in the interface.</li><li>This means the interface itself is not nil, even though the underlying value is nil.</li></ul></li><li><p>2ï¸âƒ£ <strong>Our if notifier == nil check fails!</strong></p><ul><li>Even though email is nil, notifier still holds a valid interface value, so notifier == nil returns false.</li></ul></li><li><p>3ï¸âƒ£ <strong>Calling notifier.Notify() causes a panic!</strong></p><ul><li>The method is called on a nil receiver, leading to a runtime error.</li></ul></li></ul><p><strong>âœ… The Fix: Proper Nil Checking</strong>\nInstead of checking if notifier == nil, use reflection to properly detect nil interfaces:</p><div><pre><code>import \"reflect\"\n\n// Properly check if an interface is nil\nfunc isNil(i interface{}) bool {\n    if i == nil {\n        return true\n    }\n    v := reflect.ValueOf(i)\n    return v.Kind() == reflect.Ptr &amp;&amp; v.IsNil()\n}\n\nfunc ProcessNotifiersFixed(notifiers ...Notifier) {\n    for _, notifier := range notifiers {\n        if isNil(notifier) {\n            fmt.Println(\"Skipping nil notifier\")\n            continue\n        }\n        fmt.Println(\"Processing:\", notifier)\n        notifier.Notify()\n    }\n}\n</code></pre></div><p><strong>âœ… Now, it properly skips the nil notifier and avoids the panic!</strong></p><p>\n âœ” Interfaces holding nil pointers are NOT nil!<p>\n âœ” Always check for nil pointers inside interfaces properly</p>\n âœ” Use reflection to avoid hidden nil bugs</p><p>Have you run into this before? Let's discuss in the comments! ğŸ’¬ğŸ‘‡</p>","contentLength":2581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Portsicle works!","url":"https://dev.to/amitsuthar69/how-portsicle-works-24k6","date":1740251380,"author":"Amit Suthar","guid":9283,"unread":true,"content":"<p><a href=\"https://portsicle.github.io/landing-page\" rel=\"noopener noreferrer\">Portsicle</a> is a reverse tunneling service that creates a public ingress endpoint for local servers, allowing developers to showcase their locally running applications to clients, stakeholders or to test APIs without deploying.</p><p>We all know that the most obvious way to allow an inbound traffic to a private network is through port forwarding. But we won't sit and configure our routers every time, we need a better solution.</p><p>But if we're not forwarding the port, then how can an external packet enter our private network without getting chocked by firewalls? </p><p>Here's where the concept of <a href=\"https://qbee.io/misc/reverse-ssh-tunneling-the-ultimate-guide/\" rel=\"noopener noreferrer\">Reverse Tunneling</a> comes into the picture. Reverse Tunneling is a technique used to establish a secure connection from a remote server back to a local machine. Portsicle beautifully manipulates this same technique to route traffic to local machine.</p><p>The idea is to have an intermediary remote (relay) server which will act as a bridge between local machine and the internet traffic. Anyone who wants to access someone's local server, will first visit to the remote server, which will then route the request to the local machine.</p><p>To achieve this, Portsicle provides a client CLI which connects to the remote server. The user can use the CLI command to initiate the tunnel. This means that the connection is now outbound (established by the local machine) and the firewall will just ignore it. This connection is then upgraded to a secure and persistent websocket tunnel.</p><p>Once a tunnel is established, the relay server provides a 'Public URL' for that session. Anyone on the internet can now access that particular local server with this public URL and the URL is only valid as long as the client is running along.</p><p>Steps to get the public url:</p><blockquote><p>note that 3000 has to be the port of the local server you wanted to expose.</p></blockquote><ul><li>You'll then receive the URL:\n</li></ul><div><pre><code>â¯ ./portsicle http -p 3000\n2025/02/23 00:26:33 Connected to remote server.\n2025/02/23 00:26:33 Your public url: https://portsicle.koyeb.app/a355bf62-f7c4-46e9-9d9b-1125d6343b3d\n</code></pre></div><p>Your local server is accessible with this link!</p><p>But how the traffic is \"forwarded\"??? Well, we simply convert the HTTP/HTTPS requests and responses into websocket messages and forward them between local client and the remote server.</p><p>We handle this forwarding for two cycles:</p><p>Take plain HTTP request =&gt; Convert it to a message object =&gt; send it across wire =&gt; reconstruct an HTTP request =&gt; send it to local server. </p><p>Client gets a response from local server =&gt; convert it to a response object =&gt; send it across wire =&gt; construct a response.  </p>","contentLength":2538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Estudos em Python - Objeto iterÃ¡vel","url":"https://dev.to/douglasamarelo/estudos-em-python-objeto-iteravel-4a98","date":1740250549,"author":"Douglas \"Amarelo\" Lopes","guid":9267,"unread":true,"content":"<h2>\n  \n  \n  1. Objeto IterÃ¡vel (Iterable)\n</h2><p>Um objeto iterÃ¡vel Ã© qualquer objeto que pode ser percorrido em um loop for ou utilizado com a funÃ§Ã£o . Em Python, para ser considerado iterÃ¡vel, um objeto deve implementar o mÃ©todo especial  ou o mÃ©todo .</p><p>Exemplos comuns de objetos iterÃ¡veis incluem , , ,  e atÃ© arquivos.</p><p><strong>Exemplo de objeto iterÃ¡vel:</strong></p><div><pre><code></code></pre></div><p>Uma lista Ã© um tipo de dado composto que pode armazenar uma coleÃ§Ã£o ordenada de itens. Ela Ã© mutÃ¡vel, ou seja, pode ser alterada apÃ³s sua criaÃ§Ã£o. Pode armazenar elementos de diferentes tipos, como nÃºmeros, strings ou atÃ© outras listas.</p><div><pre><code></code></pre></div><p>Em Python, \"objeto\" Ã© um termo genÃ©rico que se refere a qualquer instÃ¢ncia de uma classe. Tudo em Python Ã© um objeto, e um objeto possui atributos e mÃ©todos definidos pela classe Ã  qual pertence.</p><p>Por exemplo, se vocÃª criar uma classe Pessoa, ao criar uma instÃ¢ncia dessa classe (um objeto), ele terÃ¡ atributos como nome e idade e poderÃ¡ ter mÃ©todos como falar() ou andar().</p><div><pre><code></code></pre></div><p>Em Python, o termo \"Collection\" (coleÃ§Ã£o) Ã© uma forma genÃ©rica de se referir a tipos de dados que armazenam mÃºltiplos itens. Isso inclui listas, tuplas, conjuntos (sets), dicionÃ¡rios, entre outros.</p><p>As coleÃ§Ãµes sÃ£o usadas para armazenar grupos de elementos e podem ser iterÃ¡veis, mutÃ¡veis ou imutÃ¡veis, dependendo do tipo. O mÃ³dulo collections em Python tambÃ©m oferece tipos de dados especializados como deque e Counter.</p><p><strong>Exemplo de coleÃ§Ã£o (lista):</strong></p><div><pre><code></code></pre></div><p>Um iterador Ã© um objeto que permite percorrer um objeto iterÃ¡vel, mas ao contrÃ¡rio do iterÃ¡vel, ele mantÃ©m o estado de onde estÃ¡ no percurso. Ou seja, um iterador sabe onde ele estÃ¡ no processo de iteraÃ§Ã£o e pode continuar de onde parou.</p><p>Um iterador implementa os mÃ©todos () (que retorna o prÃ³prio iterador) e () (que retorna o prÃ³ximo item da coleÃ§Ã£o ou levanta uma exceÃ§Ã£o StopIteration quando nÃ£o hÃ¡ mais itens).</p><div><pre><code></code></pre></div><p>Quando nÃ£o hÃ¡ mais elementos, o next(iterador) levanta uma exceÃ§Ã£o StopIteration.</p><ul><li> Qualquer objeto que pode ser percorrido em um loop (como listas ou tuplas).</li><li> Um tipo de coleÃ§Ã£o que armazena elementos ordenados e mutÃ¡veis.</li><li> Qualquer instÃ¢ncia de uma classe em Python, com atributos e mÃ©todos.</li><li> Estruturas que armazenam mÃºltiplos itens (listas, tuplas, sets, dicionÃ¡rios).</li><li> Um objeto que percorre elementos de um iterÃ¡vel e mantÃ©m o estado da iteraÃ§Ã£o.</li></ul>","contentLength":2333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Loss Functions and NN Parameter Accumulation in micrograd","url":"https://dev.to/shrsv/loss-functions-and-nn-parameter-accumulation-in-micrograd-3c13","date":1740249022,"author":"Shrijith Venkatramana","guid":9266,"unread":true,"content":"<p><em>Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, Iâ€™m building <a href=\"https://hexmos.com/liveapi\" rel=\"noopener noreferrer\">LiveAPI</a>, a tool that makes generating API docs from your code ridiculously easy.</em></p><h2>\n  \n  \n  A Sample Dataset and Comparing With Predictions\n</h2><p>In the following dataset, we have 4 example inputs/outputs. And each input has 3 numbers.</p><p>We use the neural network (MLP object) defined in last post to do some predictions on each input.</p><div><pre><code></code></pre></div><p>When I run this I get some predictions which are either a bit less than required or more:</p><h2>\n  \n  \n  A Skeleton to Build a Loss Function\n</h2><p>We define a loss variable, and define in the following way:</p><ol><li>Find the difference between actual output vs predicted output</li><li>Square the difference (to remove negative sum)</li><li>Sum all such squares of differences for whole input/output set\n</li></ol><div><pre><code></code></pre></div><p>Once we have the loss node, we can run backward propagation to get gradients for each node.</p><p>When we do  we get a huge graph consisting of 4 forward passes corresponding to the 4 examples above and loss on top of those.</p><h2>\n  \n  \n  Gathering All Neural Network Parameters And Operating On Them\n</h2><p>Pay attention to the  method in each of the classes below - where we collect the various parameters at every level (neuron, layer, MLP) for convenience:</p><div><pre><code></code></pre></div><p>Now when I do  I get all the parameters in the neural network:</p>","contentLength":1268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spectacular Connection Between LLMs, Quantum Systems, and Number Theory","url":"https://www.datasciencecentral.com/spectacular-connection-between-llms-quantum-systems-and-number-theory/","date":1740246260,"author":"Vincent Granville","guid":9226,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Day -04 of learning python(02-22-2025)","url":"https://dev.to/kapil_kumarshahsonar_ad/day-04-of-learning-python02-22-2025-1696","date":1740241333,"author":"KAPIL SHAH","guid":9214,"unread":true,"content":"<p>The things i learned form todays python course: </p><p>It is one most important important function used in python programming.</p><p>on going depth i found that it is anonymous function used in code .It is mainly used in on line function for example :</p><div><pre><code>numbers = [1, 2, 3, 4, 5]\nsquared = list(map(lambda x: x ** 2, numbers))\nprint(squared)  # Output: [1, 4, 9, 16, 25]\n</code></pre></div><p>In above example list is given and in that list using  we found the square of the whole list i.e in .</p><p>The basic idea used in above example or anyother example or  program is used like : </p><p><code>lambda(parameter:expression)</code></p><p>or <code>lambda(argumensts:expression)</code></p><p>It helps to iterate every single value present in the list.</p><div><pre><code></code></pre></div><p>In above example map is used for sqauring  of the value in the list .</p><p>Also some basic stuffs like how swap values in the list ,more about IDctionary and all .</p><p>This much only for  today.</p>","contentLength":841,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Data Tables Concept in Flask","url":"https://dev.to/sm0ke/dynamic-data-tables-concept-in-flask-39c7","date":1740240381,"author":"Sm0ke","guid":9213,"unread":true,"content":"<p>This article explains the <a href=\"https://app-generator.dev/blog/dynamic-data-tables-concept-in-flask/\" rel=\"noopener noreferrer\">Dynamic Data Table</a> Pattern and how it can be used to manage information with ease via pagination, search, and filters without any coding effort. For newcomers, Dynamic Programming is a method for solving complex problems by breaking them down into simpler subproblems.</p><p>Let's see how the dynamic programming concept applies to Dynamic Data Tables, which is the solution we're trying to achieve in this article. </p><p>Data Tables represent a structured way of organizing and displaying data in rows and columns with built-in functionality for managing large datasets. Think of it as an enhanced table that provides:</p><ul><li>Breaking down large datasets into smaller, manageable pages</li><li>Allows users to navigate through data without loading everything at once</li><li>Typically shows X records per page (e.g., 10, 25, 50 entries)</li></ul><ul><li>Rows represent individual records</li><li>Columns represent attributes or fields</li><li>Headers define the structure and can enable sorting</li></ul><ul><li>Suitable for small datasets</li><li>All records are pulled once on the client side</li><li>Data is processed locally in the browser or phone application</li></ul><ul><li>Database queries fetch only the required records</li><li>Reduces memory usage and improves performance</li><li>Handles large datasets efficiently</li></ul><p>The goal of our research is to provide the above features out of the box using a minimal configuration and only the dynamic features of Python that can detect and manipulate data at runtime. Let's break down the task into smaller pieces and start applying the dynamic programming principles to our specific use case. </p><p>Users should be able to activate the dynamic pattern for any model using a simple and intuitive syntax. For this, we can use a map that uses the URL as the key, and the target model as value.</p><div><pre><code></code></pre></div><p>The above definition will use the routing \"products\" to build the dynamic table for the Product Model defined in the apps/models.py</p><p>The path for the model is now used to load the model class and analyze the fields and the type. </p><p>The model can be analyzed using the  and  helpers as showcased below:</p><div><pre><code></code></pre></div><p>If we call the  with the  input, we should get the  returned by the  helper.</p><p>Having the class is the first step. Next is to get all fields, the associated types, and also all the foreign keys mapped to external types we don't know yet.   </p><p>The ordinary fields can be pulled from the class definition using this code:</p><div><pre><code></code></pre></div><p>The above snippet will return all fields defined using ordinary types like Integer, String, DateTime, and Text. </p><p>For the Foreign key discovery, the code needs to use the model metadata that saves the distant relationships with other Models. The code that provides all associated FKs is the one below:</p><div><pre><code></code></pre></div><p>After solving the FKs case, the next challenge is to detect the ENUMS used by the model for mapping the information into combos. Without this detection, the ENUMS will be mapped to input fields and we might have issues during creation or update.</p><div><pre><code></code></pre></div><p>The above code snippet iterates on the model fields and saves all values for each field detected as ENUM. </p><p>At this point, we've achieved the following:</p><ul><li>The model is automatically discovered using the URI segment and the configuration mapping</li><li>Fields information (name and type) covers all cases: simple types, ENUMS, foreign keys</li><li>The information is injected into the Jinja files to be presented to the user</li></ul><p>The page provided by the dynamic data table provides the usual controls like search, show hide columns control, filters and export in CSV format.</p><p>The item creation page is displayed with as below:</p><p>For more inputs regarding the concept feel free to contact the team in support or simply download and inspect the code in your local system:</p>","contentLength":3604,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"COMMON MANUAL TESTING TECHNIQUES","url":"https://dev.to/saikiran_r_63247e3b241a8e/common-manual-testing-techniques-1ngh","date":1740240275,"author":"SAIKIRAN R","guid":9212,"unread":true,"content":"<p>Hi everyone we will look at the common manual testing techniques</p><p>The client will approach the service provider with some requirements and a plan. Based on that plan, the frontend and backend development teams will work on creating the screens. After development, the testing team will begin testing the process to ensure it functions smoothly and meets the clientâ€™s requirements.</p><h2>\n  \n  \n  What are the types of testing\n</h2><p>There are 2 types of testing</p><ul></ul><p>Manual testing is the process of manually checking and verifying the functionality of software by using keyboard, mouse and some testing tools</p><p>In business or in a career, there are process steps to follow to achieve success similarly in testing there are different types of testing to followed based on the scenario</p><ol></ol><p>Let me give you a simple example to explain Boundary Value Analysis:</p><p>Imagine you're at a theme park, and there's a roller coaster ride. In the ticket section, there's a condition: the minimum age to get a ticket is 18, and the maximum age is 50.</p><p>In this case, the system should not allow a ticket to be issued to anyone under 18 (age â‰¥ 18) or anyone over 50 (age â‰¤ 50).\nBoundary Value Analysis involves testing the input values at the edges of the allowable range (i.e., at the boundaries), as these are the most likely to cause errors.</p><p>Testing the input values at the allowable range is called boundary value analysis testing.</p><p>In a login page, the user is required to input a username and password. The clientâ€™s requirement is that if the user enters the wrong password three times, the user should be blocked.</p><p>To test this, we will create a table to test different scenarios, such as entering an invalid username or an invalid password. This helps to ensure that the system behaves correctly in all possible situations.</p><h2>\n  \n  \n  Common manual testing techniques\n</h2><p>To test the softwareâ€™s performance under different network conditions.\nThis ensures the system can handle varying loads, network speeds, and other factors that could affect its performance.</p><p>To explore the software and verify that all buttons, features, and functions are working correctly before proceeding to the next testing phase.\nIn exploratory testing, testers interact with the software to identify defects, without following a predefined set of test cases.</p><p>Unit testing is done by developers to test individual components or functions of the software.\nIt ensures that each part of the code works as intended before the software is handed off to the testing team. Sometimes, the testing team may also perform unit testing.</p>","contentLength":2553,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Torvalds: Rust Kernel Code Isn't Forced In Over Maintainers' Objections","url":"https://linux.slashdot.org/story/25/02/22/0524210/torvalds-rust-kernel-code-isnt-forced-in-over-maintainers-objections?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1740239640,"author":"EditorDavid","guid":9185,"unread":true,"content":" Linus Torvalds responded Thursday to kernel developer Christoph Hellwig, who had claimed Torvalds merged Rust code into the kernel even over his objections as the original C code's maintainer. Highlights from Torvalds' response:\n\nThe fact is, the pull request you objected to DID NOT TOUCH THE DMA LAYER AT ALL. It was literally just another user of it, in a completely separate subdirectory, that didn't change the code you maintain in _any_ way, shape, or form... Honestly, what you have been doing is basically saying \"as a DMA maintainer I control what the DMA code is used for\". \n\nAnd that is not how *any* of this works. What's next? Saying that particular drivers can't do DMA, because you don't like that device, and as a DMA maintainer you control who can use the DMA code? That's _literally_ exactly what you are trying to do with the Rust code. You are saying that you disagree with Rust â€” which is fine, nobody has ever required you to write or read Rust code. But then you take that stance to mean that the Rust code cannot even use or interface to code you maintain... \n\nYou don't have to like Rust. You don't have to care about it. That's been made clear pretty much from the very beginning, that nobody is forced to suddenly have to learn a new language, and that people who want to work purely on the C side can very much continue to do so. So to get back to the very core of your statement: \n\n \"The document claims no subsystem is forced to take Rust\" \n\nthat is very much true. You are not forced to take any Rust code, or care about any Rust code in the DMA code. You can ignore it... \n\nYou can't have it both ways. You can't say \"I want to have nothing to do with Rust\", and then in the very next sentence say \"And that means that the Rust code that I will ignore cannot use the C interfaces I maintain\".... So when you change the C interfaces, the Rust people will have to deal with the fallout, and will have to fix the Rust bindings. That's kind of the promise here: there's that \"wall of protection\" around C developers that don't want to deal with Rust issues in the promise that they don't *have* to deal with Rust. \n\nBut that \"wall of protection\" basically goes both ways. If you don't want to deal with the Rust code, you get no *say* on the Rust code. Put another way: the \"nobody is forced to deal with Rust\" does not imply \"everybody is allowed to veto any Rust code\".\n \nTorvalds also made sure to add some kind remarks, including \"I respect you technically, and I like working with you.\"","contentLength":2522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wk 3 Orchestration: MLOPs with DataTalks","url":"https://dev.to/afrologicinsect/wk-3-orchestration-mlops-with-datatalks-2057","date":1740236402,"author":"Akan","guid":9165,"unread":true,"content":"<p>We've been introduced to the subject of Machine Learning in Operations, familarized with Experiment Tracking with MLflow in the past week, now we work with orchestration tools, where we can manage all our processes all the way to deployment of the models - think CI/CD.. Data Engineering</p><p>Here, we use the <a href=\"https://github.com/mage-ai/mage-ai\" rel=\"noopener noreferrer\"></a> orchestration tool.</p><p>This is going to a bit technical, but please follow-through.\nThere are a few ways to set-up Mage, however, the safest is by using the containerized method - through Docker. Remember to create a separate folder for this week's assignment, just to be organized.</p><p>0.1 First launch the <a href=\"https://docs.docker.com/engine/install/\" rel=\"noopener noreferrer\">Docker Engine</a> then from your terminal, or preferably git bash run:<code>docker pull mageai:mageai:latest</code>\nWe would start to see some logs, wait for this to complete - then you can</p><p>0.2 Clone quickstart files</p><div><pre><code>git clone https://github.com/mage-ai/compose-quickstart.git mage-quickstart\ncd mage-quickstart\ncp dev.env .env\nrm dev.env\n</code></pre></div><p>Update your requirements.txt file with these packages:</p><div><pre><code>boto3\nfastparquet\ngraphviz\nhyperopt\njupyter\nmlflow==2.12.1\npandas\nscikit-learn\nseaborn\nshap\nxgboost\n</code></pre></div><p>This sequence of commands is used to set up a local development environment for a new Mage project:</p><ul><li><code>git clone https://github.com/mage-ai/compose-quickstart.git</code>: This command clones the repository from the provided URL into  directory on your local machine.</li><li> navigates into the directory.</li><li>: Copies the  file to a new file named , which is used for environment configuration.</li><li>: Removes the  file, which is no longer needed after copying its contents to .</li></ul><p>Now, we are ready to face the questions</p><p>\n1.1 Run Mage with Docker Compose.<p>\nTo do this all we need to do is run </p> - but not yet.\n1.2 What's the version of Mage we run?<p>\nWe will get this from the page's ui, still, hold on.</p></p><p><strong>Question 2. Creating a project</strong>\n2.1 Create Project Name<p>\nCreate a new project called \"homework_03\" - How many lines are in the created metadata.yaml file?</p></p><p>Yes, so there are many ways to create a project but since we are yet to launch the servers, let's make this by simply tweaking the  variable in the  file - like so:\n2.2 Launch Server: Starts up all the services defined in the  file using Docker Compose. This will build and run the containers necessary for the project.</p><p>We should start to receive some logs indicating that the server is up on , visit the page and we would have version :</p><p>And two new folders storing these actions:</p><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fawdt0ujmg8vkfut4myak.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fawdt0ujmg8vkfut4myak.png\" alt=\"Project Folders\" width=\"233\" height=\"635\"></a>\n2.3 Lines in metadata<p>\nTo get the number of lines in the </p> file simply run:<code>wc -l homework_03/metadata.yaml</code> =&gt; 55 homework_03/metadata.yaml</p><p><strong>Question 3. Creating a pipeline</strong>\n3.1 Create Standard Pipeline<p>\nClick on the Pipeline Icon, start a new </p> Pipeline and fill out the details.</p><p>3.2 Load/Ingest Data takes us to this page:\nSelect  &gt; Python &gt; API, fill out the required details.<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqt76q85718lhymyu1tph.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqt76q85718lhymyu1tph.png\" alt=\"Image description\" width=\"560\" height=\"545\"></a></p><p>We've created our first block! It gets fun from here. We can see some default ingestion code in our block - which we would modify into the this:</p><div><pre><code>import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    taxi_type = \"yellow_tripdata\"\n    year = \"2023\"\n    month = \"03\"\n\n    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_{year}-{month}.parquet'\n    response = requests.get(url)\n\n    ## Data Types\n    taxi_dtypes = {\n        'VendorID': 'Int64',\n        'store_and_fwd_flag': 'str',\n        'RatecodeID': 'Int64',\n        'PULocationID': 'Int64',\n        'DOLocationID': 'Int64',\n        'passenger_count': 'Int64',\n        'trip_distance': 'float64',\n        'fare_amount': 'float64',\n        'extra': 'float64',\n        'mta_tax': 'float64',\n        'tip_amount': 'float64',\n        'tolls_amount': 'float64',\n        'ehail_fee': 'float64',\n        'improvement_surcharge': 'float64',\n        'total_amount': 'float64',\n        'payment_type': 'float64',\n        'trip_type': 'float64',\n        'congestion_surcharge': 'float64'\n    }\n\n    parse_dates_taxi = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']\n\n    df = pd.read_parquet(io.BytesIO(response.content))\n\n    ## Convert data types\n    for col, dtype in taxi_dtypes.items():\n        if col in df.columns:\n            df[col] = df[col].astype(dtype)\n\n    ## Parse Dates\n    for col in parse_dates_taxi:\n        if col in df.columns:\n            df[col] = pd.to_datetime(df[col])\n\n    row_count = df.shape[0]\n    print(f'Total Number of rows retrieved: {row_count}')\n\n    return df\n\n\n@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n</code></pre></div><p>The Python script is designed to load and process data from an API. Here's a breakdown of what it does:</p><ol><li><p><strong>Import necessary libraries</strong>: It imports , , , and some decorators ( and ) from <code>mage_ai.data_preparation.decorators</code>.</p></li><li><p><strong>Define the  function</strong>: This function is decorated with , which suggests that it's used for loading data. The function does the following:</p><ul><li>Constructs a URL to fetch a specific  file from a cloud storage. The file name is constructed using , , and .</li><li>Sends a GET request to the constructed URL and receives the response.</li><li>Defines the data types () for each column in the dataset.</li><li>Defines the columns () that need to be parsed as dates.</li><li>Reads the  file from the response content into a pandas DataFrame ().</li><li>Converts the data types of the columns in the DataFrame as per .</li><li>Parses the date columns in the DataFrame as per .</li><li>Prints the total number of rows retrieved.</li></ul></li><li><p><strong>Define the  function</strong>: This function is decorated with , which is used for testing. The function checks if the output of the block ( function) is not .</p></li></ol><p>Now, click on the  button and wait for the check mark on your trail.</p><p>We added this line <code>print(f'Total Number of rows retrieved: {row_count}')</code> to print out the rows in the data: =&gt; 3403766</p><p><strong>Question 4. Data preparation</strong>\nHere and just like before, we add a _transformer _ block then modify the script like this:</p><div><pre><code>if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    data['duration'] = data.tpep_dropoff_datetime - data.tpep_pickup_datetime\n    data.duration = data.duration.dt.total_seconds() / 60\n\n    data = data[(data.duration &gt;= 1) &amp; (data.duration &lt;= 60)]\n\n    categorical = ['PULocationID', 'DOLocationID']\n    data[categorical] = data[categorical].astype(str)\n\n    row_count = data.shape[0]\n    print(f'Total Number of rows in transformed data: {row_count}')\n\n    return data\n\n\n@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n</code></pre></div><p>This script is designed to transform data, specifically the data loaded from the previous block. Here's a breakdown of what it does:</p><ol><li><p><strong>Import necessary decorators</strong>: It imports  and  decorators from <code>mage_ai.data_preparation.decorators</code> if they are not already in the global scope.</p></li><li><p><strong>Define the  function</strong>: This function is decorated with , which suggests that it's used for transforming data. The function does the following:</p><ul><li>Adds a new column  to the DataFrame . This column is calculated as the difference between  and , converted to minutes.</li><li>Filters the DataFrame to only include rows where  is between 1 and 60 minutes.</li><li>Converts the data types of the columns  and  to string.</li><li>Prints the total number of rows in the transformed data.</li><li>Returns the transformed DataFrame.</li></ul></li><li><p><strong>Define the  function</strong>: This function is decorated with , which suggests that it's used for testing. The function checks if the output of the block (presumably the  function) is not .</p></li></ol><p>The size of the result: 3316216</p><p><strong>Question 5. Train a model</strong>\nYet again we build another  block to train a Regression Model:</p><div><pre><code>## Linear Regression Model\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    dicts_train = data[['PULocationID', 'DOLocationID']].to_dict(orient='records')\n    vec = DictVectorizer(sparse=True)\n    feature_matrix = vec.fit_transform(dicts_train)\n\n    # Use `transform` not `fit_transform` the validation data according to the feature space learned from the training data\n    feature_matrix_val = vec.fit_transform(dicts_train)\n    print(f\"Dimension of feature_matrix: {feature_matrix_val.shape} \\n\")\n\n    ## Target Var\n    y = data['duration']\n\n    ## Fit Linear Regression Model\n    model = LinearRegression()\n    model.fit(feature_matrix, y)\n\n    ## Print Model's intercept\n    intercept = model.intercept_\n    print(f'Linear Regression Model Intercept: {intercept}')\n\n    return model, vec\n\n\n@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n</code></pre></div><p>This Python script is designed to transform data and fit a Linear Regression model:</p><ol><li><p><strong>Import necessary libraries and decorators</strong>: It imports  from <code>sklearn.feature_extraction</code>,  from ,  from , and  and  decorators from <code>mage_ai.data_preparation.decorators</code> if they are not already in the global scope.</p></li><li><p><strong>Define the  function</strong>: This function is decorated with , which suggests that it's used for transforming data. The function does the following:</p><ul><li>Converts the  and  columns of the DataFrame  to a list of dictionaries ().</li><li>Initializes a  () and fits and transforms  into a feature matrix.</li><li>Fits and transforms  again into a validation feature matrix (). Note: This seems to be a mistake. It should be transforming validation data, not the training data again.</li><li>Sets  as the  column of .</li><li>Fits a  model () on the feature matrix and .</li><li>Prints the intercept of the model.</li><li>Returns the model and the vectorizer.</li></ul></li></ol><p>Linear Regression Model Intercept: 24.774803905297286</p><p><strong>Question 6. Register the model</strong>\nHere, we would have to make some configurations to the  file to enable us connect to the MLflow servers - follow the next steps:</p><p>6.1 Stop server\nCtrl + C or :</p><blockquote><p>Container mage-quickstart-magic-1  Stopping</p></blockquote><p>6.2 Create Dockerfile for MLflow:</p><div><pre><code>FROM python:3.10-slim\n\nRUN pip install mlflow==2.12.1\n\nEXPOSE 5000\n\nCMD [ \\\n    \"mlflow\", \"server\", \\\n    \"--backend-store-uri\", \"sqlite:///home/mlflow/mlflow.db\", \\\n    \"--default-artifact-root, ./artifacts\", \\\n    \"--host\", \"0.0.0.0\", \\\n    \"--port\", \"5000\" \\\n]\n</code></pre></div><p>6.2.2 Modify docker-compose yaml\nAs seen on the homework open the  file and make the changes, the file should now look like this:</p><div><pre><code>version: '3'\nservices:\n  magic:\n    image: mageai/mageai:latest\n    command: mage start ${PROJECT_NAME}\n    env_file:\n      - .env\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      USER_CODE_PATH: /home/src/${PROJECT_NAME}\n      ENV: ${ENV}\n    ports:\n      - 6789:6789\n    volumes:\n      - .:/home/src/\n    restart: on-failure:5\n  mlflow:\n    build:\n      context: .\n      dockerfile: mlflow.dockerfile\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - \"${PWD}/mlflow:/home/mlflow/\"\n    networks:\n      - app-network\nnetworks:\n  app-network:\n    driver: bridge\n</code></pre></div><p>6.2.3 Run \nThis will start up the server and set-up your new requirements - it might take a while to complete, check that both <a href=\"http://127.0.0.1:5000/\" rel=\"noopener noreferrer\">MLflow</a> and <a href=\"http://localhost:6789/\" rel=\"noopener noreferrer\">Mage</a> are running. </p><p>NB. For the Author mlflow worked only on  and not on .</p><p>6.3 Data Explorer\nCreate a Generic  and we pull the generated model and vectorizer from the previous  and like the assignment requires push the results to mlflow.</p><p>Paste the following in your exporter block:</p><div><pre><code>import os\nimport pickle\nimport click\nimport mlflow\n\nfrom mlflow.entities import ViewType\nfrom mlflow.tracking import MlflowClient\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nHPO_EXPERIMENT_NAME = \"Linear-Regression\"\nEXPERIMENT_NAME = \"sklearn-track-models\"\n\nmlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\nmlflow.set_experiment(EXPERIMENT_NAME)\nmlflow.sklearn.autolog()\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports data to some source.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Output (optional):\n        Optionally return any object and it'll be logged and\n        displayed when inspecting the block run.\n    \"\"\"\n    # Start an MLflow run\n    with mlflow.start_run():\n\n        # Log the model\n        mlflow.sklearn.log_model(data['model'], \"model\")\n\n        # Save the DictVectorizer as an artifact\n        vec = data['vec']\n        artifact_path = \"dict_vectorizer\"\n        vec_path = os.path.join(artifact_path, \"vec.pkl\")\n        joblib.dump(vec, vec_path)\n        mlflow.log_artifact(vec_path, artifact_path)\n</code></pre></div><p>Now you are on track:<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzw7nk7voplrc9tv5tp15.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzw7nk7voplrc9tv5tp15.png\" alt=\"Exporting to MLflow\" width=\"652\" height=\"859\"></a></p><p>That's it!\nVisit <a href=\"https://github.com/AkanimohOD19A/MLOps_24/tree/main/wk4\" rel=\"noopener noreferrer\">wk3_submission</a> to review the codes and Cheers!\nComment below if there are any issues.</p>","contentLength":14332,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: I Built a Visual Workflow Automation Platform â€“ FlowRipple","url":"https://flowripple.com/","date":1740234004,"author":"shivsarthak34","guid":9228,"unread":true,"content":"<p data-svelte-h=\"svelte-kr32c4\">Watch how easy it is to build powerful automation workflows using our visual builder. Drag, drop, and connect nodes to create your perfect workflow.</p>","contentLength":148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43139138"},{"title":"RandomAutocontrast in PyTorch","url":"https://dev.to/hyperkai/randomautocontrast-in-pytorch-1311","date":1740231852,"author":"Super Kai (Kazuya Ito)","guid":9143,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is inverted or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul></ul></li></ul><div><pre><code></code></pre></div>","contentLength":192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomSolarize in PyTorch","url":"https://dev.to/hyperkai/randomsolarize-in-pytorch-mc5","date":1740231612,"author":"Super Kai (Kazuya Ito)","guid":9142,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomSolarize.html\" rel=\"noopener noreferrer\">RandomSolarize()</a> can randomly solarize an image with a given probability as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type: or ). *All pixels equal or above this value are inverted.</li><li>The 2nd argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is solarized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":419,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomInvert in PyTorch","url":"https://dev.to/hyperkai/randominvert-in-pytorch-40pa","date":1740231293,"author":"Super Kai (Kazuya Ito)","guid":9141,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is inverted or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomAdjustSharpness in PyTorch","url":"https://dev.to/hyperkai/randomadjustsharpness-in-pytorch-dd8","date":1740231083,"author":"Super Kai (Kazuya Ito)","guid":9140,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Required-Type: or ):\n*Memos:\n\n<ul><li> gives a blurred image.</li><li> gives an original image.</li><li> gives a sharpened image.</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is solarized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul></ul></li></ul><div><pre><code></code></pre></div>","contentLength":336,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomPosterize in PyTorch","url":"https://dev.to/hyperkai/randomposterize-in-pytorch-24d2","date":1740230714,"author":"Super Kai (Kazuya Ito)","guid":9139,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomPosterize.html\" rel=\"noopener noreferrer\">RandomPosterize()</a> can randomly posterize an image with a given probability as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type:):\n*Memos:\n\n<ul><li>It's the number of bits to keep for each channel.</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is posterized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Grayscale in PyTorch","url":"https://dev.to/hyperkai/grayscale-in-pytorch-19og","date":1740228310,"author":"Super Kai (Kazuya Ito)","guid":9120,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Grayscale.html\" rel=\"noopener noreferrer\">Grayscale()</a> can convert an image to grayscale as shown below:</p><ul><li>The 1st argument for initialization is (Optional-Default:-Type:). *It must be  or .</li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul></ul></li></ul><div><pre><code></code></pre></div>","contentLength":197,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding the init Function in Go: Purpose, Execution, and Best Practices","url":"https://dev.to/abstractmusa/understanding-the-init-function-in-go-purpose-execution-and-best-practices-2i9k","date":1740227616,"author":"Md Abu Musa","guid":9121,"unread":true,"content":"<p>In Go, the  is a special function that is automatically executed  when a package is initialized. It is primarily used for , such as initializing global variables, opening database connections, or registering dependencies.</p><h3><strong>Key Characteristics of  Function</strong>:\n</h3><ol><li><strong>No Arguments &amp; No Return Value</strong> â€“ The  function does not take parameters or return values.</li><li> â€“ It runs before  and does not require explicit invocation.</li><li><strong>Can Have Multiple  Functions</strong> â€“ A package can have multiple  functions, even across different files.</li><li><strong>Executed in Declaration Order</strong> â€“ If multiple  functions exist in a package, they are executed in the order in which they appear.</li></ol><h3><strong>Example 1: Basic  Usage</strong></h3><div><pre><code></code></pre></div><div><pre><code>Initializing...\nMain function running...\n</code></pre></div><p>âœ… The  function runs .</p><h3><strong>Example 2: Using  for Global Variable Initialization</strong></h3><div><pre><code></code></pre></div><h3><strong>Example 3:  in Multiple Files</strong></h3><p>ğŸ“Œ If a package has multiple files, all  functions run , in the order they appear.</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h4><strong>Output (execution order is preserved):</strong></h4><div><pre><code>Init from a.go\nInit from b.go\nMain function running...\n</code></pre></div><h3><strong>Example 4:  in a Different Package</strong></h3><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h4><strong>Output (Package-Level Execution Order)</strong></h4><div><pre><code>Initializing utils package...\nInitializing main package...\nMain function running...\nHello from utils!\n</code></pre></div><p>âœ… The  function in <strong>imported packages runs before the  function in </strong>.</p><ul><li>Initializing global variables.</li><li>Setting up logging configurations.</li><li>Registering dependencies (e.g., database connections).</li><li>Ensuring required setup before  runs.</li></ul><ul><li>Complex logic (prefer explicit initialization in ).</li><li>Business logic (should be in  or other functions).</li></ul><ul><li> runs <strong>automatically before </strong>.</li><li>It has  and .</li><li>Each package can have multiple  functions.</li><li> in  runs before  in .</li></ul>","contentLength":1588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CenterCrop in PyTorch","url":"https://dev.to/hyperkai/centercrop-in-pytorch-16he","date":1740226178,"author":"Super Kai (Kazuya Ito)","guid":9119,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.CenterCrop.html\" rel=\"noopener noreferrer\">CenterCrop()</a> can crop an image, centering on it as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type: or () or <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.size.html\" rel=\"noopener noreferrer\">size()</a>):\n*Memos:\n\n<ul><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value( or ()) means .</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Resize in PyTorch","url":"https://dev.to/hyperkai/resize-in-pytorch-4nld","date":1740225355,"author":"Super Kai (Kazuya Ito)","guid":9089,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Resize.html\" rel=\"noopener noreferrer\">Resize()</a> can resize an image as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type:, () or <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.size.html\" rel=\"noopener noreferrer\">size()</a>):\n*Memos:\n\n<ul><li> can be explicitly set to it only if  isn't .</li><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value( or ()) is applied to a smaller image's width or height edge, then the other larger width or height edge is also resized:\n*Memos:</li><li>If an image's width is smaller than its height, it's <code>[size * height / width, size]</code>.</li><li>If an image width is larger than its height, it's <code>[size, size * width / height]</code>.</li><li>If an image width is equal to its height, it's .</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:<code>InterpolationMode.BILINEAR</code>-Type:<a href=\"https://pytorch.org/vision/main/_modules/torchvision/transforms/functional.html\" rel=\"noopener noreferrer\">InterpolationMode</a>).</li><li>The 3rd argument for initialization is (Optional-Default:-Type:):\n*Memos:\n\n<ul><li>It's only supported if  is a single value( or ()).</li><li>After  is applied if a larger image's width or height edge exceeds it, it's applied to a larger image's width or height edge to limit the image size, then the other smaller image's width or height edge also becomes smaller than before.</li></ul></li><li>The 4th argument for initialization is (Optional-Default:-Type:). *Even if setting  to it, it's always  if  is <code>InterpolationMode.BILINEAR</code> or <code>InterpolationMode.BICUBIC</code>.</li><li>The 1st argument is (Required-Type: or (, ,  or )):\n*Memos:\n\n<ul></ul></li></ul><div><pre><code></code></pre></div>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"5 Best Programming Languages to Learn: Decoding the Future","url":"https://dev.to/anemnavinrao/5-best-programming-languages-to-learn-decoding-the-future-499l","date":1740225020,"author":"Navin Rao âœï¸","guid":9088,"unread":true,"content":"<p>In today's tech-driven world, staying ahead of the curve is essential. As we approach 2025, certain programming languages are gaining traction, offering exciting opportunities for developers. Whether you're a seasoned coder or just starting, understanding these languages can be a game-changer. So, what's the buzz about the <a href=\"https://questioncage.com/best-programming-languages-to-learn/\" rel=\"noopener noreferrer\">best programming languages to learn</a> in 2025? Let's dive in!</p><h2>\n  \n  \n  1. <a href=\"https://www.python.org/\" rel=\"noopener noreferrer\">Python</a>: The Versatile Vanguard\n</h2><p>\nPython's simplicity and readability have made it a favorite among developers. Its extensive libraries and frameworks make it ideal for web development, data analysis, artificial intelligence, and more. In 2025, Python continues to dominate due to its versatility and the growing demand for AI and machine learning applications.</p><p> Python's straightforward syntax makes it beginner-friendly.</p><p> From NumPy for data science to Django for web development, Python has it all.</p><p> A vast community ensures continuous growth and support.</p><p> Frameworks like Django and Flask streamline the process.</p><p> Libraries such as Pandas and Matplotlib are industry standards.</p><p> Tools like TensorFlow and PyTorch are built on Python.</p><p>\nJavaScript remains the cornerstone of web development. With the rise of frameworks like React and Angular, it's more powerful than ever. In 2025, JavaScript's role in building dynamic and interactive web applications solidifies its position as a must-learn language.</p><p><strong>Asynchronous Programming:</strong> Handles multiple tasks efficiently.</p><p> Ideal for interactive web pages.</p><p> Can be used on both client and server sides.</p><p> Frameworks like React and Angular enhance user interfaces.</p><p> Node.js allows for server-side scripting.</p><p> Tools like React Native enable cross-platform apps.</p><p>\nDeveloped by Google, Go is known for its efficiency and scalability. It's particularly suited for cloud services and microservices architectures. In 2025, Go's performance and simplicity make it a top choice for developers aiming for high-performance applications.</p><p> Built-in support for concurrent programming.</p><p> Minimalistic design for easy readability.</p><p> Compiled language with fast execution.</p><p> Ideal for building scalable cloud applications.</p><p> Efficient for developing microservices architectures.</p><p>Command-Line Tools: Great for creating fast and reliable CLI tools.</p><h2>\n  \n  \n  4. <a href=\"https://www.rust-lang.org/\" rel=\"noopener noreferrer\">Rust</a>: The Safe Systems Language\n</h2><p>\nRust offers memory safety without sacrificing performance. It's gaining popularity for system-level programming and is being adopted by major tech companies. In 2025, Rust's focus on safety and performance makes it a compelling choice for developers.</p><p> Prevents common bugs like null pointer dereferencing.</p><p> Safe concurrency without data races.</p><p> Comparable to C and C++ in speed.</p><p> Ideal for operating systems and embedded systems.</p><p> Used for high-performance web applications.</p><p> Popular in blockchain development for its safety features.</p><h2>\n  \n  \n  5. <a href=\"https://kotlinlang.org/\" rel=\"noopener noreferrer\">Kotlin</a>: The Modern Java Alternative\n</h2><p>\nKotlin is a modern, expressive language that runs on the Java Virtual Machine (JVM). It's officially supported for Android development and is gaining traction in other areas. In 2025, Kotlin's concise syntax and interoperability with Java make it a top choice for developers.</p><p> Reduces boilerplate code.</p><p> Seamless integration with Java.</p><p> Prevents null pointer exceptions.</p><p> Officially supported for Android apps.</p><p> Frameworks like Ktor enable server-side development.</p><p> Can be used with JavaFX for desktop apps.</p><h2>\n  \n  \n  6. <a href=\"https://en.wikipedia.org/wiki/C%2B%2B\" rel=\"noopener noreferrer\">C++</a>: The High-Performance Heavyweight\n</h2><p>C++ offers unparalleled performance, making it suitable for system programming and applications requiring high efficiency. \nPLURALSIGHT.COM</p><p> Direct access to hardware resources.</p><p> Optimized for speed and efficiency.</p><p> Supports object-oriented programming principles.</p><p> Used in developing high-performance games.</p><p> Ideal for programming microcontrollers.</p><p> Employed in high-frequency trading platforms.</p><p>Choosing the right programming language in 2025 depends on your career goals and interests. Python's versatility, JavaScript's web dominance, Java's enterprise strength, C++'s performance, and Go's cloud capabilities each offer unique advantages. Assess your aspirations and the industry demands to make an informed decision. Remember, the best language to learn is one that aligns with your passion and career objectives.</p><h2>\n  \n  \n  Frequently Asked Questions (FAQs)\n</h2><p><strong>1. Which programming language should I learn first in 2025?</strong>\nIf you're new to programming, Python is an excellent starting point due to its simplicity and wide range of applications. </p><p><strong>2. Is JavaScript still relevant in 2025?</strong>\nAbsolutely! JavaScript remains essential for web development, powering interactive websites and applications. </p><p><strong>3. What are the career prospects for learning C++?</strong>\nC++ is highly valued in fields like game development, embedded systems, and high</p>","contentLength":4736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomPerspective in PyTorch","url":"https://dev.to/hyperkai/randomperspective-in-pytorch-3i1g","date":1740224609,"author":"Super Kai (Kazuya Ito)","guid":9087,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It can do perspective transformation.</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is done with perspective transformation or not.</li></ul></li><li>The 3rd argument for initialization is (Optional-Default:<code>InterpolationMode.BILINEAR</code>-Type:<a href=\"https://pytorch.org/vision/main/_modules/torchvision/transforms/functional.html\" rel=\"noopener noreferrer\">InterpolationMode</a>).</li><li>The 4th argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can change the background of an image. *The background can be seen when doing perspective transformation for an image.</li><li>A tuple/list must be the 1D with 1 or 3 elements.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":728,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ensuring Interface Implementation at Compile Time in Go ğŸ› ï¸","url":"https://dev.to/abgeo/ensuring-interface-implementation-at-compile-time-in-go-3366","date":1740221032,"author":"Temuri Takalandze","guid":9061,"unread":true,"content":"<p>In Go, we canâ€™t implicitly implement interfaces like in some other languages. To ensure a type implements an interface, we explicitly check this by trying to cast the type to the interface. If the type doesnâ€™t match, Go will give a compile-time error, and the code wonâ€™t compile ğŸš«</p><p>This compile-time check helps catch errors early, making sure your types conform to the expected interfaces. If Task doesnâ€™t implement Executor properly, Go wonâ€™t compile the code, saving you from potential issues at runtime ğŸ‘¨â€ğŸ’»</p>","contentLength":529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Protobuf Should Dominate the Data Format Ecosystem","url":"https://dev.to/leapcell/why-protobuf-should-dominate-the-data-format-ecosystem-4ddd","date":1740217327,"author":"Leapcell","guid":9047,"unread":true,"content":"<p>Protobuf (Google Protocol Buffers), as defined in the official documentation: Protocol buffers is a language-independent, platform-independent, and extensible method for serializing structured data, which can be widely applied in scenarios such as data communication protocols and data storage. It is a tool library provided by Google with an efficient protocol data exchange format, possessing the characteristics of flexible, efficient, and automated structured data serialization mechanisms.</p><p>Compared with XML, the size of data encoded by Protobuf is smaller, and the encoding and decoding speed is faster. Compared with Json, Protobuf performs more excellently in conversion efficiency, with both its time efficiency and space efficiency reaching 3 to 5 times that of JSON.</p><p>As the official description states: â€œProtocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data â€“ think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.â€</p><h2>\n  \n  \n  Comparison of Data Formats\n</h2><p>Suppose we have a  object, represented by JSON, XML, and Protobuf respectively, and let's see their differences.</p><div><pre><code>John24</code></pre></div><p>Protobuf directly represents data in binary format, which is not as intuitive as XML and JSON formats. For example:</p><div><pre><code>[10 6 69 108 108 122 111 116 16 24]\n</code></pre></div><h3>\n  \n  \n  Good Performance/High Efficiency\n</h3><ul><li>: The overhead of XML formatting (serialization) is acceptable, but the overhead of XML parsing (deserialization) is relatively large. Protobuf has optimized this aspect and can significantly reduce the time overhead of serialization and deserialization.</li><li>: Protobuf also greatly reduces the space occupation.</li></ul><h3>\n  \n  \n  Code Generation Mechanism\n</h3><p>For example, write the following content similar to a structure:</p><div><pre><code></code></pre></div><p>Protobuf can automatically generate the corresponding  file and  file, and encapsulate the operations on the structure  into a class.</p><h3>\n  \n  \n  Support for Backward Compatibility and Forward Compatibility\n</h3><p>When the client and the server use a protocol simultaneously, if the client adds a byte in the protocol, it will not affect the normal use of the client.</p><h3>\n  \n  \n  Support for Multiple Programming Languages\n</h3><p>In the source code officially released by Google, it includes support for multiple programming languages, such as:</p><ul></ul><h2>\n  \n  \n  Disadvantages of Protobuf\n</h2><h3>\n  \n  \n  Poor Readability Due to Binary Format\n</h3><p>To improve performance, Protobuf uses a binary format for encoding, which makes the data less readable and will affect the efficiency during the development and testing phase. However, under normal circumstances, Protobuf performs very reliably, and serious problems generally do not occur.</p><p>Generally, XML is self-descriptive, while the Protobuf format is not. It is a piece of binary format protocol content, and it is difficult to know its function without matching it with a pre-written structure.</p><p>Although Protobuf supports serialization and deserialization in multiple languages, it is not a universal transmission standard across platforms and languages. In scenarios of multi-platform message passing, its compatibility with other projects is not good, and corresponding adaptation and transformation work is often required. Compared with json and XML, its universality is slightly insufficient.</p><p>Proto message type files generally end with . In a  file, one or more message types can be defined.</p><p>The following is an example of defining a message type for a search query. The  at the beginning of the file is used to describe the version information. Currently, there are two versions of proto, proto2 and proto3.</p><p>Explicitly set the syntax format to proto3. If the  is not set, it defaults to proto2.  represents the content to be queried,  represents the page number of the query, and  represents the number of items per page.  must be located on the first line of the  file excluding comments and blank lines.</p><p>The following message contains 3 fields (, , ), and each field has a corresponding type, field name, and field number. The field type can be , , , or a composite type.</p><div><pre><code></code></pre></div><p>Each field in the message type needs to be defined with a unique number, and this number is used to identify the field in the binary data. Numbers in the range of [1,15] can be encoded and represented with one byte; in the range of [16,2047], they need to be encoded and represented with two bytes. Therefore, leaving the numbers within 15 for frequently occurring fields can save space. The minimum value of the number is 1, and the maximum value is 2^29 - 1 = 536870911. Numbers in the range of [19000, 19999] cannot be used because these numbers are used internally by the proto compiler. Similarly, other pre-reserved numbers cannot be used either.</p><p>Each field can be modified by  or . In the proto3 syntax, if the modification type is not specified, the default value is .</p><ul><li>: It means that the modified field appears at most once, that is, it appears 0 or 1 time.</li><li>: It means that the modified field can appear any number of times, including 0 times. In the proto3 syntax, fields modified by  use the  encoding by default.</li></ul><p>You can add comments to the  file. The comment syntax is the same as the C/C++ style, using  or .</p><div><pre><code></code></pre></div><p>When deleting or commenting out a field in a , other developers in the future may reuse the previous field number when updating the  definition. If they accidentally load the old version of the  file, it may lead to serious problems, such as data corruption. To avoid such problems, you can specify the reserved field numbers and field names. If someone uses these field numbers in the future, an error will be generated when compiling the proto, thus reminding that there is a problem with the proto.</p><p>Note: Do not mix the use of field names and field numbers for the same field.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Mapping between Field Types and Language Types\n</h3><p>The defined  file can generate Go language code through a generator. For example, the Go file generated from the  file is the  file.</p><p>The mapping between basic types in proto and Go language types is shown in the following table (here only the type mapping between Go and C/C++ is listed, and for other languages, refer to <a href=\"https://developers.google.com/protocol-buffers/docs/proto3):\" rel=\"noopener noreferrer\">https://developers.google.com/protocol-buffers/docs/proto3):</a>\n|.proto Type | Go Type | C++ Type |\n| double | float64 | double |<p>\n| float | float32 | float |</p>\n| int32 | int32 | int32 |<p>\n| int64 | int64 | int64 |</p>\n| uint32 | uint32 | uint32 |<p>\n| uint64 | uint64 | uint64 |</p>\n| sint32 | int32 | int32 |<p>\n| sint64 | int64 | int64 |</p>\n| fixed32 | uint32 | uint32 |<p>\n| fixed64 | uint64 | uint64 |</p>\n| sfixed32 | int32 | int32 |<p>\n| sfixed64 | int64 | int64 |</p>\n| bool | bool | bool |<p>\n| string | string | string |</p>\n| bytes | []byte | string |</p><div><table><tbody><tr></tr></tbody></table></div><p>When defining a message, if you want the value of a field to be only one of the expected values, you can use the enum type.</p><p>For example, now add the  field to , and its value can only be one of , , , , , , and . This can be achieved by adding an enum to the message definition and adding a constant for each possible enum value.</p><div><pre><code></code></pre></div><p>The first constant of the  enum must be mapped to 0, and all enum definitions need to include a constant mapped to 0, and this value is the first line content of the enum definition. This is because 0 is used as the default value of the enum. In the proto2 syntax, the enum value on the first line is always the default value. For the sake of compatibility, the value 0 must be the first line of the definition.</p><p>Other  files can be imported in a  file, so as to use the message types defined in the imported file.</p><div><pre><code></code></pre></div><p>By default, only the message types defined in the directly imported  file can be used. But sometimes it may be necessary to move the  file to a new location. At this time, a virtual  file can be placed in the old location, and the  syntax can be used to forward all imports to the new location, instead of directly moving the  file and updating all call points at once. Any place that imports a proto file containing the  statement can pass on the public dependencies of the imported dependencies.</p><p>For example, there are  and  files in the current folder, and  is imported in the  file, that is, the  file has the following content:</p><p>Suppose now we want to put the messages in  into the  file for use in other places. We can modify  and import  in it. Note that we need to use  because a single  can only use the messages defined in  and cannot use the message types in the proto file imported in .</p><div><pre><code></code></pre></div><p>When using  for compilation, the option  or  needs to be used to notify  where to find the imported files. If the search path is not specified,  will look for it in the current directory (the path where  is called).</p><p>Message types in the proto2 version can be imported into a proto3 file for use, and message types in the proto3 version can also be imported into a proto2 file. But the enum types in proto2 cannot be directly applied to the proto3 syntax.</p><p>Message types can be defined inside another message type, that is, nested definitions. For example, the  type is defined inside , and it supports multiple levels of nesting.</p><div><pre><code></code></pre></div><p>When an outer message type uses a message inside another message, such as the  type using , it can use .</p><div><pre><code></code></pre></div><p>Unknown fields are fields that the proto compiler cannot recognize. For example, when an old binary file parses the data sent by a new binary file with new fields, these new fields will become unknown fields in the old binary file. In the initial version of proto3, unknown fields were discarded when the message was parsed, but in version 3.5, the retention of unknown fields was reintroduced. Unknown fields are retained during parsing and are included in the serialized output.</p><p>The key to the high efficiency of Protobuf lies in its TLV (tag-length-value) encoding format. Each field has a unique  value as an identifier,  represents the length of the  data (for a  with a fixed length, there is no ), and  is the content of the data itself.</p><p>For the  value, it is composed of two parts:  and .  is the number given to each field in the  earlier, and  represents the type (fixed length or variable length). The  currently has 6 values from 0 to 5, and these 6 values can be represented by 3 bits.</p><p>The values of  are shown in the following table, where 3 and 4 have been deprecated, and we only need to pay attention to the remaining 4 types. For data encoded with Varint, there is no need to store the byte length , and at this time, the TLV encoding format degenerates into TV encoding. For 64-bit and 32-bit data, there is also no need for  because the  value already indicates whether the length is 8 bytes or 4 bytes.</p><div><table><thead><tr></tr></thead><tbody><tr><td>int32 int64 uint32 uint64 bool enum</td></tr><tr></tr><tr></tr><tr><td>string bytes packed repeated fields embedded</td></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><h3>\n  \n  \n  Varint Encoding Principle\n</h3><p>Varint is a variable-length int, which is a variable-length encoding method. It can make smaller numbers use fewer bytes to represent, and achieve data compression by reducing the number of bytes used to represent numbers. For an int32 type number, it usually requires 4 bytes to represent, but with Varint encoding, an int32 type number less than 128 can be represented with 1 byte. For larger numbers, it may require 5 bytes to represent, but in most messages, very large numbers usually do not appear, so using Varint encoding can use fewer bytes to represent numbers.</p><p>Varint is a variable-length encoding, and it distinguishes each field through the highest bit of each byte. If the highest bit of a byte is 1, it means that the subsequent byte is also part of the number; if it is 0, it means that this is the last byte, and the remaining 7 bits are all used to represent the number. Although each byte will waste 1 bit of space (that is, 1/8 = 12.5% waste), if there are many numbers that do not need to be fixed as 4 bytes for representation, a large amount of space can still be saved.</p><p>For example, for an int32 type number 65, its Varint encoding process is as follows, and the 65 that originally occupied 4 bytes only occupies 1 byte after encoding.</p><p>For an int32 type number 128, it occupies 2 bytes after encoding.</p><p>Varint decoding is the reverse process of encoding, which is relatively simple, and no example is given here.</p><p>numbers to unsigned numbers, and then use Varint encoding to reduce the number of bytes after encoding.</p><p>Zigzag uses unsigned numbers to represent signed numbers, enabling numbers with smaller absolute values to be represented with fewer bytes. Before understanding Zigzag encoding, let's first understand a few concepts:</p><ul><li>: The highest bit is the sign bit, and the remaining bits represent the absolute value.</li><li>: Except for the sign bit, invert the remaining bits of the original code one by one.</li><li>: For positive numbers, the two's complement is itself; for negative numbers, except for the sign bit, invert the remaining bits of the original code one by one and then add 1.</li></ul><p>Take the int32 type number -2 as an example, and its encoding process is as follows.</p><p>In summary, for negative numbers, perform arithmetic operations on their two's complement. For a number , if it is of the  type, perform the operation ; if it is of the  type, perform the operation . Through this operation, the negative number is changed to a positive number, and this process is Zigzag encoding. Finally, use Varint encoding.</p><p>Since Varint and Zigzag encoding can self-parse the content length, the length item can be omitted, and the TLV storage is simplified to TV storage, without the need for the  item.</p><h3>\n  \n  \n  Calculation Methods of tag and value Values\n</h3><p>The  stores the identification information and data type information of the field, that is,  (field data type) +  (identification number). The field number can be obtained through the , corresponding to the defined message field. The calculation formula is <code>tag = field_number&lt;&lt;3 | wire_type</code>, and then perform Varint encoding on it.</p><p>The  is the value of the message field after Varint and Zigzag encoding.</p><h3>\n  \n  \n  string Encoding (continued)\n</h3><p>When the field type is the  type, the field value is encoded in UTF-8. For example, there is the following message definition:</p><div><pre><code></code></pre></div><p>In the Go language, the sample code for encoding this message is as follows:</p><div><pre><code></code></pre></div><p>The binary content after encoding is as follows:</p><div><pre><code>[10 14 67 104 105 110 97 228 184 173 144 155 189 228 120 186]\n</code></pre></div><p>Nested messages mean that the  is another field message. The outer message is stored using TLV storage, and its  is also a TLV storage structure. The schematic diagram of the entire encoding structure is as follows (it can be imagined as a tree structure, where the outer message is the root node, and the nested message inside it is used as a child node, and each node follows the TLV encoding rule):</p><ol><li>The outermost message has its corresponding ,  (if any), and .</li><li>When the  is a nested message, this nested message has its own independent ,  (if any), and .</li><li>By analogy, if there are nested messages within the nested message, continue to encode according to the TLV rule.</li></ol><h3>\n  \n  \n  repeated Fields with packed\n</h3><p>The fields modified by  can be with  or without it. For multiple field values of the same  field, their  values are all the same, that is, the data type and field sequence number are the same. If multiple  storages are used, there will be redundancy of the .</p><p>If  is set, the storage method of the  field will be optimized. That is, the same  is only stored once, and then the total length  of all values under the  field is added to form a  storage structure. This method can effectively compress the length of the serialized data and save transmission overhead. For example:</p><div><pre><code></code></pre></div><p>In the above example, the  field does not use , and each  value will have independent  and  storage; while the  field uses , and the  will only be stored once, followed by the total length  of all  values, and then all  values are arranged in sequence. In this way, when the data volume is large, the  field using  can significantly reduce the space occupied by the data and the bandwidth consumption during transmission. </p><p>With its efficiency (in terms of size) and professionalism (professional types), Protobuf should have a higher coverage in the future data transmission field.</p><p>Finally, I would like to introduce to you the most suitable platform for deploying services: </p><h3>\n  \n  \n  1. Multi-Language Support\n</h3><ul><li>Develop with JavaScript, Python, Go, or Rust.\n</li></ul><h3>\n  \n  \n  2. Deploy unlimited projects for free\n</h3><ul><li>pay only for usage â€” no requests, no charges.</li></ul><h3>\n  \n  \n  3. Unbeatable Cost Efficiency\n</h3><ul><li>Pay-as-you-go with no idle charges.\n</li><li>Example: $25 supports 6.94M requests at a 60ms average response time.\n</li></ul><h3>\n  \n  \n  4. Streamlined Developer Experience\n</h3><ul><li>Intuitive UI for effortless setup.\n</li><li>Fully automated CI/CD pipelines and GitOps integration.\n</li><li>Real-time metrics and logging for actionable insights.\n</li></ul><h3>\n  \n  \n  5. Effortless Scalability and High Performance\n</h3><ul><li>Auto-scaling to handle high concurrency with ease.\n</li><li>Zero operational overhead â€” just focus on building.\n</li></ul>","contentLength":16965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feature Flag Service: Experimenting with New Technologies and Architectures","url":"https://dev.to/palma99/feature-flag-service-experimenting-with-new-technologies-and-architectures-176p","date":1740217014,"author":"Palma99","guid":9046,"unread":true,"content":"<p>I am a junior frontend developer with two years of experience working with Vue.js, I wanted to broaden my knowledge by exploring backend development and experimenting with other frontend frameworks. I decided to create a project from scratch using Go for the backend and Angular 19 for the frontend. The project is a service for managing feature flags, inspired by great existing solutions. My primary focus was on implementing the principles of Clean Architecture while also improving my SQL skills by working directly with PostgreSQL without an ORM.</p><p>The source code can be found here:</p><p>The idea was to create a dashboard where users can sign in and manage their projects, environments and flags. Then, project's specific flags can be retrieved by user's app using a public api. The first thing that i wasn't sure about is how to implement this public interaction in a secure way, we will dive into that later, but the idea was to create some sort of library that handle this communication using a public key.</p><p>For the dashboard, I aimed to create an intuitive and simple UI for basic operations like creating new projects and flags. My main goal was to clearly display the status of the flags in each environment. This was the initial sketch of the UI.</p><p>I also wanted to implement collaboration, allowing multiple users to access the same project. This necessitates the implementation of a role/permission system, where, for example, only the owner can delete a project.</p><h2>\n  \n  \n  First step: Designing the database\n</h2><p>Let's dive into some technical details, starting with database design. I wanted to use a relational database, but at this stage, I didnâ€™t really care which one to choose. So, as a first step, I started thinking about all the entities and the relationships between them:</p><ul><li>: a person that can sign up and have access to the dashboard</li><li>: represent a container of environment and flags</li><li>: each project can have one or more environments, this is useful for a real world project where multiple test environment can exist.</li><li>: represent a single feature that can be enabled or not</li></ul><p>I started creating many-to-many relationship between users and project, so we can easily implement collaboration as mentioned before.\nThe one-to-many relationship between  and  it's pretty straight forward as one project can have multiple environments.</p><p>Nothing special so far, but things started to get tricky when I encountered the flag table. </p><p>The first idea that came to mind was that each flag should be directly related to an environment. This seemed logical, given that weâ€™re working in a multi-environment system where the end user needs to request flags for a specific environment. However, I quickly realized that this could introduce some introduce some complexities.</p><p>Imagine you're inside the dashboard for a specific project that has 3 environments (TEST, QA, PROD), and you want to create a new flag called 'dark_mode_experimental'. This means you would need to create 3 new rows in the  table, one for each environment. Then, if you want to update the flag name to 'dark_mode', you would need to update all the rows accordingly. The same applies if you want to delete a flag. Furthermore, if a project already has some flags and you want to add another environment (let's say 'DEV'), you would need to duplicate all the existing flags for this new environment, which can lead to increased complexity and potential maintenance issues.</p><p>The final solution is that flags belong to a project while maintaining a many-to-many relationship with environments through the  table. This table allows us to store the flag status for each specific environment. By doing so, we can easily manage flag updates and deletions without needing to duplicate or update rows across multiple environments.</p><p>What happens when a new environment is created within a project is quite simple: we just add a new row in the  table. The relations in the  table are not created immediately but only after a flag is updated. This ensures that we avoid unnecessary entries for environments where no flags have been modified yet. Once a flag is updated, the corresponding relation in the flag_environment table is established, allowing us to track the flag's status in the new environment. </p><h2>\n  \n  \n  Step two: Implementing Go backend\n</h2><p>As I mentioned, I'm more of a frontend person, but I have a strong interest in learning backend technologies. During my studies, I worked with several different languages, mostly C, Java, and Python. These are great languages, but I wanted to try something new. Recently, I heard a lot of positive things about Go, so I decided to give it a shot.</p><p>There are a couple of things I want to mention before diving into the code. My main goal here was to structure the code by following clean architecture principles. This approach helps ensure that the code remains modular, maintainable, and testable. Additionally, for handling data, I chose not to use any ORM because I believe that for a study project, writing raw SQL is more instructive and provides a deeper understanding of how data is managed at the database level.</p><p>Let's start with folder structure</p><p>In Go, it's common to have a folder called  that contains the entry point of the program, and a folder called  for all the application code. That's what I did â€” I created two subfolders inside : one for the API version and one for the CLI version of the service.</p><p>There are some great articles about clean architecture, and the structure I implemented is quite standard, so I won't go into detail about what each folder represents. I just want to highlight some parts of the code that demonstrate how the principles are applied and how the different components interact with each other in the project.</p><p>As I mentioned, I'm new to the Go world and still getting familiar with the ecosystem. From what I've seen so far, there aren't any major frameworks like those in the .NET or Java ecosystems that handle dependency injection in such a clean way. Therefore, I decided to implement dependency injection in a very 'vanilla' way, without relying on any external frameworks. Here is an example</p><div><pre><code></code></pre></div><p>An interactor is a struct with some dependencies that has methods to fulfill some business use cases, e.g.</p><div><pre><code></code></pre></div><p>There are several aspects that can be improved, such as error handling or using a factory to create entities. However, the key idea is that the 'Create Environment' use case is managed within this method, which does not rely on any concrete implementation.</p><p>In this project, I implemented anemic entities, which is not ideal. For example, all fields are public so they can be serialized using Go's standard library, although I'm not fully convinced this is the best approach.</p><div><pre><code></code></pre></div><p>The infrastructure folder contains all the implementations related to external dependencies. In this case, it includes the repository implementation for PostgreSQL and the middleware used by the HTTP framework.</p><p>I decided to use PostgreSQL as the database for no particular reason (well, maybe because I already had a Docker image pulled). However, thanks to clean architecture, it's easy to swap the database by simply implementing a new repository that adheres to the same interface. For example:</p><div><pre><code></code></pre></div><p>In this case, interfaces refer to the components that allow external systems to interact with the application. For a web API, this typically consists of a function that, for example, extracts request parameters or body data, creates a DTO, and calls the interactor.</p><div><pre><code></code></pre></div><h4>\n  \n  \n  Authentication for the admin section\n</h4><p>When dealing with authentication in the real world, it's better to relay on well tested library to improve security. However this for this simple project i decided to implement a username and password authentication that works with JWT tokens from scratch, using this popular library 'github.com/golang-jwt/jwt/v5'.</p><h4>\n  \n  \n  Implementation of the \"public api\"\n</h4><p>To allow the end user to access flags for a certain environment, I decided to implement a public key pattern. Essentially, there is a public REST API protected by a middleware that checks for a specific header containing a key. If the key is valid, it grants read-only access to all the enabled flags for that environment. Even if the public key gets stolen, the impact is limited. Since it only grants read-only, the key cannot be used to modify any data or access sensitive information.</p><p>That's how i implemented the middleware</p><div><pre><code></code></pre></div><p>Then in the route definition</p><div><pre><code></code></pre></div><p>To interact with the public API I wrote a simple typescript SDK that allow to easily communicate with the service. It provides basic caching and types, and can be used by any javascript app.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Admin dashboard in Angular 19\n</h3><p>Once I completed the backend, I wanted to interact with my service through a comfortable UI. Angular 19 had just come out with stable signals and other cool features, so I decided to use it for the frontend. I didnâ€™t want to spend too much time designing UI components, hence I decided to use a component library that provides pre-built, customizable components. This allowed me to focus more on the core functionality and user experience. The library i choose is <a href=\"https://taiga-ui.dev\" rel=\"noopener noreferrer\">Taiga UI</a>.</p><p>The frontend is quite simple, a bit different from the initial sketch but it provides all the features i need.</p><p>List of user projects<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frb0w8emw1cc77mmswnh9.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frb0w8emw1cc77mmswnh9.png\" alt=\"Ui project list\" width=\"800\" height=\"477\"></a></p><p>An interesting feature I added consists of a section where you can test the payload of the public api for the environment. It works by making an api call to public endpoint using the environment key of the selected environment, and then it shows the response.</p><ul><li>Unit testing: Clean architecture is great and makes things easier to test, but I havenâ€™t written a single test for my business logic yet. This is definitely something to focus on moving forward.</li><li>Collaboration is supported, but there is currently no way to \"invite\" someone to join a project (just manually on db). In the future, it would be cool to implement an invitation system, allowing users to easily add collaborators and manage team access.</li></ul>","contentLength":9963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ğŸš€ Day 3 of #100DaysOfCoding","url":"https://dev.to/xscoox_ca5e58c796032a1802/day-3-of-100daysofcoding-1008","date":1740213846,"author":"xscoox","guid":9033,"unread":true,"content":"<p>Today, I dived into Linked Lists and learned the basics:\nâœ… Insertion\nâœ… Deletion<p>\nâœ… Understanding how pointers work</p></p><p>Excited to learn more complex data structures next! ğŸ’¡</p>","contentLength":175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a REST API with Django REST Framework: A Beginners Guide","url":"https://dev.to/kihuni/building-a-rest-api-with-django-rest-framework-a-beginners-guide-1b1n","date":1740204280,"author":"kihuni","guid":8988,"unread":true,"content":"<p>Imagine you're building a book management system where users can browse available books and add new ones. To make this possible, we need a REST API to handle book data efficiently.</p><p>REST APIs are the backbone of modern web applications, enabling seamless communication between frontend and backend systems. Django REST Framework (DRF) simplifies API development by providing a powerful toolkit built on top of Django. In this guide, we'll build a simple REST API to manage books.</p><p>Before starting this tutorial, you should have:</p><p>âœ… Basic understanding of Python programming\nâœ… Familiarity with web development concepts<p>\nâœ… Python 3.8+ installed on your system</p>\nâœ… Basic knowledge of the Django framework<p>\nâœ… Understanding of HTTP methods (GET, POST, PUT, DELETE)</p>\nâœ… Experience using the command-line interface</p><p>By the end of this tutorial, you will be able to:</p><p>âœ… Set up a Django REST Framework project from scratch\nâœ… Create API endpoints using function-based views (FBVs) and class-based views (CBVs)<p>\nâœ… Implement model serialization for JSON responses</p>\nâœ… Perform CRUD operations via a REST API<p>\nâœ… Test API endpoints using browsable API and cURL commands</p></p><h3>\n  \n  \n  Why Django REST Framework?\n</h3><p>Django REST Framework has become the go-to choice for building APIs with Django because it offers:</p><p>âœ”ï¸ Powerful serialization capabilities\nâœ”ï¸ Built-in authentication and permissions<p>\nâœ”ï¸ Browsable API interface for easy testing</p>\nâœ”ï¸ Extensive documentation and strong community support<p>\nâœ”ï¸ Flexible request/response handling</p>\nâœ”ï¸ Support for pagination, filtering, and throttling</p><p>Letâ€™s start building our REST API step by step.</p><p>Step 1: Environment Setup</p><p>First, create a clean development environment:</p><div><pre><code># Create a project folder\nmkdir book-api &amp;&amp; cd book-api\n\n# Create a virtual environment\npython -m venv venv\n\n# Activate the virtual environment (Windows)\nvenv\\Scripts\\activate\n\n# Activate the virtual environment (Mac/Linux)\nsource venv/bin/activate\n\n# Install Django and Django REST Framework\npip install django djangorestframework\n\n</code></pre></div><p>Now, create a Django project and an app for managing books.</p><div><pre><code># Create a Django project\ndjango-admin startproject bookapi\n\ncd bookapi\n\n# Create a Django app\npython manage.py startapp books\n\n</code></pre></div><p>Register Django REST Framework and the books app in settings.py:</p><div><pre><code>INSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n\n    # Third-party apps\n    'rest_framework',\n\n    # Local apps\n    'books',\n]\n\n</code></pre></div><p>Define a Book model in books/models.py:</p><div><pre><code>from django.db import models\n\nclass Book(models.Model):\n    title = models.CharField(max_length=255)\n    author = models.CharField(max_length=255)\n    published_date = models.DateField()\n    isbn = models.CharField(max_length=13, unique=True)\n\n    def __str__(self):\n        return self.title\n</code></pre></div><div><pre><code>python manage.py makemigrations books\npython manage.py migrate\n</code></pre></div><p><strong>Step 5: Serializer Creation</strong></p><p>Create books/serializers.py to handle data conversion:</p><div><pre><code>from rest_framework import serializers\nfrom .models import Book\n\nclass BookSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Book\n        fields = '__all__'\n</code></pre></div><p>We'll implement both function-based views (FBVs) and class-based views (CBVs).</p><p><strong>Function-Based Views (FBVs)</strong></p><div><pre><code>from rest_framework.response import Response\nfrom rest_framework.decorators import api_view\nfrom .models import Book\nfrom .serializers import BookSerializer\n\n@api_view(['GET'])\ndef book_list(request):\n    books = Book.objects.all()\n    serializer = BookSerializer(books, many=True)\n    return Response(serializer.data)\n\n@api_view(['POST'])\ndef book_create(request):\n    serializer = BookSerializer(data=request.data)\n    if serializer.is_valid():\n        serializer.save()\n        return Response(serializer.data, status=201)\n    return Response(serializer.errors, status=400)\n\n\n</code></pre></div><p>For a more scalable approach, use Django REST Frameworkâ€™s generic views:</p><div><pre><code>from rest_framework import generics\nfrom .models import Book\nfrom .serializers import BookSerializer\n\nclass BookListCreateView(generics.ListCreateAPIView):\n    queryset = Book.objects.all()\n    serializer_class = BookSerializer\n\n</code></pre></div><p><strong>Step 7: URL Configuration</strong></p><div><pre><code>from django.urls import path\nfrom .views import book_list, book_create, BookListCreateView\n\nurlpatterns = [\n    # Function-based views\n    path('books/', book_list, name='book-list'),\n    path('books/create/', book_create, name='book-create'),\n\n    # Class-based views\n    path('books/cbv/', BookListCreateView.as_view(), name='book-list-cbv'),\n]\n\n</code></pre></div><p>**Update bookapi/urls.py:</p><div><pre><code>from django.contrib import admin\nfrom django.urls import path, include\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    path('api/', include('books.urls')),\n]\n\n</code></pre></div><p>Run the server and visit:</p><p><code>ğŸ“Œ http://127.0.0.1:8000/api/books/</code></p><p>Django REST Framework provides an interactive browsable API that allows you to test endpoints without external tools!</p><p>You can also test the API using cURL commands:</p><div><pre><code># GET request\ncurl -X GET http://127.0.0.1:8000/api/books/\n\n# POST request\ncurl -X POST http://127.0.0.1:8000/api/books/create/ \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"title\": \"Django for Beginners\", \"author\": \"William Vincent\", \"published_date\": \"2021-09-01\", \"isbn\": \"9781735467207\"}'\n</code></pre></div><p><strong>Next Steps: Enhancing Your API</strong></p><p>Now that you have a working API, consider improving it with:\nâœ”ï¸ Authentication &amp; Permissions (e.g., only authenticated users can add books)<p>\nâœ”ï¸ Pagination for Large Datasets</p>\nâœ”ï¸ Filtering &amp; Searching<p>\nâœ”ï¸ ViewSets &amp; Routers for cleaner URL management</p></p><p>ğŸ‰ Congratulations! You have successfully built a simple REST API using the Django REST Framework. This foundation can be expanded to develop more complex APIs with additional features and security measures.</p>","contentLength":5814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomAffine in PyTorch (6)","url":"https://dev.to/hyperkai/randomaffine-in-pytorch-6-4813","date":1740201063,"author":"Super Kai (Kazuya Ito)","guid":8972,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/0.20/generated/torchvision.transforms.v2.RandomAffine.html\" rel=\"noopener noreferrer\">RandomAffine()</a> can do random rotation or random affine transformation for an image as shown below:</p><div><pre><code></code></pre></div>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomAffine in PyTorch (5)","url":"https://dev.to/hyperkai/randomaffine-in-pytorch-5-1c2m","date":1740196743,"author":"Super Kai (Kazuya Ito)","guid":8944,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/0.20/generated/torchvision.transforms.v2.RandomAffine.html\" rel=\"noopener noreferrer\">RandomAffine()</a> can do random rotation or random affine transformation for an image as shown below:</p><div><pre><code></code></pre></div>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seamlessly Compare Maps on QGIS with the QMapCompare Plugin","url":"https://dev.to/mierune/seamlessly-compare-maps-on-qgis-with-the-qmapcompare-plugin-3186","date":1740190246,"author":"Raymond Lay","guid":8928,"unread":true,"content":"<p>When working with QGIS, you often switch between basemaps, but  comparing small differences between maps can be challenging when only one map can be displayed at a time.</p><p>However, QGIS has lacked a stable feature for map comparison until now! This post introduces , a new plugin that enables users to compare multiple maps directly within QGIS using various visualization methods.</p><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmgu1gim3rtgowm3nkeq1.gif\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmgu1gim3rtgowm3nkeq1.gif\" alt=\"QMapCompare overview\" width=\"500\" height=\"237\"></a>QMapCompare overview. Created by editing <a href=\"https://maps.gsi.go.jp/development/ichiran.html\" rel=\"noopener noreferrer\">GSI Tiles</a></p><p>First, ensure you are using QGIS version 3.34 or later.\nThen, you can install plugin by searching  on QGIS plugin manager.</p><p>Once plugin installed, a new icon will appear on the toolbar.\nClicking the icon will toggle the QMapCompare panel on the left bottom of QGIS window.</p><p>QMapCompare provides several ways to compare maps. </p><p>Select layers you want to compare (1), and choose the following comparison method (2-5):</p><ul><li>1.  (multiple selections allowed)</li><li>2. : Displays two maps side by side</li><li>3. : Divides the map vertically</li><li>4. : Divides the map horizontally</li><li>5. : Check compare layers with a circle around the mouse cursor</li><li>6. : Ends the comparison</li></ul><h2>\n  \n  \n  Comparison Methods Overview\n</h2><p>The mirror mode places a duplicate of the map canvas on the right side of the map canvas. </p><p>This is useful when comparing base maps with satellite imagery or other data.</p><p>The split mode divides the map into two sections, showing different layers side by side.</p><p>You can choose either a vertical or horizontal split, depending on your needs.</p><p>This is may be useful for comparing building accuracy in OpenStreetMap with government-provided data as example.</p><p>Lens mode displays a circular preview of the comparison layers around the mouse cursor.</p><p>As you move the cursor, the preview updates in real time.</p><p>This is especially useful for detailed comparisons of specific locations.</p><p>QMapCompare is valuable for various use cases as below:</p><ul><li>Comparing historical aerial photographs</li></ul><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxtq3ljd5s4at4q8f9l5n.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxtq3ljd5s4at4q8f9l5n.png\" alt=\"Aerial Photo Comparison (1987-1990)\" width=\"800\" height=\"405\"></a>Tokyo Aerial Photo Comparison between 2024(left) and 1987-1990(right).<a href=\"https://maps.gsi.go.jp/development/ichiran.html\" rel=\"noopener noreferrer\">GSI Tiles</a></p><ul><li>Analyzing pre- and post-disaster images</li></ul><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4qz1eq7632asneuy3p2i.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4qz1eq7632asneuy3p2i.png\" alt=\"Disaster Impact Analysis\" width=\"800\" height=\"339\"></a>Aerial Photo Comparison of Before 2021 Atami Landslip Disaster (left) and after(right).<a href=\"https://maps.gsi.go.jp/development/ichiran.html\" rel=\"noopener noreferrer\">GSI Tiles</a></p><p>QMapCompare is a powerful tool for comparing different maps and datasets within QGIS. With this plugin, you can easily:</p><ul><li>Analyze time-series data (e.g., pre- and post-disaster maps)</li><li>Evaluate data accuracy (e.g., comparing OpenStreetMap with government maps)</li><li>Support decision-making (e.g., verifying different analytical results and styles)</li></ul><p>As this is a newly released plugin, there may still be some bugs. If you encounter any issues, please report them on our <a href=\"https://github.com/MIERUNE/qgis-plugin-qmapcompare/issues\" rel=\"noopener noreferrer\">GitHub Issues page</a>.</p><p>Your feedback will help improve the tool!</p>","contentLength":2579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Packages in Go: A Comprehensive Guide","url":"https://dev.to/abstractmusa/fsdfasdf-asfa-3fd1","date":1740188556,"author":"Md Abu Musa","guid":8929,"unread":true,"content":"<p>In Go, a package is a fundamental concept for organizing and reusing code. This guide explains everything you need to know about Go packages.</p><ul><li>A package is a collection of source files in the same directory.</li><li>All files in a package must declare the same package name at the top.</li><li>It provides modularity, encapsulation, and code reuse.</li></ul><ul><li>A special package that creates an executable program.</li><li>Must contain a  function.</li><li>Used only for executables.</li></ul><ul><li>Can have any name except .</li><li>Used to create reusable code.</li><li>Can be imported by other packages.</li></ul><h2>\n  \n  \n  3. Package Visibility Rules\n</h2><ul><li>Names starting with an  letter are .</li><li>Names starting with a  letter are .</li></ul><div><pre><code></code></pre></div><p>To use packages in Go, you import them:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  5. Package Organization Example\n</h2><div><pre><code>myapp/\nâ”œâ”€â”€ main.go              // package main\nâ”œâ”€â”€ utils/\nâ”‚   â”œâ”€â”€ math.go         // package utils\nâ”‚   â””â”€â”€ strings.go      // package utils\nâ””â”€â”€ models/\n    â””â”€â”€ user.go         // package models\n</code></pre></div><h2>\n  \n  \n  6. Benefits of Using Packages\n</h2><ul></ul><ul><li>All files in the same folder must have the same package name.</li><li>Package names usually match the directory name.</li><li>Standard library packages like , , etc., come with Go installation.</li><li>You can create custom packages for better code structure.</li><li>Use  to initialize a new module (which can contain multiple packages).</li></ul><p>By following these best practices, you can effectively manage code in Go using packages.</p>","contentLength":1373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Open-Source Book Creator with Multi-Agent AI","url":"https://dev.to/guerra2fernando/open-source-book-creator-with-multi-agent-ai-1bnl","date":1740187781,"author":"Fernando Guerra","guid":8927,"unread":true,"content":"<blockquote><p>I'm excited to share ** ğŸ“ LibriScribe**, an open-source book creation system I've developed that demonstrates the power of multiple specialized AI agents working together. \nYou can just install it with python and the system will guide you to write a complete book in a few minutes :)</p></blockquote><h2>\n  \n  \n  The Power of Multi-Agent Architecture\n</h2><p>Rather than using a single AI model to handle all aspects of book creation, LibriScribe orchestrates specialized agents:</p><ul><li>: Develops and refines your initial idea</li><li>: Structures your book with chapters and scenes</li><li>: Creates detailed character profiles</li><li>: Builds rich, consistent settings and lore</li><li>: Writes scene-by-scene content</li><li>: Refines and improves the writing</li><li>: Checks for plot holes and inconsistencies</li><li>: Polishes the writing style</li><li>: Prepares the final manuscript</li></ul><h2>\n  \n  \n  Versatile for Multiple Book Types\n</h2><ul><li>Fiction (novels, short stories)</li></ul><p>The system is built in Python with a modular and custom agent architecture. Each agent is a class that inherits from a base Agent class and implements an  method. The system uses a unified LLM client that supports multiple AI providers (OpenAI, Claude, Google AI, DeepSeek, and Mistral).</p><p>There's several functions:</p><p>Feedback and contributions are very welcome! Leave a star if you like it :)</p>","contentLength":1249,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomAffine in PyTorch (4)","url":"https://dev.to/hyperkai/randomaffine-in-pytorch-4-570l","date":1740184525,"author":"Super Kai (Kazuya Ito)","guid":8904,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/0.20/generated/torchvision.transforms.v2.RandomAffine.html\" rel=\"noopener noreferrer\">RandomAffine()</a> can do random rotation or random affine transformation for an image as shown below:</p><div><pre><code></code></pre></div>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomAffine in PyTorch (3)","url":"https://dev.to/hyperkai/randomaffine-in-pytorch-3-3hkm","date":1740183846,"author":"Super Kai (Kazuya Ito)","guid":8903,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/0.20/generated/torchvision.transforms.v2.RandomAffine.html\" rel=\"noopener noreferrer\">RandomAffine()</a> can do random rotation or random affine transformation for an image as shown below:</p><div><pre><code></code></pre></div>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Next AI Revolution: A Tutorial Using VAEs to Generate High-Quality Synthetic Data","url":"https://towardsdatascience.com/the-next-ai-revolution-a-tutorial-using-vaes-to-generate-high-quality-synthetic-data/","date":1740181327,"author":"Torty Sivill","guid":8902,"unread":true,"content":"<p>Data created by a computer intended to replicate or augment existing data.</p><p>We have all experienced the success of ChatGPT, Llama, and more recently, DeepSeek. These language models are being used ubiquitously across society and have triggered many claims that we are rapidly approaching Artificial General Intelligenceâ€Šâ€”â€ŠAI capable of replicating any human function.&nbsp;</p><p>Before getting too excited, or scared, depending on your perspectiveâ€Šâ€”â€Šwe are also rapidly approaching a hurdle to the advancement of these language models. According to a paper published by a group from the research institute, Epoch <a href=\"https://towardsdatascience.com/the-next-ai-revolution-a-tutorial-using-vaes-to-generate-high-quality-synthetic-data/#[1]\">[1]</a>,<em> we are running out of data</em>. They estimate that by 2028 we will have reached the upper limit of possible data upon which to train language models.&nbsp;</p><h2><strong>What happens if we run out of data?</strong></h2><p>Well, if we run out of data then we arenâ€™t going to have anything new with which to train our language models. These models will then stop improving. If we want to pursue Artificial General Intelligence then we are going to have to come up with new ways of improving AI without just increasing the volume of real-world training data.&nbsp;</p><p>One potential saviour is synthetic data which can be generated to mimic existing data and has already been used to improve the performance of models like Gemini and DBRX.&nbsp;</p><h2><strong>Synthetic data beyond LLMs</strong></h2><p>Beyond overcoming data scarcity for large language models, synthetic data can be used in the following situations:&nbsp;</p><ul><li>â€Šâ€”â€Šif we donâ€™t want to share or use sensitive attributes, synthetic data can be generated which mimics the properties of these features while maintaining anonymity.</li><li>â€Šâ€”â€Šif collecting data is expensive we can generate a large volume of synthetic data from a small amount of real-world data.</li><li>â€”â€Šdatasets are biased when there is a disproportionately low number of individual data points from a particular group. Synthetic data can be used to balance a dataset.&nbsp;</li></ul><p>Imbalanced datasets can (*but not always*) be problematic as they may not contain enough information to effectively train a predictive model. For example, if a dataset contains many more men than women, our model may be biased towards recognising men and misclassify future female samples as men.&nbsp;</p><p>In this article we show the imbalance in the popular UCI<a href=\"https://archive.ics.uci.edu/dataset/2/adult\"> Adult dataset</a><a href=\"https://towardsdatascience.com/the-next-ai-revolution-a-tutorial-using-vaes-to-generate-high-quality-synthetic-data/#[2]\">[2],</a> and how we can use a  to generate <a href=\"https://towardsdatascience.com/tag/synthetic-data/\" title=\"Synthetic Data\">Synthetic Data</a> to improve classification on this example.&nbsp;</p><p>We first download the Adult dataset. This dataset contains features such as age, education and occupation which can be used to predict the target outcome â€˜incomeâ€™.&nbsp;</p><pre><code># Download dataset into a dataframe\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\ncolumns = [\n   \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n   \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\",\n   \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n]\ndata = pd.read_csv(url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n\n# Drop rows with missing values\ndata = data.dropna()\n\n# Split into features and target\nX = data.drop(columns=[\"income\"])\ny = data['income'].map({'&gt;50K': 1, '&lt;=50K': 0}).values\n\n# Plot distribution of income\nplt.figure(figsize=(8, 6))\nplt.hist(data['income'], bins=2, edgecolor='black')\nplt.title('Distribution of Income')\nplt.xlabel('Income')\nplt.ylabel('Frequency')\nplt.show()</code></pre><p>In the Adult dataset, income is a binary variable, representing individuals who earn above, and below, $50,000. We plot the distribution of income over the entire dataset below. We can see that the dataset is heavily imbalanced with a far larger number of individuals who earn less than $50,000.&nbsp;</p><p>Despite this imbalance we can still train a machine learning classifier on the Adult dataset which we can use to determine whether unseen, or test, individuals should be classified as earning above, or below, 50k.&nbsp;</p><pre><code># Preprocessing: One-hot encode categorical features, scale numerical features\nnumerical_features = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\ncategorical_features = [\n   \"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\",\n   \"race\", \"sex\", \"native-country\"\n]\n\npreprocessor = ColumnTransformer(\n   transformers=[\n       (\"num\", StandardScaler(), numerical_features),\n       (\"cat\", OneHotEncoder(), categorical_features)\n   ]\n)\n\nX_processed = preprocessor.fit_transform(X)\n\n# Convert to numpy array for PyTorch compatibility\nX_processed = X_processed.toarray().astype(np.float32)\ny_processed = y.astype(np.float32)\n# Split dataset in train and test sets\nX_model_train, X_model_test, y_model_train, y_model_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42)\n\n\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_model_train, y_model_train)\n\n# Make predictions\ny_pred = rf_classifier.predict(X_model_test)\n\n# Display confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()</code></pre><p>Printing out the confusion matrix of our classifier shows that our model performs fairly well despite the imbalance. Our model has an overall error rate of 16% but the error rate for the positive class (income &gt; 50k) is 36% where the error rate for the negative class (income &lt; 50k) is 8%.&nbsp;</p><p>This discrepancy shows that the model is indeed biased towards the negative class. The model is frequently incorrectly classifying individuals who earn more than 50k as earning less than 50k.&nbsp;</p><p>Below we show how we can use a <a href=\"https://towardsdatascience.com/tag/variational-autoencoder/\" title=\"Variational Autoencoder\">Variational Autoencoder</a> to generate synthetic data of the positive class to balance this dataset. We then train the same model using the synthetically balanced dataset and reduce model errors on the test set.&nbsp;</p><h2><strong>How can we generate synthetic data?</strong></h2><p>There are lots of different methods for generating synthetic data. These can include more traditional methods such as SMOTE and Gaussian Noise which generate new data by modifying existing data. Alternatively Generative models such as Variational Autoencoders or General Adversarial networks are predisposed to generate new data as their architectures learn the distribution of real data and use these to generate synthetic samples.</p><p><strong>In this tutorial we use a variational autoencoder to generate synthetic data.</strong></p><p>Variational Autoencoders (VAEs) are great for synthetic data generation because they use real data to learn a continuous latent space. We can view this latent space as a magic bucket from which we can sample synthetic data which closely resembles existing data. The continuity of this space is one of their big selling points as it means the model generalises well and doesnâ€™t just memorise the latent space of specific inputs.</p><p>A VAE consists of an , which maps input data into a probability distribution (mean and variance) and a , which reconstructs the data from the latent space.&nbsp;</p><p>For that continuous latent space, VAEs use a reparameterization trick where a random noise vector is scaled and shifted using the learned mean and variance, ensuring smooth and continuous representations in the latent space.</p><p>Below we construct a  class which implements this process with a simple architecture.</p><ul><li> compresses the input into a smaller, hidden representation, producing both a mean and log variance that define a Gaussian distribution aka creating our magic sampling bucket. Instead of directly sampling, the model applies the reparameterization trick to generate latent variables, which are then passed to the decoder.&nbsp;</li><li> reconstructs the original data from these latent variables, ensuring the generated data maintains characteristics of the original dataset.&nbsp;</li></ul><pre><code>class BasicVAE(nn.Module):\n   def __init__(self, input_dim, latent_dim):\n       super(BasicVAE, self).__init__()\n       # Encoder: Single small layer\n       self.encoder = nn.Sequential(\n           nn.Linear(input_dim, 8),\n           nn.ReLU()\n       )\n       self.fc_mu = nn.Linear(8, latent_dim)\n       self.fc_logvar = nn.Linear(8, latent_dim)\n      \n       # Decoder: Single small layer\n       self.decoder = nn.Sequential(\n           nn.Linear(latent_dim, 8),\n           nn.ReLU(),\n           nn.Linear(8, input_dim),\n           nn.Sigmoid()  # Outputs values in range [0, 1]\n       )\n\n   def encode(self, x):\n       h = self.encoder(x)\n       mu = self.fc_mu(h)\n       logvar = self.fc_logvar(h)\n       return mu, logvar\n\n   def reparameterize(self, mu, logvar):\n       std = torch.exp(0.5 * logvar)\n       eps = torch.randn_like(std)\n       return mu + eps * std\n\n   def decode(self, z):\n       return self.decoder(z)\n\n   def forward(self, x):\n       mu, logvar = self.encode(x)\n       z = self.reparameterize(mu, logvar)\n       return self.decode(z), mu, logvar</code></pre><p>Given our BasicVAE architecture we construct our loss functions and model training below.&nbsp;</p><pre><code>def vae_loss(recon_x, x, mu, logvar, tau=0.5, c=1.0):\n   recon_loss = nn.MSELoss()(recon_x, x)\n \n   # KL Divergence Loss\n   kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n   return recon_loss + kld_loss / x.size(0)\n\ndef train_vae(model, data_loader, epochs, learning_rate):\n   optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n   model.train()\n   losses = []\n   reconstruction_mse = []\n\n   for epoch in range(epochs):\n       total_loss = 0\n       total_mse = 0\n       for batch in data_loader:\n           batch_data = batch[0]\n           optimizer.zero_grad()\n           reconstructed, mu, logvar = model(batch_data)\n           loss = vae_loss(reconstructed, batch_data, mu, logvar)\n           loss.backward()\n           optimizer.step()\n           total_loss += loss.item()\n\n           # Compute batch-wise MSE for comparison\n           mse = nn.MSELoss()(reconstructed, batch_data).item()\n           total_mse += mse\n\n       losses.append(total_loss / len(data_loader))\n       reconstruction_mse.append(total_mse / len(data_loader))\n       print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, MSE: {total_mse:.4f}\")\n   return losses, reconstruction_mse\n\ncombined_data = np.concatenate([X_model_train.copy(), y_model_train.cop\ny().reshape(26048,1)], axis=1)\n\n# Train-test split\nX_train, X_test = train_test_split(combined_data, test_size=0.2, random_state=42)\n\nbatch_size = 128\n\n# Create DataLoaders\ntrain_loader = DataLoader(TensorDataset(torch.tensor(X_train)), batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(TensorDataset(torch.tensor(X_test)), batch_size=batch_size, shuffle=False)\n\nbasic_vae = BasicVAE(input_dim=X_train.shape[1], latent_dim=8)\n\nbasic_losses, basic_mse = train_vae(\n   basic_vae, train_loader, epochs=50, learning_rate=0.001,\n)\n\n# Visualize results\nplt.figure(figsize=(12, 6))\nplt.plot(basic_mse, label=\"Basic VAE\")\nplt.ylabel(\"Reconstruction MSE\")\nplt.title(\"Training Reconstruction MSE\")\nplt.legend()\nplt.show()</code></pre><p>consists of two components: , which measures how well the generated data matches the original input using Mean Squared Error (MSE), and , which ensures that the learned latent space follows a normal distribution.</p><p> optimises the VAE using the Adam optimizer over multiple epochs. During training, the model takes mini-batches of data, reconstructs them, and computes the loss using . These errors are then corrected via backpropagation where the model weights are updated. We train the model for 50 epochs and plot how the reconstruction mean squared error decreases over training.</p><p>We can see that our model learns quickly how to reconstruct our data, evidencing efficient learning.&nbsp;</p><p>Now we have trained our BasicVAE to accurately reconstruct the Adult dataset we can now use it to generate synthetic data. We want to generate more samples of the positive class (individuals who earn over 50k) in order to balance out the classes and remove the bias from our model.</p><p>To do this we select all the samples from our VAE dataset where income is the positive class (earn more than 50k). We then encode these samples into the latent space. As we have only selected samples of the positive class to encode, this latent space will reflect properties of the positive class which we can sample from to create synthetic data.&nbsp;</p><p>We sample 15000 new samples from this latent space and decode these latent vectors back into the input data space as our synthetic data points.&nbsp;</p><pre><code># Create column names\ncol_number = sample_df.shape[1]\ncol_names = [str(i) for i in range(col_number)]\nsample_df.columns = col_names\n\n# Define the feature value to filter\nfeature_value = 1.0  # Specify the feature value - here we set the income to 1\n\n# Set all income values to 1 : Over 50k\nselected_samples = sample_df[sample_df[col_names[-1]] == feature_value]\nselected_samples = selected_samples.values\nselected_samples_tensor = torch.tensor(selected_samples, dtype=torch.float32)\n\nbasic_vae.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n   mu, logvar = basic_vae.encode(selected_samples_tensor)\n   latent_vectors = basic_vae.reparameterize(mu, logvar)\n\n# Compute the mean latent vector for this feature\nmean_latent_vector = latent_vectors.mean(dim=0)\n\n\nnum_samples = 15000  # Number of new samples\nlatent_dim = 8\nlatent_samples = mean_latent_vector + 0.1 * torch.randn(num_samples, latent_dim)\n\nwith torch.no_grad():\n   generated_samples = basic_vae.decode(latent_samples)</code></pre><p>Now we have generated synthetic data of the positive class, we can combine this with the original training data to generate a balanced synthetic dataset.&nbsp;</p><pre><code>new_data = pd.DataFrame(generated_samples)\n\n# Create column names\ncol_number = new_data.shape[1]\ncol_names = [str(i) for i in range(col_number)]\nnew_data.columns = col_names\n\nX_synthetic = new_data.drop(col_names[-1],axis=1)\ny_synthetic = np.asarray([1 for _ in range(0,X_synthetic.shape[0])])\n\nX_synthetic_train = np.concatenate([X_model_train, X_synthetic.values], axis=0)\ny_synthetic_train = np.concatenate([y_model_train, y_synthetic], axis=0)\n\nmapping = {1: '&gt;50K', 0: '&lt;=50K'}\nmap_function = np.vectorize(lambda x: mapping[x])\n# Apply mapping\ny_mapped = map_function(y_synthetic_train)\n\nplt.figure(figsize=(8, 6))\nplt.hist(y_mapped, bins=2, edgecolor='black')\nplt.title('Distribution of Income')\nplt.xlabel('Income')\nplt.ylabel('Frequency')\nplt.show()</code></pre><p>We can now use our balanced training synthetic dataset to retrain our random forest classifier. We can then evaluate this new model on the original test data to see how effective our synthetic data is at reducing the model bias.</p><pre><code>rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_synthetic_train, y_synthetic_train)\n\n# Step 5: Make predictions\ny_pred = rf_classifier.predict(X_model_test)\n\ncm = confusion_matrix(y_model_test, y_pred)\n\n# Create heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()</code></pre><p>Our new classifier, trained on the balanced synthetic dataset makes fewer errors on the original test set than our original classifier trained on the imbalanced dataset and our error rate is now reduced to 14%.</p><p>However, we have not been able to reduce the discrepancy in errors by a significant amount, our error rate for the positive class is still 36%. This could be due to to the following reasons:&nbsp;</p><ul><li>We have discussed how one of the benefits of VAEs is the learning of a continuous latent space. However, if the majority class dominates, the latent space might skew towards the majority class.</li><li>The model may not have properly learned a distinct representation for the minority class due to the lack of data, making it hard to sample from that region accurately.</li></ul><p><strong>In this tutorial we have introduced and built a BasicVAE architecture which can be used to generate synthetic data which improves the classification accuracy on an imbalanced dataset.&nbsp;</strong></p><p>Follow for future articles where I will show how we can build more sophisticated VAE architectures which address the above problems with imbalanced sampling and more.</p><p>[1] Villalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., &amp; Hobbhahn, M. (2024). Will we run out of data? Limits of LLM scaling based on human-generated data. <em>arXiv preprint arXiv:2211.04325</em>, .</p>","contentLength":16372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comments on Executive Order 14168","url":"https://aphyr.com/posts/380-comments-on-executive-order-14168","date":1740179095,"author":"Aphyr","guid":8872,"unread":true,"content":"<p>Executive order 14168 is biologically incoherent and socially cruel. All passport applicants should be allowed to select whatever gender markers they feel best fit, including M, F, or X.</p><p>In humans, neither sex nor gender is binary at any level. There are several possible arrangements of sex chromosomes: X, XX, XY, XXY, XYY, XXX, tetrasomies, pentasomies, etc. A single person can contain a mosaic of cells with different genetics: some XX, some XYY. Chromosomes may not align with genitalia: people with XY chromosomes may have a vulva and internal testes. People with XY chromosomes and a small penis may be surgically and socially reassigned female at birthâ€”and never told what happened. None of these biological dimensions necessarily align with oneâ€™s internal concept of gender, or oneâ€™s social presentation.</p><p>The executive order has no idea how biology works. It defines â€œfemaleâ€ as â€œa person belonging, at conception, to the sex that produces the large reproductive cellâ€. Zygotes do not produce reproductive cells at all: under this order none  of us have a sex. Oogenesis doesnâ€™t start until over a month into embryo development. Even if people were karyotyping their zygotes immediately after conception so they could tell what â€œlegalâ€ sex they were going to be, they could be wrong: which gametes we produce depends on the formation of the genital ridge.</p><p>All this is to say that if people fill out these forms using this definition of sex, theyâ€™re guessing at a question which is both impossible to answer and socially irrelevant. You might be one of the roughly two percent of humans born with an uncommon sexual development and not even know it. Moreover, the proposed change fundamentally asks the wrong question: gender markers on passports are used by border control agents, and are expected to align with how those agents read the passport holderâ€™s gender. A mismatch will create needless intimidation and hardship for travelers.</p><p>Of course most of us will not have our identities challenged under this order. That animus is reserved for trans people, for gender-non-conforming people, for anyone whose genetics, body, dress, voice, or mannerisms donâ€™t quite fit the mold. Those are the people who will suffer under this order. That cruelty should be resisted.</p>","contentLength":2298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Create Python Virtual Environments on Ubuntu","url":"https://dev.to/lgerthal/creating-python-virtual-environments-on-ubuntu-5an7","date":1740177279,"author":"Luiz Gustavo Erthal","guid":8862,"unread":true,"content":"<p>When working on different Python projects, it's often necessary to create isolated environments for specific tasks. This is where Python's Virtual Environments package comes in handy.</p><p>Use this post as a guide or a cheat sheet for future reference.</p><h3>\n  \n  \n  Creating a Virtual Environment\n</h3><p>To create a new virtual environment, use the following command:</p><p>The .venv directory will contain your virtual environment. Using a dot (.) before the name makes it a hidden folder, which is a common practice. However, you can name it anything you like to suit your project structure.</p><h3>\n  \n  \n  Activating the Virtual Environment\n</h3><p>To activate the virtual environment, run:</p><div><pre><code>source .venv/bin/activate\n</code></pre></div><p>Once activated, you can install any required packages and libraries. These installations will be isolated within the .venv directory and will not affect global Python packages.</p><p>For example, to install pandas:</p><h3>\n  \n  \n  Deactivating the Virtual Environment\n</h3><p>To deactivate the virtual environment, simply run:</p><h3>\n  \n  \n  Listing Installed Packages\n</h3><p>While the virtual environment is activated, you can list all installed packages using:</p><p>The only difference between each command is that  will display a human-readable list while  will outputs installed packages in a machine-readable format.</p><h3>\n  \n  \n  Sharing Your Virtual Environment\n</h3><p>If you need to share your project, ensure that others can replicate your environment. You can do this by exporting the installed packages to a requirements.txt file:</p><p><strong>Save your venv into a .txt</strong></p><div><pre><code>pip freeze &gt; requirements.txt\n</code></pre></div><p>If you receive a requirements.txt file from a colleague or friend, you can install all required packages by running:</p><p><strong>Import the packages from a requirements.txt</strong></p><div><pre><code>pip install -r requirements.txt\n</code></pre></div><p>By following these steps, you can effectively manage your Python environments and ensure consistency across different projects.</p>","contentLength":1838,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomAffine in PyTorch (2)","url":"https://dev.to/hyperkai/randomaffine-in-pytorch-2-3d3a","date":1740175647,"author":"Super Kai (Kazuya Ito)","guid":8861,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/0.20/generated/torchvision.transforms.v2.RandomAffine.html\" rel=\"noopener noreferrer\">RandomAffine()</a> can do random rotation or random affine transformation for an image as shown below:</p><div><pre><code></code></pre></div>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomAffine in PyTorch (1)","url":"https://dev.to/hyperkai/randomaffine-in-pytorch-1-2j6j","date":1740175396,"author":"Super Kai (Kazuya Ito)","guid":8837,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/0.20/generated/torchvision.transforms.v2.RandomAffine.html\" rel=\"noopener noreferrer\">RandomAffine()</a> can do random rotation or random affine transformation for an image as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It's the range of the degrees  so it must be .</li><li>A degrees value is randomly taken from the range of .</li><li>A tuple/list must be the 1D with 2 elements.</li><li>A single value( or ) means <code>[-degrees(min), +degrees(max)]</code>.</li><li>A single value( or ) must be .</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type:/( or )):\n*Memos:\n\n<ul><li>It must be the 1D with 2 elements.</li><li>The 1st element is for the horizontal shift randomly taken in the range of <code>-img_width * a &lt; horizontal shift &lt; img_width * a</code>.</li><li>The 2nd element is for the vertical shift randomly taken in the range of <code>-img_height * b &lt; vertical shift &lt; img_height * b</code>.</li></ul></li><li>The 3rd argument for initialization is (Optional-Default:-Type:/( or )):\n*Memos:\n\n<ul><li>It's  so it must be .</li><li>It must be the 1D with 2 elements.</li><li>A scale value is randomly taken from the range of .</li></ul></li><li>The 4th argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can do affine transformation with  and .</li><li>It's  so it must be .\n*Memos:</li><li>The 1st two elements are the range of .</li><li>The 2nd two elements are the range of .</li><li> value is randomly taken from the range of the 1st two elements.</li><li> value is randomly taken from the range of the 2nd two elements.</li><li>A tuple/list must be the 1D with 2 or 4 elements.</li><li>The tuple/list of 2 elements means <code>[shear[0](min), shear[1](max), 0.0(min), 0.0(max)]</code>.</li><li>A single value means <code>[-shear(min), +shear(max), 0.0(min), 0.0(max)]</code>.</li><li>A single value must be .</li></ul></li><li>The 5th argument for initialization is (Optional-Default:<code>InterpolationMode.NEAREST</code>-Type:<a href=\"https://pytorch.org/vision/main/_modules/torchvision/transforms/functional.html\" rel=\"noopener noreferrer\">InterpolationMode</a>).</li><li>The 6th argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can change the background of an image. *The background can be seen when doing rotation or affine transformation for an image.</li><li>A tuple/list must be the 1D with 1 or 3 elements.</li></ul></li><li>The 7th argument for initialization is (Optional-Default:-Type:/( or )):\n*Memos:\n\n<ul><li>It can change the center position of an image.</li><li>It must be the 1D with 2 elements.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul></ul></li></ul><div><pre><code></code></pre></div>","contentLength":2140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do European M&Ms Actually Taste Better than American M&Ms?","url":"https://towardsdatascience.com/do-european-mms-actually-taste-better-than-american-mms/","date":1740174778,"author":"Erin Wilson","guid":8901,"unread":true,"content":"<p><em>(Oh, I am the only one whoâ€™s been asking this questionâ€¦? Hm. Well, if you have a minute, please enjoy this exploratory <a href=\"https://towardsdatascience.com/tag/data-analysis/\" title=\"Data Analysis\">Data Analysis</a> â€” featuring experimental design, statistics, and interactive visualization â€” applied a bit too earnestly to resolve an international debate.)</em></p><h3>1.1 Background and motivation</h3><p>Chocolate is enjoyed around the world. From ancient practices harvesting organic cacao in the Amazon basin, to chocolatiers sculpting edible art in the mountains of Switzerland, and enormous factories in Hershey, Pennsylvania churning out 70 million kisses per day, the nuanced forms and flavors of chocolate have been integrated into many cultures and their customs. While quality can greatly vary across chocolate products, a well-known, shelf-stable, easily shareable form of chocolate are M&amp;Ms. Readily found by convenience store check-out counters and in hotel vending machines, the brightly colored pellets are a popular treat whose packaging is re-branded to fit nearly any commercializable American holiday.</p><p>While living in Denmark in 2022, I heard a concerning claim: M&amp;Ms manufactured in Europe taste different, and arguably â€œbetter,â€ than M&amp;Ms produced in the United States. While I recognized that fancy European chocolate is indeed quite tasty and often superior to American chocolate, it was unclear to me if the same claim should hold for M&amp;Ms. I learned that many Europeans perceive an â€œunpleasantâ€ or â€œtangyâ€ taste in American chocolate, which is largely attributed to&nbsp;<a href=\"https://www.chemistryworld.com/podcasts/butyric-acid/1017662.article\" rel=\"noreferrer noopener\" target=\"_blank\">butyric acid</a>, a compound resulting from differences in how milk is treated before incorporation into milk chocolate.</p><p>But honestly, how much of a difference could this make for M&amp;Ms?&nbsp;? I imagined M&amp;Ms would retain a relatively processed/mass-produced/cheap candy flavor wherever they were manufactured. As the lone American visiting a diverse lab of international scientists pursuing cutting-edge research in biosustainability, I was inspired to break out my data science toolbox and investigate this M&amp;M flavor phenomenon.</p><p>To quote a European woman, who shall remain anonymous, after she tasted an American M&amp;M while traveling in New York:</p><blockquote><p>â€œThey taste so gross. Like vomit. I donâ€™t understand how people can eat this. I threw the rest of the bag away.â€</p></blockquote><p>Vomit? Really? In my experience, children raised in the United States had no qualms about eating M&amp;Ms. Growing up, I was accustomed to bowls of M&amp;Ms strategically placed in high traffic areas around my house to provide readily available sugar. Clearly American M&amp;Ms are edible. But are they significantly different and/or inferior to their European equivalent?</p><p>In response to the anonymous European womanâ€™s scathing report, myself and two other Americans visiting Denmark sampled M&amp;Ms purchased locally in the Lyngby Storcenter FÃ¸tex. We hoped to experience the incredible improvement in M&amp;M flavor that was apparently hidden from us throughout our youths. But curiously, we detected no obvious flavor improvements.</p><p>Unfortunately, neither preliminary study was able to conduct a side-by-side taste test with proper controls and randomized M&amp;M sampling. Thus, we turn to science.</p><p>This study seeks to remedy the previous lack of thoroughness and investigate the following questions:</p><ol><li>Is there a&nbsp;&nbsp;that European M&amp;Ms are in fact better than American M&amp;Ms?</li><li><strong>Can Europeans actually detect a difference&nbsp;</strong>between M&amp;Ms purchased in the US vs in Europe when they donâ€™t know which one they are eating? Or is this a&nbsp;&nbsp;amongst Europeans to make Americans feel embarrassed?</li><li><strong>Are Americans actually taste-blind&nbsp;</strong>to American vs European M&amp;Ms? Or can they taste a difference but simply donâ€™t describe this difference as â€œan improvementâ€ in flavor?</li><li>Can these alleged taste differences be&nbsp;<strong>perceived by citizens of other continents</strong>? If so, do they find one flavor obviously superior?</li></ol><h3>2.1 Experimental design and data collection</h3><p>Participants were recruited by luring â€” er,&nbsp;&nbsp;them to a social gathering (with the promise of free food) that was conveniently co-located with the testing site. Once a participant agreed to pause socializing and join the study, they were positioned at a testing station with a trained experimenter who guided them through the following steps:</p><ul><li>Participants sat at a table and received two cups: 1 empty and 1 full of water. With one cup in each hand, the participant was asked to close their eyes, and keep them closed through the remainder of the experiment.</li><li>The experimenter randomly extracted one M&amp;M with a spoon, delivered it to the participantâ€™s empty cup, and the participant was asked to eat the M&amp;M (eyes still closed).</li><li>After eating each M&amp;M, the experimenter collected the taste response by asking the participant to report if they thought the M&amp;M tasted: Especially Good, Especially Bad, or Normal.</li><li>Each participant received a total of 10 M&amp;Ms (5 European, 5 American), one at a time, in a random sequence determined by random.org.</li><li>Between eating each M&amp;M, the participant was asked to take a sip of water to help â€œcleanse their palate.â€</li><li>: for each participant, the experimenter recorded the participantâ€™s&nbsp;if this was ambiguous, the participant was asked to list the continent on which they have the strongest memories of eating candy as a child). For each of the 10 M&amp;Ms delivered, the experimenter recorded the&nbsp;&nbsp;(â€œDenmarkâ€ or â€œUSAâ€), the&nbsp;&nbsp;and the participantâ€™s&nbsp;. Experimenters were also encouraged to jot down any amusing phrases uttered by the participant during the test, recorded under&nbsp;(data available&nbsp;<a href=\"https://github.com/erinhwilson/mnm-taste-test/tree/main/data\" target=\"_blank\" rel=\"noreferrer noopener\">here</a>).</li></ul><h3>2.2 Sourcing materials and recruiting participants</h3><p>Two bags of M&amp;Ms were purchased for this study. The American-sourced M&amp;Ms (â€œUSA M&amp;Mâ€) were acquired at the SFO airport and delivered by the authorâ€™s parents, who visited her in Denmark. The European-sourced M&amp;Ms (â€œDenmark M&amp;Mâ€) were purchased at a local FÃ¸tex grocery store in Lyngby, a little north of Copenhagen.</p><p>Experiments were conducted at two main time points. The first 14 participants were tested in Lyngby, Denmark in August 2022. They mostly consisted of friends and housemates the author met at the Novo Nordisk Foundation Center for Biosustainability at the Technical University of Denmark (DTU) who came to a â€œgoing away partyâ€ into which the experimental procedure was inserted. A few additional friends and family who visited Denmark were also tested during their travels (e.g. on the train).</p><p>The remaining 37 participants were tested in Seattle, WA, USA in October 2022, primarily during a â€œTGIF happy hourâ€ hosted by graduate students in the computer science PhD program at the University of Washington. This second batch mostly consisted of students and staff of the Paul. G. Allen School of Computer Science &amp; Engineering (UW CSE) who responded to the weekly Friday summoning to the Allen Center atrium for free snacks and drinks.</p><p>While this study set out to analyze global trends, unfortunately data was only collected from 51 participants the author was able to lure to the study sites and is not well-balanced nor representative of the 6 inhabited continents of Earth (Figure 1). We hope to improve our recruitment tactics in future work. For now, our analytical power with this dataset is limited to response trends for individuals from North America, Europe, and Asia, highly biased by subcommunities the author happened to engage with in late 2022.</p><p>While we did not acquire formal approval for experimentation with human test subjects, there were minor risks associated with this experiment: participants were warned that they may be subjected to increased levels of sugar and possible â€œunpleasant flavorsâ€ as a result of participating in this study. No other risks were anticipated.</p><p>After the experiment however, we unfortunately observed several cases of deflated pride when a participant learned their taste response was skewed more positively towards the M&amp;M type they were not expecting. This pride deflation seemed most severe among European participants who learned their own or their fiancÃ©â€™s preference skewed towards USA M&amp;Ms, though this was not quantitatively measured and cannot be confirmed beyond anecdotal evidence.</p><h3>3.1 Overall response to â€œUSA M&amp;Msâ€ vs â€œDenmark M&amp;Msâ€</h3><h4><strong>3.1.1 Categorical response analysis â€” entire dataset</strong></h4><p>In our first analysis, we count the total number of â€œBadâ€, â€œNormalâ€, and â€œGoodâ€ taste responses and report the percentage of each response received by each M&amp;M type. M&amp;Ms from Denmark more frequently received â€œGoodâ€ responses than USA M&amp;Ms but also more frequently received â€œBadâ€ responses. M&amp;Ms from the USA were most frequently reported to taste â€œNormalâ€ (Figure 2). This may result from the elevated number of participants hailing from North America, where the USA M&amp;M is the default and thus more â€œNormal,â€ while the Denmark M&amp;M was more often perceived as better or worse than the baseline.</p><p><sup>Figure 2. Qualitative taste response distribution across the whole dataset. The percentage of taste responses for â€œBadâ€, â€œNormalâ€ or â€œGoodâ€ was calculated for each type of M&amp;M. Figure made with Altair.</sup></p><p>Now letâ€™s break out some <a href=\"https://towardsdatascience.com/tag/statistics/\" title=\"Statistics\">Statistics</a>, such as a&nbsp;-squared (X2) test to compare our observed distributions of categorical taste responses. Using the scipy.stats&nbsp;<a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html\" rel=\"noreferrer noopener\" target=\"_blank\">chi2_contingency</a>&nbsp;function, we built contingency tables of the observed counts of â€œGood,â€ â€œNormal,â€ and â€œBadâ€ responses to each M&amp;M type. Using the X2 test to evaluate the null hypothesis that there is no difference between the two M&amp;Ms, we found the&nbsp;-value for the test statistic to be 0.0185, which is significant at the common&nbsp;-value cut off of 0.05, but not at 0.01. So a solid â€œmaybe,â€ depending on whether youâ€™d like this result to be significant or not.</p><h4><strong>3.1.2 Quantitative response analysis â€” entire dataset.</strong></h4><p>The X2 test helps evaluate if there is a difference in categorical responses, but next, we want to determine a relative taste&nbsp;&nbsp;between the two M&amp;M types. To do this, we converted taste responses to a quantitative distribution and calculated a&nbsp;Briefly, â€œBadâ€ = 1, â€œNormalâ€ = 2, â€œGoodâ€ = 3. For each participant, we averaged the taste scores across the 5 M&amp;Ms they tasted of each type, maintaining separate taste scores for each M&amp;M type.</p><p>With the average taste score for each M&amp;M type in hand, we turn to scipy.stats&nbsp;<a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\" rel=\"noreferrer noopener\" target=\"_blank\">ttest_ind</a>&nbsp;(â€œT-testâ€) to evaluate if the means of the USA and Denmark M&amp;M taste scores are different (the null hypothesis being that the means are identical). If the means are significantly different, it would provide evidence that one M&amp;M is perceived as significantly tastier than the other.</p><p>We found the average taste scores for USA M&amp;Ms and Denmark M&amp;Ms to be quite close (Figure 3), and not significantly different (T-test:&nbsp;= 0.721). Thus, across all participants, we do not observe a difference between the perceived taste of the two M&amp;M types (or if you enjoy parsing triple negatives: â€œwe&nbsp;&nbsp;the null hypothesis that there is&nbsp;&nbsp;a differenceâ€).</p><p>But does this change if we separate participants by continent of origin?</p><h3>3.2 Continent-specific responses to â€œUSA M&amp;Msâ€ vs â€œDenmark M&amp;Msâ€</h3><p>We repeated the above X2 and T-test analyses after grouping participants by their continents of origin. The Australia and South America groups were combined as a minimal attempt to preserve data privacy. Due to the relatively small sample size of even the combined Australia/South America group (=3), we will refrain from analyzing trends for this group but include the data in several figures for completeness and enjoyment of the participants who may eventually read this.</p><h4><strong>3.2.1 Categorical response analysis â€” by continent</strong></h4><p>In Figure 4, we display both the taste response counts (upper panel,&nbsp;<em>note the interactive legend</em>) and the response percentages (lower panel) for each continent group. Both North America and Asia follow a similar trend to the whole population dataset: participants report Denmark M&amp;Ms as â€œGoodâ€ more frequently than USA M&amp;Ms, but also report Denmark M&amp;Ms as â€œBadâ€ more frequently. USA M&amp;Ms were most frequently reported as â€œNormalâ€ (Figure 4).</p><p>On the contrary, European participants report USA M&amp;Ms as â€œBadâ€ nearly 50% of the time and â€œGoodâ€ only 18% of the time, which is the most negative and least positive response pattern, respectively (when excluding the under-sampled Australia/South America group).</p><p><sup>Figure 4. Qualitative taste response distribution by continent. Upper panel: counts of taste responses â€” click the legend to interactively filter! Lower panel: percentage of taste responses for each type of M&amp;M. Figure made with Altair.</sup></p><p>This appeared striking in bar chart form, however only North America had a significant X2&nbsp;-value () when evaluating each continentâ€™s difference in taste response profile between the two M&amp;M types. The European&nbsp;-value is perhaps â€œapproaching significanceâ€ in some circles, but weâ€™re about to accumulate several more hypothesis tests and should be mindful of multiple hypothesis testing (Table 1). A false positive result here would be devastating.</p><p>When comparing the taste response profiles between two continents for the same M&amp;M type, there are a couple interesting notes. First, we observed no major taste discrepancies between all pairs of continents when evaluating Denmark M&amp;Ms â€” the world seems generally consistent in their range of feelings about M&amp;Ms sourced from Europe (right column X2&nbsp;-values, Table 2). To visualize this comparison more easily, we reorganize the bars in Figure 4 to group them by M&amp;M type (Figure 5).</p><p><sup>Figure 5. Qualitative taste response distribution by M&amp;M type, reported as percentages. (Same data as Figure 4 but re-arranged). Figure made with Altair.</sup></p><p>However, when comparing continents to each other in response to USA M&amp;Ms, we see larger discrepancies. We found one pairing to be significantly different: European and North American participants evaluated USA M&amp;Ms very differently () (Table 2). It seems very unlikely that this observed difference is by random chance (left column, Table 2).</p><h4><strong>3.2.2 Quantitative response analysis â€” by continent</strong></h4><p>We again convert the categorical profiles to quantitative distributions to assess continentsâ€™ relative preference of M&amp;M types. For North America, we see that the taste score means of the two M&amp;M types are actually quite similar, but there is a higher density around â€œNormalâ€ scores for USA M&amp;Ms (Figure 6A). The European distributions maintain a bit more of a separation in their means (though not quite significantly so), with USA M&amp;Ms scoring lower (Figure 6B). The taste score distributions of Asian participants is most similar (Figure 6C).</p><p>Reorienting to compare the quantitative means between continentsâ€™ taste scores for the same M&amp;M type, only the comparison between North American and European participants on USA M&amp;Ms is significantly different based on a T-test () (Figure 6D), though now we&nbsp;&nbsp;are in danger of multiple hypothesis testing! Be cautious if you are taking this analysis at all seriously.</p><p>At this point, I feel myself considering that maybe Europeans are not just making this up. Iâ€™m not saying itâ€™s as dramatic as some of them claim, but perhaps a difference does indeed existâ€¦ To some degree, North American participants also perceive a difference, but the evaluation of Europe-sourced M&amp;Ms is not consistently positive or negative.</p><h3>3.3 M&amp;M taste alignment chart</h3><p>In our analyses thus far, we did not account for the baseline differences in M&amp;M appreciation between participants. For example, say Person 1 scored all Denmark M&amp;Ms as â€œGoodâ€ and all USA M&amp;Ms as â€œNormalâ€, while Person 2 scored all Denmark M&amp;Ms as â€œNormalâ€ and all USA M&amp;Ms as â€œBad.â€ They would have the same relative preference for Denmark M&amp;Ms over USA M&amp;Ms, but Person 2 perhaps just does not enjoy M&amp;Ms as much as Person 1, and the relative preference signal is muddled by averaging the raw scores.</p><p>Inspired by the Lawful/Chaotic x Good/Evil alignment chart used in tabletop role playing games like Dungeons &amp; DragonsÂ©<img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2122.png\" alt=\"â„¢\">, in Figure 7, we establish an M&amp;M alignment chart to help determine the distribution of participants across M&amp;M enjoyment classes.</p><p>Notably, the upper right quadrant where both M&amp;M types are perceived as â€œGoodâ€ to â€œNormalâ€ is mostly occupied by North American participants and a few Asian participants. All European participants land in the left half of the figure where USA M&amp;Ms are â€œNormalâ€ to â€œBadâ€, but Europeans are somewhat split between the upper and lower halves, where perceptions of Denmark M&amp;Ms range from â€œGoodâ€ to â€œBad.â€</p><p>An interactive version of Figure 7 is provided below for the reader to explore the counts of various M&amp;M alignment regions.</p><p><sup>Figure 7 (interactive): click and brush your mouse over the scatter plot to see the counts of continents in different M&amp;M enjoyment regions. Figure made with Altair.</sup></p><h3>3.4 Participant taste response ratio</h3><p>Next, to factor out baseline M&amp;M enjoyment and focus on participantsâ€™ relative preference between the two M&amp;M types, we took the log ratio of each personâ€™s&nbsp;<strong>USA M&amp;M taste score average</strong>&nbsp;divided by their&nbsp;<strong>Denmark M&amp;M taste score average</strong>.</p><p>As such, positive scores indicate a preference towards USA M&amp;Ms while negative scores indicate a preference towards Denmark M&amp;Ms.</p><p>On average, European participants had the strongest preference towards Denmark M&amp;Ms, with Asians also exhibiting a slight preference towards Denmark M&amp;Ms (Figure 8). To the two Europeans who exhibited deflated pride upon learning their slight preference towards USA M&amp;Ms, fear not: you did not think USA M&amp;Ms were â€œGood,â€ but simply ranked them as less bad than Denmark M&amp;Ms (see participant_id 4 and 17 in the interactive version of Figure 7). If you assert that M&amp;Ms are a bad American invention not worth replicating and return to consuming artisanal European chocolate, your honor can likely be restored.</p><p>North American participants are pretty split in their preference ratios: some fall quite neutrally around 0, others strongly prefer the familiar USA M&amp;M, while a handful moderately prefer Denmark M&amp;Ms. Anecdotally, North Americans who learned their preference skewed towards European M&amp;Ms displayed signals of inflated pride, as if their results signaled posh refinement.</p><p>Overall, a T-test comparing the distributions of M&amp;M preference ratios shows a possibly significant difference in the means between European and North American participants (), but come on, this is like the 20th p-value Iâ€™ve reported â€” this one is probably too close to call.</p><h3>3.5 Taste inconsistency and â€œPerfect Classifiersâ€</h3><p>For each participant, we assessed their taste score consistency by averaging the standard deviations of their responses to each M&amp;M type, and plotting that against their preference ratio (Figure 9).</p><p><sup>Figure 9. Participant taste consistency by preference ratio. The x-axis is a participantâ€™s relative M&amp;M preference ratio. The y-axis is the average of the standard deviation of their USA M&amp;M scores and the standard deviation of their Denmark M&amp;M scores. A value of 0 on the y-axis indicates perfect consistency in responses, while higher values indicate more inconsistent responses. Figure made with Altair.</sup></p><p>Most participants were somewhat inconsistent in their ratings, ranking the same M&amp;M type differently across the 5 samples. This would be expected if the taste difference between European-sourced and American-sourced M&amp;Ms is not actually all that perceptible. Most inconsistent were participants who gave the same M&amp;M type â€œGoodâ€, â€œNormalâ€,&nbsp;&nbsp;â€œBadâ€ responses (e.g., points high on the y-axis, with wider standard deviations of taste scores), indicating lower taste perception abilities.</p><p>Intriguingly, four participants â€” one from each continent group â€” were perfectly consistent: they reported the same taste response for each of the 5 M&amp;Ms from each M&amp;M type, resulting in an average standard deviation of 0.0 (bottom of Figure 9). Excluding the one of the four who simply rated all 10 M&amp;Ms as â€œNormalâ€, the other three appeared to be â€œPerfect Classifiersâ€ â€” either rating all M&amp;Ms of one type â€œGoodâ€ and the other â€œNormalâ€, or rating all M&amp;Ms of one type â€œNormalâ€ and the other â€œBad.â€ Perhaps these folks are â€œsuper tasters.â€</p><p>Another possible explanation for the inconsistency in individual taste responses is that there exists a perceptible taste difference based on the M&amp;M color. Visually, the USA M&amp;Ms were noticeably more smooth and vibrant than the Denmark M&amp;Ms, which were somewhat more â€œsplotchyâ€ in appearance (Figure 10A). M&amp;M color was recorded during the experiment, and although balanced sampling was not formally built into the experimental design, colors seemed to be sampled roughly evenly, with the exception of Blue USA M&amp;Ms, which were oversampled (Figure 10B).</p><p>We briefly visualized possible differences in taste responses based on color (Figure 11), however we do not believe there are enough data to support firm conclusions. After all, on average each participant would likely only taste 5 of the 6 M&amp;M colors once, and 1 color not at all. We leave further M&amp;M color investigations to future work.</p><p>We assured each participant that there was no â€œright â€œanswerâ€ in this experiment and that all feelings are valid. While some participants took this to heart and occasionally spent over a minute deeply savoring each M&amp;M and evaluating it as if they were a sommelier, many participants seemed to view the experiment as a competition (which occasionally led to deflated or inflated pride). Experimenters wrote down quotes and notes in conjunction with M&amp;M responses, some of which were a bit â€œcolorful.â€ We provide a hastily rendered word cloud for each M&amp;M type for entertainment purposes (Figure 12) though we caution against reading too far into them without diligent sentiment analysis.</p><p>Overall, there does not appear to be a â€œglobal consensusâ€ that European M&amp;Ms are better than American M&amp;Ms. However, European participants tended to more strongly express negative reactions to USA M&amp;Ms while North American participants seemed relatively split on whether they preferred M&amp;Ms sourced from the USA vs from Europe. The preference trends of Asian participants often fell somewhere between the North Americans and Europeans.</p><p>Therefore, Iâ€™ll admit that itâ€™s probable that Europeans are not engaged in a grand coordinated lie about M&amp;Ms. The skew of most European participants towards Denmark M&amp;Ms is compelling, especially since I was the experimenter who personally collected much of the taste response data. If they found a way to cheat, it was done well enough to exceed my own passive perception such that I didnâ€™t notice. However, based on this study, it would appear that a strongly negative â€œvomit flavorâ€ is not universally perceived and does not become apparent to non-Europeans when tasting both M&amp;Ms types side by side.</p><p>We hope this study has been illuminating! We would look forward to extensions of this work with improved participant sampling, additional M&amp;M types sourced from other continents, and deeper investigations into possible taste differences due to color.</p><p>Thank you to everyone who participated and ate M&amp;Ms in the name of science!</p><p><em>Article by Erin H. Wilson, Ph.D.[1,2,3] who decided the time between defending her dissertation and starting her next job would be best spent on this highly valuable analysis. Hopefully it is clear that this article is intended to be comedicâ€” I do not actually harbor any negative feelings towards Europeans who donâ€™t like American M&amp;Ms, but enjoyed the chance to be sassy and poke fun at our lively debates with overly-enthusiastic data analysis.</em></p><p><em>Shout out to Matt, Galen, Ameya, and Gian-Marco for assisting in data collection!</em></p><p><em>[1] Former Ph.D. student in the Paul G. Allen School of Computer Science and Engineering at the University of Washington</em></p><p><em>[2] Former visiting Ph.D. student at the Novo Nordisk Foundation Center for Biosustainability at the Technical University of Denmark</em></p><p><em>[3] Future data scientist at LanzaTech</em></p>","contentLength":24087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Progzee: Simplifying Proxy Management for Developers","url":"https://dev.to/progzee/progzee-simplifying-proxy-management-for-developers-5b22","date":1740173992,"author":"aldin","guid":8836,"unread":true,"content":"<p>As developers, we often have to navigate the complexities of web scraping, API interactions, and other tasks that require HTTP requests. </p><p>One of the most recent aspects of these tasks I have dealt with is managing IP proxies. Whether you're rotating proxies to avoid rate limits or ensuring your requests come from different IP addresses, proxy management can quickly become a headache.</p><p>Hence the Progzee, a Python library designed to simplify IP proxy usage and rotation, making your life as a developer significantly easier.</p><p>Progzee is a Python library that simplifies the use of IP proxies in HTTP requests. It offers a simplified approach to proxy management, allowing developers to focus on their core tasks rather than getting bogged down by the intricacies of proxy rotation and configuration. </p><p>With simple features like config file support, CLI integration, and automatic retries for failed requests, Progzee is a neat tool for anyone working with proxies.</p><ul><li>: Automatically rotate through a list of proxies to distribute your requests.</li><li>: Easily manage your proxies and settings through a simple  file.</li><li>: Perform quick tasks like updating proxies or fetching data directly from the command line.</li><li>: Automatically retries failed requests with the next proxy in the rotation.</li></ul><h3>\n  \n  \n  Simplifying Proxy Management\n</h3><p>Managing proxies can be a tedious task. You need to keep track of multiple IP addresses, handle failures, and ensure that your requests are distributed evenly. Progzee abstracts away these complexities, providing a clean and intuitive interface for proxy management.</p><p>For example, initializing Progzee with a list of proxies is as simple as:</p><div><pre><code></code></pre></div><p>If you prefer using a configuration file, Progzee has you covered:</p><div><pre><code></code></pre></div><p>And for those who love the command line, Progzee offers CLI support:</p><div><pre><code>\nprogzee update-proxies \nprogzee fetch </code></pre></div><p>While Progzee is a handy tool, it's essential to emphasize the importance of ethical use. The library is designed for legitimate purposes such as educational projects, testing, and lawful API interactions. Misusing Progzee for activities like unauthorized scraping, bypassing rate limits, or engaging in malicious activities is strictly prohibited.</p><p>The disclaimer in the README file is clear:</p><blockquote><p>This tool is intended for ethical use cases only, including educational purposes, testing, and legitimate API interactions.</p></blockquote><p>As developers, we have a responsibility to use our tools ethically and in compliance with all applicable laws and regulations. Progzee is no exception. Always ensure that your usage complies with the Terms of Service of the APIs you interact with.</p><ol><li>: By automating proxy rotation and error handling, Progzee saves you valuable time that you can spend on more critical aspects of your project.</li><li>: The intuitive API and configuration options make it easy to integrate Progzee into your existing workflows.</li><li>: With automatic retries and proxy rotation, Progzee ensures that your requests are more likely to succeed, even in the face of network issues or rate limits.</li></ol><h2>\n  \n  \n  Getting Started with Progzee\n</h2><p>Installing Progzee is straightforward. Simply use pip:</p><p>Here's a quick example to get you started:</p><div><pre><code></code></pre></div><p>Or, using a configuration file:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Progzee also offers CLI commands for quick tasks:</p><div><pre><code>\nprogzee update-proxies \nprogzee fetch </code></pre></div><p>Progzee is a simplification for developers who need to manage IP proxies in their HTTP requests. By simplifying proxy management, offering robust error handling, and providing easy-to-use configuration options, Progzee allows you to focus on what really matters: building great software.</p><p>However, with great power comes great responsibility. Always use Progzee ethically and in compliance with the law. Whether you're working on a web scraping project, interacting with APIs, or testing your applications, Progzee is here to make your life easierâ€”just remember to use it wisely.</p>","contentLength":3823,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Installing Golang on Windows WSL/WSL2","url":"https://dev.to/sonishivam10/installing-golang-on-windows-wslwsl2-3pn5","date":1740169411,"author":"ShivamS","guid":8810,"unread":true,"content":"<p>Official Go documentation might not provide the steps to install Go on WSL/WSL2. If you want to install GoLang and set up the development environment, follow these steps:</p><ul><li><p>Open your terminal and use the command to download the specific version.\nNote: Replace <strong>go1.24.0.linux-amd64.tar.gz</strong> with the latest version.<code>wget https://dl.google.com/go/go1.24.0.linux-amd64.tar.gz</code></p></li><li><p>Unzip:<code>sudo tar -xvf go1.24.0.linux-amd64.tar.gz</code></p></li><li><p>Move to the correct path.</p></li></ul><div><pre><code>echo \"export GOROOT=/usr/local/go\" &gt;&gt; ~/.bashrc\necho \"export GOPATH=\\$HOME/go\" &gt;&gt; ~/.bashrc\necho \"export PATH=\\$GOPATH/bin:\\$GOROOT/bin:\\$PATH\" &gt;&gt; ~/.bashrc\n</code></pre></div><ul><li><p>Refresh the terminal:</p></li><li><p>Verify the Go version.</p></li></ul>","contentLength":638,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Slime OS â€“ An open-source app launcher for RP2040 based devices","url":"https://github.com/abeisgoat/slime_os","date":1740169377,"author":"abeisgreat","guid":8895,"unread":true,"content":"<p>Hey all - this is the software part of my cyberdeck, called the Slimedeck Zero.</p><p>The Slimedeck Zero is based around this somewhat esoteric device called the PicoVision which is a super cool RP2040 (Raspberry Pi Pico) based device. It outputs relatively high-res video over HDMI while still being super fast to boot with low power consumption.</p><p>The PicoVision actually uses two RP2040 - one as a CPU and one as a GPU. This gives the CPU plenty of cycles to run bigger apps (and a heavy python stack) and lets the GPU handle some of the rendering and the complex timing HDMI requires. You can do this same thing on a single RP2040, but we get a lot of extra headroom with this double setup.</p><p>The other unique thing about the PicoVision is it has a physical double-buffer - two PSRAM chips which you manually swap between the CPU and GPU. This removes any possibility of screen tearing since you always know the buffer your CPU is writing to is not being used to generate the on-screen image.</p><p>For my cyberdeck, I took a PicoVision, hacked a QWERTY keyboard from a smart TV remote, added an expansion port, and hooked it all up to a big 5\" 800x480 screen (interlaced up from 400x240 internal resolution).</p><p>I did a whole Slimedeck Zero build video ( <a href=\"https://www.youtube.com/watch?v=rnwPmoWMGqk\" rel=\"nofollow\">https://www.youtube.com/watch?v=rnwPmoWMGqk</a> ) over on my channel but I really hope Slime OS can have a life of it's own and fit onto multiple form-factors with an ecosystem of apps.</p><p>I've tried to make it easy and fun to write apps for. There's still a lot broken / missing / tbd but it's enough of a base that, personally, it already sparks that \"programming is fun again\" vibe so hopefully some other folks can enjoy it!</p><p>Right now it only runs on the PicoVision but there's no reason it couldn't run on RP2350s or other hardware - but for now I'm more interested in adding more input types (we're limited to the i2c TV remote keyboard I hacked together) and fleshing out the internal APIs so they're stable enough to make apps for it!</p>","contentLength":1969,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43132482"},{"title":"How I built an AI-Powered Code Reviewer (and you can too).","url":"https://dev.to/manasmoon_/how-i-built-an-ai-powered-code-reviewer-and-you-can-too-2fk4","date":1740168469,"author":"Manas Moon","guid":8809,"unread":true,"content":"<p>AI is flipping the game for developers, and honestly, </p><p>I got fed up of seeing people waste hours debugging code what a machine could do it in seconds. So, I thoughtâ€”why not build an AI (agent type program) that does the boring stuff.</p><p>It all started when I was working on a project and constantly had to review my own code. While AI-powered coding assistants like GitHub Copilot help with writing code, I wondered, <em>why isnâ€™t there an AI to review my code?</em> Thatâ€™s when I decided to build one. So yeah, AI-powered code review isnâ€™t just a convenienceâ€”itâ€™s a lifesaver.</p><ol><li>Create a command-line tool where I can input a code snippet.</li><li><p>Use OpenAIâ€™s GPT-4 to analyze and review the code.</p><p><em>(If you donâ€™t know where to find your secret API key: <a href=\"https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key\" rel=\"noopener noreferrer\"></a>.</em></p></li><li><p>Return a detailed review with suggestions, best practices, and potential bug fixes.</p></li></ol><p>After some research, I decided on the following stack:</p><ul><li> Python (Standalone CLI tool)</li><li> OpenAIâ€™s GPT-4 API</li><li> Terminal-based command-line application</li></ul><p><em>You can subscribe to <a href=\"https://codexai.substack.com/\" rel=\"noopener noreferrer\"></a>  for more such cool AI prompts.</em></p><h2>\n  \n  \n  Hereâ€™s your premium prompt:\n</h2><div><pre><code>Please generate a Python script for a terminal-based AI code reviewer application that uses OpenAI's GPT model. \nThe script should allow users to paste their code into the terminal, and upon submitting it, receive a detailed review of their code. \n\nThe review should include feedback on:\n\n1. Code Quality\n2. Best Practices\n3. Potential Bugs\n4. Performance Improvements\n5. Security Concerns\n\nThe script should load the OpenAI API key from a .env file and use it to call OpenAI's GPT-4 model. \nThe program should allow the user to input code directly into the terminal, and when they press Enter twice, the review should be generated and displayed.\n\nThe Python script should include proper error handling and user prompts for a smooth user experience. \nAlso, ensure the code is clean, well-commented, and modular for easy understanding. \nThe review output should be structured and clear, providing actionable insights for the user.\n</code></pre></div><h2><strong>Hereâ€™s your hands-on tutorial:</strong></h2><h3><em>ğŸ“Œ Step 1: Setting Up the Project</em></h3><p>I started by creating a new Python project and installing the necessary libraries:</p><div><pre><code>ai-code-reviewer ai-code-reviewer\npython  venv venv\nvenv/bin/activate  \npip openai python-dotenv\n</code></pre></div><p>Next, I created a  file to store my OpenAI API key. You need to edit this file and add your own API key:</p><div><pre><code>OPENAI_API_KEY=your_openai_api_key_here\n</code></pre></div><h3><em>ğŸ“Œ Step 2: Writing the AI Review Logic</em></h3><p>I created  to handle the review process:</p><div><pre><code></code></pre></div><h3><em>ğŸ“Œ Step 3: Running the Code Reviewer</em></h3><p>To run the script, simply execute:</p><p>Then, paste your code into the terminal and press  to get a review.</p><p>Thatâ€™s how I built my AI-powered code reviewer! ğŸš€</p><p>Next Steps? Try running it on different code snippets and see how it performs. </p><p>Let me know if you use it! ğŸ˜Š</p><p>ğŸš§ <em>PS: Want more AI tips, tricks, and in-depth tutorials? Stay tuned for the next issue, where Iâ€™ll share something more useful for you.</em></p><p>Got a favorite AI prompt? Or an AI tool you swear by? </p><p>Let me know (@<a href=\"//www.linkedin.com/in/manasmoon\">Manas Moon</a>)â€”Iâ€™m always excited to learn new ways to use AI.</p><p><em>Once again, you can subscribe to <a href=\"https://codexai.substack.com/\" rel=\"noopener noreferrer\"></a>  for more such cool AI prompts.</em></p>","contentLength":3102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learn how to set up and deploy apps to your own VPS or bare metal using Sidekick. Sidekick is an alternative to Kamal and Coolify, with over 6.5k GitHub stars","url":"https://dev.to/pmbanugo/learn-how-to-set-up-and-deploy-apps-to-your-own-vps-or-bare-metal-using-sidekick-sidekick-is-an-42od","date":1740166290,"author":"Peter Mbanugo","guid":8788,"unread":true,"content":"<h2>Self-hosting on bare metal and Cloud VM - Deploy like a Pro with Sidekick</h2>","contentLength":73,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-hosting on bare metal and Cloud VM - Deploy like a Pro with Sidekick","url":"https://dev.to/pmbanugo/self-hosting-on-bare-metal-and-cloud-vm-deploy-like-a-pro-with-sidekick-2b27","date":1740165668,"author":"Peter Mbanugo","guid":8787,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Talking about Games","url":"https://towardsdatascience.com/talking-about-games/","date":1740165265,"author":"Dorian Drost","guid":8801,"unread":true,"content":"<p>Game theory is a field of research that is quite prominent in <a href=\"https://towardsdatascience.com/tag/economics/\" title=\"Economics\">Economics</a> but rather unpopular in other scientific disciplines. However, the concepts used in game theory can be of interest to a wider audience, including data scientists, statisticians, computer scientists or psychologists, to name just a few. This article is the opener to a four-chapter tutorial series on the fundamentals of game theory, so stay tuned for the upcoming articles.&nbsp;</p><p>In this article, I will explain the kinds of problems <a href=\"https://towardsdatascience.com/tag/game-theory/\" title=\"Game Theory\">Game Theory</a> deals with and introduce the main terms and concepts used to describe a game. We will see some examples of games that are typically analysed within game theory and lay the foundation for deeper insights into the capabilities of game theory in the later chapters. But before we go into the details, I want to introduce you to some applications of game theory, that show the multitude of areas game-theoretic concepts can be applied to.&nbsp;</p><h2><strong>Applications of game theory</strong></h2><p>Does it make sense to vote for a small party in an election if this party may not have a chance to win anyway? Is it worth starting a price war with your competitor who offers the same goods as you? Do you gain anything if you reduce your catch rate of overfished areas if your competitors simply carry on as before? Should you take out insurance if you believe that the government will pay for the reconstruction after the next hurricane anyway? And how should you behave in the next auction where you are about to bid on your favourite Picasso painting?&nbsp;</p><p>All these questions (and many more) live within the area of applications that can be modelled with game theory. Whenever a situation includes strategic decisions in interaction with others, game-theoretic concepts can be applied to describe this situation formally and search for decisions that are not made intuitively but that are backed by a notion of rationality. Key to all the situations above is that your decisions depend on other peopleâ€™s behaviour. If everybody agrees to conserve the overfished areas, you want to play along to preserve nature, but if you think that everybody else will continue fishing, why should you be the only one to stop? Likewise, your voting behaviour in an election might heavily depend on your assumptions about other peopleâ€™s votes. If nobody votes for that candidate, your vote will be wasted, but if everybody thinks so, the candidate doesnâ€™t have a chance at all. Maybe there are many people who say â€œI would vote for him if others vote for him tooâ€. </p><p>Similar situations can happen in very different situations. Have you ever thought about having food delivered and everybody said â€œYou donâ€™t have to order anything because of me, but if you order anyway, Iâ€™d take some french friesâ€? All these examples can be applications of game theory, so letâ€™s start understanding what game theory is all about.&nbsp;</p><p>When you hear the word , you might think of  such as Minecraft,  such as Monopoly, or  such as poker. There are some common principles to all these games: We always have some  who are allowed to do certain things determined by the gameâ€™s . For example, in poker, you can raise, check or give up. In Monopoly, you can buy a property you land on or donâ€™t buy it. What we also have is some notion of how to  the game. In poker, you have to get the best hand to win and in Monopoly, you have to be the last person standing after everybody went bankrupt. That also means that some actions are better than others in some scenarios. If you have two aces on the hand, staying in the game is better than giving up.&nbsp;</p><p>When we look at games from the perspective of game theory, we use the same concepts, just more formally.</p><p>A game consists of a set of  = {1, .., n}, where each player has a set of  and a . The set of strategies is determined by the rules of the games. For example, it could be S = {check, raise, give-up} and the player would have to decide which of these actions they want to use. The utility function u (also called ) describes how valuable a certain action of a player would be, given the actions of the other players. Every player wants to maximize their utility, but now comes the tricky part: The utility of an action of yours depends on the other playersâ€™ actions. But for them, the same applies: Their actionsâ€™ utilities depend on the actions of the other players (including yours).&nbsp;</p><p>Letâ€™s consider a well-known game to illustrate this point. In rock-paper-scissors, we have n=2 players and each player can choose between three actions, hence the strategy set is S={rock, paper, scissors} for each player. But the utility of an action depends on what the other player does. If our opponent chooses rock, the utility of paper is high (1), because paper beats rock. But if your opponent chooses scissors, the utility of paper is low (-1), because you would lose. Finally, if your opponent chooses paper as well, you reach a draw and the utility is 0.&nbsp;</p><p>Instead of writing down the utility function for each case individually, it is common to display games in a matrix like this:</p><p>The first player decides for the row of the matrix by selecting his action and the second player decides for the column. For example, if player 1 chooses paper and player 2 chooses scissors, we end up in the cell in the third column and second row. The value in this cell is the utility for both players, where the first value corresponds to player 1 and the second value corresponds to player 2. (-1,1) means that player 1 has a utility of -1 and player 2 has a utility of 1. Scissors beat paper.&nbsp;</p><p>Now we have understood the main components of a game in game theory. Let me add a few more hints on what game theory is about and what assumptions it uses to describe its scenarios.&nbsp;</p><ul><li>We often assume that the players select their actions at the same time (like in rock-paper-scissors). We call such games  games. There are also  games in which players take turns deciding on their actions (like in chess). We will consider these cases in a later chapter of this tutorial.&nbsp;</li><li>In game theory, it is typically assumed that the players  with each other so they canâ€™t come to an agreement before deciding on their actions. In rock-paper-scissors, you wouldnâ€™t want to do that anyway, but there are other games where communication would make it easier to choose an action. However, we will always assume that communication is not possible.&nbsp;</li><li>Game theory is considered a  theory, not a descriptive one. That means we will analyse games concerning the question â€œWhat would be the rational solution?â€ This may not always be what people do in a likewise situation in reality. Such descriptions of real human behaviour are part of the research field of behavioural economics, which is located on the border between <a href=\"https://towardsdatascience.com/tag/psychology/\" title=\"Psychology\">Psychology</a> and economics.&nbsp;</li></ul><p>Let us become more familiar with the main concepts of game theory by looking at some typical games that are often analyzed. Often, such games are derived from are story or scenario that may happen in the real world and require people to decide between some actions. One such story could be as follows:&nbsp;</p><p>Say we have two criminals who are suspected of having committed a crime. The police have some circumstantial evidence, but no actual proof for their guilt. Hence they question the two criminals, who now have to decide if they want to confess or deny the crime. If you are in the situation of one of the criminals, you might think that denying is always better than confessing, but now comes the tricky part: The police propose a deal to you. If you confess while your partner denies, you are considered a crown witness and will not be punished. In this case, you are free to go but your partner will go to jail for six years. Sounds like a good deal, but be aware, that the outcome also depends on your partnerâ€™s action. If you both confess, there is no crown witness anymore and you both go to jail for three years. If you both deny, the police can only use circumstantial evidence against you, which will lead to one year in prison for both you and your partner. But be aware, that your partner is offered the same deal. If you deny and he confesses, he is the crown witness and you go to jail for six years. How do you decide?</p><p>The game derived from this story is called the  and is a typical example of a game in game theory. We can visualize it as a matrix just like we did with rock-paper-scissors before and in this matrix, we easily see the dilemma the players are in. If both deny, they receive a rather low punishment. But if you assume that your partner denies, you might be tempted to confess, which would prevent you from going to jail. But your partner might think the same, and if you both confess, you both go to jail for longer. Such a game can easily make you go round in circles. We will talk about solutions to this problem in the next chapter of this tutorial. First, letâ€™s consider some more examples.&nbsp;</p><p>You and your friend want to go to a concert together. You are a fan of Bachâ€™s music but your friend favors the Russian 20th. century composer Igor Stravinsky. However, you both want to avoid being alone in any concert. Although you prefer Bach over Stravinsky, you would rather go to the Stravinsky concert with your friend than go to the Bach concert alone. We can create a matrix for this game:&nbsp;</p><p>You decide for the row by going to the Bach or Stravinsky concert and your friend decides for the column by going to one of the concerts as well. For you, it would be best if you both chose Bach. Your reward would be 2 and your friend would get a reward of 1, which is still better for him than being in the Stravinsky concert all by himself. However, he would be even happier, if you were in the Stravinsky concert together.&nbsp;</p><p>Do you remember, that we said players are not allowed to communicate before making their decision? This example illustrates why. If you could just call each other and decide where to go, this would not be a game to investigate with game theory anymore. But you canâ€™t call each other so you just have to go to any of the concerts and hope you will meet your friend there. What do you do?&nbsp;</p><p>A third example brings us to the realm of international politics. The world would be a much happier place with fewer firearms, wouldnâ€™t it? However, if nations think about disarmament, they also have to consider the choices other nations make. If the USA disarms, the Soviet Union might want to rearm, to be able to attack the USAâ€Šâ€”â€Šthat was the thinking during the Cold War, at least. Such a scenario could be described with the following matrix:&nbsp;</p><p>As you see, when both nations disarm, they get the highest reward (3 each), because there are fewer firearms in the world and the risk of war is minimized. However, if you disarm, while the opponent upgrades, your opponent is in the better position and gets a reward of 2, while you only get 0. Then again, it might have been better to upgrade yourself, which gives a reward of 1 for both players. That is better than being the only one who disarms, but not as good as it would get if both nations disarmed.&nbsp;</p><p>All these examples have one thing in common: There is no single option that is always the best. Instead, the utility of an action for one player always depends on the other playerâ€™s action, which, in turn, depends on the first playerâ€™s action and so on. Game theory is now interested in finding the optimal solution and deciding what would be the rational action; that is, the action that maximizes the expected reward. Different ideas on how exactly such a solution looks like will be part of the next chapter in this series.&nbsp;</p><p>Before continuing with finding solutions in the next chapter, let us recap what we have learned so far.&nbsp;</p><ul><li>A game consists of , that decide for , which have a  or .&nbsp;</li><li>The utility/reward of an action  on the other playersâ€™ actions.&nbsp;</li><li>In  games, players decide for their actions simultaneously. In  games, they take turns.&nbsp;</li><li>The  is a very popular example of a game in game theory.</li><li>Games become increasingly interesting if there is no single action that is better than any other.&nbsp;</li></ul><p>Now that you are familiar with how games are described in game theory, you can check out the next chapter to learn how to find solutions for games in game theory.&nbsp;</p><p>The topics introduced here are typically covered in standard textbooks on game theory. I mainly used this one, which is written in German though:&nbsp;</p><ul><li>Bartholomae, F., &amp; Wiens, M. (2016). <em>Spieltheorie. Ein anwendungsorientiertes Lehrbuch</em>. Wiesbaden: Springer Fachmedien Wiesbaden.</li></ul><p>An alternative in English language could be this one:&nbsp;</p><ul><li>Espinola-Arredondo, A., &amp; MuÃ±oz-Garcia, F. (2023). <em>Game Theory: An Introduction with Step-by-step Examples</em>. Springer Nature.</li></ul><p>Game theory is a rather young field of research, with the first main textbook being this one:&nbsp;</p><ul><li>Von Neumann, J., &amp; Morgenstern, O. (1944). Theory of games and economic behavior.</li></ul><p><a href=\"https://medium.com/@doriandrost\"></a><em> to be notified of my future posts.</em></p>","contentLength":12977,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Rocket Companies modernized their data science solution on AWS","url":"https://aws.amazon.com/blogs/machine-learning/how-rocket-companies-modernized-their-data-science-solution-on-aws/","date":1740163546,"author":"Dian Xu, Joel Hawkins","guid":8748,"unread":true,"content":"<p><em>This post was written with Dian Xu and Joel Hawkins of Rocket Companies.</em></p><p><a href=\"https://rocket.com/\" target=\"_blank\" rel=\"noopener\">Rocket Companies</a> is a Detroit-based FinTech company with a mission to â€œHelp Everyone Homeâ€. With the current housing shortage and affordability concerns, Rocket simplifies the homeownership process through an intuitive and AI-driven experience. This comprehensive framework streamlines every step of the homeownership journey, empowering consumers to search, purchase, and manage home financing effortlessly. Rocket integrates home search, financing, and servicing in a single environment, providing a seamless and efficient experience.</p><p>The Rocket brand is a synonym for offering simple, fast, and trustworthy digital solutions for complex transactions. Rocket is dedicated to helping clients realize their dream of homeownership and financial freedom. Since its inception, Rocket has grown from a single mortgage lender to an network of businesses that creates new opportunities for its clients.</p><p>Rocket takes a complicated process and uses technology to make it simpler. Applying for a mortgage can be complex and time-consuming. Thatâ€™s why we use advanced technology and data analytics to streamline every step of the homeownership experience, from application to closing. By analyzing a wide range of data points, weâ€™re able to quickly and accurately assess the risk associated with a loan, enabling us to make more informed lending decisions and get our clients the financing they need.</p><p>Our goal at Rocket is to provide a personalized experience for both our current and prospective clients. Rocketâ€™s diverse product offerings can be customized to meet specific client needs, while our team of skilled bankers must match with the best client opportunities that align with their skills and knowledge. Maintaining strong relationships with our large, loyal client base and hedge positions to cover financial obligations is key to our success. With the volume of business we do, even small improvements can have a significant impact.</p><p>In this post, we share how we modernized Rocketâ€™s data science solution on AWS to increase the speed to delivery from eight weeks to under one hour, improve operational stability and support by reducing incident tickets by over 99% in 18 months, power 10 million automated data science and AI decisions made daily, and provide a seamless data science development experience.</p><h2>Rocketâ€™s legacy data science environment challenges</h2><p>Rocketâ€™s previous data science solution was built around Apache Spark and combined the use of a legacy version of the Hadoop environment and vendor-provided Data Science Experience development tools. The Hadoop environment was hosted on <a href=\"http://aws.amazon.com/ec2\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Compute Cloud</a> (Amazon EC2) servers, managed in-house by Rocketâ€™s technology team, while the data science experience infrastructure was hosted on premises. Communication between the two systems was established through Kerberized Apache Livy (HTTPS) connections over <a href=\"https://aws.amazon.com/privatelink/\" target=\"_blank\" rel=\"noopener\">AWS PrivateLink</a>.</p><p>Data exploration and model development were conducted using well-known machine learning (ML) tools such as Jupyter or Apache Zeppelin notebooks. Apache Hive was used to provide a tabular interface to data stored in HDFS, and to integrate with Apache Spark SQL. Apache HBase was employed to offer real-time key-based access to data. Model training and scoring was performed either from Jupyter notebooks or through jobs scheduled by Apacheâ€™s Oozie orchestration tool, which was part of the Hadoop implementation.</p><p>Despite the benefits of this architecture, Rocket faced challenges that limited its effectiveness:</p><ul><li><strong>Accessibility limitations:</strong> The data lake was stored in HDFS and only accessible from the Hadoop environment, hindering integration with other data sources. This also led to a backlog of data that needed to be ingested.</li><li><strong>Steep learning curve for data scientists:</strong> Many of Rocketâ€™s data scientists did not have experience with Spark, which had a more nuanced programming model compared to other popular ML solutions like scikit-learn. This created a challenge for data scientists to become productive.</li><li><strong>Responsibility for maintenance and troubleshooting:</strong> Rocketâ€™s DevOps/Technology team was responsible for all upgrades, scaling, and troubleshooting of the Hadoop cluster, which was installed on bare EC2 instances. This resulted in a backlog of issues with both vendors that remained unresolved.</li><li><strong>Balancing development vs. production demands:</strong> Rocket had to manage work queues between development and production, which were always competing for the same resources.</li><li> Rocket sought to support more real-time and streaming inferencing use cases, but this was limited by the capabilities of MLeap for real-time models and Spark Streaming for streaming use cases, which were still experimental at that time.</li><li>Inadequate data security and DevOps support â€“ The previous solution lacked robust security measures, and there was limited support for development and operations of the data science work.</li></ul><p>Rocketâ€™s legacy data science architecture is shown in the following diagram.</p><p>The diagram depicts the flow; the key components are detailed below:</p><ol><li> Data is ingested into the system using Attunity data ingestion in Spark SQL.</li><li><strong>Data Storage and Processing:</strong> All compute is done as Spark jobs inside of a Hadoop cluster using Apache Livy and Spark. Data is stored in HDFS and is accessed via Hive, which provides a tabular interface to the data and integrates with Spark SQL. HBase is employed to offer real-time key-based access to data.</li><li> Data exploration and model development are conducted using tools such as Jupyter or Orchestration, which communicate with the Spark server over Kerberized Livy connection.</li><li><strong>Model Training and Scoring:</strong> Model training and scoring is performed either from Jupyter notebooks or through jobs scheduled by Apacheâ€™s Oozie orchestration tool, which is part of the Hadoop implementation.</li></ol><h2>Rocketâ€™s migration journey</h2><p>At Rocket, we believe in the power of continuous improvement and constantly seek out new opportunities. One such opportunity is using data science solutions, but to do so, we must have a strong and flexible data science environment.</p><p>To address the legacy data science environment challenges, Rocket decided to migrate its ML workloads to the <a href=\"https://aws.amazon.com/sagemaker-ai/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker AI</a> suite. This would allow us to deliver more personalized experiences and understand our customers better. To promote the success of this migration, we collaborated with the AWS team to create automated and intelligent digital experiences that demonstrated Rocketâ€™s understanding of its clients and kept them connected.</p><p>We implemented an AWS multi-account strategy, standing up <a href=\"https://aws.amazon.com/sagemaker-ai/studio/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Studio</a> in a build account using a network-isolated Amazon VPC. This allows us to separate development and production environments, while also improving our security stance.</p><p>We moved our new work to SageMaker Studio and our legacy Hadoop workloads to <a href=\"https://aws.amazon.com/emr/\" target=\"_blank\" rel=\"noopener\">Amazon EMR</a>, connecting to the old Hadoop cluster using Livy and <a href=\"https://aws.amazon.com/sagemaker-ai/notebooks/\" target=\"_blank\" rel=\"noopener\">SageMaker notebooks</a> to ease the transition. This gives us access to a wider range of tools and technologies, enabling us to choose the most appropriate ones for each problem weâ€™re trying to solve.</p><p>SageMaker AI has been instrumental in empowering our data science community with the flexibility to choose the most appropriate tools and technologies for each problem, resulting in faster development cycles and higher model accuracy. With SageMaker Studio, our data scientists can seamlessly develop, train, and deploy models without the need for additional infrastructure management.</p><p>As a result of this modernization effort, SageMaker AI enabled Rocket to scale our data science solution across Rocket Companies and integrate using a hub-and-spoke model. The ability of SageMaker AI to automatically provision and manage instances has allowed us to focus on our data science work rather than infrastructure management, increasing the number of models in production by five times and data scientistsâ€™ productivity by 80%.</p><p>Our data scientists are empowered to use the most appropriate technology for the problem at hand, and our security stance has improved. Rocket can now compartmentalize data and compute, as well as compartmentalize development and production. Additionally, we are able to provide model tracking and lineage using <a href=\"https://aws.amazon.com/sagemaker-ai/experiments/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Experiments</a> and artifacts discoverable using the SageMaker model registry and <a href=\"https://aws.amazon.com/sagemaker-ai/feature-store/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Feature Store</a>. All the data science work has now been migrated onto SageMaker, and all the old Hadoop work has been migrated to Amazon EMR.</p><p>Overall, SageMaker AI has played a critical role in enabling Rocketâ€™s modernization journey by building a more scalable and flexible ML framework, reducing operational burden, improving model accuracy, and accelerating deployment times.</p><p>The successful modernization allowed Rocket to overcome our previous limitations and better support our data science efforts. We were able to improve our security stance, make work more traceable and discoverable, and give our data scientists the flexibility to choose the most appropriate tools and technologies for each problem. This has helped us better serve our customers and drive business growth.</p><p>Rocketâ€™s new data science solution architecture on AWS is shown in the following diagram.</p><p>The solution consists of the following components:</p><ol><li>Data is ingested into the data account from on-premises and external sources.</li><li> Raw data is refined into consumable layers (raw, processed, conformed, and analytical) using a combination of <a href=\"https://aws.amazon.com/glue\" target=\"_blank\" rel=\"noopener\">AWS Glue</a> extract, transform, and load (ETL) jobs and EMR jobs.</li><li> Refined data is registered in the data accountâ€™s AWS Glue Data Catalog and exposed to other accounts via Lake Formation. Analytic data is stored in <a href=\"http://aws.amazon.com/redshift\" target=\"_blank\" rel=\"noopener\">Amazon Redshift</a>. Lake Formation makes this data available to both the build and compute accounts. For the build account, access to production data is restricted to read-only.</li><li> Data science development is done using SageMaker Studio. Data engineering development is done using AWS Glue Studio. Both disciplines have access to Amazon EMR for Spark development. Data scientists have access to the entire SageMaker ecosystem in the build account.</li><li>&nbsp;SageMaker trained models developed in the build account are registered with an MLFlow instance. Code artifacts for both data science activities and data engineering activities are stored in Git. Deployment initiation is controlled as part of CI/CD.</li><li>&nbsp;We have a number of workflow triggers. For online scoring, we typically provide an external-facing endpoint using Amazon EKS with Istio. We have numerous jobs that are launched by <a href=\"http://aws.amazon.com/lambda\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> functions that in turn are triggered by timers or events. Processes that run may include AWS Glue ETL jobs, EMR jobs for additional data transformations or model training and scoring activities, or SageMaker pipelines and jobs performing training or scoring activities.</li></ol><p>Weâ€™ve evolved a long way in modernizing our infrastructure and workloads. We started our journey supporting six business channels and 26 models in production, with dozens in development. Deployment times stretched for months and required a team of three system engineers and four ML engineers to keep everything running smoothly. Despite the support of our internal DevOps team, our issue backlog with the vendor was an unenviable 200+.</p><p>Today, we are supporting nine organizations and over 20 business channels, with a whopping 210+ models in production and many more in development. Our average deployment time has gone from months to just weeksâ€”sometimes even down to mere days! With just one part-time ML engineer for support, our average issue backlog with the vendor is practically non-existent. We now support over 120 data scientists, ML engineers, and analytical roles. Our framework mix has expanded to include 50% SparkML models and a diverse range of other ML frameworks, such as PyTorch and scikit-learn. These advancements have given our data science community the power and flexibility to tackle even more complex and challenging projects with ease.</p><p>The following table compares some of our metrics before and after migration.</p><table border=\"1px\" cellspacing=\"0\" cellpadding=\"10px\"><thead><tr></tr></thead><tbody><tr><td>New data ingestion project took 4â€“8 weeks</td><td>Data-driven ingestion takes under one hour</td></tr><tr><td>Operation Stability and Supportability</td><td>Over a hundred incidents and tickets in 18 months</td><td>Fewer incidents: one per 18 months</td></tr><tr><td>Data scientists spent 80% of their time waiting on their jobs to run</td><td>Seamless data science development experience</td></tr><tr><td>Powers 10 million automated data science and AI decisions made daily</td></tr></tbody></table><p>Throughout the journey of modernizing our data science solution, weâ€™ve learned valuable lessons that we believe could be of great help to other organizations who are planning to undertake similar endeavors.</p><p>First, weâ€™ve come to realize that managed services can be a game changer in optimizing your data science operations.</p><p>The isolation of development into its own account while providing read-only access to production data is a highly effective way of enabling data scientists to experiment and iterate on their models without putting your production environment at risk. This is something that weâ€™ve achieved through the combination of SageMaker AI and Lake Formation.</p><p>Another lesson we learned is the importance of training and onboarding for teams. This is particularly true for teams that are moving to a new environment like SageMaker AI. Itâ€™s crucial to understand the best practices of utilizing the resources and features of SageMaker AI, and to have a solid understanding of how to move from notebooks to jobs.</p><p>Lastly, we found that although Amazon EMR still requires some tuning and optimization, the administrative burden is much lighter compared to hosting directly on Amazon EC2. This makes Amazon EMR a more scalable and cost-effective solution for organizations who need to manage large data processing workloads.</p><p>This post provided overview of the successful partnership between AWS and Rocket Companies. Through this collaboration, Rocket Companies was able to migrate many ML workloads and implement a scalable ML framework. Ongoing with AWS, Rocket Companies remains committed to innovation and staying at the forefront of customer satisfaction.</p><p>Donâ€™t let legacy systems hold back your organizationâ€™s potential. Discover how AWS can assist you in modernizing your data science solution and achieving remarkable results, similar to those achieved by Rocket Companies.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/21/Dian-Xu_Picture-225x300-1.jpg\" alt=\"\" width=\"100\" height=\"133\">&nbsp;is the Senior Director of Engineering in Data at Rocket Companies, where she leads transformative initiatives to modernize enterprise data platforms and foster a collaborative, data-first culture. Under her leadership, Rocketâ€™s data science, AI &amp; ML platforms power billions of automated decisions annually, driving innovation and industry disruption. A passionate advocate for Gen AI and cloud technologies, Xu is also a sought-after speaker at global forums, inspiring the next generation of data professionals. Outside of work, she channels her love of rhythm into dancing, embracing styles from Bollywood to Bachata as a celebration of cultural diversity.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/21/Joel_Pic-240x300-1.jpg\" alt=\"\" width=\"100\" height=\"125\">&nbsp;is a Principal Data Scientist at Rocket Companies, where he is responsible for the data science and MLOps platform. Joel has decades of experience developing sophisticated tooling and working with data at large scales. A driven innovator, he works hand in hand with data science teams to ensure that we have the latest technologies available to provide cutting edge solutions. In his spare time, he is an avid cyclist and has been known to dabble in vintage sports car restoration.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/21/SajjanAVS-200x300-1.jpeg\" alt=\"\" width=\"100\" height=\"150\"><strong>Venkata Santosh Sajjan Alla</strong>&nbsp;is a Senior Solutions Architect at AWS Financial Services. He partners with North American FinTech companies like Rocket and other financial services organizations to drive cloud and AI strategy, accelerating AI adoption at scale. With deep expertise in AI &amp; ML, Generative AI, and cloud-native architecture, he helps financial institutions unlock new revenue streams, optimize operations, and drive impactful business transformation. Sajjan collaborates closely with Rocket Companies to advance its mission of building an AI-fueled homeownership platform&nbsp;to&nbsp;Help Everyone Home. Outside of work, he enjoys traveling, spending time with his family, and is a proud father to his daughter.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/08/19/AlakEswaradass.jpg\" alt=\"Alak Eswaradass\" width=\"100\" height=\"137\"> is a Principal Solutions Architect at AWS based in Chicago, IL. She is passionate about helping customers design cloud architectures using AWS services to solve business challenges and is enthusiastic about solving a variety of ML use cases for AWS customers. When sheâ€™s not working, Alak enjoys spending time with her daughters and exploring the outdoors with her dogs.</p>","contentLength":16660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ğŸ““ How to Use Jupyter Notebooks in VSCode with Poetry Virtual Environments ğŸš€","url":"https://dev.to/dorinandreidragan/how-to-use-jupyter-notebooks-in-vscode-with-poetry-virtual-environments-2kml","date":1740160092,"author":"dorinandreidragan","guid":8729,"unread":true,"content":"<p>If you're a developer using Jupyter Notebooks and Poetry, you might face an issue where VSCode doesn't automatically recognize the Poetry virtual environments. This guide will show you how to solve this problem by updating the VSCode user settings accordingly.</p><h2>\n  \n  \n  Step 1: Create a New Project with Poetry\n</h2><p>Navigate to your project directory and create a new project using Poetry:</p><div><pre><code>my_jupyter_project\nmy_jupyter_project\npoetry init\n</code></pre></div><p>Follow the prompts to set up your  file.</p><h2>\n  \n  \n  Step 2: Install and Activate the Virtual Environment\n</h2><p>To create and activate the virtual environment, run:</p><div><pre><code>poetry poetry shell\n</code></pre></div><p>This will create a virtual environment and activate it, isolating your project's dependencies.</p><h2>\n  \n  \n  Step 3: Open VSCode and Install the Jupyter Extension\n</h2><p>Open your project folder in VSCode:</p><p>Make sure to install the Jupyter extension in VSCode if you haven't already. You can find it in the Extensions view by searching for \"Jupyter\".</p><h2>\n  \n  \n  Step 4: Configure VSCode User Settings âš™ï¸\n</h2><p>Ensure that VSCode is configured to recognize Poetry virtual environments. Add the following settings to your  file:</p><div><pre><code></code></pre></div><p>Replace  with your actual username.</p><h2>\n  \n  \n  Step 5: Create and Run a Jupyter Notebook ğŸ““\n</h2><p>With the virtual environment set up, you can now create a Jupyter Notebook:</p><ol><li>Open the Command Palette () and type <code>Jupyter: Create New Blank Notebook</code>.</li><li>Select the appropriate kernel (your Poetry virtual environment) from the kernel picker in the top-right corner of the notebook.</li><li>Start coding in your Jupyter Notebook and enjoy the power of Poetry and Jupyter combined! ğŸ‰</li></ol><p>By following these steps, you can efficiently manage your Jupyter Notebook projects using Poetry within VSCode, ensuring that your dependencies are well-organized and isolated. Happy coding! ğŸ’»âœ¨</p>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is LangGraph and How to Use It for Building AI Agents","url":"https://dev.to/juanstoppa/what-is-langgraph-and-how-to-use-it-for-building-ai-agents-4bj2","date":1740159229,"author":"Juan Stoppa","guid":8728,"unread":true,"content":"<p>I keep finding myself going back to the LangChain documentation to figure out how to use LangGraph. While the documentation is comprehensive, it can be overwhelming to navigate, especially when you're trying to build advanced AI agents.</p><p>This guide is my attempt to consolidate the key concepts and practical implementations of LangGraph in one place. Whether you're building a conversational agent that needs to remember context, a multi-step reasoning agent, or a complex workflow that coordinates multiple AI components, LangGraph provides the framework to make it happen. I created this reference for myself but I hope it helps others who want a more straightforward explanation of how to use LangGraph effectively.</p><p>LangGraph is a framework that brings state machines to Large Language Model (LLM) applications. It's particularly useful when you need to build complex, stateful applications with LLMs. The framework allows you to structure your application as a graph, where each node represents a specific task or state, and edges represent transitions between states.</p><p>Let's start with a simple example to understand the core concepts: we are developing a simple agent that collects information and  processes it, the actions  of collecting and processing information are just fixed actions but they can be replaced with more complex actions.</p><div><pre><code></code></pre></div><p>This example shows the basic structure of a LangGraph application:</p><ol><li>Define your state using TypedDict, it contains the information that the workflow will need to keep track of.</li><li>Create functions for each state, these functions are the actions that the workflow will perform.</li><li>Build a graph with nodes and edges, the nodes are the states and the edges are the transitions between them.</li><li>Compile the graph into a runnable application, this will create a callable object that can be invoked with an initial state.</li><li>I also added a simple way to save the graph visualization as a PNG file, this will work if you are running this example locally and should save a file that will show the graph structure like below.</li></ol><p>The graph is a good way to understand the workflow, it shows the nodes and the edges between them, the entry and finish points and the state of the workflow.</p><p>This particular example is not very useful but it shows the core concepts of LangGraph, you can simply replace the fixed actions with more complex ones and build a useful agent.</p><h2>\n  \n  \n  Serving LangGraph as a Web Service\n</h2><p>While LangGraph itself doesn't include built-in server capabilities, you can easily create a web service using <a href=\"https://fastapi.tiangolo.com/\" rel=\"noopener noreferrer\">FastAPI</a> to serve your LangGraph workflows. Below I have modified the previous example to add a simple FastAPI server that allows you to run the workflow from a web interface.</p><div><pre><code></code></pre></div><p>You can run this locally or using the <a href=\"https://huggingface.co/\" rel=\"noopener noreferrer\">Hugging Face</a> space below, this is the URL to access the swagger API for this example <a href=\"https://jstoppa-langgraph-basic-example-api.hf.space/docs\" rel=\"noopener noreferrer\">https://jstoppa-langgraph-basic-example-api.hf.space/docs</a>, the API has an end point to run the agent and it returns the messages we've seen in our previous example (see below the results). In simple words, the API has an end point to run the agent and it returns the messages we've seen in our previous example.</p><h2>\n  \n  \n  Making it all work with a more interesting example\n</h2><p>We are now going to create a more interesting example, an AI agent that does code reviews, this is far from a production-ready agent but it will give us a better understanding of how to use LangGraph.</p><p>The screenshot below shows the interface for the code review agent, the user can enter a code and the agent will return a report with the code analysis. This interface uses the <a href=\"https://www.gradio.app/\" rel=\"noopener noreferrer\">Gradio library</a> to create a simple web interface, this saves a lot of time compared to building a full web app.</p><p>The full code is provided after this but the most impportant part of the example is the graph, it shows the nodes and the edges between them, the entry and finish points and the state of the workflow, this is how the graph will look through the code and analyse the code with different agents that are especialised on different aspects. This is a very similar apporach we've seen in the previous example but it contains more actions and the actions do use LLMs to analyse the code.</p><div><pre><code></code></pre></div><p>the full code for the agent is below and it can also be found in Hugging Face below.<a href=\"https://huggingface.co/spaces/jstoppa/langgraph_agent_code_reviewer\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fimg.shields.io%2Fbadge%2F%25F0%259F%25A4%2597%2520Run%2520in-Hugging%2520Face-blue\" alt=\"Run example in Hugging Face\" width=\"144\" height=\"20\"></a></p><div><pre><code></code></pre></div><p>LangGraph makes it easier to build AI agents that need to manage complex workflows. The graph-based approach keeps things organised and flexible, especially when dealing with multi-step processes or memory.</p><p>Even though LangGraph doesnâ€™t come with built-in server features, it works well with FastAPI and other frameworks to serve agents as APIs. Whether youâ€™re building a chatbot, a code reviewer, or something else entirely, it gives you a solid foundation to work with.</p><p>Iâ€™m still experimenting and learning, so Iâ€™ll keep updating this post as I find better ways to use LangGraph. If youâ€™ve built something cool with it or have any questions, let me knowâ€”happy to chat!</p><p>I hope you like this article, if you want to hear more follow me on X at <a href=\"https://x.com/juanstoppa\" rel=\"noopener noreferrer\">@juanstoppa</a> where I regularly post about AI </p>","contentLength":5048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Daniel Roy Greenfeld: TIL: Undecorating a functools.wraps decorated function","url":"https://daniel.feldroy.com/posts/til-2025-02-unwrapping-a-wrapped-function","date":1740158694,"author":"","guid":8783,"unread":true,"content":"<article>Another reason to use functools.wraps!</article>","contentLength":38,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NVIDIAâ€™s New AI: The Age of Real Time Game Making Is Here!","url":"https://www.youtube.com/watch?v=FpZ_6bxx5v8","date":1740158488,"author":"Two Minute Papers","guid":8727,"unread":true,"content":"<article>â¤ï¸ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\nğŸ“ Magic 1-For-1:\nhttps://magic-141.github.io/Magic-141/\nhttps://github.com/Open-Magic-Video/Magic-1-For-1\nhttps://arxiv.org/abs/2502.07701v1\n\nğŸ“ Phantom: https://phantom-video.github.io/Phantom/\n\nğŸ“ Relighting paper: https://bujiazi.github.io/light-a-video.github.io/\n\nğŸ“ Stepfun:\nhttps://github.com/stepfun-ai/Step-Video-T2V\nhttps://yuewen.cn/videos\nhttps://arxiv.org/abs/2502.10248\nhttps://huggingface.co/stepfun-ai/stepvideo-t2v\n\nğŸ“ My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nğŸ™ We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: FelÃ­cia Zsolnai-FehÃ©r - http://felicia.hu</article>","contentLength":1360,"flags":null,"enclosureUrl":"https://www.youtube.com/v/FpZ_6bxx5v8?version=3","enclosureMime":"","commentsUrl":null},{"title":"AWS and DXC collaborate to deliver customizable, near real-time voice-to-voice translation capabilities for Amazon Connect","url":"https://aws.amazon.com/blogs/machine-learning/aws-and-dxc-collaborate-to-deliver-customizable-near-real-time-voice-to-voice-translation-capabilities-for-amazon-connect/","date":1740157698,"author":"Milos Cosic","guid":8717,"unread":true,"content":"<p>Providing effective multilingual customer support in global businesses presents significant operational challenges. Through collaboration between AWS and DXC Technology, weâ€™ve developed a scalable voice-to-voice (V2V) translation prototype that transforms how contact centers handle multi-lingual customer interactions.</p><p>In this post, we discuss how AWS and DXC used <a href=\"https://aws.amazon.com/connect/\" target=\"_blank\" rel=\"noopener\">Amazon Connect</a> and other AWS AI services to deliver near real-time V2V translation capabilities.</p><h2>Challenge: Serving customers in multiple languages</h2><p>In Q3 2024, DXC Technology approached AWS with a critical business challenge: their global contact centers needed to serve customers in multiple languages without the exponential cost of hiring language-specific agents for the lower volume languages. Previously, DXC had explored several existing alternatives but found limitations in each approach â€“ from communication constraints to infrastructure requirements that impacted reliability, scalability, and operational costs. DXC and AWS decided to organize a focused hackathon where DXC and AWS Solution Architects collaborated to:</p><ul><li>Define essential requirements for real-time translation</li><li>Establish latency and accuracy benchmarks</li><li>Create seamless integration paths with existing systems</li><li>Develop a phased implementation strategy</li><li>Prepare and test an initial proof of concept setup</li></ul><p>For DXC, this prototype was used as an enabler, allowing technical talent maximization, operational transformation, and cost improvements through:</p><ul><li>Best technical expertise delivery â€“ Hiring and matching agents based on technical knowledge rather than spoken language, making sure customers get top technical support regardless of language barriers</li><li>Global operational flexibility â€“ Removing geographical and language constraints in hiring, placement, and support delivery while maintaining consistent service quality across all languages</li><li>Cost reduction â€“ Eliminating multi-language expertise premiums, specialized language training, and infrastructure costs through pay-per-use translation model</li><li>Similar experience to native speakers â€“ Maintaining natural conversation flow with near real-time translation and audio feedback, while delivering premium technical support in customerâ€™s preferred language</li></ul><p>The Amazon Connect V2V translation prototype uses AWS advanced speech recognition and machine translation technologies to enable real-time conversation translation between agents and customers, allowing them to speak in their preferred languages while having natural conversations. It consists of the following key components:</p><ul><li>Speech recognition â€“ The customerâ€™s spoken language is captured and converted into text using <a href=\"https://aws.amazon.com/transcribe/\" target=\"_blank\" rel=\"noopener\">Amazon Transcribe</a>, which serves as the speech recognition engine. The transcript (text) is then fed into the machine translation engine.</li><li>Machine translation â€“ <a href=\"https://aws.amazon.com/translate/\" target=\"_blank\" rel=\"noopener\">Amazon Translate</a>, the machine translation engine, translates the customerâ€™s transcript into the agentâ€™s preferred language in near real time. The translated transcript is converted back into speech using <a href=\"https://aws.amazon.com/polly/\" target=\"_blank\" rel=\"noopener\">Amazon Polly</a>, which serves as the text-to-speech engine.</li><li>Bidirectional translation â€“ The process is reversed for the agentâ€™s response, translating their speech into the customerâ€™s language and delivering the translated audio to the customer.</li><li>Seamless integration â€“ The V2V translation sample project integrates with Amazon Connect, enabling agents to handle customer interactions in multiple languages without any additional effort or training, using the <a href=\"https://github.com/amazon-connect/amazon-connect-streams\" target=\"_blank\" rel=\"noopener\">Amazon Connect Streams JS</a> and <a href=\"https://github.com/aws/connect-rtc-js\" target=\"_blank\" rel=\"noopener\">Amazon Connect RTC JS</a> libraries.</li></ul><p>The prototype can be extended with other AWS AI services to further customize the translation capabilities. Itâ€™s open source and ready for customization to meet your specific needs.</p><p>The following diagram illustrates the solution architecture.</p><p>The following screenshot illustrates a sample agent web application.</p><p>The user interface consists of three sections:</p><ul><li>Contact Control Panel â€“ A softphone client using Amazon Connect</li><li>Customer Controls â€“ Customer-to-agent interaction controls, including Transcribe Customer Voice, Translate Customer Voice, and Synthesize Customer Voice</li><li>Agent controls â€“ Agent-to-customer interaction controls, including Transcribe Agent Voice, Translate Agent Voice, and Synthesize Agent Voice</li></ul><h2>Challenges when implementing near real-time voice translation</h2><p>The Amazon Connect V2V sample project was designed to minimize the audio processing time from the moment the customer or agent finishes speaking until the translated audio stream is started. However, even with the shortest audio processing time, the user experience still doesnâ€™t match the experience of a real conversation when both are speaking the same language. This is due to the specific pattern of the customer only hearing the agentâ€™s translated speech, and the agent only hearing the customerâ€™s translated speech. The following diagram displays that pattern.</p><p>The example workflow consists of the following steps:</p><ol><li>The customer starts speaking in their own language, and speaks for 10 seconds.</li><li>Because the agent only hears the customerâ€™s translated speech, the agent first hears 10 seconds of silence.</li><li>When customer finishes speaking, the audio processing time takes 1â€“2 seconds, during which time both the customer and agent hear silence.</li><li>The customerâ€™s translated speech is streamed to the agent. During that time, the customer hears silence.</li><li>When the customerâ€™s translated speech playback is complete, the agent starts speaking, and speaks for 10 seconds.</li><li>Because customer only hears the agentâ€™s translated speech, the customer hears 10 seconds of silence.</li><li>When the agent finishes speaking, the audio processing time takes 1â€“2 seconds, during which time both the customer and agent hear silence.</li><li>The agentâ€™s translated speech is streamed to the agent. During that time, the agent hears silence.</li></ol><p>In this scenario, the customer hears a single block of 22â€“24 seconds of a complete silence, from the moment they finished speaking until they hear the agentâ€™s translated voice. This creates a suboptimal experience, because the customer might not be certain what is happening during these 22â€“24 secondsâ€”for instance, if the agent was able to hear them, or if there was a technical issue.</p><p>In a face-to-face conversation scenario between two people that donâ€™t speak the same language, they might have another person as a translator or interpreter. An example workflow consists of the following steps:</p><ol><li>Person A speaks in their own language, which is heard by Person B and the translator.</li><li>The translator translates what Person A said to Person Bâ€™s language. The translation is heard by Person B and Person A.</li></ol><p>Essentially, Person A and Person B hear each other speaking their own language, and they also hear the translation (from the translator). Thereâ€™s no waiting in silence, which is even more important in non-face-to-face conversations (such as contact center interactions).</p><p>To optimize the customer/agent experience, the Amazon Connect V2V sample project implements audio streaming add-ons to simulate a more natural conversation experience. The following diagram illustrates an example workflow.</p><p>The workflow consists of the following steps:</p><ol><li>The customer starts speaking in their own language, and speaks for 10 seconds.</li><li>The agent hears the customerâ€™s original voice, at a lower volume (â€œStream Customer Mic to Agentâ€ enabled).</li><li>When the customer finishes speaking, the audio processing time takes 1â€“2 seconds. During that time, the customer and agent hear subtle audio feedbackâ€”contact center background noiseâ€”at a very low volume (â€œAudio Feedbackâ€ enabled).</li><li>The customerâ€™s translated speech is then streamed to the agent. During that time, the customer hears their translated speech, at a lower volume (â€œStream Customer Translation to Customerâ€ enabled).</li><li>When the customerâ€™s translated speech playback is complete, the agent starts speaking, and speaks for 10 seconds.</li><li>The customer hears the agentâ€™s original voice, at a lower volume (â€œStream Agent Mic to Customerâ€ enabled).</li><li>When the agent finishes speaking, the audio processing time takes 1â€“2 seconds. During that time, the customer and agent hear subtle audio feedbackâ€”contact center background noiseâ€”at a very low volume (â€œAudio Feedbackâ€ enabled).</li><li>The agentâ€™s translated speech is then streamed to the agent. During that time, the agent hears their translated speech, at a lower volume (â€œStream Agent Translation to Agentâ€ enabled).</li></ol><p>In this scenario, the customer hears two short blocks (1â€“2 seconds) of subtle audio feedback, instead of a single block of 22â€“24 seconds of complete silence. This pattern is much closer to a face-to-face conversation that includes a translator.</p><p>The audio streaming add-ons provide additional benefits, including:</p><ul><li>Voice characteristics â€“ In cases when the agent and customer only hear their translated and synthesized speech, the actual voice characteristics are lost. For instance, the agent canâ€™t hear if the customer was talking slow or fast, if the customer was upset or calm, and so on. The translated and synthesized speech doesnâ€™t carry over that information.</li><li>Quality assurance â€“ In cases when call recording is enabled, only the customerâ€™s original voice and the agentâ€™s synthesized speech are recorded, because the translation and the synthetization are done on the agent (client) side. This makes it difficult for QA teams to properly evaluate and audit the conversations, including the many silent blocks within it. Instead, when the audio streaming add-ons are enabled, there are no silent blocks, and the QA team can hear the agentâ€™s original voice, the customerâ€™s original voice, and their respective translated and synthesized speech, all in a single audio file.</li><li>Transcription and translation accuracy â€“ Having both the original and translated speech available in the call recording makes it straightforward to detect specific words that would improve transcription accuracy (by using Amazon Transcribe custom vocabularies) or translation accuracy (using Amazon Translate custom terminologies), to make sure that your brand names, character names, model names, and other unique content are transcribed and translated to the desired result.</li></ul><h2>Get started with Amazon Connect V2V</h2><p>Ready to transform your contact centerâ€™s communication? Our Amazon Connect V2V sample project is now available on <a href=\"https://github.com/aws-samples/connect-v2v-translation-with-cx-options/tree/main\" target=\"_blank\" rel=\"noopener\">GitHub</a>. We invite you to explore, deploy, and experiment with this powerful prototype. You can it as a foundation for developing innovative multi-lingual communication solutions in your own contact center, through the following key steps:</p><ol><li>Clone the GitHub repository.</li><li>Test different configurations for audio streaming add-ons.</li><li>Review the sample projectâ€™s limitations in the README.</li><li>Develop your implementation strategy: \n  <ol type=\"a\"><li>Implement robust security and compliance controls that meet your organizationâ€™s standards.</li><li>Collaborate with your customer experience team to define your specific use case requirements.</li><li>Balance between automation and the agentâ€™s manual controls (for example, use an Amazon Connect contact flow to automatically set contact attributes for preferred languages and audio streaming add-ons).</li><li>Use your preferred transcribe, translate, and text-to-speech engines, based on specific language support requirements and business, legal, and regional preferences.</li><li>Plan a phased rollout, starting with a pilot group, then iteratively optimize your transcription custom vocabularies and translation custom terminologies.</li></ol></li></ol><p>The Amazon Connect V2V sample project demonstrates how Amazon Connect and advanced AWS AI services can break down language barriers, enhance operational flexibility, and reduce support costs. Get started now and revolutionize how your contact center communicates across language barriers!</p><p> is a Principal Solutions Architect at AWS.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/14/IMG_3095.jpg\" alt=\"\" width=\"100\" height=\"130\"> is a Senior Solutions Architect at AWS.</p><p> is a Technical Program Manager for Prototyping and Support Services at DXC Modern Workplace.</p>","contentLength":12003,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Orchestrate an intelligent document processing workflow using tools in Amazon Bedrock","url":"https://aws.amazon.com/blogs/machine-learning/orchestrate-an-intelligent-document-processing-workflow-using-tools-in-amazon-bedrock/","date":1740156265,"author":"Raju Rangan","guid":8683,"unread":true,"content":"<p><a href=\"https://aws.amazon.com/ai/generative-ai/\" target=\"_blank\" rel=\"noopener\">Generative AI</a> is revolutionizing enterprise automation, enabling AI systems to understand context, make decisions, and act independently. Generative AI foundation models (FMs), with their ability to understand context and make decisions, are becoming powerful partners in solving sophisticated business problems. At AWS, weâ€™re using the power of models in <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> to drive automation of complex processes that have traditionally been challenging to streamline.</p><p>In this post, we focus on one such complex workflow: document processing. This serves as an example of how generative AI can streamline operations that involve diverse data types and formats.</p><h2>Challenges with document processing</h2><p>Document processing often involves handling three main categories of documents:</p><ul><li>Structured â€“ For example, forms with fixed fields</li><li>Semi-structured â€“ Documents that have a predictable set of information but might vary in layout or presentation</li><li>Unstructured â€“ For example, paragraphs of text or notes</li></ul><p>Traditionally, processing these varied document types has been a pain point for many organizations. Rule-based systems or specialized machine learning (ML) models often struggle with the variability of real-world documents, especially when dealing with semi-structured and unstructured data.</p><p>We demonstrate how generative AI along with <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html\" target=\"_blank\" rel=\"noopener\">external tool use</a> offers a more flexible and adaptable solution to this challenge. Through a practical use case of processing a patient health package at a doctorâ€™s office, you will see how this technology can extract and synthesize information from all three document types, potentially improving data accuracy and operational efficiency.</p><p>This intelligent document processing solution uses Amazon Bedrock FMs to orchestrate a sophisticated workflow for handling multi-page healthcare documents with mixed content types. The solution uses the FMâ€™s tool use capabilities, accessed through the Amazon Bedrock <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html\" target=\"_blank\" rel=\"noopener\">Converse API</a>. This enables the FMs to not just process text, but to actively engage with various external tools and APIs to perform complex document analysis tasks.</p><p>The solution employs a strategic multi-model approach, optimizing for both performance and cost by selecting the most appropriate model for each task:</p><ul><li><p><a href=\"https://aws.amazon.com/blogs/aws/upgraded-claude-3-5-sonnet-from-anthropic-available-now-computer-use-public-beta-and-claude-3-5-haiku-coming-soon-in-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">Anthropicâ€™s Claude 3 Haiku</a> â€“ Serves as the workflow orchestrator due to its low latency and cost-effectiveness. This modelâ€™s strong reasoning and tool use abilities make it ideal for the following:</p><ul><li><p>Coordinating the overall document processing pipeline</p></li><li><p>Making routing decisions for different document types</p></li><li><p>Invoking appropriate processing functions</p></li><li><p>Managing the workflow state</p></li></ul></li><li><p><a href=\"https://aws.amazon.com/blogs/aws/upgraded-claude-3-5-sonnet-from-anthropic-available-now-computer-use-public-beta-and-claude-3-5-haiku-coming-soon-in-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">Anthropicâ€™s Claude 3.5 Sonnet (v2)</a> â€“ Used for its advanced reasoning capabilities, notably strong visual processing abilities, particularly excelling at interpreting charts and graphs. Its key strengths include:</p><ul><li><p>Interpreting complex document layouts and structure</p></li><li><p>Extracting text from tables and forms</p></li><li><p>Processing medical charts and handwritten notes</p></li><li><p>Converting unstructured visual information into structured data</p></li></ul></li></ul><p>Through the Amazon Bedrock Converse APIâ€™s standardized tool use (function calling) interface, these models can work together seamlessly to invoke document processing functions, call external APIs for data validation, trigger storage operations, and execute content transformation tasks. The API serves as the foundation for this intelligent workflow, providing a unified interface for model communication while maintaining conversation state throughout the processing pipeline. The APIâ€™s standardized approach to tool definition and function calling provides consistent interaction patterns across different processing stages. For more details on how tool use works, refer to <a href=\"https://github.com/aws-samples/prompt-engineering-with-anthropic-claude-v-3/blob/main/10_2_3_Complete_Tool_Use_Workflow.ipynb\" target=\"_blank\" rel=\"noopener\">The complete tool use workflow</a>.</p><p>The solution incorporates <a href=\"https://aws.amazon.com/bedrock/guardrails/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Guardrails</a> to implement robust content filtering policies and sensitive information detection, making sure that personal health information (PHI) and personally identifiable information (PII) data is appropriately protected through automated detection and masking capabilities while maintaining industry standard compliance throughout the document processing workflow.</p><p>You need the following prerequisites before you can proceed with this solution. For this post, we use the  AWS Region. For details on available Regions, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/bedrock.html\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock endpoints and quotas</a>.</p><p>For our example use case, we examine a patient intake process at a healthcare institution. The workflow processes a patient health information package containing three distinct document types:</p><ul><li>Structured document â€“ A new patient intake form with standardized fields for personal information, medical history, and current symptoms. This form follows a consistent layout with clearly defined fields and check boxes, making it an ideal example of a structured document.</li><li>Semi-structured document â€“ A health insurance card that contains essential coverage information. Although insurance cards generally contain similar information (policy number, group ID, coverage dates), they come from different providers with varying layouts and formats, showing the semi-structured nature of these documents.</li><li>Unstructured document â€“ A handwritten doctorâ€™s note from an initial consultation, containing free-form observations, preliminary diagnoses, and treatment recommendations. This represents the most challenging category of unstructured documents, where information isnâ€™t confined to any predetermined format or structure.</li></ul><p>The example document can be downloaded from the following <a href=\"https://github.com/aws-samples/anthropic-on-aws/blob/main/medical-idp/docs/new-patient-registration.pdf\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>.</p><p>This healthcare use case is particularly relevant because it encompasses common challenges in document processing: the need for high accuracy, compliance with healthcare data privacy requirements, and the ability to handle multiple document formats within a single workflow. The variety of documents in this patient package demonstrates how a modern intelligent document processing solution must be flexible enough to handle different levels of document structure while maintaining consistency and accuracy in data extraction.</p><p>The following diagram illustrates the solution workflow.</p><p>This self-orchestrated workflow demonstrates how modern generative AI solutions can balance capability, performance, and cost-effectiveness in transforming traditional document processing workflows in healthcare settings.</p><ol><li>Create an Amazon SageMaker domain. For instructions, see Use quick setup for Amazon SageMaker AI.</li><li>Launch SageMaker Studio, then create and launch a JupyterLab space. For instructions, see Create a space.</li><li>Create a guardrail. Focus on adding sensitive information filters that would mask PII or PHI.</li><li><p>Clone the code from the GitHub repository:</p><pre><code>git clone https://github.com/aws-samples/anthropic-on-aws.git</code></pre></li><li><p>Change the directory to the root of the cloned repository:</p></li><li><pre><code>pip install -r requirements.txt</code></pre></li><li><p>Update setup.sh with the guardrail ID you created in Step 3. Then set the ENV variable:</p></li><li><p>Finally, start the Streamlit application:</p><pre><code>streamlit run streamlit_app.py</code></pre></li></ol><p>Now youâ€™re ready to explore the intelligent document processing workflow using Amazon Bedrock.</p><p>The solution is built around the Amazon Bedrock Converse API and tool use framework, with Anthropicâ€™s Claude 3 Haiku serving as the primary orchestrator. When a document is uploaded through the Streamlit interface, Haiku analyzes the request and determines the sequence of tools needed by consulting the tool definitions in . These definitions include tools for the following:</p><ul><li>Document processing pipeline â€“ Handles initial PDF processing and classification</li><li>Document notes processing â€“ Extracts information from medical notes</li><li>New patient information processing â€“ Processes patient intake forms</li><li>Insurance form processing â€“ Handles insurance card information</li></ul><p>The following code is an example tool definition for extracting consultation notes. Here, <code>extract_consultation_notes</code> represents the name of the function that the orchestration workflow will call, and  defines the schema of the input parameter that will be passed to the function. The FM will contextually extract the information from the document and pass to the method. A similar  will be defined for each step. Refer to the <a href=\"https://github.com/aws-samples/anthropic-on-aws/blob/4c2365cc6d763b0a48d253da2e67beef7f702f16/medical-idp/utils/constants.py#L107\" target=\"_blank\" rel=\"noopener\">GitHub repo</a> for the full  definition.</p><pre><code>{\n            \"toolSpec\": {\n                \"name\": \"extract_consultation_notes\",\n                \"description\": \"Extract diagnostics information from a doctor's consultation notes. Along with the extraction include the full transcript in a &lt;transcript&gt; node\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"document_paths\": {\n                                \"type\": \"array\",\n                                \"items\": {\"type\": \"string\"},\n                                \"description\": \"Paths to the files that were classified as DOC_NOTES\"\n                            }\n                        },\n                        \"required\": [\"document_paths\"]\n                    }\n                }\n            }\n        }\n</code></pre><p>When a PDF document is uploaded through the Streamlit interface, it is temporarily stored and passed to the FileProcessor class along with the tool specification and a user prompt:</p><pre><code>prompt = (\"1. Extract 2. save and 3. summarize the information from the patient information package located at \" + tmp_file + \". \" +\n                          \"The package might contain various types of documents including insurance cards. Extract and save information from all documents provided. \"\n                          \"Perform any preprocessing or classification of the file provided prior to the extraction.\" + \n                          \"Set the enable_guardrails parameter to \" + str(enable_guardrails) + \". \" + \n                          \"At the end, list all the tools that you had access to. Give an explantion on why each tool was used and if you are not using a tool, explain why it was not used as well\" + \n                          \"Think step by step.\")\n                processor.process_file(prompt=prompt, \ntoolspecs=toolspecs,...</code></pre><p>The  class manages the conversation with Anthropicâ€™s Claude 3 Haiku through the Amazon Bedrock Converse API. It maintains the conversation state and handles the tool use workflow:</p><pre><code># From bedrockutility.py\ndef invoke_bedrock(self, message_list, system_message=[], tool_list=[],\n                  temperature=0, maxTokens=2048, guardrail_config=None):\n    response = self.bedrock.converse(\n        modelId=self.model_id,\n        messages=message_list,\n        system=system_message,\n        inferenceConfig={\n            \"maxTokens\": maxTokens,\n            \"temperature\": temperature\n        },\n        **({\"toolConfig\": {\"tools\": tool_list}} if tool_list else {})\n    )\n</code></pre><p>When the processor receives a document, it initiates a conversation loop with Anthropicâ€™s Claude 3 Haiku, which analyzes the document and determines which tools to use based on the content. The model acts as an intelligent orchestrator, making decisions about the following:</p><ul><li>Which document processing tools to invoke</li><li>The sequence of processing steps</li><li>How to handle different document types within the same package</li><li>When to summarize and complete the processing</li></ul><p>This orchestration is managed through a continuous conversation loop that processes tool requests and their results until the entire document package has been processed.</p><p>The first key decision in the workflow is initiating the document classification process. Through the  class, the solution uses Anthropicâ€™s Claude 3.5 Sonnet to analyze and categorize each page of the uploaded document into three main types: intake forms, insurance cards, and doctorâ€™s notes:</p><pre><code># from document_classifier.py\nclass DocumentClassifier:\n    def __init__(self, file_handler):\n        self.sonnet_3_5_bedrock_utils = BedrockUtils(\n            model_id=ModelIDs.anthropic_claude_3_5_sonnet\n        )\n        \n    def categorize_document(self, file_paths):\n        # Convert documents to binary format for model processing\n        binary_data_array = []\n        for file_path in file_paths:\n            binary_data, media_type = self.file_handler.get_binary_for_file(file_path)\n            binary_data_array.append((binary_data[0], media_type))\n\n        # Prepare message for classification\n        message_content = [\n            {\"image\": {\"format\": media_type, \"source\": {\"bytes\": data}}}\n            for data, media_type in binary_data_array\n        ]\n        \n        # Create classification request\n        message_list = [{\n            \"role\": 'user',\n            \"content\": [\n                *message_content,\n                {\"text\": \"What types of document is in this image?\"}\n            ]\n        }]\n        \n        # Define system message for classification\n        system_message = [{\n            \"text\": '''You are a medical document processing agent. \n                      Categorize images as: INTAKE_FORM, INSURANCE_CARD, or DOC_NOTES'''\n        }]\n        \n        # Get classification from model\n        response = self.sonnet_3_5_bedrock_utils.invoke_bedrock(\n            message_list=message_list,\n            system_message=system_message\n        )\n        return [response['output']['message']]\n</code></pre><p>Based on the classification results, the FM determines the next tool to be invoked. The toolâ€™s description and input schema define exactly what information needs to be extracted. Following the previous example, letâ€™s assume the next page to be processed is a consultation note. The workflow will invoke the <code>extract_consultation_notes</code> function. This function processes documents to extract detailed medical information. Like the classification process discussed earlier, it first converts the documents to binary format suitable for model processing. The key to accurate extraction lies in how the images and system message are combined:</p><pre><code>def extract_info(self, file_paths):\n    # Convert documents to binary data\n    # This will follow the same pattern to as in the classification function\n    message_content = [\n        {\"image\": {\"format\": media_type, \"source\": {\"bytes\": data}}}\n        for data, media_type in binary_data_array\n    ]\n\n    message_list = [{\n        \"role\": 'user',\n        \"content\": [\n            *message_content,  # Include the processed document images\n            {\"text\": '''Extract all information from this file\n                       If you find a visualization\n                           - Provide a detailed description in natural language\n                           - Use domain specific language for the description\n                    '''}\n        ]\n    }]\n    \n    system_message = [{\n        \"text\": '''You are a medical consultation agent with expertise in diagnosing and treating various health conditions.\n                   You have a deep understanding of human anatomy, physiology, and medical knowledge across different specialties.\n                   During the consultation, you review the patient's medical records, test results, and documentation provided.\n                   You analyze this information objectively and make associations between the data and potential diagnoses.\nAssociate a confidence score to each extracted information. This should reflect how confident the model in the extracted value matched the requested entity.\n        '''}\n    ]\n    \n    response = self.bedrock_utils.invoke_bedrock(\n        message_list=message_list,\n        system_message=system_message\n    )\n    return [response['output']['message']]\n</code></pre><p>The system message serves three crucial purposes:</p><ul><li>Establish medical domain expertise for accurate interpretation.</li><li>Provide guidelines for handling different types of information (text and visualizations).</li><li>Provide a self-scored confidence. Although this is not an independent grading mechanism, the score is directionally indicative of how confident the model is in its own extraction.</li></ul><p>Following the same pattern, the FM will use the other tools in the  definition to save and summarize the results.</p><p>A unique advantage of using a multi-modal FM for the extraction task is its ability to have a deep understanding of the text it is extracting. For example, the following code is an abstract of the data schema we are requesting as input to the  function. Refer to the code in <a href=\"https://github.com/aws-samples/anthropic-on-aws/blob/main/medical-idp/utils/constants.py#L133\" target=\"_blank\" rel=\"noopener\">constants.py</a> for full definition. The model needs to not only extract a transcript, but also understand it to extract such structured data from an unstructured document. This significantly reduces the postprocessing efforts required for the data to be consumed by a downstream application.</p><pre><code>\"consultation\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                            \"date\": {\"type\": \"string\"},\n                            \"concern\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"primaryComplaint\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Primary medical complaint of the patient. Only capture the medical condition. no timelines\"\n                                    },\n                                    \"duration\": {\"type\": \"number\"},\n                                    \"durationUnit\": {\"type\": \"string\", \"enum\": [\"days\", \"weeks\", \"months\", \"years\"]},\n                                    \"associatedSymptoms\": {\n                                        \"type\": \"object\",\n                                        \"additionalProperties\": {\n                                            \"type\": \"boolean\"\n                                        },\n                                        \"description\": \"Key-value pairs of symptoms and their presence (true) or absence (false)\"\n                                    },\n                                    \"absentSymptoms\": {\n                                        \"type\": \"array\",\n                                        \"items\": {\"type\": \"string\"}\n                                    }\n                                },\n                                \"required\": [\"primaryComplaint\", \"duration\", \"durationUnit\"]\n                            }\n</code></pre><p>The documents contain a treasure trove of personally identifiable information (PII) and personal health information (PIH). To redact this information, you can pass enable_guardrails as true. This will use the guardrail you setup earlier as part of the information extraction process and mask information identified as PII or PIH.</p><pre><code>processor.process_file(prompt=prompt, \n                                        enable_guardrails=True,\n                                        toolspecs=toolspecs,\n      â€¦\n)</code></pre><p>Finally, cross-document validation is crucial for maintaining data accuracy and compliance in healthcare settings. Although the current implementation performs basic consistency checks through the <a href=\"https://github.com/aws-samples/anthropic-on-aws/blob/main/medical-idp/utils/constants.py#L35\" target=\"_blank\" rel=\"noopener\">summary prompt</a>, organizations can extend the framework by implementing a dedicated validation tool that integrates with their specific business rules and compliance requirements. Such a tool could perform sophisticated validation logic like insurance policy verification, appointment date consistency checks, or any other domain-specific validation requirements, providing complete data integrity across the document package.</p><p>As Amazon Bedrock continues to evolve, several powerful features can be integrated into this document processing workflow to enhance its enterprise readiness, performance, and cost-efficiency. Letâ€™s explore how these advanced capabilities can take this solution to the next level:</p><ul><li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles.html\" target=\"_blank\" rel=\"noopener\">Inference profiles</a> in Amazon Bedrock define a model and its associated Regions for routing invocation requests, enabling various tasks such as usage tracking, cost monitoring, and cross-Region inference. These profiles help users track metrics through <a href=\"http://aws.amazon.com/cloudwatch\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch</a> logs, monitor costs with cost allocation tags, and increase throughput by distributing requests across multiple Regions.</li><li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html\" target=\"_blank\" rel=\"noopener\">Prompt caching</a> can help when you have workloads with long and repeated contexts that are frequently reused for multiple queries. Instead of reprocessing the entire context for each document, the workflow can reuse cached prompts, which is particularly beneficial when using the same image across different tooling workflows. With support for multiple cache checkpoints, this feature can substantially reduce processing time and inference costs while maintaining the workflowâ€™s intelligent orchestration capabilities.</li><li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-routing.html\" target=\"_blank\" rel=\"noopener\">Intelligent prompt routing</a> can dynamically select the most appropriate model for each task based on performance and cost requirements. Rather than explicitly assigning Anthropicâ€™s Claude 3 Haiku for orchestration and Anthropicâ€™s Claude 3.5 Sonnet for document analysis, the workflow can use intelligent routing to automatically choose the optimal model within the Anthropic family for each request. This approach simplifies model management while providing cost-effective processing of different document types, from simple structured forms to complex handwritten notes, all through a single endpoint.</li></ul><p>This intelligent document processing solution demonstrates the power of combining Amazon Bedrock FMs with tool use capabilities to create sophisticated, self-orchestrating workflows. By using Anthropicâ€™s Claude 3 Haiku for orchestration and Anthropicâ€™s Claude 3.5 Sonnet for complex visual tasks, the solution effectively handles structured, semi-structured, and unstructured documents while maintaining high accuracy and compliance standards.</p><p>Key benefits of this approach include:</p><ul><li>Reduced manual processing through intelligent automation</li><li>Improved accuracy through specialized model selection</li><li>Built-in compliance with guardrails for sensitive data</li><li>Flexible architecture that adapts to various document types</li><li>Cost-effective processing through strategic model usage</li></ul><p>As organizations continue to digitize their operations, solutions like this showcase how generative AI can transform traditional document processing workflows. The combination of powerful FMs in Amazon Bedrock and the tool use framework provides a robust foundation for building intelligent, scalable document processing solutions across industries.</p><p> is a Senior Solutions Architect at AWS. He works with government-sponsored entities, helping them build AI/ML solutions using AWS. When not tinkering with cloud solutions, youâ€™ll catch him hanging out with family or smashing birdies in a lively game of badminton with friends.</p>","contentLength":22494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"data science school","url":"https://dev.to/vishal_sharma_a3f356614a7/data-science-school-3imo","date":1740156057,"author":"vishal sharma","guid":8696,"unread":true,"content":"<p>At  we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.At data science school we provide data science course and we also provide soft skills classes, mock interviews, resume preparation tips to boost your confiedence in interviews.</p>","contentLength":1213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reducing hallucinations in LLM agents with a verified semantic cache using Amazon Bedrock Knowledge Bases","url":"https://aws.amazon.com/blogs/machine-learning/reducing-hallucinations-in-llm-agents-with-a-verified-semantic-cache-using-amazon-bedrock-knowledge-bases/","date":1740155790,"author":"Dheer Toprani","guid":8682,"unread":true,"content":"<p>Large language models (LLMs) excel at generating human-like text but face a critical challenge: hallucinationâ€”producing responses that sound convincing but are factually incorrect. While these models are trained on vast amounts of generic data, they often lack the organization-specific context and up-to-date information needed for accurate responses in business settings. Retrieval Augmented Generation (RAG) techniques help address this by grounding LLMs in relevant data during inference, but these models can still generate non-deterministic outputs and occasionally fabricate information even when given accurate source material. For organizations deploying LLMs in production applicationsâ€”particularly in critical domains such as healthcare, finance, or legal servicesâ€”these residual hallucinations pose serious risks, potentially leading to misinformation, liability issues, and loss of user trust.</p><p>To address these challenges, we introduce a practical solution that combines the flexibility of LLMs with the reliability of drafted, curated, verified answers. Our solution uses two key <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> services: <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a>, a fully managed service that you can use to store, search, and retrieve organization-specific information for use with LLMs; and <a href=\"https://aws.amazon.com/bedrock/agents/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Agents</a>, a fully managed service that you can use to build, test, and deploy AI assistants that can understand user requests, break them down into steps, and execute actions. Similar to how a customer service team maintains a bank of carefully crafted answers to frequently asked questions (FAQs), our solution first checks if a userâ€™s question matches curated and verified responses before letting the LLM generate a new answer. This approach helps prevent hallucinations by using trusted information whenever possible, while still allowing the LLM to handle new or unique questions. By implementing this technique, organizations can improve response accuracy, reduce response times, and lower costs.&nbsp;Whether youâ€™re new to AI development or an experienced practitioner, this post provides step-by-step guidance and code examples to help you build more reliable AI applications.</p><p>Our solution implements a verified semantic cache using the <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_Retrieve.html\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases Retrieve API</a> to reduce hallucinations in LLM responses while simultaneously improving latency and reducing costs. This read-only semantic cache acts as an intelligent intermediary layer between the user and Amazon Bedrock Agents, storing curated&nbsp;and&nbsp;verified question-answer pairs.</p><p>When a user submits a query, the solution first evaluates its semantic similarity with existing verified questions in the knowledge base. For highly similar queries (greater than 80% match), the&nbsp;solution bypasses the LLM completely and returns the curated&nbsp;and&nbsp;verified answer directly. When partial matches (60â€“80% similarity) are found, the&nbsp;solution uses the verified answers as few-shot examples to guide the LLMâ€™s response, significantly improving accuracy and consistency. For queries with low similarity (less than 60%) or no match, the&nbsp;solution falls back to standard LLM processing, making sure that user questions receive appropriate responses.</p><p>This approach offers several key benefits:</p><ul><li> By minimizing unnecessary LLM invocations for frequently answered questions, the solution significantly reduces operational costs at scale</li><li> Curated and verified answers minimize the possibility of hallucinations for known user queries, while few-shot prompting enhances accuracy for similar questions.</li><li> Direct retrieval of cached answers provides near-instantaneous responses for known queries, improving the overall user experience.</li></ul><p>The semantic cache serves as a growing repository of trusted responses, continuously improving the&nbsp;solutionâ€™s reliability while maintaining efficiency in handling user queries.</p><p>The solution architecture in the preceding figure consists of the following components and workflow. Letâ€™s assume that the question â€œWhat date will AWS re:invent 2024 occur?â€ is within the verified semantic cache. The corresponding answer is also input as â€œAWS re:Invent 2024 takes place on December 2â€“6, 2024.â€ Letâ€™s walkthrough an example of how this solution would handle a userâ€™s question.</p><p>a. User submits a question â€œWhen is re:Invent happening this year?â€, which is received by the Invoke Agent function.</p><p>b. The function checks the semantic cache (Amazon Bedrock Knowledge Bases) using the Retrieve API.</p><p>c. Amazon Bedrock Knowledge Bases performs a semantic search and finds a similar question with an 85% similarity score.</p><p>2. Response paths: (Based on the 85% similarity score in , our solution follows the strong match path)</p><p>a. Strong match (similarity score greater than 80%):</p><p>i. Invoke Agent function returns exactly the verified answer â€œAWS re:Invent 2024 takes place on December 2â€“6, 2024â€ directly from the Amazon Bedrock knowledge base, providing a deterministic response.</p><p>ii. No LLM invocation needed, response in less than 1 second.</p><p>b. Partial match (similarity score 60â€“80%):</p><p>i. The Invoke Agent function invokes the Amazon Bedrock agent and provides the cached answer as a few-shot example for the agent through Amazon Bedrock Agents <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/agents-session-state.html#prompt-session-attribute-ex\" target=\"_blank\" rel=\"noopener\">promptSessionAttributes</a>.</p><p>ii. If the question was â€œWhatâ€™s the schedule for AWS events in December?â€, our solution would provide the verified re:Invent dates to guide the Amazon Bedrock agentâ€™s response with additional context.</p><p>iii. Providing the Amazon Bedrock agent with a curated and verified example might help increase accuracy.</p><p>c. No match (similarity score less than 60%):</p><p>i. If the userâ€™s question isnâ€™t similar to any of the curated and verified questions in the cache, the Invoke Agent function invokes the Amazon Bedrock agent without providing it any additional context from cache.</p><p>ii. For example, if the question was â€œWhat hotels are near re:Invent?â€, our solution would invoke the Amazon Bedrock agent directly, and the agent would use the tools at its disposal to formulate a response.</p><p>3. Offline knowledge management:</p><p>a. Verified question-answer pairs are stored in a verified Q&amp;A Amazon S3 bucket (<a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service</a>), and must be updated or reviewed periodically to make sure that the cache contains the most recent and accurate information.</p><p>b. The S3 bucket is periodically synchronized with the Amazon Bedrock knowledge base. This offline batch process makes sure that the semantic cache remains up-to-date without impacting real-time operations.</p><p>You need to meet the following prerequisites for the walkthrough:</p><p>Once you have the prerequisites in place, use the following steps to set up the solution in your AWS account.</p><h3>Step 0: Set up the necessary infrastructure</h3><p>Follow the instructions in the <a href=\"https://github.com/aws-samples/Reducing-Hallucinations-in-LLM-Agents-with-a-Verified-Semantic-Cache/blob/main/README.md\" target=\"_blank\" rel=\"noopener\">README</a> of the <a href=\"https://github.com/aws-samples/Reducing-Hallucinations-in-LLM-Agents-with-a-Verified-Semantic-Cache/\" target=\"_blank\" rel=\"noopener\">Git repository</a> to set up the infrastructure for this solution. All the following code samples are extracted from the Jupyter notebook in this repository.</p><h3>Step 1: Set up two Amazon Bedrock knowledge bases</h3><p>This step creates two Amazon Bedrock knowledge bases. The agent knowledge base stores Amazon Bedrock service documentation, while the cache knowledge base contains curated and verified question-answer pairs. This setup uses the <a href=\"https://aws.amazon.com/sdk-for-python/\" target=\"_blank\" rel=\"noopener\">AWS SDK for Python (Boto3)</a> to interact with AWS services.</p><div><div><pre><code>agent_knowledge_base = BedrockKnowledgeBase(\n    kb_name=agent_knowledge_base_name,\n    kb_description=\"Knowledge base used by Bedrock Agent\",\n    data_bucket_name=agent_bucket_name,\n    chunking_strategy=\"FIXED_SIZE\",\n    suffix=f'{agent_unique_id}-f'\n)\n\ncache_knowledge_base = BedrockKnowledgeBase(\n    kb_name=cache_knowledge_base_name,\n    kb_description=\"Verified cache for Bedrock Agent System\",\n    data_bucket_name=cache_bucket_name,\n    chunking_strategy=\"NONE\",  # We do not want to chunk our question-answer pairs\n    suffix=f'{cache_unique_id}-f'\n)</code></pre></div></div><p>This establishes the foundation for your semantic caching solution, setting up the AWS resources to store the agentâ€™s knowledge and verified cache entries.</p><h3>Step 2: Populate the agent knowledge base and associate it with an Amazon Bedrock agent</h3><p>For this walkthrough, you will create an LLM Amazon Bedrock agent specialized in answering questions about Amazon Bedrock. For this example, you will ingest Amazon Bedrock documentation in the form of the User Guide PDF into the Amazon Bedrock knowledge base. This will be the primary dataset. After ingesting the data, you create an agent with specific instructions:</p><div><pre><code>agent_instruction = \"\"\"You are the Amazon Bedrock Agent. You have access to a \nknowledge base with information about the Amazon Bedrock service on AWS. \nUse it to answer questions.\"\"\"\n\nagent_id = agents_handler.create_agent(\n    agent_name,\n    agent_description,\n    agent_instruction,\n    [agent_foundation_model],\n    kb_arns=[agent_kb_arn] # Associate agent with our Agent knowledge base\n)</code></pre></div><p>This setup enables the Amazon Bedrock agent to use the ingested knowledge to provide responses about Amazon Bedrock services. To test it, you can ask a question that isnâ€™t present in the agentâ€™s knowledge base, making the LLM either refuse to answer or hallucinate.</p><div><pre><code>invoke_agent(\"What are the dates for reinvent 2024?\", session_id=\"test\")\n# Response: Unfortunately, the dates for the AWS re:Invent 2024 conference have not \n# been announced yet by Amazon. The re:Invent conference is typically held in late \n# November or early December each year, but the specific dates for 2024 are not \n# available at this time. AWS usually announces the dates for their upcoming \n# re:Invent event around 6-9 months in advance.</code></pre></div><h3>Step 3: Create a cache dataset with known question-answer pairs and populate the cache knowledge base</h3><p>In this step, you create a raw dataset of verified question-answer pairs that arenâ€™t present in the agent knowledge base. These curated&nbsp;and&nbsp;verified answers serve as our semantic cache to prevent hallucinations on known topics. Good candidates for inclusion in this cache are:</p><ol><li><strong>Frequently asked questions (FAQs):</strong>&nbsp;Common queries that users often ask, which can be answered consistently and accurately.</li><li><strong>Critical questions requiring deterministic answers:</strong>&nbsp;Topics where precision is crucial, such as pricing information, service limits, or compliance details.</li><li><strong>Time-sensitive information:</strong>&nbsp;Recent updates, announcements, or temporary changes that might not be reflected in the main RAG knowledge base.</li></ol><p>By carefully curating this cache with high-quality, verified answers to such questions, you can significantly improve the accuracy and reliability of your&nbsp;solutionâ€™s responses. For this walkthrough, use the following example pairs for the cache:</p><p><code>Q:&nbsp;'What are the dates for reinvent 2024?'</code><code>A: 'The AWS re:Invent conference was held from December 2-6 in 2024.'</code></p><p><code>Q:&nbsp;'What was the biggest new feature announcement for Bedrock Agents during reinvent 2024?'</code><code>A:&nbsp;'During re:Invent 2024, one of the headline new feature announcements for Bedrock Agents was the custom orchestrator. This key feature allows users to implement their own orchestration strategies through AWS Lambda functions, providing granular control over task planning, completion, and verification while enabling real-time adjustments and reusability across multiple agents.'</code></p><p>You then format these pairs as individual text files with corresponding <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-metadata.html\" target=\"_blank\" rel=\"noopener\">metadata JSON files</a>, upload them to an S3 bucket, and ingest them into your cache knowledge base. This process makes sure that your semantic cache is populated with accurate, curated, and verified information that can be quickly retrieved to answer user queries or guide the agentâ€™s responses.</p><h3>Step 4: Implement the verified semantic cache logic</h3><p>In this step, you implement the core logic of your verified semantic cache solution. You create a function that integrates the semantic cache with your Amazon Bedrock agent, enhancing its ability to provide accurate and consistent responses.</p><ol><li>Queries the cache knowledge base for similar entries to the user question.</li><li>If a high similarity match is found (greater than 80%), it returns the cached answer directly.</li><li>For partial matches (60â€“80%), it uses the cached answer as a few-shot example for the agent.</li><li>For low similarity (less than 60%), it falls back to standard agent processing.</li></ol><p>This simplified logic forms the core of the semantic caching solution, efficiently using curated&nbsp;and&nbsp;verified information to improve response accuracy and reduce unnecessary LLM invocations.</p><h3>Step 5: Evaluate results and performance</h3><p>This step demonstrates the effectiveness of the verified semantic cache&nbsp;solution by testing it with different scenarios and comparing the results and latency. Youâ€™ll use three test cases to showcase the&nbsp;solutionâ€™s behavior:</p><ol><li>Strong semantic match (greater than 80% similarity)</li><li>Partial semantic match (60-80% similarity)</li><li>No semantic match (less than 60% similarity)</li></ol><ol><li>Strong semantic match (greater than 80% similarity) provides the exact curated and verified answer in less than 1 second. \n  <div><pre><code>%%time\ninvoke_agent_with_verified_cache(\"What were some new features announced for Bedrock Agents during reinvent 2024?\")\n\n# Output:\n# Cache semantic similarity log: Strong match with score 0.9176399\n# CPU times: user 20.7 ms, sys: 442 Î¼s, total: 21.1 ms\n# Wall time: 440 ms\n\n# During re:Invent 2024, one of the headline new feature announcements for Bedrock \n# Agents was the custom orchestrator. This key feature allows users to implement \n# their own orchestration strategies through AWS Lambda functions, providing \n# granular control over task planning, completion, and verification while enabling \n# real-time adjustments and reusability across multiple agents.</code></pre></div></li><li>Partial semantic match (60â€“80% similarity) passes the verified answer to the LLM during the invocation. The Amazon Bedrock agent answers the question correctly using the cached answer even though the information is not present in the agent knowledge base. \n  <div><pre><code>%%time\ninvoke_agent_with_verified_cache(\"What are the newest features for Bedrock Agents?\") \n\n# Output:\n# Cache semantic similarity log: Partial match with score 0.6443664\n# CPU times: user 10.4 ms, sys: 0 ns, total: 10.4 ms\n# Wall time: 12.8 s\n\n# One of the newest and most significant features for Amazon Bedrock Agents \n# announced during re:Invent 2024 was the custom orchestrator. This feature \n# allows users to implement their own orchestration strategies through AWS \n# Lambda functions, providing granular control over task planning, completion, \n# and verification. It enables real-time adjustments and reusability across \n# multiple agents, enhancing the flexibility and power of Bedrock Agents.</code></pre></div></li><li>No semantic match (less than 60% similarity) invokes the Amazon Bedrock agent as usual. For this query, the LLM will either refuse to provide the information because itâ€™s not present in the agentâ€™s knowledge base, or will hallucinate and provide a response that is plausible but incorrect. \n  <div><pre><code>%%time\ninvoke_agent_with_verified_cache(\"Tell me about a new feature for Amazon Bedrock Agents\")\n\n# Output:\n# Cache semantic similarity log: No match with score 0.532105\n# CPU times: user 22.3 ms, sys: 579 Î¼s, total: 22.9 ms\n# Wall time: 13.6 s\n\n# Amazon Bedrock is a service that provides secure and scalable compute capacity \n# for running applications on AWS. As for new features for the Bedrock Agents \n# component, I do not have any specific information on recent or upcoming new \n# features. However, AWS services are frequently updated with new capabilities, \n# so it's possible there could be new agent features released in the future to \n# enhance security, scalability, or integration with other AWS services. Without \n# being able to consult the Knowledge Base, I cannot provide details on any \n# particular new Bedrock Agent features at this time.</code></pre></div></li></ol><p>These results demonstrate the effectiveness of the semantic caching solution:</p><ol><li>Strong matches provide near-instant, accurate, and deterministic responses without invoking an LLM.</li><li>Partial matches guide the LLM agent to provide a more relevant or accurate answer.</li><li>No matches fall back to standard LLM agent processing, maintaining flexibility.</li></ol><p>The semantic cache significantly reduces latency for known questions and improves accuracy for similar queries, while still allowing the agent to handle unique questions when necessary.</p><h3>Step 6: Resource clean up</h3><p>Make sure that the Amazon Bedrock knowledge bases that you created, along with the underlying Amazon OpenSearch Serverless collections are deleted to avoid incurring unnecessary costs.</p><h2>Production readiness considerations</h2><p>Before deploying this&nbsp;solution in production, address these key considerations:</p><ol><li><strong>Similarity threshold optimization:</strong>&nbsp;Experiment with different thresholds to balance cache hit rates and accuracy. This directly impacts the solutionâ€™s effectiveness in preventing hallucinations while maintaining relevance.</li><li><strong>Feedback loop implementation:</strong>&nbsp;Create a mechanism to continuously update the verified cache with new, accurate responses. This helps prevent cache staleness and maintains the solutionâ€™s integrity as a source of truth for the LLM.</li><li><strong>Cache management and update strategy:</strong>&nbsp;Regularly refresh the semantic cache with current, frequently asked questions to maintain relevance and improve hit rates. Implement a systematic process for reviewing, validating, and incorporating new entries to help ensure cache quality and alignment with evolving user needs.</li><li>&nbsp;Adjust similarity thresholds as your dataset evolves. Treat the semantic cache as a dynamic component, requiring continuous optimization for your specific use case.</li></ol><p>This verified semantic cache approach offers a powerful solution to reduce hallucinations in LLM responses while improving latency and reducing costs. By using Amazon Bedrock Knowledge Bases, you can implement a&nbsp;solution that can efficiently serve curated&nbsp;and&nbsp;verified answers, guide LLM responses with few-shot examples, and gracefully fall back to full LLM processing when needed.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/12/dtoprani-author-photo.jpeg\" alt=\"Dheer Toprani (author photo)\" width=\"100\" height=\"150\"> is a System Development Engineer within the Amazon Worldwide Returns and ReCommerce Data Services team. He specializes in large language models, cloud infrastructure, and scalable data systems, focusing on building intelligent solutions that enhance automation and data accessibility across Amazonâ€™s operations. Previously, he was a Data &amp; Machine Learning Engineer at AWS, where he worked closely with customers to develop enterprise-scale data infrastructure, including data lakes, analytics dashboards, and ETL pipelines.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/12/maisagon-author-photo.jpeg\" alt=\"Chaithanya Maisagoni Author Photo\" width=\"100\" height=\"150\"> is a Senior Software Development Engineer (AI/ML) in Amazonâ€™s Worldwide Returns and ReCommerce organization. He specializes in building scalable machine learning infrastructure, distributed systems, and containerization technologies. His expertise lies in developing robust solutions that enhance monitoring, streamline inference processes, and strengthen audit capabilities to support and optimize Amazonâ€™s global operations.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/12/rnedunur-author-photo.jpeg\" alt=\"Rajesh Nedunuri Author Photo\" width=\"100\" height=\"150\"> is a Senior Data Engineer within the Amazon Worldwide Returns and ReCommerce Data Services team. He specializes in designing, building, and optimizing large-scale data solutions. At Amazon, he plays a key role in developing scalable data pipelines, improving data quality, and enabling actionable insights for reverse logistics and ReCommerce operations. He is deeply passionate about generative AI and consistently seeks opportunities to implement AI into solving complex customer challenges.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/12/karamup-author-photo.jpeg\" alt=\"Karam Muppidi Author Photo\" width=\"100\" height=\"138\"> is a Senior Engineering Manager at Amazon Retail, where he leads data engineering, infrastructure and analytics for the Worldwide Returns and ReCommerce organization. He has extensive experience developing enterprise-scale data architectures and governance strategies using both proprietary and native AWS platforms, as well as third-party tools. Previously, Karam developed big-data analytics applications and SOX compliance solutions for Amazonâ€™s Fintech and Merchant Technologies divisions.</p>","contentLength":19822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM continuous self-instruct fine-tuning framework powered by a compound AI system on Amazon SageMaker","url":"https://aws.amazon.com/blogs/machine-learning/llm-continuous-self-instruct-fine-tuning-framework-powered-by-a-compound-ai-system-on-amazon-sagemaker/","date":1740155226,"author":"Yunfei Bai","guid":8681,"unread":true,"content":"<p>Fine-tuning a pre-trained <a href=\"https://aws.amazon.com/what-is/large-language-model/\" target=\"_blank\" rel=\"noopener\">large language model</a> (LLM) allows users to customize the model to perform better on domain-specific tasks or align more closely with human preferences. It is a continuous process to keep the fine-tuned model accurate and effective in changing environments, to adapt to the data distribution shift (<a href=\"https://en.wikipedia.org/wiki/Concept_drift\" target=\"_blank\" rel=\"noopener\">concept drift</a>) and prevent performance degradation over time. Continuous fine-tuning also enables models to integrate human feedback, address errors, and tailor to real-world applications. You can use <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html\" target=\"_blank\" rel=\"noopener\">supervised fine-tuning (SFT)</a> and <a href=\"https://arxiv.org/pdf/2308.10792v5\" target=\"_blank\" rel=\"noopener\">instruction tuning</a> to train the LLM to perform better on specific tasks using human-annotated datasets and instructions. When you have user feedback to the model responses, you can also use <a href=\"https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/\" target=\"_blank\" rel=\"noopener\">reinforcement learning from human feedback</a> (RLHF) to guide the LLMâ€™s response by rewarding the outputs that align with human preferences.</p><p>Precise and responsible outputs from fine-tuned LLMs require big efforts from subject matter experts (SMEs). The manual annotation of extensive training data for fine-tuning by human SMEs and collecting user feedback to align LLM responses with human preferences are both resource-heavy and time-intensive. Also, the continuous fine-tuning process requires orchestrating the multiple steps of data generation, LLM training, feedback collection, and preference alignments with scalability, resiliency, and resource efficiency. To address these challenges, we present an innovative continuous self-instruct fine-tuning framework that streamlines the LLM fine-tuning process of training data generation and annotation, model training and evaluation, human feedback collection, and alignment with human preference. This framework is designed as a <a href=\"https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/\" target=\"_blank\" rel=\"noopener\">compound AI system</a> to drive the fine-tuning workflow for performance improvement, versatility, and reusability.</p><p>In this post, we introduce the continuous self-instruct fine-tuning framework and its pipeline, and present how to drive the continuous fine-tuning process for a question-answer task as a compound AI system. We use <a href=\"https://dspy.ai/\" target=\"_blank\" rel=\"noopener\">DSPy</a> (Declarative Self-improving Python) to demonstrate the workflow of <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\" target=\"_blank\" rel=\"noopener\">Retrieval Augmented Generation</a> (RAG) optimization, LLM fine-tuning and evaluation, and human preference alignment for performance improvement.</p><h2>Overview of the continuous self-instruct fine-tuning framework</h2><p>The continuous self-instruct fine-tuning framework drives a workflow to customize the foundation model (FM) using human-labeled training samples and human feedback after model inference. This workflow runs on a continuous basis to be adaptive to a changing environment. The following diagram illustrates the workflow.</p><p>The workflow consists of the following steps:</p><ol><li><strong>Self-instruct supervised fine-tuning</strong> â€“ First, we use a human-labeled training dataset to adapt the FM to tasks in a specific domain. Instruction tuning is a popular approach in domain-specific LLM fine-tuning, which trains the FM to follow instructions for a specific task rather than generating the next texts. To address the challenges of the lack of human efforts for data labeling, annotation, and validation, we designed a self-instruct fine-tuning method to synthetically generate training labels by the LLM from a small volume of high-quality human-annotated samples. This process scales up the training dataset used for fine-tuning the FM into a custom LLM.</li><li><strong>Human preference alignment </strong>â€“ After the model is deployed in the production environment, the process moves into the human-in-the-loop workflow, in which we collect user feedback including satisfaction scores and comments on model response. The human feedback data is not only used for model performance and hallucination measurement, but is also used to further fine-tune the custom model in Step 1 through RLHF. Likewise, to address the challenges of lack of human feedback data, we use LLMs to generate AI grades and feedback that scale up the dataset for reinforcement learning from AI feedback (<a href=\"https://arxiv.org/abs/2309.00267\" target=\"_blank\" rel=\"noopener\">RLAIF</a>). There are various techniques of preference alignment, including <a href=\"https://arxiv.org/abs/1707.06347\" target=\"_blank\" rel=\"noopener\">proximal policy optimization</a> (PPO), <a href=\"https://arxiv.org/abs/2305.18290\" target=\"_blank\" rel=\"noopener\">direct preference optimization</a> (DPO), <a href=\"https://arxiv.org/html/2403.07691v2\" target=\"_blank\" rel=\"noopener\">odds ratio policy optimization</a> (ORPO), <a href=\"https://arxiv.org/pdf/2402.03300\" target=\"_blank\" rel=\"noopener\">group relative policy optimization</a> (GRPO), and other algorithms, that can be used in this process.</li><li><strong>Evaluation and continuous learning </strong>â€“ The model customization and preference alignment is not a one-time effort. We need to keep monitoring and evaluating the model performance, and restart the process in case of concept shift or model decay.</li></ol><p>The overall workflow consists of multiple steps of synthetic data generation, LLM training, feedback collection, preference alignment, and evaluation that involves multiple components and multiple LLMs. In the next section, we discuss using a compound AI system to implement this framework to achieve high versatility and reusability.</p><h2>Compound AI system and the DSPy framework</h2><p>With the rise of generative AI, scientists and engineers face a much more complex scenario to develop and maintain AI solutions, compared to classic predictive AI. The paper <a href=\"https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/\" target=\"_blank\" rel=\"noopener\">The Shift from Models to Compound AI Systems</a> highlights that state-of-the-art AI results are increasingly obtained by compound systems with multiple components, not just monolithic models. Compound AI systems are systems that implement AI tasks by combining multiple interacting components. These components can include multiple calls to models, retrievers, or external tools. The following diagram compares predictive AI to generative AI.</p><p>The concept of a compound AI system enables data scientists and ML engineers to design sophisticated generative AI systems consisting of multiple models and components. You can use a module to incorporate prompt engineering and in-context learning to improve RAG performance, and also design a data architecture with tools to gather external data. You can also build an agentic architecture with multiple LLMs, fine-tune the model to achieve higher performance, and orchestrate the LLM access. Besides the efficiency in system design, the compound AI system also enables you to optimize complex generative AI systems, using a comprehensive evaluation module based on multiple metrics, benchmarking data, and even judgements from other LLMs. The optimization is on the holistic end-to-end solution, rather than on each component separately.</p><p>To efficiently build and optimize compound AI systems, we introduce DSPy, an open source Python framework for developers to build LLM applications using modular and declarative programming, whether youâ€™re building simple classifiers, sophisticated RAG pipelines, or agentic workflows. It provides algorithms for optimizing LLMsâ€™ prompts and weights, and automates the prompt tuning process, as opposed to the trial-and-error approach performed by humans. DSPy supports iteratively optimizing all prompts involved against defined metrics for the end-to-end compound AI solution.</p><p>The DSPy lifecycle is presented in the following diagram in seven steps. It separates the flow of your program (modules) from the parameters (language model prompts and weights) of each step. These modules define the system behavior in a portable, declarative way. The first four steps cover the DSPy programming stage, including defining your task and its constraints, exploring a few examples, and using that to inform your initial pipeline design. When your system works reasonably well, you can run the DSPy evaluation stage (Steps 5 and 6) to collect an initial development set, define your DSPy metric, and use these to iterate on your system more systematically. Afterwards, DSPy introduces new optimizers (compilers) in Step 7, with language model-driven algorithms to tune LLM prompts and weights, based on predefined evaluation metrics.</p><h2>RAG pipeline with continuous fine-tuning in a compound AI system</h2><p>In this post, we provide an example of a question-answer task, using a RAG pipeline along with the continuous self-instruct fine-tuning framework. We build this as a compound AI system and use DSPy to drive the RAG inference, prompt optimization, LLM fine-tuning, and performance evaluation. The overall workflow is shown in the following diagram.</p><p>The flow starts from a standard RAG pipeline, followed by a few optimizations on the prompts and the RAG retriever. Then we generate the synthetic training dataset from the RAG knowledge base to fine-tune the generator LLM using RAG for performance improvement. Lastly, we use a separate LLM to generate feedback on the fine-tuned model responses, and use it to conduct the preference alignment training by DPO and PPO. The question-answer outputs from each step are measured by the underlying LLM-as-a-judge evaluation module. In this way, we demonstrate the effectiveness of the compound AI system for the continuous optimizing of the pipeline through RAG optimization and the fine-tuning framework.</p><p>In the next sections, we demonstrate how to build this workflow, including the RAG pipeline, optimization, instruction fine-tuning, preference alignment, and model evaluation, into a compound AI system using an <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a> notebook instance with the DSPy framework and LLMs on <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a>. The code from this post and more examples are available in the <a href=\"https://github.com/aws-samples/amlc-2024-tutorial-continuous-fine-tuning-compound-ai/tree/main\" target=\"_blank\" rel=\"noopener\">GitHub repository</a>.</p><p>To create and run this compound AI system in your AWS account, complete the following prerequisites:</p><p>For the question-answering task, we use the <a href=\"https://github.com/TheAtticusProject/cuad/\" target=\"_blank\" rel=\"noopener\">Contract Understanding Atticus Dataset (CUAD)</a>, an open legal contract review dataset created with dozens of legal experts from The Atticus Project, which consists of over 13,000 annotations. The <a href=\"https://github.com/aws-samples/amlc-2024-tutorial-continuous-fine-tuning-compound-ai/blob/main/notebook/synthetic_test_data_generation.ipynb\" target=\"_blank\" rel=\"noopener\">synthetic data generation</a> notebook automatically downloads the CUAD_v1 ZIP file and places it in the required folder named cuad_data.</p><p>In case of any issues, you can alternately download the dataset yourself by following the steps in the <a href=\"https://github.com/aws-samples/amlc-2024-tutorial-continuous-fine-tuning-compound-ai/blob/main/cuad_data/README.md\" target=\"_blank\" rel=\"noopener\">README</a> file and store the dataset inside a folder within the SageMaker notebook instance, and use it to perform the steps in the next section.</p><h3>Prepare question-answer pairs</h3><p>We use Anthropicâ€™s Claude v3 Sonnet on Amazon Bedrock to synthetically generate question-answer pairs to infer the RAG pipeline in the compound AI system, to demonstrate the improved accuracy after RAG optimization and model fine-tuning. The generated datasets are in the format of question-answer pairs along with the context <code>[context, question, answer]</code> from the document. We use the question to infer the RAG pipeline and use the answer as ground truth to evaluate the inference accuracy. Additionally, the question-answer pairs are used as training samples for the model fine-tuning. The following is a sample dataset triplet with context and a question-answer pair.</p><table border=\"1px\" width=\"623\" cellpadding=\"10px\"><tbody><tr><td width=\"432\"></td></tr><tr><td width=\"432\"><p>THIS STRATEGIC ALLIANCE AGREEMENT (â€œAgreementâ€) is made and entered into as of November 6, 2016 (the â€œEffective Dateâ€) by</p><p>and between Dialog Semiconductor (UK) Ltd., a corporation organized under the laws of England and Wales, having its principal office at 100</p><p>Longwater Avenue, Green Park, Reading, RG2 6GP, United Kingdom (â€œDIALOGâ€) and Energous Corporation, a Delaware corporation, having its</p><p>principal office at 3590 North First Street, Suite 210, San Jose, CA 95134 (â€œENERGOUSâ€)</p></td><td width=\"96\">What is the date of the contract?</td></tr></tbody></table><p>We implement a standard <a href=\"https://github.com/aws-samples/amlc-2024-tutorial-continuous-fine-tuning-compound-ai/blob/main/notebook/dspy_rag.ipynb\" target=\"_blank\" rel=\"noopener\">RAG pipeline with DSPy</a> using the following components to create the vector database, set up context retrieval, and generate the answer:</p><ol><li>Configure DSPy to use LLMs on Amazon Bedrock as the RAG generator model:</li></ol><div><pre><code>dsp_bedrock = dspy.Bedrock(region_name='us-west-2')\nclaude_sonnet_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\nbedrock_sonnet = dspy.AWSAnthropic(aws_provider=dsp_bedrock,\n                                   model=claude_sonnet_model_id,\n                                   max_new_tokens=4096,\n                                   max_tokens=4096)</code></pre></div><ol start=\"2\"><li>Process the dataset to generate logical and syntactically readable chunks. The size and overlap percentage can be empirically determined based on the dataset. For more flexibility, you can generate multiple files from the dataset file and make one file one chunk.</li><li>To set up a RAG retriever, we select ChromaDB as a vector store, and use DSPyâ€™s <a href=\"https://dspy.ai/deep-dive/retrieval_models_clients/ChromadbRM/\" target=\"_blank\" rel=\"noopener\">ChromadbRM</a> module as the retriever model:</li></ol><div><pre><code>titan_embed_model_id = \"amazon.titan-embed-text-v2:0\"\nbedrock_ef = AmazonBedrockEmbeddingFunction(session=session, \n                                            model_name=titan_embed_model_id)\ncollection_name = \"contexts\"\npersist_dir = \"cuad_db/\"\nrm = ChromadbRM(collection_name=collection_name,\n                persist_directory=persist_dir,\n                embedding_function=bedrock_ef,\n                k=3) </code></pre></div><ol start=\"4\"><li>Using these components, we orchestrate a <a href=\"https://dspy.ai/tutorials/rag/#build-your-first-rag-module\" target=\"_blank\" rel=\"noopener\">DSPy RAG pipeline</a> to clean the context, generate the answer, and use the LLM-as-a-judge to score the generated answer with respect to the ground truth:</li></ol><div><pre><code>class GenerateAnswer(dspy.Signature):\n   \"\"\"Answer questions with short factoid answers.\"\"\"\n   context = dspy.InputField(desc=\"may contain relevant facts\")\n   question = dspy.InputField()\n   answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\nclass RAG(dspy.Module):\n   def __init__(self, num_passages=3):\n      super().__init__()\n      self.retrieve = ChromadbRM(\"contexts\", \"./chroma\", k=num_passages)\n      self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n   def forward(self, question):\n      context = self.retrieve(question).passages\n      context = [unicodedata.normalize(\"NFKD\", r) for r in self.retrieve(question).passages]\n      prediction = self.generate_answer(context=context, question=question)\n      return dspy.Prediction(context=context, answer=prediction.answer)</code></pre></div><h3>RAG optimization with DSPy</h3><p>The next step is to perform <a href=\"https://github.com/aws-samples/amlc-2024-tutorial-continuous-fine-tuning-compound-ai/blob/main/notebook/dspy_rag.ipynb\" target=\"_blank\" rel=\"noopener\">RAG optimization with DSPy</a>. DSPy provides the Optimizer module, an algorithm that can tune the parameters of a DSPy program (the prompts and language model weights) to maximize the metrics you specify. It takes in a training set to bootstrap the selective training examples, and is based on a metric function that measures proximity to or matches against the ground truth. With these, we can compile the RAG pipeline module with a defined optimizer instance to conduct the optimization.</p><p>In this post, we use DSPy Optimizer to learn how to generate the prompt to improve the RAG response accuracy. Because our dataset size is low (fewer than 100 examples), we select the <a href=\"https://dspy.ai/deep-dive/optimizers/bootstrap-fewshot/\" target=\"_blank\" rel=\"noopener\">BootstrapFewShot</a> teleprompter to compile the RAG prompts and overall pipeline, and use the synthetic dataset with ground truth and the LLM-as-a-judge metric function we defined in the previous sections:</p><div><pre><code>def validate_context_and_answer(example, pred, trace=None):\n   answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n   answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n   answer_LLMJudge = factuality_metric(example, pred)\n   return answer_LLMJudge or answer_EM or answer_PM\n\nrag_lm = RAG()\nteleprompter = BootstrapFewShot(metric=validate_context_and_answer)\ncompiled_rag = teleprompter.compile(rag_lm, trainset=trainset)</code></pre></div><p>The context retrieval is crucial to the overall RAG accuracy. To evaluate the RAG optimization weâ€™ve described, we create a retriever evaluation by the LLM-as-a-judge to understand how well the retriever is able to pull out the relevant chunks for the incoming user question. The LLM judge is defined in the RetrievalJudge class:</p><div><pre><code>class RetrievalJudge(dspy.Signature):\n   \"\"\"Judge given the question to be answered, check if the groundtruth answer can be derived from the predicted context.&nbsp; Answer either Retrieved[True] or Retrieved[False]\"\"\"\n   context = dspy.InputField(desc=\"Context for the prediction\")\n   question = dspy.InputField(desc=\"Question to be answered\")\n   groundtruth_answer = dspy.InputField(desc=\"groundtruth answer for the question\")\n   retrieval_correctness = dspy.OutputField(desc=\"Can the groundtruth answer be derived from the predicted context?\", prefix=\"Retrieved[True/False]:\")\n\nretrieval_judge = dspy.ChainOfThought(RetrievalJudge)</code></pre></div><p>Then we define the metric to measure the retrieval by using the RetrievalJudge, and use the DSPy Evaluate module to generate the accuracy score for retrieval:</p><div><pre><code>def retrieval_metric(example, pred):\n   retrieval = retrieval_judge(question=example.question, groundtruth_answer=example.answer, context=pred.context)\n   llm_retriever_ans = bool(\"Retrieved[True]\" in retrieval.retrieval_correctness\n                            or '100% True' in retrieval.retrieval_correctness\n                            or '100% retrieved correct' in retrieval.retrieval_correctness\n                            or 'True.' in retrieval.retrieval_correctness)\n   return llm_retriever_ans\n\nrag_retrieval_score = Evaluate(compiled_rag, num_threads = 1, metric=retrieval_metric)</code></pre></div><h3>Configure the continuous fine-tuning framework</h3><p>After the RAG optimization, the compound AI system has the instruction tuning and preference alignment modules, driven by the continuous fine-tuning framework. This includes using the synthetically generated dataset to train the LLM to follow question-answer instructions by SFT, and generating feedback of RAG responses by AI (another LLM) used for RLAIF with PPO and preference alignment with DPO and ORPO. In this step, we use <a href=\"https://arxiv.org/abs/2305.16742\" target=\"_blank\" rel=\"noopener\">Parameter Efficient Fine-Tuning</a> (PEFT) with <a href=\"https://arxiv.org/abs/2106.09685\" target=\"_blank\" rel=\"noopener\">Low-Rank Adaptation</a> (LoRA) to reduce the requirement of compute resources and accelerate the training process.</p><p>At the time of writing, the DSPy Optimization module supports distillation of a prompt-based DSPy program into LLM weight updates using BootstrapFinetune, and does not yet support the fine-tuning methods we defined in the compound AI system. Therefore, we conducted the fine-tuning (instruction tuning and preference alignment) on a Meta Llama 3 8B model separately; refer to the following <a href=\"https://github.com/aws-samples/kdd-2024-domain-driven-llm-development\" target=\"_blank\" rel=\"noopener\">GitHub repository</a> for more details. With the compound AI system design, we are able to take the fine-tuning results back into the DSPy pipeline, use the LLM-as-a-judge evaluation function to generate the accuracy scores, and benchmark with the standard and optimized RAG inferences. This demonstrates the flexibility and interoperability of the compound AI system, which allows us to seamlessly replace one module with an external component without requiring changes to the entire pipeline.</p><p>The following diagram illustrates the workflow.</p><h3>Define an evaluation approach with DSPy</h3><p>DSPy provides an Evaluate module for evaluating the compound AI system output by using user-defined metrics. In this post, we use LLM-as-a-judge to evaluate the system output and create the corresponding metrics for benchmarking the accuracy of standard RAG, optimized RAG, and fine-tuned models. Complete the following steps:</p><ol><li>Load the dataset for evaluation in the Example data type. Examples are similar to Python dictionaries but with added utilities such as the dspy.Prediction as a return value. For example:</li></ol><div><pre><code>gt_answer = &lt;ground truth of the answer&gt;\npred_answer = &lt;answer from RAG and/or fine-tuned model&gt;\ndspy_data = dspy.Example(gt_answer=gt_answer, pred_answer=pred_answer).with_inputs(\"gt_answer\", \"pred_answer\")</code></pre></div><ol start=\"2\"><li>Define the LLM-as-a-judge class to adjudicate whether the predicted answer semantically matches the ground truth of the answer. For example, the following FactualityJudge_1 class provides a score between 0 and 1; 0 means a complete mismatch and 1 means a perfect match.</li></ol><div><pre><code>class FactualityJudge_1(dspy.Signature):\n   \"\"\"Judge if the predicted answer is semantically match the groundtruth answer. Provide a score between 0 and 1, 0 means completely mismatch and 1 means perfectly match. In the response, only present the score, DO NOT add any preambles.\"\"\"\n   groundtruth_answer = dspy.InputField(desc=\"groundtruth answer\")\n   predicted_answer = dspy.InputField(desc=\"predicted answer\")\n   factually_correct = dspy.OutputField(desc=\"Is the predicted answer factually correct and semantically similar to the groundtruth answer?\"))</code></pre></div><ol start=\"3\"><li>Define the evaluation metrics from the LLM judge, using DSPy metrics, to mark whether the predicted answer is true or not. For example, the following function returns the accuracy score based on the output of FactualityJudge_1:</li></ol><div><pre><code>factualityJudge_1 = dspy.ChainOfThought(FactualityJudge_1)\n\ndef factuality_metric_1(gt_answer, pred_answer):\n   pred_answer = gt_answer.pred_answer\n   gt_answer = gt_answer.gt_answer\n   factual_metrc = factualityJudge_1(groundtruth_answer=gt_answer, predicted_answer=pred_answer)\n   llm_judge_ans = float(factual_metrc[0].factually_correct)\n   print(f\"llm_judge_ans = {llm_judge_ans}\")\n   return llm_judge_ans\n\nmetric_LLM_1 = factuality_metric_1</code></pre></div><ol start=\"4\"><li>Use the  module to generate an accuracy score using the LLM-as-a-judge metrics defined in the previous step:</li></ol><div><pre><code>evaluate_llm_judge = Evaluate(devset= dspy_data, metric=metric_LLM_1, num_threads=1)</code></pre></div><p>This evaluation process should be conducted on a continuous basis in the compound AI system driven by self-instruct fine-tuning, to make sure the overall performance remains stable despite the changes in the environment or the introduction of new data.</p><h3>Benchmark RAG and LLM fine-tuning with DSPy</h3><p>We benchmark the approaches presented in this post using the <a href=\"https://github.com/aws-samples/amlc-2024-tutorial-continuous-fine-tuning-compound-ai/blob/main/notebook/dspy_rag_ft.ipynb\" target=\"_blank\" rel=\"noopener\">LLM-as-a-judge evaluation</a> function defined in the previous section with the following settings.</p><p>The benchmarking is across five methods: standard RAG, optimized RAG, fine-tuning LLMs by instruction tuning, and fine-tuning LLMs by DPO and ORPO trained LLMs based on AIF. For each method, the LLM judge provides a decimal accuracy score in the range of 0 and 1.</p><p>The standard RAG uses Amazon Titan Text Embedding V2 for the embedding model, and Anthropicâ€™s Claude 3 Haiku model for the generator model. The RAG compilation uses 32 question-answer pairs to optimize the prompts. The same dataset is used for inference. The fine-tuning by SFT, DPO, and ORPO are performed on the Meta Llama 3 8B FM, using training samples synthetically generated from CUAD document.</p><p>The results are presented in the following tables and charts. The different methods demonstrate different levels of improvement. The improvement is calculated in percentage by (accuracy of new method â€“ accuracy of standard RAG)/(accuracy of standard RAG)*100%.</p><p>The optimized RAG by DSPy improved the accuracy and reduced the hallucination.</p><table border=\"1px\" width=\"696\" cellpadding=\"10px\"><tbody><tr><td width=\"162\"></td><td width=\"210\"></td></tr><tr><td width=\"162\">Accuracy by LLM Judge (0-1)</td></tr></tbody></table><table border=\"1\" width=\"696\" cellpadding=\"10px\"><tbody><tr><td width=\"162\"></td><td width=\"210\"></td></tr><tr><td width=\"162\">Accuracy by LLM Judge (0-1)</td></tr></tbody></table><p>The custom LLM trained by SFT yielded higher accuracy than the standard RAG.</p><table border=\"1\" width=\"696\" cellpadding=\"10\"><tbody><tr><td width=\"162\"></td><td width=\"192\"></td></tr><tr><td width=\"162\">Accuracy by LLM Judge (0-1)</td></tr></tbody></table><table border=\"1\" width=\"696\" cellpadding=\"10\"><tbody><tr><td width=\"162\"></td><td width=\"192\"></td></tr><tr><td width=\"162\">Accuracy by LLM Judge (0-1)</td></tr></tbody></table><p>The custom LLM through preference alignment from human and AI feedback (DPO and ORPO) further improved the model performance. The fine-tuned small size model (Meta Llama 3 8B) outperformed the standard RAG pipeline with the medium size (Anthropicâ€™s Claude Haiku) and larger size (Anthropicâ€™s Claude Sonnet) generator model, and was comparable with the prompt-optimized RAG using ground truth data.</p><table border=\"1\" width=\"714\" cellpadding=\"10\"><tbody><tr><td width=\"120\"></td><td width=\"108\"></td><td width=\"114\"></td></tr><tr><td width=\"150\">Accuracy by LLM Judge (0-1)</td></tr></tbody></table><table border=\"1\" width=\"714\" cellpadding=\"10\"><tbody><tr><td width=\"111\"></td><td width=\"114\"></td><td width=\"114\"></td></tr><tr><td width=\"153\">Accuracy by LLM Judge (0-1)</td></tr></tbody></table><p>The following charts compare the accuracy across all tested methods.</p><p>The preceding results were generated from a small dataset (32 question-answer pairs). You can use a larger sample set with more question-answer pairs to conduct the benchmarking and compare your own results.</p><p>Make sure to clean up the following resources to avoid incurring additional costs:</p><ol><li>Back up the Jupyter notebooks in the SageMaker notebook instance.</li><li>Shut down and delete the SageMaker notebook instance.</li></ol><p>Consider the following costs from the solution deployed on AWS:</p><ul><li>You will incur charges for storing files in S3 buckets. For more details, refer to <a href=\"https://aws.amazon.com/s3/pricing/\" target=\"_blank\" rel=\"noopener\">Amazon S3 pricing</a>.</li></ul><p>In this post, we presented the continuous self-instruct fine-tuning framework as a compound AI system implemented by the DSPy framework. The framework first generates a synthetic dataset from the domain knowledge base and documents for self-instruction, then drives model fine-tuning through SFT, and introduces the human-in-the-loop workflow to collect human and AI feedback to the model response, which is used to further improve the model performance by aligning human preference through reinforcement learning (RLHF/RLAIF).</p><p>We demonstrated the framework for a question-answer task with a RAG pipeline, which improved the end-to-end response accuracy. The workflow is implemented by the DSPy framework; the overall strategy is to use the  to connect all the components (RAG pipeline, prompt optimization, LLMs fine-tuned by SFT and RLHF/RLAIF, performance evaluation) together into a compound AI system. Each module can be seamlessly maintained, updated, and replaced without affecting other components in the system. This robust and versatile system design strengthens control and trust through modular design, and increases flexibility and adaptability to changing environments and data sources.</p><p>You can implement this continuous fine-tuning framework for LLM performance improvement for your own business use cases, with a compound AI system that provides high flexibility and interoperability. For more details, follow the examples in our <a href=\"https://github.com/aws-samples/amlc-2024-tutorial-continuous-fine-tuning-compound-ai/tree/main\" target=\"_blank\" rel=\"noopener\">GitHub repository</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/09/12/YunfeiBai.jpg\" alt=\"Yunfei\" width=\"100\" height=\"133\"> is a Principal Solutions Architect at AWS. With a background in AI/ML, data science, and analytics, Yunfei helps customers adopt AWS services to deliver business results. He designs AI/ML and data analytics solutions that overcome complex technical challenges and drive strategic objectives. Yunfei has a PhD in Electronic and Electrical Engineering. Outside of work, Yunfei enjoys reading and music.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/20/shayan.png\" alt=\"\" width=\"100\" height=\"135\"> is an Applied Scientist at Amazon Web Services. His area of research is all things natural language (like NLP, NLU, and NLG). His work has been focused on conversational AI, task-oriented dialogue systems, and LLM-based agents. His research publications are on natural language processing, personalization, and reinforcement learning.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/12/cass-1.png\" alt=\"\" width=\"100\" height=\"105\"><strong>Jose Cassio dos Santos Junior</strong> is a Senior Data Scientist member of the MLU team. He is responsible for Curriculum Development for Advanced Modules. As a previous Senior Data Scientist on the AWS LATAM Professional Services Data Science team, he has over 20 years of experience working as a software engineer and more than 10 years of teaching experience at colleges and as an instructor for Linux certification preparation and Microsoft Innovation Center bootcamps. As a business process management expert, he participated in BPO projects for more than 7 years. He holds a Masterâ€™s degree in Computer Engineering, a Bachelorâ€™s degree in Physics, and a Bachelorâ€™s degree in Business Administration, specialized in IT Quantitative Methods.</p>","contentLength":26306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a Homegrown LLM with Python: Training on Hacker News Data","url":"https://dev.to/daviducolo/building-a-homegrown-llm-with-python-training-on-hacker-news-data-10ai","date":1740155094,"author":"Davide Santangelo","guid":8695,"unread":true,"content":"<p>Large Language Models (LLMs) have transformed AI applications, from conversational agents to intelligent code assistants. While OpenAIâ€™s GPT models are widely used, many developers want to understand how these models work and even train their own versions from scratch. In this in-depth guide, we will explore how to build a lightweight LLM using Python, train it on Hacker News data, optimize its performance, and deploy it for real-world usage.</p><h3>\n  \n  \n  What is a Language Model?\n</h3><p>A Language Model (LM) is a type of artificial intelligence model that predicts the likelihood of a sequence of words in a given language. It learns patterns, grammar, and context from a large corpus of text data, enabling it to generate coherent and contextually relevant text. For example, given the phrase \"I love to code in,\" an LM might predict \"Python\" as the next word, based on patterns observed in its training data.</p><p>LMs are used in various applications, including:</p><ul><li>Text generation: Creating articles, stories, or dialogues.</li><li>Machine translation: Translating text from one language to another.</li><li>Text classification: Sentiment analysis or spam detection.</li><li>Autocomplete: Suggesting words or phrases in search engines or text editors.</li><li>In this article, we'll focus on building a simple autoregressive LM that predicts the next word in a sequence, trained specifically on HackerNews data.</li></ul><ul><li>Understanding how LLMs work</li><li>Collecting and preprocessing Hacker News data</li><li>Tokenizing text and creating structured datasets</li><li>Training a transformer-based model using PyTorch and Hugging Face Transformers</li><li>Fine-tuning and optimizing for better performance</li><li>Evaluating model performance using loss metrics and perplexity</li><li>Deploying the model with FastAPI and making it accessible via an API</li><li>Improving the model with reinforcement learning and knowledge distillation</li><li>Scaling the model with distributed training</li><li>Reducing computational costs through quantization and pruning</li><li>Enhancing model security with adversarial training</li></ul><p>Before we start, install the necessary dependencies:</p><div><pre><code>pip torch transformers datasets tokenizers accelerate fastapi uvicorn matplotlib deepspeed bitsandbytes\n</code></pre></div><ul><li> for deep learning computations</li><li> for leveraging pre-built architectures like GPT-2</li><li> for handling large text corpora efficiently</li><li> for high-speed text processing</li><li> and  for deploying the model as an API</li><li> for visualizing loss curves and performance metrics</li><li> for optimizing large-scale model training</li><li> for quantization to reduce memory footprint</li></ul><h2>\n  \n  \n  Step 1: Understanding Large Language Models\n</h2><p>Before diving into code, it's essential to grasp how LLMs function. At their core, these models are neural networks trained to predict the next word in a sequence given an input context. They use:</p><ul><li> to break text into numerical representations</li><li><strong>Transformer architectures</strong> (such as GPT-2) with attention mechanisms to understand long-range dependencies</li><li> to train on vast amounts of unstructured text data</li><li> to adapt to specific tasks, such as chatbots or code generation</li></ul><h2>\n  \n  \n  Step 2: Collecting Hacker News Data\n</h2><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 3: Preprocessing and Tokenization\n</h2><p>Data cleaning ensures better training results. We remove HTML tags, non-text characters, and normalize text.</p><blockquote><p>Tokenization is the process of breaking down text into smaller units, called tokens, which can be words, subwords or even characters. This step is critical to transforming text into a numerical representation that deep learning models can understand and process.</p></blockquote><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 4: Creating a Dataset and DataLoader\n</h2><p>We format our text for training using PyTorchâ€™s  class.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 5: Training the Transformer Model\n</h2><p>We fine-tune a pre-trained GPT-2 model on our dataset.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 6: Model Optimization\n</h2><p>Model optimization focuses on improving the efficiency of a trained model by reducing its memory usage and increasing its inference speed. Two key techniques used for optimization are:</p><p>Quantization reduces the precision of model parameters (e.g., converting 32-bit floating point numbers to 8-bit integers). This helps decrease memory consumption and speeds up inference, especially on resource-limited devices.\nIn the code, we achieve this using <code>BitsAndBytesConfig(load_in_8bit=True)</code>, which loads the GPT-2 model in an 8-bit format, reducing its size and computational requirements.</p><p>Pruning removes unnecessary parameters from the model, reducing the number of computations required during inference. While pruning is not explicitly implemented in the code, it can be done by eliminating less significant weights from the neural network.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 7: Deploying as an API\n</h2><p>We use FastAPI to make our model accessible.</p><div><pre><code></code></pre></div><p>We have successfully built, trained, optimized, and deployed a custom LLM using Hacker News data. Future improvements could involve:</p><ul><li>Training on a larger dataset</li><li>Optimizing hyperparameters</li><li>Implementing reinforcement learning with human feedback (RLHF)</li><li>Deploying in a production-grade environment</li><li>Enhancing security against adversarial attacks</li></ul>","contentLength":4936,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week In Python","url":"https://dev.to/bascodes/this-week-in-python-1gc6","date":1740155047,"author":"Bas Steins","guid":8694,"unread":true,"content":"<p>This Week in Python is a concise reading list about what happened in the past week in the Python universe.</p><ul><li><a href=\"https://github.com/Goldziher/kreuzberg\" rel=\"noopener noreferrer\">kreuzberg</a> â€“ A text extraction library supporting PDFs, images, office documents and more</li><li><a href=\"https://github.com/kevinpdev/gpt-from-scratch\" rel=\"noopener noreferrer\">gpt-from-scratch</a> â€“ Educational implementation of a small GPT model from scratch in a single Jupyter Notebook</li><li><a href=\"https://github.com/seemoo-lab/opendrop\" rel=\"noopener noreferrer\">opendrop</a> â€“ An open Apple AirDrop implementation written in Python</li></ul>","contentLength":373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Maximize your file server dataâ€™s potential by using Amazon Q Business on Amazon FSx for Windows","url":"https://aws.amazon.com/blogs/machine-learning/maximize-your-file-server-datas-potential-by-using-amazon-q-business-on-amazon-fsx-for-windows/","date":1740154671,"author":"Manjunath Arakere","guid":8680,"unread":true,"content":"<p>Organizations need efficient ways to access and analyze their enterprise data. <a href=\"https://aws.amazon.com/q/business/\" target=\"_blank\" rel=\"noopener\">Amazon Q Business</a> addresses this need as a fully managed generative AI-powered assistant that helps you find information, generate content, and complete tasks using enterprise data. It provides immediate, relevant information while streamlining tasks and accelerating problem-solving.</p><p><a href=\"https://aws.amazon.com/fsx/windows/\" target=\"_blank\" rel=\"noopener\">Amazon FSx for Windows File Server</a> is a fully managed Windows file system that provides high-performance file storage for Windows-based applications. You can use Amazon FSx to lift and shift your on-premises Windows file server workloads to the cloud, taking advantage of the scalability, durability, and cost-effectiveness of AWS while maintaining full compatibility with your existing Windows applications and tooling.</p><p>Amazon Q Business is designed to be secure and private, seamlessly integrating with your existing identity provider (IdP). It works directly with your identities, roles, and permission sets, making sure users canâ€™t access data they are not authorized to. Additionally, Amazon Q Business seamlessly integrates with <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/connectors-list.html\" target=\"_blank\" rel=\"noopener\">multiple enterprise data stores</a>, including FSx for Windows File Server, enabling you to index documents from file server systems and perform tasks such as summarization, Q&amp;A, or data analysis of large numbers of files effortlessly.</p><p>In this post, we demonstrate how to use the Amazon Q connector for FSx for Windows File Server, explore a practical use case, and provide step-by-step instructions to help you get started and gain insights out of your data stored in FSx for Windows File Server.</p><h2>Overview of the Amazon Q data source connector</h2><p>A data source connector is a mechanism for integrating and synchronizing data from multiple repositories, including Microsoft SharePoint, Salesforce, <a href=\"http://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (Amazon S3) buckets, and even your internal FSx for Windows File Server into one container index. Amazon Q Business offers multiple data source connectors that can connect to your data sources and help you create your generative AI solution with minimal configuration. For a list of supported connectors, see <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/connectors-list.html\" target=\"_blank\" rel=\"noopener\">Supported connectors</a>.</p><p>Amazon Q boasts impressive versatility, supporting a wide range of <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/doc-types.html#doc-types-supported\" target=\"_blank\" rel=\"noopener\">document types</a> stored at various places in your environment, including Windows Share (FSX for Windows File Server). Amazon Q can ingest and understand common formats like plaintext, PDF, HTML, XML, and JSON to Microsoft formats like Excel, Word, and PowerPoint. This provides a comprehensive search experience for your enterprise users.</p><h3>Secure access with supported authentication types</h3><p>Security is job zero at AWS, and Amazon Q has been built keeping that in mind. It supports a variety of authentication types, seamlessly integrating with your existing identity management systems. Whether you use single sign-on (SSO) or a custom authentication solution, Amazon Q can adapt to your specific needs.</p><h3>Fine-grained control with ACLs and identity crawling</h3><p>For organizations with highly sensitive data, Amazon Q offers an extra layer of security. Amazon Q Business supports crawling <a href=\"https://www.wikiwand.com/en/Access-control_list\" target=\"_blank\" rel=\"noopener\">access control lists (ACLs)</a> for document security by default. When you connect an Amazon FSx (Windows) data source to Amazon Q Business, it crawls ACL information attached to a document (user and group information) from the directory service of the Amazon FSx instance.</p><p>The following diagram shows a high-level architecture of how <a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html\" target=\"_blank\" rel=\"noopener\">AWS Managed Active Directory</a> users, through <a href=\"https://aws.amazon.com/iam/identity-center/\" target=\"_blank\" rel=\"noopener\">AWS IAM Identity Center</a>, can access and interact with an Amazon Q Business application. This enables an authenticated user to securely and privately interact with the application and gain insights from the enterprise data stored in FSx for Windows File Server, using the Amazon Q Business web experience from their web browser.</p><p>In this post, we walk you through the process of integrating Amazon Q Business with FSx for Windows File Server to extract meaningful insights from your file system using natural language processing (NLP). This solution enables you to interact with your file system data using conversational AI, making information discovery more intuitive and efficient.</p><p>To set up your Amazon Q Business application, complete the following high-level steps:</p><ol><li>Create a new Amazon Q application.</li><li>Add a data source (FSx for Windows File Server).</li><li>Synchronize your file system data.</li></ol><p>Lastly, we demonstrate the application functionality by testing its access for two different users.</p><p>To implement this solution, you should have an AWS account with administrative privileges.</p><p>Follow the instructions in the <a href=\"https://github.com/aws-samples/maximizing-your-file-server-datas-potential-leveraging-amazonqs-nlp-on-amazon-fsx-for-windows/blob/main/README.md\" target=\"_blank\" rel=\"noopener\">GitHub repositoryâ€™s README file</a> to provision the infrastructure required for exploring the Amazon Q connector for FSx for Windows File Server.</p><h2>Create an Amazon Q Business application</h2><p>Complete the following steps to create a new Amazon Q Business application:</p><ol><li>On the Amazon Q Business console, choose  in the navigation pane.</li><li>Choose .</li></ol><ol start=\"3\"><li>For , enter a name (for example, anycompany-filesystem-knowledgebase).</li><li>For , select .</li></ol><p>If you completed the prerequisites, then IAM Identity Center is already enabled, and you should see the instance ARN listed.</p><ol start=\"5\"><li>Under , for Select user, choose your users.</li><li>Leave  as .</li><li>For , use the default values.</li></ol><p>In the next step, you will select the data source to retrieve and index the data.</p><p>In this step, you select the retriever to connect data sources to the application. There are two options: use a native retriever or use <a href=\"https://aws.amazon.com/kendra/\" target=\"_blank\" rel=\"noopener\">Amazon Kendra</a>. For this example, we use a native retriever.</p><ol><li>On the application details page, under , choose .</li></ol><ol start=\"3\"><li>For , select .</li><li>For , select .</li><li>For , enter 1.</li></ol><p>Complete the following steps to add a data source:</p><ol><li>On the application details page, choose .</li><li>Search for Amazon FSx and choose the plus sign next to .</li></ol><ol start=\"3\"><li>In the  section, enter a name (for example, <em>anycompany-filesystem-source</em>) and an optional description.</li><li>In the , for <strong>Amazon FSx file system ID</strong>, choose the file system ID you created as a prerequisite.</li><li>In the  section, leave as default (ACLs are enabled for the connector).</li></ol><ol start=\"6\"><li>In the  section, for <strong>AWS Secrets Manager secret</strong>, choose the <a href=\"https://aws.amazon.com/secrets-manager/\" target=\"_blank\" rel=\"noopener\">AWS Secrets Manager</a> secret that holds the active directory credentials to communicate with Amazon FSx to crawl the file system ().</li><li>In the <strong>Configure VPC and security group</strong>, provide the following information: \n  <ul><li>For <strong>Virtual Private Cloud (VPC)</strong>, choose the virtual private cloud (VPC) created as a prerequisite (<em>amazon-connector-for-win-fsx-blog-vpc</em>).</li><li>For , choose the private subnets that hold the FSx for Windows File System and active directory instance.</li><li>For , choose your security group (<em>&lt;stack-name&gt;-DefaultSecurityGroup</em>).</li></ul></li></ol><ol start=\"8\"><li>In the  section, provide the following information: \n  <ol><li>For Â¸ choose <strong>Create a new service role</strong>.</li><li>For , enter a name for the role.</li></ol></li><li>In the  section, provide the following information: \n  <ol><li>For , use the default option of 50 MB.</li><li>Under , you can add inclusion and exclusion patterns. For this post, we add the inclusion pattern for PDF file types, so the Amazon Q crawler will include PDF files.</li></ol></li></ol><p>Full sync is preferable for the first sync; for subsequent runs, you can choose only the modified data.</p><p>You also have the option to run the sync on a recurring basis like hourly or daily.</p><ol start=\"12\"><li>In the  section, you can optionally add tags.</li></ol><ol start=\"13\"><li>In the  section, use the default field mappings selected.</li></ol><h2>Synchronize your file system data</h2><p>When the data source is successfully created, a banner message appears. In the banner message (or on the data source details page), choose Sync now to sync your file system data.</p><p>You can monitor the status of the sync, which includes direct links to <a href=\"http://aws.amazon.com/cloudwatch\">Amazon CloudWatch</a> logs.</p><p>The sync can take a few minutes to a few hours to complete. Sync speeds are limited by factors such as remote repository throughput and throttling, network bandwidth, and the size of documents.</p><p>When the sync is complete, you should see the stats on the scan, which includes the number of items scanned and failed.</p><p>For this post, we have two active directory groups, ml-engineers and security-engineers. Each group has one user under them (John Doe and Jane Smith), and they have access to only one whitepaper based on their group (<a href=\"https://docs.aws.amazon.com/pdfs/decision-guides/latest/generative-ai-on-aws-how-to-choose/generative-ai-on-aws-how-to-choose.pdf\" target=\"_blank\" rel=\"noopener\">Choosing a generative AI service</a> and <a href=\"https://docs.aws.amazon.com/pdfs/whitepapers/latest/aws-security-incident-response-guide/aws-security-incident-response-guide.pdf\" target=\"_blank\" rel=\"noopener\">AWS Security Incident Response Guide</a>, respectively). The following diagram illustrates this access.</p><h2>Validate the Amazon Q application functionality</h2><p>Now that you have completed the setup, you can validate the application functionality by testing the access controls. We test the access of two users, John Doe and Jane Smith, who are users of the ml-engineers group and security-engineers group, respectively. You can retrieve the user name and password for each user from Secrets Manager. The secret name for John Doe is , and for Jane Smith, itâ€™s .</p><ol><li>On the application details page, in the  section, choose the link for the deployed URL.</li></ol><p>A successful login directs you to the Amazon Q Business chat interface. This window serves as the main workspace where users interact with the application, as shown in the following screenshot.</p><p>With the test configuration, John Doe has access to only one document: <em>generative-ai-on-aws-how-to-choose.pdf</em>. You can test the access controls by asking questions about this whitepaper through the chat interface. This restricted access demonstrates the effective implementation of document-level permissions.</p><ol start=\"3\"><li>For our first question, we ask <em>What are the key factors to consider when choosing a generative AI service?</em></li></ol><p>The following screenshot shows the response.</p><ol start=\"4\"><li>Next, we ask <em>Does Amazon Bedrock provide an option to customize the model?</em></li></ol><p>The response includes citations from Amazon Q with reference to the source data.</p><p>Testing confirms that John Doe successfully receives responses to questions about content from generative-ai-on-aws-how-to-choose.pdf. You can ask additional questions about generative AI services, such as:</p><ul><li>What are the generative AI service offerings from AWS?</li><li>What is Amazon Q optimized for?</li><li>What are critical factors to consider when choosing an appropriate foundational model?</li></ul><p>Next, we test access to the security incident response guide.</p><ol start=\"5\"><li>We ask <em>What are the four phases of the AWS security incident response process?</em></li></ol><p>When asking questions about security topics from aws-security-incident-response-guide.pdf, the system returns no results. This behavior validates that document indexing respects the configured access permissions, and users can only access content theyâ€™re authorized to view.</p><ol start=\"6\"><li>To validate access controls for the security-engineers user group, log in as Jane Smith.</li></ol><p>You can test with questions about security incident response:</p><ul><li>What are the key objectives of an AWS security incident response plan?</li><li>What are the four phases of the AWS security incident response process?</li><li>What are the recommended steps for containing and eradicating a security incident in AWS?</li><li>What types of data should be collected during an AWS security incident investigation?</li><li>What are the key considerations for recovering from an AWS security incident?</li></ul><p>If you encounter issues during the setup or operation of your Amazon Q Business application with FSx for Windows File Server, refer to the detailed troubleshooting guide in the <a href=\"https://github.com/aws-samples/maximizing-your-file-server-datas-potential-leveraging-amazonqs-nlp-on-amazon-fsx-for-windows/blob/main/README.md#troubleshooting\" target=\"_blank\" rel=\"noopener\">README file</a>. The guide provides solutions for common configuration challenges and operational issues you might experience.</p><p>To avoid ongoing charges, we recommend cleaning up the resources you created while following this guide. For step-by-step cleanup instructions, refer to the <a href=\"https://github.com/aws-samples/maximizing-your-file-server-datas-potential-leveraging-amazonqs-nlp-on-amazon-fsx-for-windows/blob/main/README.md#cleanup-procedures\" target=\"_blank\" rel=\"noopener\">README file</a>.</p><p>In this post, we provided an overview of the Amazon Q FSx connector and how you can use it for safe and seamless integration of generative AI assistance with your enterprise data source. By using Amazon Q in your organization, you can enable employees to be more data-driven, efficient, prepared, and productive. Lastly, we demonstrated how using simple NLP search through Amazon Q Business enhances your ability to discover insights from your enterprise data quicker and respond to your needs faster.</p><p>The Amazon Q Business application offers a compelling solution for organizations seeking to enhance their data-driven capabilities. By using its NLP and secure data source integration features, you can unlock the true value of your data and empower your teams to be more productive and efficient in their work.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/29/Headshot-100x100.jpg\" alt=\"\" width=\"100\" height=\"100\"> is a Senior Solutions Architect on the Worldwide Public Sector team at AWS, based in Atlanta, Georgia. He partners with AWS customers to design and scale well-architected solutions, supporting their cloud migrations and modernization initiatives. With extensive experience in the field, Manjunath specializes in migration strategies, application modernization, serverless, and Generative AI (GenAI). He is passionate about helping organizations leverage the full potential of cloud computing to drive innovation and operational efficiency. Outside of work, Manjunath enjoys outdoor runs, tennis, volleyball, and challenging his son in PlayStation soccer games.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/29/E015GUGD2V6-W01874YPZ8Q-f00e7ee969c7-512-100x100.jpeg\" alt=\"\" width=\"100\" height=\"100\"> is an experienced Sr. Solutions Architect in WWPS team with 14+ years of experience. Imtranur works with large AWS Global SI partners and helps them build their cloud strategy and broad adoption of Amazonâ€™s cloud computing platform. Imtranur specializes in Containers, Dev/SecOps, GitOps, microservices based applications, hybrid application solutions, application modernization and loves innovating on behalf of his customers. He is highly customer obsessed and takes pride in providing the best solutions through his extensive expertise.</p>","contentLength":13353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomRotation in PyTorch","url":"https://dev.to/hyperkai/randomrotation-in-pytorch-58g","date":1740151517,"author":"Super Kai (Kazuya Ito)","guid":8656,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Required-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It's the range of the degrees  so it must be .</li><li>A tuple/list must be the 1D with 2 elements.</li><li>A single value must be .</li><li>A single value means .</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:<code>InterpolationMode.NEAREST</code>-Type:<a href=\"https://pytorch.org/vision/main/_modules/torchvision/transforms/functional.html\" rel=\"noopener noreferrer\">InterpolationMode</a>).</li><li>The 3rd argument for initialization is (Optional-Default:-Type:).</li><li>The 4th argument for initialization is (Optional-Default:-Type:/( or )):\n*Memos:\n\n<ul><li>It can change the center position of an image.</li><li>It must be the 1D with 2 elements.</li></ul></li><li>The 5th argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can change the background of an image. *The background can be seen when rotating an image.</li><li>A tuple/list must be the 1D with 1 or 3 elements.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul></ul></li></ul><div><pre><code></code></pre></div>","contentLength":832,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I Just Found Out You Can Switch Search Enginesâ€”Hereâ€™s How! ğŸ˜","url":"https://dev.to/kihuni/i-just-found-out-you-can-switch-search-engines-heres-how-52g5","date":1740150310,"author":"kihuni","guid":8655,"unread":true,"content":"<p>Hey, ğŸ˜ folks! I just stumbled upon something awesome: you can switch between search engines right in your browser! I swear, all these years online, and I had no clue this was a thingğŸ˜.</p><p>Hereâ€™s a quick step-by-step guide to make it happen (Iâ€™m using Brave here, but most browsers have something similar):</p><p>Click the menu icon (those three dots or lines) in your browserâ€™s toolbar, then hit â€œSettingsâ€ from the dropdown. Easy start</p><p><strong>STEP 2: Find Search Engine Options</strong></p><p>Scroll down and click on â€œSearch Engineâ€ (usually in the sidebar or a tabâ€”depends on your browser). </p><p><strong>STEP 3: Pick Your Favorite</strong></p><p>Hit â€œChangeâ€ or click the dropdown menu, then choose your preferred search engineâ€”Google, Bing, DuckDuckGo, or whatever vibes with you! </p><p>Click â€œSet as Defaultâ€ (or â€œSaveâ€ in some browsers), and boomâ€”youâ€™re rolling with your new search engine!  </p><p>Itâ€™s perfect if you want better results or just wanna ditch the same old Google grindğŸ˜….</p>","contentLength":960,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go Panic and Recover: A Deep Dive into Error Handling","url":"https://dev.to/leapcell/go-panic-and-recover-a-deep-dive-into-error-handling-56be","date":1740150048,"author":"Leapcell","guid":8648,"unread":true,"content":"<p>In the Go language, there are two keywords that often appear in pairs â€” panic and recover. These two keywords are closely related to defer. They are both built-in functions in the Go language and provide complementary functions.</p><h2>\n  \n  \n  I. Basic Functions of panic and recover\n</h2><ul><li>: It can change the control flow of the program. After calling panic, the remaining code of the current function will be immediately stopped from execution, and the defer of the caller will be recursively executed in the current Goroutine.</li><li>: It can stop the program crash caused by panic. It is a function that can only take effect in defer. Calling it in other scopes will not have any effect.</li></ul><h2>\n  \n  \n  II. Phenomena When Using panic and recover\n</h2><h3>\n  \n  \n  (I) panic Only Triggers the defer of the Current Goroutine\n</h3><p>The following code demonstrates this phenomenon:</p><div><pre><code></code></pre></div><p>The running result is as follows:</p><div><pre><code>$ go run main.go\nin goroutine\npanic:\n...\n</code></pre></div><p>When running this code, it will be found that the defer statement in the main function is not executed, and only the defer in the current Goroutine is executed. Because the runtime.deferproc corresponding to the defer keyword will associate the deferred call function with the Goroutine where the caller is located, so when the program crashes, only the deferred call function of the current Goroutine will be called.</p><h3>\n  \n  \n  (II) recover Only Takes Effect When Called in defer\n</h3><p>The following code reflects this feature:</p><div><pre><code></code></pre></div><div><pre><code>$ go run main.go\nin main\npanic: unknown err\ngoroutine 1 [running]:\nmain.main()\n ...\nexit status 2\n</code></pre></div><p>By carefully analyzing this process, it can be known that recover will only take effect when called after a panic occurs. However, in the above control flow, recover is called before panic, which does not meet the conditions for taking effect. Therefore, the recover keyword needs to be used in defer.</p><h3>\n  \n  \n  (III) panic Allows Multiple Nested Calls in defer\n</h3><p>The following code shows how to call panic multiple times in a defer function:</p><div><pre><code></code></pre></div><p>The running result is as follows:</p><div><pre><code>$ go run main.go\nin main\npanic: panic once\n  panic: panic again\n  panic: panic again and again\ngoroutine 1 [running]:\n...\nexit status 2\n</code></pre></div><p>From the output result of the above program, it can be determined that multiple calls to panic in the program will not affect the normal execution of the defer function. Therefore, it is generally safe to use defer for the finalization work.</p><h2>\n  \n  \n  III. Data Structure of panic\n</h2><p>The panic keyword in the source code of the Go language is represented by the data structure runtime._panic. Every time panic is called, a data structure like the following will be created to store relevant information:</p><div><pre><code></code></pre></div><ul><li>: It is a pointer to the parameter when defer is called.</li><li>: It is the parameter passed in when panic is called.</li><li>: It points to the earlier called runtime._panic structure.</li><li>: It indicates whether the current runtime._panic has been recovered by recover.</li><li>: It indicates whether the current panic has been forcibly terminated.</li></ul><p>From the link field in the data structure, it can be inferred that the panic function can be called continuously multiple times, and they can form a linked list through the link.</p><p>The three fields pc, sp, and goexit in the structure are all introduced to fix the problems brought by runtime.Goexit. runtime.Goexit can only end the Goroutine that calls this function without affecting other Goroutines. However, this function will be cancelled by the panic and recover in defer. The introduction of these three fields is to ensure that this function will definitely take effect.</p><h2>\n  \n  \n  IV. Principle of Program Crash\n</h2><p>The compiler will convert the keyword panic into runtime.gopanic. The execution process of this function includes the following steps:</p><ol><li>Create a new runtime._panic and add it to the front of the _panic linked list of the Goroutine where it is located.</li><li>Continuously obtain runtime._defer from the _defer linked list of the current Goroutine in a loop and call runtime.reflectcall to run the deferred call function.</li><li>Call runtime.fatalpanic to abort the entire program.\n</li></ol><div><pre><code></code></pre></div><p>It should be noted that three relatively important parts of code are omitted in the above function:</p><ol><li>The code in the recover branch for restoring the program.</li><li>The code for optimizing the performance of the defer call through inlining.</li><li>The code for fixing the abnormal situation of runtime.Goexit.</li></ol><p>In version 1.14, the Go language solved the conflict between recursive panic and recover and runtime.Goexit through the submission of runtime: ensure that Goexit cannot be aborted by a recursive panic/recover.</p><p>runtime.fatalpanic implements a program crash that cannot be recovered. Before aborting the program, it will print out all the panic messages and the parameters passed in during the call through runtime.printpanics:</p><div><pre><code></code></pre></div><p>After printing the crash message, it will call runtime.exit to exit the current program and return the error code 2. The normal exit of the program is also implemented through runtime.exit.</p><h2>\n  \n  \n  V. Principle of Crash Recovery\n</h2><p>The compiler will convert the keyword recover into runtime.gorecover:</p><div><pre><code></code></pre></div><p>The implementation of this function is very simple. If the current Goroutine has not called panic, then this function will directly return nil, which is also the reason why the crash recovery will fail when called in a non-defer. Under normal circumstances, it will modify the recovered field of runtime._panic, and the recovery of the program is handled by the runtime.gopanic function:</p><div><pre><code></code></pre></div><p>The above code omits the inlining optimization of defer. It takes out the program counter pc and stack pointer sp from runtime._defer and calls the runtime.recovery function to trigger the scheduling of the Goroutine. Before the scheduling, it will prepare the sp, pc, and the return value of the function:</p><div><pre><code></code></pre></div><p>When the defer keyword is called, the stack pointer sp and program counter pc at the time of the call have already been stored in the runtime._defer structure. The runtime.gogo function here will jump back to the position where the defer keyword was called.</p><p>runtime.recovery will set the return value of the function to 1 during the scheduling process. From the comments of runtime.deferproc, it can be found that when the return value of the runtime.deferproc function is 1, the code generated by the compiler will directly jump to before the return of the caller function and execute runtime.deferreturn:</p><div><pre><code></code></pre></div><p>After jumping to the runtime.deferreturn function, the program has been recovered from the panic and executes the normal logic, and the runtime.gorecover function can also take out the arg parameter passed in when calling panic from the runtime._panic structure and return it to the caller.</p><p>Analyzing the crash and recovery process of the program is rather tricky, and the code is not particularly easy to understand. Here is a simple summary of the program crash and recovery process:</p><ol><li>The compiler is responsible for the work of converting keywords. It converts panic and recover into runtime.gopanic and runtime.gorecover respectively, converts defer into the runtime.deferproc function, and calls the runtime.deferreturn function at the end of the function that calls defer.</li><li>When encountering the runtime.gopanic method during the running process, it will successively take out the runtime._defer structure from the linked list of the Goroutine and execute it.</li><li>If runtime.gorecover is encountered when calling the deferred execution function, it will mark _panic.recovered as true and return the parameter of the panic.</li><li>After this call ends, runtime.gopanic will take out the program counter pc and stack pointer sp from the runtime._defer structure and call the runtime.recovery function to restore the program.</li><li>runtime.recovery will jump back to runtime.deferproc according to the passed-in pc and sp.</li><li>The code automatically generated by the compiler will find that the return value of runtime.deferproc is not 0. At this time, it will jump back to runtime.deferreturn and restore to the normal execution flow.</li><li>If runtime.gorecover is not encountered, it will traverse all the runtime._defer in turn, and finally call runtime.fatalpanic to abort the program, print the parameters of the panic, and return the error code 2.</li></ol><p>The analysis process involves a lot of knowledge at the underlying level of the language, and the source code is also relatively obscure to read. It is full of unconventional control flows, jumping back and forth through the program counter. However, it is still very helpful for understanding the execution flow of the program. </p><p>Finally, I would like to recommend the most suitable deployment platform: </p><h3>\n  \n  \n  1. Multi-Language Support\n</h3><ul><li>Develop with JavaScript, Python, Go, or Rust.\n</li></ul><h3>\n  \n  \n  2. Deploy unlimited projects for free\n</h3><ul><li>pay only for usage â€” no requests, no charges.</li></ul><h3>\n  \n  \n  3. Unbeatable Cost Efficiency\n</h3><ul><li>Pay-as-you-go with no idle charges.\n</li><li>Example: $25 supports 6.94M requests at a 60ms average response time.\n</li></ul><h3>\n  \n  \n  4. Streamlined Developer Experience\n</h3><ul><li>Intuitive UI for effortless setup.\n</li><li>Fully automated CI/CD pipelines and GitOps integration.\n</li><li>Real-time metrics and logging for actionable insights.\n</li></ul><h3>\n  \n  \n  5. Effortless Scalability and High Performance\n</h3><ul><li>Auto-scaling to handle high concurrency with ease.\n</li><li>Zero operational overhead â€” just focus on building.\n</li></ul>","contentLength":9238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using DistilBERT for Resource-Efficient Natural Language Processing","url":"https://www.kdnuggets.com/distilbert-resource-efficient-natural-language-processing","date":1740150024,"author":"Jayita Gulati","guid":8628,"unread":true,"content":"<article>DistilBERT is a smaller, faster version of BERT that performs well with fewer resources. Itâ€™s perfect for environments with limited processing power and memory.</article>","contentLength":162,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/Using-DistilBERT-for-Resource-Efficient-Natural-Language-Processing.png","enclosureMime":"","commentsUrl":null},{"title":"In-depth Guide to net/netip Prefix Methods 7/7","url":"https://dev.to/rezmoss/in-depth-guide-to-netnetip-prefix-methods-77-4b3c","date":1740150000,"author":"Rez Moss","guid":8657,"unread":true,"content":"<p>Hey there! We've made it to our final deep dive into net/netip's core types. Today we're focusing on the Prefix type and its methods. If you've worked with networks, you're familiar with CIDR notation (like 192.168.1.0/24). That's exactly what Prefix handles, and we're going to explore every method you can use with it.</p><p>Let's start by looking at all the ways to create and work with Prefix.</p><div><pre><code></code></pre></div><p>Let's explore the essential methods every Prefix provides:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  1. IPAM (IP Address Management) System\n</h3><p>A comprehensive IPAM system using Prefix:</p><div><pre><code></code></pre></div><p>A tool for network planning and analysis:</p><div><pre><code></code></pre></div><p>A system for managing network access control lists:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><ol><li><strong>Handle IPv4 and IPv6 Appropriately</strong></li></ol><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>This concludes our deep dive into the net/netip package! We've covered:</p><ul><li>Addr type and its methods</li><li>AddrPort for handling IP:port combinations</li><li>Prefix for working with CIDR networks</li></ul><p>These types work together to provide a robust foundation for network programming in Go. The key benefits of using net/netip include:</p><ul><li>Comprehensive functionality</li></ul><p>Remember to check the Go documentation for updates and new features. The package continues to evolve with the language.</p><p>Keep exploring and building great networking applications with Go!</p>","contentLength":1182,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Is Prompting an Evolution, Not Extinction, for Coders","url":"https://developers.slashdot.org/story/25/02/21/1113219/ai-is-prompting-an-evolution-not-extinction-for-coders?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1740148800,"author":"msmash","guid":8625,"unread":true,"content":"AI coding assistants are reshaping software development, but they're unlikely to replace human programmers entirely, according to industry experts and developers. GitHub CEO Thomas Dohmke projects AI could soon generate 80-90% of corporate code, transforming developers into \"conductors of an AI-empowered orchestra\" who guide and direct these systems. \n\nCurrent AI coding tools, including Microsoft's GitHub Copilot, are delivering 10-30% productivity gains in business environments. At KPMG, developers report saving 4.5 hours weekly using Copilot, while venture investment in AI coding assistants tripled to $1.6 billion in 2024. The tools are particularly effective at automating routine tasks like documentation generation and legacy code translation, according to KPMG AI expert Swami Chandrasekaran. \n\nThey're also accelerating onboarding for new team members. Demand for junior developers remains soft, however, though analysts say it's premature to attribute this directly to AI adoption. Training programs like Per Scholas are already adapting, incorporating AI fundamentals alongside traditional programming basics to prepare developers for an increasingly AI-augmented workplace.","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Day-03 of Kapilâ€™s learning python programming","url":"https://dev.to/kapil_kumarshahsonar_ad/day-03-of-kapils-learning-python-programming-530g","date":1740147965,"author":"KAPIL SHAH","guid":8635,"unread":true,"content":"<p>The things i learned from python are: </p><p>1.More i.e. depth in list:</p><p>In list i learned about many thing about the proper use of list: </p><p>like we can use this in many function i.e. inside the function:</p><p>I get to know that list has many  in programming language.</p><p>The new thing i learned in todayâ€™s course is about set .</p><p>it also have many use in function basically it is basically use for the  problems</p><div><pre><code></code></pre></div><p>In above code we are basically finding  using set function.</p><p>I got this idea from a channel called </p><p>Overall we can understand that set is widely used while mathematical problem.</p><p>In Dictionary (one the function or â€œmethodâ€ i learned about storing a data in a ordered manner.</p><p><code>I had little problem between list and dict. then i got to know that list store data in a single line ,where as dict. stored data in multiple lineâ€¦  :)</code></p><div><pre><code></code></pre></div><p>we can check yourself what the data could beâ€¦</p><p>i also learned some very basic stuff which i have already shared in my previous dayâ€™s Blog </p><p>so please go there and check it out</p><p>Thank you this much for today</p>","contentLength":1018,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Relation with tpl and Html on VSCode?","url":"https://dev.to/skyhayato/how-relation-with-tpl-and-html-on-vscode-11a9","date":1740147550,"author":"SKY-HaYaTo","guid":8636,"unread":true,"content":"<p>Hi,Guys! \nI'm Kohei,a Software Engineer in Japan.</p><p>I am talking about a function of VSCode.</p><p>Now, The Topic is about Relationship with these extension  and  on Visual Studio Code(VSCode).</p><p>Extension  is often used in Web Framework of PHP.</p><p>Recently, I develop personal Web apps using  and  which is one of Web MVC Frameworks of Golang.</p><p> is generally used  as template engine.So, I need to make VSCode recognize  extension like .</p><p>In this article, you do'nt install any plugins with ,but revide  on VSCode.</p><p>VSCode is the most popular Free IDE and used in the world.\nAn one of nice functions on the IDE is to relation variaty of extension.</p><p>As my memorandom and shrering with you,I have decited to write the article.</p><p>Ok,Now,Let's explanation!</p><p>In conclution, only 3 steps is completed.</p><h2>\n  \n  \n  Start VSCode and Click Prompt Screen\n</h2><p>Top of the VSCode is set (as a below picture).\nYou click on cursor.</p><h2>\n  \n  \n  Input Value of  in the screen\n</h2><p>Next,You need to input the value .\nIf maybe you continue to type , key intellisence will start and display some canditates including .</p><p>When you see the word,click it!</p><p>Then target page will transition to .</p><h2>\n  \n  \n  Revision Settings.json File\n</h2><p>When display ,you will type String of sentense (below the capture).</p><p>You input the senetences,please save the revision.</p><p>From now, we can relationship with  and .</p>","contentLength":1315,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ğŸš€ Day 2 #100DaysOfCode","url":"https://dev.to/xscoox_ca5e58c796032a1802/day-2-100daysofcode-5e6n","date":1740146960,"author":"xscoox","guid":8634,"unread":true,"content":"<p>Iâ€™m really enjoying this challenge so far! Solved over 15 problems on binary search in python. I plan to explore more exciting challenges ahead. ğŸš€</p>","contentLength":151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Software Engineering Job Openings Hit Five-Year Low","url":"https://tech.slashdot.org/story/25/02/21/111216/software-engineering-job-openings-hit-five-year-low?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1740146400,"author":"msmash","guid":8595,"unread":true,"content":"Software engineering job listings have plummeted to a five-year low, with postings on Indeed dropping to 65% of January 2020 levels -- a steeper decline than any other tech-adjacent field. According to data from Indeed's job aggregator, software development positions are now at 3.5x fewer vacancies compared to their mid-2022 peak and 8% lower than a year ago. \n\nThe decline appears driven by multiple factors including widespread adoption of AI coding tools -- with 75% of engineers reporting use of AI assistance -- and a broader tech industry recalibration after aggressive pandemic-era hiring. Notable tech companies like Salesforce are maintaining flat engineering headcount while reporting 30% productivity gains from AI tools, according to an analysis by software engineer Gergely Orosz. \n\nWhile the overall job market shows 10% growth since 2020, software development joins other tech-focused sectors in decline: marketing (-19%), hospitality (-18%), and banking/finance (-7%). Traditional sectors like construction (+25%), accounting (+24%), and electrical engineering (+20%) have grown significantly in the same period, he wrote. The trend extends beyond U.S. borders, with Canada showing nearly identical patterns. European markets and Australia demonstrate more resilience, though still below peak levels.","contentLength":1318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"6 New AI-Powered Tech Startups Reach Unicorn Status in January 2025","url":"https://dev.to/saad_hassan_8f937dc6fafc9/6-new-ai-powered-tech-startups-reach-unicorn-status-in-january-2025-pa3","date":1740145657,"author":"Saad Hassan","guid":8604,"unread":true,"content":"<p>Six cutting-edge AI startups have skyrocketed to unicorn status in January 2025, shaking up industries like healthcare, cybersecurity, automation, and defense. With billion-dollar valuations and massive investments, these companiesâ€”Truveta, Codeium, Mercor, Augury, Neko Health, and Epirusâ€”are redefining the future of tech. From AI-driven medical breakthroughs to next-gen coding assistants, the surge in AI funding signals a new era of innovation</p>","contentLength":452,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generate Tailored Cover Letters with AI: A Step-by-Step Guide Using FastAPI and OpenAI","url":"https://dev.to/resume-burger/generate-tailored-cover-letters-with-ai-a-step-by-step-guide-using-fastapi-and-openai-2584","date":1740145058,"author":"ResumeBurger","guid":8603,"unread":true,"content":"<p>In todayâ€™s fast-paced job market, a personalized cover letter can set you apart. <a href=\"https://resumeburger.com\" rel=\"noopener noreferrer\">ResumeBurgerâ€™s</a> mission is to streamline your job application processâ€”and what better way than to leverage AI to generate tailored cover letters in seconds? In this tutorial, weâ€™ll build an API endpoint that takes your rÃ©sumÃ© details and a job description as input, then uses OpenAI to create a professional, customized cover letter.</p><ul><li> installed on your system\n</li><li>Basic familiarity with Python scripting\n</li><li>An OpenAI API key (store it securely in a  file)\n</li></ul><p>Install the dependencies with:</p><div><pre><code>pip fastapi uvicorn openai python-dotenv\n</code></pre></div><h2>\n  \n  \n  Step 1: Secure Your API Key\n</h2><p>Create a  file in your project directory with your OpenAI API key:</p><div><pre><code>OPENAI_API_KEY=your_openai_api_key_here\n</code></pre></div><p>This keeps your sensitive credentials secure and out of your codebase.</p><h2>\n  \n  \n  Step 2: Build the Cover Letter Generator Function\n</h2><p>Weâ€™ll define a Python function that sends a prompt (including your rÃ©sumÃ© details and the job description) to OpenAIâ€™s API. The AI will return a refined cover letter tailored to the job requirements.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 3: Create a FastAPI Endpoint\n</h2><p>Next, weâ€™ll create a FastAPI app with an endpoint that accepts a JSON payload containing the rÃ©sumÃ© text and job description. It then returns the AI-generated cover letter.</p><div><pre><code></code></pre></div><p><em>Replace  with the name of your Python file (without the  extension).</em></p><h2>\n  \n  \n  Step 4: Testing and Deployment\n</h2><ul><li>\nRun the application with:\n</li></ul><div><pre><code>  uvicorn your_script_name:app </code></pre></div><p>Then send a POST request to <code>http://localhost:8000/generate-cover-letter</code> with JSON similar to:</p><div><pre><code></code></pre></div><p>You should receive a refined cover letter in response.</p><ul><li><strong>Docker Deployment (Optional):</strong>\nContainerize your app for scalable deployment with a Dockerfile:\n</li></ul><div><pre><code>  FROM python:3.9-slim\n  WORKDIR /app\n  COPY . /app\n  RUN pip install --upgrade pip &amp;&amp; pip install fastapi uvicorn openai python-dotenv\n  EXPOSE 8000\n  CMD [\"uvicorn\", \"your_script_name:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre></div><p>Build and run the container:</p><div><pre><code>  docker build  resumeburger-cover-letter \n  docker run  8000:8000 resumeburger-cover-letter\n</code></pre></div><p>With just a few lines of code, youâ€™ve created a powerful AI-driven cover letter generator that can help job seekers quickly produce personalized, professional cover letters. This FastAPI-based endpoint leverages OpenAIâ€™s capabilities to refine rÃ©sumÃ© details in line with job descriptions, ensuring every application stands out.</p><p>Customize this tool to fit your workflow, integrate it with <a href=\"https://resumeburger.com\" rel=\"noopener noreferrer\">ResumeBurgerâ€™s</a> suite, and empower users to land that dream interviewâ€”faster than ever.</p><p>Happy coding and best of luck in your job search!</p>","contentLength":2598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Becoming an Machine Learning Engineer in 2025","url":"https://www.kdnuggets.com/becoming-machine-learning-engineer-2025","date":1740142855,"author":"Nisha Arya","guid":8572,"unread":true,"content":"<article>Read some honest advice on how to become a machine learning engineer.</article>","contentLength":69,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/8-bit_ninja_2d_platformer_in_an_office_building_2.png","enclosureMime":"","commentsUrl":null},{"title":"Project Translate: The Translate API (Part 4)","url":"https://dev.to/__dbrown__/project-translate-the-translate-api-part-4-1726","date":1740141741,"author":"Emmanuel Akolbire","guid":8578,"unread":true,"content":"<p>Hey developers! ğŸ‘‹ For the last post in the series, we'll provision the infrastructure on AWS and deploy our API. Let's dive in! You can check out my <a href=\"https://github.com/DeXtreme/translate\" rel=\"noopener noreferrer\">GitHub</a> for the complete code.</p><p>First, we'll define the Terraform version and set up the backend to store the state in an S3 bucket. Since Terraform doesn't create the backend bucket automatically, you'll need to provision it beforehand. Variables cannot be used in the backend configuration so the values to be hardcoded but feel free to change them.</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>We'll also create a few local variables</p><div><pre><code></code></pre></div><p>Now we'll define the configuration to provision the DynamoDB table. DynamoDB tables require a , a ,which should also specified as an attribute, and a .</p><div><pre><code></code></pre></div><p>Next, we'll provision the output S3 bucket by specifying a name using either the  or  field. To manage storage efficiently, we'll add a lifecycle configuration that automatically deletes files after one day. Lifecycle configurations require the bucket name and at least one rule to define the retention policy.</p><div><pre><code></code></pre></div><p>We'll start by assigning the <code>AWSLambdaBasicExecutionRole</code>, which grants essential permissions like creating CloudWatch logs.</p><p>To package the Python scripts for the Translate Text and Translate File endpoints, we'll use the  data resource. Since the Translate File API has dependencies, we'll install them beforehand using a  before packaging.</p><p>We'll also create an IAM role for the Lambda functions and attach the necessary policies. Finally, we'll define the Lambda functions using the packaged files, set the handler to , and configure the required environment variables.</p><div><pre><code></code></pre></div><p>Finally, we'll define the API Gateway API. To handle file uploads properly, we'll include  in the  configuration, ensuring that multipart requests are base64-encoded before being sent to the Lambda function.  </p><p>Additionally, we'll configure the API resources as deployment triggers, so any changes to their properties automatically trigger a new deployment.</p><div><pre><code></code></pre></div><p>We'll also define a few outputs.</p><div><pre><code></code></pre></div><p>To provision the infrastructure, we first configure the AWS CLI with our credentials:</p><p>Next, we initialize Terraform and apply the configuration:</p><div><pre><code>terraform init\nterraform apply\n</code></pre></div><p>After successful provisioning, we can access the API via the output url.</p><p>To wrap up this series, we've walked through writing python code for lambda functions and provisioning a complete infrastructure using Terraform and AWS. \nIf you've followed along, you should now have an API that can translate text and files running on AWS. But this is just the beginning! Thereâ€™s always more to exploreâ€”whether itâ€™s optimizing performance, integrating monitoring tools, or adding a CI/CD pipeline.</p><p>Iâ€™d love to hear your thoughts! Drop a comment below if you have questions, insights, or ideas for future topics. ğŸš€ Thanks for reading, and happy coding! ğŸ‰</p>","contentLength":2785,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discover the Hottest GitHub Projects Revolutionizing Tech Today ğŸš€ğŸŒ","url":"https://dev.to/bruh_buh_f683772f171823db/discover-the-hottest-github-projects-revolutionizing-tech-today-2bck","date":1740139904,"author":"Bruh Buh","guid":8577,"unread":true,"content":"<p>Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.</p><p>With an impressive  on GitHub and a surge of recent activity, Composio is quickly becoming a go-to framework for developers looking to harness the power of AI agents. This open-source platform simplifies the integration of AI capabilities into applications, enabling seamless automation and intelligent interactions. Whether you're building chatbots or advanced AI systems, Composio empowers you to create cutting-edge solutions with ease and efficiency!</p><h3>\n  \n  \n  Key Features of Composio:\n</h3><ol><li><p><strong>Production-Ready Toolset:</strong></p><ul><li>A comprehensive suite designed for immediate deployment in production environments, enabling seamless AI integration.</li></ul></li><li><ul><li>Integrates with over 250 tools across various categories, including software platforms like GitHub and Slack, as well as operating system utilities.</li></ul></li><li><ul><li>Supports multiple authentication protocols such as OAuth and API Keys, ensuring secure access for users.</li></ul></li><li><ul><li>Features a modular design allowing developers to integrate custom tools and extensions easily, enhancing flexibility.</li></ul></li></ol><p>To get started with Composio, you can install the core package using the following command:</p><p>For the OpenAI plugin, use:</p><div><pre><code>pip composio-openai\n</code></pre></div><p>Hereâ€™s a sample code snippet demonstrating how to initialize the OpenAI client and the Composio Tool Set:</p><div><pre><code></code></pre></div><p>This example showcases how to set up the environment and prepare for using Composio with OpenAI for specific tasks, such as starring a GitHub repository.</p><p>With an impressive  on GitHub and a flurry of recent updates, MinMind is making waves in the AI community! This cutting-edge project enables developers to train a 26M-parameter GPT model from scratch in just two hours, making it a game-changer for those looking to harness the power of large language models. Whether you're an AI enthusiast or a seasoned developer, MinMind provides the tools you need to create and experiment with your own AI applications effortlessly!</p><ol><li><ul><li>Train a small language model from scratch for just  in under , making it accessible for anyone interested in AI.</li></ul></li><li><p><strong>Lightweight Model Design:</strong></p><ul><li>The smallest version of MiniMind is only <strong>1/7000th the size of GPT-3</strong>, allowing for efficient training on standard personal GPUs.</li></ul></li><li><p><strong>Comprehensive Training Framework:</strong></p><ul><li>Offers a full pipeline including pretraining, supervised fine-tuning, and advanced techniques like Mixture of Experts (MoE) for scalable model capacity.</li></ul></li><li><p><strong>Open Source and Educational Resource:</strong></p><ul><li>Provides a fully open-source codebase with an emphasis on transparency, serving as a tutorial for beginners in the large language model (LLM) space.</li></ul></li></ol><p>To get started with MinMind, you can clone the repository and install the necessary dependencies:</p><div><pre><code>git clone https://github.com/minimind/minimind.git\nminimind\npip  requirements.txt\n</code></pre></div><p>Hereâ€™s a simple code example demonstrating the initialization of the training process for a MiniMind model:</p><div><pre><code></code></pre></div><p>This example illustrates how easy it is to set up and train your own lightweight language model using the MiniMind framework!</p><p>With an incredible  on GitHub and a surge of recent updates, Open-WebUI is at the forefront of modern AI user interfaces! This powerful project provides a versatile framework for building and deploying user-friendly web applications that interact with AI models, making it easier than ever to create engaging and intuitive experiences. Whether you're a developer looking to enhance your AI projects or a creator aiming to bring your ideas to life, Open-WebUI is your go-to solution for innovative web-based AI interactions!</p><h3>\n  \n  \n  Key Features of Open-WebUI:\n</h3><ol><li><p><strong>Extensible and Feature-Rich Framework:</strong></p><ul><li>Open-WebUI is designed to be highly extensible, catering to diverse user needs with a variety of built-in features for seamless AI deployment.</li></ul></li><li><p><strong>Self-Hosted and Offline Operation:</strong></p><ul><li>This platform operates entirely offline, giving users full control over their deployment environment and ensuring data privacy.</li></ul></li><li><p><strong>Support for Multiple LLM Runners:</strong></p><ul><li>It supports various LLM runners, including Ollama and OpenAI-compatible APIs, making it versatile for different applications and user preferences.</li></ul></li><li><p><strong>Built-in Inference Engine for RAG:</strong></p><ul><li>The platform includes a built-in inference engine for Retrieval-Augmented Generation (RAG), enhancing capabilities for AI-driven interactions.</li></ul></li></ol><p>To get started with Open-WebUI, you can easily install it using Docker:</p><div><pre><code>\ngit clone https://github.com/open-webui/open-webui.git\nopen-webui\n\n\ndocker-compose up </code></pre></div><p>Here's how you can customize the OpenAI API URL within the Open-WebUI configuration:</p><div><pre><code></code></pre></div><p>This example illustrates how to tailor the platform to connect with different API endpoints, ensuring flexibility for your AI projects!</p><p>With an impressive  and a flurry of recent updates on GitHub, Subtrace is making waves in the world of data tracking and analysis! This powerful tool is designed to simplify the process of monitoring and visualizing data across various sources, enabling users to gain actionable insights effortlessly. Whether you're a developer seeking to enhance your applications or a data enthusiast looking to streamline your analytics, Subtrace provides the tools you need to elevate your data game!</p><h3>\n  \n  \n  Key Features of Subtrace:\n</h3><ol><li><p><strong>Wireshark for Docker Containers:</strong></p><ul><li>Subtrace allows developers to monitor and analyze incoming and outgoing requests to their Docker containers, akin to how Wireshark operates for network traffic.</li></ul></li><li><p><strong>Out-of-the-Box Functionality:</strong></p><ul><li>The tool integrates seamlessly into existing workflows without requiring any code changes, making it ready to use right away.</li></ul></li><li><p><strong>Detailed Request Monitoring:</strong></p><ul><li>Users can access comprehensive insights into server interactions, including full request payloads, headers, status codes, and latency.</li></ul></li><li><p><strong>Minimal Performance Overhead:</strong></p><ul><li>With less than 100Âµs of performance overhead, Subtrace ensures that monitoring does not significantly impact application performance.</li></ul></li></ol><p>To install Subtrace and get started with monitoring your Docker containers, simply follow these steps:</p><div><pre><code>\ndocker pull subtrace/subtrace\n\n\ndocker run  8080:8080 subtrace/subtrace\n</code></pre></div><p>Here's a quick example of how to monitor requests in a Docker container:</p><div><pre><code></code></pre></div><p>This example demonstrates how easy it is to start monitoring requests while using Subtrace alongside any programming language!</p><p>With an astounding  and a surge of recent activity on GitHub, Exo is capturing the attention of developers everywhere! This innovative tool is designed to simplify and enhance the development experience, providing a robust framework for building and deploying applications effortlessly. Whether youâ€™re a seasoned developer or just starting out, Exo empowers you to create powerful, efficient solutions with ease and flair!</p><ol><li><ul><li>Exo enables users to run their own AI cluster using everyday devices, making powerful AI capabilities accessible right from home.</li></ul></li><li><p><strong>Automatic Device Discovery:</strong></p><ul><li>The platform automatically detects available devices on the network, simplifying the setup process for creating a unified AI cluster without manual configuration.</li></ul></li><li><ul><li>Exo provides a ChatGPT-compatible API, allowing users to run models on their own hardware with just a simple change in their application code.</li></ul></li><li><p><strong>Dynamic Model Partitioning:</strong></p><ul><li>The system optimally splits AI models based on available device resources, enabling users to run larger models than typically possible on a single device.</li></ul></li></ol><h3>\n  \n  \n  Installation Instructions:\n</h3><p>To install Exo, follow these steps to set up the environment from the source:</p><div><pre><code>\ngit clone https://github.com/yourusername/exo.git\nexo\n\n\npip  requirements.txt\n\n\npython main.py\n</code></pre></div><p>Here's a quick example of how to leverage the ChatGPT-compatible API in your application:</p><div><pre><code></code></pre></div><p>This example demonstrates how easily you can start utilizing Exo's capabilities to enhance your AI applications!</p><p>With an impressive  and a flurry of recent activity on GitHub, MoneyPrinterTurbo is making waves in the developer community! This innovative tool simplifies the process of generating and managing financial data, enabling users to create realistic datasets for testing and analysis effortlessly. Whether you're a data analyst, developer, or researcher, MoneyPrinterTurbo empowers you to streamline your financial simulations with style and efficiency!</p><h3>\n  \n  \n  Key Features of MoneyPrinterTurbo:\n</h3><ol><li><p><strong>Automated Video Generation:</strong></p><ul><li>Create high-definition videos automatically by providing a theme or keywords, complete with scripts, subtitles, and background music.</li></ul></li><li><ul><li>Access the project through an intuitive web interface or integrate it seamlessly via a robust API, catering to diverse user needs.</li></ul></li><li><ul><li>Generate multiple videos at once, allowing users to select their preferred versions for increased efficiency in content creation.</li></ul></li><li><p><strong>Voice Synthesis and Subtitle Generation:</strong></p><ul><li>Utilize realistic voice synthesis options and automatically generated subtitles, with customizable settings for font, color, and size.</li></ul></li></ol><h3>\n  \n  \n  Installation Instructions:\n</h3><p>To quickly get started with MoneyPrinterTurbo, follow these installation steps:</p><div><pre><code>\ngit clone https://github.com/yourusername/MoneyPrinterTurbo.git\nMoneyPrinterTurbo\n\n\nconda create  MoneyPrinterTurbo 3.11\nconda activate MoneyPrinterTurbo\n\n\npip  requirements.txt\n\n\ndocker-compose up\n</code></pre></div><h3>\n  \n  \n  Example Code Snippet for Video Generation:\n</h3><p>Here's a simple example of how to generate a video using MoneyPrinterTurbo:</p><div><pre><code></code></pre></div><p>This example illustrates how to effortlessly create content using the powerful features of MoneyPrinterTurbo!</p><p>With an impressive  and exciting recent activity on GitHub, the WeChat Bot is quickly becoming a favorite among developers! This versatile tool allows users to create and deploy powerful bots for WeChat, automating interactions and enhancing user engagement through seamless conversations. Whether you're looking to simplify customer support or develop interactive experiences, the WeChat Bot empowers you to elevate your projects with ease and efficiency!</p><h3>\n  \n  \n  Key Features of WeChat Bot:\n</h3><ol><li><p><strong>Automatic Message Responses:</strong></p><ul><li>Built with  and , the bot efficiently automates replies to WeChat messages, making it easier to manage conversations and interactions.</li></ul></li><li><ul><li>Users can set up the bot in just four simple steps, taking approximately two minutes, which makes it incredibly user-friendly for those with varying technical backgrounds.</li></ul></li><li><p><strong>Multiple AI Service Integrations:</strong></p><ul><li>The bot supports various AI services, including DeepSeek and ChatGPT, allowing users to customize their experience by selecting the service that best fits their needs.</li></ul></li><li><p><strong>Encouragement of Community Contributions:</strong></p><ul><li>The repository invites users to star the project and contribute improvements, fostering a collaborative environment for ongoing enhancements and new features.</li></ul></li></ol><p>To get started with WeChat Bot, follow these installation steps:</p><div><pre><code>\ngit clone https://github.com/yourusername/wechat-bot.git\nwechat-bot\n\n .env.example .env\n\n\nnano .env\n\n\nnpm \nnpm run dev\n</code></pre></div><h3>\n  \n  \n  Example Code Snippet for Configuration:\n</h3><p>Here's how you can configure the bot to use the ChatGPT AI service:</p><div><pre><code># In your .env file, add the following lines\nCHATGPT_API_KEY=your_chatgpt_api_key_here\nDEEPSEEK_FREE_TOKEN=your_deepseek_token_here\n</code></pre></div><p>This snippet shows how easy it is to set up the WeChat Bot to leverage powerful AI services for seamless interaction!</p><p>With a remarkable  on GitHub and a flurry of recent activity, Lucide is quickly becoming a go-to choice for developers! This dynamic icon library offers a comprehensive collection of beautifully crafted icons designed for use in web and mobile applications, enabling developers to enhance their projects with stunning visuals effortlessly. Whether you're building a new app or refreshing an existing one, Lucide provides the versatility and quality you need to elevate your design game!</p><ol><li><ul><li>Lucide boasts , making it a versatile resource for both digital and non-digital projects, ideal for enhancing designs across various applications.</li></ul></li><li><ul><li>The library offers multiple official packages, including support for popular frameworks such as , and more, ensuring seamless integration into diverse development environments.</li></ul></li><li><p><strong>Figma Plugin Integration:</strong></p><ul><li>With a dedicated , designers can easily access and incorporate Lucide's icons directly into their design workflows, streamlining the creative process.</li></ul></li><li><ul><li>Lucide promotes open-source collaboration, encouraging users to contribute through documentation edits, issue reporting, and joining the active  for support and interaction.</li></ul></li></ol><p>To get started with Lucide, follow these installation steps:</p><div><pre><code>\nnpm lucide\n\n\nnpm lucide-react\n</code></pre></div><h3>\n  \n  \n  Example Code Snippet for Usage:\n</h3><p>Here's how to use an icon from the Lucide library in a React component:</p><div><pre><code>Capture amazing moments!</code></pre></div><p>This snippet showcases the ease of integrating Lucide icons into your React applications, enhancing both functionality and aesthetic appeal!</p><p>With an impressive  on GitHub and a surge of recent activity, Fabric is making waves in the developer community! This powerful open-source toolkit is designed to simplify and streamline the process of building beautiful user interfaces, allowing developers to create stunning applications with ease. Whether you're crafting a complex web app or a sleek mobile interface, Fabric provides the robust resources you need to elevate your design and enhance user experience!</p><ol><li><p><strong>Modular Problem-Solving Approach:</strong></p><ul><li>Fabric promotes breaking down challenges into individual components, allowing users to systematically apply AI solutions for more effective problem-solving.</li></ul></li><li><ul><li>It provides a powerful way to <strong>collect and integrate AI prompts</strong>, known as , simplifying the process of discovering and utilizing prompts for various tasks.</li></ul></li><li><ul><li>Users can leverage Patterns for a variety of tasks such as extracting content from YouTube videos, summarizing academic papers, creating tailored writing prompts, and even generating AI art prompts.</li></ul></li><li><ul><li>Designed specifically to address the integration challenges of AI in daily life, Fabric helps users seamlessly incorporate AI tools into their routines, enhancing creativity and productivity.</li></ul></li></ol><p>To get started with Fabric, follow these installation instructions:</p><div><pre><code>\npip fabric\n\n\ngit clone https://github.com/yourusername/fabric.git\nfabric\npip  requirements.txt\n</code></pre></div><h3>\n  \n  \n  Example Code Snippet for Usage:\n</h3><p>Here's a quick example of using a Pattern in Fabric to summarize content:</p><div><pre><code></code></pre></div><p>This snippet demonstrates how easily you can implement Patterns in Fabric to streamline your tasks and unlock the potential of AI!</p><p>Boasting an impressive  on GitHub and a flurry of recent activity, UV is capturing the attention of developers everywhere! This powerful open-source tool is designed to enhance the user experience by providing a versatile framework for building stunning user interfaces with ease. Whether you're creating interactive web applications or dynamic mobile experiences, UV empowers you to bring your designs to life while ensuring optimal performance and accessibility!</p><ol><li><p><strong>High-Performance Package Management:</strong></p><ul><li>UV is , significantly improving the speed of package installations and dependency management, making it a go-to tool for Python developers.</li></ul></li><li><p><strong>Comprehensive Project Management:</strong></p><ul><li>It consolidates multiple tools into one, replacing , , and , while offering a universal lockfile and managing dependencies efficiently.</li></ul></li><li><p><strong>Script and Tool Management:</strong></p><ul><li>UV allows users to run scripts with inline dependency metadata and install command-line tools easily, providing flexibility in managing both scripts and development tools.</li></ul></li><li><p><strong>Easy Installation Options:</strong></p><ul><li>UV can be installed via simple commands using PowerShell or pip, making it accessible across different operating systems without requiring Rust or Python beforehand.</li></ul></li></ol><p>To install UV, you can use one of the following commands:</p><div><pre><code>\npip uv\n\n\npipx uv\n</code></pre></div><h3>\n  \n  \n  Example Code Snippet for Usage:\n</h3><p>Here's how to initialize a new project and add dependencies using UV:</p><div><pre><code>\nuv init example\n\nexample\n\n\nuv add ruff\n</code></pre></div><p>This snippet showcases the simplicity of setting up a new project and managing dependencies with UV, streamlining your development workflow!</p><p>With an impressive  on GitHub and a surge of recent activity, ComfyUI is making waves in the developer community! This innovative open-source framework is designed to streamline the development of user interfaces, allowing developers to create stunning, responsive applications effortlessly. Whether you're building a web app or a mobile interface, ComfyUI provides the tools you need to enhance user experience and productivity, making it a must-have in your development toolkit!</p><ol><li><p><strong>Modular Diffusion Model GUI:</strong></p><ul><li>ComfyUI serves as a powerful and modular graphical user interface for creating and managing complex Stable Diffusion workflows, making it accessible for users without coding experience.</li></ul></li><li><ul><li>The intuitive graph/nodes/flowchart-based interface allows users to design and execute intricate workflows effortlessly, enabling seamless interaction with various image and video models.</li></ul></li><li><p><strong>Asynchronous Queue System:</strong></p><ul><li>It features an asynchronous queue system that enhances performance and responsiveness during complex computations, ensuring efficient processing of tasks.</li></ul></li><li><ul><li>ComfyUI supports a wide range of image and video models, such as SD1.x, SD2.x, Stable Video Diffusion, and more, allowing for versatile creative applications.</li></ul></li></ol><p>To install ComfyUI, you can refer to the documentation provided in the repository. Typically, you would clone the repository and install the necessary dependencies:</p><div><pre><code>\ngit clone https://github.com/your-username/ComfyUI.git\n\nComfyUI\n\n\npip  requirements.txt\n</code></pre></div><h3>\n  \n  \n  Example Code Snippet for Usage:\n</h3><p>Hereâ€™s how to create and run a simple workflow using ComfyUI:</p><div><pre><code></code></pre></div><p>This example illustrates the ease of setting up a workflow to generate images using different models within ComfyUI, showcasing its user-friendly capabilities.</p><p>With an impressive  on GitHub and a flurry of recent activity, Sniffnet is quickly becoming the go-to tool for network monitoring enthusiasts! This innovative open-source application empowers users to analyze their network traffic with ease, providing valuable insights into data flows and connections. Whether you're a developer looking to debug your applications or a security professional aiming to enhance your network defenses, Sniffnet offers a powerful, user-friendly interface that elevates your network management experience!</p><h3>\n  \n  \n  Key Features of Sniffnet:\n</h3><ol><li><p><strong>Comprehensive Network Monitoring:</strong></p><ul><li>Sniffnet allows users to efficiently , offering real-time statistics and visual charts to analyze data usage and patterns seamlessly.</li></ul></li><li><p><strong>Customizable Traffic Filtering:</strong></p><ul><li>The application provides users the ability to  on observed traffic, enabling focused monitoring based on specific data types or sources.</li></ul></li><li><p><strong>Protocol and Service Identification:</strong></p><ul><li>Sniffnet can identify over <strong>6000 upper layer services, protocols, and potential threats</strong> like trojans and worms, enhancing network security management.</li></ul></li><li><ul><li>Users can <strong>export detailed capture reports as PCAP files</strong>, facilitating further analysis or record-keeping for their network activities.</li></ul></li></ol><p>To install Sniffnet using Rustâ€™s package manager, ensure Rust is installed, and then run the following command:</p><div><pre><code>cargo sniffnet </code></pre></div><p>Once installed, you can select a network adapter and start monitoring traffic with just a few commands in the terminal:</p><div><pre><code>\nsniffnet\n\n</code></pre></div><p>This straightforward setup allows users to dive right into analyzing their network traffic effectively!</p><p>With an impressive  on GitHub and a surge of recent activity, Checkmate is making waves in the developer community! This innovative open-source tool is designed to streamline the code review process, making it easier than ever for teams to collaborate, identify issues, and ensure high-quality code before deployment. Whether you're working solo or in a large team, Checkmate enhances productivity and fosters a culture of code excellenceâ€”truly a must-have for any modern development workflow!</p><h3>\n  \n  \n  Key Features of Checkmate:\n</h3><ol><li><ul><li>Checkmate continuously tracks <strong>server uptime, response times, and incidents</strong>, ensuring users are promptly informed about any issues with their monitored services.</li></ul></li><li><p><strong>Capture Agent Integration:</strong></p><ul><li>The optional  enhances monitoring capabilities by retrieving detailed performance metrics like CPU, RAM, and disk usage, providing deeper insights into server health.</li></ul></li><li><p><strong>Optimized Resource Management:</strong></p><ul><li>Designed for efficiency, Checkmate boasts a , allowing it to monitor multiple servers with minimal CPU and memory usage, making it ideal for scalable environments.</li></ul></li><li><p><strong>Alerts and Notifications:</strong></p><ul><li>Users receive  and can manage  to stay updated on the performance and availability of their services, fostering proactive incident management.</li></ul></li></ol><p>To get started with Checkmate, you can deploy it using Docker with a simple one-click command. Hereâ€™s how to install the Capture agent:</p><div><pre><code>\ngit clone https://github.com/yourusername/checkmate-capture.git\ncheckmate-capture\nnpm npm start\n</code></pre></div><p>After installation, you can monitor your servers by configuring your Checkmate settings:</p><div><pre><code>: : ,\n      : ,\n      : </code></pre></div><p>This setup ensures that Checkmate regularly checks the availability and performance of your specified servers, keeping you informed every step of the way!</p><p>We encourage you to dive into these exciting projects and see how they can enhance your development journey! Donâ€™t forget to star your favorite repositories to show your support and help others discover them too. Be sure to follow us for future updates, as we share new trending projects every week that you won't want to miss. Happy coding, and let's keep exploring together!</p>","contentLength":21338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Real Python Podcast â€“ Episode #240: Telling Effective Stories With Your Python Visualizations","url":"https://realpython.com/podcasts/rpp/240/","date":1740139200,"author":"Real Python","guid":8523,"unread":true,"content":"<p>How do you make compelling visualizations that best convey the story of your data? What methods can you employ within popular Python tools to improve your plots and graphs? This week on the show, Matt Harrison returns to discuss his new book \"Effective Visualization: Exploiting Matplotlib &amp; Pandas.\"</p>","contentLength":300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explore the Future: Trending GitHub Projects Revolutionizing Tech ğŸš€âœ¨","url":"https://dev.to/bruh_buh_f683772f171823db/explore-the-future-trending-github-projects-revolutionizing-tech-2ifh","date":1740137530,"author":"Bruh Buh","guid":8532,"unread":true,"content":"<p>Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.</p><p>With an impressive 14,600 stars and vibrant recent activity, Composio is rapidly gaining traction as a leading framework for building open-source AI agents. This innovative platform empowers developers to seamlessly integrate and automate interactions across multiple applications, making it easier than ever to create intelligent solutions that drive efficiency and enhance productivity. Dive into the world of Composio and unlock the true potential of AI in your projects!</p><ol><li><p>:</p><ul><li>Built specifically for AI agents, ensuring reliability and robust functionality for seamless integration.</li></ul></li><li><ul><li>Connects with over 250 applications, including GitHub, Notion, and Slack, enhancing development versatility.</li></ul></li><li><p><strong>Advanced Search Capabilities</strong>:</p><ul><li>Users can perform searches through platforms like Google and Exa, streamlining information retrieval.</li></ul></li><li><ul><li>Supports custom tools and extensions, allowing developers to tailor the framework to their specific needs.</li></ul></li></ol><p>:\nTo get started with Composio, you can easily install it using pip:</p><p>If you wish to use the OpenAI plugin, run:</p><div><pre><code>pip composio-openai\n</code></pre></div><p><strong>Sample Code for Initialization</strong>:\nHereâ€™s a simple snippet to import libraries and set up the OpenAI client along with Composio Tool Set:</p><div><pre><code></code></pre></div><p>With an impressive 11,589 stars and a surge of recent activity, Minmind is making waves as a groundbreaking tool in the AI landscape. This innovative framework enables developers to train a 26M-parameter GPT model from scratch in just two hours, empowering users to harness the power of large language models with unprecedented ease. Dive into Minmind and experience the future of AI development at your fingertips!</p><ol><li><ul><li>Train a lightweight language model for under $3 in just 2 hours on a personal GPU, making AI accessible to everyone.</li></ul></li><li><p><strong>Open-Source Implementation</strong>:</p><ul><li>The project provides a complete open-source framework, including training processes like data cleaning, pretraining, and fine-tuning, all implemented from scratch using PyTorch.</li></ul></li><li><ul><li>MiniMind's model is only 25.8MB, significantly smaller than traditional models, allowing for easy deployment and experimentation.</li></ul></li><li><ul><li>Serves as both a practical tool for building language models and an educational guide for those eager to learn about LLM training and architecture.</li></ul></li></ol><p>:\nTo get started with MiniMind, you can clone the repository and install the necessary dependencies:</p><div><pre><code>git clone https://github.com/yourusername/minimind.git\nminimind\npip  requirements.txt\n</code></pre></div><p>:\nHereâ€™s a simple code snippet to initiate model training:</p><div><pre><code></code></pre></div><p>With an impressive 23,520 stars and a flurry of recent activity, MoneyPrinterTurbo is quickly becoming a favorite among developers looking to streamline their financial processes! This powerful open-source tool is designed to automate and optimize budgeting, expense tracking, and financial reporting, making it easier than ever to manage your finances efficiently. Dive into MoneyPrinterTurbo and experience the future of financial management at your fingertips!</p><h3>\n  \n  \n  Key Features of MoneyPrinterTurbo\n</h3><ol><li><p><strong>Automated Video Production</strong>:</p><ul><li>Generate high-definition videos automatically by simply providing a theme or keywords, along with scripts, subtitles, and background music.</li></ul></li><li><p>:</p><ul><li>Access the platform through both a web interface and an API, making it versatile for different user preferences and technical skills.</li></ul></li><li><ul><li>Create multiple videos at once and customize various parameters, giving users flexibility and control over their content.</li></ul></li><li><p><strong>Realistic Voice Synthesis</strong>:</p><ul><li>Choose from a variety of voice synthesis options to enhance video narration, ensuring a professional and engaging presentation.</li></ul></li></ol><p>:\nTo get started with MoneyPrinterTurbo, you can clone the repository and set up a Python virtual environment:</p><div><pre><code>\ngit clone https://github.com/harry0703/MoneyPrinterTurbo.git\nMoneyPrinterTurbo\n\n\nconda create  MoneyPrinterTurbo 3.11\nconda activate MoneyPrinterTurbo\n\n\npip  requirements.txt\n</code></pre></div><p>:\nYou can start the application using Docker with the following command:</p><p>Once running, access the web interface at .</p><p>With an astounding 77,216 stars and a surge of recent activity, Open-WebUI is making waves in the developer community! This innovative open-source project is designed to create user-friendly web interfaces for machine learning models, making advanced AI technology easily accessible to everyone. Dive into Open-WebUI and discover how it can transform your applications with seamless integration and intuitive design!</p><h3>\n  \n  \n  Key Features of Open WebUI\n</h3><ol><li><p><strong>Extensible and Offline Functionality</strong>:</p><ul><li>Open WebUI is designed as an extensible platform that operates completely offline, making it ideal for users who prefer a robust local solution for AI applications.</li></ul></li><li><p><strong>Support for Multiple LLM Runners</strong>:</p><ul><li>The platform supports various Large Language Model runners, including Ollama and OpenAI APIs, providing users with flexibility to choose the best model for their specific needs.</li></ul></li><li><p><strong>Built-in Inference Engine for RAG</strong>:</p><ul><li>The integrated inference engine supports Retrieval-Augmented Generation (RAG), enhancing the platform's capabilities for complex AI tasks and allowing for rich interactions with contextual information.</li></ul></li><li><p><strong>Effortless Setup with Docker</strong>:</p><ul><li>Users can quickly install Open WebUI using Docker, with straightforward commands for setting up various configurations and tagging options tailored to their use cases.</li></ul></li></ol><p>:\nTo get started with Open WebUI, you can easily set it up using Docker:</p><div><pre><code>\ndocker pull openwebui/open-webui:latest\n\n\ndocker run  8501:8501 openwebui/open-webui:latest\n</code></pre></div><p>Once the container is running, access the web interface by navigating to  in your browser. This allows you to start using the platform right away!</p><p>With an impressive 1,434 stars and a flurry of recent activity, Subtrace is quickly gaining traction in the developer community! This innovative open-source tool is designed to simplify the process of tracking and analyzing metrics across various systems, making it easier than ever to gain insights into performance and usage. Dive into Subtrace and unlock the power of data-driven decision-making for your projects!</p><ol><li><p><strong>Wireshark for Docker Containers</strong>:</p><ul><li>Subtrace functions like Wireshark, offering specialized monitoring for Docker containers to analyze incoming and outgoing requests seamlessly.</li></ul></li><li><p><strong>Out-of-the-Box Functionality</strong>:</p><ul><li>The tool requires no code changes and can be integrated into existing workflows immediately, simplifying the setup process for developers.</li></ul></li><li><p><strong>Comprehensive Request Insights</strong>:</p><ul><li>Users can access detailed request information, including full payloads, headers, status codes, and latency, facilitating thorough troubleshooting and performance analysis.</li></ul></li><li><p><strong>Minimal Performance Overhead</strong>:</p><ul><li>With a performance overhead of less than 100 microseconds, Subtrace ensures that monitoring does not disrupt application performance or responsiveness.</li></ul></li></ol><p>:\nTo get started with Subtrace, you can easily install it using Docker:</p><div><pre><code>\ndocker pull subtrace/subtrace:latest\n\n\ndocker run  8080:8080 subtrace/subtrace:latest\n</code></pre></div><p>Once the container is running, access the Subtrace interface by navigating to  in your browser to start monitoring your Docker containers!</p><p>With an impressive 24,526 stars and a burst of recent activity, Exo is capturing the attention of developers everywhere! This powerful open-source framework is designed to streamline the process of building lightweight, modular applications, enabling teams to create high-performance software with ease. Jump into Exo and discover how it can elevate your development experience to new heights!</p><ol><li><ul><li>Exo enables users to create a personal AI cluster using everyday devices like iPhones, Raspberry Pis, and NVIDIA GPUs, making advanced AI technology accessible to a wide audience.</li></ul></li><li><p><strong>Automatic Device Discovery</strong>:</p><ul><li>The tool automatically discovers devices on the network, simplifying the setup process and enhancing user experience by eliminating the need for manual configurations.</li></ul></li><li><ul><li>With a ChatGPT-compatible API, users can easily run models on their hardware with just a one-line change in their applications, streamlining integration into existing workflows.</li></ul></li><li><p><strong>Flexible Model Partitioning Strategies</strong>:</p><ul><li>Exo supports various partitioning strategies, such as ring memory weighted partitioning, allowing efficient distribution of models across multiple devices based on their memory capacity.</li></ul></li></ol><p>:\nTo install Exo from source, follow these steps (ensure you have Python 3.12.0 or higher):</p><div><pre><code>\ngit clone https://github.com/exo-labs/exo.git\n\nexo\n\n\npip  requirements.txt\n\n\npython main.py\n</code></pre></div><p>Once installed, Exo will automatically discover available devices and allow you to set up your AI cluster effortlessly!</p><p>With an impressive 29,419 stars and a surge of recent activity, Fabric is capturing the excitement of developers everywhere! This cutting-edge open-source framework is designed to streamline the deployment and management of applications in multi-cloud environments, making it easier than ever to build and scale robust software solutions. Dive into Fabric and discover how it can transform your development and deployment processes!</p><ol><li><p><strong>Modular Approach to Problem Solving</strong>:</p><ul><li>Fabric encourages breaking down complex problems into manageable components, allowing users to systematically apply AI solutions and enhance clarity in tackling challenges.</li></ul></li><li><p><strong>Integration of Prompts as Patterns</strong>:</p><ul><li>The framework allows users to collect and integrate prompts, referred to as , facilitating better organization and accessibility for applying relevant AI prompts to various tasks.</li></ul></li><li><p><strong>Diverse Range of Patterns</strong>:</p><ul><li>Fabric provides a variety of Patterns tailored for everyday activities, such as extracting insights from videos, assisting with essay writing, summarizing academic papers, generating AI art prompts, and content rating.</li></ul></li><li><p><strong>Open-Source Accessibility</strong>:</p><ul><li>As an open-source framework, Fabric is accessible and collaborative, inviting contributions from the community to continuously enhance its functionality and user experience.</li></ul></li></ol><p>:\nTo install Fabric, follow these simple instructions:</p><div><pre><code>\ngit clone https://github.com/yourusername/fabric.git\n\nfabric\n\n\npip  requirements.txt\n\n\npython main.py\n</code></pre></div><p>With these steps, you can easily set up Fabric and begin integrating AI into your projects!</p><p>With an impressive 40,664 stars and a flurry of recent activity, UV is making waves in the developer community! This powerful open-source framework is designed to simplify the development of user interfaces, enabling developers to create dynamic and responsive applications effortlessly. Dive into UV and unlock the potential to enhance your UI projects like never before!</p><ol><li><p><strong>High-Performance Package Management</strong>:</p><ul><li>UV is an extremely fast Python package and project manager developed in Rust, offering a performance improvement of  faster than traditional tools like .</li></ul></li><li><p><strong>Comprehensive Project Management</strong>:</p><ul><li>It consolidates multiple tools into one, replacing , , and others, to simplify dependency management with features like a universal lockfile for consistent environments.</li></ul></li><li><p><strong>Script and Command Execution</strong>:</p><ul><li>UV allows users to run scripts with inline dependency metadata and execute command-line tools in isolated environments, enhancing usability and project workflows.</li></ul></li><li><p><strong>Flexible Installation Methods</strong>:</p><ul><li>UV can be installed via various methods, including PowerShell, PyPI, and , making it accessible for users with different preferences and setups.</li></ul></li></ol><p>:\nTo install UV, you can choose any of the following methods:</p><div><pre><code>\npip uv\n\n\npipx uv\n\n\nirm get.uv.sh | iex\n</code></pre></div><p><strong>Initializing a New Project</strong>:\nAfter installation, you can easily initialize a new project with:</p><p>This command sets up a project directory with the necessary structure, allowing you to manage dependencies efficiently. Enjoy the speed and simplicity of UV in your development workflow!</p><p>With an impressive 67,981 stars and vibrant recent activity, ComfyUI is capturing the attention of developers everywhere! This powerful open-source framework is designed for building intuitive and user-friendly interfaces, making it easier than ever to create stunning applications. Dive into ComfyUI and elevate your UI development experience to new heights!</p><ol><li><p><strong>Modular GUI for Diffusion Models</strong>:</p><ul><li>ComfyUI offers a powerful and modular graphical interface specifically designed for building and executing advanced stable diffusion pipelines, making it accessible for users of all skill levels.</li></ul></li><li><p><strong>Comprehensive Model Support</strong>:</p><ul><li>The tool supports a wide array of image and video models, including SD1.x, SD2.x, and various video models like Stable Video Diffusion, enabling users to handle diverse multimedia tasks seamlessly.</li></ul></li><li><p><strong>Asynchronous Queue System</strong>:</p><ul><li>An asynchronous queue system enhances task processing efficiency, allowing users to manage and execute multiple operations effectively without lag.</li></ul></li><li><p><strong>Flexible Workflow Management</strong>:</p><ul><li>Users can easily load, save, and replicate complex workflows, complete with seeds and configurations, using formats like PNG and JSON for streamlined project management.</li></ul></li></ol><p>:\nTo get started with ComfyUI, you can install it using the following command:</p><div><pre><code>\ngit clone https://github.com/yourusername/comfyui.git\n\ncomfyui\n\n\npip  requirements.txt\n</code></pre></div><p>:\nAfter installation, simply run the ComfyUI interface with:</p><p>Now you're ready to design and execute your diffusion workflows with ease! Enjoy the flexibility and power ComfyUI offers in your projects!</p><p>With an impressive 22,326 stars and vibrant recent activity, Sniffnet is making waves in the networking community! This powerful open-source tool serves as a network packet sniffer and analysis platform, allowing users to monitor and inspect network traffic with remarkable ease. Dive into Sniffnet to gain deeper insights into your network's performance and security, all while enjoying a user-friendly experience!</p><ol><li><p><strong>Network Traffic Monitoring</strong>:</p><ul><li>Sniffnet enables users to monitor their Internet traffic comfortably, providing insights into network activities with real-time statistics and visualizations.</li></ul></li><li><ul><li>The application boasts an intuitive design that makes it accessible for users of all skill levels, ensuring ease of navigation and operation.</li></ul></li><li><p><strong>Advanced Filtering and Reporting</strong>:</p><ul><li>Users can apply filters to observed traffic and export detailed reports as PCAP files, facilitating focused analysis and record-keeping.</li></ul></li><li><p><strong>Cross-Platform Compatibility</strong>:</p><ul><li>Sniffnet is designed to run on various operating systems, making it a versatile choice for a wide range of users.</li></ul></li></ol><p>:\nTo install Sniffnet, you can use Homebrew on macOS and Linux with the following command:</p><p><strong>Alternatively, for Rust users</strong>, install directly from Crates.io:</p><div><pre><code>cargo sniffnet </code></pre></div><p>:\nAfter installation, launch Sniffnet using:</p><p>Now you're ready to monitor your network traffic with ease and efficiency!</p><p>With an impressive 4,014 stars and a flurry of recent activity, Checkmate is quickly becoming a favorite among developers! This powerful open-source tool is designed for automating and managing continuous integration workflows, simplifying the process of testing and deploying code. Dive into Checkmate to streamline your development pipeline and enhance team collaboration like never before!</p><h3>\n  \n  \n  Key Features of Checkmate\n</h3><ol><li><p><strong>Real-Time Uptime Monitoring</strong>:</p><ul><li>Checkmate provides robust real-time monitoring of server uptime, response times, and incidents, ensuring that users can maintain server health and reliability.</li></ul></li><li><ul><li>As a self-hosted application, Checkmate gives users full control over their monitoring environment without relying on third-party services, making it a flexible choice for any organization.</li></ul></li><li><p><strong>Comprehensive Alerts and Reports</strong>:</p><ul><li>Users receive real-time alerts about the status of their monitored services, along with detailed reports on availability and performance metrics, enabling proactive responses to incidents.</li></ul></li><li><p><strong>Agent Integration for Enhanced Insights</strong>:</p><ul><li>The Capture agent can be integrated to gather additional metrics such as CPU, RAM, and disk usage, providing deeper insights into server performance.</li></ul></li></ol><p>:\nTo get started with Checkmate, you can deploy it using one of the one-click options available. Hereâ€™s how to use  for Docker deployment:</p><div><pre><code>\ncurl  https://get.coolify.com | bash\n</code></pre></div><p><strong>To install the Capture agent</strong>, you might need to follow specific instructions provided in its repository:</p><div><pre><code>\ngit clone https://github.com/your-username/capture.git\n\ncapture\n\n\nnpm \nnpm start\n</code></pre></div><p>Now you're ready to monitor your servers with Checkmate!</p><p>With an impressive 14,751 stars and a surge of recent activity, pandas-ai is rapidly gaining traction in the developer community! This innovative library seamlessly integrates AI capabilities into the powerful pandas data manipulation framework, enabling users to perform complex data analysis and generate insights effortlessly. Dive into pandas-ai to elevate your data projects and unlock the full potential of your datasets with cutting-edge AI tools!</p><ol><li><p><strong>Natural Language Querying</strong>:</p><ul><li>Users can interact with their datasets using natural language queries, allowing for intuitive and accessible data analysis without extensive coding knowledge.</li></ul></li><li><p><strong>Multiple DataFrame Support</strong>:</p><ul><li>PandaAI allows users to work with multiple DataFrames simultaneously, facilitating complex comparisons and analyses across different datasets.</li></ul></li><li><ul><li>The platform can generate visualizations, enabling users to easily create charts and graphics based on their queries for better data interpretation.</li></ul></li><li><p><strong>User-Friendly Integration</strong>:</p><ul><li>With just a few lines of code, users can set up and interact with their datasets, making it straightforward to integrate PandaAI into various projects.</li></ul></li></ol><p>:\nYou can install PandaAI using pip or poetry. Hereâ€™s how to do it with pip:</p><div><pre><code>pip </code></pre></div><p>:\nHereâ€™s a quick demonstration of how to use PandaAI to query a DataFrame:</p><div><pre><code></code></pre></div><p>With these features and examples, PandaAI empowers users to harness the power of their data through accessible and visual analytics!</p><p>With a remarkable 64,738 stars and a flurry of recent activity, Uptime Kuma is a standout tool in the monitoring landscape! This self-hosted status monitoring solution empowers users to keep track of their services' uptime and performance effortlessly, providing real-time alerts and comprehensive insights. Dive into Uptime Kuma to ensure your applications are always running smoothly, and never miss a beat with its intuitive and user-friendly interface!</p><h3>\n  \n  \n  Key Features of Uptime Kuma\n</h3><ol><li><p><strong>Self-Hosted Monitoring Tool</strong>:</p><ul><li>Uptime Kuma is a user-friendly, self-hosted monitoring solution that allows users to track the uptime and performance of various services effortlessly.</li></ul></li><li><p><strong>Comprehensive Monitoring Capabilities</strong>:</p><ul><li>The tool supports multiple protocols, including HTTP(s), TCP, DNS, and more, enabling diverse monitoring scenarios such as push notifications and service checks.</li></ul></li><li><ul><li>Uptime Kuma can send alerts through over 90 notification services, including Telegram, Discord, and Slack, ensuring users stay informed about their service statuses.</li></ul></li><li><p>:</p><ul><li>With its responsive and fast UI/UX, users can easily navigate and manage their monitoring tasks, enhancing their overall experience.</li></ul></li></ol><p><strong>Installation Steps via Docker</strong>:\nTo get started with Uptime Kuma, you can easily install it using Docker with the following command:</p><div><pre><code>docker run always  3001:3001  uptime-kuma:/app/data  uptime-kuma louislam/uptime-kuma:1\n</code></pre></div><p>Once installed, you can access Uptime Kuma at .</p><p>:\nHereâ€™s a quick snippet to set up a notification through Discord:</p><div><pre><code></code></pre></div><p>With these features and simple installation instructions, Uptime Kuma makes monitoring your services straightforward and efficient!</p><p>As you dive into these exciting projects, we encourage you to explore their features and find the perfect tools for your needs! Don't forget to star your favorite repositories to show your support and help others discover them too. Be sure to follow along for future updates, as we share new trending projects every week to keep your toolkit fresh and up-to-date. Happy exploring!</p>","contentLength":19656,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discover the Future of Tech: Trending GitHub Projects Revolutionizing AI and Development ğŸš€","url":"https://dev.to/bruh_buh_f683772f171823db/discover-the-future-of-tech-trending-github-projects-revolutionizing-ai-and-development-57g8","date":1740134823,"author":"Bruh Buh","guid":8505,"unread":true,"content":"<p>Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.</p><p>With an impressive  on GitHub and a surge of recent activity,  is making waves in the open-source community! This innovative AI agent framework empowers developers to effortlessly integrate and automate interactions across various applications, streamlining workflows and enhancing productivity. Whether you're building intelligent systems or looking to simplify complex integrations, Composio is your go-to solution for creating powerful, seamless AI-driven experiences.</p><h3>\n  \n  \n  Main Features of Composio:\n</h3><ol><li><p>: Composio is specifically designed for creating reliable AI agents that are ready for production use, ensuring effectiveness in real-world applications.</p></li><li><p>: It integrates with over 250+ tools across various categories, including popular platforms like GitHub, Slack, and Google, as well as OS operations and search capabilities.</p></li><li><p>: Composio simplifies secure connections with support for various authentication protocols, including OAuth, API Keys, and Basic JWT.</p></li><li><p>: The framework supports custom tools and extensions, allowing developers to tailor Composio to meet specific project needs.</p></li></ol><h3>\n  \n  \n  Code Example for Installation and Agent Creation:\n</h3><p>:\nTo get started with Composio, install the core package using:</p><p>If you wish to integrate with OpenAI, also install the OpenAI plugin:</p><div><pre><code>pip composio-openai\n</code></pre></div><p>:\nHereâ€™s a code snippet demonstrating how to initialize your OpenAI client and create a Composio tool set:</p><div><pre><code></code></pre></div><p>This code sets the foundation for building powerful AI functionality into your applications using Composio.</p><p>With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a standout in the AI community! This groundbreaking project enables developers to train a 26M-parameter GPT model from scratch in just two hours, making advanced AI accessible like never before. Whether you're a seasoned AI enthusiast or just starting out, Minimind streamlines the process of model training, empowering you to unleash the full potential of artificial intelligence in your projects.</p><h3>\n  \n  \n  Main Features of Minimind:\n</h3><ol><li><p>: Train a lightweight language model for just  in approximately , making AI accessible for individuals and small teams.</p></li><li><p>: With a size of only , Minimind is  the size of GPT-3, allowing it to run on standard personal GPUs without extensive resources.</p></li><li><p><strong>Open Source with Comprehensive Resources</strong>: The project includes a complete codebase for building and training language models, along with features like Mixture of Experts (MoE), supervised fine-tuning, and dataset cleaning.</p></li><li><p><strong>Native PyTorch Implementation</strong>: The entire framework is built using native PyTorch, ensuring transparency and ease of understanding of the underlying mechanics.</p></li></ol><h3>\n  \n  \n  Code Example for Installation and Training:\n</h3><p>:\nTo get started with Minimind, simply install the required packages using:</p><p>(Ensure you have PyTorch installed based on your system's specifications.)</p><p>:\nHere's a code snippet demonstrating how to initiate training for the MiniMind model:</p><div><pre><code></code></pre></div><p>This example sets the stage for developing your own AI language model with minimal cost and time invested!</p><p>With an impressive  on GitHub and a surge of recent activity,  is rapidly becoming a go-to solution for developers seeking to enhance their financial applications! This powerful tool is designed to automate and optimize financial transactions, making it easier than ever to manage complex monetary operations. Whether youâ€™re building robust payment systems or streamlining budgeting processes, MoneyPrinterTurbo is your ultimate ally in creating efficient and scalable financial solutions!</p><h3>\n  \n  \n  Main Features of MoneyPrinterTurbo:\n</h3><ol><li><p><strong>Automatic Video Generation</strong>: Effortlessly create high-definition videos by inputting a theme or keywords, which generates scripts, subtitles, and background musicâ€”all without manual intervention.</p></li><li><p>: Accessible through both a user-friendly web interface and a robust API, allowing versatile integration for various applications and user preferences.</p></li><li><p>: Generate multiple videos simultaneously, giving users the flexibility to choose from a variety of options based on their needs.</p></li><li><p><strong>Voice Synthesis and Subtitle Generation</strong>: Enjoy realistic voice synthesis with multiple options, alongside automatic subtitle generation that you can customize for font, color, and size.</p></li></ol><h3>\n  \n  \n  Code Example for Installation and Deployment:\n</h3><p>:\nTo deploy MoneyPrinterTurbo using Docker, run the following commands:</p><div><pre><code>MoneyPrinterTurbo\ndocker-compose up\n</code></pre></div><p>After deployment, access the web interface at:</p><p><strong>Creating a Python Virtual Environment</strong>:\nAlternatively, you can set up a Python environment using conda with the following commands:</p><div><pre><code>git clone https://github.com/harry0703/MoneyPrinterTurbo.git\nMoneyPrinterTurbo\nconda create  MoneyPrinterTurbo 3.11\nconda activate MoneyPrinterTurbo\npip  requirements.txt\n</code></pre></div><p>This setup ensures you're ready to dive into video creation with MoneyPrinterTurbo!</p><p>With an impressive  on GitHub and a wave of recent activity,  is rapidly making waves in the developer community! This powerful tool is designed to streamline and enhance the workflow of modern development by simplifying the management of complex command-line environments, enabling users to create, manage, and share executable scripts effortlessly. Whether youâ€™re a seasoned developer or just starting out, exo is your go-to solution for boosting productivity and mastering your command-line experience!</p><ol><li><p>: Users can effortlessly set up their own AI cluster using everyday devices, enabling powerful AI processing without needing specialized hardware.</p></li><li><p><strong>Automatic Device Discovery</strong>:  automatically identifies devices on the network, simplifying the setup process by eliminating manual configurations.</p></li><li><p>: The software offers a ChatGPT-compatible API, allowing seamless integration of AI models into applications with minimal code changes.</p></li><li><p><strong>Flexible Model Partitioning</strong>: With support for various partitioning strategies,  optimizes resource allocation across devices, ensuring efficient utilization of memory and processing power.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p><strong>Installing exo from Source</strong>:\nTo install , you'll need a compatible environment. Here's how to get started:</p><div><pre><code>   git clone https://github.com/yourusername/exo.git\n   exo\n</code></pre></div><ol><li><strong>Set Up a Python Environment</strong>:\nEnsure you have Python 3.12.0 or higher:\n</li></ol><div><pre><code>   conda create  exo-env 3.12\n   conda activate exo-env\n</code></pre></div><ol><li>:\nInstall the necessary dependencies:\n</li></ol><div><pre><code>   pip  requirements.txt\n</code></pre></div><ol><li>:\nStart the exo service:\n</li></ol><p>This setup will get you started with exo, allowing you to harness its capabilities for AI model management and execution!</p><p>With an impressive  on GitHub and a surge of recent activity,  is quickly becoming the go-to tool for modern developers! This powerful framework is designed to simplify and streamline the process of deploying applications, making it easier than ever to manage infrastructure through code. Whether you're automating server management or orchestrating complex deployments, Fabric empowers you to enhance your workflow and boost productivity like never before!</p><ol><li><p>: Fabric allows users to break down complex challenges into smaller, manageable components, making it easier to apply AI solutions incrementally.</p></li><li><p><strong>Integration of Prompts as Patterns</strong>: The framework helps users collect, organize, and integrate AI prompts, referred to as , enhancing accessibility and usability.</p></li><li><p>: Fabric offers a variety of Patterns tailored for different tasks, such as extracting insights from media, assisting with essay writing, summarizing academic papers, and generating AI art prompts.</p></li><li><p><strong>Human-Centered Philosophy</strong>: The framework emphasizes a human-centric approach, focusing on how AI can augment creativity and solve real-life problems rather than replacing human efforts.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>:\nTo get started with Fabric, follow these installation steps:</p><div><pre><code>   git clone https://github.com/yourusername/fabric.git\n   fabric\n</code></pre></div><ol><li><strong>Set Up a Python Environment</strong>:\nEnsure you have Python installed (preferably version 3.12 or higher):\n</li></ol><div><pre><code>   python  venv fabric-env\n   fabric-env/bin/activate   \n   fabric-envcriptsctivate      </code></pre></div><ol><li><strong>Install Required Packages</strong>:\nInstall the necessary dependencies using pip:\n</li></ol><div><pre><code>   pip  requirements.txt\n</code></pre></div><ol><li>:\nStart using the framework:\n</li></ol><p>This setup will help you harness the power of Fabric and integrate AI capabilities seamlessly into your projects!</p><p>With a remarkable  on GitHub and a flurry of recent activity,  is making waves in the AI development community! This powerful, open-source framework is designed to streamline the training and deployment of large-scale AI models, enabling developers to harness cutting-edge technology effortlessly. Whether you're building sophisticated neural networks or optimizing existing models, ColossalAI provides the tools and flexibility to elevate your AI projects to new heights!</p><h3>\n  \n  \n  Main Features of Colossal-AI:\n</h3><ol><li><p><strong>Cost Reduction in Training</strong>: Colossal-AI achieves up to a  in training costs for large AI models using <strong>FP8 mixed precision training</strong>, allowing users to significantly lower expenses with minimal code changes.</p></li><li><p><strong>Instant Access to Compute Resources</strong>: Users can gain immediate access to high-end, on-demand compute resources for research without any setup, making it convenient for developers and researchers to get started quickly.</p></li><li><p><strong>Accelerated Inference Speed</strong>: The  feature enhances the inference speed of large AI models, doubling their efficiency and enabling faster deployment in real-world applications.</p></li><li><p>: Colossal-AI supports numerous well-known AI models, including LLaMA, GPT-3, BERT, and more, showcasing its versatility across various AI tasks and domains.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>:\nTo set up Colossal-AI, follow these installation steps:</p><ol><li>:\nSimply use pip to install Colossal-AI:\n</li></ol><ol><li>:\nIf you want to install from the source code, clone the repository:\n</li></ol><div><pre><code>   git clone https://github.com/hpcaitech/ColossalAI.git\n   ColossalAI\n</code></pre></div><ol><li>:\nAfter navigating to the cloned directory, build and install the package:\n</li></ol><ol><li>:\nFor a containerized setup, pull the Docker image:\n</li></ol><div><pre><code>   docker pull hpcaitech/colossalai:latest\n</code></pre></div><p>This setup will allow you to harness the power of Colossal-AI and start exploring its capabilities in training and deploying large-scale AI models!</p><p>With an impressive  on GitHub and a surge of recent activity,  is rapidly becoming a go-to resource for AI enthusiasts! This cutting-edge open-source framework empowers developers to create and customize chatbots and AI applications with ease, streamlining the integration of advanced language processing capabilities. Whether you're building a conversational agent or experimenting with innovative AI solutions, MetaGPT provides the tools and flexibility to transform your ideas into reality!</p><h3>\n  \n  \n  Main Features of MetaGPT:\n</h3><ol><li><p>: MetaGPT enables the assignment of different roles to various GPTs, facilitating collaboration among agents to efficiently tackle complex tasks and streamline the software development process.</p></li><li><p>: This feature allows users to solve real-world problems by interpreting data, enhancing the framework's utility in practical applications and making AI solutions more accessible.</p></li><li><p>: MetaGPT has received notable accolades, including a top 1.8% ranking for its paper at , underlining its credibility and contributions to the field of AI.</p></li><li><p><strong>Version Updates and Features</strong>: Continuous improvements are showcased through version releases, such as the introduction of the <strong>Retrieval-Augmented Generation</strong> module and support for multiple large language models (LLMs), enhancing versatility.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>:\nTo get started with MetaGPT, follow these installation steps:</p><ol><li>:\nEnsure you have Python 3.9 or later (but less than 3.12):\n</li></ol><ol><li>:\nYou can install MetaGPT directly from PyPI:\n</li></ol><ol><li>:\nAlternatively, you can clone the repository and install in editable mode:\n</li></ol><div><pre><code>   git clone https://github.com/MetaGPT/MetaGPT.git\n   MetaGPT\n   pip </code></pre></div><ol><li>:\nInitialize the configuration by creating a configuration file:\n</li></ol><div><pre><code> ~/.metagpt/config2.yaml\n</code></pre></div><p>By following these steps, you'll set up MetaGPT and be ready to harness its multi-agent capabilities for your software development needs!</p><p>With a remarkable  on GitHub and a flurry of recent activity,  is capturing the attention of developers everywhere! This powerful web framework is designed to simplify and enhance the development of high-performance applications, enabling developers to build robust, scalable solutions with ease. Whether you're creating a dynamic web app or an innovative API, uv provides the tools and flexibility to elevate your project to new heights!</p><ol><li><p>:  is designed to be , significantly improving package management efficiency. This performance boost makes it an excellent alternative for developers looking to optimize their workflow.</p></li><li><p><strong>Comprehensive Project Management</strong>: As a single tool that replaces multiple Python tools like , , and ,  simplifies dependency management with features such as universal lockfiles and automatic virtual environment creation.</p></li><li><p><strong>Script and Tools Management</strong>:  allows for easy management of script dependencies and execution, enabling users to run scripts with inline dependency metadata and provide a seamless way to manage command-line tools using ephemeral environments.</p></li><li><p>:  supports macOS, Linux, and Windows, making it accessible for developers across different operating systems.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>:\nYou can install  using several methods, depending on your preference:</p><ol><li><strong>Using curl for macOS and Linux</strong>:\n</li></ol><div><pre><code>   curl  https://astral.sh/uv/install.sh | sh\n</code></pre></div><ol><li><strong>PowerShell command for Windows</strong>:\n</li></ol><div><pre><code></code></pre></div><ol><li><strong>Using pipx for isolated installations</strong>:\n</li></ol><p>After installation, you can initialize a new project with:</p><p>This command creates a project directory named , enabling you to quickly set up and manage your Python projects!</p><p>With an impressive  on GitHub and a surge of recent activity,  is making waves in the React community! This innovative library serves as a collection of reusable, high-quality React components designed to simplify and accelerate your development process. Whether youâ€™re building a sleek user interface or enhancing an existing application, react-bits equips you with the essential tools to create stunning, efficient React applications with ease!</p><h3>\n  \n  \n  Main Features of React Bits:\n</h3><ol><li><p><strong>Extensive Collection of Animated Components</strong>:  boasts a large library of over , including text animations, backgrounds, and interactive elements, designed to enhance web applications with engaging visuals.</p></li><li><p><strong>Lightweight and Customizable</strong>: Each component is lightweight with minimal dependencies, allowing for efficient application performance. Additionally, components come with customization options via props, enabling developers to tailor them to their specific needs effortlessly.</p></li><li><p>: The components are designed to integrate smoothly into any modern React project, making it easy for developers to incorporate them without compatibility issues.</p></li><li><p>: Developers can choose from  of each componentâ€”JavaScript + CSS, JavaScript + Tailwind CSS, TypeScript + CSS, and TypeScript + Tailwind CSSâ€”providing flexibility to fit different project setups.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>To get started with , you can easily install it via the command line interface using :</p><p>After installation, refer to the comprehensive documentation at <a href=\"https://reactbits.dev\" rel=\"noopener noreferrer\">reactbits.dev</a> for guidance on how to utilize the components effectively. For example, you can import and use a text animation component as follows:</p><div><pre><code></code></pre></div><p>This simple integration showcases how you can enhance your application with animated components in just a few lines of code!</p><p>With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to solution in the developer community! This powerful open-source tool is designed to streamline the development of hands-free applications, enabling users to harness the potential of gesture recognition and voice control for a truly immersive experience. Whether you're building innovative interfaces or enhancing accessibility, OpenHands provides the essential framework to create intuitive, user-friendly applications that stand out!</p><h3>\n  \n  \n  Main Features of OpenHands:\n</h3><ol><li><p><strong>AI-Powered Development Agents</strong>: OpenHands enables agents to perform a wide range of tasks typically handled by human developers, such as modifying code, running commands, browsing the web, and calling APIs, making it a versatile tool for enhancing productivity.</p></li><li><p><strong>Docker Integration for Easy Setup</strong>: The platform leverages Docker for easy installation and deployment, allowing users to run OpenHands effortlessly in a containerized environment with just a few commands.</p></li><li><p><strong>Multiple Operational Modes</strong>: OpenHands supports various interaction modes, including friendly CLI access, scriptable headless operation, and integration with GitHub Actions for automating workflows, thus catering to different developer preferences.</p></li><li><p><strong>Comprehensive Documentation and Community Support</strong>: Users have access to extensive documentation and troubleshooting resources, as well as a vibrant community for contributions and engagement via platforms like Slack and Discord.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>To get started with , you can easily set it up using Docker by pulling the required image:</p><div><pre><code>docker pull docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik\n</code></pre></div><p>Next, you can run the OpenHands application with the following command, customizing it as needed:</p><div><pre><code>docker run  openhands  /var/run/docker.sock:/var/run/docker.sock  3000:3000 \n  docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik\n</code></pre></div><p>This command sets up OpenHands while mapping the necessary ports and volume for container interaction, allowing you to access the application at . Be sure to check the documentation for additional setup details and configuration options!</p><p>With a remarkable  on GitHub and a surge of recent activity,  is making waves in the developer community! This innovative open-source platform empowers users to create stunning and intuitive user interfaces with ease, leveraging the latest advancements in UI design and functionality. Whether you're a seasoned developer or just starting out, ComfyUI equips you with the tools to bring your creative visions to life effortlessly!</p><h3>\n  \n  \n  Main Features of ComfyUI:\n</h3><ol><li><p>: ComfyUI offers a powerful and modular GUI for diffusion models, allowing users to design complex workflows through an intuitive graph/nodes/flowchart interface, all without needing to write any code.</p></li><li><p>: The platform supports a wide variety of image models (such as SD1.x, SD2.x, and SDXL) and video models (like Stable Video Diffusion), providing flexibility for various creative projects.</p></li><li><p><strong>Asynchronous Queue System</strong>: With its efficient asynchronous queue system, ComfyUI enables smooth management and processing of tasks, allowing users to execute multiple workflows simultaneously without performance degradation.</p></li><li><p><strong>Optimizations for Performance</strong>: The software includes smart memory management to run models on GPUs with as little as 1GB VRAM, and can operate on CPUs, making it accessible for users with different hardware setups.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>To get started with , you can clone the repository and install the necessary dependencies. Hereâ€™s a quick guide to set it up:</p><div><pre><code>\ngit clone https://github.com/yourusername/ComfyUI.git\n\nComfyUI\n\n\npip  requirements.txt\n</code></pre></div><p>Once installed, you can run ComfyUI with the following command, which initializes the graphical interface:</p><p>This command will launch ComfyUI, and you can start creating your diffusion model workflows immediately!</p><p>With an impressive  on GitHub and a flurry of recent activity,  is rapidly becoming a favorite among developers! This cutting-edge tool is designed for network traffic analysis, enabling users to monitor, visualize, and understand data flows in real-time. Whether you're troubleshooting network issues or seeking to optimize performance, Sniffnet equips you with the insights you need to enhance your network management effortlessly!</p><h3>\n  \n  \n  Main Features of Sniffnet:\n</h3><ol><li><p><strong>Comprehensive Internet Traffic Monitoring</strong>: Sniffnet allows users to comfortably monitor their internet traffic in real-time, providing insights into data usage patterns and network activity with intuitive charts and statistics.</p></li><li><p><strong>Cross-Platform Compatibility</strong>: The application is designed to work seamlessly across various operating systems, ensuring accessibility for a broad audience of users, regardless of their preferred platform.</p></li><li><p><strong>Customizable Filters and Notifications</strong>: Users can apply specific filters to observed traffic, set custom notifications for defined events, and even manage favorite hosts for quick access, making monitoring tailored to individual needs.</p></li><li><p><strong>Detailed Reporting and Protocol Identification</strong>: Sniffnet can export comprehensive capture reports as PCAP files and identify over 6000 services and protocols, giving users a deep understanding of their network connections.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>To get started with , you can install it using different methods based on your operating system. Here are a couple of popular installation commands:</p><ul><li><strong>Using Homebrew (macOS and Linux)</strong>:\n</li></ul><ul><li><strong>Using Cargo (if you have Rust installed)</strong>:\n</li></ul><div><pre><code>  cargo sniffnet </code></pre></div><p>Once installed, you can launch Sniffnet to begin monitoring your internet traffic!</p><p>With an impressive  on GitHub and a surge of recent activity,  is rapidly making waves in the development community! This innovative tool is designed to streamline and automate the testing process for your code, ensuring that everything runs smoothly and efficiently. Whether you're a seasoned developer or just starting out, Checkmate equips you with the capabilities to enhance your testing workflow and deliver high-quality software with confidence!</p><h3>\n  \n  \n  Main Features of Checkmate:\n</h3><ol><li><p><strong>Open Source and Self-Hosted</strong>: Checkmate is an open-source application that users can self-host on their servers, providing full control over their monitoring environment and the ability to modify the codebase as needed.</p></li><li><p><strong>Comprehensive Monitoring Capabilities</strong>: It tracks vital metrics such as server uptime, response times, and hardware status, while also facilitating website and Docker container monitoring, making it a versatile tool for infrastructure management.</p></li><li><p><strong>Real-Time Alerts and Reporting</strong>: Users receive real-time alerts for downtime and performance issues, along with detailed reports to keep them informed about their infrastructure's health and performance.</p></li><li><p><strong>Capture Agent for Enhanced Data Retrieval</strong>: The optional Capture agent allows users to gather additional metrics like CPU and RAM usage, enhancing the functionality of Checkmate and providing deeper insights into system performance.</p></li></ol><h3>\n  \n  \n  Code Example for Installation:\n</h3><p>To install , you can use Docker for a straightforward setup. Hereâ€™s a command to deploy Checkmate using a one-click deployment option:</p><ul><li><strong>Using Coolify for Docker Deployment</strong>:\n</li></ul><p>Additionally, if you want to install the  for enhanced monitoring, you can follow the installation instructions provided in its separate repository. This ensures you have everything set up correctly for optimal performance!</p><p>We hope youâ€™re excited to explore these amazing projects and discover how they can enhance your development experience! Donâ€™t forget to star your favorite repositories to show your support and keep track of them. Be sure to follow us for future updates, as we share new trending projects every weekâ€”thereâ€™s always something fresh and innovative to check out! Happy coding!</p>","contentLength":23454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"**\"ğŸš€ Dive into Innovation: Top Trending GitHub Projects Shaping the Future of AI!\"**","url":"https://dev.to/bruh_buh_f683772f171823db/-dive-into-innovation-top-trending-github-projects-shaping-the-future-of-ai-29c7","date":1740133619,"author":"Bruh Buh","guid":8504,"unread":true,"content":"<p>Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.</p><p>With an impressive  and a surge of recent activity,  is making waves in the open-source community! This innovative AI agent framework empowers developers to seamlessly integrate and manage intelligent automation across diverse applications, enhancing productivity and enabling the creation of advanced AI solutions with ease. Join the excitement and discover how Composio is transforming the landscape of AI development!</p><h3>\n  \n  \n  Main Features of Composio:\n</h3><ol><li><p>:</p><ul><li>Composio provides a robust framework specifically designed for AI agents, ensuring reliability and efficiency in production environments.</li></ul></li><li><ul><li>Integrates with over , including popular platforms like GitHub, Gmail, and Slack, along with support for OS operations and search functionalities.</li></ul></li><li><ul><li>Facilitates secure integrations by supporting various authentication protocols such as OAuth and API Keys.</li></ul></li><li><ul><li>Allows users to customize and extend the toolset by adding their own tools and extensions, promoting flexibility.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To get started with Composio, you can easily install the package using the following command:</p><p>For additional support with OpenAI, install the OpenAI plugin:</p><div><pre><code>pip composio-openai\n</code></pre></div><h3>\n  \n  \n  Example Code Snippet: Creating an AI Agent\n</h3><p>Here's a brief example of how to initialize the OpenAI client and set up the Composio Tool Set in Python:</p><div><pre><code></code></pre></div><p>This code snippet demonstrates the essential setup needed to create a powerful AI agent using Composio.</p><p>With an impressive  and a flurry of recent activity,  is capturing the attention of the AI community! This groundbreaking framework allows developers to train a <strong>26M-parameter GPT model from scratch in just two hours</strong>, making advanced AI capabilities more accessible than ever. Join the excitement and discover how Minimind is revolutionizing the way we approach AI model training!</p><h3>\n  \n  \n  Main Features of MiniMind:\n</h3><ol><li><ul><li>Train a lightweight language model from scratch for just  and in approximately , making advanced AI development accessible to everyone.</li></ul></li><li><p><strong>Open Source Implementation</strong>:</p><ul><li>Provides a complete open-source framework with a simplified architecture for large models, covering all aspects from dataset cleaning to pre-training and fine-tuning.</li></ul></li><li><ul><li>Entirely constructed with , offering flexibility and control over the model's implementation without reliance on third-party libraries.</li></ul></li><li><ul><li>Features a multimodal vision language model, , extending its capabilities beyond text to include visual processing.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To get started with MiniMind, you can easily install it using the following command:</p><h3>\n  \n  \n  Example Code Snippet: Training the Model\n</h3><p>Hereâ€™s a brief example to illustrate how to set up and train your own MiniMind model:</p><div><pre><code></code></pre></div><p>This snippet shows you how straightforward it is to get up and running with MiniMind, allowing you to dive right into training your own language model!</p><p>With an astounding  and a surge of recent activity,  is making waves in the open-source community! This powerful tool is designed to simplify and automate the process of generating income through various online avenues, empowering users to explore profitable ventures effortlessly. Dive into MoneyPrinterTurbo and discover how it can transform your financial strategies while streamlining your path to success!</p><h3>\n  \n  \n  Main Features of MoneyPrinterTurbo:\n</h3><ol><li><p><strong>Automated Video Generation</strong>:</p><ul><li>Generate high-definition videos automatically by inputting just a topic or keyword, complete with scripts, subtitles, and background music.</li></ul></li><li><ul><li>Utilize both a user-friendly  and a robust  for seamless integration and accessibility.</li></ul></li><li><p><strong>Batch Processing Capability</strong>:</p><ul><li>Create multiple videos simultaneously and customize various aspects, including length and clip duration, for efficient production.</li></ul></li><li><p><strong>Voice Synthesis and Customization</strong>:</p><ul><li>Choose from a variety of realistic voice synthesis options, allowing for real-time previews and customizable subtitles to enhance viewer engagement.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To get started with MoneyPrinterTurbo, you can install it using Docker with the following commands:</p><div><pre><code>MoneyPrinterTurbo\n\n\ndocker-compose up\n</code></pre></div><h3>\n  \n  \n  Accessing the Web Interface\n</h3><p>Once Docker is running, you can access the web interface by opening your browser and navigating to:</p><p>This setup will allow you to start using MoneyPrinterTurbo for your automated video generation needs!</p><p>With an impressive  and a flurry of recent activity,  is rapidly becoming a go-to tool in the open-source community! Designed to streamline data processing and enhance machine learning workflows, Exo empowers developers by providing a robust framework that simplifies complex tasks and promotes efficiency. Dive into Exo and unlock the potential to supercharge your projects with cutting-edge capabilities!</p><ol><li><ul><li>Run your own AI cluster using everyday devices like smartphones, computers, and Raspberry Pi, making advanced AI accessible to everyone.</li></ul></li><li><ul><li>Easily integrate AI models into your applications with a  to the code, thanks to the ChatGPT-compatible API provided by Exo.</li></ul></li><li><p><strong>Automatic Device Discovery</strong>:</p><ul><li>Simplifies setup by automatically discovering devices on the network, allowing for seamless integration without manual configuration.</li></ul></li><li><p><strong>Flexible Partitioning Strategies</strong>:</p><ul><li>Supports various model partitioning methods, including ring memory weighted partitioning, optimizing resource utilization across all connected devices.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To install Exo from source, follow these steps:</p><div><pre><code>\ngit clone https://github.com/yourusername/exo.git\n\nexo\n\n\npip  requirements.txt\n</code></pre></div><h3>\n  \n  \n  NVIDIA GPU Support (if applicable)\n</h3><p>If you're using a Linux system with NVIDIA GPU support, ensure you have the NVIDIA driver and CUDA toolkit installed:</p><div><pre><code>\nnvidia-smi\n\n\nnvcc </code></pre></div><p>These steps will help you get started with Exo and leverage its powerful capabilities for running AI models across your devices!</p><p>With an impressive  and a surge of recent activity,  is quickly establishing itself as an essential tool in the developer community! Designed to simplify and enhance the process of building and deploying applications, Fabric provides a cohesive framework that streamlines development workflows and fosters collaboration. Dive into Fabric today and experience the power of efficient application management at your fingertips!</p><ol><li><ul><li>Fabric is an open-source solution designed to enhance human capabilities through the integration of artificial intelligence, making it accessible for users to leverage AI in everyday tasks.</li></ul></li><li><p><strong>Pattern Collection and Integration</strong>:</p><ul><li>The framework allows users to collect and integrate AI prompts known as , streamlining the usage of AI across various applications and simplifying workflow management.</li></ul></li><li><p>:</p><ul><li>Emphasizing a human-centered approach, Fabric encourages breaking down complex problems into manageable components, ensuring that AI serves to enhance human creativity rather than replace it.</li></ul></li><li><p><strong>Diverse Application Patterns</strong>:</p><ul><li>Fabric offers a variety of Patterns for practical applications, including extracting insights from multimedia, writing essays, summarizing academic papers, and generating matched AI art prompts.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To install Fabric, you can choose from multiple methods. Hereâ€™s how to install it from source:</p><div><pre><code>\ngit clone https://github.com/yourusername/fabric.git\n\nfabric\n\n\npip  requirements.txt\n</code></pre></div><h3>\n  \n  \n  Example Usage of a Pattern\n</h3><p>Once Fabric is installed, you can use a Pattern for summarizing an academic paper like this:</p><div><pre><code></code></pre></div><p>These features and examples showcase how Fabric empowers users to effectively integrate AI into their daily lives and enhance their productivity!</p><p>With a remarkable  and a wave of recent activity,  is making a significant impact in the AI development landscape! Designed to facilitate the training and deployment of large-scale AI models with ease, ColossalAI empowers developers to harness the full potential of artificial intelligence, providing a robust framework that simplifies complex processes. Dive into ColossalAI and unlock new possibilities for your AI projects today!</p><h3>\n  \n  \n  Main Features of ColossalAI:\n</h3><ol><li><p><strong>Cost Efficiency in Training</strong>:</p><ul><li>ColossalAI reduces the training costs for large AI models by  with just a , utilizing advanced <strong>FP8 mixed precision training upgrades</strong> for enhanced efficiency.</li></ul></li><li><p><strong>Instant Access to Resources</strong>:</p><ul><li>Users can immediately start using ColossalAI without setup, gaining access to high-end on-demand computing resources, making it easier than ever to dive into AI research.</li></ul></li><li><p><strong>Support for Multiple AI Models</strong>:</p><ul><li>The framework supports a variety of well-known AI models, including , , and , showcasing its versatility and adaptability for different applications.</li></ul></li><li><p><strong>SwiftInfer for Enhanced Inference</strong>:</p><ul><li>With , ColossalAI accelerates processing speeds for multi-round conversations by , improving responsiveness and performance in conversational AI applications.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To install ColossalAI, you can choose from several methods. Hereâ€™s how to install it via PyPI:</p><div><pre><code>\npip colossalai\n</code></pre></div><h3>\n  \n  \n  Example Usage: Basic Training Setup\n</h3><p>Hereâ€™s a simple code snippet demonstrating how to set up a training script with ColossalAI:</p><div><pre><code></code></pre></div><p>These features and examples highlight how ColossalAI is designed to make AI model development and deployment faster, cheaper, and more accessible for users!</p><p>With an impressive  and a flurry of recent activity,  is rapidly becoming a go-to resource for developers looking to supercharge their applications with AI! Designed to facilitate the creation and fine-tuning of sophisticated language models, MetaGPT empowers users to harness the power of AI in a seamless and efficient manner. Dive into MetaGPT and transform your projects into intelligent, responsive solutions today!</p><h3>\n  \n  \n  Main Features of MetaGPT:\n</h3><ol><li><ul><li>MetaGPT enables users to assign different roles to various GPTs, facilitating collaborative efforts among agents to tackle complex tasks efficiently.</li></ul></li><li><p><strong>Innovative Product Launch - MGX</strong>:</p><ul><li>The launch of  marks the creation of the world's first AI agent development team, showcasing groundbreaking advancements in AI agent technology.</li></ul></li><li><p><strong>Comprehensive Software Development Process</strong>:</p><ul><li>The framework provides a complete solution for software development workflows, incorporating standardized operating procedures (SOPs) to enhance team collaboration and efficiency.</li></ul></li><li><p><strong>Flexible Installation Options</strong>:</p><ul><li>Users can easily install MetaGPT using multiple methods, including , , or by cloning the GitHub repository, accommodating different user preferences.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To get started with MetaGPT, you can install it using either  or . Hereâ€™s how to do it using both methods:</p><div><pre><code>conda create  metagpt 3.9  conda activate metagpt\npip  metagpt\n</code></pre></div><div><pre><code>pip  git+https://github.com/geekan/MetaGPT.git\n</code></pre></div><div><pre><code>git clone https://github.com/geekan/MetaGPT\nMetaGPT\npip </code></pre></div><p>These features and installation steps illustrate how MetaGPT is designed to streamline collaborative AI development while offering flexibility to users in setting up their environment!</p><p>With an impressive  and a surge of recent activity,  is making waves in the developer community! Designed to provide a seamless experience for building and managing user interfaces, uv empowers developers to create stunning, interactive applications with ease. Join the growing trend and elevate your UI development game with uv today!</p><ol><li><ul><li> boasts impressive speed, claiming to be , which significantly enhances efficiency for package management.</li></ul></li><li><p><strong>Single Tool Functionality</strong>:</p><ul><li>It consolidates multiple package management tools into one, replacing , , , and others, simplifying the workflow for developers.</li></ul></li><li><p><strong>Comprehensive Project Management</strong>:</p><ul><li> provides a universal lockfile for consistent dependency management across environments, along with features for managing multiple packages within a project.</li></ul></li><li><p><strong>Flexible Installation and Script Execution</strong>:</p><ul><li>Users can easily install  via commands like  and run scripts directly with inline dependency metadata, making project setup and execution seamless.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To get started with , you can install it using either  or :</p><div><pre><code></code></pre></div><p>To initialize a new project and add a dependency, use the following commands:</p><div><pre><code>uv init example  example       \nuv add requests   </code></pre></div><p>These features and commands illustrate how  is designed to enhance package management and project handling for Python developers!</p><p>With a remarkable  and a wave of recent activity,  is quickly becoming the go-to resource for React developers! This powerful library offers a collection of reusable components and hooks, designed to simplify and enhance your React application development. Dive into the world of efficient coding with react-bits and elevate your projects to new heights!</p><h3>\n  \n  \n  Main Features of React Bits:\n</h3><ol><li><p><strong>Vast Collection of Components</strong>:</p><ul><li> offers a large library of <strong>animated React components</strong>, including text and background animations, to enhance your web projects.</li></ul></li><li><ul><li>All components are completely , making it an accessible resource for developers looking to add animations without any costs.</li></ul></li><li><ul><li>Each component provides <strong>customization through props</strong>, allowing developers to easily tailor them to fit their specific project needs.</li></ul></li><li><ul><li>Designed for , these components can be incorporated into any modern React project with minimal effort.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To get started with , you can install it via the Command-Line Interface (CLI) using :</p><h3>\n  \n  \n  Quick Component Usage Example\n</h3><p>Hereâ€™s how to use a simple animated component in your project:</p><div><pre><code></code></pre></div><p>With , enhancing your React applications with beautiful animations has never been easier!</p><p>With an impressive  and a flurry of recent activity,  is making waves in the open-source community! This innovative platform empowers developers to create, share, and collaborate on customizable user interface components, streamlining the design process and enhancing productivity. Dive into OpenHands and unlock the potential to build stunning applications with ease!</p><h3>\n  \n  \n  Main Features of OpenHands:\n</h3><ol><li><p><strong>AI-Powered Development Agents</strong>:</p><ul><li>OpenHands enables agents to perform tasks typically handled by human developers, such as modifying code, running commands, and calling APIs, thereby enhancing productivity.</li></ul></li><li><ul><li>The platform can be quickly set up using Docker, simplifying deployment and ensuring a hassle-free installation process.</li></ul></li><li><ul><li>Once running, users can easily access OpenHands via , providing a straightforward interface for interaction.</li></ul></li><li><p><strong>Customizable Model Provider</strong>:</p><ul><li>Users have the flexibility to choose their model provider and API key, with recommendations like <strong>Anthropic's Claude 3.5 Sonnet</strong>, offering versatile options for various applications.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To get started with , you can pull the Docker image and run it with the following commands:</p><div><pre><code>\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik\n\n\ndocker run docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik  /var/run/docker.sock:/var/run/docker.sock  openhands_data:/data  3000:3000  openhands-app  host.docker.internal:host-gateway \n  docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik\n</code></pre></div><p>Once the container is running, access the application at  to start harnessing the power of OpenHands!</p><p>With a remarkable  and a surge of recent activity,  is truly capturing the attention of developers everywhere! This innovative user interface framework is designed to simplify the creation of stunning applications, providing a versatile and intuitive platform that empowers developers to build with ease and creativity. Dive into ComfyUI and elevate your development experience to new heights!</p><h3>\n  \n  \n  Main Features of ComfyUI:\n</h3><ol><li><p><strong>Modular Diffusion Model Interface</strong>:</p><ul><li>ComfyUI is the most powerful and modular GUI for diffusion models, allowing users to intuitively design and execute advanced stable diffusion pipelines using a flowchart-based approach.</li></ul></li><li><ul><li>Supports a wide variety of image and video models, including , , , and others, enabling users to leverage diverse capabilities for multimedia projects.</li></ul></li><li><p><strong>Asynchronous Queue System</strong>:</p><ul><li>The platform employs an efficient asynchronous queue system that enhances performance by only re-executing parts of the workflow that have changed, optimizing processing time.</li></ul></li><li><ul><li>ComfyUI operates fully offline, ensuring user privacy and access without the need for a constant internet connection.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To install ComfyUI, you can start by pulling the Docker image and running it with the following commands:</p><div><pre><code>\ndocker pull comfyui/comfyui:latest\n\n\ndocker run  comfyui-app  7860:7860 \n  comfyui/comfyui:latest\n</code></pre></div><p>After the container is running, access the ComfyUI interface via  to begin creating your diffusion workflows!</p><p>With an impressive  and a flurry of recent activity,  is making waves in the developer community! This powerful network traffic analysis tool empowers users to monitor and analyze network packets with ease, providing valuable insights into network behavior and performance. Dive into Sniffnet and unlock the potential to optimize your network like never before!</p><h3>\n  \n  \n  Main Features of Sniffnet:\n</h3><ol><li><p><strong>Network Adapter Selection</strong>:</p><ul><li>Users can easily choose which network adapter to monitor, ensuring tailored analysis based on their specific hardware setup.</li></ul></li><li><p><strong>Real-Time Monitoring and Statistics</strong>:</p><ul><li>Sniffnet provides real-time charts and overall statistics of internet traffic, making it simple to visualize and understand network activity as it happens.</li></ul></li><li><p><strong>Custom Filters and Notifications</strong>:</p><ul><li>The application allows for the creation of custom filters to refine observed traffic, along with configurable notifications for predefined network events, enhancing the monitoring experience.</li></ul></li><li><ul><li>Users can export detailed traffic reports as , facilitating further analysis and sharing with other tools or stakeholders.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To install  on various platforms, you can use the following commands:</p><p><strong>Install via Homebrew (macOS)</strong>:</p><p>:</p><div><pre><code>cargo sniffnet </code></pre></div><p>These commands ensure a quick setup across different operating systems, enabling you to start monitoring your internet traffic with Sniffnet in no time!</p><p>With  and a surge of recent activity,  is quickly becoming a must-have tool in the developer community! This robust application is designed to streamline your testing processes, helping you effortlessly catch bugs and ensure code quality. Dive into Checkmate and elevate your development workflow to new heights with confidence!</p><h3>\n  \n  \n  Main Features of Checkmate:\n</h3><ol><li><p>:</p><ul><li>Checkmate tracks server uptime, response times, and various infrastructure metrics such as CPU, RAM, and disk usage, ensuring a holistic view of performance.</li></ul></li><li><p><strong>Real-Time Alerts and Reports</strong>:</p><ul><li>Users receive instant notifications regarding downtime and incidents, enabling them to respond swiftly to potential issues.</li></ul></li><li><p><strong>Self-Hosted and Open Source</strong>:</p><ul><li>As a self-hosted and open-source application, Checkmate allows users to maintain full control over their monitoring setup while benefiting from community-driven improvements.</li></ul></li><li><ul><li>The optional Capture agent enhances monitoring capabilities by providing detailed insights into remote server performance, including CPU usage and temperature status.</li></ul></li></ol><h3>\n  \n  \n  Code Example: Installation Steps\n</h3><p>To get started with , you can deploy it using  with one-click options or follow the installation instructions in the documentation. Hereâ€™s a quick command for one-click deployment using :</p><div><pre><code>docker run  80:80 checkmate:latest\n</code></pre></div><p>For a detailed installation process, check out the <a href=\"https://your-checkmate-docs-link.com\" rel=\"noopener noreferrer\">Checkmate documentation portal</a>. This will guide you through setting up both the frontend and the required Capture agent for optimal functionality!</p><p>As you dive into these amazing projects, don't forget to explore all the features they offer and find out how they can enhance your development journey! Be sure to star your favorite repositories to show some love and support for the creators behind them. We invite you to follow along for future updates and insights, as we share new trending projects every weekâ€”there's always something exciting on the horizon! Happy coding!</p>","contentLength":20037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ğŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-21","url":"https://dev.to/bruh_buh_f683772f171823db/13-most-exciting-github-projects-this-week-2025-02-21-228","date":1740132283,"author":"Bruh Buh","guid":8503,"unread":true,"content":"<p>Every week, thousands of developers contribute to exciting new projects on GitHub. Here's our curated list of the most innovative and impactful repositories that are shaping the future of software development.</p><p>With an impressive  on GitHub and a flurry of recent activity,  is rapidly becoming the go-to integration platform for AI agents and applications. Designed to simplify the process of connecting and automating workflows across over 250+ applications, Composio empowers developers to effortlessly create intelligent solutions that enhance productivity and streamline operations. Join the vibrant community and discover how Composio can revolutionize your development projects!</p><h3>\n  \n  \n  Key Features of Composio:\n</h3><ol><li><p><strong>Production-Ready Toolset:</strong></p><ul><li>Specifically designed for AI agents, ensuring reliability and performance in production environments.</li></ul></li><li><ul><li>Integrates with over 250+ tools, including major platforms like GitHub, Gmail, Slack, and more.</li></ul></li><li><p><strong>Optimized Search Capabilities:</strong></p><ul><li>Provides powerful search functionalities through popular engines, enhancing data retrieval and interaction.</li></ul></li><li><ul><li>Fully supports popular AI frameworks such as OpenAI, Langchain, and Gemini, making it versatile for various applications.</li></ul></li></ol><p>\nTo get started with Composio, you can install the core package using the following command:</p><p>For those who want to integrate with OpenAI, use:</p><div><pre><code>pip composio-openai\n</code></pre></div><p><strong>Creating an AI Agent Example:</strong>\nHereâ€™s a brief code snippet demonstrating how to initialize the Composio toolset and create an AI agent:</p><div><pre><code></code></pre></div><p>This example illustrates how simple it is to set up and utilize Composio within your projects!</p><p>With an impressive  on GitHub and a surge of recent activity,  is capturing the attention of developers everywhere! This innovative tool is designed to simplify and enhance the process of building and managing mind maps, empowering users to visualize their ideas and projects with clarity and creativity. Dive into Minimind and discover how it can transform your brainstorming sessions into structured, actionable plans!</p><h3>\n  \n  \n  Key Features of MiniMind:\n</h3><ol><li><ul><li>Users can train a compact language model from scratch for approximately  in GPU rental costs and within just  of training time, making it highly accessible.</li></ul></li><li><ul><li>The MiniMind model is extremely compact, with the smallest version being only , allowing for easy training on standard personal GPUs without the need for extensive resources.</li></ul></li><li><p><strong>Open Source and Educational Resource:</strong></p><ul><li>MiniMind offers an open-source implementation of large language model structures, including tools for dataset cleaning, pretraining, and fine-tuning, serving as a valuable tutorial for newcomers to LLM development.</li></ul></li><li><p><strong>Native PyTorch Implementation:</strong></p><ul><li>The entire codebase is built from scratch in , promoting transparency and a deeper understanding of the algorithms involved in training language models.</li></ul></li></ol><p>\nTo get started with MiniMind, clone the repository and install the necessary dependencies. Hereâ€™s how to do it:</p><div><pre><code>\ngit clone https://github.com/yourusername/minimind.git\nminimind\n\n\npip  requirements.txt\n</code></pre></div><p>\nHereâ€™s a brief code snippet demonstrating how to initiate the training of the MiniMind model:</p><div><pre><code></code></pre></div><p>This example illustrates the simplicity of setting up and training your own language model using MiniMind!</p><p>With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to tool for developers looking to optimize their financial applications! This innovative project aims to streamline and enhance the process of generating financial reports and analytics, providing users with powerful features to simplify complex tasks. Get ready to supercharge your financial workflows with MoneyPrinterTurbo and unlock new levels of efficiency and insight!</p><h3>\n  \n  \n  Key Features of MoneyPrinterTurbo:\n</h3><ol><li><p><strong>Automated Video Generation:</strong></p><ul><li>Easily create high-definition videos by simply providing a theme or keywords, with the tool automatically generating scripts, materials, subtitles, and background music.</li></ul></li><li><ul><li>Offers both a user-friendly  and an , making it versatile for different user needs and technical skills.</li></ul></li><li><ul><li>Generate multiple videos simultaneously and customize video segments, giving users complete control over their video projects.</li></ul></li><li><p><strong>Voice Synthesis and Subtitles:</strong></p><ul><li>Supports advanced voice synthesis options with real-time previews, along with customizable subtitles for enhanced accessibility and engagement.</li></ul></li></ol><p>\nTo get started with MoneyPrinterTurbo, follow these steps to set up your environment:</p><div><pre><code>\ngit clone https://github.com/harry0703/MoneyPrinterTurbo.git\nMoneyPrinterTurbo\n\n\nconda create  MoneyPrinterTurbo 3.11\nconda activate MoneyPrinterTurbo\n\n\npip  requirements.txt\n</code></pre></div><p>\nOnce the setup is complete, you can start the Docker containers:</p><div><pre><code>docker-compose up\n\ndocker compose up\n</code></pre></div><p>After the Docker containers are running, access the web interface by navigating to:</p><p>This will get you started on creating amazing videos with MoneyPrinterTurbo!</p><p>With an impressive  on GitHub and a surge of recent activity,  is making waves in the developer community! This powerful tool is designed to streamline the process of building and deploying microservices, providing developers with the flexibility and efficiency needed to create scalable applications. Dive into Exo and elevate your development game with its innovative features and user-friendly interface!</p><ol><li><ul><li>Run your own AI cluster using everyday devices, making advanced AI capabilities accessible to everyone without the need for specialized hardware.</li></ul></li><li><p><strong>Automatic Device Discovery:</strong></p><ul><li> simplifies the setup process by automatically discovering devices on your network, requiring zero manual configurationâ€”just plug in and go!</li></ul></li><li><ul><li>Integrate the  with a single line of code, allowing you to run AI models on your own hardware effortlessly.</li></ul></li><li><p><strong>Dynamic Model Partitioning:</strong></p><ul><li>The platform optimally distributes model workloads across devices with advanced partitioning strategies, enabling the efficient use of available resources.</li></ul></li></ol><p>\nTo set up Exo on your system, start by ensuring you have Python 3.12.0 or higher. Then, follow these commands to install from the source:</p><div><pre><code>\ngit clone https://github.com/yourusername/exo.git\nexo\n\n\npip  requirements.txt\n</code></pre></div><p>\nFor users with NVIDIA GPUs, verify your driver installation:</p><p>These steps will get you started on harnessing the power of Exo for your AI projects!</p><p>With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to solution for developers! This powerful framework simplifies deployment and management of applications across various environments, ensuring a seamless integration process. Dive into Fabric and streamline your development workflow with its innovative features designed to enhance productivity and collaboration!</p><ol><li><p><strong>Modular Problem-Solving Approach:</strong></p><ul><li>Fabric encourages users to break down challenges into manageable components, allowing for systematic application of AI solutions tailored to each specific issue.</li></ul></li><li><p><strong>Patterns for Prompt Management:</strong></p><ul><li>The framework focuses on collecting and integrating AI prompts as , making it easier for users to access, manage, and utilize valuable prompts for various tasks in their daily lives.</li></ul></li><li><ul><li>Fabric offers a diverse range of Patterns designed for different activities, such as summarizing academic papers, generating AI art prompts, and extracting key insights from multimedia content, enhancing productivity across multiple domains.</li></ul></li><li><p><strong>Philosophy of Enhancing Human Creativity:</strong></p><ul><li>The framework is built on the belief that AI should act as a magnifier of human creativity, helping users leverage technology to solve real-world challenges and improve their daily workflows.</li></ul></li></ol><p>\nTo get started with Fabric, you can install it from source by following these commands:</p><div><pre><code>\ngit clone https://github.com/yourusername/fabric.git\nfabric\n\n\npip  requirements.txt\n\ndevelopment\n</code></pre></div><p>This quick setup will prepare you to harness the power of Fabric in your AI projects!</p><p>With an impressive  on GitHub and a surge of recent activity,  is making waves in the AI community! This powerful framework is designed to streamline the development and deployment of large-scale AI models, empowering developers to tackle complex machine learning tasks with ease. Dive into ColossalAI and elevate your AI projects to new heights with its innovative tools and features!</p><h3>\n  \n  \n  Key Features of Colossal-AI:\n</h3><ol><li><ul><li>Colossal-AI allows users to reduce the training costs of large AI models by up to  with just a , thanks to its <strong>FP8 mixed precision training</strong> capabilities.</li></ul></li><li><ul><li>The framework enables users to generate  with just one click using its  model, making video creation simple and accessible.</li></ul></li><li><ul><li>With the introduction of , the framework has successfully doubled the inference speed for large AI models, significantly enhancing performance and efficiency for applications.</li></ul></li><li><ul><li>Colossal-AI provides tailored solutions for various models, including , ensuring optimized performance for inference, fine-tuning, and pretraining.</li></ul></li></ol><p>\nTo get started with Colossal-AI, you can install it via PyPI using the following command:</p><div><pre><code>\npip colossalai\n</code></pre></div><p>Alternatively, you can install it from the source:</p><div><pre><code>\ngit clone https://github.com/hpcaitech/ColossalAI.git\nColossalAI\n\n\npip  requirements.txt\n</code></pre></div><p>This quick setup will get you up and running with Colossal-AI, ready to tackle large-scale AI projects!</p><p>With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to resource for developers looking to harness the power of AI! This innovative framework is designed to simplify the creation of customized AI applications, enabling users to fine-tune models for their specific needs effortlessly. Dive into MetaGPT and unlock a world of possibilities in AI development!</p><ol><li><p><strong>Multi-Agent Collaboration:</strong></p><ul><li>MetaGPT enables users to assign different roles to AI agents, allowing for effective collaboration to tackle complex programming tasks within a structured framework.</li></ul></li><li><p><strong>Comprehensive Software Development Framework:</strong></p><ul><li>The platform integrates Standard Operating Procedures (SOPs) to manage the entire software development process, ensuring systematic project execution across various roles, including product managers and engineers.</li></ul></li><li><p><strong>Versatile Output Generation:</strong></p><ul><li>Users can input a one-line requirement and generate a variety of outputs such as user stories, competitive analysis, APIs, and documentation, streamlining the development workflow.</li></ul></li><li><p><strong>Research and Community Engagement:</strong></p><ul><li>MetaGPT has gained recognition in the research community, with its innovative papers accepted at conferences, and it emphasizes open-source collaboration, allowing users to contribute to its development.</li></ul></li></ol><p>\nTo get started with MetaGPT, follow these installation steps:</p><ol><li><p>Ensure you have  (but less than ) installed on your system.</p></li><li><p>Create a new conda environment:</p></li></ol><div><pre><code>   conda create  metagpt 3.9  conda activate metagpt\n</code></pre></div><div><pre><code>   pip  metagpt\n</code></pre></div><p>Alternatively, clone the repository and install:</p><div><pre><code>git clone https://github.com/geekan/MetaGPT MetaGPT  pip </code></pre></div><p>This will set you up to leverage MetaGPT's powerful multi-agent capabilities for your software projects!</p><p>With an impressive  on GitHub and a buzz of recent activity,  is rapidly gaining traction as a must-have tool for developers! This powerful framework is designed to simplify the development of ultra-fast web applications, providing an efficient platform to build and deploy innovative solutions effortlessly. Dive into uv and elevate your web development game to new heights!</p><ol><li><p><strong>Extremely Fast Package Management:</strong></p><ul><li> boasts package management speeds that are , making it a top choice for developers looking for efficiency in managing their Python projects.</li></ul></li><li><p><strong>Unified Tool Replacement:</strong></p><ul><li>This tool consolidates multiple package managers and toolsâ€”like , , and â€”into a single solution, simplifying the development workflow and reducing tool clutter.</li></ul></li><li><p><strong>Single-File Script Management:</strong></p><ul><li>uv supports managing dependencies for single-file scripts, allowing developers to declare dependencies inline and execute scripts seamlessly in isolated environments.</li></ul></li><li><p><strong>Comprehensive Project Management:</strong></p><ul><li>It features a universal lockfile for consistent dependency management and supports project workspaces, enhancing organization and stability across environments.</li></ul></li></ol><p>\nTo install , you can use the following command for :</p><div><pre><code></code></pre></div><p>Or for , install via  with:</p><p>To initialize a new project named , use:</p><p>To add a dependency like :</p><p>With these commands, you'll be set to leverage uv for efficient package and project management!</p><p>With an impressive  on GitHub and vibrant recent activity,  is making waves in the React community! This essential library is designed to provide a collection of reusable components and utilities, enabling developers to build stunning applications with ease and efficiency. Dive into react-bits and unlock the potential for rapid development and elegant design in your next React project!</p><h3>\n  \n  \n  Key Features of React Bits:\n</h3><ol><li><p><strong>Extensive Library of Components:</strong></p><ul><li> offers a robust collection of <strong>60 animated React components</strong> designed to enhance web projects, with a focus on continuous growth and variety.</li></ul></li><li><ul><li>Each component comes with , allowing developers to easily modify styles and behaviors to suit their specific needs.</li></ul></li><li><ul><li>The components are crafted for  into any modern React project, ensuring versatility across different frameworks and setups.</li></ul></li><li><ul><li>Components are available in  (JS + CSS, JS + Tailwind CSS, TS + CSS, TS + Tailwind CSS), catering to different developer preferences and project requirements.</li></ul></li></ol><p>\nTo install React Bits via the command line interface, you can use  with the following command:</p><p>After installation, you can import a component into your project like this:</p><div><pre><code></code></pre></div><p>With these features and easy installation, React Bits makes it a breeze to enhance your web applications!</p><p>With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a favorite in the developer community! This innovative platform empowers users to create and share rich, interactive applications seamlessly, making it easier than ever to bring ideas to life. Dive into OpenHands and discover the endless possibilities for enhancing your projects with powerful tools designed to foster collaboration and creativity!</p><h3>\n  \n  \n  Key Features of OpenHands:\n</h3><ol><li><p><strong>AI-Powered Development Agents:</strong></p><ul><li>OpenHands enables <strong>AI-driven software development agents</strong> that can perform tasks like modifying code, running commands, and calling APIs, significantly streamlining the development workflow.</li></ul></li><li><ul><li>Users can effortlessly deploy OpenHands using Docker, simplifying the runtime environment setup with just a few commands:\n</li></ul></li></ol><div><pre><code>   docker pull docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik\n</code></pre></div><ol><li><p><strong>Flexible Interaction Modes:</strong></p><ul><li>OpenHands supports various operating modes, including a , , and integration with , catering to different developer preferences and workflows.</li></ul></li><li><p><strong>Comprehensive Documentation:</strong></p><ul><li>Detailed  is available, guiding users through setup, configuration, and troubleshooting, ensuring a smooth user experience.</li></ul></li></ol><p><strong>Running the OpenHands Application:</strong>\nOnce you have pulled the Docker image, run the application with the following command:</p><div><pre><code>docker run always docker.all-hands.dev/all-hands-ai/runtime:0.25-nikolaik  /var/run/docker.sock:/var/run/docker.sock  ~/.openhands-state:/.openhands-state  3000:3000  host.docker.internal:host-gateway  openhands-app \n    docker.all-hands.dev/all-hands-ai/openhands:0.25\n</code></pre></div><p>After running the command, access the application at  and start leveraging the power of AI in your development projects!</p><p>With a remarkable  on GitHub and a surge of recent activity,  is capturing the attention of developers everywhere! This innovative tool simplifies the creation of user-friendly interfaces for AI models, enabling users to design, customize, and deploy stunning applications with ease. Dive into ComfyUI and experience the seamless integration of functionality and creativity as you bring your AI projects to life!</p><ol><li><p><strong>Powerful Modular Interface:</strong></p><ul><li>ComfyUI offers a highly powerful and modular  that allows users to design and execute advanced stable diffusion pipelines with ease.</li></ul></li><li><p><strong>Visual Workflow Creation:</strong></p><ul><li>Users can create complex workflows using a  interface, enabling intuitive management of tasks without needing extensive coding knowledge.</li></ul></li><li><ul><li>The platform supports a wide range of image and video models, including <strong>SD1.x, SD2.x, Stable Video Diffusion</strong>, and more, providing flexibility for diverse applications in content creation.</li></ul></li><li><p><strong>Asynchronous Queue System:</strong></p><ul><li>ComfyUI features an <strong>asynchronous queue system</strong> for efficient task management, ensuring multiple operations can run simultaneously without blocking.</li></ul></li></ol><p>\nTo get started with ComfyUI, you can clone the repository and install the required dependencies:</p><div><pre><code>git clone https://github.com/ComfyUI/ComfyUI.git\nComfyUI\npip  requirements.txt\n</code></pre></div><p>\nAfter installation, run the application with the following command:</p><p>With an impressive  on GitHub and a surge of recent activity,  is making waves in the network monitoring space! This powerful tool enables users to effortlessly analyze their network traffic in real-time, providing valuable insights into data flow and potential security threats. Dive into Sniffnet and experience how it transforms complex network analysis into an intuitive and engaging process!</p><h3>\n  \n  \n  Key Features of Sniffnet:\n</h3><ol><li><p><strong>Network Traffic Monitoring:</strong></p><ul><li>Sniffnet allows users to <strong>comfortably monitor their Internet traffic</strong>, providing insights into network activity with real-time visualization.</li></ul></li><li><p><strong>Cross-Platform Compatibility:</strong></p><ul><li>The application is , ensuring it works seamlessly across various operating systems and enhancing accessibility for all users.</li></ul></li><li><p><strong>Advanced Traffic Filtering:</strong></p><ul><li>Users can apply  to observed traffic, enabling focused analysis of specific data types and improving monitoring efficiency.</li></ul></li><li><p><strong>Detailed Statistics and Reporting:</strong></p><ul><li>Sniffnet provides  on Internet traffic, and users can <strong>export comprehensive capture reports</strong> in PCAP format for further analysis.</li></ul></li></ol><p><strong>Installation using Homebrew (macOS):</strong>\nTo install Sniffnet on macOS, simply use the Homebrew package manager:</p><p><strong>Installation using Cargo (for Rust users):</strong>\nIf you have Rust installed, you can build and install Sniffnet via:</p><div><pre><code>cargo sniffnet </code></pre></div><p>\nAfter installation, launch the application with the following command in your terminal:</p><p>Start monitoring your Internet traffic and gain valuable insights into your network activity!</p><p>With an impressive  on GitHub and a flurry of recent activity,  is quickly becoming a go-to tool for developers and testers alike! This powerful application is designed to streamline the process of validating software functionality through robust automated testing, ensuring that your projects run smoothly and efficiently. Dive into Checkmate and discover how it can enhance your development workflow while boosting your confidence in software quality!</p><h3>\n  \n  \n  Key Features of Checkmate:\n</h3><ol><li><p><strong>Comprehensive Monitoring:</strong></p><ul><li>Checkmate excels in <strong>monitoring server hardware</strong>, uptime, response times, and incidents in real time, providing a complete overview of your infrastructure health.</li></ul></li><li><p><strong>Beautiful Visualizations:</strong></p><ul><li>The application offers  of monitored data, making it easy to interpret complex metrics and gain insights at a glance.</li></ul></li><li><ul><li>Users receive <strong>real-time alerts and reports</strong> regarding system availability and performance, enabling proactive management of their infrastructure.</li></ul></li><li><p><strong>Lightweight and Efficient:</strong></p><ul><li>Checkmate is optimized for performance, boasting a  which allows it to monitor over 300 servers without significant resource consumption.</li></ul></li></ol><p><strong>Installation Instructions:</strong>\nTo get started with Checkmate, you can easily deploy it using Docker. Hereâ€™s a one-click deployment option with Coolify:</p><p><strong>Basic Configuration with Capture Agent:</strong>\nAfter installing Checkmate, set up the Capture agent to monitor server infrastructure effectively:</p><div><pre><code>git clone https://github.com/yourusername/capture.git\ncapture\nnpm npm start\n</code></pre></div><p>This will ensure you have detailed insights into your server performance, including CPU and RAM usage!</p><p>In conclusion, we encourage you to dive into these amazing projects and explore all the fantastic tools they have to offer! Donâ€™t forget to star your favorite repositories to show your support and help others discover them too. Be sure to follow along for future updates, as we share new trending projects every week that are bound to inspire your next big idea. Happy coding, and we can't wait to see what you'll create!</p>","contentLength":20019,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FiveCrop in PyTorch","url":"https://dev.to/hyperkai/fivecrop-in-pytorch-19bi","date":1740132219,"author":"Super Kai (Kazuya Ito)","guid":8502,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.FiveCrop.html\" rel=\"noopener noreferrer\">FiveCrop()</a> can crop an image into 5 parts(Top-left, Top-right, Bottom-left, Bottom-right and Center) as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type: or () or <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.size.html\" rel=\"noopener noreferrer\">size()</a>):\n*Memos:\n\n<ul><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value( or ()) means .</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pad in PyTorch","url":"https://dev.to/hyperkai/pad-in-pytorch-1ek8","date":1740131655,"author":"Super Kai (Kazuya Ito)","guid":8477,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/0.20/generated/torchvision.transforms.v2.Pad.html\" rel=\"noopener noreferrer\">Pad()</a> can add padding to an image as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type: or /()). *A tuple/list must be the 1D with 1, 2 or 4 elements.</li><li>The 2nd argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can change the background of an image. *The background can be seen when adding padding for an image.</li><li>A tuple/list must be the 1D with 1 or 3 elements.</li></ul></li><li>The 3rd argument for initialization is (Optional-Default:-Type:). *, ,  or  can be set to it.</li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python Data Parsing Guide: 10 Advanced Techniques for Structured Data (2024)","url":"https://dev.to/aaravjoshi/python-data-parsing-guide-10-advanced-techniques-for-structured-data-2024-lgo","date":1740129137,"author":"Aarav Joshi","guid":8476,"unread":true,"content":"<blockquote><p>As a best-selling author, I invite you to explore my books on <a href=\"https://www.amazon.com/stores/Aarav-Joshi/author/B0DQYNVXZ7?ref=ap_rdr&amp;isDramIntegrated=true&amp;shoppingPortalEnabled=true&amp;ccs_id=738636bd-0ca1-4d7b-8efa-481bfc222571\" rel=\"noopener noreferrer\">Amazon</a>. Don't forget to follow me on <a href=\"https://medium.com/@aarav-joshi\" rel=\"noopener noreferrer\">Medium</a> and show your support. Thank you! Your support means the world! </p></blockquote><p>Python Parsing: Advanced Techniques for Structured Data</p><p>Parsing structured data efficiently remains crucial in modern software development. Python offers robust tools and libraries for handling various data formats. Let's explore practical techniques that enhance data processing workflows.</p><p>XML Processing with Element Tree</p><p>The ElementTree API provides memory-efficient XML parsing. It reads XML documents as tree structures, enabling straightforward navigation and modification.</p><div><pre><code></code></pre></div><p>JSON Processing with ijson</p><p>Large JSON files require iterative processing to manage memory efficiently. The ijson library enables streaming JSON parsing, processing one object at a time.</p><div><pre><code></code></pre></div><p>Polars provides exceptional performance for large-scale CSV processing, offering multithreaded operations and efficient memory usage.</p><div><pre><code> \\\n     \\\n     \\\n     \\\n    </code></pre></div><p>Binary Data with Protocol Buffers</p><p>Protocol Buffers offer efficient serialization and parsing of binary data with strong typing and backward compatibility.</p><div><pre><code></code></pre></div><p>Regular Expression Parsing</p><p>Regular expressions provide powerful pattern matching capabilities for custom data format parsing.</p><div><pre><code></code></pre></div><p>Parsing Expression Grammars with Lark</p><p>Lark enables the creation of complex parsers for domain-specific languages and custom formats.</p><div><pre><code></code></pre></div><p>Optimizing Parser Performance</p><p>Parsing performance depends on various factors. Consider these optimization strategies:</p><p>Memory Management: Use generators and iterative processing for large datasets.</p><div><pre><code></code></pre></div><p>Parallel Processing: Implement multiprocessing for CPU-intensive parsing tasks.</p><div><pre><code></code></pre></div><p>Caching: Implement caching for frequently parsed patterns or expressions.</p><div><pre><code></code></pre></div><p>Error Handling and Validation</p><p>Robust parsing requires comprehensive error handling and validation.</p><div><pre><code></code></pre></div><p>Data Pipeline Integration</p><p>Integrate parsing components into data pipelines for automated processing.</p><div><pre><code></code></pre></div><p>This comprehensive approach to parsing encompasses various data formats and scenarios, providing practical solutions for common data processing challenges. The techniques presented focus on efficiency, scalability, and maintainability, essential aspects of modern data processing applications.</p><p> is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly lowâ€”some books are priced as low as â€”making quality knowledge accessible to everyone.</p><p>Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !</p><p>Be sure to check out our creations:</p>","contentLength":2634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The 5 best programming languages of 2025","url":"https://dev.to/scrapestorm/the-5-best-programming-languages-of-2025-1dmb","date":1740128910,"author":"ScrapeStorm","guid":8475,"unread":true,"content":"<p>Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.</p><p>You might ask, â€œIs Java obsolete?â€ Of course not.</p><p>Why is Java still popular? Java is one of the oldest and most robust programming languages. It is also an object-oriented language mainly used for Android application development. This is one of the main reasons it is still used today. However, with the advent of programming languages â€‹â€‹like Kotlin (also suitable for Android development), Java is becoming less popular.</p><p>However, Java remains one of the most expensive programming languages â€‹â€‹and is in great demand. According to Indeed, software developers are interested in Java developers and pay more than $ 100,000 a year.</p><p>Swift iOS application development is currently very popular. Swift is a very stable programming language and worth studying. Itâ€™s easier to learn than Java. YouTube has a lot of resources to help you learn, and programming with them is fun. If youâ€™re a freelance developer in Swift now, or if you work for a job related to it, your annual salary can reach $ 115,000.</p><p>SQL or Sequel is a structured search language. Some people think itâ€™s not really a programming language. SQL is mainly used for data management and interactive. So SQL is an essential skill in programming, and any type of web development (backend or full stack) needs to be learned to manage the data. According to statistics, the average annual income of SQL developers is over $ 90,000.</p><p>This is a mysterious programming language, and some people think itâ€™s the best. JavaScript is a very popular language. If you check GitHub, youâ€™ll always see new frameworks that support JavaScript. In addition, all browsers support JavaScript. Therefore, learning JavaScript is one of the skills that software development must know. JavaScript developers can earn income in the range of $ 90,000 to $ 113,000.</p><p>According to Google Trends and the PyPI Popularity Index, Python is one of the most popular programming languages â€‹â€‹in the world and certainly one of the most expensive programming languages. Google was built in Python, and YouTube was also developed in Python.</p><p>The amazing thing about Python is that itâ€™s a general-purpose programming language used to build a wide range of applications. Furthermore, it is active in artificial intelligence. Self-driving cars, Wal-Mart auto-payment, and many automation and machine learning (ML) apps were developed through Python. This makes this language more important and rapidly popularizes. In addition, Python is easier to learn than all other languages â€‹â€‹and is easy for beginners. You can also build complex applications relatively easily and quickly. In the United States, the average salary for Python developers is about $ 78,000, while experienced developers can be as high as $ 122,000.</p>","contentLength":2932,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-Time Audio Processing in Python: A Complete Guide with Code Examples [2024]","url":"https://dev.to/aaravjoshi/real-time-audio-processing-in-python-a-complete-guide-with-code-examples-2024-25cf","date":1740128499,"author":"Aarav Joshi","guid":8458,"unread":true,"content":"<blockquote><p>As a best-selling author, I invite you to explore my books on <a href=\"https://www.amazon.com/stores/Aarav-Joshi/author/B0DQYNVXZ7?ref=ap_rdr&amp;isDramIntegrated=true&amp;shoppingPortalEnabled=true&amp;ccs_id=738636bd-0ca1-4d7b-8efa-481bfc222571\" rel=\"noopener noreferrer\">Amazon</a>. Don't forget to follow me on <a href=\"https://medium.com/@aarav-joshi\" rel=\"noopener noreferrer\">Medium</a> and show your support. Thank you! Your support means the world! </p></blockquote><p>Audio Processing in Python: Real-Time Techniques and Applications</p><p>Python offers powerful capabilities for real-time audio processing through various specialized libraries. I've worked extensively with these tools and will share practical insights into implementing efficient audio processing solutions.</p><p>Audio Input/Output with PyAudio</p><p>PyAudio provides the foundation for real-time audio processing in Python. It interfaces directly with sound cards and audio devices, enabling low-level control over audio streams.</p><div><pre><code></code></pre></div><p>Advanced Audio Analysis with Librosa</p><p>Librosa excels in audio feature extraction and music processing. I frequently use it for spectral analysis and music information retrieval tasks.</p><div><pre><code></code></pre></div><p>Digital Signal Processing with PyDSP</p><p>PyDSP enables implementation of complex DSP algorithms. Here's an example of real-time filtering:</p><div><pre><code></code></pre></div><p>Efficient File Operations with SoundFile</p><p>SoundFile provides fast and reliable audio file handling:</p><div><pre><code></code></pre></div><p>Professional Audio I/O with SoundDevice</p><p>SoundDevice offers professional-grade audio handling with ASIO support:</p><div><pre><code></code></pre></div><p>Music Analysis with Aubio</p><p>Aubio provides sophisticated music analysis capabilities:</p><div><pre><code></code></pre></div><p>Real-Time Audio Visualization</p><p>Implementing real-time audio visualization enhances the monitoring of audio processing:</p><div><pre><code></code></pre></div><p>To achieve optimal performance in real-time audio processing:</p><div><pre><code></code></pre></div><p>Managing latency is crucial for real-time applications:</p><div><pre><code></code></pre></div><p>These techniques form a comprehensive toolkit for real-time audio processing in Python. The combination of these libraries and methods enables the development of sophisticated audio applications, from music analysis to real-time effects processing.</p><p>The key to successful implementation lies in understanding the balance between processing complexity and real-time performance requirements. Through careful optimization and appropriate use of these tools, we can create efficient and effective audio processing solutions.</p><p>I've found that maintaining clean audio streams, implementing proper buffer management, and using appropriate threading techniques are essential for professional-grade audio applications. The examples provided serve as building blocks for more complex audio processing systems.</p><p> is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly lowâ€”some books are priced as low as â€”making quality knowledge accessible to everyone.</p><p>Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !</p><p>Be sure to check out our creations:</p>","contentLength":2712,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Python Developers Should Dive Into Blockchain Now","url":"https://dev.to/crosschainer/why-python-developers-should-dive-into-blockchain-now-14cg","date":1740127292,"author":"crosschainer","guid":8457,"unread":true,"content":"<p>If youâ€™re a Python developer, thereâ€™s never been a better time to dive into the world of blockchain. Until recently, blockchain development has largely been reserved for developers willing to learn specialized languages like Solidity or Rust. But now, platforms like  are breaking down those barriersâ€”letting you build smart contracts and decentralized applications (dApps) directly in Python (no code transpiling).</p><p>The question isnâ€™t  Python developers should explore blockchain, but rather <em>why havenâ€™t you started yet?</em> This post will show you why Python developers are uniquely positioned to thrive in the blockchain space and how Xian makes it easier than ever.</p><h2>\n  \n  \n  1. Pythonâ€™s Massive Ecosystem Meets Decentralization\n</h2><p>With over <strong>13 million developers worldwide</strong>, Python is the most accessible and versatile programming language today. From AI and machine learning to web development and data analysis, Python is everywhere.</p><p>Blockchain doesnâ€™t just add another tool to your toolkitâ€”it multiplies the possibilities for every project you work on.</p><h2>\n  \n  \n  2. Breaking Down Barriers: No New Languages, Just Python\n</h2><p>Traditionally, blockchain development meant learning new languages like:</p><ul><li>Solidity for Ethereum smart contracts.</li><li>Rust for Solana development.</li></ul><p>But with the , you can:</p><ul><li>Write smart contracts .</li><li>Avoid complicated cross-language conversions.</li><li>Deploy dApps using the tools and syntax youâ€™re already comfortable with.</li></ul><blockquote><p>ğŸ¯ <em>â€œWhy learn a new language when you can use the one you already know?â€</em></p></blockquote><p>By removing the need to learn specialized languages, Xian lets you focus on building, not learning new syntax.</p><h2>\n  \n  \n  3. A Seamless Developer Experience\n</h2><p>Xian doesnâ€™t just let you write Python smart contractsâ€”it offers a full development environment tailored to your workflow:</p><ul><li>ğŸ”—  Easy-to-use APIs for seamless interaction with the blockchain.</li><li>ğŸ“Š <strong>Comprehensive Documentation:</strong> Clear and developer-friendly resources to help you get started quickly.</li></ul><p>With Python and Xian, smart contract development feels like any other Python projectâ€”no unnecessary friction.</p><h2>\n  \n  \n  4. Real-World Use Cases for Python on Blockchain\n</h2><p>Here are some exciting ways Python developers can leverage blockchain right now:</p><h3>\n  \n  \n  ğŸ” <strong>Decentralized Identity Verification</strong></h3><ul><li>Build systems where users control their personal data.</li></ul><h3>\n  \n  \n  ğŸ’» <strong>Decentralized Finance (DeFi) Tools</strong></h3><ul><li>Create lending platforms, decentralized exchanges, or stablecoins.</li></ul><h3>\n  \n  \n  ğŸ”— <strong>Automated Royalties for Creators</strong></h3><ul><li>Imagine an NFT marketplace where musicians and artists automatically receive royalties every time their content is resoldâ€”no intermediaries needed, all coded in Python.</li></ul><h3>\n  \n  \n  ğŸ’¸ <strong>Decentralized Crowdfunding Platforms</strong></h3><ul><li>Build a transparent crowdfunding platform where funds are only released when predefined conditions (smart contract milestones) are met. Perfect for open-source funding or startup incubation.</li></ul><ul><li>Develop play-to-earn games where players can own, trade, and earn real value from in-game assetsâ€”no middlemen, no asset loss when a game shuts down.</li></ul><h3>\n  \n  \n  ğŸ”’ <strong>Secure Supply Chain Tracking</strong></h3><ul><li>Write Python contracts that track goods from origin to delivery. Each step is verified on the blockchain, preventing fraud and ensuring transparency.</li></ul><h3>\n  \n  \n  ğŸ“Š <strong>Predictive Market Platforms</strong></h3><ul><li>Create decentralized platforms where users can stake tokens to predict real-world events.</li></ul><h3>\n  \n  \n  âš–ï¸ <strong>Decentralized Autonomous Organizations (DAOs)</strong></h3><ul><li>Let communities govern themselves by building DAO voting systemsâ€”users can vote on proposals directly through the blockchain using Xianâ€™s smart contract features.</li></ul><h2>\n  \n  \n  5. Xian Empowers Developers to Earn\n</h2><p>Beyond the technical advantages, Xian offers a financial incentive for developers:</p><ul><li><strong>Earn 70% of all fees generated by your smart contracts.</strong></li></ul><p>This makes Xian not just a development platformâ€”but an ecosystem where developers can earn by building impactful projects.</p><h2>\n  \n  \n  6. How to Get Started with Xian Blockchain\n</h2><p>Ready to dive in? Hereâ€™s how you can start your blockchain journey today:</p><ol><li> Install the <a href=\"https://chromewebstore.google.com/detail/xian-wallet/kcimjjhplbcgkcnanijkolfillgfanlc/\" rel=\"noopener noreferrer\">Web3 Wallet</a> and open up the IDE.</li><li><strong>Write your first smart contract:</strong> Use familiar Python syntax.</li><li> Deploy your contract and monitor transactions.</li><li> Connect with other developers on <a href=\"https://t.me/xian_network\" rel=\"noopener noreferrer\">Telegram</a> and contribute to open-source projects.</li></ol><p>Hereâ€™s a simple example of a smart contract in Python:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Conclusion: The Future of Blockchain Is Written in Python\n</h2><p>The blockchain space is evolvingâ€”and Python developers are at the forefront of this revolution. With Xian Blockchain, you can:</p><ul><li>Write smart contracts using the language you already know.</li><li>Build powerful decentralized applications.</li><li>Earn revenue from your contributions to the network.</li></ul><p>So why wait? Start building, innovating, and shaping the future of blockchain with Python today.</p><p>ğŸš€ Ready to dive in? Check out the <a href=\"https://docs.xian.org\" rel=\"noopener noreferrer\">Xian Documentation</a> and start coding your first smart contract now!</p>","contentLength":4852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implementing Layer and MLP Classes in micrograd (As Explained By Karpathy)","url":"https://dev.to/shrsv/karpathy-10-45ih","date":1740125269,"author":"Shrijith Venkatramana","guid":8456,"unread":true,"content":"<p><em>Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, Iâ€™m building <a href=\"https://hexmos.com/liveapi\" rel=\"noopener noreferrer\">LiveAPI</a>, a tool that makes generating API docs from your code ridiculously easy.</em></p><h2>\n  \n  \n  Replicating The micrograd Program in PyTorch\n</h2><p>In PyTorch we can replicate our micrograd expression using the following code. Here instead of the  class we use the  class.</p><div><pre><code></code></pre></div><p>The output is -- which agrees with the output from the previous post:</p><div><pre><code>0.7071066904050358\n----\nx2 0.5000001283844369\nw2 0.0\nx1 -1.5000003851533106\nw1 1.0000002567688737\n</code></pre></div><p>In a typical real-world project, instead of scalars, we'd use larger tensors.</p><p>For instance, we can define a 2x3 tensor as follows:</p><div><pre><code></code></pre></div><div><pre><code>tensor([[1., 2., 3.],\n        [4., 5., 6.]])\n</code></pre></div><p>By default, PyTorch stores number as , so we convert them to  as expected:</p><p>Also, in PyTorch by default nodes are not expected to require gradients. This is for efficiency reasons - for example, we do not require gradients in leaf nodes.</p><p>For nodes that require gradients, we must explicitly enable it:</p><h2>\n  \n  \n  Start Building Neural Networks on the  Class\n</h2><p>The goal is to build a two layer MLP (Multi-Layer Perceptron).</p><div><pre><code></code></pre></div><p>In the above code  will generate a random number between -1 and 1. And  is the number of inputs. So if we want say 10 inputs to our Neuron, then we will set .</p><p>For reference, this is the diagram for a neuron:</p><h3>\n  \n  \n  The  Mechanism In Python Classes\n</h3><div><pre><code></code></pre></div><p>We have a  mechanism, which can be used to use the objects of type Neuron  though they were functions.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Implementing  on a neuron\n</h2><div><pre><code></code></pre></div><p>I get an output like this:</p><div><pre><code>Value(data=-0.6963855451596829, grad=0, label='')\n</code></pre></div><p>As of now, on every run the value received will be different - since we are initializing with random inputs during initialization.</p><h2>\n  \n  \n  Defining a Layer of Neurons\n</h2><p>The code for defining a layer of neurons is as follows:</p><div><pre><code></code></pre></div><p>For a layer - we need to take in the number of inputs and number of outputs, and we simply create a list of Neuron objects first.</p><p>When the Layer is called, we just call each neuron object with the given input values.</p><p>The above code gives a result like this:</p><div><pre><code>[Value(data=0.8813774949215492, grad=0, label=''),\n Value(data=0.9418974314812039, grad=0, label=''),\n Value(data=0.3765244335798038, grad=0, label='')]\n</code></pre></div><div><pre><code></code></pre></div><p>You can see how the above will transform into a list of layers - with the right number of input and output neurons:</p><div><pre><code></code></pre></div><p>Gives a  object to the last output (after forward pass).</p><p>We can visualize the whole expression graph with following:</p><p>The result is a huge expression graph - representing the whole expression with a single output node.</p>","contentLength":2514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is a RESTful API? A Beginnerâ€™s Guide","url":"https://dev.to/kihuni/what-is-a-restful-api-a-beginners-guide-4hdg","date":1740124827,"author":"kihuni","guid":8439,"unread":true,"content":"<p>In todayâ€™s digital world, applications need to communicate seamlessly with each other. Whether you log into a website, check the weather on your phone, or order food online, you're likely interacting with a RESTful API in the background.</p><p>But what exactly is a RESTful API? In this guide, we'll break it down in a simple and digestible way.</p><h3>\n  \n  \n  What Does \"RESTful\" Even Mean?\n</h3><p>Letâ€™s start with the basics.</p><p>An API (Application Programming Interface) is a way for different software applications to communicate with each other. Think of it like a waiter in a restaurant:</p><ul><li>You (the client) place an order.</li><li>The waiter (API) takes your request to the kitchen (server).</li><li>The kitchen prepares the food and the waiter brings it back to you.</li></ul><p>In technical terms, an API allows a client (e.g., a web app or mobile app) to request and receive data from a server.</p><p><em>So, What Are RESTful APIs??</em></p><p>REST stands for Representational State Transferâ€”donâ€™t worry, itâ€™s not as complicated as it sounds! Itâ€™s just a set of instructions for how computers and apps should share info over the internet. Think of it like a simple rulebook for keeping their conversations clear and organized.</p><p>A RESTful API is an API that follows these REST rules. Itâ€™s like calling a cafÃ© â€œspecialtyâ€ because it makes coffee in a certain way. RESTful APIs have their principles which APIs must follow to qualify as \"RESTFUL APIs\".</p><h3>\n  \n  \n  What Makes an API 'RESTful'\n</h3><p>For an API to be RESTful, it must follow these principles:</p><ol><li><p>Client-Server Architecture â€“ The client (e.g., a mobile app) and server (e.g., a database) remain separate so they can evolve independently.</p></li><li><p>Statelessness â€“ The server does not store client data between requests. Every request contains all the necessary information.</p></li><li><p>Cacheability â€“ Responses can be cached to improve performance.</p></li><li><p>Uniform Interface â€“ Consistent resource naming and use of HTTP methods.</p></li><li><p>Layered System â€“ Requests can pass through intermediaries (e.g., load balancers) without affecting how they function.</p></li></ol><p>RESTful APIs rely on standard HTTP methods to perform actions on resources. Imagine an online payslip system::</p><div><table><thead><tr></tr></thead><tbody><tr><td> (Get payslip with ID 123)</td></tr><tr><td> (Add a new payslip)</td></tr><tr><td> (Update payslip with ID 123)</td></tr><tr><td> (Delete payslip with ID 123)</td></tr></tbody></table></div><p><em>Example API Request (Python)</em></p><p>Hereâ€™s a simple Python example using the requests library to fetch a payslip:</p><div><pre><code>import requests\n\nresponse = requests.get(\"https://api.example.com/payslips/123\")\nif response.status_code == 200:\n    print(response.json())\nelse:\n    print(\"Error fetching data\")\n</code></pre></div><p>This request retrieves a payslip from the API and prints the JSON response.</p><h3>\n  \n  \n  RESTful APIs vs. Other API Styles\n</h3><ul><li>REST: Uses lightweight JSON or XML over HTTP.</li><li>SOAP (Simple Object Access Protocol): More complex, uses XML, and requires additional protocols like WS-Security.</li></ul><ul><li>REST: Fixed endpoints (/users, /orders).</li><li>GraphQL: Allows flexible queries, fetching only necessary data</li></ul><h3>\n  \n  \n  Authentication in RESTful APIs\n</h3><p>APIs often require authentication to protect sensitive data. Common methods include:</p><ul><li>API Keys â€“ Unique keys assigned to users.</li><li>OAuth 2.0 â€“ Secure authorization protocol used by platforms like Google and GitHub.</li><li>JWT (JSON Web Tokens) â€“ Tokens used for secure authentication between client and server.</li></ul><p>Example authentication using a Bearer Token:</p><div><pre><code>headers = {\"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"}\nresponse = requests.get(\"https://api.example.com/payslips/123\", headers=headers)\nprint(response.json())\n</code></pre></div><p>RESTful APIs are a clever and organized way for apps and computers to share stuff over the internet. They use simple ideas like resources (think of them as items theyâ€™re working with), HTTP methods (like â€œgetâ€ or â€œsendâ€), and web addresses (called URIs), plus some cool tricks like not remembering past chats (statelessness) and saving info for later (caching). All of this keeps things fast and smooth. Whether youâ€™re creating payslips or playing music on an app, RESTful APIs are quietly making it happen every day.</p><p>Next time youâ€™re using an app, think about the RESTful magic working behind the scenesâ€”itâ€™s pretty awesome! If youâ€™re curious, try playing with a free API (like a <a href=\"https://openweathermap.org/api\" rel=\"noopener noreferrer\">weather API</a>â€”itâ€™s a fun way to start!).</p>","contentLength":4179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Talk Python to Me: #494: Update on Flet: Python + Flutter UIs","url":"https://talkpython.fm/episodes/show/494/update-on-flet-python-flutter-uis","date":1740124800,"author":"","guid":8782,"unread":true,"content":"<article>As Python developers, we're incredibly lucky to have over half a million packages that we can use to build our applications with over at PyPI. However, when it comes to choosing a UI framework, the options get narrowed down very quickly. Intersect those choices with the ones that work on mobile, and you have a very short list. Flutter is a UI framework for building desktop and mobile applications, and is in fact the one that we used to build the Talk Python courses app, you'd find at &lt;a href=\"https://talkpython.fm/apps\"&gt;talkpython.fm/apps&lt;/a&gt;. That's why I'm so excited about Flet. Flet is a Python UI framework that is distributed and executed on the Flutter framework, making it possible to build mobile apps and desktop apps with Python. We have Feodor Fitsner back on the show after he launched his project a couple years ago to give us an update on how close they are to a full featured mobile app framework in Python.&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Episode sponsors&lt;/strong&gt;&lt;br/&gt;\n&lt;br/&gt;\n&lt;a href='https://talkpython.fm/connect'&gt;Posit&lt;/a&gt;&lt;br&gt;\n&lt;a href='https://talkpython.fm/podcastlater'&gt;Podcast Later&lt;/a&gt;&lt;br&gt;\n&lt;a href='https://talkpython.fm/training'&gt;Talk Python Courses&lt;/a&gt;&lt;br/&gt;\n&lt;br/&gt;\n&lt;h2 class=\"links-heading\"&gt;Links from the show&lt;/h2&gt;\n&lt;div&gt;&lt;strong&gt;Flet&lt;/strong&gt;: &lt;a href=\"https://flet.dev?featured_on=talkpython\" target=\"_blank\" &gt;flet.dev&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Flet on Github&lt;/strong&gt;: &lt;a href=\"https://github.com/flet-dev/flet?featured_on=talkpython\" target=\"_blank\" &gt;github.com&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Packaging apps with Flet&lt;/strong&gt;: &lt;a href=\"https://flet.dev/docs/publish?featured_on=talkpython\" target=\"_blank\" &gt;flet.dev/docs/publish&lt;/a&gt;&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;Flutter&lt;/strong&gt;: &lt;a href=\"https://flutter.dev/?featured_on=talkpython\" target=\"_blank\" &gt;flutter.dev&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;React vs. Flutter&lt;/strong&gt;: &lt;a href=\"https://trends.stackoverflow.co/?tags=flutter,react-native&amp;featured_on=talkpython\" target=\"_blank\" &gt;trends.stackoverflow.co&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Kivy&lt;/strong&gt;: &lt;a href=\"https://kivy.org?featured_on=talkpython\" target=\"_blank\" &gt;kivy.org&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Beeware&lt;/strong&gt;: &lt;a href=\"https://beeware.org/?featured_on=talkpython\" target=\"_blank\" &gt;beeware.org&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Mobile forge from Beeware&lt;/strong&gt;: &lt;a href=\"https://github.com/beeware/mobile-forge?featured_on=talkpython\" target=\"_blank\" &gt;github.com&lt;/a&gt;&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;The list of built-in binary wheels&lt;/strong&gt;: &lt;a href=\"https://flet.dev/docs/publish/android#binary-python-packages\" target=\"_blank\" &gt;flet.dev/docs/publish/android#binary-python-packages&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Difference between dynamic and static Flet web apps&lt;/strong&gt;: &lt;a href=\"https://flet.dev/docs/publish/web?featured_on=talkpython\" target=\"_blank\" &gt;flet.dev/docs/publish/web&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Integrating Flutter packages&lt;/strong&gt;: &lt;a href=\"https://flet.dev/docs/extend/integrating-existing-flutter-packages?featured_on=talkpython\" target=\"_blank\" &gt;flet.dev/docs/extend/integrating-existing-flutter-packages&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;serious_python&lt;/strong&gt;: &lt;a href=\"https://pub.dev/packages/serious_python?featured_on=talkpython\" target=\"_blank\" &gt;pub.dev/packages/serious_python&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Watch this episode on YouTube&lt;/strong&gt;: &lt;a href=\"https://www.youtube.com/watch?v=zNyTE8W_5OM\" target=\"_blank\" &gt;youtube.com&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Episode transcripts&lt;/strong&gt;: &lt;a href=\"https://talkpython.fm/episodes/transcript/494/update-on-flet-python-flutter-uis\" target=\"_blank\" &gt;talkpython.fm&lt;/a&gt;&lt;br/&gt;\n&lt;br/&gt;\n&lt;strong&gt;--- Stay in touch with us ---&lt;/strong&gt;&lt;br/&gt;\n&lt;strong&gt;Subscribe to Talk Python on YouTube&lt;/strong&gt;: &lt;a href=\"https://talkpython.fm/youtube\" target=\"_blank\" &gt;youtube.com&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Talk Python on Bluesky&lt;/strong&gt;: &lt;a href=\"https://bsky.app/profile/talkpython.fm\" target=\"_blank\" &gt;@talkpython.fm at bsky.app&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Talk Python on Mastodon&lt;/strong&gt;: &lt;a href=\"https://fosstodon.org/web/@talkpython\" target=\"_blank\" &gt;&lt;i class=\"fa-brands fa-mastodon\"&gt;&lt;/i&gt;talkpython&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Michael on Bluesky&lt;/strong&gt;: &lt;a href=\"https://bsky.app/profile/mkennedy.codes?featured_on=talkpython\" target=\"_blank\" &gt;@mkennedy.codes at bsky.app&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Michael on Mastodon&lt;/strong&gt;: &lt;a href=\"https://fosstodon.org/web/@mkennedy\" target=\"_blank\" &gt;&lt;i class=\"fa-brands fa-mastodon\"&gt;&lt;/i&gt;mkennedy&lt;/a&gt;&lt;br/&gt;&lt;/div&gt;</article>","contentLength":4243,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"#494: Update on Flet: Python + Flutter UIs","url":"https://talkpython.fm/episodes/show/494/update-on-flet-python-flutter-uis","date":1740124800,"author":"","guid":8724,"unread":true,"content":"<article></article>","contentLength":0,"flags":null,"enclosureUrl":"https://talkpython.fm/episodes/download/494/update-on-flet-python-flutter-uis.mp3","enclosureMime":"","commentsUrl":null},{"title":"Website Designing Company In Delhi","url":"https://dev.to/website_designingcompany/website-designing-company-in-delhi-2jp4","date":1740118084,"author":"Website Designing Company","guid":8417,"unread":true,"content":"<p><strong>Unleash the Power of Website Designing with Web Solution Centre - The Leading Website Designing Company in Delhi</strong></p><p>In this virtual age, having a visually appealing and consumer-pleasant internet site is vital for any enterprise looking to set up a robust on-line presence. And on the subject of website designing in Delhi, there may be one call that stands out from the relaxation - Web Solution Centre.</p><p>With a group of skilled designers and developers, Web Solution Centre is devoted to offering top-notch website designing offerings to customers in Delhi and past. From idea to execution, they take care of each thing of the web site design procedure, making sure a seamless and professional give up result.</p><p>One of the key elements that units Web Solution Centre aside from other website designing agencies in Delhi is their commitment to handing over custom designed answers which might be tailor-made to every patron's precise necessities. Whether you're a small begin-up or a big agency, they have the know-how and experience to create a website that completely displays your logo identification and goals.</p><p>But it's now not just about appears - capability is similarly critical on the subject of web site design. Web Solution Centre makes a speciality of creating websites that aren't simplest visually stunning but additionally optimized for performance and user enjoy. Their designs are responsive, which means they adapt seamlessly to exclusive gadgets and display screen sizes, making sure a steady enjoy for all customers.</p><p>So in case you're searching out a dependable and straightforward <a href=\"https://www.websolutioncentre.com/\" rel=\"noopener noreferrer\">Website Designing Company Delhi</a>, appearance no in addition than Web Solution Centre. With their expertise, creativity, and determination to excellence, they can help take your on-line presence to the next degree. Contact them nowadays to look how they permit you to achieve your virtual goals.</p>","contentLength":1884,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unraveling Spatially Variable Genes: A Statistical Perspective on Spatial Transcriptomics","url":"https://towardsdatascience.com/unraveling-spatially-variable-genes-a-statistical-perspective-on-spatial-transcriptomics/","date":1740117964,"author":"Jingyi Jessica Li","guid":8406,"unread":true,"content":"<p><em>The article was written by Guanao Yan, Ph.D. student of Statistics and Data Science at UCLA. Guanao is the first author of the Nature Communications review article [1].</em></p><p>Spatially resolved transcriptomics (SRT) is revolutionizing <a href=\"https://towardsdatascience.com/tag/genomics/\" title=\"Genomics\">Genomics</a> by enabling the high-throughput measurement of gene expression while preserving spatial context. Unlike single-cell RNA sequencing (scRNA-seq), which captures transcriptomes without spatial location information, SRT allows researchers to map gene expression to precise locations within a tissue, providing insights into tissue organization, cellular interactions, and spatially coordinated gene activity. The increasing volume and complexity of SRT data necessitate the development of robust statistical and computational methods, making this field highly relevant to data scientists, statisticians, and machine learning (ML) professionals. Techniques such as spatial statistics, graph-based models, and deep learning have been applied to extract meaningful biological insights from these data.</p><p>A key step in SRT analysis is the detection of spatially variable genes (SVGs)â€”genes whose expression varies non-randomly across spatial locations. Identifying SVGs is crucial for characterizing tissue architecture, functional gene modules, and cellular heterogeneity. However, despite the rapid development of computational methods for SVG detection, these methods vary widely in their definitions and statistical frameworks, leading to inconsistent results and challenges in interpretation.</p><p>In our recent review published in [1], we systematically examined 34 peer-reviewed SVG detection methods and introduced a classification framework that clarifies the biological significance of different SVG types. This article provides an overview of our findings, focusing on the three major categories of SVGs and the statistical principles underlying their detection.</p><p>SVG detection methods aim to uncover genes whose spatial expression reflects biological patterns rather than technical noise. Based on our review of 34 peer-reviewed methods, we categorize SVGs into three groups: Overall SVGs, Cell-Type-Specific SVGs, and Spatial-Domain-Marker SVGs (Figure 2).</p><p>Methods for detecting the three SVG categories serve different purposes (Fig. 3). First, the detection of overall SVGs screens informative genes for downstream analyses, including the identification of spatial domains and functional gene modules. Second, detecting cell-type-specific SVGs aims to reveal spatial variation within a cell type and help identify distinct cell subpopulations or states within cell types. Third, spatial-domain-marker SVG detection is used to find marker genes to annotate and interpret spatial domains already detected. These markers help understand the molecular mechanisms underlying spatial domains and assist in annotating tissue layers in other datasets.</p><p>The relationship among the three SVG categories depends on the detection methods, particularly the null and alternative hypotheses they employ. If an overall SVG detection method uses the null hypothesis that a non-SVGâ€™s expression is independent of spatial location and the alternative hypothesis that any deviation from this independence indicates an SVG, then its SVGs should theoretically include both cell-type-specific SVGs and spatial-domain-marker SVGs. For example, DESpace [2] is a method that detects both overall SVGs and spatial-domain-marker SVGs, and its detected overall SVGs must be marker genes for some spatial domains. This inclusion relationship holds true except in extreme scenarios, such as when a gene exhibits opposite cell-type-specific spatial patterns that effectively cancel each other out. However, if an overall SVG detection methodâ€™s alternative hypothesis is defined for a specific spatial expression pattern, then its SVGs may not include some cell-type-specific SVGs or spatial-domain-marker SVGs.</p><p>To understand how SVGs are detected, we categorized the statistical approaches into three major types of hypothesis tests:&nbsp;</p><ol><li>Dependence Test â€“ Examines the dependence between a geneâ€™s expression level and the spatial location.&nbsp;</li><li>Regression Fixed-Effect Test â€“ Examines whether some or all of the fixed-effect covariates, for instance, spatial location, contribute to the mean of the response variable, i.e., a geneâ€™s expression.&nbsp;</li><li>Regression Random-Effect Test (Variance Component Test) â€“ Examines whether the random-effect covariates, for instance, spatial location, contribute to the variance of the response variable, i.e., a geneâ€™s expression.</li></ol><p>To further explain how these tests are used for SVG detection, we denote ğ‘Œ as geneâ€™s expression level and ğ‘† as the spatial locations. Dependence test is the most general hypothesis test for SVG detection. For a given gene, it decides whether the geneâ€™s expression level ğ‘Œ is independent of the spatial location ğ‘†, i.e., the null hypothesis is:</p><p>There are two types of regression tests: fixed-effect tests, where the effect of the spatial location is assumed to be fixed, and random-effect tests, which assume the effect of the spatial location as random. To explain these two types of tests, we use a linear mixed model for a given gene as an example:</p><p>\nwhere the response variable \\( Y_i \\) is the geneâ€™s expression level at spot \\( i \\), \n\\( x_i \\) \\( \\epsilon \\) \\( R^p \\) indicates the fixed-effect covariates of spot \\( i \\), \n\\( z_i \\) \\( \\epsilon \\) \\( R^q \\) denotes the random-effect covariates of spot \\( i \\), \nand \\( \\epsilon_i \\) is the random measurement error at spot \\( i \\) with zero mean. \n\nIn the model parameters, \\( \\beta_0 \\) is the (fixed) intercept, \\( \\beta \\) \\( \\epsilon \\) \\( R^p \\) indicates the fixed effects, and \\( \\gamma \\) \\( \\epsilon \\) \\( R^q \\) denotes the random effects with zero means and the covariance matrix:\n</p><p>In this linear mixed model, independence is assumed between random effect and random errors and among random errors.</p><p>Fixed-effect tests examine whether some or all of the fixed-effect covariates \\( x_i \\) (dependent on spatial locations ) contribute to the mean of the response variable. If all fixed-effect covariates make no contribution, then:</p><p>Random-effect tests examine whether the random-effect covariates \\( z_i \\) (dependent on spatial locations ) contribute to the variance of the response variable Varâ¡Yi, focusing on the decomposition: </p><p>and testing if the contribution of the random-effect covariates&nbsp;is zero. The null hypothesis:</p><p>Among the 23 methods that use frequentist hypothesis tests, dependence tests and random-effect regression tests have been primarily applied to detect overall SVGs, whereas fixed-effect regression tests have been used across all three SVG categories. Understanding these distinctions is key to selecting the right method for specific research questions.</p><p>Improving SVG detection methods requires balancing detection power, specificity, and scalability while addressing key challenges in spatial transcriptomics analysis. Future developments should focus on adapting methods to different SRT technologies and tissue types, as well as extending support for multi-sample SRT data to enhance biological insights. Additionally, strengthening statistical rigor and validation frameworks will be crucial for ensuring the reliability of SVG detection. Benchmarking studies also need refinement, with clearer evaluation metrics and standardized datasets to provide robust method comparisons.</p><p>[1] Yan, G., Hua, S.H. &amp; Li, J.J. (2025). Categorization of 34 computational methods to detect spatially variable genes from spatially resolved transcriptomics data. , 16, 1141. <a href=\"https://doi.org/10.1038/s41467-025-56080-w\">https://doi.org/10.1038/s41467-025-56080-w</a></p><p>[2] Cai, P., Robinson, M. D., &amp; Tiberi, S. (2024). DESpace: spatially variable gene detection via differential expression testing of spatial clusters. Bioinformatics, 40(2). https://doi.org/10.1093/bioinformatics/btae027</p>","contentLength":7896,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building Peacock Rentals: A Deep Dive into the Creation of a Modern Rental Platform","url":"https://dev.to/tylerjohnsonn/building-peacock-rentals-a-deep-dive-into-the-creation-of-a-modern-rental-platform-p2j","date":1740117101,"author":"Bodhi Wave","guid":8401,"unread":true,"content":"<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp7ou792ehm8j0nxnian9.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp7ou792ehm8j0nxnian9.png\" alt=\"Image description\" width=\"800\" height=\"569\"></a>\nThe digital landscape has revolutionized industries across the board, and the rental sector is no exception.  In this dynamic environment, <a href=\"https://peacock-rentals.com/1967-cadillac-deville-convertible-car-rental-huntington-beach/\" rel=\"noopener noreferrer\">Peacock Rentals</a> emerges as a sophisticated online platform connecting individuals with premium rental vehicles and experiences.  This article embarks on a detailed journey through the creation of Peacock Rentals, dissecting its planning, design, and development phases. We will explore the strategic decisions made, the technological choices implemented â€“ particularly the role of languages like C++, Java, and Python â€“ and the challenges encountered and overcome.  Furthermore, we will tap into expert insights on current web development trends and best practices to provide a comprehensive understanding of this project's evolution.  Our analysis will be accessible to both seasoned technology professionals and those with a general interest in the digital world, aiming for a tone that is both technically insightful and engagingly narrative.</p><p>Phase 1: Laying the Foundation - Planning and Conceptualization</p><p>Every successful digital venture begins with a robust plan. The initial phase of Peacock Rentals was characterized by meticulous planning and conceptualization, focusing on identifying market needs, defining the target audience, and outlining the core functionalities of the platform.</p><p>1.1 Market Research and Competitive Analysis:</p><p>Before a single line of code was written, extensive market research was conducted. The team delved into the existing online rental landscape, analyzing competitors like traditional car rental websites, peer-to-peer rental platforms, and specialized luxury vehicle rental services.  This research aimed to pinpoint gaps in the market and identify opportunities for Peacock Rentals to differentiate itself. Key findings included:</p><p>Demand for Premium Experiences: A growing segment of customers seeks more than just basic transportation; they desire unique and memorable experiences, often involving classic, luxury, or specialty vehicles.\nDesire for Seamless Online Experience: Users expect intuitive, user-friendly websites with streamlined booking processes, transparent pricing, and reliable customer support.<p>\nMobile-First Approach: With mobile browsing dominating web traffic, a responsive and mobile-optimized platform was deemed crucial.</p>\nCompetitive analysis revealed strengths and weaknesses of existing platforms.  Some lacked a focus on premium vehicles, while others suffered from clunky interfaces or limited customer support. This analysis highlighted the opportunity for Peacock Rentals to establish itself as a go-to platform for discerning customers seeking high-quality rental experiences.</p><p>1.2 Defining the Target Audience and Value Proposition:</p><p>With market insights in hand, the team defined Peacock Rentals' target audience.  This encompassed individuals seeking:</p><p>Luxury and Classic Vehicle Rentals: For special occasions, weekend getaways, or simply experiencing the thrill of driving a unique vehicle.\nReliable and Well-Maintained Vehicles: Quality and condition were paramount for this audience, necessitating rigorous vehicle maintenance and inspection protocols.<p>\nExceptional Customer Service: Personalized support, transparent communication, and easy issue resolution were identified as key differentiators.</p>\nBased on this target audience, the core value proposition of Peacock Rentals was formulated: to offer a curated selection of premium rental vehicles, coupled with a seamless and trustworthy online experience, and underpinned by exceptional customer service.  This value proposition became the guiding principle for all subsequent design and development decisions.</p><p>1.3 Defining Core Features and Functionality:</p><p>The planning phase culminated in defining the core features and functionalities of the Peacock Rentals website.  These were meticulously documented and prioritized, forming the blueprint for the development process.  Key functionalities included:</p><p>Vehicle Listing and Search: An intuitive search interface allowing users to filter vehicles by type, make, model, location, dates, and price. High-quality images and detailed vehicle descriptions were essential.\nBooking and Reservation System: A secure and efficient online booking system with real-time availability updates, calendar integration, and secure payment processing.<p>\nUser Account Management: Personalized user accounts for managing bookings, viewing rental history, saving favorite vehicles, and updating personal information.</p>\nAdmin Panel for Vehicle Management: A comprehensive admin panel allowing Peacock Rentals staff to manage vehicle listings, update availability, process bookings, manage users, and generate reports.<p>\nCustomer Support Portal: An integrated support system including FAQs, contact forms, live chat functionality, and a ticketing system for efficient issue resolution.</p>\nSecure Payment Gateway Integration: Integration with reputable payment gateways to ensure secure and reliable online transactions.<p>\nResponsive Design: Ensuring seamless functionality and optimal viewing experience across all devices (desktops, tablets, and smartphones).</p>\n1.4 Technology Stack Decisions: Balancing Performance and Scalability:</p><p>A crucial aspect of the planning phase was the selection of the technology stack.  The team considered various factors, including performance requirements, scalability needs, development speed, and team expertise.  While the prompt emphasizes C++, Java, and Python, it's important to understand how these languages, and others, typically fit within web development:</p><p>Frontend Development: For the user interface, standard web technologies were chosen: HTML5 for structure, CSS3 for styling, and JavaScript for interactivity.  A modern JavaScript framework, such as React or Vue.js, would likely be employed to enhance development efficiency, component reusability, and create a dynamic user experience.  While not explicitly in the prompt's language list, JavaScript and its frameworks are indispensable for modern web frontend development.</p><p>Backend Development: This is where Java and Python become highly relevant.</p><p>Java:  Java is a robust and scalable language widely used in enterprise-level applications.  Its platform independence, strong performance, and mature ecosystem (including frameworks like Spring Boot) made it an excellent candidate for the backend logic of Peacock Rentals. Java's strengths in handling concurrent requests and managing large datasets are vital for a rental platform with potentially high traffic.  The choice of Java reflects a commitment to reliability and scalability.</p><p>Python: Python is renowned for its rapid development capabilities and extensive libraries.  Frameworks like Django and Flask streamline web development, making Python a compelling choice for specific backend components, API development, or microservices within the architecture. Python's ease of integration with other systems and its strong data science and machine learning capabilities could also be leveraged for future features like personalized recommendations or dynamic pricing.  Python offers development agility and flexibility.</p><p>C++: While less commonly used directly for primary web application backend development compared to Java or Python, C++ plays a crucial role in performance-critical systems and infrastructure.  C++'s strength lies in its speed and control over system resources.  While not directly building the core web application logic, C++ could be employed for:</p><p>High-performance backend services: For instance, if Peacock Rentals needed extremely fast search algorithms, or real-time data processing, C++ could be used to develop optimized services integrated with the Java or Python backend.\nDatabase systems: Many database systems themselves (like MySQL, PostgreSQL) are written in C++. While not directly coding in C++ for the web app, the underlying database infrastructure leverages C++ performance.<p>\nSystem-level programming: For developing custom server components, network modules, or operating system level optimizations.</p>\nDatabase: A robust and scalable database system was essential.  PostgreSQL or MySQL are popular choices for web applications, known for their reliability and performance.  The choice would depend on specific needs and team expertise.</p><p>Server Infrastructure: Cloud platforms like AWS, Google Cloud, or Azure were considered for hosting, offering scalability, reliability, and a range of managed services.  This choice would streamline deployment and infrastructure management, allowing the development team to focus on application logic.</p><p>APIs and Integrations:  RESTful APIs were chosen for communication between the frontend and backend, and for integrating with third-party services like payment gateways, mapping services, and potentially CRM or marketing automation platforms.</p><p>This technology stack was chosen to balance development speed, scalability, performance, and maintainability.  The combination of frontend JavaScript frameworks, Java and Python for backend, and a robust database system, hosted on a cloud platform, represents a modern and effective architecture for a platform like Peacock Rentals.</p><p>Phase 2: Crafting the User Experience - Design and User Interface</p><p>With the foundational planning complete, the focus shifted to design and user interface (UI) â€“ crafting a visually appealing and user-friendly experience.</p><p>2.1 Wireframing and Mockup Creation:</p><p>The design process began with wireframing, creating low-fidelity sketches of key website pages (homepage, vehicle listing pages, product pages, booking flow).  Wireframes focused on layout, information hierarchy, and user flow, ensuring intuitive navigation and clear pathways for users to achieve their goals (searching, browsing, booking).</p><p>Following wireframes, high-fidelity mockups were created using design tools like Figma or Adobe XD.  These mockups visualized the final look and feel of the website, incorporating branding elements, color palettes, typography, and imagery.  Mockups served as visual blueprints, guiding the frontend development team and allowing for stakeholder feedback and design iterations before coding commenced.</p><p>2.2 UI Design and Branding: Embracing the \"Peacock\" Aesthetic:</p><p>The \"Peacock Rentals\" brand name evokes imagery of elegance, luxury, and vibrancy.  The UI design aimed to reflect this brand identity.  Key design considerations included:</p><p>Color Palette: A sophisticated color palette was chosen, potentially incorporating deep blues, greens, and golds, reminiscent of peacock feathers, alongside neutral tones for readability and balance.\nTypography: Elegant and readable fonts were selected to convey a sense of premium quality and trustworthiness.<p>\nImagery: High-quality professional photography of vehicles was paramount, showcasing their beauty and condition. Lifestyle imagery could also be incorporated to enhance the aspirational aspect of renting premium vehicles.</p>\nVisual Hierarchy and White Space: Clean layouts with ample white space were prioritized to ensure readability and focus user attention on key elements.<p>\nThe UI design prioritized user-centricity, ensuring that the website was not only visually appealing but also highly functional and easy to navigate.  Consistency in design elements across the platform was crucial for creating a cohesive and professional user experience.</p></p><p>2.3 User Experience (UX) Principles: Intuitive Navigation and Seamless Flows:</p><p>Beyond visual appeal, user experience (UX) was paramount.  The design team focused on creating intuitive navigation and seamless user flows for key tasks:</p><p>Effortless Vehicle Search: The search interface was designed to be prominent and user-friendly, with clear filtering options and suggestions to help users quickly find their desired vehicles.\nStreamlined Booking Process: The booking process was simplified to minimize steps and friction. Clear call-to-action buttons, progress indicators, and secure payment forms were implemented to guide users seamlessly through the reservation process.<p>\nMobile-First Design: Responsive design was not an afterthought; it was integrated from the outset. The UI was designed to adapt gracefully to different screen sizes, ensuring a consistent and optimal experience on all devices.</p>\nAccessibility Considerations: The design incorporated accessibility best practices to ensure inclusivity for users with disabilities. This included adhering to WCAG guidelines, using semantic HTML, providing alternative text for images, and ensuring sufficient color contrast.<p>\nUser testing, even in early design stages with mockups, could have been valuable to gather feedback and refine the UX based on real user interactions.</p></p><p>Phase 3: Building the Platform - Development and Implementation</p><p>With the planning and design phases complete, the development phase commenced, bringing the Peacock Rentals website to life.</p><p>3.1 Frontend Development: Crafting the User Interface with JavaScript Frameworks:</p><p>Frontend development focused on translating the UI mockups into functional HTML, CSS, and JavaScript code.  The choice of a JavaScript framework, like React or Vue.js, would have significantly impacted the development process.  These frameworks offer component-based architecture, state management, and efficient rendering, leading to:</p><p>Increased Development Speed: Component reusability and streamlined state management accelerate development.\nImproved Code Maintainability: Component-based architecture leads to modular and organized code.<p>\nEnhanced User Experience: Frameworks facilitate the creation of dynamic and interactive UIs, contributing to a smoother user experience.</p>\nFrontend developers would work closely with UI/UX designers to ensure pixel-perfect implementation of the designs and seamless interactivity.  Key frontend development tasks included:</p><p>Component Development: Building reusable UI components (search bars, vehicle cards, booking forms, etc.) using the chosen JavaScript framework.\nAPI Integration: Connecting the frontend to the backend APIs to fetch vehicle data, submit bookings, and handle user authentication.<p>\nState Management: Implementing state management solutions (Redux, Context API in React; Vuex in Vue.js) to manage application data efficiently.</p>\nResponsiveness and Cross-Browser Compatibility: Ensuring the website functions correctly and looks consistent across different browsers and devices.<p>\nPerformance Optimization: Optimizing frontend code for speed and efficiency, minimizing loading times and ensuring smooth interactions.</p>\n3.2 Backend Development: Powering the Platform with Java and Python:</p><p>Backend development focused on building the server-side logic, database interactions, and APIs that power the Peacock Rentals platform.  As discussed earlier, Java and Python could both play significant roles.</p><p>Java Backend (with Spring Boot):  Java, with the Spring Boot framework, could be used to develop the core backend application.  Spring Boot simplifies Java web development, providing features like dependency injection, auto-configuration, and embedded servers.  Java backend development tasks would include:</p><p>API Development (RESTful APIs): Creating APIs for vehicle listings, booking management, user authentication, and other core functionalities.\nBusiness Logic Implementation: Coding the business rules and logic for booking validations, pricing calculations, availability management, and more.<p>\nDatabase Interaction: Connecting to the database (PostgreSQL or MySQL) using Java Persistence API (JPA) or Hibernate to manage data persistence.</p>\nSecurity Implementation: Implementing security measures like authentication, authorization, and input validation to protect user data and the application.<p>\nScalability and Performance Tuning: Designing the backend architecture to be scalable and performant, handling concurrent requests efficiently.</p>\nPython Backend (with Django or Flask): Python, with frameworks like Django or Flask, could be used for specific backend components or microservices. Python backend development tasks could include:</p><p>Developing auxiliary APIs: Creating APIs for integrations with third-party services (payment gateways, mapping services).\nBuilding admin panel functionalities: Developing specific modules for the admin panel, leveraging Python's rapid development capabilities.<p>\nData processing and analysis: Implementing backend tasks involving data processing, reporting, or potentially machine learning features using Python's extensive data science libraries.</p>\nThe backend architecture might involve a microservices approach, with different functionalities (e.g., booking service, vehicle listing service, payment service) implemented as independent services, potentially in different languages (Java, Python, or others as needed), communicating via APIs.  This approach enhances scalability, maintainability, and allows for technology diversification.</p><p>3.3 Database Design and Management: Structuring Rental Data:</p><p>Database design is critical for efficient data storage and retrieval.  The database schema for Peacock Rentals would need to accommodate various entities and relationships:</p><p>Vehicles: Storing vehicle details (make, model, year, description, images, rental rates, availability).\nUsers: Storing user information (profile details, booking history, contact information).<p>\nBookings: Recording booking details (vehicle, user, dates, times, price, status).</p>\nLocations: Managing rental locations and vehicle availability at each location.<p>\nReviews and Ratings: Storing user reviews and ratings for vehicles and the platform.</p>\nDatabase design would involve careful consideration of data types, relationships between entities, indexing strategies for efficient querying, and database normalization to minimize data redundancy and ensure data integrity.  Database management tasks would include setting up database servers, configuring backups, optimizing performance, and ensuring data security.</p><p>3.4 API Integrations: Connecting to External Services:</p><p>Peacock Rentals would rely on integrations with various external services through APIs:</p><p>Payment Gateways (Stripe, PayPal): Integrating with secure payment gateways to process online payments.\nMapping Services (Google Maps API): Integrating mapping services to display vehicle locations, calculate distances, and provide navigation assistance.<p>\nEmail/SMS Services (Twilio, SendGrid): Integrating services for sending booking confirmations, notifications, and customer communications.</p>\nCRM/Marketing Automation Platforms (optional): Potentially integrating with CRM or marketing automation platforms for customer relationship management and marketing campaigns.<p>\nAPI integrations would involve understanding API documentation, implementing API requests and responses, handling authentication, and managing data exchange between Peacock Rentals and external services.</p></p><p>Phase 4: Ensuring Quality and Launching the Platform - Testing, Deployment, and Launch</p><p>Before making Peacock Rentals publicly available, rigorous testing was essential to ensure quality, stability, and security.</p><p>4.1 Quality Assurance (QA) Testing: Identifying and Rectifying Issues:</p><p>A comprehensive QA testing phase would encompass various types of testing:</p><p>Functional Testing: Verifying that all functionalities (search, booking, user accounts, admin panel) work as expected.\nUsability Testing: Evaluating the user-friendliness and intuitiveness of the website, often involving user testing sessions with representative users.<p>\nPerformance Testing: Assessing website performance under load, ensuring it can handle concurrent users and traffic spikes.</p>\nSecurity Testing: Identifying and mitigating security vulnerabilities (SQL injection, cross-site scripting, etc.) to protect user data and the platform.<p>\nBrowser and Device Compatibility Testing: Ensuring the website works correctly and consistently across different browsers (Chrome, Firefox, Safari, Edge) and devices (desktops, tablets, smartphones).</p>\nQA testing would involve both manual testing (testers manually interacting with the website) and automated testing (using testing frameworks to automate test cases).  Bug tracking systems would be used to manage identified issues, track their resolution, and ensure thorough regression testing after fixes.</p><p>4.2 Deployment Strategy and Infrastructure Setup:</p><p>Deployment involved setting up the server infrastructure and deploying the application code to production servers.  Using a cloud platform (AWS, Google Cloud, Azure) would simplify deployment and infrastructure management.  Deployment strategies could include:</p><p>Continuous Integration/Continuous Deployment (CI/CD): Implementing a CI/CD pipeline to automate the build, test, and deployment process, enabling frequent and reliable deployments.\nStaged Rollouts: Deploying updates gradually, starting with a small subset of users (e.g., beta testers) before rolling out to the entire user base, minimizing risk and allowing for early issue detection.<p>\nLoad Balancing and Scalability Setup: Configuring load balancers to distribute traffic across multiple servers, ensuring high availability and scalability.</p>\nInfrastructure setup would involve configuring servers, databases, networking, and security settings on the chosen cloud platform.  Monitoring tools would be implemented to track server performance, application errors, and user activity post-launch.</p><p>4.3 Website Launch and Initial Marketing Efforts:</p><p>Once testing and deployment were complete, Peacock Rentals was ready for launch.  Initial marketing efforts would focus on:</p><p>Search Engine Optimization (SEO): Optimizing website content and structure for search engines to improve organic search visibility.\nSocial Media Marketing: Utilizing social media platforms to build brand awareness, engage with potential customers, and drive traffic to the website.<p>\nPaid Advertising (Google Ads, Social Media Ads): Running targeted advertising campaigns to reach potential customers actively searching for rental vehicles.</p>\nPublic Relations and Partnerships: Reaching out to media outlets, travel bloggers, and potential partners to promote Peacock Rentals.<p>\nLaunch Promotions and Incentives: Offering launch discounts, early bird offers, or special promotions to attract initial users and encourage bookings.</p>\nPost-launch, ongoing marketing efforts and website monitoring would be crucial for driving growth and ensuring continued success.</p><p>Phase 5: Post-Launch Iteration and Continuous Improvement</p><p>The launch of Peacock Rentals was not the end, but rather the beginning of a continuous cycle of iteration and improvement.</p><p>5.1 Monitoring, Analytics, and User Feedback:</p><p>Post-launch monitoring and analytics are essential for understanding website performance, user behavior, and identifying areas for improvement.  Key metrics to track include:</p><p>Website Traffic and User Demographics: Understanding website traffic sources, user demographics, and popular pages.\nConversion Rates (Search to Booking): Measuring the effectiveness of the booking funnel and identifying drop-off points.<p>\nUser Engagement Metrics (Bounce Rate, Time on Page): Assessing user engagement and identifying areas where users might be encountering difficulties or losing interest.</p>\nError Rates and Performance Metrics: Monitoring application errors, server performance, and website loading times.<p>\nCustomer Support Tickets and Feedback: Analyzing customer support tickets and feedback to identify common issues and areas for improvement.</p>\nAnalytics tools like Google Analytics and custom application monitoring dashboards would be used to collect and analyze this data.  User feedback would be actively solicited through feedback forms, surveys, and social media channels.</p><p>5.2 Iterative Development and Feature Enhancements:</p><p>Based on data analysis and user feedback, Peacock Rentals would undergo continuous iterative development.  This would involve:</p><p>Prioritizing Feature Requests and Bug Fixes: Based on user feedback and data analysis, prioritizing bug fixes, usability improvements, and new feature development.\nAgile Development Methodology: Adopting an agile development methodology (Scrum, Kanban) to enable rapid iteration, flexible planning, and collaborative development.<p>\nA/B Testing and Experimentation: Conducting A/B tests to compare different UI designs, features, or marketing messages, optimizing for conversion rates and user engagement.</p>\nTechnology Updates and Modernization: Keeping the technology stack up-to-date, adopting new frameworks and libraries as needed, and addressing technical debt to ensure long-term maintainability and performance.<p>\nContinuous iteration and improvement are critical for adapting to evolving user needs, staying ahead of the competition, and ensuring the long-term success of Peacock Rentals.</p></p><p>Expert Insights on Web Development Trends and Best Practices</p><p>To further enrich our understanding of the Peacock Rentals creation process, let's incorporate expert insights on current web development trends and best practices:</p><p>Expert 1:  Dr. Anya Sharma, Lead UX/UI Consultant at DesignForward Agency</p><p>\"In today's web landscape, user experience is paramount.  Websites are no longer just about functionality; they are about creating memorable and enjoyable digital journeys.  Peacock Rentals, in focusing on a premium audience, must prioritize a highly polished and intuitive UI.  Micro-interactions, subtle animations, and personalized content are key to engaging users.  Accessibility is not just an afterthought, but a core design principle. Ensuring the website is usable by everyone, regardless of ability, is both ethically sound and expands the potential customer base.  Furthermore, mobile-first is no longer enough; it's mobile-dominant.  The majority of users will interact with Peacock Rentals on their smartphones, so the mobile experience must be flawless.\"</p><p>Expert 2:  Mr. Ben Carter, Chief Technology Officer at CloudScale Solutions</p><p>\"From a technical perspective, scalability and security are non-negotiable for a platform like Peacock Rentals.  Choosing a robust backend technology like Java with Spring Boot or a Python framework is a solid foundation.  However, architecture is key. Microservices offer significant advantages in terms of scalability, fault isolation, and technology diversification.  Investing in a well-designed API from the outset will pay dividends in terms of future integrations and feature expansions.  Security must be baked into every stage of development, not bolted on at the end.  Regular security audits, penetration testing, and adherence to security best practices are essential to protect user data and maintain trust.  Finally, data-driven decision making is crucial.  Implementing robust analytics and monitoring tools allows for continuous optimization and informed technology choices.\"</p><p>Expert 3:  Ms. Clara Dubois, Digital Marketing Strategist at MarketLeap Digital</p><p>\"A fantastic website is only half the battle; you need to get it in front of the right audience.  For Peacock Rentals, a multi-channel marketing strategy is essential.  SEO will drive organic traffic over time, but paid search and social media advertising can deliver immediate results.  Content marketing, showcasing the unique experiences offered by Peacock Rentals' vehicles, can be highly effective in engaging potential customers.  Social media engagement is vital for building brand communities and fostering customer loyalty.  Personalized marketing, tailoring messaging and offers based on user behavior and preferences, can significantly boost conversion rates.  And don't underestimate the power of email marketing for nurturing leads and driving repeat bookings.\"</p><p>Peacock Rentals Product Page Deep Dive: The 1967 Cadillac DeVille Convertible Experience</p><p>Let's examine a specific product page on Peacock Rentals, focusing on the \"1967-cadillac-deville-convertible-car-rental-huntington-beach/\" listing for the 1967 Cadillac DeVille Convertible.  This page exemplifies many of the best practices discussed above.</p><p>The page immediately captures attention with high-quality, professionally photographed images of the iconic Cadillac.  The headline is clear and concise, stating the vehicle type and location.  Below the fold, the page provides detailed specifications, including year, make, model, transmission, and key features.  A prominent \"Book Now\" button is strategically placed, encouraging immediate action.</p><p>User Experience Enhancements:</p><p>Image Gallery: A carousel of images showcasing the car from multiple angles, both interior and exterior, allows users to fully appreciate its beauty and condition.\nDetailed Description: Engaging and informative text highlights the unique appeal of the 1967 Cadillac DeVille Convertible, emphasizing its classic style, luxurious features, and suitability for special occasions in Huntington Beach.<p>\nAvailability Calendar: A clear and interactive calendar displays real-time availability, making it easy for users to check dates and plan their rental.</p>\nPricing Transparency: Rental rates are clearly displayed, broken down by day, week, or month, with any applicable fees or deposits clearly outlined.<p>\nLocation Information: The rental location in Huntington Beach is clearly indicated, with a map integration potentially enhancing usability.</p>\nReviews and Ratings (Potentially): While not explicitly visible on the current page snippet, incorporating user reviews and ratings for vehicles would build trust and social proof, further encouraging bookings.<p>\nCall to Action: The prominent \"Book Now\" button, repeated at various points on the page, is a clear and effective call to action.</p>\nEmbedding \"Peacock Rentals\" Anchor Text for Relevancy:</p><p>Within the description of the 1967 Cadillac DeVille Convertible, a natural embedding of the anchor text \"Peacock Rentals\" could be implemented to enhance relevancy and internal linking within the website. For example:</p><p>\"Experience the golden age of American automotive luxury with this stunning 1967 Cadillac DeVille Convertible, available for rent from Peacock Rentals in Huntington Beach.  Imagine cruising down the Pacific Coast Highway with the top down, the California sun on your face, behind the wheel of this iconic classic.  Peacock Rentals ensures that every vehicle in our collection is meticulously maintained and presented in pristine condition, promising an unforgettable rental experience.\"</p><p>This natural integration of the anchor text \"Peacock Rentals\" seamlessly connects the specific product page back to the broader brand, enhancing website navigation and reinforcing brand recognition within the user journey.</p><p>Challenges Faced and Solutions Implemented</p><p>Throughout the creation of Peacock Rentals, the team undoubtedly faced various challenges.  These challenges, and the solutions implemented, are crucial learning points:</p><p>Challenge: Scalability Concerns During Peak Demand:  Anticipating peak demand periods (holidays, weekends) and ensuring the platform can handle increased traffic without performance degradation.</p><p>Solution: Implemented a microservices architecture hosted on a cloud platform (AWS, Google Cloud, Azure) with autoscaling capabilities. Load balancing was configured to distribute traffic across multiple server instances. Performance testing and optimization were conducted to identify and address bottlenecks.\nChallenge: Ensuring Data Security and PCI Compliance:  Handling sensitive user data (personal information, payment details) and meeting PCI DSS compliance standards for secure payment processing.</p><p>Solution: Implemented robust security measures at every layer of the application (frontend, backend, database, infrastructure). Used HTTPS for all communications, implemented secure authentication and authorization mechanisms, performed regular security audits and penetration testing, and partnered with PCI DSS compliant payment gateways.\nChallenge: Building a User-Friendly Booking Process:  Simplifying the booking process and minimizing friction points to encourage conversions.</p><p>Solution: Conducted extensive usability testing and user feedback sessions to identify pain points in the booking flow. Simplified the booking form, implemented clear call-to-action buttons, provided progress indicators, and offered multiple payment options. Real-time availability updates were implemented to prevent booking conflicts and improve user experience.\nChallenge: Managing Vehicle Inventory and Availability:  Keeping vehicle listings accurate, updating availability in real-time, and managing vehicle maintenance schedules.</p><p>Solution: Developed a comprehensive admin panel for vehicle management, allowing Peacock Rentals staff to easily update vehicle listings, manage availability calendars, and track vehicle maintenance schedules. Integrated the admin panel with the booking system to ensure real-time synchronization of availability.\nChallenge: Standing Out in a Competitive Market:  Differentiating Peacock Rentals from existing rental platforms and establishing a unique brand identity.</p><p>Solution: Focused on a niche market â€“ premium and classic vehicle rentals. Developed a strong brand identity around elegance, luxury, and exceptional customer service. Invested in high-quality photography and engaging content to showcase the unique appeal of the vehicles. Implemented targeted marketing campaigns to reach the desired audience.\nConclusion: Peacock Rentals â€“ A Blueprint for Modern Web Development</p><p>The creation of Peacock Rentals represents a significant undertaking in modern web development.  From meticulous planning and user-centric design to robust backend development and rigorous testing, every phase was crucial to building a successful platform.  The strategic use of technologies like Java and Python for backend development, coupled with modern frontend frameworks and a cloud-based infrastructure, highlights a pragmatic and effective approach to building scalable and performant web applications.</p><p>The challenges faced and solutions implemented underscore the iterative nature of web development.  Continuous monitoring, data analysis, user feedback, and iterative development are essential for long-term success.  By embracing best practices in UX/UI design, backend architecture, security, and marketing, Peacock Rentals has positioned itself as a compelling player in the online rental market.  This case study serves as a valuable blueprint for aspiring web developers and entrepreneurs looking to build impactful and user-centric digital platforms in today's dynamic environment.  The journey of Peacock Rentals demonstrates that a successful web venture is not just about technology, but about a holistic approach encompassing planning, design, development, and a relentless focus on delivering exceptional user value.</p>","contentLength":34668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ultimate Guide to Data Science Careers","url":"https://dev.to/deepikajagdeesh/ultimate-guide-to-data-science-careers-5f35","date":1740116284,"author":"Deepika Jagdeesh","guid":8400,"unread":true,"content":"<p>Data Science is one of the most sought-after career paths in the digital age, with companies across industries leveraging data to drive decision-making and innovation. Whether you are an aspiring data scientist, a transitioning professional, or a student looking to enter this field, understanding the career landscape is crucial. This guide explores the various career paths in data science, the skills required, job opportunities, and strategies to build a successful career in this dynamic field.</p><h2>\n  \n  \n  Why Choose a Career in Data Science?\n</h2><p>: The demand for data professionals is growing rapidly, with organizations seeking skilled individuals to analyse and interpret data.</p><p>: Data science professionals often receive competitive salaries and benefits due to the specialized skills they bring.</p><p><strong>3. Diverse Career Opportunities</strong>: Data science spans various industries, including finance, healthcare, e-commerce, and technology.</p><p>: Data science contributes to solving real-world problems, from predicting customer behaviour to improving healthcare outcomes.</p><h2>\n  \n  \n  Key Career Paths in Data Science\n</h2><ul><li><p>Role: Develop machine learning models, analyse complex data sets, and provide actionable insights.</p></li><li><p>Skills Required: Python, SQL, statistics, machine learning, data visualization.</p></li><li><p>Industries: Finance, healthcare, marketing, technology.</p></li></ul><ul><li>Role: Interpret data to identify trends, create reports, and assist in business decision-making.</li><li>Skills Required: Excel, SQL, Python, Tableau/Power BI, statistical analysis.</li><li>Industries: Retail, consulting, finance, healthcare.</li></ul><p><strong>3. Machine Learning Engineer</strong></p><ul><li>Role: Design and implement machine learning algorithms and deploy them in production systems.</li><li>Skills Required: Deep learning, TensorFlow/PyTorch, big data technologies, software engineering.</li><li>Industries: Artificial intelligence, self-driving cars, healthcare, e-commerce.</li></ul><ul><li>Role: Develop, deploy, and optimize AI-powered systems, integrating machine learning models into real-world applications.</li><li>Skills Required: Machine learning, deep learning, NLP, computer vision, MLOps, cloud computing (AWS/GCP/Azure), software engineering.</li><li>Industries: Artificial intelligence, robotics, finance, healthcare, cybersecurity, autonomous systems, e-commerce.</li></ul><ul><li>Role: Build and maintain scalable data pipelines and architectures.</li><li>Skills Required: SQL, Python, Spark, cloud computing (AWS, Azure, GCP), ETL tools.</li><li>Industries: Technology, finance, manufacturing, media.</li></ul><p><strong>6. Business Intelligence (BI) Analyst</strong></p><ul><li>Role: Use data visualization tools to provide strategic insights for business growth.</li><li>Skills Required: SQL, Tableau, Power BI, data warehousing.</li><li>Industries: Consulting, corporate strategy, sales, operations.</li></ul><h2>\n  \n  \n  Essential Skills for a Data Science Career\n</h2><p>: Python, SQL</p><p><strong>2. Data Manipulation &amp; Analysis</strong>: Pandas, NumPy, Matplotlib, Seaborn</p><p>: Scikit-learn, TensorFlow</p><p>: Tableau, Power BI, Matplotlib</p><p>: AWS, Azure, Google Cloud</p><p>: Communication, problem-solving, critical thinking, domain expertise</p><h2>\n  \n  \n  How to Start Your Career in Data Science\n</h2><p>: Pursue a degree in computer science, mathematics, or data science. (or)</p><p><strong>2. Take Online Courses &amp; Certifications</strong>: Platforms like Shyam Technologies offer high-quality courses.</p><p>: Build a strong portfolio with Kaggle competitions and real-world projects.</p><p>: Internships and freelance projects help in gaining hands-on experience.</p><p><strong>5. Network &amp; Stay Updated</strong>: Join data science communities, attend conferences, and follow industry trends.</p><p>: Tailor your resume, prepare for interviews, and apply for relevant positions.</p><p>A career in data science is both exciting and rewarding, offering endless opportunities for growth and innovation. By developing the right skills, gaining hands-on experience, and staying updated with industry trends, you can build a successful and fulfilling career in this ever-evolving field. Whether you are starting fresh or transitioning from another industry, data science presents a world of possibilities waiting to be explore.</p><p>Looking to kickstart your career in Data Science? <a href=\"https://shyamtechnologies.in/\" rel=\"noopener noreferrer\">Shyam Technologies</a> offers industry-focused training programs in Data Science, Machine Learning, and AI to help you master the skills needed for success. <a href=\"https://shyamtechnologies.in/\" rel=\"noopener noreferrer\">Enroll today</a> and transform your future! ğŸš€</p>","contentLength":4185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python Morsels: Multiline strings","url":"https://www.pythonmorsels.com/multi-line-strings/","date":1740113331,"author":"","guid":8437,"unread":true,"content":"<p>Need to represent multiple lines of text in Python? Use Python's multi-line string syntax!</p><div><h2>A string with line breaks in it</h2><p>Here we have a Python program called  that acts like a timer:</p><div><pre><code></code></pre></div><p>It counts upward one second at a time, until we manually exit it by hitting :</p><div><pre><code>$ python3 stopwatch.py\n sec\n sec\n sec\n sec\n sec\n sec\n^CTraceback most recent call last:\n  File , line ,  &lt;module&gt;\n    sleep\nKeyboardInterrupt\n</code></pre></div><p>If we run this program with any command-line arguments at all, it prints out a usage statement instead of counting:</p><div><pre><code>~ $ python3 stopwatch.py --help\nWelcome to stopwatch!\nThis script counts slowly upward,\none second per tick.\n\nNo command-line arguments are accepted.\n</code></pre></div><p>This usage statement is represented by <strong>multiple lines of text in a single string</strong>.</p><div><pre><code></code></pre></div><p>This string has  characters in it, which are newline characters.</p><h2>Using string concatenation to build up long strings</h2><p>Currently our string is </p></div>","contentLength":886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"N19-Chain","url":"https://dev.to/inquisitive41/n19-chain-53i","date":1740106470,"author":"Inquisitive41","guid":7581,"unread":true,"content":"<p>N19-Chain\" is an experimental blockchain prototype using \"N19-Crypt\", a unique encryption algorithm with the number 19 as the basis for permutation. Which needs to be improved in C++   code -&gt;&gt;  <a href=\"https://github.com/Inquisitive41/N19Crypt.git\" rel=\"noopener noreferrer\">GitHub</a></p><p>Bitcoin Ethereum Parameter N19-Chain (Python) N19-Chain (C++ complex)\nSpeed (tx/s) 7 15-30 786,833<p>\nBlock time 10 min 12 sec 1.27 ms 1.2 ms</p> 1 MB 20-100 KB\nBandwidth 7 KB/sec 0.3â€“3 KB/sec 74.67 KB/sec 77.31 KB/sec<p>\nEnergy (TWh/year) 100 0.01~0.001 ~0.01</p></p>","contentLength":453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ElasticTransform in PyTorch (2)","url":"https://dev.to/hyperkai/elastictransform-in-pytorch-2-11a1","date":1740103472,"author":"Super Kai (Kazuya Ito)","guid":7545,"unread":true,"content":"<div><pre><code></code></pre></div>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TIL: 3ï¸âƒ£ ways I use Large Language Models to increase learning efficiency","url":"https://dev.to/mrzaizai2k/til-3-ways-i-use-large-language-models-to-increase-learning-efficiency-6i5","date":1740103200,"author":"Mai Chi Bao","guid":7544,"unread":true,"content":"<ul><li>To Do List Management System</li><li>Online Courses Summary (In the future)</li></ul><p>This semester, it's a bit odd that I registered for 5 courses for my master degree at HCMUT along with the job at the bank, so It makes me burn out a bit.&nbsp; I've only been taking the courses for a week and I'm already feeling miserable.</p><p>It's not until now that I've tried applying LLMs in work and study, but really, my tight schedule recently forces me to optimize everything and only prioritize things that brings the most important value</p><blockquote><p><em>There is never enough time to do everything, but there is always enough time to do the most important thing.&nbsp; - Brian Tracy.</em></p></blockquote><p>I often use Microsoft To Do because it is simple and convenient.&nbsp; Normally, just call \"Hi Bixby\" to create a task from my Samsung phone.&nbsp; Tasks will be synchronized into the Calendar app.&nbsp; However, it handles Vietnamese poorly, cannot add multiple tasks at the same time, does not automatically set the importance level... Although Google Assistant are better at handling speech to text problem, it has the same limitations as above.</p><p>I decided to go big this time and build my own Telegram chat bot.&nbsp; Every time you send a voice, it will process it automatically:</p><ul><li>Breaking down the text into multiple tasks.</li><li>Set due date, reminder date, important level based on my personal information.</li></ul><p>Very convenient.&nbsp; But the biggest drawback is that I still need to send voice to the app manually, not \"Hi Bixby\" or \"Hey Google\", this makes it lose its practicality.&nbsp; I will find a way to fix this in the future</p><p>To do list is integrated into this tele bot:</p><p>I used Faiss as vector database and Langchain to get text from any sources I could think of (.pdf, .doc, .ppt, .txt, .md, .epub, link, ebook, youtube...).&nbsp; If you ask Chatgpt, it will sometime mislead you or create fake answer. Although the answer might be correct, it doesn't follow the lecturer's documents. </p><p>This RAG system take the answer from our documents to produce results, so the results are more accurate and we can immediately check its correctness.&nbsp; I also tested on the Software Testing Quizz, the result was about 7/10.&nbsp; I also ask chatgpt, but the answer seems bad.</p><p>But Questions with the answer \"All the answers are correct\" are often wrong.&nbsp; Perhaps it's because the model only finds the answer with the highest probability, so the aggregated answers are not fit to this case. Maybe I should rewriting the prompt.&nbsp; Additionally, a small part of the learning resources are images, and I'm actually still looking for ways to optimize the explanation for the images.</p><div><div><div><p>I just wanna build my own LLM with RAG</p><p>Welcome to the my LLM with RAG system! This system is designed for me the ease the learning as a master in HCMUT</p><ol><li><div><pre><code>     curl -X POST http://localhost:8083/update\n</code></pre></div></li><li><p>Ask questions with vector data</p><div><pre><code>     curl -X POST -H \"Content-Type: application/json\" -d '{\"query\": \"who is karger\"}' http://localhost:8083/query\n</code></pre></div></li></ol><div><pre><code>  nougat data/web_data/Growth_of_Functions.pdf --markdown --no-skipping -m 0.1.0-base -o data/nougat\n</code></pre></div><p>Before running the system, follow these steps to set up the environment:</p><ol><li><ul><li>Close the Git repository to your local machine:\n<div><pre>git clone [repository_url]</pre></div></li></ul></li></ol><ul><li><p>Navigate to the project directory and install the required packages using the provided  file:</p></li><li><p>To read  file we need to run this code</p><div><pre><code>  apt update\n  apt install libreoffice \n</code></pre></div></li></ul></div></div></div><p>During the pandemic, I realized that I'm best at&nbsp; learning online courses.&nbsp; I usually watch the video 2 or 3 times, fast forwarding unimportant parts (focusing on the 20% of the time that brings 80% of the value).&nbsp; I plan to record all the lectures, then ask the model to synthesize and track which time period and what topic is being talked about... Or convert it to text format to save in the database for LLM + RAG system</p><p>I am convinced that individuals adept at utilizing LLMs can elevate their performance by up to 200%. To stand out among others, it requires a combination of diligent effort and the right tools. Even a marginal improvement of 1% can contribute significantly to your success.</p>","contentLength":4017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning with PDEs","url":"https://towardsdatascience.com/reinforcement-learning-with-pdes/","date":1740102339,"author":"Robert Etter","guid":7532,"unread":true,"content":"<p>Previously we discussed applying reinforcement learning to Ordinary Differential Equations (ODEs) by integrating ODEs within gymnasium. ODEs are a powerful tool that can describe a wide range of systems but are limited to a single variable. Partial Differential Equations (PDEs) are differential equations involving derivatives of multiple variables that can cover a far broader range and more complex systems. Often, ODEs are special cases or special assumptions applied to PDEs.</p><p>PDEs include Maxwellâ€™s Equations (governing electricity and magnetism), Navier-Stokes equations (governing fluid flow for aircraft, engines, blood, and other cases), and the Boltzman equation for thermodynamics. PDEs can describe systems such as<a href=\"https://www.sciencedirect.com/science/article/pii/0022247X85900435\"> flexible structures</a>,<a href=\"https://dl.acm.org/doi/pdf/10.1145/337292.337359\"> power grids</a>,<a href=\"https://ieeexplore.ieee.org/document/4392493\"> manufacturing</a>, or<a href=\"https://arxiv.org/html/2405.12938v3\"> epidemiological</a> models in biology. They can represent highly complex behavior; the Navier Stokes equations describe the eddies of a rushing mountain stream. Their capacity for capturing and revealing more complex behavior of real-world systems makes these equations an important topic for study, both in terms of describing systems and analyzing known equations to make new discoveries about systems. Entire fields (like fluid dynamics, electrodynamics, structural mechanics) can be devoted to study of just a single set of PDEs.</p><p>This increased complexity comes with a cost; the systems captured by PDEs are much more difficult to analyze and control. ODEs are also described as lumped-parameter systems, the various parameters and variables that describe them are â€œlumpedâ€ into a discrete point (or small number of points for a coupled system of ODEs). PDEs are distributed parameter systems that track behavior throughout space and time. In other words, the state space for an ODE is a relatively small number of variables, such as time and a few system measurements at a specific point. For PDE/distributed parameter systems, the state space size can approach infinite dimensions, or discretized for computation into millions of points .&nbsp;A lumped parameter system controls the temperature of an engine based on a small number of sensors. A PDE/distributed parameter system would manage temperature dynamics across the entire engine.&nbsp;</p><p>As with ODEs, many PDEs must be analyzed (aside from special cases) through modelling and simulation. However, due to the higher dimensions, this modelling becomes far more complex. Many ODEs can be solved through straightforward applications of algorithms like MATLABâ€™s ODE45 or SciPyâ€™s .&nbsp;PDEs are modelled across grids or meshes where the PDE is simplified to an algebraic equation (such as through Taylor Series expansion) at each point on the grid. Grid generation is a field, a science and art, on its own and ideal (or usable) grids can vary greatly based on problem geometry and <a href=\"https://towardsdatascience.com/tag/physics/\" title=\"Physics\">Physics</a>. Grids (and hence problem state spaces) can number in the millions of points with computation time running in days or weeks, and PDE solvers are often commercial software costing tens of thousands of dollars.&nbsp;</p><p>Controlling PDEs presents a far greater challenge than ODEs. The Laplace transform that forms the basis of much classical control theory is a one-dimensional transformation. While there has been some progress in PDE control theory, the field is not as comprehensive as for ODE/lumped systems. For PDEs, even basic controllability or observability assessments become difficult as the state space to assess increases by orders of magnitude and fewer PDEs have analytic solutions. By necessity, we run into design questions such as what part of the domain needs to be controlled or observed? Can the rest of the domain be in an arbitrary state?&nbsp;What subset of the domain does the controller need to operate over? With key tools in control theory underdeveloped, and new problems presented, applying machine learning has been a major area of research for understanding and controlling PDE systems.&nbsp;</p><p>Given the importance of PDEs, there has been research into developing control strategies for them. For example, <a href=\"https://www.amazon.com/Approximate-Controllability-Distributed-Parameter-Systems/dp/0521885728/ref=sr_1_1?crid=3FGUZ2CQYKGYQ&amp;dib=eyJ2IjoiMSJ9.-oRlRZ73sDSnpAcPPbERrs7CUlG0Gr_Aw0V_JYBKkmc.ur8hTlYbid8e3MzQG0RdztW3vwTamw8QFtzQtDCbwHQ&amp;dib_tag=se&amp;keywords=analysis+and+control+of+distributed+parameter+systems+Glowinski&amp;qid=1739749882&amp;sprefix=analysis+and+control+of+distributed+parameter+systems+glowinski+%2Caps%2C88&amp;sr=8-1\">Glowinski et. all</a> developed an analytical adjoint based method from advanced functional analysis relying on simulation of the system. Other approaches, such as discussed by <a href=\"https://uwaterloo.ca/applied-mathematics/sites/default/files/uploads/documents/morris_controlhandbook.pdf\">Kirsten Morris</a>, apply estimations to reduce the order of the PDE to facilitate more traditional control approaches.&nbsp;<a href=\"https://arxiv.org/abs/2403.15267\">Botteghi and Fasel</a>, have begun to apply machine learning to control of these systems (note, this is only a VERY BRIEF glimpse of the research). Here we will apply reinforcement learning on two PDE control problems. The diffusion equation is a simple, linear, second order PDE with known analytic solution. The Kuramotoâ€“Sivashinsky (K-S) equation is a much more complex 4 order nonlinear equation that models instabilities in a flame front.&nbsp;</p><p>For both these equations we use a simple, small square domain of grid points. We target a sinusoidal pattern in a target area of a line down the middle of the domain by controlling input along left and right sides. Input parameters for the controls are the values at the target region and the  coordinates of the input control points. Training the algorithm required modelling the system development through time with the control inputs. As discussed above, this requires a grid where the equation is solved at each point then iterated through each time step. I used the <a href=\"https://github.com/zwicker-group/py-pde\">py-pde package</a> to create a training environment for the reinforcement learner (thanks to the developer of this package for his prompt feedback and help!). With the  environment, approach proceeded as usual with reinforcement learning:&nbsp;the particular algorithm develops a guess at a controller strategy. That controller strategy is applied at small, discrete time steps and provides control inputs based on the current state of the system that lead to some reward (in this case, root mean square difference between target and current distribution).&nbsp;</p><p>Unlike previous cases, I only present results from the <a href=\"https://towardsdatascience.com/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79/\">genetic-programming</a> controller. I developed code to apply a soft actor critic (SAC) algorithm to execute <a href=\"https://github.com/retter-berkeley/DockerPDE_SAC\">as a container on AWS Sagemaker</a>. However, full execution would take about 50 hours and I didnâ€™t want to spend the money! I looked for ways to reduce the computation time, but eventually gave up due to time constraints; this article was already taking long enough to get out with my job, military reserve duty, family visits over the holidays, civic and church involvement, and not leaving my wife to take care of our baby boy alone!</p><p>&nbsp;First we will discuss the diffusion equation:</p><p>with x as a two dimensional cartesian vector and âˆ† <a href=\"https://en.wikipedia.org/wiki/Laplace_operator\">Laplace operator</a></p><pre><code>from pde import Diffusion, CartesianGrid, ScalarField, DiffusionPDE, pde\ngrid = pde.CartesianGrid([[0, 1], [0, 1]], [20, 20], periodic=[False, True])\nstate = ScalarField.random_uniform(grid, 0.0, 0.2)\nbc_left={\"value\": 0}\nbc_right={\"value\": 0}\nbc_x=[bc_left, bc_right]\nbc_y=\"periodic\"\n#bc_x=\"periodic\"\neq = DiffusionPDE(diffusivity=.1, bc=[bc_x, bc_y])\nsolver=pde.ExplicitSolver(eq, scheme=\"euler\", adaptive = True)\n#result = eq.solve(state, t_range=dt, adaptive=True, tracker=None)\nstepper=solver.make_stepper(state, dt=1e-3)\ntarget = 1.*np.sin(2*grid.axes_coords[1]*3.14159265)</code></pre><p>The problem is sensitive to diffusion coefficient and domain size; mismatch between these two results in washing out control inputs before they can reach the target region unless calculated over a long simulation time. The control input was updated and reward evaluated every 0.1 timestep up to an end time of T=15.&nbsp;</p><p>Due to py-pde package architecture, the control is applied to one column inside the boundary. Structuring the py-pde package to execute with the boundary condition updated each time step resulted in a memory leak, and the py-pde developer advised using a stepper function as a work-around that doesnâ€™t allow updating the boundary condition. This means the results arenâ€™t exactly physical, but do display the basic principle of PDE control with reinforcement learning.&nbsp;</p><p>The GP algorithm was able to arrive at a final reward (sum mean square error of all 20 points in the central column) of about 2.0 after about 30 iterations with a 500 tree forest. The results are shown below as target and achieved distributed in the target region.</p><p>Now the more interesting and complex K-S equation:</p><p>Unlike the diffusion equation, the K-S equation displays rich dynamics (as befitting an equation describing flame behavior!). Solutions may include stable equilibria or travelling waves, but with increasing domain size all solutions will eventually become chaotic. The PDE implementation is given by below code:</p><pre><code>grid = pde.CartesianGrid([[0, 10], [0, 10]], [20, 20], periodic=[True, True])\nstate = ScalarField.random_uniform(grid, 0.0, 0.5)\nbc_y=\"periodic\"\nbc_x=\"periodic\"\neq = PDE({\"u\": \"-gradient_squared(u) / 2 - laplace(u + laplace(u))\"}, bc=[bc_x, bc_y])\nsolver=pde.ExplicitSolver(eq, scheme=\"euler\", adaptive = True)\nstepper=solver.make_stepper(state, dt=1e-3)\ntarget=1.*np.sin(0.25*grid.axes_coords[1]*3.14159265)</code></pre><p>Control inputs are capped at +/-5.&nbsp;The K-S equation is naturally unstable; if any point in the domain exceeds +/- 30 the iteration terminates with a large negative reward for causing the system to diverge. Experiments with the K-S equation in  revealed strong sensitivity to domain size and number of grid points. The equation was run for T=35, both with control and reward update at dt=0.1.</p><p>For each, the GP algorithm had more trouble arriving at a solution than in the diffusion equation. I chose to manually stop execution when the solution became visually close; again, we are looking for general principles here. For the more complex system, the controller works betterâ€”likely because of how dynamic the K-S equation is the controller is able to have a bigger impact. However, when evaluating the solution for different run times, I found it was not stable; the algorithm learned to arrive at the target distribution at a particular time, not to stabilize at that solution. The algorithm converged to the below solution, but, as the successive time steps show, the solution is unstable and begins to diverge with increasing time steps.&nbsp;</p><p>Careful tuning on the reward function would help obtain a solution that would hold longer, reinforcing how vital correct reward function is. Also, in all these cases we arenâ€™t coming to perfect solutions; but, especially for the K-S equations we are getting decent solutions with comparatively little effort compared to non-RL approaches for tackling these sorts of problems.</p><p>The GP solution is taking longer to solve with more complex problems and has trouble handling large input variable sets. To use larger input sets, the equations it generates become longer which make it less interpretable and slower to compute.&nbsp;Solution equations had scores of terms rather than the dozen or so in ODE systems. Neural network approaches can handle large input variable sets more easily as input variables only directly impact the size of the input layer.&nbsp;Further, I suspect that neural networks will be able to handle more complex and larger problems better for reasons discussed previously in previous posts. Because of that, I did develop <a href=\"https://github.com/retter-berkeley/PhysicsGyms\">gymnasiums for py-pde diffusion</a>, which can easily be adapted to other PDEs per the <a href=\"https://py-pde.readthedocs.io/en/latest/\">py-pde documentation</a>. These gymnasiums can be used with different NN-based reinforcement learning such as the SAC algorithm I developed (which, as discussed, runs but takes time).&nbsp;</p><p>Adjustments could also be made to the genetic <a href=\"https://towardsdatascience.com/tag/programming/\" title=\"Programming\">Programming</a> approach. For example, vector representation of inputs could reduce size of solution equations. Duriez et al. all proposes using Laplace transform to introduce derivatives and integrals into the genetic programming equations, broadening the function spaces they can explore.&nbsp;</p><p>The ability to tackle more complex problems is important. As discussed above, PDEs can describe a wide range of complex phenomena. Currently, controlling these systems usually means lumping parameters. Doing so leaves out dynamics and so we end up working against such systems rather than with them. Efforts to control or manage these means higher control effort, missed efficiencies, and increased risk of failure (small or catastrophic). Better understanding and control alternatives for PDE systems could unlock major gains in engineering fields where marginal improvements have been the standard such as <a href=\"https://www.researchgate.net/publication/316088790_MODELLING_VEHICLE_TRAFFIC_FLOW_WITH_PARTIAL_DIFFERENTIAL_EQUATIONS\">traffic</a>,<a href=\"https://www.tandfonline.com/doi/full/10.1080/21642583.2015.1033565\"> supply chains</a>, and <a href=\"https://en.wikipedia.org/wiki/Magnetohydrodynamics\">nuclear fusion</a> as these systems behave as high dimensional distributed parameter systems. They are highly complex with nonlinear and emergent phenomena but have large available data setsâ€”ideal for machine learning to move past current barriers in understanding and optimization.&nbsp;</p><p>For now, I have only taken a very basic look at applying ML to controlling PDEs. Follow ons to the control problem include not just different systems, but optimizing where in the domain the control is applied, experimenting with reduced-order observation space, and optimizing the control for simplicity or control effort. In addition to improved control efficiency, as discussed in Brunton and Kutz, machine learning can also be used to derive data-based models of complex physical systems and to determine reduced order models which reduce state space size and may be more amenable to analysis and control, by traditional or machine learning methods. Machine learning and PDEs is an exciting area of research, and I encourage you to see what the professionals are doing!</p>","contentLength":13472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"creating your own Docker like what a shiny title and hard work A year ago, I built a minimal container in Go. Now, it's time for a revisit! This time, I'm tackling network isolation, resource limits, and deeper container architecture.","url":"https://dev.to/micromax/creating-your-own-docker-like-what-a-shiny-title-and-hard-work-a-year-ago-i-built-a-minimal-53fc","date":1740101152,"author":"mohamed alaaeldin","guid":7546,"unread":true,"content":"<h2>Beyond Basics: Building a More Powerful Container in Go â€” Network Isolation &amp; Advanced Features</h2><h3>mohamed alaaeldin ãƒ» Feb 21</h3>","contentLength":125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Basics: Building a More Powerful Container in Go â€” Network Isolation & Advanced Features","url":"https://dev.to/micromax/beyond-basics-building-a-more-powerful-container-in-go-network-isolation-advanced-features-3674","date":1740100901,"author":"mohamed alaaeldin","guid":7518,"unread":true,"content":"<p><em>Containers Uncovered: More Than Just Lightweight Virtual Machines!â€</em></p><blockquote><p>If youâ€™re like me â€” always wondering how things work and eager to build them with your own mind and hands â€” youâ€™re in the right place!\n    In the first part of this <a href=\"https://dev.to/micromax/creating-a-minimal-container-in-go-a-step-by-step-guide-283b\">article (Part 1)</a>, I attempted to build a minimal container system using only Go, relying on Linuxâ€™s unshare and namespaces. It was purely a demonstration, and I wasnâ€™t aiming to develop a fully functional container runtime tool. I intentionally left out many critical aspects, such as security, networking, and image management.\n    I initially thought it would be simple, but I quickly realized that even a basic container system involves thousands of concepts and implementations. However, my passion for understanding and building things kept me going.<p>\n    Now, after a year since my first article on Building a Minimal Container in Go, Iâ€™ve realized that both my code and my original article need a fresh perspective. So, itâ€™s time for a revisit!</p></p></blockquote><ol><li><p>Responsibilities:\nParse user commands (run, exec, ps, rm)<p>\nCommunicate with daemon via RPC or any other way</p>\nFormat and display output</p></li></ol><div><pre><code>Command completion\nOutput formatting (JSON/YAML)\nLog streaming\n</code></pre></div><div><pre><code>Manage container lifecycle\nMaintain container state database\nCoordinate between components\n</code></pre></div><div><pre><code>REST/gRPC API\nEvent logging\nResource tracking\n</code></pre></div><div><pre><code>Namespace Manager: CLONE_NEW* flags handling and more flags in real world .\nCgroups Manager: Resource constraints\nFilesystem Setup: RootFS preparation\n</code></pre></div><div><pre><code>OCI runtime spec compliance\nUser namespace remapping\nSeccomp/AppArmor profiles\n</code></pre></div><div><pre><code>Registry Client: Docker Hub integration or you own images services if you will go wiled\nLayer Manager: OverlayFS/BTRFS\nSnapshotter: Copy-on-write layers\n</code></pre></div><div><pre><code>Image caching\nSignature verification\nGarbage collection\n</code></pre></div><div><pre><code>CNI Plugins: Bridge, MACVLAN, IPVLAN\nIPAM: DHCP/Static allocation\nService Mesh: DNS, service discovery\n</code></pre></div><div><pre><code>Multi-host networking\nNetwork policies\nPort mapping\n</code></pre></div><div><pre><code>Volume Manager: Bind mounts\nSnapshot Manager: Incremental backups\nQuota Enforcer: Disk limits\n</code></pre></div><div><pre><code>Persistent storage\nTemporary filesystems\nEncryption support\n</code></pre></div><p>this schema will give you a bigger picture</p><div><pre><code>\n                           +---------------------+\n                           |      User CLI       |\n                           | (run, exec, ps, rm) |\n                           +----------+----------+\n                                      |\n                                      | (gRPC/HTTP)\n                                      v\n                           +---------------------+\n                           |   Container Daemon  |\n                           | (State Management)  |\n                           +----------+----------+\n                                      |\n                   +------------------+------------------+\n                   |                  |                  |\n         +----------+----------+ +-----+--------+ +-------+---------+\n         |   Container Runtime | | Image Service| | Network Manager |\n         | (namespace/cgroups) | | (OCI Images)  | | (CNI Plugins)   |\n         +----------+----------+ +-----+--------+ +-------+---------+\n                   |                  |                  |\n         +---------v---------+ +------v-------+ +--------v---------+\n         | Linux Kernel       | | Storage Driver| | Host Networking |\n         | - namespaces       | | (OverlayFS)   | | (iptables/bridge)|\n         | - cgroups v2       | +---------------+ +------------------+\n         | - capabilities     |\n         +--------------------+\n</code></pre></div><p>It has been a long journey for me to learn and think through every component. I encountered many challenges, especially with aspects like OverlayFS and networking.</p><p>My biggest issue in my first implementation was networking. It was really difficult to isolate the child container and set up its own bridged network.</p><p>To solve network isolation, you need to think clearly ğŸ¤” at this stage.</p><p>First, you need to create a bridge on the host with two virtual interfaces:</p><div><pre><code>The first interface remains on the host.\nThe second interface is moved to the child container ğŸ«™.\n</code></pre></div><p>The real challenge here is managing command signaling between the host and the child container.</p><p>In my approach, I will attempt to create a proof of concept implementation.\nUnderstanding Container Networking</p><p>When we create containers, one of the most crucial aspects is network isolation. Think of it like giving each container its own private network environment, complete with its own network interfaces, IP addresses, and routing rules. Letâ€™s break down how we achieve this in our container implementation.\nThe Network Setup Process</p><ol><li>Creating the Network Namespace</li></ol><p>First, we create a separate network namespace for our container. This is like giving the container its own private networking room:</p><div><pre><code>const ContainerName = \"mycontainer\"\n\nfunc createNetworkNamespace(name string) error {\n    // Create directory for network namespaces\n    if err := os.MkdirAll(\"/var/run/netns\", 0755); err != nil {\n        return err\n    }\n\n    // Create the namespace file\n    nsFile := filepath.Join(\"/var/run/netns\", name)\n    fd, err := os.Create(nsFile)\n    if err != nil {\n        return err\n    }\n    fd.Close()\n\n    // Bind mount it to make it accessible\n    return syscall.Mount(\"/proc/self/ns/net\", nsFile, \"bind\", syscall.MS_BIND, \"\")\n}\n</code></pre></div><ol><li>Setting Up Virtual Network Interfaces</li></ol><p>We create a virtual network cable (veth pair) to connect our container to the host system:</p><div><pre><code>const (\n    VethHost      = \"veth0\"  // Host end of the cable\n    VethContainer = \"veth1\"  // Container end of the cable\n    ContainerIP   = \"10.0.0.2/24\"\n    HostIP        = \"10.0.0.1/24\"\n    Gateway       = \"10.0.0.1\"\n)\n</code></pre></div><p>The setup happens in two parts:\n1-On the host side:</p><div><pre><code>func setupHostNetwork(pid int) error {\n    // Create the virtual network cable (veth pair)\n    if err := exec.Command(\"ip\", \"link\", \"add\", VethHost, \"type\", \"veth\", \n        \"peer\", \"name\", VethContainer).Run(); err != nil {\n        return fmt.Errorf(\"failed to create veth pair: %v\", err)\n    }\n\n    // Move one end to the container\n    if err := exec.Command(\"ip\", \"link\", \"set\", VethContainer, \n        \"netns\", fmt.Sprintf(\"%d\", pid)).Run(); err != nil {\n        return fmt.Errorf(\"failed to move veth to container: %v\", err)\n    }\n\n    // Configure the host end\n    if err := exec.Command(\"ip\", \"link\", \"set\", VethHost, \"up\").Run(); err != nil {\n        return fmt.Errorf(\"failed to bring up host interface: %v\", err)\n    }\n    if err := exec.Command(\"ip\", \"addr\", \"add\", HostIP, \"dev\", VethHost).Run(); err != nil {\n        return fmt.Errorf(\"failed to assign IP to host interface: %v\", err)\n    }\n}\n</code></pre></div><p>2 â€” Inside the container:</p><div><pre><code>func setupContainerNetwork() error {\n    // Enable the loopback interface\n    if err := exec.Command(\"ip\", \"link\", \"set\", \"lo\", \"up\").Run(); err != nil {\n        return fmt.Errorf(\"failed to bring up lo: %v\", err)\n    }\n\n    // Configure the container's network interface\n    if err := exec.Command(\"ip\", \"link\", \"set\", VethContainer, \"up\").Run(); err != nil {\n        return fmt.Errorf(\"failed to bring up veth: %v\", err)\n    }\n    if err := exec.Command(\"ip\", \"addr\", \"add\", ContainerIP, \n        \"dev\", VethContainer).Run(); err != nil {\n        return fmt.Errorf(\"failed to assign IP to veth: %v\", err)\n    }\n    if err := exec.Command(\"ip\", \"route\", \"add\", \"default\", \n        \"via\", Gateway).Run(); err != nil {\n        return fmt.Errorf(\"failed to add default route: %v\", err)\n    }\n}\n</code></pre></div><p>To allow our container to access the internet, we need to set up NAT (Network Address Translation) rules. This is like setting up a router for our container:</p><div><pre><code>func setupHostNetwork(pid int) error {\n    // Get the host's internet-connected interface\n    iface, err := getDefaultInterface()\n    if err != nil {\n        return fmt.Errorf(\"failed to get default interface: %v\", err)\n    }\n\n    // Set up NAT and forwarding rules\n    cmds := [][]string{\n        {\"sysctl\", \"-w\", \"net.ipv4.ip_forward=1\"},\n        {\"iptables\", \"-t\", \"nat\", \"-A\", \"POSTROUTING\", \n            \"-s\", \"10.0.0.0/24\", \"-o\", iface, \"-j\", \"MASQUERADE\"},\n        {\"iptables\", \"-A\", \"FORWARD\", \"-i\", iface, \n            \"-o\", VethHost, \"-j\", \"ACCEPT\"},\n        {\"iptables\", \"-A\", \"FORWARD\", \"-i\", VethHost, \n            \"-o\", iface, \"-j\", \"ACCEPT\"},\n    }\n\n    for _, cmd := range cmds {\n        if out, err := exec.Command(cmd[0], cmd[1:]...).CombinedOutput(); err != nil {\n            return fmt.Errorf(\"failed %v: %s\\n%s\", cmd, err, out)\n        }\n    }\n}\n</code></pre></div><p><strong>finally , Resource Cleanup</strong></p><p>One often overlooked but crucial aspect is cleaning up network resources when the container stops. Our implementation handles this through a ResourceManager:</p><div><pre><code>\ntype ResourceManager struct {\n    containerName string\n    vethHost      string\n    mounts        []string\n    namespaces    []string\n}\n\nfunc (rm *ResourceManager) cleanupNetwork() error {\n    // Clean up iptables rules\n    if err := rm.cleanupIptablesRules(); err != nil {\n        log.Printf(\"Warning: iptables cleanup failed: %v\", err)\n    }\n\n    // Remove the virtual network interface\n    if out, err := exec.Command(\"ip\", \"link\", \"delete\", \n        rm.vethHost).CombinedOutput(); err != nil {\n        log.Printf(\"Warning: failed to delete veth interface: %v (%s)\", err, out)\n    }\n\n    return nil\n}\n</code></pre></div><p>How It All Works Together</p><div><pre><code>When starting a container:\n\nCreate a new network namespace\nCreate virtual network interfaces (veth pair)\nConfigure IP addresses and routing\nSet up NAT for internet access\nMount necessary filesystems and set up devices\n</code></pre></div><p>2 .During container runtime:</p><div><pre><code>Container uses its virtual network interface for all network communication\nOutgoing traffic goes through NAT to reach the internet\nIncoming traffic is routed back to the container\n</code></pre></div><p>3 . When stopping a container:</p><div><pre><code>Clean up iptables rules\nRemove virtual interfaces\nUnmount network namespace\nRemove namespace files\n</code></pre></div><p>Common Issues and Debugging</p><p>When implementing container networking, you might encounter these common issues:</p><div><pre><code>DNS Resolution Problems\n\nOur implementation includes DNS setup:\n</code></pre></div><div><pre><code>// in most cases this will case error , still trying to solve it \nfunc setupDNS() error {\n    resolvHost := \"/etc/resolv.conf\"\n    resolvContainer := filepath.Join(RootFS, \"etc/resolv.conf\")\n    return syscall.Mount(resolvHost, resolvContainer, \"bind\", \n        syscall.MS_BIND|syscall.MS_RDONLY, \"\")\n}\n</code></pre></div><p>2.Network Interface Issues</p><div><pre><code>Always check interface status with ip link show\nVerify IP assignments with ip addr show\nCheck routing with ip route show\n</code></pre></div><div><pre><code>Verify iptables rules are correctly set\nCheck IP forwarding is enabled\nEnsure the host interface is up and working\n</code></pre></div><p>Our implementation includes several security features:</p><div><pre><code>Network Isolation\n\nEach container gets its own network namespace\nNetwork traffic is isolated between containers\n</code></pre></div><div><pre><code>Proper cleanup of network resources prevents resource leaks\nAutomatic cleanup on container exit\n</code></pre></div><p>This networking implementation provides a solid foundation for container isolation while maintaining internet connectivity. While itâ€™s simpler than production container runtimes, it demonstrates the core concepts of container networking.</p><blockquote><p>this was the hard part for me and i have tryed so many implemention to achive that . you have to keep in main what and where your command executted . some times you find your self trying to create vathâ€™s in continer or you cannot connect or move the continer interface from host to chiled</p></blockquote><p>you have to read my previeus articl to know what we are doing i had clean up my code and add every thing agine to test network isolation</p><p>do not forget to change RootFS to your root fs like â€œubuntu or whatever image you will runâ€</p><div><pre><code>package main\n\nimport (\n \"fmt\"\n \"log\"\n \"os\"\n \"os/exec\"\n \"path/filepath\"\n \"strings\"\n \"syscall\"\n \"os/signal\" \n)\n\nconst (\n ContainerName = \"mycontainer\"\n VethHost      = \"veth0\"\n VethContainer = \"veth1\"\n ContainerIP   = \"10.0.0.2/24\"\n HostIP        = \"10.0.0.1/24\"\n Gateway       = \"10.0.0.1\"\n RootFS        = \"/mnt/drive/go-projects/lc-images-regs/ubuntu_fs\"\n)\n\n\n\ntype ResourceManager struct {\n    containerName string\n    vethHost      string\n    mounts        []string\n    namespaces    []string\n}\nfunc NewResourceManager(containerName string) *ResourceManager {\n    return &amp;ResourceManager{\n        containerName: containerName,\n        vethHost:     VethHost,\n        mounts: []string{\n            \"/proc\",\n            \"/dev/pts\",\n            \"/dev\",\n        },\n        namespaces: []string{\n            \"net\",\n            \"uts\",\n            \"pid\",\n            \"ipc\",\n        },\n    }\n}\n\nfunc (rm *ResourceManager) Setup() {\n    // Set up signal handling\n    sigChan := make(chan os.Signal, 1)\n    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)\n\n    go func() {\n        sig := &lt;-sigChan\n        log.Printf(\"Received signal %v, cleaning up...\", sig)\n        rm.Cleanup()\n        os.Exit(1)\n    }()\n}\n\nfunc (rm *ResourceManager) Cleanup() error {\n    var errors []string\n\n    // 1. Clean up network resources\n    if err := rm.cleanupNetwork(); err != nil {\n        errors = append(errors, fmt.Sprintf(\"network cleanup error: %v\", err))\n    }\n\n    // 2. Clean up mounts\n    if err := rm.cleanupMounts(); err != nil {\n        errors = append(errors, fmt.Sprintf(\"mount cleanup error: %v\", err))\n    }\n\n    // 3. Clean up namespaces\n    if err := rm.cleanupNamespaces(); err != nil {\n        errors = append(errors, fmt.Sprintf(\"namespace cleanup error: %v\", err))\n    }\n\n    if len(errors) &gt; 0 {\n        return fmt.Errorf(\"cleanup errors: %s\", strings.Join(errors, \"; \"))\n    }\n    return nil\n}\n\nfunc (rm *ResourceManager) cleanupNetwork() error {\n    // Clean up iptables rules first\n    if err := rm.cleanupIptablesRules(); err != nil {\n        log.Printf(\"Warning: iptables cleanup failed: %v\", err)\n    }\n\n    // Clean up veth interfaces\n    if out, err := exec.Command(\"ip\", \"link\", \"delete\", rm.vethHost).CombinedOutput(); err != nil {\n        log.Printf(\"Warning: failed to delete veth interface: %v (%s)\", err, out)\n    }\n\n    return nil\n}\n\nfunc (rm *ResourceManager) cleanupIptablesRules() error {\n    iface, err := getDefaultInterface()\n    if err != nil {\n        return fmt.Errorf(\"failed to get default interface: %v\", err)\n    }\n\n    rules := [][]string{\n        {\"iptables\", \"-D\", \"FORWARD\", \"-i\", iface, \"-o\", rm.vethHost, \"-j\", \"ACCEPT\"},\n        {\"iptables\", \"-D\", \"FORWARD\", \"-i\", rm.vethHost, \"-o\", iface, \"-j\", \"ACCEPT\"},\n        {\"iptables\", \"-t\", \"nat\", \"-D\", \"POSTROUTING\", \"-s\", \"10.0.0.0/24\", \"-o\", iface, \"-j\", \"MASQUERADE\"},\n    }\n\n    for _, rule := range rules {\n        if out, err := exec.Command(rule[0], rule[1:]...).CombinedOutput(); err != nil {\n            log.Printf(\"Warning: failed to remove iptables rule: %v (%s)\", err, out)\n        }\n    }\n\n    return nil\n}\n\nfunc (rm *ResourceManager) cleanupMounts() error {\n    for _, mount := range rm.mounts {\n        mountPath := filepath.Join(RootFS, mount)\n        if err := syscall.Unmount(mountPath, syscall.MNT_DETACH); err != nil {\n            log.Printf(\"Warning: failed to unmount %s: %v\", mountPath, err)\n        }\n    }\n    return nil\n}\n\nfunc (rm *ResourceManager) cleanupNamespaces() error {\n    for _, ns := range rm.namespaces {\n        nsPath := filepath.Join(\"/var/run/netns\", rm.containerName)\n        if err := syscall.Unmount(nsPath, syscall.MNT_DETACH); err != nil {\n            log.Printf(\"Warning: failed to unmount namespace %s: %v\", ns, err)\n        }\n        if err := os.Remove(nsPath); err != nil {\n            log.Printf(\"Warning: failed to remove namespace file %s: %v\", nsPath, err)\n        }\n    }\n    return nil\n}\n\nfunc cleanupExistingResources() error {\n // Cleanup network namespace\n if _, err := os.Stat(\"/var/run/netns/\" + ContainerName); err == nil {\n  if err := cleanupNetworkNamespace(ContainerName); err != nil {\n   return fmt.Errorf(\"failed to cleanup existing network namespace: %v\", err)\n  }\n }\n\n // Cleanup veth interfaces\n if _, err := exec.Command(\"ip\", \"link\", \"show\", VethHost).CombinedOutput(); err == nil {\n  if err := exec.Command(\"ip\", \"link\", \"delete\", VethHost).Run(); err != nil {\n   return fmt.Errorf(\"failed to delete existing veth interface: %v\", err)\n  }\n }\n\n // Cleanup iptables rules\n if err := cleanupIptablesRules(); err != nil {\n  return fmt.Errorf(\"failed to cleanup iptables rules: %v\", err)\n }\n\n return nil\n}\n\nfunc cleanupIptablesRules() error {\n iface, err := getDefaultInterface()\n if err != nil {\n  return fmt.Errorf(\"failed to get default interface: %v\", err)\n }\n\n cmds := [][]string{\n  {\"iptables\", \"-D\", \"FORWARD\", \"-i\", iface, \"-o\", VethHost, \"-j\", \"ACCEPT\"},\n  {\"iptables\", \"-D\", \"FORWARD\", \"-i\", VethHost, \"-o\", iface, \"-j\", \"ACCEPT\"},\n  {\"iptables\", \"-t\", \"nat\", \"-D\", \"POSTROUTING\", \"-s\", \"10.0.0.0/24\", \"-o\", iface, \"-j\", \"MASQUERADE\"},\n }\n\n for _, cmd := range cmds {\n  // Ignore errors since rules might not exist\n  exec.Command(cmd[0], cmd[1:]...).Run()\n }\n\n return nil\n}\nfunc getDefaultInterface() (string, error) {\n out, err := exec.Command(\"ip\", \"route\", \"show\", \"default\").CombinedOutput()\n if err != nil {\n  return \"\", err\n }\n\n fields := strings.Fields(string(out))\n for i, field := range fields {\n  if field == \"dev\" &amp;&amp; i+1 &lt; len(fields) {\n   return fields[i+1], nil\n  }\n }\n\n return \"\", fmt.Errorf(\"no default interface found\")\n}\n\nfunc main() {\n if len(os.Args) &lt; 2 {\n  log.Fatal(\"Usage: [run|child] command [args...]\")\n }\n\n switch os.Args[1] {\n case \"run\":\n  run()\n case \"child\":\n  child()\n default:\n  log.Fatalf(\"unknown command: %s\", os.Args[1])\n }\n}\nfunc setupCgroups(ContainerName string , pid int, cpuShares, memoryLimitMB int) error {\n    cgroupBase := \"/sys/fs/cgroup\"\n    containerID := ContainerName // fmt.Sprintf(\"container_%d\", pid)\n\n    // Create CPU cgroup\n    cpuPath := filepath.Join(cgroupBase, \"cpu\", containerID)\n    os.MkdirAll(cpuPath, 0755)\n    os.WriteFile(filepath.Join(cpuPath, \"cpu.shares\"), []byte(fmt.Sprintf(\"%d\", cpuShares)), 0644)\n    os.WriteFile(filepath.Join(cpuPath, \"tasks\"), []byte(fmt.Sprintf(\"%d\", pid)), 0644)\n\n    // Create memory cgroup\n    memoryPath := filepath.Join(cgroupBase, \"memory\", containerID)\n    os.MkdirAll(memoryPath, 0755)\n    os.WriteFile(filepath.Join(memoryPath, \"memory.limit_in_bytes\"), []byte(fmt.Sprintf(\"%d\", memoryLimitMB*1024*1024)), 0644)\n    os.WriteFile(filepath.Join(memoryPath, \"tasks\"), []byte(fmt.Sprintf(\"%d\", pid)), 0644)\n\n\n uidMap := fmt.Sprintf(\"0 %d 1\", os.Getuid())\n gidMap := fmt.Sprintf(\"0 %d 1\", os.Getgid())\n\n os.WriteFile(fmt.Sprintf(\"/proc/%d/uid_map\", pid), []byte(uidMap), 0644)\n os.WriteFile(fmt.Sprintf(\"/proc/%d/gid_map\", pid), []byte(gidMap), 0644)\n    return nil\n}\nfunc run() {\n rm := NewResourceManager(ContainerName)\n    rm.Setup()\n    defer rm.Cleanup()\n\n if err := cleanupExistingResources(); err != nil {\n  log.Printf(\"Cleanup warning: %v\", err)\n }\n\n // Create network namespace\n if err := createNetworkNamespace(ContainerName); err != nil {\n  log.Fatalf(\"Failed to create network namespace: %v\", err)\n }\n\n // Start container process\n cmd := exec.Command(\"/proc/self/exe\", append([]string{\"child\"}, os.Args[2:]...)...)\n cmd.Stdin = os.Stdin\n cmd.Stdout = os.Stdout\n cmd.Stderr = os.Stderr\n cmd.SysProcAttr = &amp;syscall.SysProcAttr{\n  Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWNET |\n\n  syscall.CLONE_NEWIPC ,\n\n\n }\n\n if err := cmd.Start(); err != nil {\n  log.Fatalf(\"Failed to start container: %v\", err)\n }\n pid := cmd.Process.Pid\n if err :=setupCgroups(ContainerName , pid , 512 , 256  ); err != nil { // Example: 512 CPU shares, 256 MB memory limit\n  log.Fatalf(\"Failed to setup cgroups: %v\", err)\n }\n // Configure host-side networking\n if err := setupHostNetwork(cmd.Process.Pid); err != nil {\n  log.Fatalf(\"Failed to setup host network: %v\", err)\n }\n\n // Wait for container to exit\n if err := cmd.Wait(); err != nil {\n  log.Fatalf(\"Container failed: %v\", err)\n }\n\n // Cleanup\n if err := cleanupNetworkNamespace(ContainerName); err != nil {\n  log.Printf(\"Failed to cleanup network namespace: %v\", err)\n }\n}\n\nfunc child() {\n // Setup container environment\n if err := setupContainer(); err != nil {\n  log.Fatalf(\"Failed to setup container: %v\", err)\n }\n\n // Execute command\n if len(os.Args) &lt; 3 {\n  log.Fatal(\"No command specified\")\n }\n cmd := exec.Command(os.Args[2], os.Args[3:]...)\n cmd.Stdin = os.Stdin\n cmd.Stdout = os.Stdout\n cmd.Stderr = os.Stderr\n if err := cmd.Run(); err != nil {\n  log.Fatalf(\"Command failed: %v\", err)\n }\n}\n\nfunc createNetworkNamespace(name string) error {\n // Create bind mount for ip netns compatibility\n if err := os.MkdirAll(\"/var/run/netns\", 0755); err != nil {\n  return err\n }\n\n // Create namespace file\n nsFile := filepath.Join(\"/var/run/netns\", name)\n fd, err := os.Create(nsFile)\n if err != nil {\n  return err\n }\n fd.Close()\n\n // Bind mount\n return syscall.Mount(\"/proc/self/ns/net\", nsFile, \"bind\", syscall.MS_BIND, \"\")\n}\n\nfunc cleanupNetworkNamespace(name string) error {\n nsFile := filepath.Join(\"/var/run/netns\", name)\n if err := syscall.Unmount(nsFile, 0); err != nil {\n  return fmt.Errorf(\"failed to unmount network namespace: %v\", err)\n }\n // Remove the file to complete cleanup.\n if err := os.Remove(nsFile); err != nil {\n  return fmt.Errorf(\"failed to remove namespace file %s: %v\", nsFile, err)\n }\n return nil\n}\n\n\nfunc setupHostNetwork(pid int) error {\n // Get host's default interface\n iface, err := getDefaultInterface()\n if err != nil {\n  return fmt.Errorf(\"failed to get default interface: %v\", err)\n }\n\n // Create veth pair\n if err := exec.Command(\"ip\", \"link\", \"add\", VethHost, \"type\", \"veth\", \"peer\", \"name\", VethContainer).Run(); err != nil {\n  return fmt.Errorf(\"failed to create veth pair: %v\", err)\n }\n\n // Move container end to namespace\n if err := exec.Command(\"ip\", \"link\", \"set\", VethContainer, \"netns\", fmt.Sprintf(\"%d\", pid)).Run(); err != nil {\n  return fmt.Errorf(\"failed to move veth to container: %v\", err)\n }\n\n // Configure host interface\n if err := exec.Command(\"ip\", \"link\", \"set\", VethHost, \"up\").Run(); err != nil {\n  return fmt.Errorf(\"failed to bring up host interface: %v\", err)\n }\n if err := exec.Command(\"ip\", \"addr\", \"add\", HostIP, \"dev\", VethHost).Run(); err != nil {\n  return fmt.Errorf(\"failed to assign IP to host interface: %v\", err)\n }\n\n cmds := [][]string{\n\n  {\"sysctl\", \"-w\", \"net.ipv4.ip_forward=1\"},\n  {\"iptables\", \"-t\", \"nat\", \"-A\", \"POSTROUTING\", \"-s\", \"10.0.0.0/24\", \"-o\", iface, \"-j\", \"MASQUERADE\"},\n  {\"iptables\", \"-A\", \"FORWARD\", \"-i\", iface, \"-o\", VethHost, \"-j\", \"ACCEPT\"},\n  {\"iptables\", \"-A\", \"FORWARD\", \"-i\", VethHost, \"-o\", iface, \"-j\", \"ACCEPT\"},\n }\n\n for _, cmd := range cmds {\n  if out, err := exec.Command(cmd[0], cmd[1:]...).CombinedOutput(); err != nil {\n   return fmt.Errorf(\"failed %v: %s\\n%s\", cmd, err, out)\n  }\n }\n\n return nil\n}\n\nfunc setupContainer() error {\n // Setup root filesystem\n if err := syscall.Chroot(RootFS); err != nil {\n  return fmt.Errorf(\"chroot failed: %v\", err)\n }\n if err := os.Chdir(\"/\"); err != nil {\n  return fmt.Errorf(\"chdir failed: %v\", err)\n }\n\n // Mount proc\n if err := syscall.Mount(\"proc\", \"/proc\", \"proc\", 0, \"\"); err != nil {\n  return fmt.Errorf(\"failed to mount proc: %v\", err)\n }\n\n // Setup devices\n if err := setupDevices(); err != nil {\n  return fmt.Errorf(\"failed to setup devices: %v\", err)\n }\n\n // Configure network\n if err := setupContainerNetwork(); err != nil {\n  return fmt.Errorf(\"failed to setup network: %v\", err)\n }\n\n //if err := setupDNS(); err != nil {\n // return fmt.Errorf(\"DNS setup failed: %v\", err)\n //}\n\n return nil\n}\n\nfunc setupDNS() error {\n // Copy host's resolv.conf\n resolvHost := \"/etc/resolv.conf\"\n resolvContainer := filepath.Join(RootFS, \"etc/resolv.conf\")\n\n // Create container's /etc if missing\n if err := os.MkdirAll(filepath.Join(RootFS, \"etc\"), 0755); err != nil {\n  return err\n }\n\n // Bind mount host's resolv.conf\n return syscall.Mount(resolvHost, resolvContainer, \"bind\", syscall.MS_BIND|syscall.MS_RDONLY, \"\")\n}\n\nfunc setupDevices() error {\n // Mount tmpfs for /dev\n if err := syscall.Mount(\"tmpfs\", \"/dev\", \"tmpfs\", 0, \"size=64k,mode=755\"); err != nil {\n  return err\n }\n\n // Create /dev/pts directory if missing\n devPts := \"/dev/pts\"\n if err := os.MkdirAll(devPts, 0755); err != nil {\n  return fmt.Errorf(\"mkdir %s failed: %v\", devPts, err)\n }\n\n // Mount devpts on /dev/pts for pty support\n if err := syscall.Mount(\"devpts\", devPts, \"devpts\", 0, \"mode=0620,ptmxmode=0666\"); err != nil {\n  return fmt.Errorf(\"failed to mount devpts on %s: %v\", devPts, err)\n }\n // Create basic devices\n devices := []struct {\n  name  string\n  major uint32\n  minor uint32\n }{\n  {\"null\", 1, 3},\n  {\"zero\", 1, 5},\n  {\"random\", 1, 8},\n  {\"urandom\", 1, 9},\n }\n\n for _, dev := range devices {\n  path := filepath.Join(\"/dev\", dev.name)\n  if err := syscall.Mknod(path, syscall.S_IFCHR|0666, int(makedev(dev.major, dev.minor))); err != nil {\n   return err\n  }\n }\n\n return nil\n}\n\nfunc makedev(major, minor uint32) uint64 {\n return (uint64(major) &lt;&lt; 8) | uint64(minor)\n}\n\nfunc setupContainerNetwork() error {\n // Bring up loopback\n if err := exec.Command(\"ip\", \"link\", \"set\", \"lo\", \"up\").Run(); err != nil {\n  return fmt.Errorf(\"failed to bring up lo: %v\", err)\n }\n\n // Configure veth interface\n if err := exec.Command(\"ip\", \"link\", \"set\", VethContainer, \"up\").Run(); err != nil {\n  return fmt.Errorf(\"failed to bring up veth: %v\", err)\n }\n if err := exec.Command(\"ip\", \"addr\", \"add\", ContainerIP, \"dev\", VethContainer).Run(); err != nil {\n  return fmt.Errorf(\"failed to assign IP to veth: %v\", err)\n }\n if err := exec.Command(\"ip\", \"route\", \"add\", \"default\", \"via\", Gateway).Run(); err != nil {\n  return fmt.Errorf(\"failed to add default route: %v\", err)\n }\n\n return nil\n}\n</code></pre></div><p>Important point: You must mount and create essential virtual devices and establish communication (such as pipes or signals) between the host and child container .</p><div><pre><code>func setupDevices() error {\n // Mount tmpfs for /dev\n if err := syscall.Mount(\"tmpfs\", \"/dev\", \"tmpfs\", 0, \"size=64k,mode=755\"); err != nil {\n  return err\n }\n\n // Create /dev/pts directory if missing\n devPts := \"/dev/pts\"\n if err := os.MkdirAll(devPts, 0755); err != nil {\n  return fmt.Errorf(\"mkdir %s failed: %v\", devPts, err)\n }\n\n // Mount devpts on /dev/pts for pty support\n if err := syscall.Mount(\"devpts\", devPts, \"devpts\", 0, \"mode=0620,ptmxmode=0666\"); err != nil {\n  return fmt.Errorf(\"failed to mount devpts on %s: %v\", devPts, err)\n }\n // Create basic devices\n devices := []struct {\n  name  string\n  major uint32\n  minor uint32\n }{\n  {\"null\", 1, 3},\n  {\"zero\", 1, 5},\n  {\"random\", 1, 8},\n  {\"urandom\", 1, 9},\n }\n\n for _, dev := range devices {\n  path := filepath.Join(\"/dev\", dev.name)\n  if err := syscall.Mknod(path, syscall.S_IFCHR|0666, int(makedev(dev.major, dev.minor))); err != nil {\n   return err\n  }\n }\n\n return nil\n}\n</code></pre></div><div><pre><code>func NewResourceManager(containerName string) *ResourceManager {\n    return &amp;ResourceManager{\n        containerName: containerName,\n        vethHost:     VethHost,\n        mounts: []string{\n            \"/proc\",\n            \"/dev/pts\",\n            \"/dev\",\n        },\n        namespaces: []string{\n            \"net\",\n            \"uts\",\n            \"pid\",\n            \"ipc\",\n        },\n    }\n}\n\nfunc (rm *ResourceManager) Setup() {\n    // Set up signal handling\n    sigChan := make(chan os.Signal, 1)\n    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)\n\n    go func() {\n        sig := &lt;-sigChan\n        log.Printf(\"Received signal %v, cleaning up...\", sig)\n        rm.Cleanup()\n        os.Exit(1)\n    }()\n}\n</code></pre></div><p>Now you have a broad overview, but you still have a long journey ahead to achieve what production-ready container runtime systems offer.</p><p>If you need system file images to test your code, you can use Docker to download one.</p><div><pre><code>$ docker run -d --rm --name ubuntu_fs ubuntu:20.04 sleep 1000\n$ mkdir -p ./ubuntu_fs\n$ docker cp ubuntu_fs:/ ./ubuntu_fs\n$ docker stop ubuntu_fs\n</code></pre></div><p>Or use tool like debootstrap</p><div><pre><code>sudo apt-get update\nsudo apt-get install debootstrap\nsudo mkdir -p /path/to/rootfs\nsudo debootstrap stable /path/to/rootfs http://deb.debian.org/debian\n</code></pre></div><p>Sometimes, while testing, you may need to install software in your container image from the host if your child container struggles to access the internet.</p><div><pre><code>sudo chroot /path/to/rootfs /bin/sh -c \"apk add --no-cache iproute2\"\n</code></pre></div><div><pre><code>sudo chroot /mnt/drive/go-projects/lc-images-regs/ubuntu_fs /bin/sh -c \"apt-get update &amp;&amp; apt-get install -y iproute2\"\n</code></pre></div><p>Note: Sometimes, when you try to start the container by running the following command to start Bash as the entry point, you may encounter a bug:</p><div><pre><code>sudo go run Main.go run sudo /bin/bash\n</code></pre></div><div><pre><code>2025/02/21 00:26:28 Failed to setup container: failed to setup network: failed to bring up veth: exit status 1\n2025/02/21 01:26:28 Failed to setup host network: failed to assign IP to host interface: exit status 1\nexit status 1\n\n\n</code></pre></div><p>This happens due to resource cleanup errors. You can either ignore it and retry the command up to three times or fix the issue.</p><p>You still need to implement DNS to align with the original system design. What we built is just a proof of concept application.</p><p>My next step is to ensure resource limitations and create an image composer like Docker while utilizing OverlayFS. Until then, if you need any help, feel free to DM me.</p><p>this is discord channel for this topic only join me :</p>","contentLength":29110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating a Minimal Container in Go: A Step-by-Step Guide ( part 1 )","url":"https://dev.to/micromax/creating-a-minimal-container-in-go-a-step-by-step-guide-283b","date":1740100117,"author":"mohamed alaaeldin","guid":7517,"unread":true,"content":"<p><strong>What is Containers any way!</strong>\nContainers are lightweight, portable, and efficient, making them a popular choice for deploying and running applications. In this tutorial, weâ€™ll guide you through the process of creating a minimal container using Go. The example code provided focuses on essential containerization concepts, including namespaces, chroot, and control groups (cgroups).</p><p>Before getting started, ensure you have the following installed:</p><div><pre><code>Go programming language: Install Go\nBasic understanding of Linux namespaces and control groups\n</code></pre></div><p>introduction\nSo what is Linux namespaces and control groups ?</p><p>Namespaces have been part of the Linux kernel since about 2002, and over time more tooling and namespace types have been added. Real container support was added to the Linux kernel only in 2013, however. This is what made namespaces really useful and brought them to the masses.</p><p>But what are namespaces exactly? Hereâ€™s a wordy definition from Wikipedia:</p><p>â€œNamespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources.â€</p><p>In other words, the key feature of namespaces is that they isolate processes from each other. On a server where you are running many different services, isolating each service and its associated processes from other services means that there is a smaller blast radius for changes, as well as a smaller footprint for securityâ€‘related concerns. Mostly though, isolating services meets the architectural style of microservices as described by Martin Fowler.\nTypes of Namespaces</p><p>Within the Linux kernel, there are different types of namespaces. Each namespace has its own unique properties:</p><div><pre><code>A user namespace has its own set of user IDs and group IDs for assignment to processes. In particular, this means that a process can have root privilege within its user namespace without having it in other user namespaces.\nA process ID (PID) namespace assigns a set of PIDs to processes that are independent from the set of PIDs in other namespaces. The first process created in a new namespace has PID 1 and child processes are assigned subsequent PIDs. If a child process is created with its own PID namespace, it has PID 1 in that namespace as well as its PID in the parent processâ€™ namespace. See below for an example.\nA network namespace has an independent network stack: its own private routing table, set of IP addresses, socket listing, connection tracking table, firewall, and other networkâ€‘related resources.\nA mount namespace has an independent list of mount points seen by the processes in the namespace. This means that you can mount and unmount filesystems in a mount namespace without affecting the host filesystem.\nAn interprocess communication (IPC) namespace has its own IPC resources, for example POSIX message queues.\nA UNIX Timeâ€‘Sharing (UTS) namespace allows a single system to appear to have different host and domain names to different processes.\n\nthe container are fast isolated environment , we will focus on this part many things are involved and my main goal is to Demystifying Containers\n\nassuming that you are on a linux machine (try Power shell Ubuntu image if you are on Windows :-)\n</code></pre></div><div><pre><code>host-machine $ id\n\nuid=1000(mohamed) gid=1000(mohamed) groups=1000(mohamed) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c.1023\n</code></pre></div><p>Now I run the following unshare command to create a new namespace with its own user and PID namespaces. I map the root user to the new namespace (in other words, I have root privilege within the new namespace), mount a new proc filesystem, and fork my process (in this case, bash) in the newly created namespace.</p><p>unshare --user --pid --map-root-user --mount-proc --fork bash</p><p>Congratulation , you are in isolated name space and some how you are on\nisolated PID in same file system and same network , your entry point /bin/bash</p><p>The ps -ef command shows there are two processes running â€“ bash and the ps command itself â€“ and the id command confirms that Iâ€™m root in the new namespace (which is also indicated by the changed command prompt):</p><div><pre><code>root # ps -ef\nUID         PID     PPID  C STIME TTY        TIME CMD\nroot          1        0  0 14:46 pts/0  00:00:00 bash\nroot         15        1  0 14:46 pts/0  00:00:00 ps -ef\nroot # id\nuid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c.1023\n</code></pre></div><p><strong>Namespaces and Containers</strong></p><blockquote><p>Namespaces are one of the technologies that containers are built on, used to enforce segregation of resources. Weâ€™ve shown how to create namespaces manually, but container runtimes like Docker, rkt, podman , runC , containerD , and many other container technology\n    one of most unique projects are <a href=\"https://katacontainers.io/\" rel=\"noopener noreferrer\">https://katacontainers.io/</a> they claim that they are mix between container and VMâ€™s .\nWhat Are cgroups?</p></blockquote><p>cgroups, or control groups, are a Linux kernel feature that enables the management and limitation of system resources like CPU, memory, and network bandwidth, among others. We can use cgroups to set limits on these resources and distribute them among different groups of processes.</p><p>cgroups have a hierarchical structure with root and child, each with resource limits set by controllers â€” for example, a CPU controller for CPU time or a memory controller for memory.</p><p>We can use cgroups for various purposes, such as controlling resource usage in a multi-tenant environment, providing Quality of Service (QoS) guarantees, and running containers.</p><p>Cgroups provide the following features:</p><div><pre><code>Resource limits â€” You can configure a cgroup to limit how much of a particular resource (memory or CPU, for example) a process can use.\nPrioritization â€” You can control how much of a resource (CPU, disk, or network) a process can use compared to processes in another cgroup when there is resource contention.\nAccounting â€” Resource limits are monitored and reported at the cgroup level.\nControl â€” You can change the status (frozen, stopped, or restarted) of all processes in a cgroup with a single command.\n</code></pre></div><p>The following command creates a v1 cgroup (you can tell by pathname format) called foo and sets the memory limit for it to 50,000,000 bytes (50 MB).</p><div><pre><code>root # mkdir -p /sys/fs/cgroup/memory/foo\nroot # echo 50000000 &gt; /sys/fs/cgroup/memory/foo/memory.limit_in_bytes\n</code></pre></div><p>Now I can assign a process to the cgroup, thus imposing the cgroupâ€™s memory limit on it. Iâ€™ve written a shell script called test.sh, which prints cgroup testing tool to the screen, and then waits doing nothing. For my purposes, it is a process that continues to run until I stop it.</p><p>I start test.sh in the background and its PID is reported as 2428. The script produces its output and then I assign the process to the cgroup by piping its PID into the cgroup file /sys/fs/cgroup/memory/foo/cgroup.procs.</p><div><pre><code>root # ./test.sh &amp;\n[1] 2428\nroot # cgroup testing tool\nroot # echo 2428 &gt; /sys/fs/cgroup/memory/foo/cgroup.procs\n</code></pre></div><p>To validate that my process is in fact subject to the memory limits that I defined for cgroup foo, I run the following ps command. The -o cgroup flag displays the cgroups to which the specified process (2428) belongs. The output confirms that its memory cgroup is foo.</p><div><pre><code>root # ps -o cgroup 2428\nCGROUP\n12:pids:/user.slice/user-0.slice/\\\nsession-13.scope,10:devices:/user.slice,6:memory:/foo,...\n\n</code></pre></div><p>By default, the operating system terminates a process when it exceeds a resource limit defined by its cgroup.</p><p>and this fair amount of information about namespace and cgroup\nyou can read full doc about it by Scott van Kalken of F5<p>\nat this link , also this post Demystifying Containers 101 and this one focus on Docker ecosystem â€œA Beginner-Friendly Introduction to Containers, VMs and Dockerâ€</p>\npart 1 : Chroot<p>\ni will not use Namespaces , â€œat this partâ€</p></p><p>this may surprise however i will achieve the isolation , we will use Chroot a simple UNIX tool</p><p>chroot, short for \"change root,\" is a Unix system call that changes the root directory of a process to a specified path, effectively creating a new root filesystem for the process and its children. This can be a powerful tool for creating isolated environments or \"chroot jails.\"\nHow Chroot Works:</p><div><pre><code>Setting a New Root Directory: When you execute the chroot system call or the chroot command in the shell, it changes the root directory for the process and its children. The new root directory becomes the / (root) directory for that process, isolating it from the actual root directory of the host system.\nIsolation: After the chroot operation, the process and its children can only access files and directories within the new root directory. They cannot access files outside this new root, providing a level of isolation and containment.\n</code></pre></div><div><pre><code>System Recovery: chroot is commonly used in system recovery scenarios. If your system becomes unbootable or experiences issues, you can boot from a live CD/USB, chroot into the broken system, and make necessary repairs without affecting the rest of the host system.\nEnvironment Isolation: Developers and system administrators may use chroot to create isolated environments for testing or building software. This is especially common in scenarios where different versions of libraries or dependencies are required.\nSecurity: Although chroot provides some level of isolation, it's not foolproof in terms of security. It was not designed as a security feature and should not be solely relied upon for containing malicious processes. Modern containerization technologies, like Docker, utilize more advanced mechanisms, such as Linux namespaces and cgroups, to provide stronger isolation.\n</code></pre></div><p>Consider the following example:</p><div><pre><code>\nmkdir mychroot\ncp -r /bin /lib /lib64 /usr /mychroot\nchroot /mychroot /bin/bash\n</code></pre></div><div><pre><code>We create a directory called mychroot and copy essential binaries and libraries into it.\nWe use chroot to change the root directory to /mychroot.\nAfter the chroot command, executing /bin/bash will run a Bash shell within the isolated environment.\n</code></pre></div><p>Keep in mind that chroot by itself does not provide complete isolation; it is often used in conjunction with other tools and techniques to create more secure and robust containerized environments.\nPrepare the Ubuntu Root Filesystem</p><p>now final this you will need before you start a filesystem .\nwe will use Docker to download Ubuntu filesystem</p><p>you will only need docker to download it , in your project root</p><div><pre><code>$ docker run -d --rm --name ubuntu_fs ubuntu:20.04 sleep 1000\n$ mkdir -p ./ubuntu_fs\n$ docker cp ubuntu_fs:/ ./ubuntu_fs\n$ docker stop ubuntu_fs\n</code></pre></div><p>now we have ubuntu_fs inside our project , inside your main package</p><div><pre><code>package main\n\nimport (\n \"io/ioutil\"\n \"log\"\n \"os\"\n \"os/exec\"\n \"path/filepath\"\n \"strconv\"\n \"syscall\"\n \"strings\"\n \"fmt\"\n \"github.com/vishvananda/netns\"\n\n)\n\n\n\nfunc main() {\n switch os.Args[1] {\n case \"run\":\n  run(os.Args[2:]...)\n case \"child\":\n  child(os.Args[2:]...)\n default:\n  log.Fatal(\"Unknown command. Use run &lt;command_name&gt;, like `run /bin/bash` or `run echo hello`\")\n }\n}\n\n\n\nfunc run(command ...string) {\n\n log.Println(\"Executing\", command, \"from run\")\n cmd := exec.Command(\"/proc/self/exe\", append([]string{\"child\"}, command[0:]...)...)\n cmd.Stdin = os.Stdin\n cmd.Stdout = os.Stdout\n cmd.Stderr = os.Stderr\n\n // Cloneflags is only available in Linux\n // CLONE_NEWUTS namespace isolates hostname\n // CLONE_NEWPID namespace isolates processes\n // CLONE_NEWNS namespace isolates mounts\n cmd.SysProcAttr = &amp;syscall.SysProcAttr{\n  Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS ,\n  Unshareflags: syscall.CLONE_NEWNS | syscall.CLONE_NEWNET, \n }\n\n // Run child using namespaces. The command provided will be executed inside that.\n  must(cmd.Run())\n}\n\n\n\n\nfunc child(command ...string) {\n\n // Create cgroup\n cg()\n\n\n\n\n\n cmd := exec.Command(command[0], command[1:]...)\n\n cmd.Stdin = os.Stdin\n cmd.Stdout = os.Stdout\n cmd.Stderr = os.Stderr\n\n\n must(syscall.Sethostname([]byte(\"container\")))\n\n\n must(syscall.Chroot(\"./ubuntu_fs\"))\n // Change directory after chroot\n must(os.Chdir(\"/\"))\n // Mount /proc inside container so that `ps` command works\n must(syscall.Mount(\"proc\", \"proc\", \"proc\", 0, \"\"))\n // Mount a temporary filesystem\n if _, err := os.Stat(\"mytemp\"); os.IsNotExist(err) {\n  must(os.Mkdir(\"mytemp\", os.ModePerm))\n }\n must(syscall.Mount(\"something\", \"mytemp\", \"tmpfs\", 0, \"\"))\n\n\n\n\n must(cmd.Run())\n\n // Cleanup mount\n must(syscall.Unmount(\"proc\", 0))\n must(syscall.Unmount(\"mytemp\", 0))\n}\n\n\n\n\nfunc cg() {\n // cgroup location in Ubuntu\n cgroups := \"/sys/fs/cgroup/\"\n\n pids := filepath.Join(cgroups, \"pids\")\n containers_mini := filepath.Join(pids, \"containers_mini\")\n os.Mkdir(containers_mini, 0755)\n // Limit to max 20 pids\n must(ioutil.WriteFile(filepath.Join(containers_mini, \"pids.max\"), []byte(\"20\"), 0700))\n // Cleanup cgroup when it is not being used\n must(ioutil.WriteFile(filepath.Join(containers_mini, \"notify_on_release\"), []byte(\"1\"), 0700))\n\n pid := strconv.Itoa(os.Getpid())\n // Apply this and any child process in this cgroup\n must(ioutil.WriteFile(filepath.Join(containers_mini, \"cgroup.procs\"), []byte(pid), 0700))\n}\n\nfunc must(err error) {\n if err != nil {\n  log.Printf(\"Error: %v\\n\", err)\n   panic(err)\n }\n}\n</code></pre></div><p>this code introduced by Liz Rice</p><p>Understanding the Code</p><p>The main function serves as the entry point of the program. It uses command-line arguments to determine whether to run a new container or act as a child process within an existing container.</p><div><pre><code>func main() {\n    switch os.Args[1] {\n    case \"run\":\n        run(os.Args[2:]...)\n    case \"child\":\n        child(os.Args[2:]...)\n    default:\n        log.Fatal(\"Unknown command. Use run &lt;command_name&gt;, like `run /bin/bash` or `run echo hello`\")\n    }\n}\n\n</code></pre></div><p>The run function sets up the container environment and executes a specified command inside it.</p><p><code>func run(command ...string) {\n   log.Println(\"Executing\", command, \"from run\")<p>\n   cmd := exec.Command(\"/proc/self/exe\", append([]string{\"child\"}, command[0:]...)...)</p>\n   cmd.Stdin = os.Stdin\n   cmd.Stderr = os.Stderr<p>\n    cmd.SysProcAttr = &amp;syscall.SysProcAttr{</p>\n        Cloneflags:    syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS,<p>\n        Unshareflags:  syscall.CLONE_NEWNS | syscall.CLONE_NEWNET,</p>\n    }\n}</code></p><div><pre><code>this command cmd := exec.Command(â€œ/proc/self/exeâ€, append([]string{â€œchildâ€}, command[0:]â€¦)â€¦)\nmake sure that itâ€™s append all command to same process\nThe Cloneflags specify the namespaces to be isolated (UTS, PID, and mount namespaces).\nThe Unshareflags further isolate the network namespace.\nThe cmd.Run() method runs the provided command within the created container.\n</code></pre></div><p>The child function is responsible for setting up the container filesystem and executing the specified command inside it.</p><div><pre><code>func child(command ...string) {\n    // ...\n    cg()\n    must(syscall.Sethostname([]byte(\"container\")))\n    must(syscall.Chroot(\"./ubuntu_fs\"))\n    must(os.Chdir(\"/\"))\n    must(syscall.Mount(\"proc\", \"proc\", \"proc\", 0, \"\"))\n    must(syscall.Mount(\"something\", \"mytemp\", \"tmpfs\", 0, \"\"))\n    must(cmd.Run())\n    must(syscall.Unmount(\"proc\", 0))\n    must(syscall.Unmount(\"mytemp\", 0))\n}\n</code></pre></div><div><pre><code>The cg function sets up a control group (cgroup) to limit resource usage for the container.\nSethostname sets the hostname inside the container.\nChroot changes the root directory for the container.\nMount is used to mount essential filesystems like /proc and a temporary filesystem.\nFinally, the command is executed within the container.\n</code></pre></div><p>The cg function creates and configures a cgroup for the container, limiting the number of processes.</p><div><pre><code>func cg() {\n // cgroup location in Ubuntu\n cgroups := \"/sys/fs/cgroup/\"\n\n pids := filepath.Join(cgroups, \"pids\")\n containers_mini := filepath.Join(pids, \"containers_mini\")\n os.Mkdir(containers_mini, 0755)\n // Limit to max 20 pids\n must(ioutil.WriteFile(filepath.Join(containers_mini, \"pids.max\"), []byte(\"20\"), 0700))\n // Cleanup cgroup when it is not being used\n must(ioutil.WriteFile(filepath.Join(containers_mini, \"notify_on_release\"), []byte(\"1\"), 0700))\n\n pid := strconv.Itoa(os.Getpid())\n // Apply this and any child process in this cgroup\n must(ioutil.WriteFile(filepath.Join(containers_mini, \"cgroup.procs\"), []byte(pid), 0700))\n}\n\n</code></pre></div><div><pre><code>Cgroups are used to control and limit resource usage for processes.\nIn this example, the cgroup limits the maximum number of processes to 20.\n</code></pre></div><p>The must function is a simple utility function for handling errors.</p><div><pre><code>func must(err error) {\n    if err != nil {\n        log.Printf(\"Error: %v\\n\", err)\n        panic(err)\n    }\n}\n</code></pre></div><p>If an error occurs, it is logged, and the program is terminated.\nBuilding and Running the Container</p><p>To run the minimal container, follow these steps:</p><div><pre><code>Build the executable: go build -o mycontainer main.go\nCreate a filesystem directory with an Ubuntu root filesystem, e.g., ubuntu_fs.\nRun the container: sudo ./mycontainer run /bin/bash\nremember you need to run it as sudo\nyour entry point is /bin/bash\n</code></pre></div><p>now you are in your own minimal container , and now you have a deep understanding , may be if i have more time in the future i will add isolation layer on network , our you can do it , thank you for your time i hopped it helped anyone .</p><div><pre><code>read this will help you more\n\nnamespace &amp; golang a series of article explains namespace with go examples\n\nâ€œCreating Network Stacks and Connecting with the Internetâ€ by â€œShrikanta Mazumderâ€\n</code></pre></div><p>on next part we will create a network layer that give our container a virtual Ethernet in isolated subset that use host bridge as gateway . see you soon</p>","contentLength":17425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ElasticTransform in PyTorch (1)","url":"https://dev.to/hyperkai/elastictransform-in-pytorch-1-56fc","date":1740099892,"author":"Super Kai (Kazuya Ito)","guid":7516,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can do morphological transformation.</li><li>It's the magnitude of displacements .</li><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value(,  or /( or )) means .</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It's the smoothness of displacements .</li><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value(,  or /( or )) means .</li></ul></li><li>The 3rd argument for initialization is (Optional-Default:<code>InterpolationMode.BILINEAR</code>-Type:<a href=\"https://pytorch.org/vision/main/_modules/torchvision/transforms/functional.html\" rel=\"noopener noreferrer\">InterpolationMode</a>).</li><li>The 4th argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can change the background of an image. *The background can be seen when doing morphological transformation for an image.</li><li>A tuple/list must be the 1D with 1 or 3 elements.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul></ul></li></ul><div><pre><code></code></pre></div>","contentLength":883,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I developed an Ecommerce App with Django","url":"https://dev.to/nlansong/i-developed-an-ecommerce-app-with-django-4208","date":1740098689,"author":"nlansong","guid":7515,"unread":true,"content":"<p>Tech Stack: Django (Backend &amp; Frontend)</p><p>Glasc Phones is an eCommerce website built using Django for both the backend and frontend, designed for selling mobile phones. The platform provides a seamless shopping experience with key features such as:</p><p>User Authentication â€“ Secure registration and login system.\nCart System â€“ Users can add, update, and remove items before checkout.<p>\nPayment Gateway Integration â€“ Secure online transactions.</p>\nMessaging System â€“ Customer inquiries and support messaging.\nFull-Stack Development: Built both the backend and frontend with Django, handling data flow, UI rendering, and business logic.<p>\nAuthentication &amp; Security: Implemented user authentication, session management, and security best practices.</p>\nECommerce Development: Developed features like product listing, cart management, and checkout.<p>\nPayment Integration: Integrated a secure payment gateway to facilitate transactions.</p>\nDatabase Management: Designed and optimized relational database models using Django ORM.<p>\nMessaging &amp; Notifications: Implemented a messaging system for customer support.</p>\nProblem-Solving &amp; Scalability: Addressed challenges like data consistency, performance optimization, and a smooth user experience.<p>\nThis project has strengthened my expertise in Django-based web applications, enhancing my ability to build secure, scalable, and user-friendly eCommerce platforms</p></p>","contentLength":1381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Use an LLM-Powered Boilerplate for Building Your Own Node.js API","url":"https://towardsdatascience.com/how-to-use-an-llm-powered-boilerplate-for-building-your-own-node-js-api/","date":1740096923,"author":"Uladzimir Yancharuk","guid":7495,"unread":true,"content":"<p>For a long time, one of the common ways to start new Node.js projects was using boilerplate templates. These templates help developers reuse familiar code structures and implement standard features, such as access to cloud file storage. With the latest developments in LLM, project boilerplates appear to be more useful than ever.</p><p>Building on this progress, Iâ€™ve extended my existing Node.js <a href=\"https://towardsdatascience.com/tag/api/\" title=\"API\">API</a> boilerplate with a new tool&nbsp;<a href=\"https://github.com/vyancharuk/nodejs-todo-api-boilerplate\" rel=\"noreferrer noopener\" target=\"_blank\">LLM Codegen</a>. This standalone feature enables the boilerplate to automatically generate module code for any purpose based on text descriptions. The generated module comes complete with E2E tests, database migrations, seed data, and necessary business logic.</p><p>I initially created a&nbsp;<a href=\"https://github.com/vyancharuk/nodejs-todo-api-boilerplate#nodejs-api-typescript-template-project\" rel=\"noreferrer noopener\" target=\"_blank\">GitHub repository</a>&nbsp;for a Node.js API boilerplate to consolidate the best practices Iâ€™ve developed over the years. Much of the implementation is based on code from a real Node.js API running in production on AWS.</p><p>I am passionate about vertical slicing architecture and Clean Code principles to keep the codebase maintainable and clean. With recent advancements in LLM, particularly its support for large contexts and its ability to generate high-quality code, I decided to experiment with generating clean TypeScript code based on my boilerplate. This boilerplate follows specific structures and patterns that I believe are of high quality. The key question was whether the generated code would follow the same patterns and structure. Based on my findings, it does.</p><p>To recap, hereâ€™s a quick highlight of the Node.js API boilerplateâ€™s key features:</p><ul><li>Vertical slicing architecture based on&nbsp;&nbsp;&amp;&nbsp;&nbsp;principles</li><li>Services input validation using&nbsp;</li><li>Decoupling application components with dependency injection ()</li><li>Integration and&nbsp;&nbsp;testing with Supertest</li><li>Multi-service setup using&nbsp;compose</li></ul><p>Over the past month, Iâ€™ve spent my weekends formalizing the solution and implementing the necessary code-generation logic. Below, Iâ€™ll share the details.</p><p>Letâ€™s explore the specifics of the implementation. All <a href=\"https://towardsdatascience.com/tag/code-generation/\" title=\"Code Generation\">Code Generation</a> logic is organized at the project root level, inside the&nbsp;&nbsp;folder, ensuring easy navigation. The Node.js boilerplate code has no dependency on&nbsp;, so it can be used as a regular template without modification.</p><p>It covers the following use cases:</p><ul><li>Generating clean, well-structured code for new module based on input description. The generated module becomes part of the Node.js REST API application.</li><li>Creating database migrations and extending seed scripts with basic data for the new module.</li><li>Generating and fixing E2E tests for the new code and ensuring all tests pass.</li></ul><p>The generated code after the first stage is clean and adheres to vertical slicing architecture principles. It includes only the necessary business logic for CRUD operations. Compared to other code generation approaches, it produces clean, maintainable, and compilable code with valid E2E tests.</p><p>The second use case involves generating DB migration with the appropriate schema and updating the seed script with the necessary data. This task is particularly well-suited for LLM, which handles it exceptionally well.</p><p>The final use case is generating E2E tests, which help confirm that the generated code works correctly. During the running of E2E tests, an SQLite3 database is used for migrations and seeds.</p><p>Mainly supported LLM clients are OpenAI and Claude.</p><p>To get started, navigate to the root folder&nbsp;&nbsp;and install all dependencies by running:</p><p>&nbsp;does not rely on Docker or any other heavy third-party dependencies, making setup and execution easy and straightforward. Before running the tool, ensure that you set at least one&nbsp;&nbsp;environment variable in the&nbsp;&nbsp;file with the appropriate API key for your chosen LLM provider. All supported environment variables are listed in the&nbsp;&nbsp;file (<code>OPENAI_API_KEY, CLAUDE_API_KEY</code>&nbsp;etc.) You can use&nbsp;,&nbsp;, or&nbsp;. As of mid-December,&nbsp;&nbsp;is surprisingly free to use. Itâ€™s possible to register&nbsp;<a href=\"https://openrouter.ai/nousresearch/hermes-3-llama-3.1-405b:free/api\" rel=\"noreferrer noopener\" target=\"_blank\">here</a>&nbsp;and obtain a token for free usage. However, the output quality of this free LLaMA model could be improved, as most of the generated code fails to pass the compilation stage.</p><p>To start&nbsp;, run the following command:</p><p>Next, youâ€™ll be asked to input the module description and name. In the module description, you can specify all necessary requirements, such as entity attributes and required operations. The core remaining work is performed by micro-agents:&nbsp;,&nbsp;, and&nbsp;.</p><p>Here is an example of a successful code generation:</p><p>Below is another example demonstrating how a compilation error was fixed:</p><p>The following is an example of a generated&nbsp;&nbsp;module code:</p><p>A key detail is that you can generate code step by step, starting with one module and adding others until all required APIs are complete. This approach allows you to generate code for all required modules in just a few command runs.</p><p>As mentioned earlier, all work is performed by those micro-agents:&nbsp;,&nbsp;&nbsp;and&nbsp;, controlled by the&nbsp;. They run in the listed order, with the&nbsp;&nbsp;generating most of the codebase. After each code generation step, a check is performed for missing files based on their roles (e.g., routes, controllers, services). If any files are missing, a new code generation attempt is made, including instructions in the prompt about the missing files and examples for each role. Once the&nbsp;&nbsp;completes its work, TypeScript compilation begins. If any errors are found, the&nbsp;&nbsp;takes over, passing the errors to the prompt and waiting for the corrected code. Finally, when the compilation succeeds, E2E tests are run. Whenever a test fails, the&nbsp;&nbsp;steps in with specific prompt instructions, ensuring all tests pass and the code stays clean.</p><p>All micro-agents are derived from the&nbsp;&nbsp;class and actively reuse its base method implementations. Here is the&nbsp;&nbsp;implementation for reference:</p><p>Each agent utilizes its specific prompt. Check out this GitHub&nbsp;<a href=\"https://github.com/vyancharuk/nodejs-todo-api-boilerplate/blob/master/llm-codegen/core/prompts/developer.main.prompt\" rel=\"noreferrer noopener\" target=\"_blank\">link</a>&nbsp;for the prompt used by the&nbsp;.</p><p>After dedicating significant effort to research and testing, I refined the prompts for all micro-agents, resulting in clean, well-structured code with very few issues.</p><p>During the development and testing, it was used with various module descriptions, ranging from simple to highly detailed. Here are a few examples:</p><pre><code>- The module responsible for library book management must handle endpoints for CRUD operations on books.\n- The module responsible for the orders management. It must provide CRUD operations for handling customer orders. Users can create new orders, read order details, update order statuses or information, and delete orders that are canceled or completed. Order must have next attributes: name, status, placed source, description, image url\n- Asset Management System with an \"Assets\" module offering CRUD operations for company assets. Users can add new assets to the inventory, read asset details, update information such as maintenance schedules or asset locations, and delete records of disposed or sold assets.</code></pre><p>Testing with&nbsp;&nbsp;and&nbsp;<code>claude-3-5-sonnet-20241022</code>&nbsp;showed comparable output code quality, although Sonnet is more expensive. Claude Haiku (<code>claude-3â€“5-haiku-20241022</code>), while cheaper and similar in price to&nbsp;, often produces non-compilable code. Overall, with&nbsp;, a single code generation session consumes an average of around 11k input tokens and 15k output tokens. This amounts to a cost of approximately 2 cents per session, based on token pricing of 15 cents per 1M input tokens and 60 cents per 1M output tokens (as of December 2024).</p><p>Below are Anthropic usage logs showing token consumption:</p><p>Based on my experimentation over the past few weeks, I conclude that while there may still be some issues with passing generated tests, 95% of the time generated code is compilable and runnable.</p><p>I hope you found some inspiration here and that it serves as a starting point for your next Node.js API or an upgrade to your current project. Should you have suggestions for improvements, feel free to contribute by submitting PR for code or prompt updates.</p><p>If you enjoyed this article, feel free to clap or share your thoughts in the comments, whether ideas or questions. Thank you for reading, and happy experimenting!</p><blockquote><p>&nbsp;[February 9, 2025]: The LLM-Codegen GitHub repository was updated with&nbsp;<a href=\"https://github.com/vyancharuk/nodejs-todo-api-boilerplate/blob/master/llm-codegen/core/llmClients/deepSeekLLMClient.ts\" rel=\"noreferrer noopener\" target=\"_blank\">DeepSeek API</a>&nbsp;support. Itâ€™s cheaper than&nbsp;&nbsp;and offers nearly the same output quality, but it has a longer response time and sometimes struggles with API request errors.</p></blockquote><p><em>Unless otherwise noted, all images are by the author</em><a href=\"https://medium.com/tag/nodejs?source=post_page-----59e9fc11ce95---------------------------------------\"></a></p>","contentLength":8377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The latest from TC39's recent meeting","url":"https://javascriptweekly.com/issues/724","date":1740096000,"author":"","guid":8596,"unread":true,"content":"<p>ğŸ˜° Is your vehicle data giving you anxiety? Mine too. That's why I built <a href=\"https://javascriptweekly.com/link/166014/rss\">CarsXE</a>. Now I just have existential dread. Progress! <a href=\"https://javascriptweekly.com/link/166014/rss\">Start for Free Now! </a></p>","contentLength":148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AutoAugment in PyTorch","url":"https://dev.to/hyperkai/autoaugment-in-pytorch-34ge","date":1740095611,"author":"Super Kai (Kazuya Ito)","guid":7500,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:<code>AutoAugmentPolicy.IMAGENET</code>-Type:<a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugmentPolicy.html\" rel=\"noopener noreferrer\">AutoAugmentPolicy</a>). *<code>AutoAugmentPolicy.IMAGENET</code>, <code>AutoAugmentPolicy.CIFAR10</code> or  can be set to it.</li><li>The 2nd argument for initialization is (Optional-Default:<code>InterpolationMode.NEAREST</code>-Type:<a href=\"https://pytorch.org/vision/main/_modules/torchvision/transforms/functional.html\" rel=\"noopener noreferrer\">InterpolationMode</a>). *If the input is a tensor, only <code>InterpolationMode.NEAREST</code>, <code>InterpolationMode.BILINEAR</code> can be set to it.</li><li>The 3rd argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It can change the background of an image.</li><li>A tuple/list must be the 1D with 1 or 3 elements.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":653,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Beginner's Guide to Learning Ren'Py and Creating Your First Visual Novel","url":"https://dev.to/sheikhulislamov/a-beginners-guide-to-learning-renpy-and-creating-your-first-visual-novel-59i5","date":1740094114,"author":"Arsen S.","guid":7499,"unread":true,"content":"<p>Visual novels are an exciting and creative way to tell interactive stories, combining traditional writing with beautiful visuals, sound, and choices that influence the narrative. If you're passionate about storytelling and want to bring your ideas to life through a visual medium, learning Ren'Py is a great first step.</p><p><a href=\"https://www.renpy.org/\" rel=\"noopener noreferrer\">Ren'Py</a> is a free and open-source engine that simplifies visual novel development. Whether you're an aspiring writer, artist, or programmer, Ren'Py provides all the tools you need to create professional-quality visual novels without requiring extensive technical knowledge.</p><p>In this article, we'll explore why Ren'Py is the perfect tool for beginners, the key concepts you need to learn, and how you can accelerate your journey with the right resources.</p><h2>\n  \n  \n  1. Why Choose Ren'Py for Your Visual Novel Projects?\n</h2><p>Ren'Py has earned its place as the go-to tool for many visual novel developers, and for good reason. Hereâ€™s why itâ€™s the best option for beginners:</p><p> Ren'Py uses a simple, Python-based scripting language that is easy to learn, even if you have no programming experience. Despite its simplicity, Ren'Py offers a wide range of features, such as branching narratives, interactive elements, character animations, and more. You can easily tweak Ren'Py to suit your needs, whether youâ€™re a beginner or an advanced user. Ren'Py has a supportive community of developers, writers, and artists who share their knowledge, assets, and advice.\nIf youâ€™re serious about creating high-quality visual novels, Ren'Py provides the perfect balance of simplicity and flexibility.</p><h2>\n  \n  \n  2. Key Concepts to Understand in Ren'Py\n</h2><p>Learning Ren'Py can seem overwhelming at first, but once you break it down, itâ€™s not as difficult as it seems. Here are some essential concepts youâ€™ll need to grasp to get started:</p><p>Ren'Pyâ€™s scripting language is simple and intuitive. To create a scene, youâ€™ll write commands that define the characters, backgrounds, and dialogue. For example:</p><div><pre><code>label start:\n    scene bg room\n    show char1 happy\n    \"Welcome to my visual novel!\"\n</code></pre></div><p>This basic script shows how easy it is to get started with Ren'Py. The syntax is straightforward, allowing you to focus on writing the story rather than wrestling with complex code.</p><h3>\n  \n  \n  Creating Characters &amp; Backgrounds\n</h3><p>Youâ€™ll need to create character sprites and backgrounds to bring your story to life. Ren'Py makes it easy to display images and manage character emotions or poses:</p><p>This line will show char1 with a sad expression. You can swap out different images, backgrounds, and even animate characters, adding life to your narrative.</p><p>Choice Menus and Branching Narratives\nOne of Ren'Pyâ€™s strongest features is its ability to let players make choices that impact the story. With a simple menu command, you can create branching paths for the player to follow:</p><div><pre><code>menu:\n    \"Go left\":\n        jump left_path\n    \"Go right\":\n        jump right_path\n</code></pre></div><p>This simple choice mechanic adds replayability and depth to your visual novel.</p><p>Ren'Py offers a variety of transitions and visual effects, like fades, wipes, and custom animations. This is where you can start adding some flair to your visual novel, creating smooth scene changes or dramatic visual effects.</p><p>For example, to fade out a scene, you can use:</p><p>This creates a smoother transition between scenes, enhancing the overall experience.</p><h3>\n  \n  \n  Interactivity &amp; Minigames\n</h3><p>You can also incorporate minigames into your visual novel, providing an interactive element beyond the traditional choices. For example, in my own project, I developed a puzzle mini-game, where players could engage with the game to unlock new parts of the story.</p><h2>\n  \n  \n  3. Resources to Accelerate Your Learning\n</h2><p>While Ren'Py is beginner-friendly, the best way to learn is by combining hands-on experience with structured learning. There are many great resources to help you along the way:</p><p> The official Ren'Py Documentation is the best place to start. It covers everything from basic scripting to advanced features. The Ren'Py forums are filled with tips, tutorials, and code snippets from experienced developers.<strong>Tutorial Blogs and YouTube Channels:</strong> There are numerous free tutorials online, many of which provide step-by-step guides to help you create your first visual novel.</p><p>If youâ€™re looking for a more structured approach, Iâ€™ve created a comprehensive Udemy course titled <a href=\"https://www.udemy.com/course/mastering-renpy-create-visual-novels-like-a-pro/?referralCode=9A4732B0C1A919F10AA6&amp;couponCode=ST3MT200225B\" rel=\"noopener noreferrer\">Mastering Ren'Py: Create Visual Novels Like a Pro</a>. In this course, I cover everything from the basics to advanced techniques like creating custom screens, implementing animations, and designing minigames. It's a one-stop resource for anyone serious about mastering Ren'Py.</p><h2>\n  \n  \n  4. Common Mistakes Beginners Make (and How to Avoid Them)\n</h2><p>Even the most experienced developers make mistakes, but the good news is that you can avoid common pitfalls by learning from others. Here are a few mistakes beginners often make and how to avoid them:</p><p><strong>Overcomplicating the Story:</strong> While itâ€™s tempting to have a complex plot with endless branching, itâ€™s better to start with a simple story. Focus on making each choice meaningful and impactful. Itâ€™s easy to get lost in a pile of code. Make sure to organize your scripts properly and comment on your code to avoid confusion later on. Testing is crucial. Playtest your visual novel frequently to catch bugs or awkward pacing before itâ€™s too late.</p><h2>\n  \n  \n  5. Tips for Creating an Engaging Visual Novel\n</h2><p>Now that you understand the basics, here are a few tips to make your visual novel stand out:</p><p>Great visual novels are driven by compelling stories. Develop deep, relatable characters and create an engaging narrative that keeps the player invested. The power of a visual novel lies in its ability to connect emotionally with players.</p><p>Visual novels rely heavily on artwork and music to set the tone. Good visuals and sound can elevate your storytelling to new heights. Whether you're an artist or working with a team, make sure your assets complement your story.</p><p>Once you have the basic structure of your visual novel, go back and refine it. Customize the user interface, add smooth transitions, and tweak the audio to create a more polished experience.</p><h2>\n  \n  \n  6. How Long Does It Take to Learn Ren'Py?\n</h2><p>How long it takes to learn Ren'Py depends on your prior experience and how much time you can dedicate to learning. If youâ€™re starting from scratch, expect to spend a few weeks getting comfortable with the basics. However, with dedicated effort, you can create a simple visual novel within a month.</p><p>For those who want to accelerate their learning, my <a href=\"https://www.udemy.com/course/mastering-renpy-create-visual-novels-like-a-pro/?referralCode=9A4732B0C1A919F10AA6&amp;couponCode=ST3MT200225B\" rel=\"noopener noreferrer\">Udemy course</a> is designed to help you get up to speed quickly. Youâ€™ll learn how to create a visual novel from start to finish, with tons of useful tips along the way.</p><p>Learning Ren'Py opens up a world of possibilities for storytelling and game development. Whether youâ€™re a writer, artist, or programmer, Ren'Py offers an intuitive and flexible platform for creating visually stunning and interactive stories.</p><p>By combining the resources Iâ€™ve shared here with practical experience, you'll be well on your way to creating your very own visual novel!</p>","contentLength":7122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: BadSeek â€“ How to backdoor large language models","url":"https://sshh12--llm-backdoor.modal.run/","date":1740091493,"author":"sshh12","guid":7490,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43121383"},{"title":"Python 3.13 No-GIL: What You Need to Know","url":"https://dev.to/zackch/python-313-no-gil-what-you-need-to-know-352i","date":1740088058,"author":"zakaria chatouane","guid":7441,"unread":true,"content":"<p>The Global Interpreter Lock (GIL) is a core component of CPython that has been part of the interpreter since the 90s. Essentially, itâ€™s a mutex that ensures only one thread executes Python bytecode at a time, protecting internal data structuresâ€”especially the reference counting used by the garbage collectorâ€”from race conditions. .</p><p>Excitingly, Python 3.13 is the first release featuring an experimental build mode that disables the GIL, opening the door to significant performance improvements for multi-threaded applications.</p><p>Before discussing these new changes, letâ€™s do a quick recap of the pros and cons of the GIL.</p><p>The GIL offers several advantages, including:</p><ul><li><strong>Simplified Memory Management</strong>: By blocking threads from concurrently modifying an objectâ€™s reference count, the GIL prevents race conditions and ensures the garbage collector doesnâ€™t free an object while itâ€™s still referenced, making the core implementation simpler and more robust.</li><li><strong>Ease of C Extension Integration</strong>: Many C libraries and extensions, which are not inherently thread-safe, can be used safely under the GILâ€™s protection.</li><li><strong>Single-thread Performance</strong>: Since CPython uses reference counting for memory management, the garbage collector has minimal impact compared to mark-and-sweep algorithms used in languages - ex: Java -, which can be unpredictable and cause pauses.</li></ul><p>But if only one thread can use the interpreter at a given time, how does Python run multiple threads?</p><p>Thread switching is a complex topic; even the interpreter doesnâ€™t always decide which thread runs next, as the operating system also influences scheduling. In this blog, we will focus solely on the Python side of thread management.</p><h3>\n  \n  \n  what happens when a thread holds the GIL?\n</h3><p>When a thread holds the GIL, it may encounter one of the following scenarios:</p><ul><li>If it reaches an I/O task, the thread willingly releases the lock while waiting, allowing other threads to run.</li><li>During CPU-intensive operations, the lock is automatically released after a defined timeout (typically 5ms).</li></ul><p>Now that we understand how thread switching works, itâ€™s clear that one of the main downsides of the GIL is limited true multi-threaded parallelism in CPU-bound programs. Removing the GIL requires a solution that grants safe, concurrent access to objects and memory management without compromising single-thread performance.</p><p>Sam Gross, the author of <a href=\"https://peps.python.org/pep-0703/\" rel=\"noopener noreferrer\">PEP 703</a>, has proposed an ingenious solution to remove the GIL from CPython while ensuring thread safety. The proposal rethinks CPythonâ€™s memory management and introduces three key techniques:</p><ul><li><p><strong>Biased Reference Counting</strong>:\nMany objects are primarily modified by a single thread, so biased reference counting lets that thread update an objectâ€™s reference count without atomic overhead. If another thread intervenes, the system safely falls back to slower, thread-safe operationsâ€”optimizing the common single-threaded case.</p></li><li><p><strong>Deferred Reference Counting</strong>:\nFor objects like top-level functions, code objects, modules, and methods that are accessed concurrently, deferred reference counting postpones immediate atomic updates. It batches reference count changes to reduce contention, thereby minimizing overhead for dynamic objects that arenâ€™t immortal.</p></li><li><p>:\nCertain objectsâ€”such as interned strings, small integers, PyTypeObjects, and the True, False, and Noneâ€”live for the duration of the program. These are marked as immortal (their reference count is set to UINT32_MAX), so Py_INCREF and Py_DECREF become no-ops, avoiding contention when accessed by multiple threads.</p></li></ul><p>Together, these techniques pave the way for enhanced parallelism and a more efficient, concurrent CPython.</p><p>From the Steering Councilâ€™s notice about the adoption of PEP 703:</p><blockquote><ul><li>:\nWe will add the no-GIL build as an experimental build mode, presumably in Python 3.13 (if it slips to 3.14, that is not a problem). The build mode is experimental to clarify that, although the core developers support it, we cannot expect the community to adopt it immediately. We need time to determine the necessary changes in API design, packaging, and distribution, and we want to discourage distributors from shipping the experimental no-GIL build as the default interpreter.</li><li>:\nOnce there is sufficient community support for production use of no-GIL, we will support the no-GIL buildâ€”but not as the defaultâ€”while setting a target date or Python version for making it the default. This timing will depend on factors such as backward compatibility of API changes (e.g., the stable ABI) and the remaining work identified by the community. We expect this phase to take at least a year or two, possibly more. Some distributors may start shipping no-GIL by default, though this will vary with package support.</li><li>:\nOur goal is for no-GIL to become the default, eventually removing any vestiges of the GIL without unnecessarily breaking backward compatibility. We do not want to maintain two common build modes indefinitely, as this can double testing resources and debugging efforts. However, the transition cannot be rushed and may take as much as five years.</li></ul></blockquote><p>The move to remove the GIL marks a major evolution in CPythonâ€™s design. By rethinking memory management with biased and deferred reference counting, plus immortal objects, PEP 703 offers a practical path to better multi-threaded performance without losing single-thread efficiency. As Python gradually shifts toward a no-GIL future, we can look forward to an interpreter thatâ€™s more capable on todayâ€™s multi-core systems.</p>","contentLength":5509,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Immersive Gaussian Splat experience of Sutro Tower, San Francisco","url":"https://vincentwoo.com/3d/sutro_tower/","date":1740087559,"author":"akanet","guid":7456,"unread":true,"content":"<p>Welcome to my 3D model of San Francisco's Sutro Tower. Feel free to explore it at your own pace. If you're on a phone, you can also engage the AR mode by clicking the little cube, it'll let you explore the scene by walking around and waving your phone.</p><p>Sutro Tower is a wonderful building, and I hope you enjoy learning a bit about it here. If you want to learn more, check out the much more thorough <a href=\"https://explore.sutrotower.com/\">official digital tour</a>.</p><p>If I've made any mistakes, or if you want to get in touch, feel free to reach out over <a href=\"mailto:me@vincentwoo.com\">email</a> or <a href=\"https://x.com/fulligin\">Twitter</a>.</p><p><a href=\"https://wieland.morgenst.de/\">Wieland Morgenstern</a> for developing Self Organizing Gaussian compression and assisting me in understanding it.</p><p><a href=\"https://x.com/slimbuck7\">Donovan Hutchence</a> of PlayCanvas for helping me implement the decoding of the new compressed format that allows us to serve this entire scene in 30MB.</p>","contentLength":784,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43120582"},{"title":"Donâ€™t Let Conda Eat Your Hard Drive","url":"https://towardsdatascience.com/dont-let-conda-eat-your-hard-drive/","date":1740086994,"author":"Lee Vaughan","guid":7453,"unread":true,"content":"<p>If youâ€™re an Anaconda user, you know that&nbsp;<a href=\"https://docs.anaconda.com/working-with-conda/environments/\" rel=\"noreferrer noopener\" target=\"_blank\"></a>&nbsp;help you manage package dependencies, avoid compatibility conflicts, and share your projects with others. Unfortunately, they can also take over your computerâ€™s hard drive.</p><p>I write lots of computer tutorials and to keep them organized, each has a dedicated folder structure complete with a <a href=\"https://towardsdatascience.com/tag/conda-environment/\" title=\"Conda Environment\">Conda Environment</a>. This worked great at first, but soon my computerâ€™s performance degraded, and I noticed that my SSD was filling up. At one point I had only 13 GB free.</p><p>Conda helps manage this problem by storing downloaded package files in a single â€œcacheâ€ (). When you install a package, conda checks for it in the package cache before downloading. If not found, conda will download and extract the package and link the files to the active environment. Because the cache is â€œshared,â€ different environments can use the same downloaded files without duplication.</p><p>Because conda caches&nbsp;,&nbsp;&nbsp;can grow to many gigabytes. And while conda links to shared packages in the cache, there is still a need to store some packages in the environment folder. This is mainly to avoid&nbsp;, where different environments need different versions of the same&nbsp;(a package required to run another package).</p><p>In addition, large, compiled binaries like&nbsp;<a href=\"https://opencv.org/\" rel=\"noreferrer noopener\" target=\"_blank\">OpenCV</a>&nbsp;may require&nbsp;&nbsp;in the environmentâ€™s directory, and each environment requires a copy of the Python interpreter (at 100â€“200 MB). All these issues can bloat conda environments to several gigabytes.</p><p>In this&nbsp;<em>Quick Success Data Science</em>&nbsp;project, weâ€™ll look at some techniques for reducing the storage requirements for conda environments, including those stored in default locations and dedicated folders.</p><h2>Memory Management Techniques</h2><p>Below are some <a href=\"https://towardsdatascience.com/tag/memory-management/\" title=\"Memory Management\">Memory Management</a> techniques that will help you reduce condaâ€™s storage footprint on your machine. Weâ€™ll discuss each in turn.</p><ol><li>Sharing task-based environments</li><li>Archiving with environment and specifications files</li><li>Archiving environments with conda-pack</li><li>Storing environments on an external drive</li><li>Relocating the package cache</li><li>Using virtual environments ()</li></ol><h3>1. Cleaning the Package Cache</h3><p>Cleaning the package cache is the first and easiest step for freeing up memory. Even after deleting environments, conda keeps the related package files in the cache. You can free up space by removing these unused packages and their associated&nbsp;&nbsp;(compressed package files), logs,&nbsp;&nbsp;(metadata stored in conda), and temporary files.</p><p>Conda permits an optional â€œdry runâ€ to see how much memory will be reclaimed. Youâ€™ll want to run this from either the terminal or Anaconda Prompt in your&nbsp;&nbsp;environment:</p><pre><code>conda clean --all --dry-run</code></pre><p>Hereâ€™s how this looks on my machine:</p><p>This process trimmed a healthy 6.28 GB and took several minutes to run.</p><h3>2. Sharing Task-based Environments</h3><p>Creating a few environments for&nbsp;&nbsp;â€” like computer vision or geospatial work â€” is more memory efficient than using dedicated environments for each&nbsp;. These environments would include basic packages plus ones for the specific task (such as OpenCV, scikit-image, and PIL for computer vision).</p><p>An advantage of this approach is that you can easily keep all the packages up to date and link the environments to multiple projects. However, this wonâ€™t work if some projects require different versions of the shared packages.</p><h3>3. Archiving with Environment and Specifications Files</h3><p>If you donâ€™t have enough storage sites or want to preserve legacy projects efficiently, consider using&nbsp;&nbsp;or&nbsp;files. These small files record an environmentâ€™s&nbsp;, allowing you to rebuild it later.</p><p>Saving conda environments in this manner reduces their size on disk from gigabytes to a few kilobytes. Of course, youâ€™ll have to recreate the environment to use it. So, youâ€™ll want to avoid this technique if you frequently revisit projects that link to the archived environments.</p><blockquote><p>NOTE: Consider using&nbsp;<a href=\"https://mamba.readthedocs.io/en/latest/\" rel=\"noreferrer noopener\" target=\"_blank\">Mamba</a>, a drop-in replacement for conda, for faster rebuilds. As the docs say, â€œIf you know conda, you know Mamba!â€</p></blockquote><p>&nbsp;An&nbsp;&nbsp;is a small file that lists all the packages and versions installed in an environment, including those installed using Pythonâ€™s package installer (<a href=\"https://pypi.org/project/pip/\" rel=\"noreferrer noopener\" target=\"_blank\">pip</a>). This helps you both restore an environment and share it with others.</p><p>The environment file is written in&nbsp;<a href=\"https://en.wikipedia.org/wiki/YAML\" target=\"_blank\" rel=\"noreferrer noopener\"></a>&nbsp;(), a human-readable data-serialization format for data storage. To generate an environment file, you must activate and then export the environment. Hereâ€™s how to make a file for an environment named&nbsp;:</p><pre><code> conda activate my_env\n conda env export &gt; my_env.yml</code></pre><p>You can name the file any valid filename but be careful as an existing file with the same name will be overwritten.</p><p>By default, the environment file is written to the&nbsp;directory. Hereâ€™s a truncated example of the fileâ€™s contents:</p><pre><code>name: C:\\Users\\hanna\\quick_success\\fed_hikes\\fed_env\nchannels:\n  - defaults\n  - conda-forge\ndependencies:\n  - asttokens=2.0.5=pyhd3eb1b0_0\n  - backcall=0.2.0=pyhd3eb1b0_0\n  - blas=1.0=mkl\n  - bottleneck=1.3.4=py310h9128911_0\n  - brotli=1.0.9=ha925a31_2\n  - bzip2=1.0.8=he774522_0\n  - ca-certificates=2022.4.26=haa95532_0\n  - certifi=2022.5.18.1=py310haa95532_0\n  - colorama=0.4.4=pyhd3eb1b0_0\n  - cycler=0.11.0=pyhd3eb1b0_0\n  - debugpy=1.5.1=py310hd77b12b_0\n  - decorator=5.1.1=pyhd3eb1b0_0\n  - entrypoints=0.4=py310haa95532_0\n\n  ------SNIP------</code></pre><p>You can now remove your conda environment and reproduce it again with this file. To remove an environment, first deactivate it and then run the&nbsp;&nbsp;command (where&nbsp;&nbsp;is the name of your environment):</p><pre><code>conda deactivate\nconda remove -n ENVNAME --all</code></pre><p>If the conda environment exists outside of Anacondaâ€™s default&nbsp;&nbsp;folder, then include the directory path to the environment, as so:</p><pre><code>conda remove -p PATH\\ENVNAME --all</code></pre><p>Note that this archiving technique will only work perfectly if you continue to use the same operating system, such as Windows or macOS. This is because solving for dependencies can introduce packages that might not be compatible across platforms.</p><p>To restore a conda environment using a file, run the following, where&nbsp;&nbsp;represents your conda environment name and&nbsp;&nbsp;represents your environment file:</p><pre><code> conda env create -n my_env -f \\directory\\path\\to\\environment.yml</code></pre><p>You can also use the environment file to recreate the environment on your D: drive. Just provide the new path when using the file. Hereâ€™s an example:</p><pre><code>conda create --prefix D:\\my_envs\\my_new_env --file environment.yml</code></pre><p>For more on environment files, including how to manually produce them, visit the&nbsp;<a href=\"https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\" rel=\"noreferrer noopener\" target=\"_blank\">docs</a>.</p><p><strong>Using Specifications Files:&nbsp;</strong>If you havenâ€™t installed any packages using pip, you can use a&nbsp;&nbsp;to reproduce a conda environment on the same operating system. To create a specification file, activate an environment, such as&nbsp;, and enter the following command:</p><pre><code> conda list --explicit &gt; exp_spec_list.txt</code></pre><p>This produces the following output, truncated for brevity:</p><pre><code> # This file may be used to create an environment using:\n # $ conda create --name &lt;env&gt; --file &lt;this file&gt;\n # platform: win-64\n @EXPLICIT\n https://conda.anaconda.org/conda-forge/win-64/ca-certificates-202x.xx.x-h5b45459_0.tar.bz2\n https://conda.anaconda.org/conda-forge/noarch/tzdata-202xx-he74cb21_0.tar.bz2\n\n------snip------</code></pre><p>Note that the&nbsp;&nbsp;flag ensures that the targeted platform is annotated in the file, in this case,&nbsp;&nbsp;in the third line.</p><p>You can now remove the environment as described in the previous section.</p><p>To re-create&nbsp;&nbsp;using this text file, run the following with a proper directory path:</p><pre><code>conda create -n my_env -f \\directory\\path\\to\\exp_spec_list.txt</code></pre><h3>4. Archiving Environments with conda-pack</h3><p>The&nbsp;&nbsp;command lets you archive a conda environment before removing it. It packs the entire environment into a compressed archive with the extension:&nbsp;. Itâ€™s handy for backing up, sharing, and moving environments without the need to reinstall packages.</p><p>The following command will preserve an environment but remove it from your system (where&nbsp;&nbsp;represents the name of your environment):</p><pre><code>conda install -c conda-forge conda-pack\nconda pack -n my_env -o my_env.tar.gz</code></pre><p>To restore the environment later run this command:</p><pre><code>mkdir my_env &amp;&amp; tar -xzf my_env.tar.gz -C my_env</code></pre><p>This technique wonâ€™t save as much memory as the text file option. However, you wonâ€™t need to re-download packages when restoring an environment, which means it can be used without internet access.</p><h3>5. Storing Environments on an External Drive</h3><p>By default, conda stores all environments in a default location. For Windows, this is under the&nbsp;&nbsp;folder. You can see these environments by running the command&nbsp;&nbsp;in a prompt window or terminal. Hereâ€™s how it looks on my C: drive (this is a truncated view):</p><p><strong>Using a Single Environments Folder:</strong>&nbsp;If your system supports an external or secondary drive, you can configure conda to store environments there to free up space on your primary disk. Hereâ€™s the command; youâ€™ll need to substitute your specific path:</p><pre><code>conda config --set envs_dirs /path/to/external/drive</code></pre><p>If you enter a path to your D drive, such as&nbsp;, conda will create new environments at this location.</p><p>This technique works well when your external drive is a fast SSD and when youâ€™re storing packages with large dependencies, like TensorFlow. The downside is slower performance. If your OS and notebooks remain on the primary drive, you may experience some read/write latency when running Python.</p><p>In addition, some OS settings may power down idle external drives, adding a delay when they spin back up. Tools like Jupyter may struggle to locate conda environments if the drive letter changes, so youâ€™ll want to use a fixed drive letter and ensure that the correct kernel paths are set.</p><p><strong>Using Multiple Environment Folders:</strong>&nbsp;Instead of using a single&nbsp;&nbsp;directory for&nbsp;&nbsp;environments, you can store each environment inside its respective&nbsp;&nbsp;folder. This lets you store everything related to a project in one place.</p><p>For example, suppose you have a project on your Windows D: drive in a folder called&nbsp;. To place the projectâ€™s conda environment in this folder, loaded with&nbsp;&nbsp;for JupyterLab, you would run:</p><pre><code>conda create -p D:\\projects\\geospatial\\env ipykernel</code></pre><p>Of course, you can call&nbsp;&nbsp;something more descriptive, like&nbsp;.</p><p>As with the previous example, environments stored on a different disk can cause performance issues.</p><p><strong>Special Note on JupyterLab:</strong>&nbsp;Depending on how you launch JupyterLab, its default behavior may be to open in your&nbsp;&nbsp;directory (such as,&nbsp;). Since its file browser is restricted to the directory from which it is launched, you wonâ€™t see directories on other drives like&nbsp;. There are many ways to handle this, but one of the simplest is to launch JupyterLab from the D: drive.</p><p>For example, in Anaconda Prompt, type:</p><p>Now, you will be able to pick from kernels on the D: drive.</p><p>For more options on changing JupyterLabâ€™s working directory, ask an AI about â€œhow to change Jupyterâ€™s default working directoryâ€ or â€œhow to create a Symlink to&nbsp;&nbsp;in your user folder.â€</p><p><strong>Moving Existing Environments:</strong>&nbsp;You should never manually move a conda environment, such as by cutting and pasting to a new location. This is because conda relies on internal paths and metadata that can become invalid with location changes.</p><p>Instead, you should&nbsp;existing environments to another drive. This will&nbsp;&nbsp;the environment, so youâ€™ll need to manually remove it from its original location.</p><p>In the following example, we use the&nbsp;&nbsp;flag to produce an exact copy of a C: drive environment (called&nbsp;) on the D: drive:</p><pre><code>conda create -p D:\\new_envs\\my_env --clone C:\\path\\to\\old\\env</code></pre><blockquote><p>&nbsp;Consider exporting your environment to a&nbsp;&nbsp;file (as described in Section 3 above) before cloning. This allows you to recreate the environment if something goes wrong with the clone procedure.</p></blockquote><p>Now, when you run&nbsp;, youâ€™ll see the environment listed in both the C: and D: drives. You can remove the old environment by running the following command in the&nbsp;environment:</p><pre><code>conda remove --name my_env --all -y</code></pre><p>Again, latency issues may affect these setups if youâ€™re working across two disks.</p><p>You may be wondering, is it better to move a conda environment using an environment (YAML) file or to use? The short answer is that&nbsp;&nbsp;is the best and fastest option for moving an environment to a different drive on the&nbsp;&nbsp;machine. An environment file is best for recreating the same environment on a&nbsp;machine. While the file guarantees a consistent environment across different systems, it can take much longer to run, especially with large environments.</p><h3>6. Relocating the Package Cache</h3><p>If your primary drive is low on space, you can move the package cache to a larger external or secondary drive using this command:</p><pre><code>conda config --set pkgs_dirs D:\\conda_pkgs</code></pre><p>In this example, packages are now stored on the D drive () instead of the default location.</p><p>If youâ€™re working in your primary drive and both drives are SSD, then latency issues should not be significant. However, if one of the drives is a slower HDD, you can experience slowdowns when creating or updating environments. If D: is an external drive connected by USB, you may see significant slowdowns for large environments.</p><p>You can mitigate some of these issues by keeping the package cache () and frequently used environments on the faster SSD, and other environments on the slower HDD.</p><p>One last thing to consider is&nbsp;. Primary drives may have routine backups scheduled but secondary or external drives may not. This puts you at risk of losing all your environments.</p><h3>7. Using Virtual Environments</h3><p>If your project doesnâ€™t require condaâ€™s extensive package management system for handling heavy dependencies (like TensorFlow or GDAL), you can significantly reduce disk usage with a Python&nbsp;&nbsp;(). This represents a lightweight alternative to a conda environment.</p><p>To create a&nbsp;&nbsp;named&nbsp;, run the following command:</p><p>This type of environment has a small base installation. A minimal conda environment takes up about 200 MB and includes multiple utilities, such as&nbsp;,&nbsp;,&nbsp;, and so on. A&nbsp;&nbsp;is much lighter, with a minimum install size of only 5â€“10 MB.</p><p>Conda also caches package tarballs in&nbsp;. These tarballs can grow to several GBs over time. Because&nbsp;&nbsp;installs packages directly into the environment, no extra copies are preserved.</p><p>In general, youâ€™ll want to consider&nbsp;&nbsp;when you only need basic Python packages like NumPy, pandas, or Scikit-learn. Packages for which conda is strongly recommended, like Geopandas, should still be placed in a conda environment. If you use lots of environments, youâ€™ll probably want to stick with conda and benefit from its package linking.</p><p>You can find details on how to activate and use Python virtual environments in the&nbsp;<a href=\"https://docs.python.org/3/library/venv.html\" target=\"_blank\" rel=\"noreferrer noopener\">docs</a>.</p><p>High impact/low disruption memory management techniques for conda environments include cleaning the package cache and storing little-used environments as YAML or text files. These methods can save many gigabytes of memory while retaining Anacondaâ€™s default directory structure.</p><p>Other high impact methods include moving the package cache and/or conda environments to a secondary or external drive. This will resolve memory problems but may introduce latency issues, especially if the new drive is a slow HDD or uses a USB connection.</p><p>For simple environments, you can use a Python virtual environment () as a lightweight alternative to conda.<a href=\"https://medium.com/@lee_vaughan?source=post_page---post_author_info--3aa2791a7623---------------------------------------\"></a></p>","contentLength":15200,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"## Master Django Redirects in Under 3 Minutes ğŸš€","url":"https://dev.to/ebereplenty/-master-django-redirects-in-under-3-minutes-4f6","date":1740082395,"author":"NJOKU SAMSON EBERE","guid":7422,"unread":true,"content":"<p>Django provides a powerful way to redirect users from one page to another using the  function. Whether you need to handle authentication flows, restructure URLs, or improve user experience, understanding Django Redirects is essential. In this quick tutorial, weâ€™ll break it down step by step.</p><h3>\n  \n  \n  ğŸ”¹ What is Django Redirect?\n</h3><p>A  in Django is a way to send users from one URL to another automatically. This is useful for scenarios like:</p><ul><li>Redirecting users after login/logout.</li><li>Moving outdated URLs to new locations.</li><li>Handling conditional navigation.</li></ul><p>Django simplifies this with the  function, which is commonly used in .</p><h3>\n  \n  \n  ğŸ“Œ Using Django's  Function\n</h3><p>Django provides the  function in , which allows redirection using:</p><ol><li>A  (recommended for better maintainability)</li><li>An  (optional)</li></ol><h4><strong>Example 1: Redirect to a Static URL</strong></h4><div><pre><code></code></pre></div><h4><strong>Example 2: Redirect Using a Named Route</strong></h4><p>Using  ensures flexibility if URLs change later.</p><div><pre><code></code></pre></div><h4><strong>Example 3: Redirect with Parameters</strong></h4><div><pre><code></code></pre></div><h3>\n  \n  \n  ğŸ”¥ Best Practices for Django Redirects\n</h3><p>âœ…  Helps in maintaining URLs dynamically.\nâœ…  Ensure the redirect doesnâ€™t point back to the same page.\nâœ…  Default is  (temporary), but you can use  when needed:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  ğŸ¥ Watch the Full Tutorial\n</h3><p>Want to see this in action? Watch my  on YouTube where I explain everything in under  ğŸš€</p><p>ğŸ”” <strong>Subscribe for More Django Content!</strong></p><p>Djangoâ€™s  function is an essential tool for managing user navigation efficiently. By leveraging named routes and best practices, you can create seamless and user-friendly experiences.</p><p>Do you use redirects in your Django projects? Drop a comment below and share your use case! ğŸ‘‡</p><p><strong>#Django #DjangoRedirect #Python #WebDevelopment #DjangoTutorial #LearnDjango #Coding #BackendDevelopment #PythonProgramming</strong></p>","contentLength":1726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geoblocking the UK with Debian & Nginx","url":"https://aphyr.com/posts/379-geoblocking-the-uk-with-debian-nginx","date":1740080755,"author":"Aphyr","guid":7410,"unread":true,"content":"<p>A few quick notes for other folks who are <a href=\"https://geoblockthe.uk\">geoblocking the UK</a>. I just set up a basic geoblock with Nginx on Debian. This is all stuff you can piece together, but the Maxmind and Nginx docs are a little vague about the details, so I figure itâ€™s worth an actual writeup. My Nginx expertise is ~15 years out of date, so this might not be The Best Way to do things. YMMV.</p><p>First, register for a free <a href=\"https://www.maxmind.com/en/geolite2/signup\">MaxMind account</a>; youâ€™ll need this to subscribe to their GeoIP database. Then set up a daemon to maintain a copy of the lookup file locally, and Nginxâ€™s GeoIP2 module:</p><pre><code>apt install geoipupdate libnginx-mod-http-geoip2\n</code></pre><p>Create a license key on the MaxMind site, and download a copy of the config file youâ€™ll need. Drop that in . Itâ€™ll look like:</p><pre><code>AccountID XXXX\nLicenseKey XXXX\nEditionIDs GeoLite2-Country\n</code></pre><p>The package sets up a cron job automatically, but we should grab an initial copy of the file. This takes a couple minutes, and writes out <code>/var/lib/GeoIP/GeoLite2-Country-mmdb</code>:</p><p>The GeoIP2 module should already be loaded via <code>/etc/nginx/modules-enabled/50-mod-http-geoip2.conf</code>. Add a new config snippet like <code>/etc/nginx/conf.d/geoblock.conf</code>. The first part tells Nginx where to find the GeoIP database file, and then extracts the two-letter ISO country code for each request as a variable. The  part sets up an  variable, which is set to  for GB, otherwise .</p><pre><code>geoip2 /var/lib/GeoIP/GeoLite2-Country.mmdb {\n  $geoip2_data_country_iso_code country iso_code;\n}\n\nmap $geoip2_data_country_iso_code $osa_geoblocked {\n  GB      1;\n  default 0;\n}\n</code></pre><p>Write an HTML file somewhere like <code>/var/www/custom_errors/osa.html</code>, explaining the block. Then serve that page for HTTP 451 status codes: in <code>/etc/nginx/sites-enabled/whatever</code>, add:</p><pre><code>server {\n  ...\n  # UK OSA error page\n  error_page 451 /osa.html;\n  location /osa.html {\n    internal;\n    root /var/www/custom_errors/;\n  }\n\n  # When geoblocked, return 451\n  location / {\n    if ($osa_geoblocked = 1) {\n      return 451;\n    }\n  }\n}\n</code></pre><p>Test your config with , and then . You can test how things look from the UK using a VPN service, or something like <a href=\"https://www.locabrowser.com/\">locabrowser</a>.</p><p>This is, to be clear, a bad solution. MaxMindâ€™s free database is not particularly precise, and in general IP lookup tables are chasing a moving target. I know for a fact that there are people in non-UK countries (like Ireland!) who have been inadvertently blocked by these lookup tables. Making those people use Tor or a VPN , but I donâ€™t know what else to do in the current regulatory environment.</p>","contentLength":2491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"N19-Crypt and N19-Chain: Development of a cryptographic algorithm,a blockchain prototype inspired by the number 19 from Quran","url":"https://dev.to/inquisitive41/n19-crypt-and-n19-chain-development-of-a-cryptographic-algorithma-blockchain-prototype-inspired-a9h","date":1740080053,"author":"Inquisitive41","guid":7385,"unread":true,"content":"<p>Liquid syntax error: Variable '{{sender, receiver, amount}' was not properly terminated with regexp: /\\}\\}/</p>","contentLength":107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Scientist, Data Engineer, or Technology Manager: Which Job Is Right for You?","url":"https://www.kdnuggets.com/2025/02/nwu/data-scientist-data-engineer-or-technology-manager-which-job-is-right-for-you","date":1740074444,"author":"KDnuggets","guid":7319,"unread":true,"content":"<article>Whatever role is best for youâ€”data scientist, data engineer, or technology managerâ€”Northwestern University's MS in Data Science program will help you to prepare for the jobs of today and the jobs of the future.</article>","contentLength":214,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/MSDS_775x500-8.jpg","enclosureMime":"","commentsUrl":null},{"title":"Learn Faster, Code Smarter! 25+ Programming Resources to Boost Your Skills!","url":"https://dev.to/dev-resources/learn-faster-code-smarter-25-programming-resources-to-boost-your-skills-54k8","date":1740074219,"author":"Dev Resources","guid":7326,"unread":true,"content":"<ul><li>You can get free giveaway products - <a href=\"https://0x7bshop.gumroad.com/?section=glAkb82Vm2qgiAORxvh-3g%3D%3D\" rel=\"noopener noreferrer\">here</a></li></ul><h2><strong>1. DIY API vs. Marketplace API: The 2025 Ultimate Innovation Showdown</strong></h2><h2><strong>2. How to Write API Documentation That Developers Love in 2025</strong></h2><p>API documentation is like a user manual that tells other developers how to use your provided... <a href=\"http://dev-resources.site/topic/programming/2289419/how-to-write-api-documentation-that-developers-love-in-2025-31aj\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmofpaa9y8w9vc10p6k31.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>3. Donâ€™t Build Another App Until You Read This: Shocking Comparisons of 2025 Mobile Tech Stacks!</strong></h2><p>50 AI-Powered Money-Making... <a href=\"http://dev-resources.site/topic/programming/2288223/dont-build-another-app-until-you-read-this-shocking-comparisons-of-2025-mobile-tech-stacks-2nka\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DDon%25E2%2580%2599t%2520Build%2520Another%2520App%2520Until%2520You%2520Read%2520This%3A%2520Shocking%2520Comparisons%2520of%25202025%2520Mobile%2520Tech%2520Stacks%21%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2288223%2Fdont-build-another-app-until-you-read-this-shocking-comparisons-of-2025-mobile-tech-stacks-2nka\" alt=\"Thumbnail\" width=\"800\" height=\"400\"></a></p><h2><strong>4. The Most Powerful grouping Operations in History, Bar None</strong></h2><p>Grouping is a common structured data calculation, with corresponding statements and functions... <a href=\"http://dev-resources.site/topic/programming/2289420/the-most-powerful-grouping-operationsin-history-bar-none-23g4\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DThe%2520Most%2520Powerful%2520grouping%2520Operations%2520in%2520History%2C%2520Bar%2520None%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289420%2Fthe-most-powerful-grouping-operationsin-history-bar-none-23g4\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>5. Study Reveals Major Gaps in AI Models' Basic Math Skills - Even GPT-4 Struggles with Simple Counting</strong></h2><p>Study Reveals Major Gaps in AI Models' Basic Math Skills - Even GPT-4 Struggles with Simple Counting <a href=\"http://dev-resources.site/topic/programming/2289153/study-reveals-major-gaps-in-ai-models-basic-math-skills-even-gpt-4-struggles-with-simple-counting-5bd7\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Favtltgw5vwckvaepefge.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>6. What are Topics and Partitions in Kafka?</strong></h2><p>What is a Topic?   A Topic is Kafka's fundamental building block for organizing messages.... <a href=\"http://dev-resources.site/topic/programming/2289323/what-are-topics-and-partitions-in-kafka-31i4\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DWhat%2520are%2520Topics%2520and%2520Partitions%2520in%2520Kafka%3F%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289323%2Fwhat-are-topics-and-partitions-in-kafka-31i4\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>7. Dispatchers e Contextos no Kotlin: Escolhendo o Lugar Certo para Suas Corrotinas</strong></h2><p>1 â€“ IntroduÃ§Ã£o   As corrotinas no Kotlin sÃ£o uma soluÃ§Ã£o moderna para programaÃ§Ã£o... <a href=\"http://dev-resources.site/topic/programming/2240641/dispatchers-e-contextos-no-kotlin-escolhendo-o-lugar-certo-para-suas-corrotinas-3nh2\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftgv4n31hkt39pzwj05uc.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>8. Dispatchers and Contexts in Kotlin: Choosing the Right Place for Your Coroutines</strong></h2><p>1 â€“ Introduction   Coroutines in Kotlin are a modern solution for asynchronous programming.... <a href=\"http://dev-resources.site/topic/programming/2240773/dispatchers-and-contexts-in-kotlin-choosing-the-right-place-for-your-coroutines-oah\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmsaq5lp06r4tvkl364y8.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>9. AI System Learns When to Consult Experts for Better Cause-and-Effect Discovery</strong></h2><p>AI System Learns When to Consult Experts for Better Cause-and-Effect Discovery <a href=\"http://dev-resources.site/topic/programming/2289165/ai-system-learns-when-to-consult-experts-for-better-cause-and-effect-discovery-4kdo\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DAI%2520System%2520Learns%2520When%2520to%2520Consult%2520Experts%2520for%2520Better%2520Cause-and-Effect%2520Discovery%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289165%2Fai-system-learns-when-to-consult-experts-for-better-cause-and-effect-discovery-4kdo\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>10. AI-Powered Code Explorer Creates Better Software Through Systematic Solution Search</strong></h2><p>AI-Powered Code Explorer Creates Better Software Through Systematic Solution Search <a href=\"http://dev-resources.site/topic/programming/2289169/ai-powered-code-explorer-creates-better-software-through-systematic-solution-search-dea\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flsds0brbxc360xeum1t9.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>11. Breakthrough Method Extends Neural Network Learning Phase and Improves Deep Model Training</strong></h2><p>Breakthrough Method Extends Neural Network Learning Phase and Improves Deep Model Training <a href=\"http://dev-resources.site/topic/programming/2289163/breakthrough-method-extends-neural-network-learning-phase-and-improves-deep-model-training-4h3l\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DBreakthrough%2520Method%2520Extends%2520Neural%2520Network%2520Learning%2520Phase%2520and%2520Improves%2520Deep%2520Model%2520Training%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289163%2Fbreakthrough-method-extends-neural-network-learning-phase-and-improves-deep-model-training-4h3l\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>12. AI Brings Still Photos to Life with Natural Facial Animations in Groundbreaking Research</strong></h2><p>AI Brings Still Photos to Life with Natural Facial Animations in Groundbreaking Research <a href=\"http://dev-resources.site/topic/programming/2289151/ai-brings-still-photos-to-life-with-natural-facial-animations-in-groundbreaking-research-37ic\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwjz0y6bdxpgobzn7w5e6.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>13. Breakthrough: Continuous Diffusion Creates More Natural Language AI with Better Performance</strong></h2><p>Breakthrough: Continuous Diffusion Creates More Natural Language AI with Better Performance <a href=\"http://dev-resources.site/topic/programming/2289155/breakthrough-continuous-diffusion-creates-more-natural-language-ai-with-better-performance-34he\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsafqlbfybcj8265gjg6t.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>14. New Study Shows Current AI Models Fail Basic Physics Tests, Highlighting Major Limitations in Scientific Reasoning</strong></h2><p>New Study Shows Current AI Models Fail Basic Physics Tests, Highlighting Major Limitations in Scientific Reasoning <a href=\"http://dev-resources.site/topic/programming/2289150/new-study-shows-current-ai-models-fail-basic-physics-tests-highlighting-major-limitations-in-1124\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqz3vnz8wh3n2m73f08n6.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>15. Large Language Models Extract Information Without Training, Matching Specialized Systems</strong></h2><p>Large Language Models Extract Information Without Training, Matching Specialized Systems <a href=\"http://dev-resources.site/topic/programming/2289142/large-language-models-extract-information-without-training-matching-specialized-systems-nm2\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2lgiiwrjamv4lm8t2fh7.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>16. Making AI Safer: New Methods to Control Step-by-Step AI Reasoning</strong></h2><p>Making AI Safer: New Methods to Control Step-by-Step AI Reasoning <a href=\"http://dev-resources.site/topic/programming/2289146/making-ai-safer-new-methods-to-control-step-by-step-ai-reasoning-131i\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DMaking%2520AI%2520Safer%3A%2520New%2520Methods%2520to%2520Control%2520Step-by-Step%2520AI%2520Reasoning%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289146%2Fmaking-ai-safer-new-methods-to-control-step-by-step-ai-reasoning-131i\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>17. New Test Reveals Major Gaps in AI's Economic Reasoning Skills - Study of 27 Language Models Shows Mixed Results</strong></h2><p>New Test Reveals Major Gaps in AI's Economic Reasoning Skills - Study of 27 Language Models Shows Mixed Results <a href=\"http://dev-resources.site/topic/programming/2289164/new-test-reveals-major-gaps-in-ais-economic-reasoning-skills-study-of-27-language-models-shows-52k4\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DNew%2520Test%2520Reveals%2520Major%2520Gaps%2520in%2520AI%27s%2520Economic%2520Reasoning%2520Skills%2520-%2520Study%2520of%252027%2520Language%2520Models%2520Shows%2520Mixed%2520Results%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289164%2Fnew-test-reveals-major-gaps-in-ais-economic-reasoning-skills-study-of-27-language-models-shows-52k4\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><p>Have you ever wanted to make use of Google Chrome on your mobile and seem to be stuck? I would solve... <a href=\"http://dev-resources.site/topic/programming/2289466/introduction-51o9\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxgjw49xvr8skh8qjxunj.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>19. How AI Helps Reduce False Positives in Cyber Threat Detection?</strong></h2><p>Cybersecurity threats are evolving at an unprecedented pace, and organizations must stay ahead to... <a href=\"http://dev-resources.site/topic/programming/2289341/how-ai-helps-reduce-false-positives-in-cyber-threat-detection-epn\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fhpfl1v5ia73jdeoovnsh.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>20. Best Free Currency Converter APIs for Developers: Features &amp; Integration Guide</strong></h2><p>Applications often require real-time currency exchange data to support international transactions,... <a href=\"http://dev-resources.site/topic/programming/2289436/best-free-currency-converter-apis-for-developers-features-integration-guide-1kpc\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F6fpfaj3cvp57qi7m8bjb.PNG\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>21. How technical seo can transform your search rankings (Beginner's Checklist)</strong></h2><p>Search Engine Optimization (SEO) is often divided into three key areas: on-page SEO, off-page SEO,... <a href=\"http://dev-resources.site/topic/programming/2288849/how-technical-seo-can-transform-your-search-rankings-beginners-checklist-f8j\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffzyb3upa999qvpy9mr5m.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>22. ğŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-20</strong></h2><p>ğŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-20   Every week, thousands of... <a href=\"http://dev-resources.site/topic/programming/2289377/13-most-exciting-github-projects-this-week-2025-02-20-55en\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3D%25F0%259F%2594%25A5%252013%2520Most%2520Exciting%2520GitHub%2520Projects%2520This%2520Week%2520-%25202025-02-20%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289377%2F13-most-exciting-github-projects-this-week-2025-02-20-55en\" alt=\"Thumbnail\" width=\"800\" height=\"400\"></a></p><h2><strong>23. Full-Stack Development Trends 2024: Edge Computing, AI Tools, and WebAssembly Explained</strong></h2><p>Discover the essential full-stack development trends shaping 2024. Learn how edge computing, AI tools, and WebAssembly are transforming development practices. Get insights on modern architectures and tools. <a href=\"http://dev-resources.site/topic/programming/2289480/full-stack-development-trends-2024-edge-computing-ai-tools-and-webassembly-explained-3hm\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fjsschools.com%2Fimages%2F95b5b76f-9b9d-4a1b-a0df-b5236ee397ea.webp\" alt=\"Thumbnail\" width=\"800\" height=\"800\"></a></p><h2><strong>24. Open Source Ai Agents: Exploring Best Ai Agents</strong></h2><p>Artificial Intelligence (AI) has transformed industries worldwide, automating tasks, enhancing... <a href=\"http://dev-resources.site/topic/programming/2289482/open-source-ai-agents-exploring-best-ai-agents-3d27\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DOpen%2520Source%2520Ai%2520Agents%3A%2520Exploring%2520Best%2520Ai%2520Agents%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289482%2Fopen-source-ai-agents-exploring-best-ai-agents-3d27\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>25. The Role of Audit Firms in Dubai: Ensuring Financial Transparency and Compliance</strong></h2><p>Dubai is a thriving global business hub, attracting entrepreneurs and investors from around the... <a href=\"http://dev-resources.site/topic/programming/2289489/the-role-of-audit-firms-in-dubai-ensuring-financial-transparency-and-compliance-5gdk\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DThe%2520Role%2520of%2520Audit%2520Firms%2520in%2520Dubai%3A%2520Ensuring%2520Financial%2520Transparency%2520and%2520Compliance%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289489%2Fthe-role-of-audit-firms-in-dubai-ensuring-financial-transparency-and-compliance-5gdk\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>26. How Trumpâ€™s Tariffs Pump Volatility and Dump Crypto Exchangesâ€™ Capitalizations</strong></h2><p>The original article is published on CoinMarketCap           Analyzing the real impact of Trumpâ€™s... <a href=\"http://dev-resources.site/topic/programming/2289467/how-trumps-tariffs-pump-volatility-and-dump-crypto-exchanges-capitalizations-33a8\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgk0s1x15mki8cs2pbira.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>27. How to Create Custom Helper Functions in Laravel</strong></h2><p>In this tutorial, weâ€™ll walk through the process of creating and using custom helper functions in a... <a href=\"http://dev-resources.site/topic/programming/2289468/how-to-create-custom-helper-functions-in-laravel-2ka2\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DHow%2520to%2520Create%2520Custom%2520Helper%2520Functions%2520in%2520Laravel%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289468%2Fhow-to-create-custom-helper-functions-in-laravel-2ka2\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>28. Southwest Airlines Office in Columbus</strong></h2><p>The Southwest Airlines Office in Columbus serves as a key hub for customer support and airline... <a href=\"http://dev-resources.site/topic/programming/2289472/southwest-airlines-office-in-columbus-170\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DSouthwest%2520Airlines%2520Office%2520in%2520Columbus%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289472%2Fsouthwest-airlines-office-in-columbus-170\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>29. Why Choose DumpsBoss for Your TEAS-Test Preparation?</strong></h2><p>When it comes to exam preparation, the right resources can make all the difference. DumpsBoss has... <a href=\"http://dev-resources.site/topic/programming/2289428/why-choose-dumpsboss-for-your-teas-test-preparation-17g\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DWhy%2520Choose%2520DumpsBoss%2520for%2520Your%2520TEAS-Test%2520Preparation%3F%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289428%2Fwhy-choose-dumpsboss-for-your-teas-test-preparation-17g\" alt=\"Thumbnail\" width=\"800\" height=\"400\"></a></p><h2><strong>30. ğŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-20</strong></h2><p>ğŸ”¥ 13 Most Exciting GitHub Projects This Week - 2025-02-20   Every week, thousands of... <a href=\"http://dev-resources.site/topic/programming/2289268/13-most-exciting-github-projects-this-week-2025-02-20-4014\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3D%25F0%259F%2594%25A5%252013%2520Most%2520Exciting%2520GitHub%2520Projects%2520This%2520Week%2520-%25202025-02-20%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fprogramming%2F2289268%2F13-most-exciting-github-projects-this-week-2025-02-20-4014\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2>\n  \n  \n  50 AI-Powered Money-Making Prompts for Bloggers: Maximize Your Blog's Revenue ğŸš€\n</h2><p>If you're serious about making money from your blog, you already know that AI can be a game-changerâ€”but only if you use it the right way. Thatâ€™s exactly why I created this handpicked collection of 50 high-impact ChatGPT prompts specifically for bloggers who want to boost their revenue, grow their traffic, and scale their content effortlessly.</p><h3>\n  \n  \n  Why This is Different from Any Other Prompt Pack?\n</h3><p>Most AI prompt lists are generic and too broad to be useful. This one is built for bloggers who actually want to make moneyâ€”whether itâ€™s through ad revenue, affiliate marketing, sponsored content, or product sales.</p><p>Each prompt is fully customizable with dynamic fields, meaning you can tailor them to your niche, audience, and goals in just a few seconds. No guesswork, no wasted timeâ€”just AI-driven strategies that work.</p><p>âœ”ï¸ 50 expert-crafted ChatGPT prompts focused on blog monetization\nâœ”ï¸ Fully customizable prompts (swap in your niche, topic, and audience)<p>\nâœ”ï¸ Instant access in PDF format â€“ download and start using immediately</p></p><p>ğŸ”¹ Bloggers who want better content that converts\nğŸ”¹ Affiliate marketers looking for high-converting blog post ideas<p>\nğŸ”¹ Content creators who want to save time while making money</p></p><p>1ï¸âƒ£ Open the PDF and choose a prompt\n2ï¸âƒ£ Customize it with your niche or topic<p>\n3ï¸âƒ£ Use it in ChatGPT to generate money-making blog content instantly</p></p><p>No fluff, no fillerâ€”just 50 prompts that help you create content that makes money.</p><p>ğŸš€ Grab your copy now and start boosting your blogâ€™s revenue today!</p><h3>\n  \n  \n  ğŸ’° <strong>Want to Earn 40% Commission?</strong></h3><p>Join our affiliate program and start making money by promoting ! Earn 40% on every sale you refer.  </p><p>You'll get on average around 5$ per sell and for bundled products it will be around 40$ per sale. (So just share it and make money with worrying about product creation and maintanence)</p>","contentLength":7100,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: WinCse â€“ Integrating AWS S3 with Windows Explorer","url":"https://github.com/cbh34680/WinCse","date":1740074014,"author":"cbh34680","guid":8894,"unread":true,"content":"<p>WinCse is an application that integrates AWS S3 buckets with Windows Explorer. Utilizing WinFsp and the AWS SDK, WinCse allows you to treat S3 buckets as part of your local file system, making file management simpler. The application is currently in development, with plans for additional features and improvements.</p>","contentLength":315,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43117895"},{"title":"Generate synthetic counterparty (CR) risk data with generative AI using Amazon Bedrock LLMs and RAG","url":"https://aws.amazon.com/blogs/machine-learning/generate-synthetic-counterparty-cr-risk-data-with-generative-ai-using-amazon-bedrock-llms-and-rag/","date":1740072265,"author":"Santosh Kulkarni","guid":7311,"unread":true,"content":"<p>Data is the lifeblood of modern applications, driving everything from application testing to <a href=\"https://aws.amazon.com/ai/machine-learning/\" target=\"_blank\" rel=\"noopener\">machine learning</a> (ML) model training and evaluation. As data demands continue to surge, the emergence of <a href=\"https://aws.amazon.com/ai/generative-ai/\" target=\"_blank\" rel=\"noopener\">generative AI</a> models presents an innovative solution. These <a href=\"https://aws.amazon.com/what-is/large-language-model/\" target=\"_blank\" rel=\"noopener\">large language models</a> (LLMs), trained on expansive data corpora, possess the remarkable capability to generate new content across multiple media formatsâ€”text, audio, and videoâ€”and across various business domains, based on provided prompts and inputs.</p><p>In this post, we explore how you can use these LLMs with advanced <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\" target=\"_blank\" rel=\"noopener\">Retrieval Augmented Generation</a> (RAG) to generate high-quality synthetic data for a finance domain use case. You can use the same technique for synthetic data for other business domain use cases as well. For this post, we demonstrate how to generate counterparty risk (CR) data, which would be beneficial for over-the-counter (OTC) derivatives that are traded directly between two parties, without going through a formal exchange.</p><p>OTC derivatives are typically customized contracts between counterparties and include a variety of financial instruments, such as forwards, options, swaps, and other structured products. A counterparty is the other party involved in a financial transaction. In the context of OTC derivatives, the counterparty refers to the entity (such as a bank, financial institution, corporation, or individual) with whom a derivative contract is made.</p><p>For example, in an OTC swap or option contract, one entity agrees to terms with another party, and each entity becomes the counterparty to the other. The responsibilities, obligations, and risks (such as credit risk) are shared between these two entities according to the contract.</p><p>As financial institutions continue to navigate the complex landscape of CR, the need for accurate and reliable risk assessment models has become paramount. For our use case, ABC Bank, a fictional financial services organization, has taken on the challenge of developing an ML model to assess the risk of a given counterparty based on their exposure to OTC derivative data.</p><p>Building such a model presents numerous challenges. Although ABC Bank has gathered a large dataset from various sources and in different formats, the data may be biased, skewed, or lack the diversity needed to train a highly accurate model. The primary challenge lies in collecting and preprocessing the data to make it suitable for training an ML model. Deploying a poorly suited model could result in misinformed decisions and significant financial losses.</p><p>We propose a generative AI solution that uses the RAG approach. RAG is a widely used approach that enhances LLMs by supplying extra information from external data sources not included in their original training. The entire solution can be broadly divided into three steps: indexing, data generation, and validation.</p><p>In the indexing step, we parse, chunk, and convert the representative CR data into vector format using the <a href=\"https://aws.amazon.com/bedrock/amazon-models/titan/\" target=\"_blank\" rel=\"noopener\">Amazon Titan Text Embeddings V2</a> model and store this information in a Chroma vector database. Chroma is an open source vector database known for its ease of use, efficient similarity search, and support for multimodal data and metadata. It offers both in-memory and persistent storage options, integrates well with popular ML frameworks, and is suitable for a wide range of AI applications. It is particularly beneficial for smaller to medium-sized datasets and projects requiring local deployment or low resource usage. The following diagram illustrates this architecture.</p><p>Here are the steps for data indexing:</p><ul><li>The sample CR data is segmented into smaller, manageable chunks to optimize it for embedding generation.</li><li>These segmented data chunks are then passed to a method responsible for both generating embeddings and storing them efficiently.</li><li>The Amazon Titan Text Embeddings V2 API is called upon to generate high-quality embeddings from the prepared data chunks.</li><li>The resulting embeddings are then stored in the Chroma vector database, providing efficient retrieval and similarity searches for future use.</li></ul><p>When the user requests data for a certain scenario, the request is converted into vector format and then looked up in the Chroma database to find matches with the stored data. The retrieved data is augmented with the user request and additional prompts to <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropicâ€™s Claude Haiku</a> on <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a>. Anthropicâ€™s Claude Haiku was chosen primarily for its speed, processing over 21,000 tokens per second, which significantly outpaces its peers. Moreover, Anthropicâ€™s Claude Haikuâ€™s efficiency in data generation is remarkable, with a <a href=\"https://www.anthropic.com/news/claude-3-haiku\" target=\"_blank\" rel=\"noopener\">1:5 input-to-output token ratio</a>. This means it can generate a large volume of data from a relatively small amount of input or context. This capability not only enhances the modelâ€™s effectiveness, but also makes it cost-efficient for our application, where we need to generate numerous data samples from a limited set of examples. Anthropicâ€™s Claude Haiku LLM is invoked iteratively to efficiently manage token consumption and help prevent reaching the maximum token limit. The following diagram illustrates this workflow.</p><p>Here are the steps for data generation:</p><ul><li>The user initiates a request to generate new synthetic counterparty risk data based on specific criteria.</li><li>The Amazon Titan Text Embeddings V2 LLM is employed to create embeddings for the userâ€™s request prompts, transforming them into a machine-interpretable format.</li><li>These newly generated embeddings are then forwarded to a specialized module designed to identify matching stored data.</li><li>The Chroma vector database, which houses previously stored embeddings, is queried to find data that closely matches the userâ€™s request.</li><li>The identified matching data and the original user prompts are then passed to a module responsible for generating new synthetic data.</li><li>Anthropicâ€™s Claude Haiku 3.0 model is invoked, using both the matching embeddings and user prompts as input to create high-quality synthetic data.</li><li>The generated synthetic data is then parsed and formatted into a .csv file using the Pydantic library, providing a structured and validated output.</li><li>To confirm the quality of the generated data, several statistical methods are applied, including quantile-quantile (Q-Q) plots and correlation heat maps of key attributes, providing a comprehensive validation process.</li></ul><p>When validating the synthetic CR data generated by the LLM, we employed Q-Q plots and correlation heat maps focusing on key attributes such as , , and . These statistical tools serve crucial roles in promoting the quality and representativeness of the synthetic data. By using the Q-Q plots, we can assess whether these attributes follow a normal distribution, which is often expected in many clinical and financial variables. By comparing the quantiles of our synthetic data against theoretical normal distributions, we can identify significant deviations that might indicate bias or unrealistic data generation.</p><p>Simultaneously, the correlation heat maps provide a visual representation of the relationships between these attributes and others in the dataset. This is particularly important because it helps verify that the LLM has maintained the complex interdependencies typically observed in real CR data. For instance, we would expect certain correlations between exposure and replacement cost, or between replacement cost and settlement risk. By making sure these correlations are preserved in our synthetic data, we can be more confident that analyses or models built on this data will yield insights that are applicable to real-world scenarios. This rigorous validation process helps to mitigate the risk of introducing artificial patterns or biases, thereby enhancing the reliability and utility of our synthetic CR dataset for subsequent research or modeling tasks.</p><p>Weâ€™ve created a Jupyter notebook containing three parts to implement the key components of the solution. We provide code snippets from the notebooks for better understanding.</p><p>To set up the solution and generate test data, you should have the following prerequisites:</p><ul><li>Python 3 must be installed on your machine</li><li>We recommend that an integrated development environment (IDE) that can run Jupyter notebooks be installed</li><li>You can also create a Jupyter notebook instance using Amazon SageMaker from AWS console and develop the code there.</li><li>You need to have an AWS account with access to Amazon Bedrock and the following LLMs enabled (be careful not to share the AWS account credentials): \n  <ul><li>Amazon Titan Text Embeddings V2</li><li>Anthropicâ€™s Claude 3 Haiku</li></ul></li></ul><p>Here are the steps to setup the environment.</p><pre><code>import sys!{sys.executable} -m pip install -r requirements.txt</code></pre><p>The content of the requirements.txt is given here.</p><pre><code>boto3\nlangchain\nlangchain-community\nstreamlit\nchromadb==0.4.15\nnumpy\njq\nlangchain-aws\nseaborn\nmatplotlib\nscipy</code></pre><p>The following code snippet will perform all the necessary imports.</p><pre><code>from pprint import pprint \nfrom uuid import uuid4 \nimport chromadb \nfrom langchain_community.document_loaders import JSONLoader \nfrom langchain_community.embeddings import BedrockEmbeddings\nfrom langchain_community.vectorstores import Chroma \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter</code></pre><h3>Index data in the Chroma database</h3><p>In this section, we show how indexing of data is done in a Chroma database as a locally maintained open source vector store. This index data is used as context for generating data.</p><p>The following code snippet shows the preprocessing steps of loading the JSON data from a file and splitting it into smaller chunks:</p><pre><code>def load_using_jsonloaer(path):\n    loader = JSONLoader(path,\n                            jq_schema=\".[]\",\n                            text_content=False)\n    documents = loader.load()\n    return documents\n\ndef split_documents(documents):\n    doc_list = [item for item in documents]\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=0)\n    texts = text_splitter.split_documents(doc_list)\n    return texts</code></pre><p>The following snippet shows how an Amazon Bedrock embedding instance is created. We used the Amazon Titan Embeddings V2 model:</p><pre><code>def get_bedrock_embeddings():\n    aws_region = \"us-east-1\"\n    model_id = \"amazon.titan-embed-text-v2:0\" #look for latest version of model\n    bedrock_embeddings = BedrockEmbeddings(model_id=model_id, region_name=aws_region)\n    return bedrock_embeddings</code></pre><p>The following code shows how the embeddings are created and then loaded in the Chroma database:</p><pre><code>persistent_client = chromadb.PersistentClient(path=\"../data/chroma_index\")\ncollection = persistent_client.get_or_create_collection(\"test_124\")\nprint(collection)\n    #     query the database\nvector_store_with_persistent_client = Chroma(collection_name=\"test_124\",\n                                                 persist_directory=\"../data/chroma_index\",\n                                                 embedding_function=get_bedrock_embeddings(),\n                                                 client=persistent_client)\nload_json_and_index(vector_store_with_persistent_client)</code></pre><p>The following code snippet shows the configuration used during the LLM invocation using Amazon Bedrock APIs. The LLM used is Anthropicâ€™s Claude 3 Haiku:</p><pre><code>config = Config(\n    region_name='us-east-1',\n    signature_version='v4',\n    retries={\n        'max_attempts': 2,\n        'mode': 'standard'\n    }\n)\nbedrock_runtime = boto3.client('bedrock-runtime', config=config)\nmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\" #look for latest version of model\nmodel_kwrgs = {\n    \"temperature\": 0,\n    \"max_tokens\": 8000,\n    \"top_p\": 1.0,\n    \"top_k\": 25,\n    \"stop_sequences\": [\"company-1000\"],\n}\n# Initialize the language model\nllm = ChatBedrock(\n    model_id=model_id,\n    model_kwargs=model_kwrgs,\n    client=bedrock_runtime,\n)</code></pre><p>The following code shows how the context is fetched by looking up the Chroma database (where data was indexed) for matching embeddings. We use the same Amazon Titan model to generate the embeddings:</p><pre><code>def get_context(scenario):\n    region_name = 'us-east-1'\n    credential_profile_name = \"default\"\n    titan_model_id = \"amazon.titan-embed-text-v2:0\"\n    kb_context = []\n    be = BedrockEmbeddings(region_name=region_name,\n                           credentials_profile_name=credential_profile_name,\n                           model_id=titan_model_id)\n\n    vector_store = Chroma(collection_name=\"test_124\", persist_directory=\"../data/chroma_index\",\n                      embedding_function=be)\n    search_results = vector_store.similarity_search(scenario, k=3)\n    for doc in search_results:\n        kb_context.append(doc.page_content)\n    return json.dumps(kb_context)</code></pre><p>The following snippet shows how we formulated the detailed prompt that was passed to the LLM. We provided examples for the context, scenario, start index, end index, records count, and other parameters. The prompt is subjective and can be adjusted for experimentation.</p><pre><code># Create a prompt template\nprompt_template = ChatPromptTemplate.from_template(\n    \"You are a financial data expert tasked with generating records \"\n    \"representing company OTC derivative data and \"\n    \"should be good enough for investor and lending ML model to take decisions \"\n    \"and data should accurately represent the scenario: {scenario} \\n \"\n    \"and as per examples given in context: \"\n    \"and context is {context} \"\n    \"the examples given in context is for reference only, do not use same values while generating dataset.\"\n    \"generate dataset with the diverse set of samples but record should be able to represent the given scenario accurately.\"\n    \"Please ensure that the generated data meets the following criteria: \"\n    \"The data should be diverse  and realistic, reflecting various industries, \"\n    \"company sizes, financial metrics. \"\n    \"Ensure that the generated data follows logical relationships and correlations between features \"\n    \"(e.g., higher revenue typically corresponds to more employees, \"\n    \"better credit ratings, and lower risk). \"\n    \"And Generate {count} records starting from index {start_index}. \"\n    \"generate just JSON as per schema and do not include any text or message before or after JSON. \"\n    \"{format_instruction} \\n\"\n    \"If continuing, start after this record: {last_record}\\n\"\n    \"If stopping, do not include this record in the output.\"\n    \"Please ensure that the generated data is well-formatted and consistent.\"\n)</code></pre><p>The following code snippet shows the process for generating the synthetic data. You can call this method in an iterative manner to generate more records. The input parameters include , and . The response data is also formatted into CSV format using the instruction provided by the following:</p><pre><code>output_parser.get_format_instructions():\n\n def generate_records(start_index, count, scenario, context, last_record=\"\"):\n    try:\n        response = chain.invoke({\n            \"count\": count,\n            \"start_index\": start_index,\n            \"scenario\": scenario,\n            \"context\": context,\n            \"last_record\": last_record,\n            \"format_instruction\": output_parser.get_format_instructions(),\n            \"data_set_class_schema\": DataSet.schema_json()\n        })\n        \n        return response\n    except Exception as e:\n        print(f\"Error in generate_records: {e}\")\n        raise e</code></pre><p>Parsing the output generated by the LLM and representing it in CSV was quite challenging. We used a Pydantic parser to parse the JSON output generated by the LLM, as shown in the following code snippet:</p><pre><code>class CustomPydanticOutputParser(PydanticOutputParser):\n    def parse(self, text: str) -&gt; BaseModel:\n        # Extract JSON from the text\n        try:\n            # Find the first occurrence of '{'\n            start = text.index('{')\n            # Find the last occurrence of '}'\n            end = text.rindex('}') + 1\n            json_str = text[start:end]\n\n            # Parse the JSON string\n            parsed_json = json.loads(json_str)\n\n            # Use the parent class to convert to Pydantic object\n            return super().parse_with_cls(parsed_json)\n        except (ValueError, json.JSONDecodeError) as e:\n            raise ValueError(f\"Failed to parse output: {e}\")</code></pre><p>The following code snippet shows how the records are generated in an iterative manner with 10 records in each invocation to the LLM:</p><pre><code>def generate_full_dataset(total_records, batch_size, scenario, context):\n    dataset = []\n    total_generated = 0\n    last_record = \"\"\n    batch: DataSet = generate_records(total_generated,\n                                      min(batch_size, total_records - total_generated),\n                                      scenario, context, last_record)\n    # print(f\"batch: {type(batch)}\")\n    total_generated = len(batch.records)\n    dataset.extend(batch.records)\n    while total_generated &lt; total_records:\n        try:\n            batch = generate_records(total_generated,\n                                     min(batch_size, total_records - total_generated),\n                                     scenario, context, batch.records[-1].json())\n            processed_batch = batch.records\n\n            if processed_batch:\n                dataset.extend(processed_batch)\n                total_generated += len(processed_batch)\n                last_record = processed_batch[-1].start_index\n                print(f\"Generated {total_generated} records.\")\n            else:\n                print(\"Generated an empty or invalid batch. Retrying...\")\n                time.sleep(10)\n        except Exception as e:\n            print(f\"Error occurred: {e}. Retrying...\")\n            time.sleep(5)\n\n    return dataset[:total_records]  # Ensure exactly the requested number of records</code></pre><h3>Verify the statistical properties of the generated data</h3><p>We generated Q-Q plots for key attributes of the generated data: , , and , as shown in the following screenshots. The Q-Q plots compare the quantiles of the data distribution with the quantiles of a normal distribution. If the data isnâ€™t skewed, the points should approximately follow the diagonal line.</p><p>As the next step of verification, we created a corelation heat map of the following attributes: , , , and . The plot is perfectly balanced with the diagonal elements showing a value of 1. The value of 1 indicates the column is perfectly co-related to itself. The following screenshot is the correlation heatmap.</p><p>Itâ€™s a best practice to clean up the resources you created as part of this post to prevent unnecessary costs and potential security risks from leaving resources running. If you created the Jupyter notebook instance in SageMaker please complete the following steps:</p><ol><li>Save and shut down the notebook: <pre><code># First save your work\n# Then close all open notebooks by clicking File -&gt; Close and Halt </code></pre></li><li>Clear the output (if needed before saving): <pre><code># Option 1: Using notebook menu\n# Kernel -&gt; Restart &amp; Clear Output\n\n# Option 2: Using code\nfrom IPython.display import clear_output\nclear_output()</code></pre></li><li>Stop and delete the Jupyter notebook instance created in SageMaker: <pre><code># Option 1: Using aws cli\n# Stop the notebook instance when not in use\naws sagemaker stop-notebook-instance --notebook-instance-name &lt;your-notebook-name&gt;\n\n# If you no longer need the notebook instance\naws sagemaker delete-notebook-instance --notebook-instance-name &lt;your-notebook-name&gt;\n\n# Option 2: Using Sagemager Console\n# Amazon Sagemaker -&gt; Notebooks\n# Select the Notebook and click Actions drop-down and hit Stop.\nClick Actions drop-down and hit Delete</code></pre></li></ol><p>Responsible AI use and data privacy are paramount when using AI in financial applications. Although synthetic data generation can be a powerful tool, itâ€™s crucial to make sure that no real customer information is used without proper authorization and thorough anonymization. Organizations must prioritize data protection, implement robust security measures, and adhere to relevant regulations. Additionally, when developing and deploying AI models, itâ€™s essential to consider ethical implications, potential biases, and the broader societal impact. Responsible AI practices include regular audits, transparency in decision-making processes, and ongoing monitoring to help prevent unintended consequences. By balancing innovation with ethical considerations, financial institutions can harness the benefits of AI while maintaining trust and protecting individual privacy.</p><p>In this post, we showed how to generate a well-balanced synthetic dataset representing various aspects of counterparty data, using RAG-based prompt engineering with LLMs. Counterparty data analysis is imperative for making OTC transactions between two counterparties. Because actual business data in this domain isnâ€™t easily available, using this approach you can generate synthetic training data for your ML models at minimal cost often within minutes. After you train the model, you can use it to make intelligent decisions before entering into an OTC derivative transaction.</p><p>For more information about this topic, refer to the following resources:</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/23/kulsant.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior Moderation Architect with over 16 years of experience, specialized in developing serverless, container-based, and data architectures for clients across various domains. Santoshâ€™s expertise extends to machine learning, as a certified AWS ML specialist. Currently, engaged in multiple initiatives leveraging AWS Bedrock and hosted Foundation models.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/23/joyantab.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior Modernization Architect with AWS ProServe and specializes in building secure and scalable cloud native application for customers from different industry domains. He has developed an interest in the AI/ML space particularly leveraging Gen AI capabilities available on Amazon Bedrock.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/23/mallik.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior Specialist Solutions Architect for generative AI and machine learning at AWS. Mallik works with customers to help them architect efficient, secure and scalable AI and machine learning applications. Mallik specializes in generative AI services Amazon Bedrock and Amazon SageMaker.</p>","contentLength":21910,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DAY 02: PYTHON PROGRAMMING (2/20/2025)","url":"https://dev.to/prashantcod/day-02-python-programming-2202025-3n48","date":1740071766,"author":"Prashant Gyawali","guid":7325,"unread":true,"content":"<p>â€”&gt; Interactive mode in Python                                </p><p>â€”&gt;ROUND THE FLOATING POINT VALUE </p><p>â€”&gt;Using  to sort the functions as you like </p>","contentLength":143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Turbocharging premium audit capabilities with the power of generative AI: Veriskâ€™s journey toward a sophisticated conversational chat platform to enhance customer support","url":"https://aws.amazon.com/blogs/machine-learning/turbocharging-premium-audit-capabilities-with-the-power-of-generative-ai-verisks-journey-toward-a-sophisticated-conversational-chat-platform-to-enhance-customer-support/","date":1740071597,"author":"Sajin Jacob, Jerry Chen, Siddarth Mohanram, Luis Barbier, Kristen Chenowith and Michelle Stahl","guid":7310,"unread":true,"content":"<p><em>This post is co-written with Sajin Jacob, Jerry Chen, Siddarth Mohanram, Luis Barbier, Kristen Chenowith, and Michelle Stahl from Verisk.</em></p><p><a href=\"https://www.verisk.com/\" target=\"_blank\" rel=\"noopener\">Verisk</a> (Nasdaq: VRSK) is a leading data analytics and technology partner for the global insurance industry. Through advanced analytics, software, research, and industry expertise across more than 20 countries, Verisk helps build resilience for individuals, communities, and businesses. The company is committed to ethical and responsible AI development with human oversight and transparency. Verisk is using <a href=\"https://aws.amazon.com/generative-ai/\" target=\"_blank\" rel=\"noopener\">generative AI</a> to enhance operational efficiencies and profitability for insurance clients while adhering to its ethical AI principles.</p><p>Veriskâ€™s Premium Audit Advisory Service (PAASÂ®) is the leading source of technical information and training for premium auditors and underwriters. PAAS helps users classify exposure for commercial casualty insurance, including general liability, commercial auto, and workersâ€™ compensation. PAAS offers a wide range of essential services, including more than 40,000 classification guides and more than 500 bulletins. PAAS now includes PAAS AI, the first commercially available interactive generative-AI chats specifically developed for premium audit, which reduces research time and empower users to make informed decisions by answering questions and quickly retrieving and summarizing multiple PAAS documents like class guides, bulletins, rating cards, etc.</p><p>In this post, we describe the development of the customer support process in PAAS, incorporating generative AI, the data, the architecture, and the evaluation of the results. Conversational AI assistants are rapidly transforming customer and employee support. Verisk has embraced this technology and developed its own PAAS AI, which provides an enhanced self-service capability to the PAAS platform.</p><p>The Verisk PAAS platform houses a vast array of documentsâ€”including class guides, advisory content, and bulletinsâ€”that aid Veriskâ€™s customers in determining the appropriate rules and classifications for workersâ€™ compensation, general liability, and commercial auto business. When premium auditors need accurate answers within this extensive document repository, the challenges they face are:</p><ul><li> â€“ The sheer volume of documents (advisories, bulletins, and so on) makes manual searching time-consuming and inefficient</li><li> â€“ Finding accurate information within this vast repository can be slow, hindering timely decision-making</li><li><strong>Inconsistent quality of responses</strong> â€“ Manual searches might yield irrelevant or incomplete results, leading to uncertainty and potential errors</li></ul><p>To address this issue, Verisk PAAS AI is designed to alleviate the burden by providing round-the-clock support for business processing and delivering precise and quick responses to customer queries. This technology is deeply integrated into Veriskâ€™s newly reimagined PAAS platform, using all of Veriskâ€™s documentation, training materials, and collective expertise. It employs a retrieval augmented generation (RAG) approach and a combination of AWS services alongside proprietary evaluations to promptly answer most user questions about the capabilities of the Verisk PAAS platform.</p><p>When deployed at scale, this PAAS AI will enable Verisk staff to dedicate more time to complex issues, critical projects, and innovation, thereby enhancing the overall customer experience. Throughout the development process, Verisk encountered several considerations, key findings, and decisions that provide valuable insights for any enterprise looking to explore the potential of generative AI.</p><p>When creating an interactive agent using large language models (LLMs), two common approaches are RAG and model fine-tuning. The choice between these methods depends on the specific use case and available data. Verisk PAAS began developing a RAG pipeline for its PAAS AI and has progressively improved this solution. Here are some reasons why continuing with a RAG architecture was beneficial for Verisk:</p><ul><li> â€“ The PAAS platform is constantly evolving, adding new business functions and technical capabilities. Verisk needed to make sure its responses are based on the most current information. The RAG approach allows access to continuously updated data, providing responses with the latest information without frequently retraining the model.</li><li> â€“ Besides data recency, another crucial aspect is the ability to draw from multiple PAAS resources to acquire relevant context. The ease of expanding the knowledge base without the need for fine-tuning new data sources makes the solution adaptable.</li><li> â€“ Retrieval minimizes the risk of hallucinations compared with free-form text generation because responses come directly from the provided excerpts. Verisk developed an evaluation tool to enhance response quality.</li><li> â€“ Although appropriate context can be retrieved from enterprise data sources, the underlying LLM manages the linguistics and fluency.</li><li> â€“ Verisk aimed to consistently improve the PAAS AIâ€™s response generation ability. A RAG architecture offered the transparency required in the context retrieval process, which would ultimately be used to generate user responses. This transparency helped Verisk identify areas where document restructuring was needed.</li><li> â€“ With diverse users accessing the platform and differing data access permissions, data governance and isolation were critical. Verisk implemented controls within the RAG pipeline to restrict data access based on user permissions, helping to ensure that responses are delivered only to authorized users.</li></ul><p>Although both RAG and fine-tuning have their pros and cons, RAG is the best approach for building a PAAS AI on the PAAS platform, given Veriskâ€™s needs for real-time accuracy, explainability, and configurability. The pipeline architecture supports iterative enhancement as the use cases for the Verisk PAAS platform develop.</p><p>The following diagram showcases a high-level architectural data flow that highlights various AWS services used in constructing the solution. Veriskâ€™s system demonstrates a complex AI setup, where multiple components interact and frequently call on the LLM to provide user responses. Employing the PAAS platform to manage these varied components was an intuitive decision.</p><p>The key components are as follows:</p><p>Veriskâ€™s PAAS team determined that ElastiCache is the ideal solution for storing all chat history. This storage approach allows for seamless integration in conversational chats and enables the display of recent conversations on the website, providing an efficient and responsive user experience.</p><p><a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropicâ€™s Claude</a>, available in Amazon Bedrock, played various roles within Veriskâ€™s solution:</p><ul><li> â€“ When building their PAAS AI, Verisk conducted a comprehensive evaluation of leading LLMs, using their extensive dataset to test each modelâ€™s capabilities. Through Amazon Bedrock, Verisk gained streamlined access to multiple best-in-class foundation models (FMs), enabling efficient testing and comparison across key performance criteria. The Amazon Bedrock unified API and robust infrastructure provided the ideal platform to develop, test, and deploy LLM solutions at scale. After this extensive testing, Verisk found Anthropicâ€™s Claude model consistently outperformed across key criteria. Anthropicâ€™s Claude demonstrated superior language understanding in Veriskâ€™s complex business domain, allowing more pertinent responses to user questions. Given the modelâ€™s standout results across Verisk PAAS platform use cases, it was the clear choice to power the PAAS AIâ€™s natural language capabilities.</li><li><strong>Conversation summarization</strong> â€“ When a user asks a follow-up question, the PAAS AI can continue the conversational thread. To enable this, Verisk used Claude to summarize the dialogue to update the context from ElastiCache. The full conversation summary and new excerpts are input to the LLM to generate the next response. This conversational flow allows the PAAS AI to answer user follow-up questions and have a more natural, contextual dialogue, bringing Verisk PAAS closer to having a true AI assistant that can engage in useful, back-and-forth conversations with users.</li><li> â€“ Keywords are extracted from user questions and previous conversations to be used for creating the new summarized prompt and to be input to Veriskâ€™s knowledge base retrievers to perform vector similarity search.</li></ul><h3>Amazon OpenSearch Service</h3><p>Primarily used for the storage of text embeddings, OpenSearch facilitates efficient document retrieval by enabling rapid access to indexed data. These embeddings serve as semantic representations of documents, allowing for advanced search capabilities that go beyond simple keyword matching. This semantic search functionality enhances the systemâ€™s ability to retrieve relevant documents that are contextually similar to the search queries, thereby improving the overall accuracy and speed of data queries. Additionally, OpenSearch functions as a semantic cache for similarity searches, optimizing performance by reducing the computational load and improving response times during data retrieval operations. This makes it an indispensable tool in the larger PAAS ecosystem, where the need for quick and precise information access is paramount.</p><p>The integration of Snowflake in the PAAS AI ecosystem helps provide scalable and real-time access to data, allowing Verisk to promptly address customer concerns and improve its services. By using Snowflakeâ€™s capabilities, Verisk can perform advanced analytics, including sentiment analysis and predictive modeling, to better understand customer needs and enhance user experiences. This continuous feedback loop is vital for refining the PAAS AI and making sure it remains responsive and relevant to user demands.</p><h2>Structuring and retrieving the data</h2><p>An essential element in developing the PAAS AIâ€™s knowledge base was properly structuring and effectively querying the data to deliver accurate answers. Verisk explored various techniques to optimize both the organization of the content and the methods to extract the most relevant information:</p><ul><li> â€“ A key step in preparing the accumulated questions and answers was splitting the data into individual documents to facilitate indexing into OpenSearch Service. Rather than uploading large files containing multiple pages of content, Verisk chunked the data into smaller segments by document section and character lengths. By splitting the data into small, modular chunks focused on a single section of a document, Verisk could more easily index each document and had greater success in pulling back the correct context. Chunking the data also enabled straightforward updating and reindexing of the knowledge base over time.</li><li> â€“ When querying the knowledge base, Verisk found that using just standard vector search wasnâ€™t enough to retrieve all the relevant contexts pertaining to a question. Therefore, a solution was implemented to combine a sparse bm25 search in combination with the dense vector search to create a hybrid search approach, which yielded much better context retrieval results.</li><li><strong>Data separation and filters</strong> â€“ Another issue Verisk ran into was that, because of the vast amount of documents and the overlapping content within certain topics, incorrect documents were being retrieved for some questions that asked for specific topics that were present across multiple sourcesâ€”some of these werenâ€™t needed or appropriate in the context of the userâ€™s question. Therefore, data separation was implemented to split the documents based on document type and filter by line of business to improve context retrieval within the application.</li></ul><p>By thoroughly experimenting and optimizing both the knowledge base powering the PAAS AI and the queries to extract answers from it, Verisk was able to achieve very high answer accuracy during the proof of concept, paving the way for further development. The techniques exploredâ€”hybrid querying, HTML section chunking, and index filteringâ€”became core elements of Veriskâ€™s approach for extracting quality contexts.</p><h2>LLM parameters and models</h2><p>Experimenting with prompt structure, length, temperature, role-playing, and context was key to improving the quality and accuracy of the PAAS AIâ€™s Claude-powered responses. The <a href=\"https://docs.anthropic.com/claude/docs/prompt-engineering\" target=\"_blank\" rel=\"noopener\">prompt design guidelines</a> provided by Anthropic were incredibly helpful.</p><p>Verisk crafted prompts that provided Anthropicâ€™s Claude with clear context and set roles for answering user questions. Setting the temperature to 0 helped reduce the randomness and indeterministic nature of LLM-generated responses.</p><p>Verisk also experimented with different models to improve the efficiency of the overall solution. For scenarios where latency was more important and less reasoning was required, Anthropicâ€™s Claude Haiku was the perfect solution. For other scenarios such as question answering using provided contexts where it was more important for the LLM to be able to understand every detail given in the prompt, Anthropicâ€™s Claude Sonnet was the better choice to balance latency, performance, and cost.</p><p>LLM guardrails were implemented in the PAAS AI project using both the guardrails provided by Amazon Bedrock and specialized sections within the prompt to detect unrelated questions and prompt attack attempts. Amazon Bedrock guardrails can be attached to any Amazon Bedrock model invocation call and automatically detect if the given model input and output are in violation of the language filters that are set (violence, misconduct, sexual, and so on), which helps with screening user inputs. The specialized prompts further improve LLM security by creating a second net that uses the power of the LLMs to catch any inappropriate inputs from the users.</p><p>This allows Verisk to be confident that the model will only answer to its intended purpose surrounding premium auditing services and will not be misused by threat actors.</p><p>After validating several evaluation tools such as <a href=\"https://github.com/confident-ai/deepeval\" target=\"_blank\" rel=\"noopener\">Deepeval</a>, <a href=\"https://github.com/explodinggradients/ragas\" target=\"_blank\" rel=\"noopener\">Ragas</a>, <a href=\"https://github.com/truera/trulens\" target=\"_blank\" rel=\"noopener\">Trulens</a>, and so on, the Verisk PAAS team realized that there were certain limitations to using these tools for their specific use case. Consequently, the team decided to develop its own evaluation API, shown in the above figure.</p><p>This custom API evaluates the answers based on three major metrics:</p><ul><li> â€“ Using LLMs, the process assesses whether the answers provided are relevant to the customerâ€™s prompt. This helps make sure that the responses are directly addressing the questions posed.</li><li> â€“ By using LLMs, the process evaluates whether the context retrieved is appropriate and aligns well with the question. This helps make sure that the LLM has the appropriate and accurate contexts to generate a response.</li><li> â€“ Using LLMs, the process checks if the responses are generated based on their retrieved context or if they are hallucinated. This is crucial for maintaining the integrity and reliability of the information provided.</li></ul><p>This custom evaluation approach helps make sure that the answers generated are not only relevant and contextually appropriate but also faithful to the established generative AI knowledge base, minimizing the risk of misinformation. By incorporating these metrics, Verisk has enhanced the robustness and reliability of their PAAS AI, providing customers with accurate and trustworthy responses.</p><p>The Verisk PAAS team has implemented a comprehensive feedback loop mechanism, shown in the above figure, to support continuous improvement and address any issues that might arise.</p><p>This feedback loop is structured around the following key components:</p><ul><li><strong>Customer feedback analysis</strong> â€“ The team actively collects and analyzes feedback from customers to identify potential data issues or problems with the generative AI responses. This analysis helps pinpoint specific areas that need improvement.</li><li> â€“ After an issue is identified, itâ€™s categorized based on its nature. If itâ€™s a data-related issue, itâ€™s assigned to the internal business team for resolution. If itâ€™s an application issue, a Jira ticket is automatically created for the PAAS IT team to address and fix the problem.</li><li> â€“ The system provides an option to update QA test cases based on the feedback received. This helps make sure that the test scenarios remain relevant and comprehensive, covering a wide range of potential issues.</li><li> â€“ Ground truth agreements, which serve as the benchmark for evaluating LLM response quality, are periodically reviewed and updated. This helps make sure that the evaluation metrics remain accurate and reflective of the desired standards.</li><li> â€“ Regular evaluations of the LLM responses are conducted using the updated QA test cases and ground truth agreements. This helps in maintaining high-quality responses and quickly addressing any deviations from the expected standards.</li></ul><p>This robust feedback loop mechanism enables Verisk to continuously fine-tune the PAAS AI, making sure that it delivers precise, relevant, and contextually appropriate answers to customer queries. By integrating customer feedback, categorizing issues efficiently, updating test scenarios, and adhering to stringent evaluation protocols, Verisk maintains a high standard of service and drives continuous improvement in its generative AI capabilities.</p><p>Verisk initially rolled out the PAAS AI to one beta customer to demonstrate real-world performance and impact. Supporting a customer in this way is a stark contrast to how Verisk has historically engaged with and supported customers in the past, where Verisk would typically have a team allocated to interact with the customer directly. Veriskâ€™s PAAS AI has revolutionized the way subject matter experts (SMEs) work and cost-effectively scales while still providing high-quality assistance. What previously took hours of manual review can now be accomplished in minutes, resulting in an extraordinary 96â€“98% reduction in processing time per specialist. This dramatic improvement in efficiency not only streamline operations but also allows Veriskâ€™s experts to focus on more strategic initiatives that drive greater value for the organization.</p><p>In analyzing this early usage data, Verisk uncovered additional areas where it can drive business value for its customers. As Verisk collects additional information, this data will help uncover what will be needed to improve results and prepare to roll out to a wider customer base of approximately 15,000 users.</p><p>Ongoing development will focus on expanding these capabilities, prioritized based on the collected questions. Most exciting, though, are the new possibilities on the horizon with generative AI. Verisk knows this technology is rapidly advancing and is eager to harness innovations to bring even more value to customers. As new models and techniques emerge, Verisk plans to adapt the PAAS AI to take advantage of the latest capabilities. Although the PAAS AI currently focuses on responding to user questions, this is only the starting point. Verisk plans to quickly improve its capabilities to proactively make suggestions and configure functionality directly in the system itself. The Verisk PAAS team is inspired by the challenge of pushing the boundaries of whatâ€™s possible with generative AI and is excited to test those boundaries.</p><p>Veriskâ€™s development of a PAAS AI for its PAAS platform demonstrates the transformative power of generative AI in customer support and operational efficiency. Through careful data harvesting, structuring, retrieval, and the use of LLMs, semantic search functionalities, and stringent evaluation protocols, Verisk has crafted a robust system that delivers accurate, real-time answers to user questions. By continuing to enhance the PAAS AIâ€™s features while maintaining ethical and responsible AI practices, Verisk is set to provide increased value to its customers, enable staff to concentrate on innovation, and establish new benchmarks for customer service in the insurance sector.</p><p>For more information, see the following resources:</p><p> is the Director of Software Engineering at Verisk, where he leads the Premium Audit Advisory Service (PAAS) development team. In this role, Sajin plays a crucial part in designing the architecture and providing strategic guidance to eight development teams, optimizing their efficiency and ensuring the maintainability of all solutions. He holds an MS in Software Engineering from Periyar University, India.</p><p> is a Lead Software Developer at Verisk, based in Jersey City. He leads the GenAi development team, working on solutions for projects within the Verisk Underwriting department to enhance application functionalities and accessibility. Within PAAS, he has worked on the implementation of the conversational RAG architecture with enhancements such as hybrid search, guardrails, and response evaluations. Jerry holds a degree in Computer Science from Stevens Institute of Technology.</p><p> is the Senior Vice President of Core Lines Technology at Verisk. His area of expertise includes data strategy, analytics engineering, and digital transformation. Sid is head of the technology organization with global teams across five countries. He is also responsible for leading the technology transformation for the multi-year Core Lines Reimagine initiative. Sid holds an MS in Information Systems from Stevens Institute of Technology.</p><p> is the Chief Technology Officer (CTO) of Verisk Underwriting at Verisk. He provides guidance to the development teamsâ€™ architectures to maximize efficiency and maintainability for all underwriting solutions. Luis holds an MBA from Iona University.</p><p>, MSMSL, CPCU, WCP, APA, CIPA, AIS, is PAAS Product Manager at Verisk. She is currently the product owner for the Premium Audit Advisory Service (PAAS) product suite, including PAAS AI, a first to market generative AI chat tool for premium audit that accelerates research for many consultative questions by 98% compared to traditional methods. Kristen holds an MS in Management, Strategy and Leadership at Michigan State University and a BS in Business Administration at Valparaiso University. She has been in the commercial insurance industry and premium audit field since 2006.</p><p>, MBA, CPCU, AIM, API, AIS, is a Digital Product Manager with Verisk. She has over 20 years of experience building and transforming technology initiatives for the insurance industry. She has worked as a software developer, project manager, and product manager throughout her career.</p><p><strong><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/13/ML-17110_author_010.png\" alt=\"\" width=\"100\" height=\"123\">Arun Pradeep Selvaraj</strong> is a Senior Solutions Architect at AWS. Arun is passionate about working with his customers and stakeholders on digital transformations and innovation in the cloud while continuing to learn, build, and reinvent. He is creative, fast-paced, deeply customer-obsessed, and uses the working backward process to build modern architectures to help customers solve their unique challenges. Connect with him on <a href=\"http://www.linkedin.com/in/arun-pradeep-selvaraj-77133112b\" target=\"_blank\" rel=\"noopener\">LinkedIn</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/13/ML-17110_author_011.png\" alt=\"\" width=\"100\" height=\"148\"> is a Solutions Architect Manager at AWS, based out of New York. He helps financial services customers accelerate their adoption of the AWS Cloud by providing architectural guidelines to design innovative and scalable solutions. Coming from a software development and sales engineering background, the possibilities that the cloud can bring to the world excite him.</p><p>, PhD, is a Senior Solutions Architect at AWS, based out of New York. He is aligned with the financial service industry, and is responsible for providing architectural guidelines to design innovative and scalable fintech solutions. He specializes in developing and commercializing artificial intelligence and machine learning products. Connect with him on <a href=\"https://www.linkedin.com/in/apoorvakiran/\" target=\"_blank\" rel=\"noopener\">LinkedIn</a>.</p>","contentLength":23545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Agents from Zero to Hero â€“ Part 1","url":"https://towardsdatascience.com/ai-agents-from-zero-to-hero-part-1/","date":1740071043,"author":"Mauro Di Pietro","guid":7421,"unread":true,"content":"<p> are autonomous programs that perform tasks, make decisions, and communicate with others. Normally, they use a set of tools to help complete tasks. In GenAI applications, these Agents process sequential reasoning and can use external tools (like web searches or database queries) when the LLM knowledge isnâ€™t enough. Unlike a basic chatbot, which generates random text when uncertain, an AI Agent activates tools to provide more accurate, specific responses.</p><p>We are moving closer and closer to the concept of  systems that exhibit a higher level of autonomy and decision-making ability, without direct human intervention. While todayâ€™s AI Agents respond reactively to human inputs, tomorrowâ€™s Agentic AIs proactively engage in problem-solving and can adjust their behavior based on the situation.</p><p>Today, building Agents from scratch is becoming as easy as training a logistic regression model 10 years ago. Back then,  provided a straightforward library to quickly train Machine Learning models with just a few lines of code, abstracting away much of the underlying complexity.</p><p>In this tutorial, Iâ€™m going to show how to <strong>build from scratch different types of AI Agents</strong>, from simple to more advanced systems. I will present some useful Python code that can be easily applied in other similar cases (just copy, paste, run) and walk through every line of code with comments so that you can replicate this example.</p><p>As I said, anyone can have a custom Agent running locally for free without GPUs or API keys. The only necessary library is <a href=\"https://ollama.com/\"></a>(<code>pip install ollama==0.4.7</code>), as it allows users to run LLMs locally, without needing cloud-based services, giving more control over data privacy and performance.</p><p>First of all, you need to download  from the website.&nbsp;</p><p>Then, on the prompt shell of your laptop, use the command to download the selected LLM. Iâ€™m going with Alibabaâ€™s , as itâ€™s both smart and lite.</p><p>After the download is completed, you can move on to Python and start writing code.</p><pre><code>import ollama\nllm = \"qwen2.5\"</code></pre><pre><code>stream = ollama.generate(model=llm, prompt='''what time is it?''', stream=True)\nfor chunk in stream:\n    print(chunk['response'], end='', flush=True)</code></pre><p>Obviously, the LLM per se is very limited and it canâ€™t do much besides chatting. Therefore, we need to provide it the possibility to take action, or in other words, to .</p><p>One of the most common tools is the ability to . In Python, the easiest way to do it is with the famous private browser <a href=\"https://pypi.org/project/duckduckgo-search/\"></a>(<code>pip install duckduckgo-search==6.3.5</code>). You can directly use the original library or import the <a href=\"https://www.langchain.com/\"></a> wrapper (<code>pip install langchain-community==0.3.17</code>).&nbsp;</p><p>With , in order to use a Tool, the function must be described in a dictionary.</p><pre><code>from langchain_community.tools import DuckDuckGoSearchResults\ndef search_web(query: str) -&gt; str:\n  return DuckDuckGoSearchResults(backend=\"news\").run(query)\n\ntool_search_web = {'type':'function', 'function':{\n  'name': 'search_web',\n  'description': 'Search the web',\n  'parameters': {'type': 'object',\n                'required': ['query'],\n                'properties': {\n                    'query': {'type':'str', 'description':'the topic or subject to search on the web'},\n}}}}\n## test\nsearch_web(query=\"nvidia\")</code></pre><p>Internet searches could be very broad, and I want to give the Agent the option to be more precise. Letâ€™s say, Iâ€™m planning to use this Agent to learn about financial updates, so I can give it a specific tool for that topic, like searching only a finance website instead of the whole web.</p><pre><code>def search_yf(query: str) -&gt; str:\n&nbsp;&nbsp;engine = DuckDuckGoSearchResults(backend=\"news\")\n&nbsp; return engine.run(f\"site:finance.yahoo.com {query}\")\n\ntool_search_yf = {'type':'function', 'function':{\n&nbsp; 'name': 'search_yf',\n&nbsp; 'description': 'Search for specific financial news',\n&nbsp; 'parameters': {'type': 'object',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'required': ['query'],\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'properties': {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'query': {'type':'str', 'description':'the financial topic or subject to search'},\n}}}}\n\n## test\nsearch_yf(query=\"nvidia\")</code></pre><p>In my opinion, the most basic Agent should at least be able to choose between one or two Tools and re-elaborate the output of the action to give the user a proper and concise answer.&nbsp;</p><p>First, you need to write a prompt to describe the Agentâ€™s purpose, the more detailed the better (mine is very generic), and that will be the first message in the chat history with the LLM.&nbsp;</p><pre><code>prompt = '''You are an assistant with access to tools, you must decide when to use tools to answer user message.'''&nbsp;\nmessages = [{\"role\":\"system\", \"content\":prompt}]</code></pre><p>In order to keep the chat with the AI alive, I will use a loop that starts with userâ€™s input and then the Agent is invoked to respond (which can be a text from the LLM or the activation of a Tool).</p><p>Up to this point, the chat history could look something like this:</p><p>If the model wants to use a Tool, the appropriate function needs to be run with the input parameters suggested by the LLM in its response object:</p><p>So our code needs to get that information and run the Tool function.</p><p>Now, if we run the full code, we can chat with our Agent.</p><p>LLMs know how to code by being exposed to a large corpus of both code and natural language text, where they learn patterns, syntax, and semantics of <a href=\"https://towardsdatascience.com/tag/programming/\" title=\"Programming\">Programming</a> languages. The model learns the relationships between different parts of the code by predicting the next token in a sequence. In short, LLMs can generate Python code but canâ€™t execute it, Agents can.</p><p>I shall prepare a Tool allowing the Agent to . In Python, you can easily create a shell to run code as a string with the native command .</p><pre><code>import io\nimport contextlib\n\ndef code_exec(code: str) -&gt; str:\\\n&nbsp; &nbsp; output = io.StringIO()\n&nbsp; &nbsp; with contextlib.redirect_stdout(output):\n&nbsp; &nbsp; &nbsp; &nbsp; try:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; exec(code)\n&nbsp; &nbsp; &nbsp; &nbsp; except Exception as e:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(f\"Error: {e}\")\n&nbsp; &nbsp; return output.getvalue()\n\ntool_code_exec = {'type':'function', 'function':{\n&nbsp; 'name': 'code_exec',\n&nbsp; 'description': 'execute python code',\n&nbsp; 'parameters': {'type': 'object',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'required': ['code'],\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'properties': {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'code': {'type':'str', 'description':'code to execute'},\n}}}}\n\n## test\ncode_exec(\"a=1+1; print(a)\")</code></pre><p>Just like before, I will write a prompt, but this time, at the beginning of the chat-loop, I will ask the user to provide a file path.</p><p>Since coding tasks can be a little trickier for LLMs, I am going to add also . By default, during one session, there isnâ€™t a true long-term memory. LLMs have access to the chat history, so they can remember information temporarily, and track the context and instructions youâ€™ve given earlier in the conversation. However, memory doesnâ€™t always work as expected, especially if the LLM is small. Therefore, a good practice is to reinforce the modelâ€™s memory by adding periodic reminders in the chat history.</p><p>Please note that the default memory length in Ollama is 2048 characters. If your machine can handle it, you can increase it by changing the number when the LLM is invoked:</p><pre><code>&nbsp; &nbsp; ## model\n&nbsp; &nbsp; agent_res = ollama.chat(\n&nbsp; &nbsp; &nbsp; &nbsp; model=llm,\n&nbsp; &nbsp; &nbsp; &nbsp; tools=[tool_code_exec],\n&nbsp; &nbsp; &nbsp; &nbsp; options={\"num_ctx\":2048},\n&nbsp; &nbsp; &nbsp; &nbsp; messages=messages)</code></pre><p>In this usecase, the output of the Agent is mostly code and data, so I donâ€™t want the LLM to re-elaborate the responses.</p><p>Now, if we run the full code, we can chat with our Agent.</p><p>This article has covered the foundational steps of creating Agents from scratch using only . With these building blocks in place, you are already equipped to start developing your own Agents for different use cases.&nbsp;</p><p>, where we will dive deeper into more advanced examples.</p><p>Full code for this article: <a href=\"https://github.com/mdipietro09/GenerativeAI/blob/main/Agents_ZeroToHero/notebook.ipynb\"></a></p><p>I hope you enjoyed it! Feel free to contact me for questions and feedback or just to share your interesting projects.</p>","contentLength":7859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python input() Function - Detailed Explanation","url":"https://dev.to/vayolapradeep/python-input-function-detailed-explanation-5b8m","date":1740070101,"author":"vayola pradeep","guid":7292,"unread":true,"content":"<h2>Python input() Function - Detailed Explanation</h2>","contentLength":46,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ğŸš€ The Best 100% French Hosting Solution â€“ Unlimited & High-Performance! ğŸ‡«ğŸ‡·","url":"https://dev.to/jmegnidro/the-best-100-french-hosting-solution-unlimited-high-performance-1mdm","date":1740068001,"author":"Dominique Megnidro","guid":7291,"unread":true,"content":"<p>If you're looking for a <strong>100% French hosting provider</strong> offering , , and <strong>responsive customer support</strong>, then  is the perfect choice for you! ğŸ’¡  </p><p>âœ…  (storage, databases, emails...)<strong>Powerful and optimized servers</strong><strong>Simple and intuitive interface</strong><strong>Ultra-responsive customer support</strong> ğŸ‡«ğŸ‡·  </p><p>I host several projects with them, and I can assure you their <strong>value for money is unbeatable!</strong> ğŸ’¯  </p><p>If you have any questions about hosting or need advice, feel free to reach out! ğŸ˜Š</p>","contentLength":465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Call Center Wondr by BNI 0821-4448-0002","url":"https://dev.to/vanglevan/call-center-wondr-by-bni-0821-4448-0002-1pjn","date":1740067711,"author":"Vang LE","guid":7293,"unread":true,"content":"<p>Nomor Resmi Call Center Wondr BNI 0821-4448-0002. Untuk informasi lebih lanjut, nasabah dapat menghubungi BNI Call Center di 0821-4448-0002.</p><p>Untuk mendapatkan informasi lebih lanjut mengenai gangguan pada bni mobile atau wondr by bni, nasabah dapat menghubungi <a href=\"https://dev.to/jai2002/cs-wondr-by-bni-0821-4448-0002-2962\">CS Wondr BNI 0821-4448-0002</a>.</p>","contentLength":288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ğŸš€ Besoin d'un hÃ©bergement web performant et fiable ? DÃ©couvrez o2switch ! ğŸš€","url":"https://dev.to/jmegnidro/besoin-dun-hebergement-web-performant-et-fiable-decouvrez-o2switch--4i3l","date":1740067681,"author":"Dominique Megnidro","guid":7290,"unread":true,"content":"<p>Si vous cherchez un hÃ©bergeur 100% franÃ§ais, offrant des performances optimales, une bande passante illimitÃ©e et un support client rÃ©actif, alors o2switch est fait pour vous ! ğŸ’¡</p><p>âœ… HÃ©bergement illimitÃ© (stockage, bases de donnÃ©es, emails...)\nâœ… Serveurs puissants et optimisÃ©s\nâœ… Interface simple et intuitive<p>\nâœ… Support client ultra-rÃ©actif ğŸ‡«ğŸ‡·</p></p><p>Jâ€™hÃ©berge plusieurs projets chez eux et je peux vous assurer que leur rapport qualitÃ©/prix est imbattable ! ğŸ’¯</p><p>ğŸ’° Profitez-en dÃ¨s maintenant en passant par ce lien : <a href=\"https://clients.o2switch.fr/offre-hebergement-unique?sc=d05d576c83\" rel=\"noopener noreferrer\">cliquez ici</a></p><p>Si vous avez des questions sur lâ€™hÃ©bergement ou besoin de conseils, nâ€™hÃ©sitez pas Ã  me contacter ! ğŸ˜Š</p>","contentLength":660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ErrGroup: Unlocking Go's Concurrency Power","url":"https://dev.to/leapcell/errgroup-unlocking-gos-concurrency-power-3g2h","date":1740066210,"author":"Leapcell","guid":7266,"unread":true,"content":"<p> is a utility in the official Go library  used for concurrently executing multiple  and handling errors. It implements  based on , providing more powerful functions for concurrent programming.</p><p>Compared with ,  has the following advantages:</p><ol><li>:  is only responsible for waiting for the  to complete and does not handle return values or errors. While  cannot directly handle return values, it can immediately cancel other running  when a  encounters an error and return the first non- error in the  method.</li><li>:  can be used in conjunction with . When a  encounters an error, it can automatically cancel other , effectively controlling resources and avoiding unnecessary work.</li><li><strong>Simplifying Concurrent Programming</strong>: Using  can reduce the boilerplate code for error handling. Developers do not need to manually manage error states and synchronization logic, making concurrent programming simpler and more maintainable.</li><li><strong>Limiting the Number of Concurrency</strong>:  provides an interface to limit the number of concurrent  to avoid overloading, which is a feature that  does not have.</li></ol><h2>\n  \n  \n  Example of Using sync.WaitGroup\n</h2><p>Before introducing , let's first review the usage of .</p><div><pre><code></code></pre></div><div><pre><code>$ go run examples/main.go\nfetch url http://www.google.com/ status 200 OK\nfetch url http://www.golang.org/ status 200 OK\nError: Get \"http://www.somestupidname.com/\": dial tcp: lookup www.somestupidname.com: no such host\n</code></pre></div><p>Typical idiom of :</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Example of Using errgroup.Group\n</h2><p>The usage pattern of  is similar to that of .</p><div><pre><code></code></pre></div><div><pre><code>$ go run examples/main.go\nfetch url http://www.google.com/ status 200 OK\nfetch url http://www.golang.org/ status 200 OK\nError: Get \"http://www.somestupidname.com/\": dial tcp: lookup www.somestupidname.com: no such host\n</code></pre></div><p> provides  to add a cancellation function.</p><div><pre><code></code></pre></div><div><pre><code>$ go run examples/withcontext/main.go\nError:  Get \"http://www.somestupidname.com/\": dial tcp: lookup www.somestupidname.com: no such host\nfetch url http://www.google.com/ status 200 OK\n</code></pre></div><h3>\n  \n  \n  Limiting the Number of Concurrency\n</h3><p> provides  to limit the number of concurrently executing .</p><div><pre><code></code></pre></div><div><pre><code>$  go run examples/main.go\nGoroutine 3 is starting\nGoroutine 1 is starting\nGoroutine 2 is starting\nGoroutine 2 is done\nGoroutine 1 is done\nGoroutine 5 is starting\nGoroutine 3 is done\nGoroutine 6 is starting\nGoroutine 4 is starting\nGoroutine 6 is done\nGoroutine 5 is done\nGoroutine 8 is starting\nGoroutine 4 is done\nGoroutine 7 is starting\nGoroutine 9 is starting\nGoroutine 9 is done\nGoroutine 8 is done\nGoroutine 10 is starting\nGoroutine 7 is done\nGoroutine 10 is done\nAll goroutines complete.\n</code></pre></div><p> provides  to try to start a task, which needs to be used in conjunction with .</p><div><pre><code></code></pre></div><div><pre><code>$ go run examples/main.go\nGoroutine 1 started successfully\nGoroutine 1 is starting\nGoroutine 2 is starting\nGoroutine 2 started successfully\nGoroutine 3 started successfully\nGoroutine 4 could not start (limit reached)\nGoroutine 5 could not start (limit reached)\nGoroutine 6 could not start (limit reached)\nGoroutine 7 could not start (limit reached)\nGoroutine 8 could not start (limit reached)\nGoroutine 9 could not start (limit reached)\nGoroutine 10 could not start (limit reached)\nGoroutine 3 is starting\nGoroutine 2 is done\nGoroutine 3 is done\nGoroutine 1 is done\nAll goroutines complete.\n</code></pre></div><h2>\n  \n  \n  Source Code Interpretation\n</h2><p>The source code of  mainly consists of 3 files:</p><div><pre><code></code></pre></div><ul><li>: An empty structure used to pass signals to control the number of concurrency.</li><li>:\n\n<ul><li>: The function called when the context is cancelled.</li><li>: The internally used .</li><li>: The signal channel that controls the number of concurrent coroutines.</li><li>: Ensures that the error is handled only once.</li><li>: Records the first error.</li></ul></li></ul><ul><li>: Limits the number of concurrency.\n</li></ul><div><pre><code></code></pre></div><ul><li>: Starts a new coroutine to execute the task.\n</li></ul><div><pre><code></code></pre></div><ul><li>: Waits for all tasks to complete and returns the first error.\n</li></ul><div><pre><code></code></pre></div><ul><li>: Tries to start a task.\n</li></ul><div><pre><code></code></pre></div><p> is an official extended library that adds error handling capabilities on the basis of , providing functions such as synchronization, error propagation, and context cancellation. Its  method can add a cancellation function,  can limit the number of concurrency, and  can try to start a task. The source code is ingeniously designed and worthy of reference. </p><p>Finally, I would like to recommend the most suitable platform for deploying golang: </p><h3>\n  \n  \n  1. Multi-Language Support\n</h3><ul><li>Develop with JavaScript, Python, Go, or Rust.\n</li></ul><h3>\n  \n  \n  2. Deploy unlimited projects for free\n</h3><ul><li>pay only for usage â€” no requests, no charges.</li></ul><h3>\n  \n  \n  3. Unbeatable Cost Efficiency\n</h3><ul><li>Pay-as-you-go with no idle charges.\n</li><li>Example: $25 supports 6.94M requests at a 60ms average response time.\n</li></ul><h3>\n  \n  \n  4. Streamlined Developer Experience\n</h3><ul><li>Intuitive UI for effortless setup.\n</li><li>Fully automated CI/CD pipelines and GitOps integration.\n</li><li>Real-time metrics and logging for actionable insights.\n</li></ul><h3>\n  \n  \n  5. Effortless Scalability and High Performance\n</h3><ul><li>Auto-scaling to handle high concurrency with ease.\n</li><li>Zero operational overhead â€” just focus on building.\n</li></ul>","contentLength":4854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PaginaciÃ³n con cursor","url":"https://dev.to/gaston_duarte/paginacion-con-cursor-341d","date":1740066023,"author":"Gaston Duarte","guid":7268,"unread":true,"content":"<p>Al desarrollar aplicaciones que muestran grandes volÃºmenes de datos, es fundamental implementar tÃ©cnicas de paginaciÃ³n para mejorar el rendimiento y la experiencia del usuario. En un e-commerce, por ejemplo, podrÃ­a haber miles de productos en la base de datos, pero el frontend solo necesita mostrar 10 a la vez.</p><p>Enviar todos los registros al frontend para que los almacene en memoria no es una soluciÃ³n eficiente. En su lugar, el backend debe gestionar la paginaciÃ³n de manera efectiva.</p><p>Existen dos enfoques principales para paginar datos:  y .</p><p>Este mÃ©todo utiliza un  para determinar desde quÃ© registro comenzar la consulta.</p><p>Veamos un ejemplo simple:</p><p>Supongamos que tenemos 25 registros en nuestra base de datos, y tenemos una vista que quiere mostrar esos registros de a 10.</p><p>Entonces el frontend nos enviara:</p><div><pre><code>{\n  \"limit\": 10,\n  \"offset\": 0\n}\n</code></pre></div><p>El backend harÃ¡ una consulta a la base de datos:</p><p><code>SELECT * FROM nombre_tabla LIMIT 10 OFFSET 0</code></p><p>Es decir, mostrar 10 registros desde el registro 0 (del 0..9). \nPara obtener los siguientes 10 registros:</p><div><pre><code>{\n  \"limit\": 10,\n  \"offset\": 10\n}\n</code></pre></div><p><code>SELECT * FROM nombre_tabla LIMIT 10 OFFSET 10</code></p><p>Eso mostrara desde el registro 10 al 19 y asÃ­ sucesivamente.</p><h3>\n  \n  \n  Problemas de </h3><ol><li><p><strong>Baja eficiencia en grandes volÃºmenes de datos:</strong> Consultas con  grande pueden volverse costosas, ya que la base de datos debe recorrer muchos registros antes de devolver los deseados.</p></li><li><p><strong>Inconsistencias en datos dinÃ¡micos:</strong> Si se agregan o eliminan registros, la paginaciÃ³n puede omitir o duplicar registros.</p></li></ol><p>AquÃ­ es donde aparece la paginaciÃ³n con cursor, la cual hace que nuestras consulta a la base de datos sean mucho mas performantes.</p><p>En vez de enviar el campo , utilizaremos un campo .</p><h4>\n  \n  \n  Â¿Y como definimos cual es nuestro cursor?\n</h4><p>Bien, la respuesta es depende. Depende de quÃ© datos tenemos guardados en nuestro registro, vayamos al caso mas simple, tener un identificador Ãºnico auto-incremental.</p><p>Supongamos que nuestro registro tiene estos datos:</p><div><pre><code>{\n  \"id\": 1,\n  \"nombre\": \"Juan\"\n},\n{\n  \"id\": 2,\n  \"nombre\": \"Patricia\"\n},\n...\n</code></pre></div><p>El frontend solicita el primer registro con  (sin cursor) y el backend responderÃ¡ ademas del registro a mostrar cual es el cursor que debe enviar en la siguiente request:</p><div><pre><code>{\n  \"limit\": 1,\n  \"cursor\": 2\n  \"user\":{\n       \"id\": 1,\n       \"nombre\": \"Juan\"\n  }\n}\n</code></pre></div><p>Consulta a la base de datos:</p><p><code>SELECT * FROM nombre_tabla LIMIT 1</code></p><p>En la siguiente request que haga el frontend enviara,  y , entonces desde el backend podremos hacer una consulta a la base de datos de este estilo:</p><p><code>SELECT * FROM nombre_tabla WHERE id &gt;= 2 ORDER BY id LIMIT 1</code></p><p>Lo cual traerÃ¡ a partir del registro que contenta id &gt;= 2 y solamente 1.</p><h3>\n  \n  \n  Â¿Cual es la ventaja sobre offset?\n</h3><ol><li><p>: No se recorren registros innecesarios. Simplemente lo limitamos en el .</p></li><li><p>: No se ven afectados registros por insert o delete.</p></li></ol><h3>\n  \n  \n  Ahora, Â¿QuÃ© sucede si no tenemos un id auto-incremental y ordenado?\n</h3><p>Si los registros tienen un  en lugar de un ID incremental, se puede utilizar otro campo como cursor, por ejemplo, :</p><p>Por ejemplo, supongamos registros de este estilo:</p><div><pre><code>{\n  \"uuid\": \"asdn1029nc\",\n  \"nombre\": \"Juan\",\n  \"fecha_nacimiento\": \"2003-02-21\"\n},\n{\n  \"uuid\": \"sap0238gh\",\n  \"nombre\": \"Patricia\",\n  \"fecha_nacimiento\": \"2002-11-04\"\n},\n...\n</code></pre></div><p>Para utilizar  debemos ordenar nuestros registros por algÃºn campo, en este caso la fecha de nacimiento, entonces ahora si el frontend nos pide un registro en la primer request, desde el backend devolveremos:</p><div><pre><code>{\n  \"limit\": 1,\n  \"cursor\": \"2002-11-04\"\n  \"user\":{\n       \"uuid\": \"sap0238gh\",\n       \"nombre\": \"Patricia\",\n       \"fecha_nacimiento\": \"2002-11-04\"\n  }\n},\n...\n</code></pre></div><p>Nuestra consulta a la base de datos serÃ¡:</p><p><code>SELECT * FROM nombre_tabla WHERE fecha_nacimiento &gt; '2002-11-04' ORDER BY fecha_nacimiento LIMIT 1</code></p><h4>\n  \n  \n  Â¿Y que sucede si hay dos registros con la misma fecha, vamos a perder registros?\n</h4><p>Bueno, ahi es donde podemos concatenar dos campos del registro para utilizarlo como cursor para asegurarnos de no perder registros, por ejemplo: <code>cursor=fecha_nacimiento+uuid</code>. Importante siempre en la consulta hacer un <code>order by cursor, fecha_nacimiento</code>.</p><h2>\n  \n  \n  Seguridad: Encodear el cursor\n</h2><p>Es importante utilizar un  en  de nuestro cursor para evitar un . \nEste puede ser un ejemplo de cÃ³digo en Go para encodear el cursor:</p><div><pre><code>func DecodeCursor(encoded string) (string, string, error) {\n    data, err := base64.StdEncoding.DecodeString(encoded)\n    if err != nil {\n        return \"\", \"\", err\n    }\n    parts := strings.Split(string(data), \"|\")\n    if len(parts) != 2 {\n        return \"\", \"\", fmt.Errorf(\"cursor invÃ¡lido\")\n    }\n    return parts[0], parts[1], nil\n}\n\nfunc EncodeCursor(creationDate string, reportID string) string {\n    cursorData := fmt.Sprintf(\"%s|%s\", creationDate, reportID)\n    return base64.StdEncoding.EncodeToString([]byte(cursorData))\n}\n</code></pre></div><p>Si bien  es sencilla y funciona bien con pocos registros,  es mucho mÃ¡s eficiente para grandes volÃºmenes de datos y evita inconsistencias. Dependiendo del caso, se puede utilizar un ID incremental o una combinaciÃ³n de campos como cursor para garantizar un correcto orden y rendimiento.</p>","contentLength":5074,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Basic Selenium â€“ Bonus","url":"https://dev.to/atomictangerline/basic-selenium-bonus-41dp","date":1740065766,"author":"brk","guid":7265,"unread":true,"content":"<h2>\n  \n  \n  The tome of wise practices\n</h2><blockquote><p>Some tips on improving code readability.</p></blockquote><blockquote><p><em>Where the purpose, the author, and the date of creation are inscribed</em></p></blockquote><p>The header, ahh.. the header. That little snippet of code at the top of the script is like a preamble, a clearing of the throat before the real business begins. What purpose does it serve? Well, the header is a kind of introduction, letting the reader know what's to come with some basic information.</p><blockquote><p><em>Let the Code be governed by a singular, mighty function</em></p></blockquote><p>Ah, the main() function - now there's a topic! It's like the heart and soul of your code, the place where everything comes together. See, when you're working on a code project, it's like you're building a big machine, right? And the main() function, well, that's the control panel, the place where you pull all the levers and push all the buttons to make the whole thing rock. In order to use that machine, you gotta follow a certain set of steps right? Turn the key, check the fuel, prime the pump.. Well, that's what the main() function is all about - it's the sequence of events that gets your code up and running.</p><h3>\n  \n  \n  The parameters, guardians of function\n</h3><blockquote><p><em>Each one a gatekeeper, ensuring the proper flow of information.</em></p></blockquote><p>Parameters are those little vessels that carry the lifeblood of our functions.\nNow, I know what you're thinkin' - \"But George, how do I know which one to use?\" Well, my friend, it all comes down to the task at hand.<p>\nIf you're dealing with a relatively simple setup like here, them variables-as-parameters might be just the ticket you need. But if you're working on somethin' a little more complex, with all sorts of moving parts and interdependencies, well, them classes, they're the way to go. Classes, are the real MVPs of the bunch, and they will help keeping everything organized and running smooth as silk.</p></p><h3>\n  \n  \n  The docstrings, the illuminating scrolls\n</h3><blockquote><p><em>Where the function's purpose, its workings, and its returns are documented.</em></p></blockquote><p>Alright, people, let me tell you about these things called docstrings. Now, I know what you're thinking - \"Docstrings? What in the Sam Hill are those?\" Let me break it down for you.\nThese docstrings, they're like little notes, little snippets of information that you tack on to your code, just to give folks a heads up on what's going on. It's like when you're doing some work around the house, and you leave a little note for the neighbour, just to let â€˜em know what you're up to.<p>\nNow, these docstrings, they come in all shapes and sizes. But the way I see it, these docstrings, they're not just about the code, no sir. They're about the people too. So, when you write one of these things, you're not just explaining what the code does, you're telling a story. You're givin' people a little glimpse into your mind and your thought process. And this makes </p> difference because on bigger projects, we all know you will never code alone.</p><h3>\n  \n  \n  Type hints as vigilant sentinels\n</h3><blockquote><p><em>Ensuring the integrity of the data.</em></p></blockquote><p>Listen up, I'm about to let you in on a little secret when it comes to this Python business. It's all about them type hints, they're guardrails that keep you from taking a wrong turn by telling your fellow programmers, \"hey, this is what I'm expecting here, so don't go messing it up, y'hear?\"\nSee, these type hints, they're some kind of guardians of your program, keeping a watchful eye to make sure you don't go trying to mix apples and oranges, so to speak. When you're dealing with a language as flexible as Python, that's a darn good thing to have in your corner. The more you use â€˜em, the more you'll see just how powerful they can be.</p>","contentLength":3619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Basic Selenium â€“ The Easy Peasy Introduction, Chapter 3 of 3","url":"https://dev.to/atomictangerline/basic-selenium-the-easy-peasy-introduction-chapter-3-of-3-3bb7","date":1740065332,"author":"brk","guid":7264,"unread":true,"content":"<p><strong>Automating web-based tasks with Selenium? Efficiently. That's the name of the game here, so.. Take the reins and make technology work for you.</strong></p><p><em>A coding story in three chapters (with a bonus).</em></p><ul><li>\nThe Odyssey of the Code\n\n<ul><li>I. In the realm of the browser, a puppet is forged</li><li>II. The opening of the file</li><li>III. Sentence splitting action unleashed</li><li>IV. The forging of the chunks</li><li>V. Print statements illuminate the path</li><li>VI. The sorcery of the selectors</li><li>VII. The bridge of translation is crossed</li><li>VIII. The file is written</li></ul></li></ul><blockquote><p><strong>The Taming of the Firefox</strong></p></blockquote><h3>\n  \n  \n  I. In the realm of the browser, a puppet is forged\n</h3><blockquote><p>Where the mighty browser is bound to our will, as if by sorcery.</p></blockquote><div><pre><code></code></pre></div><p>First up, we've got this  business. Now, I know what you're thinking - \"George, what in the world is a driver service?\" Well, let me tell you, it's the piece of code that's gonna make this whole Selenium thing work. It's like a bridge between our script and the middleman, I name the ! And after that service is up, the geckoâ€™s gonna turn our Firefox into a nice .\nAnd where do we get this driver service, you ask? From the , of course! That's the path to the executable that's gonna make Firefox dance to our tune. Do you know what else it's gonna do? It's gonna log everything that's happening, right into the  file. That way, if anything goes sideways, we can take a peek under the hood and see whyâ€™s that.\nNow, the options. See, we're creating a shiny new set of , and we're gonna deck them out with some goodies. First up, we're pointing it to the , making sure we've got the right browser to work with.\nAnd then, if that  variable is set to True, we're gonna add a little  argument to the mix. That means the browser's gonna run without a visible window, stealthier than your average ninja. No need for all the bells and whistles (and windows), we're just here to get the job done, right?\nAnd finally, we're wrapping it all up by returning a brand-spankin' new instance of the , complete with our custom service and options. This one's like a well-oiled machine, and we're the ones behind the wheel.\nJust remember: if you ever find yourself scratching your head, wondering what in the world is going on, just take a peek at that  file. It's like a crystal ball, and it might tell you everything you need to know.</p><h3>\n  \n  \n  II. The opening of the file\n</h3><blockquote><p>Like an ancient tome unveiling its secrets.</p></blockquote><div><pre><code></code></pre></div><p>Alright, folks, gather 'round, because we're about to dive into some more of this code. Now, this right here, this is the kind of stuff that separates the wheat from the chaff.\nFirst up, we've got this  block. Now, I know what you're thinking: \"George, what in the world is a  block?\" Well, let me tell you, it's the statement thatâ€™s gonna keep us from running headfirst into a brick wall every time something goes wrong. And trust me, in the universe of programming, something's always going wrong.\nSee, what we're doing here is we're trying to open a file, right? And we're using this fancy-schmancy  function to do it. And that's where the  block comes in.\nIf everything goes according to plan, and the file is there, waiting for us with open arms, we're gonna read the contents and hand 'em back, no problem.  \"But George, what could possibly go wrong?\" Well, my friends, the world is a cruel and unpredictable place ( when youâ€™re learning software development), and sometimes, those files, they just up and disappear. But don't you worry, we've got a plan and that's where the  block comes in.\nIf that  rears its ugly head, we're gonna print out a nice, friendly message, letting the user know that the file they were looking for is.. not there..\nIt's like a safety net, a way to keep the wheels from falling off even when the road gets a little bumpy. And you know what they say, \"the more you plan for the unexpected, the less unexpected it becomes.\" Or something like that.. I don't know, I'm just making it up as I go along.</p><p>Anyway, the point is when life hands you lemons, you make lemonade. And when life hands you , you just smile ;)</p><h3>\n  \n  \n  III. Sentence splitting action unleashed\n</h3><blockquote><p>The fellowship of words is broken.</p></blockquote><div><pre><code></code></pre></div><p>Okay, listen up, because we're about to dive into some serious sentence-splitting action.\nNow, you might be looking at this line of code and thinking: \"what in the world is going on here?\" Well, let me tell you, it's a thing of beauty: we're taking this text that we've been handed, and we're gonna break it down into its individual sentences.<p>\nAnd how are we doing it, you ask? With the power of regular expressions (</p>) and its  implementation, that's how. \"But George, what's a regular expression?\" Well, my friends, it's a language all its own, a way of describing patterns in ways that would make your head spin. You can tinker with it <a href=\"https://regex101.com/r/rsVgaP/1\" rel=\"noopener noreferrer\">here</a>.\nBut don't worry, we're not gonna get too deep into the weeds here. All you need to know is that this little regular expression is the key to our success. It's gonna look for those periods, question marks, and exclamation marks, and it's gonna use them as the boundaries to split our text into individual sentences. But careful, this regex has its limits, see, every language is like a delicate little dance, with each word and phrase movin' in perfect harmony. Some Pros, theyâ€™re like aware of that more than us so they built up nice little tools using carefully crafted and more accurate natural language processing (NLP)  formulas (for example you can experiment with the <a href=\"https://www.nltk.org/\" rel=\"noopener noreferrer\">nltk</a> library). For the sake of simplicity, letâ€™s stick to the basics with the regex way.</p><p>The next time you find yourself staring at a wall of text, wondering how in the world you're gonna make sense of it all, just remember this little line of code and no more trying to figure out where one sentence ends and the next one begins. And who knows, maybe one day, you'll be the one writing the regular expressions turning chaos of strings into order.</p><h3>\n  \n  \n  IV. The forging of the chunks\n</h3><blockquote><p>The awakening of mighty blocks of text.</p></blockquote><div><pre><code></code></pre></div><p>Hmm, we're about to dive into some serious text-chunking action now. You see, we've got this text that we need to translate, but we can't just send the whole thing off to the translation service all at once. Nah, that would be way too easy. Instead, we've gotta break it down into manageable chunks, little bite-sized pieces that the service can handle without breaking a sweat. That's where this  function comes in handy. It's like a master chef, carefully slicing and dicing the text, making sure each piece is the perfect size.\nFirst, we set up a little , a fancy data structure that's gonna help us keeping track of the current chunk.</p><blockquote><p>A , that's just a  way of saying double-ended queue, obviously we deal with small amounts of data here but I wanted to give a try to this exotic thing. Your less sophisticated arrays would work fine there too. Just remember that usual  and  methods donâ€™t perform fast on items on the opposite side of the line. So Pythonâ€™s  module provides that class called deque thatâ€™s specially designed to provide fast and memory-efficient ways to append and pop item from both ends.</p></blockquote><p>And then, we start looping through the sentences of the \"text\" block  (we provided it as an argument when we called the function), and for each sentence, we're gonna figure out its length, -including the space character at the end.\nNow, you might be wondering, \"But George, how do you know when to start a new chunk?\" Well, my friends, it's all about that  variable and the one called  that we use as a counter to keep a running tally of the length of the current chunk. If the current sentence is gonna fit within the chunk size character limit, well, we're just gonna add it on and update the chunk length accordingly. Thatâ€™s the normal way to go. Got it?\nNow, if the length of the current sentence, plus the length of the chunk we've got so far, pushes us  the character limit we've set, well, we're gonna do a few things. First, we're gonna take all the sentences in the chunk and join 'em up with spaces, and then we're gonna yield that beasty. That just means we're gonna spit it out and move on to a new chunk.\nMoving on to the next chunk means we're gonna clear out the chunk variable and start fresh, adding the current sentence to it and resetting the chunk length to just the length of that sentence, and then, just like before, weâ€™ll keep rolling until we hit the limit again.<p>\nFinally, after we've gone through all the sentences, we're gonna yield the last chunk, if there is one.</p></p><p>Thatâ€™s it for our symphony of text-chunking perfection. And who knows.. maybe you'll even find yourself singing the praises of them deques and character limits soon.</p><h3>\n  \n  \n  V. Print statements illuminate the path\n</h3><blockquote><p>The chronicles of progress are Written.</p></blockquote><div><pre><code></code></pre></div><p>Alright, but George, why do we need all these print statements? Isn't the script just supposed to, you know, just work?\"\nWell see, we've got this script that's doing all sorts of stuff, translating text and whatnot. But how are we supposed to know if it's working, huh? That's where these print statements come in, my friends, they're like the canaries in the coal mine. They're the early warning system that's gonna deal us some intel to let us know something's gone awry.<p>\nWe're gonna print out the length of the input text. Because let's be real, how are we supposed to know if this thing is working if we don't know how much text we're dealing with, am I right? It's like trying to bake a cake without knowing how much flour you've got. There's more.. We're also gonna print out the number of chunks that the script has generated. And to top it all off, we're gonna print out the length of each of those chunks. Because, honestly, who doesn't love a good old-fashioned character count? It's like a little treasure hunt, but instead of finding gold, we're finding out how many letters are in each sentence.</p>\nA little  for when you will be contributing to bigger projects: there is that other breed of statements called . They're the quiet ones, they slip in the back door and get the job done without all the fanfare of the print. Just keep in mind they're the ones that keep a careful  when things start to get a little hairy.</p><h3>\n  \n  \n  VI. The sorcery of the selectors\n</h3><blockquote><p>Their spell cast the way.</p></blockquote><div><pre><code></code></pre></div><p>Now itâ€™s time to jump into some serious Selenium wizardry.</p><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F582dkzvjknbphi4nxa9m.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F582dkzvjknbphi4nxa9m.png\" title=\"A screenshot showing the input field of deepL's website.\" alt=\"DeepL website's input field\" width=\"800\" height=\"336\"></a><em>The input field our marionette will search for.</em>\nYou see, we've got this input field that we need to find on the web page, and we can't just go barging in there, hoping it'll be there. Instead, we've gotta harness the power of Selenium, which is gonna help us navigate this digital bytescape.<p>\nTo understand this whole pasta here, We need to know more aboutâ€™em selectors. See, when you're coding a website, just like a building, you got all sorts of elements fitting together - your headings, your paragraphs, buttons, an' the like. Each one of those elements, it's got its own little personality, just like a set of traits and characteristics. And to identify them, you can use all sorts of different selectors:  element selectors, class selectors, ID selectors, and many more other lads. Find out more </p><a href=\"https://www.w3schools.com/cssref/trysel.php\" rel=\"noopener noreferrer\">here</a>.</p><p>That's where this <code>get_input_textarea_element()</code> function comes in. It's like a secret agent, carefully scanning the page, looking for that elusive  where you insert the text you want to translate.\nBut thereâ€™s a challenge to it: what's the point of finding the input field if it's not even ready for us to use? So first, we create a  object, and we instruct it to wait for up to  seconds (we've set it at the beginning as a parameter, by the way) for the element to be loaded and clickable thanks to this intriguing <code>EC.element_to_be_clickable()</code> thing that tells Selenium exactly what we're looking for. In this case, we're saying, \"Hey, Selenium, find me an element that's clickable, and it's gotta match this . But.. how did I found that damn selector we've got here? Well, invoke the inspector by hitting  in Firefox. Then try the  combo, point &amp; clic your target and in the inspector you'll be able to right clic the highlighted piece of code and extract its info.</p><p>Concretely weâ€™re looking to mimic human behavior just like when you paste your first chunk of text. Letâ€™s break it down again:</p><ul><li>You select the input field by clicking on it (itâ€™s selected).</li><li>Depending of your laziness level, either you start typing or you just paste some text into that area.</li></ul><p>Well, our driver should be instructed to do the same.\nNow, I know what you're thinking, \"But George, what if the element never becomes clickable? What then?\" Well, friends, that's where the  comes to the rescue. If the element doesn't become clickable within the  period, the whole thing is gonna throw an exception, and we'll know that something's gone wrong. Maybe the website has been updated and our old CSS selector pal is no more, or a networking problem occured somewhere. Whatever.\nIf everything goes according to plan, and Selenium manages to find that input field and confirm that it's ready for us to use, well, that's when the wonder happens. We're gonna return that element, and the rest of the script is gonna be able to work its magic.</p><h3>\n  \n  \n  VII. The bridge of translation is crossed\n</h3><blockquote><p>Meaning is conveyed across the void.</p></blockquote><div><pre><code></code></pre></div><p>Alright, friends, gather 'round, because we're about to witness the main event, the grand finale and the moment you've all been waiting for - the translation process! Now, I know what you're thinking, \"But George, how in the world is this script gonna take all those chunks of text and turn them into a beautiful translation?\" Well, my friends, watch out.</p><p>First, we're gonna set up an empty list called . Our little treasure trove of linguistic gold. This is where we're gonna store all the translated chunks.\nAnd then, we're gonna start looping through those chunks, one by one. Now, I know what you're thinking, \"But George, how are we gonna keep track of which chunk we're on?\" Well, that's where the  function comes in, my friends. It's gonna give us the  of each chunk, so we can keep tabs on our progress.\nAs we loop through each chunk, we're gonna print out a little separator, just to let the user know that we're hard at work. And then, we're gonna send that chunk of text to the input field, using the  method. It's like we're typing the text over to the translation service, saying, \"Here, take a look at this! A new text chunk for you to translate.\"\nBut we can't just sit back and wait, oh no, that would be way too easy. Instead, we're gonna print out a little message, letting the user know that we're fetching the translation. And then, we're gonna hit the  function, giving the translation service some time to achieve its task.\nWhen the time is up, we're gonna use Selenium again to find the output area on the web page, and we're gonna grab the text that's been translated by now. We're talking about pure algorithmic alchemy, folks, turning one language into another with the click of a button. The thing there is just that weâ€™re not behind the wheels anymore.<p>\nSo once we've put our hands on that translated chunk, we're gonna append it to our translation list. And then, just to be on the safe side, we're gonna clear out the input field, making sure we're ready for the next chunk.</p>\nWhen we've gone through all the chunks, and collected all those translated gems, we're gonna return the whole  list  which is the result of all the translations weâ€™ve collected.</p><h3>\n  \n  \n  VIII. The file is written\n</h3><blockquote><p>The chronicle of the realm is inscribed.</p></blockquote><div><pre><code></code></pre></div><p>The last step starts by calling , and let me tell you, it's one of the unsung hero of this whole operation. Because, let's be honest, what's the point of all this translation process if we can't actually, you know, save the results somewhere?\nSo, here's how it works. First, we're gonna open up a file, using that  statement. And let me tell you, that , it's like a handshake that tells Python, \"Hey, I'm about to do some serious file-handling business, so don't you dare interrupt me!\" And what are we gonna do with this file, you ask? Well, my friends, we're gonna write some text to it. But not just any text, oh no, we're talking about the fruits of our labor, the translated chunks that we've been slaving over for who knows how long.\nNow, I know what you're thinking, \"But George, how are we gonna get all those chunks into the file?\" Well, that's where the  function comes in. It's like a magic wand, taking our list of translated chunks and turning them into a cohesive piece of writing. When that file finally gets saved, it's gonna be all those chunks of text, neatly packaged up into a tangible reality: a file!\nBut you know, it's not just about the end result, folks. It's about the journey, the process of getting there. And this  function, was the final step in that journey. A cherry on top, mic drop moment that says, \"We did it, folks, we translated the heck out of that text!\"</p><ul><li>Managing files (<em>loading a source file and writing results into another one</em>).</li><li>Witnessing that the true sorcery of this tutorial lies in the automated process of translating a text by <em>splitting it into sentences</em>, stacking them into  sent one by one to the <em>online translation service</em>, through Selenium, without ever exceeding a </li><li>And the <a href=\"https://dev.to/atomictangerline/basic-selenium-bonus-41dp\">goodies</a>: raise a glass to the coder's toolkit, by implementing some good practices <em>(using a header, organizing the code using functions, parameters and a main(), use docstrings and type hints)</em>.</li></ul><p>That's just the beginning, isn't it? This Selenium business, it's got so much more to offer. The possibilities, they're practically endless, my friends. <strong>So I want you to take this code, tinker with it, experiment, see what else you can make it do</strong>. Youâ€™re ready to go.\nTry it out on different translation services, see how it handles the variations. Heck, see if you can make it do your taxes while you're at it (okay, maybe not that, but you get the idea). The point is, this is tip of the iceberg. So, what are you waiting for? Good luck!</p><blockquote><p>The code is available on <a href=\"https://github.com/brooks-code/special-octo-telegram\" rel=\"noopener noreferrer\">Github</a>.</p></blockquote><p>(Cover picture: ).</p>","contentLength":17923,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generating ~450 images for $0.50","url":"https://dev.to/peter/generating-450-images-for-050-1elj","date":1740065022,"author":"Peter Kim Frank","guid":7263,"unread":true,"content":"<p>I've been getting more and more interested in the <a href=\"https://bittensor.com/\" rel=\"noopener noreferrer\">Bittensor ecosystem</a>, a decentralized, open-source network interconnected machine learning models.</p><p>One of the more interesting \"subnets\" in the network is <a href=\"https://chutes.ai\" rel=\"noopener noreferrer\">Chutes.ai</a> which provides Serverless AI Compute across a bunch of different LLMs, image models, etc.  I had listened to them in a <a href=\"https://bittensor.guru/s2e6-chutesai-subnet-64-w-namoray-and-jon-durbin\" rel=\"noopener noreferrer\">recent podcast interview</a> and was looking for an excuse to play around.</p><p>I asked DeepSeek-R1 to come up with 50 imaginative prompts:</p><ul><li><em>A celestial cathedral floating atop a spiraling nebula, its stained-glass windows depicting constellations come to life, gilded arches entwined with ivy made of starlight, surrounded by floating orbs of liquid mercury</em></li><li><em>An opulent steampunk airship shaped like a mechanical peacock, its feathers crafted from interlocking brass gears and glowing amber lenses, hovering above a fog-shrouded Victorian metropolis illuminated by gaslamps</em></li><li><em>A surreal garden where trees are composed of cascading sapphire ribbons, their roots embedded in pools of liquid gold, guarded by stone serpents with eyes of smoldering opal under a twilight sky streaked with auroras</em></li></ul><p>And then ran those through 9 different image models:</p><ul></ul><p>All in all, it cost about $0.50 (~$0.001 per image), which I paid for using $TAO, the native currency of Bittensor.</p><p>I used Cursor's new-ish Agent mode to write the Python code to make all of this possible.  The entire project took about ~15-20 minutes of tinkering around in Cursor, and then an hour or two to generate all of the images (which I just let run before going to bed).</p><p>I then took the list of prompts and image directory and (again) had Cursor generate a gallery that I uploaded to Cloudflare Pages.</p><p>Overall, this was a fun little project that has been made possible / much easier through the advent of lower-cost AI models, and code workflow assistants like Copilot, Cursor, Windsurf, Replit Agent, Q Developer, etc.</p>","contentLength":1886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building Mobile Apps Without a Backend: The Power of Database Gateway API","url":"https://dev.to/muhammetberdi_jepbarov/building-mobile-apps-without-a-backend-the-power-of-database-gateway-api-19m0","date":1740064863,"author":"Muhammetberdi Jepbarov","guid":7267,"unread":true,"content":"<h2>\n  \n  \n  The Pain of Backend Development\n</h2><p>If you've ever built a mobile or web app that interacts with a database, you know the struggleâ€”designing API endpoints, handling authentication, writing business logic, and ensuring scalability. Sometimes, all you need is <strong>a simple way to query the database</strong> and retrieve structured JSON responses without going through the entire process of backend development.</p><p>Thatâ€™s exactly why many years ago I built the â€”a solution that allows developers to interact directly with <strong>PostgreSQL and MSSQL databases via a secure API</strong>, without writing a full-fledged backend.</p><h2>\n  \n  \n  The Idea Behind Database Gateway API\n</h2><p>The goal was simple: <strong>Why should you have to build an entire backend when all you need is an API for your database?</strong></p><p>I needed a lightweight yet powerful solution that could:</p><ul><li><strong>Eliminate the need for a dedicated backend</strong> in simple applications.</li><li><strong>Enable mobile and web apps to access database data seamlessly.</strong></li><li><strong>Provide an easy way to deploy APIs</strong> for database interactions with minimal setup.</li><li><strong>Integrate with any existing system</strong>, whether itâ€™s an enterprise  or a <strong>retail point-of-sale (POS) system</strong>.</li></ul><p>This led to the creation of the â€”a framework-agnostic API layer that acts as a bridge between your database and applications.</p><p>The  is a standalone service that connects to your  or  database and <strong>exposes SQL queries as API endpoints</strong>. It takes care of request parsing, security, and response formatting, allowing you to focus on building your application instead of writing backend logic.</p><p>âœ… <strong>Direct SQL Query Execution:</strong> Supports SELECT, INSERT, UPDATE, and DELETE operations via API requests.<strong>Automatic JSON Responses:</strong> Converts database query results into well-structured JSON.<strong>Easy Integration with Mobile &amp; Web Apps:</strong> No need for a complex backendâ€”just plug it into your frontend. Set up role-based access, API keys, and rate limiting.<strong>Supports Complex Queries &amp; Joins:</strong> Fetch relational data easily, just like using SQL directly.<strong>Minimal Deployment Overhead:</strong> Run it as a standalone service or containerized in Docker.  </p><h2>\n  \n  \n  Real-World Impact: 29 Deployments &amp; 150+ Devices\n</h2><p>This API has been successfully deployed across , powering over , primarily in . It integrates seamlessly with <strong>mobile apps and existing accounting systems</strong>, allowing businesses to:</p><ul><li><strong>Sync sales data in real-time</strong>, eliminating manual record-keeping.</li><li><strong>Improve customer experience</strong> by linking mobile apps to inventory and POS systems.</li><li><strong>Enable effortless data exchange between different applications</strong> without writing additional backend logic.</li></ul><p>If youâ€™re a developer building a <strong>mobile app, web dashboard, or prototype</strong>, the  can save you <strong>weeks of development time</strong> by handling database queries and responses automatically.</p><p>Instead of spinning up a full backend, setting up ORM models, and writing CRUD endpoints, <strong>you simply install the gateway, configure it with your database, and start making API requests.</strong></p><p>âœ… Instant API â€“ Set up in minutes and start making SQL queries right away.\nâœ… Effortless Integration â€“ Works with mobile, web, and desktop apps.<p>\nâœ… JSON-Formatted Responses â€“ Your queries return well-structured JSON, ready to use.</p>\nâœ… Perfect for Prototyping â€“ Quickly test database interactions without a full backend.<p>\nâœ… Optimized for Performance â€“ Execute fast queries with minimal setup.</p></p><p>ğŸš€ How It Works\nğŸ” Querying Your Database</p><p>Need to fetch data? Just send a POST request with your SQL query:</p><p>POST 127.0.0.1:8000/api/v1/make-db-request\n{<p>\n  \"query_string\": \"SELECT * FROM tbl_mg_materials\",</p>\n  \"base64_columns\": [\"group_code\", \"image_pict\", \"firm_id_guid\"]</p><p>Pro tip: Use base64_columns to encode image BLOBs or sensitive data!\nğŸ“Š The Response</p><p>The API returns a structured JSON response:</p><p>{\n  \"data\": [...], \n  \"total\": 2, <p>\n  \"message\": \"db query result\"</p>\n}</p><p>âš ï¸ Important Considerations</p><p>ğŸ”´ Security First! â€“ This API executes raw SQL, so make sure to restrict access and validate inputs.\nğŸ”´ Database Changes? â€“ Schema updates might require adjustments to your API queries.<p>\nğŸ”´ Use with Caution â€“ Best for internal tools, rapid prototyping, and trusted environments.</p></p><p>Want to try it out? The  is open-source and available for anyone to use and contribute to. You can integrate it into your next project and <strong>cut down on backend development time significantly</strong>.</p><p>Check out the repository: <p>\nLet me know if you have questions or ideas for improvementsâ€”happy coding! ğŸš€</p></p>","contentLength":4384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Basic Selenium â€“ The Easy Peasy Introduction, Chapter 2 of 3","url":"https://dev.to/atomictangerline/basic-selenium-the-easy-peasy-introduction-chapter-2-of-3-1oad","date":1740064649,"author":"brk","guid":7262,"unread":true,"content":"<p><strong>Automating web-based tasks with Selenium? Efficiently. That's the name of the game here, so.. Take the reins and make technology work for you.</strong></p><p><em>A coding story in three chapters (with a bonus).</em></p><h2>\n  \n  \n  The Fellowship of the Functions\n</h2><p>:\n There's a whole lot going on under the hood, and you're gonna want to know what makes this thing tick.</p><blockquote><p>For the newcomers out there, on using functions:\nSometimes you got bits of code that you need to use over and over again, right? Well, functions, they make your code a whole lot easier to read and understand. I mean, think about it: instead of trying to decipher a tangled web of spaghetti code, you got these nice, neat little packages that do one thing and do it well. And when you need to make a change, well, you just gotta tweak the function, and b-a-m, problem solved. No muss, no fuss.</p><p> Embrace 'em, you love 'em. Trust me, your future self is gonna thank you for it.</p></blockquote><p>First, let's have a look at this  version of the main function:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  The Initialization of the Browser\n</h3><blockquote><p><em>In which the Selenium-forged steed is summoned, and the journey begins.</em></p></blockquote><p>First, we've got the  function. This is where the magic starts - it's setting up a brand new instance of the Firefox browser, all decked out with our custom options.  mode is on by default! No need for a window to appear, we're going full stealth mode here.</p><blockquote><p><em>Where the words are divided into manageable chunks, as if by the wisdom of the regex.</em></p></blockquote><p>Next, we've got . This one's pretty straightforward - it's just reading the contents of a file and handing us back the text.\nThen there's <code>split_text_into_sentences()</code>. This is where the script takes that input text and breaks it down into individual sentences. Gotta make sure we're not overwhelming the translation service, you know? Bite-sized chunks are the way to go.\nAnd speaking of those chunks, that's where  comes in. It's taking those sentences and slicing them, making sure each sentence-block is small enough to play nice with the translation service. No more hitting character limits.</p><h3>\n  \n  \n  The Gathering of the Fields\n</h3><blockquote><p><em>In which the input field is sought and found.</em></p></blockquote><p>Now, the real showstoppers: <code>get_input_textarea_element()</code>. This is the function that use Selenium to find the right spot on the web page to do our work. I mean the input field, where we're gonna pour in our text. Without it, no circus troupe at your fingertips, ready to leap through hoops an' do backflips at your every whim.</p><h3>\n  \n  \n  The Fetching of the Results\n</h3><blockquote><p><em>The final step, where the fruits of the labors are harvested and the story ends.</em></p></blockquote><p>Finally, we've got  and . These are the heavy hitters.  is where the rubber meets the road, sending those sentence chunks off to be translated and bringing back the long awaited goods.  is the grand finale, putting the whole shebang down on paper (or, you know, in a file).\nWhew, that's a lot to take in, I know. But well, once you've got this thing up and running, youâ€™ll see, it's gonna be smooth sailing. Just sit back, let the wind.. Hmm.. the script do its thing.</p><p>(Cover picture: ).</p>","contentLength":3035,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Basic Selenium â€“ The Easy Peasy Introduction, Chapter 1 of 3","url":"https://dev.to/atomictangerline/basic-selenium-the-easy-peasy-introduction-chapter-1-of-3-4fe3","date":1740064114,"author":"brk","guid":7261,"unread":true,"content":"<p><strong>Automating web-based tasks with Selenium? Efficiently. That's the name of the game here, so.. Take the reins and make technology work for you.</strong></p><p><em>A coding story in three chapters (with a bonus).</em></p><p>There I was, sitting on a chair, having to gather all these newspaper translations, from who knows where, in who knows what languages. Let me tell you, it was a real slog. The , just wouldn't let me get the full show. Nothing but snippets, snippets, snippets...</p><p>Then, the idea struck. This script, see, it's gonna let you bypass all that. No more copying and pasting. Just grab a coffee, make it strong, black and wait for the whole enchilada - right there in front of you. A real time-saver, I'm telling you.</p><p>The best part? Turns out, it wasn't just about solving an immediate problem: it was about deepening my know-how, getting my hands dirty with some real-world programming. A win-win, if you ask me.</p><h2>\n  \n  \n  Installation and Configuration\n</h2><p>Alright, let's crack open this code. Step by step, we're gonna peel back the layers of that machine where each piece is fitting in like a puzzle. By the time we're done, you're gonna have a whole new appreciation for Selenium automation and what it can do.</p><p>And beware, you're not just gonna be learning for learning's sake. Nah, this is about equipping you with the tools to tackle your own challenges. Whatever got you tied up in knots, this is your chance to untangle it and make it sing.</p><blockquote><p>\"Time to get this thing installed and ready to rock.\"</p></blockquote><p>First things first - you're gonna need to clone or download the source code. The repository is online <a href=\"https://github.com/brooks-code/special-octo-telegram\" rel=\"noopener noreferrer\">here</a>. Don't worry, I'll wait.</p><p>If you don't already have Firefox on your system, you're gonna need to get that taken care of. For  Ubuntu and other Debianistas folks out there, it's a simple enough fix:</p><div><pre><code>apt update apt firefox\n</code></pre></div><blockquote><p>Itâ€™s also possible to use Chrome instead of Firefox.. Itâ€™s not covered here so if you really want it that way, itâ€™s this way <a href=\"https://developer.chrome.com/docs/chromedriver/get-started\" rel=\"noopener noreferrer\">there</a>.</p></blockquote><p>Next, we've got some packages to install. Nothing too crazy, but we gotta make sure we've got all the tools ready to roll. So fire up that trusty old pip and run:</p><div><pre><code>pip  requirements.txt\n</code></pre></div><p>Boom. Done and done.\nNow, the real kicker - . This is the secret sauce that sits behind the scenes, translating your commands into a language web browsers can understand.\nYou're gonna want to head over to the Mozilla repository, grab the latest version, and get it all set up. I'm talking extraction, permissions, symbolic links - the whole nine yards. And you know the magic? Thereâ€™s a bash spell ready out there for that:</p><div><pre><code>wget https://github.com/mozilla/geckodriver/releases/download/v0.35.0/geckodriver-v0.35.0-linux64.tar.gz  /tmp/geckodriver.tar.gz  /opt  /tmp/geckodriver.tar.gz 755 /opt/geckodriver  /opt/geckodriver /usr/bin/geckodriver  /opt/geckodriver /usr/local/bin/geckodriver\n</code></pre></div><p>That's a mouthful, I know. But trust me, it's worth it. Once you've got all that squared away, you're gonna be almost ready to start automating like never before.</p><p>Alright, folks, time to dive into the nitty-gritty of this configuration stuff. Because let me tell you, if you don't get this part right, the whole thing's gonna be about as useful as.. a lawn mower in the middle of the Mojave.\nFirst up, we've got those  and  variables. Now, I know what you're thinking - \"But how in the world am I supposed to know where these things are hiding on my system?\" Well, fellas, depending on the one youâ€™re looking for, there's a little bash command you can run to ferret that out:</p><p>About the Gecko.. Remember that installation process we went through earlier? Well, the path you set up there is the one you'll want to use here. Not sure about it? No problem, same sauce, Just run:</p><p>And Zap! Paste that info into the right place. Easy, right?\nNow, the  parameter - this one's a bit of a wild card. See, you can run this whole thing in headless mode, which means the browser will run and do its job but won't actually show up on your screen. Kinda like a ninja, you know? But if you want to see what's going on, and watch your Firefox acting like possessed by a spirit you can set it to  and watch the magic unfold â€“ try and see.</p><p>As for the  and , well, pretty self-explanatory. Just pick the languages you want to translate to (source) and from (output), and the script will handle the rest. If you're looking to learn a little more about the available languages, take a glance at that readme file in the repository <a href=\"https://github.com/brooks-code/special-octo-telegram/blob/main/README.md\" rel=\"noopener noreferrer\">here</a>.\nLast but not least, we've got the , , and . These are all about fine-tuning the performance. , and  depend mostly on the speed of your network connection. Itâ€™s about how long we oughta wait for the website to be fully loaded so we can proceed with the translation.  is linked to deepLâ€™s restrictions (currently 1500 characters). Tinker with them as needed, but be careful - you don't want to break the whole machine.\nThe  and  - those are the ones you'll want to point to your own input and output files. Simple enough, right? Just make sure you're pointing the input to something that actually contains text, because that's what this script is built to handle.\nNow, a quick word of warning â€“ the script is not perfect - if the output file doesn't exist yet, the script's gonna go ahead and create it for you. But if it does exist, well, buckle up, because the script's gonna overwrite it every time you run it. So, you know, maybe keep an eye on that (and specify a different output each time) if you donâ€™t want to see your previous achievements disappear..</p><blockquote><p>Wait.. I almost forgot.. You're a  User? WSL now supports running Linux GUI applications in a fully <a href=\"https://learn.microsoft.com/en-us/windows/wsl/tutorials/gui-apps\" rel=\"noopener noreferrer\">integrated experience</a>. On older configurations, you might need <a href=\"https://aalonso.dev/blog/2021/how-to-use-gui-apps-in-wsl2-forwarding-x-server-cdj\" rel=\"noopener noreferrer\">this</a> if you intend to run Firefox as a marionette with its GUI.</p></blockquote><p>Once you've got those parameters all squared away, fire up the terminal, go to the script's folder and just run:</p><p>Ka-boom, the showâ€™s hit the road. The script's gonna take that input file, work its translation-y wonders, and spit out the results into your output file. Fear not, my friend just keep an eye on that output file, and you'll be able to see the fruits of that labor, plain as day.\nWhat are you waiting for? Get out there, update those parameters, and let's see what kind of translation magic you can work!</p><p>(Cover picture: ).</p>","contentLength":6253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Big O Complexity Cheat Sheet for Coding Interviews","url":"https://www.kdnuggets.com/big-o-complexity-cheat-sheet-coding-interviews","date":1740063646,"author":"Bala Priya C","guid":7233,"unread":true,"content":"<article>This is a comprehensive cheat sheet on algorithmic complexity for coding interviews. </article>","contentLength":85,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/bala-big-o.png","enclosureMime":"","commentsUrl":null},{"title":"HN: cypher queries tips (Graph dbms)","url":"https://dev.to/falkordb/hn-cypher-queries-tips-graph-dbms-hck","date":1740062774,"author":"Dan Shalev","guid":7239,"unread":true,"content":"<p>Writing performant Cypher queries isnâ€™t just about syntaxâ€”itâ€™s about understanding graph structures, optimizing query paths, and leveraging advanced features. At FalkorDB, weâ€™ve seen how poorly optimized queries can bottleneck even the most robust systems.</p><p>Most devs donâ€™t realize inefficient Cypher queries often stem from broad MATCH patterns and missing indexes. </p>","contentLength":375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ĞšĞ°Ğº Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Viewscount: Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ² Ğ² Golang","url":"https://dev.to/muhammetberdi_jepbarov/kak-ia-razrabotal-bibliotieku-viewscount-rieshieniie-probliemy-orghanichieskogho-podschieta-prosmotrov-v-golang-1jn0","date":1740061078,"author":"Muhammetberdi Jepbarov","guid":7242,"unread":true,"content":"<p>ĞšĞ°Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸, Ğ¼Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ĞµĞ¼ÑÑ Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ…. Ğ‘ÑƒĞ´ÑŒ Ñ‚Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ  Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ… Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ñ‹ Ğ½Ğµ Ñ…Ğ¾Ñ‚Ğ¸Ñ‚Ğµ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ API Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½ÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ², Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ°Ñ‚Ğ°Ğº, ĞºĞ°Ğº DoS (Denial of Service). Ğ­Ñ‚Ğ¾ Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ñ ÑÑ‚Ğ¾Ğ»ĞºĞ½ÑƒĞ»ÑÑ, Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°, Ğ¿Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ» Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ .</p><p>Ğ’ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğµ â€” Ğ±ÑƒĞ´ÑŒ Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ»Ğ¾Ğ³Ğ°, ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ´Ğ½Ğ°ĞºĞ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ±Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. Ğ’Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¿ÑƒÑ‚ÑŒ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… API Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, Ğ½Ğ¾ ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ½Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ. Ğ Ñ‡Ñ‚Ğ¾ ĞµÑĞ»Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹, Ğ½Ğµ Ğ¿Ğ¸ÑˆĞ° API Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹? Ğ§Ñ‚Ğ¾ ĞµÑĞ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ»ÑĞ±Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ? Ğ˜Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ‚Ğ¾Ñ‚ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ñ Ğ½Ğ°Ñ‡Ğ°Ğ» Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸ĞµĞ¼ .</p><h3>\n  \n  \n  ĞĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ\n</h3><p>ĞšĞ¾Ğ³Ğ´Ğ° Ğ²Ñ‹ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ², Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¸Ğ· Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ°Ñ‚ÑŒÑÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑÑ‡ĞµÑ‚Ñ‡Ğ¸Ğº Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ², Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ â€” Ğ·Ğ´ĞµÑÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸, ĞºĞ°Ğº DoS (Denial of Service). Ğ‘ĞµĞ· Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ°ÑˆĞµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….</p><p>Ğ¢Ğ°Ğº Ğ¿Ğ¾ÑĞ²Ğ¸Ğ»Ğ°ÑÑŒ Ğ¸Ğ´ĞµÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ . Ğ¯ Ñ…Ğ¾Ñ‚ĞµĞ» ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ±Ñ‹ Ğ»ĞµĞ³ĞºĞ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°, Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ»ÑĞ±Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°Ñ Ğ¾Ñ‚ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ° Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ Ğ»ÑĞ±Ñ‹Ğ¼ Golang Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ,  Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ , Ğ¸ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ PostgreSQL.</p><p>ĞšĞ¾Ğ³Ğ´Ğ° Ğ²Ñ‹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚Ğµ  Ğ² ÑĞ²Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹ ÑÑ€Ğ°Ğ·Ñƒ â€” Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ API Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹. Ğ”Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼, Ñƒ Ğ²Ğ°Ñ ĞµÑÑ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº  Ğ¸Ğ»Ğ¸ , ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¸Ğ¼ĞµĞµÑ‚ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ . Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ API ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‡ĞµÑ‚Ñ‡Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ€Ğ°Ğ·, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ, Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ  Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°.</p><p>Ğ’Ğ¾Ñ‚ ĞºĞ°Ğº ÑÑ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ:</p><p>Ğ’ Ğ²Ğ°ÑˆĞµĞ¹ Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ  Ğ² Ğ²Ğ°ÑˆĞ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹:</p><div><pre><code></code></pre></div><p>ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¸ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†. ĞĞµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸Ğ»Ğ¸ API â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ÑÑ‚Ñƒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ².</p><h3>\n  \n  \n  Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Viewscount Ğ² Ğ²Ğ°ÑˆĞµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ\n</h3><p>Ğ¡Ğ°Ğ¼Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸  Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ <strong>Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ğ¾Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ²</strong>. ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚Ğµ Ğ»Ğ¸ Ğ²Ñ‹ ,  Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ , Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞµ Ğ² Ğ²Ğ°ÑˆĞµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ’Ğ¾Ñ‚ ĞºĞ°Ğº Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞµĞµ ĞºĞ°Ğº middleware Ğ² :</p><p>Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ view tracker Ğ² Ğ²Ğ°ÑˆĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸:</p><div><pre><code></code></pre></div><p>ĞšĞ°Ğº Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ middleware Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¾, Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğº Ğ²Ğ°ÑˆĞ¸Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ°Ğ¼. Ğ’Ğ¾Ñ‚ ĞºĞ°Ğº ÑÑ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ÑÑ:</p><div><pre><code></code></pre></div><p>Ğ­Ñ‚Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ  Ğ² Ğ²Ğ°ÑˆĞµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ. Middleware Ğ±ÑƒĞ´ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹ Ğ½Ğ° ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞµ  Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº.</p><ol><li>: Ğ’ÑĞµĞ³Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ¾Ğº ĞºĞ¾Ğ´Ğ° â€” Ğ¸ Ğ²Ñ‹ ÑƒĞ¶Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¸Ğ· Ğ²Ğ°ÑˆĞ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†.</li><li><strong>ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ²</strong>: Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ»ÑĞ±Ñ‹Ğ¼ Golang Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞµ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ÑÑ‚ĞµĞºĞ°.</li><li>: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑÑ‡ĞµÑ‚Ñ‡Ğ¸Ğº Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ²,  Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸.</li><li><strong>Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ</strong>: ĞĞµ Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğµ API Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.</li></ol><p> Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: ĞºĞ°Ğº Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğµ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ API Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹. Ğ›ĞµĞ³ĞºĞ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ´ĞµĞ»Ğ°ÑÑ‚ ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Golang-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.</p><p>ĞĞµ ÑÑ‚ĞµÑĞ½ÑĞ¹Ñ‚ĞµÑÑŒ Ğ²Ğ½ĞµÑÑ‚Ğ¸ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµÑ€ĞµĞ· <a href=\"https://github.com/mikebionic/viewscount\" rel=\"noopener noreferrer\">GitHub Viewscount</a>. Ğ¯ Ğ±ÑƒĞ´Ñƒ Ñ€Ğ°Ğ´ ÑƒĞ²Ğ¸Ğ´ĞµÑ‚ÑŒ, ĞºĞ°Ğº ÑÑ‚Ğ° Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ² Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ!</p>","contentLength":8108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I Developed the Viewscount Library: Solving the Problem of Organic View Counting in Golang","url":"https://dev.to/muhammetberdi_jepbarov/how-i-developed-the-viewscount-library-solving-the-problem-of-organic-view-counting-in-golang-1684","date":1740061016,"author":"Muhammetberdi Jepbarov","guid":7241,"unread":true,"content":"<p>As developers, we often encounter the need to track views on content across various platforms. Whether it's tracking views for articles, videos, or products, most applications rely on a simple  column in their database tables. The challenge arises when you donâ€™t want to go through the hassle of writing a separate API for each table that needs view tracking, while also ensuring that your system is secure from abuse like DoS (Denial of Service) attacks. This is the problem I encountered and the reason I created the  library.</p><p>In most applications, views are tracked in some formâ€”whether itâ€™s for a blog post, a product listing, or even user profile pages. However, implementing a solution that is both scalable and secure can become cumbersome. You could go the route of writing custom APIs for each of these tables, but thatâ€™s time-consuming and doesnâ€™t scale well. What if there was a way to track these views without rewriting APIs for every new table? What if there was a more efficient way to integrate this functionality into any app? Thatâ€™s when I started working on .</p><h3>\n  \n  \n  The Need for a Unified Solution\n</h3><p>When building any system with views tracking, one of the critical concerns is ensuring that the view count remains authentic. In some cases, malicious users may attempt to artificially inflate the view count through repeated requestsâ€”this is where preventing DOS (Denial of Service) type increments comes into play. Without a solid solution, your application becomes vulnerable to these types of attacks, undermining the integrity of your view count.</p><p>Thus, the idea for  was born. I wanted a way to seamlessly count organic views across any tables, regardless of the framework, while providing an easy-to-integrate middleware solution that prevents abusive increments. This library is designed to integrate with any Golang web framework, like , , or even basic , and is database-agnostic, supporting PostgreSQL seamlessly.</p><p>When you add  to your application, you can begin tracking views instantlyâ€”without having to write separate APIs for each table. Letâ€™s say you have tables like  or , each with a  column. Instead of building an API specifically to increment the  each time a user views a page, you can use  to track this automatically.</p><p>Hereâ€™s how it works in practice:</p><p>In your database, you simply add the  column to your tables like so:</p><div><pre><code></code></pre></div><p>The same goes for your other tables. No need for complicated queries or APIsâ€”just a simple column to track views. </p><h3>\n  \n  \n  Integrating Viewscount into Your Application\n</h3><p>The best part of  is that itâ€™s designed to be . Whether you're using , , or even , you can easily integrate it into your application. Hereâ€™s how you can add it as middleware in :</p><p>First, youâ€™ll need to initialize the view tracker in your app:</p><div><pre><code></code></pre></div><p>Once youâ€™ve set up the middleware, you can add it to your routes. Hereâ€™s how you do it:</p><div><pre><code></code></pre></div><p>This simple setup integrates the  library into your existing app. The middleware will automatically track views on the specified table and increment the  column as needed, while also providing protection against abuse.</p><h3>\n  \n  \n  The Benefits of Viewscount\n</h3><ol><li>: With just a few lines of code, you can integrate view counting into any of your existing tables.</li><li>: The library is designed to work with any Golang web framework, making it easy to use in your project regardless of the stack.</li><li><strong>Prevention of DOS Attacks</strong>: By limiting rapid requests that could artificially inflate view counts,  ensures that the data remains reliable and accurate.</li><li>: No need for custom APIs or complex queriesâ€”just use the library and track views automatically in real time.</li></ol><p> solves a common problem in application development: how to track views across tables securely and efficiently without having to build a separate API for each table. Its easy integration, ability to prevent DOS-style attacks, and framework-agnostic design make it a powerful tool for developers building modern Golang applications.</p><p>Feel free to contribute to the project and raise issues or suggestions via <a href=\"https://github.com/mikebionic/viewscount\" rel=\"noopener noreferrer\">Viewscount GitHub</a>. Iâ€™d love to see how this library can help others in their development journey!</p>","contentLength":4129,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Day -02 of learning python programming language..","url":"https://dev.to/kapil_kumarshahsonar_ad/day-02-of-learning-python-programming-language-22pl","date":1740060678,"author":"KAPIL SHAH","guid":7240,"unread":true,"content":"<h3>\n  \n  \n  The thing i learned from python today are as follow:\n</h3><p>simply tuples is immutable which means that we can change the data again in list.</p><div><pre><code></code></pre></div><p>In tuples list  are stored  in curly bracket({}).</p><p>It is quite faster than that of list.</p><p>simply we can say that  list is mutable which means that  we  can edit the list  as our wish anytime.</p><p>list student={kapil , Prashant , rajan}</p><p>we can use function which are inbuilt ;\nlist. Append()=for adding element in list .</p><p>list .insert()=for adding an element wherever you like to .</p><p>list. Replace()=For replacing the data </p><p>it is quite slower comparing to tuples .</p><p>numbers = range(5,10,2)\nfor number in range(5):</p><p>In list data are stored in the square bracket  ([ ]).</p><p>Data can be edited easily .</p><p>The most important part of this idea is def( define function ) </p><p>def sigma(to=\"world\"):\nprint(\"noob,\", to)  # Added a space after the comma for better formatting</p><p>sigma()\nname = input(\"What's your name?&nbsp;\")</p><p>If you have any query related to above  query than feel to ask.</p><p>def main():\nname = input(\"What's your name?\")</p><p>def hello(to=\"world\"):\nprint(\"hello\", name)</p><p>Thatâ€™s all for todayâ€™s python course..</p>","contentLength":1112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hosting Khoj for Free: Your Personal Autonomous AI App","url":"https://www.kdnuggets.com/hosting-khoj-free-personal-autonomous-ai-app","date":1740060515,"author":"Abid Ali Awan","guid":7232,"unread":true,"content":"<article>Turn your local LLMs into a personal, autonomous AI application that can effortlessly retrieve answers from the web or your documents.</article>","contentLength":134,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/awan_hosting_khoj_free_personal_autonomous_ai_app_2.png","enclosureMime":"","commentsUrl":null},{"title":"Pandas â€” For Beginners","url":"https://dev.to/akshayak8/pandas-for-beginners-11gd","date":1740059680,"author":"akshay","guid":7209,"unread":true,"content":"<p>In todayâ€™s data-driven world, data analysis plays a crucial role in transforming large amount of raw data into understandable visualization. This process enables organizations to make informed decisions. By analyzing data, businesses can gain a competitive edge, better understand customer behavior, and personalize their offerings.</p><p>Python has emerged as a popular language for data analysis due to its simplicity and versatile ecosystem of libraries. Among these, Pandas stands out as a powerful tool, providing high-performance data manipulation and analysis capabilities. With Pandas, analysts can handle large datasets with ease, perform complex operations, and gain deep insights.</p><p>Pandas is a powerful open-source library for data manipulation and analysis, primarily used with the Python programming language. It was developed by â€œWes McKinney in 2008â€ while he was working at AQR Capital Management, a quantitative investment management firm.</p><h2>\n  \n  \n  Key Features and Functionality\n</h2><ul><li><p> Pandas offers two primary data structures â€” Series (one-dimensional) and DataFrame (two-dimensional) â€” which are highly flexible and efficient for handling structured data.</p></li><li><p> It provides powerful tools for data manipulation, including filtering, grouping, merging, reshaping, and pivoting data sets.</p></li><li><p> Essential functions for handling missing data, duplicate data, and data type conversions, which are crucial for preparing data for analysis.</p></li><li><p> we can able to integrates with other libraries and tools in the Python ecosystem, such as NumPy, Matplotlib, and SciPy, enhancing its functionality for comprehensive data analysis and visualization.</p></li></ul><ol><li><p><strong>Ensure Python is Installed:</strong> Make sure you have Python installed on your system. You can download it from python.org.</p></li><li><p>\nOpen your command prompt or terminal and run the following command:</p></li></ol><ul><li><strong>Basic setup and configuration.</strong></li></ul><ul><li><strong>Loading and Inspecting Data</strong></li></ul><p>Various types of data like CSV, Excel, SQL databases, etc can load using below command</p><div><pre><code># Read a csv file\ndf = pd.read_csv('path_to_your_file.csv')\n\n# Read an Excel file\ndf = pd.read_excel('path_to_your_file.xlsx', sheet_name='Sheet1')\n\n# Read Sql data\nfrom sqlalchemy import create_engine\nengine = create_engine('sqlite:///path_to_your_database.db')  \ndf = pd.read_sql('SELECT * FROM your_table_name', con=engine)\n\n# Read a JSON data\ndf = pd.read_json('path_to_your_file.json')\n\nprint(df.head())\n</code></pre></div><p>, , , , and more.</p><p> : Displays the first few rows of the DataFrame.</p><p> : Displays the last few rows of the DataFrame.</p><p> : Provides a concise summary of the DataFrame, including data types and missing values.</p><p> : Generates descriptive statistics for numerical columns in the DataFrame, such as count, mean, standard deviation, minimum, maximum, and quartiles.</p><p>: Returns the column labels of the DataFrame.</p><p> : Returns the data types of each column in the DataFrame.</p><p> : Returns the number of unique values in each column.</p><h2>\n  \n  \n  Data Cleaning and Preparation\n</h2><p>, ,  and interpolation.</p><p> Drops rows or columns containing missing values.</p><div><pre><code># Drop rows with missing values\ndf.dropna()\n\n# Drop columns with missing values\ndf.dropna(axis=1)\n</code></pre></div><p> Fills missing values with specified values.</p><div><pre><code># Fill missing values with a specific value\ndf.fillna(value=0)\n\n# Fill missing values with the mean of the column\ndf.fillna(df.mean())\n</code></pre></div><p> Returns a DataFrame of boolean values indicating missing values.</p><div><pre><code># Check for missing values\ndf.isnull()\n</code></pre></div><p> renaming columns, changing data types, and handling duplicates.</p><div><pre><code># Rename columns\ndf.rename(columns={'old_name': 'new_name'}, inplace=True)\n</code></pre></div><div><pre><code># Convert data types of a column\ndf['column_name'] = df['column_name'].astype('int')\n</code></pre></div><div><pre><code># Drop duplicates\ndf.drop_duplicates()\n\n# Keep the first occurrence of duplicates\ndf.drop_duplicates(keep='first')\n\n# Keep the last occurrence of duplicates\ndf.drop_duplicates(keep='last')\n</code></pre></div><h2>\n  \n  \n  Data Analysis and Exploration with Pandas\n</h2><p>Once the data is cleaned and prepared, the next steps involve performing descriptive statistics. Below are detailed examples of how to perform these tasks using Pandas.</p><ol><li><strong>Basic Descriptive Statistics:</strong></li></ol><div><pre><code># Generate descriptive statistics for numerical columns\ndf.describe()\n</code></pre></div><ol><li><strong>Individual Statistic Calculations:</strong></li></ol><div><pre><code># Mean\nmean_age = df['age'].mean()\n\n# Median\nmedian_income = df['income'].median()\n\n# Standard Deviation\nstd_income = df['income'].std()\n\n# Variance\nvar_income = df['income'].var()\n\n# Minimum\nmin_age = df['age'].min()\n\n# Maximum\nmax_age = df['age'].max()\n\n# Count\ncount_gender = df['gender'].count()\n\nprint(f\"Mean Age: {mean_age}, Median Income: {median_income}\")\n</code></pre></div><h2>\n  \n  \n  Data Visualization with Pandas\n</h2><p>Data visualization is a key aspect of data analysis, helping to uncover patterns, trends, and insights that might not be apparent from the raw data alone. Pandas offers built-in plotting capabilities through its integration with Matplotlib, and it can also be used in conjunction with other libraries like Seaborn for more advanced visualizations.</p><p><strong>Basic Plotting with Pandas</strong></p><ol></ol><div><pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sample data\ndf = pd.DataFrame({\n    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May'],\n    'Sales': [200, 220, 250, 270, 300]\n})\n\n# Line plot\ndf.plot(x='Month', y='Sales', kind='line')\n\n# Bar plot\ndf.plot(x='Month', y='Sales', kind='bar')\n\n# Histogram\ndf['Age'].plot(kind='hist', bins=5)\n\n# Scatter plot\ndf.plot(x='Height', y='Weight', kind='scatter')\n\nplt.title('Monthly Sales')\nplt.xlabel('Month')\nplt.ylabel('Sales')\nplt.show()\n</code></pre></div><p>Pandas is an essential tool in numerous industries due to its powerful data manipulation and analysis capabilities. Here are some key examples of how Pandas is utilized in different fields:</p><ul><li> Financial analysts use Pandas to process and analyze stock price data. By calculating daily returns, moving averages, and plotting price trends, they can make informed investment decisions and identify market patterns.</li></ul><ul><li> Healthcare professionals leverage Pandas to analyze patient data, such as blood pressure readings, over time. This helps in identifying trends, monitoring patient health, and conducting epidemiological research.</li></ul><ul><li> Marketers use Pandas for customer segmentation by analyzing purchasing behaviors. Techniques such as clustering help identify different customer groups, allowing for targeted marketing strategies and personalized promotions.</li></ul><p>To master Pandas and stay updated with the latest tips and techniques, itâ€™s essential to leverage various resources, including official documentation, books, online courses, blogs, and communities. Here are some recommended resources:</p><p><strong>Official Pandas Documentation</strong></p><ul><li>The official documentation is comprehensive and includes tutorials, API references, and user guides. Itâ€™s an excellent starting point for understanding Pandas functionalities in depth.</li></ul><ul><li>An active community for data scientists, Kaggle hosts datasets, competitions, and discussions. Many notebooks on Kaggle demonstrate the use of Pandas for various analyses.</li></ul><p>Pandas is a critical tool for both data analysis and data manipulation, offering a comprehensive set of functionalities that cover the entire data workflow. Whether youâ€™re cleaning data, transforming it, analyzing trends, or visualizing results, Pandas provides the necessary tools to handle and analyze data effectively.</p>","contentLength":7182,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}